Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Cloners),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Regression),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Reviewer),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Since Version),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
test-clientutil target is failing,CASSANDRA-4566,12604450,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,dbrosius@apache.org,urandom,urandom,22/Aug/12 16:00,12/Mar/19 14:20,13/Mar/19 22:26,06/Sep/12 04:05,1.2.0,,Legacy/Tools,Packaging,,0,lhf,,,,,,"The {{test-clientutil}} target is failing (on trunk at least):

{noformat}
    [junit] java.lang.NoClassDefFoundError: org/apache/cassandra/io/IVersionedSerializer
    [junit] 	at java.lang.ClassLoader.defineClass1(Native Method)
    [junit] 	at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
    [junit] 	at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
    [junit] 	at org.apache.cassandra.utils.UUIDGen.<clinit>(UUIDGen.java:38)
    [junit] 	at org.apache.cassandra.cql.jdbc.ClientUtilsTest.test(ClientUtilsTest.java:59)
    [junit] Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.io.IVersionedSerializer
    [junit] 	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    [junit] 	at java.security.AccessController.doPrivileged(Native Method)
    [junit] 	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
{noformat}

This target is to ensure that the clientutil jar has no unsatisfied dependencies.  In this case it looks like static initialization is pulling in {{o.a.c.io.IVersionedSerializer}}, (probably transitively).",,,,,,,,,,,,,,,,,,,05/Sep/12 15:04;urandom;0001-4566-v2.txt.patch;https://issues.apache.org/jira/secure/attachment/12543862/0001-4566-v2.txt.patch,25/Aug/12 22:20;dbrosius@apache.org;4566.txt;https://issues.apache.org/jira/secure/attachment/12542430/4566.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-25 22:20:33.457,,,no_permission,,,,,,,,,,,,256252,,,Thu Sep 06 04:05:37 UTC 2012,,,,,,0|i0gx73:,96808,urandom,urandom,,,,,,,,,,"25/Aug/12 22:20;dbrosius@apache.org;pull out UUIDSerializer to a top level class, so that it isn't referenced from UUIDGen.

Also remove dependency on guava just for Charsets in the clientutil.jar","05/Sep/12 15:07;urandom;+1 for the approach, but please remove Guava from the classpath of the {{test-clientutil-jar}} target in build.xml, and the unused Guava imports in {{o.a.c.cql.jdbc.Jdbc\{Ascii,UTF8\}}}, (0001-4566-v2.txt.patch does all of this).",05/Sep/12 20:34;dbrosius@apache.org;looks good.,06/Sep/12 04:05;dbrosius@apache.org;committed as 978d7bb7d59492129aee2747308d55f949003b02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UNREACHABLE schema after decommissioning a non-seed node,CASSANDRA-4115,12549543,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,tpatterson,tpatterson,04/Apr/12 17:43,12/Mar/19 14:20,13/Mar/19 22:27,22/May/12 16:29,1.1.1,,,,,0,,,,,,,"decommission a non-seed node, sleep 30 seconds, then use thrift to check the schema. UNREACHABLE is listed:

{'75dc4c07-3c1a-3013-ad7d-11fb34208465': ['127.0.0.1'],
 'UNREACHABLE': ['127.0.0.2']}",ccm using the following unavailable_schema_test.py dtest.,,,,,,,,,,,,,,,,,,22/May/12 15:07;brandon.williams;4115-v2.txt;https://issues.apache.org/jira/secure/attachment/12528602/4115-v2.txt,06/Apr/12 21:13;brandon.williams;4115.txt;https://issues.apache.org/jira/secure/attachment/12521734/4115.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-04 17:47:46.009,,,no_permission,,,,,,,,,,,,234534,,,Tue May 22 16:29:31 UTC 2012,,,,,,0|i0gs5z:,95993,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"04/Apr/12 17:47;jbellis;Tyler, what version(s) did you observer this against?","04/Apr/12 17:48;jbellis;Also, did you mean to attach unavailable_schema_test.py somewhere?","05/Apr/12 01:13;tpatterson;I tested and got it on branch 1.0.9-tentative and release 1.0.8.

I added unavailable_schema_test.py to the master branch of dtest.","06/Apr/12 21:13;brandon.williams;The only way I can see this happening on 1.0 is if ring delay is set to something lower than the gossip interval *2, otherwise it might not actually send the 'left' message.  That would result in the node being stuck in a 'leaving' and down state keeping it in the unreachable map.

For 1.1 and trunk, this is caused by CASSANDRA-3936, forcing a conviction when the decom node shuts the gossiper down, after the others have already removed it.  Instead, markAlive and markDead should have a dead state check and thus ignore it, since it's never valid to do either with a dead state.","06/Apr/12 21:28;jbellis;bq. The only way I can see this happening on 1.0 is if ring delay is set to something lower than the gossip interval *2

Pretty sure Tyler didn't mess with ring delay.","06/Apr/12 21:35;tpatterson;After testing a little more I found the following:

On 1.0.9 I get the following schema when sleeping 30 seconds after the decommission:
{'00000000-0000-1000-0000-000000000000': ['127.0.0.1'],
 'UNREACHABLE': ['127.0.0.2']}

On 1.1, this is the schema 30 seconds after the decommission:
{'59adb24e-f3cd-3e02-97f0-5b395827453f': ['127.0.0.1'],
 'UNREACHABLE': ['127.0.0.2']}

If I increase the sleep after decommissioning from 30 to 90 seconds, then 1.0.9 now returns only one schema, while 1.1 still fails with the UNREACHABLE schema. 

None of these tests were done with the recent patch.","12/Apr/12 16:36;brandon.williams;This must be something with the environment, on 1.0 I see the node removed as soon as the decom node begins to announce it has left (before it has completed announcing, and thus before nodetool would even return) and it never reappears.","12/Apr/12 16:37;brandon.williams;What does strike me as odd though is that your 1.0 test has no schema at all, hence the 00000000-0000-1000-0000-000000000000 uuid.","01/May/12 21:06;brandon.williams;Setting this as blocker for 1.1.1 so it doesn't slip by, we definitely need this patch in 1.1.","21/May/12 22:24;vijay2win@yahoo.com;I think we will have a problem in handleMajorStateChange

{code}
        if (!isDeadState(epState))
            markAlive(ep, epState);
        else
        {
            logger.debug(""Not marking "" + ep + "" alive due to dead state"");
            markDead(ep, epState);
        }
{code}

but in markDead we have the following so it will never be marked dead.

{code}

        if (isDeadState(localState))
            return;
{code}","22/May/12 15:07;brandon.williams;Hmm, you're right.  v2 instead filters dead states out of convict()",22/May/12 16:23;vijay2win@yahoo.com;+1,22/May/12 16:29;brandon.williams;Committed; resolving since I can't repro on 1.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrading from 1.1.7 to 1.2.0 caused upgraded nodes to only know about other 1.2.0 nodes,CASSANDRA-5102,12625867,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,mkjellman,mkjellman,03/Jan/13 16:14,12/Mar/19 14:20,13/Mar/19 22:27,04/Jan/13 18:15,1.2.1,,,,,0,,,,,,,"I upgraded as I have since 0.86 and things didn't go very smoothly.

I did a nodetool drain to my 1.1.7 node and changed my puppet config to use the new merged config. When it came back up (without any errors in the log) a nodetool ring only showed itself. I upgraded another node and sure enough now nodetool ring showed two nodes.


I tried resetting the local schema. The upgraded node happily grabbed the schema again but still only 1.2 nodes were visible in the ring to any upgraded nodes.",,,,,,,,,,,,,,,,,,,03/Jan/13 22:25;brandon.williams;5102.txt;https://issues.apache.org/jira/secure/attachment/12563177/5102.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-03 16:50:47.976,,,no_permission,,,,,,,,,,,,302448,,,Fri Mar 22 21:12:01 UTC 2013,,,,,,0|i1738n:,249492,jbellis,jbellis,,,,,,,,,,03/Jan/13 16:50;brandon.williams;Can you add nodetool gossipinfo from one of the new nodes that doesn't see the old ones?  Do the old ones see the new nodes?,03/Jan/13 16:51;mkjellman;i'm using PropertyFileSnitch as my endpoint snitch btw,"03/Jan/13 16:56;mkjellman;at this point i just went ahead and upgraded all the nodes (was more worried about getting the cluster back up)

I do notice though that the 1.2.0 nodes show net_version of 6.

as nodes were upgraded to 1.2.0 they didn't show up in the ring on the 1.1.7 side anymore.

gossipinfo on 1.2.0 nodes (ubuntu 12.04) look like:

/10.8.30.14
  RELEASE_VERSION:1.2.0
  NET_VERSION:6
  RPC_ADDRESS:0.0.0.0
  HOST_ID:24647d52-41eb-4df3-993e-51d4f841ca62
  LOAD:2.0129361318E11
  STATUS:NORMAL,70892159775195513221536376548285044050
  DC:DC1
  SCHEMA:da921e0b-4154-3601-9c76-6f61ca5f2872
  RACK:RAC1
  SEVERITY:-3.991605743852711E-11
/10.8.25.101
  RELEASE_VERSION:1.2.0
  RPC_ADDRESS:0.0.0.0
  NET_VERSION:6
  HOST_ID:dd3a40e2-fef1-4574-87b8-e2929fd80235
  LOAD:1.56018171896E11
  STATUS:NORMAL,42535295865117307932921825928971026436
  DC:DC1
  SCHEMA:da921e0b-4154-3601-9c76-6f61ca5f2872
  RACK:RAC2
  SEVERITY:0.019533560597218058 ","03/Jan/13 17:08;brandon.williams;Did they show up on the 1.1 side as being down, or not at all?",03/Jan/13 17:09;mkjellman;not at all.,"03/Jan/13 17:14;mkjellman;very possibly unrelated but opscenter seems to be unable to identify the nodes in the cluster either:

2013-01-02 18:39:24-0800 []  WARN: Unable to find a matching cluster for [u'fe80
:0:0:0:ca60:ff:feea:9c03%2', u'10.8.25.114', u'0:0:0:0:0:0:0:1%1', u'127.0.0.1',
 u'127.0.1.1']

maybe the node's identifiers changed with the ipv6 address which caused it to not be a member of the ring?","03/Jan/13 20:34;mkjellman;from one of the last nodes to be upgraded...

{code}
ERROR [GossipStage:906] 2013-01-02 13:51:44,982 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[GossipStage:906,5,main]
java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:89)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.UnknownHostException: addr is of illegal length
        at java.net.InetAddress.getByAddress(InetAddress.java:979)
        at java.net.InetAddress.getByAddress(InetAddress.java:1374)
        at org.apache.cassandra.net.CompactEndpointSerializationHelper.deserialize(CompactEndpointSerializationHelper.java:39)
        at org.apache.cassandra.gms.EndpointStatesSerializationHelper.deserialize(GossipDigestSynMessage.java:117)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:83)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:70)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:60)
        ... 4 more
ERROR [GossipStage:907] 2013-01-02 13:51:45,984 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[GossipStage:907,5,main]
java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:89)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.UnknownHostException: addr is of illegal length
        at java.net.InetAddress.getByAddress(InetAddress.java:979)
        at java.net.InetAddress.getByAddress(InetAddress.java:1374)
        at org.apache.cassandra.net.CompactEndpointSerializationHelper.deserialize(CompactEndpointSerializationHelper.java:39)
        at org.apache.cassandra.gms.EndpointStatesSerializationHelper.deserialize(GossipDigestSynMessage.java:117)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:83)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:70)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:60)
        ... 4 more
ERROR [GossipStage:908] 2013-01-02 13:51:46,988 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[GossipStage:908,5,main]
java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:89)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.UnknownHostException: addr is of illegal length
        at java.net.InetAddress.getByAddress(InetAddress.java:979)
        at java.net.InetAddress.getByAddress(InetAddress.java:1374)
        at org.apache.cassandra.net.CompactEndpointSerializationHelper.deserialize(CompactEndpointSerializationHelper.java:39)
        at org.apache.cassandra.gms.EndpointStatesSerializationHelper.deserialize(GossipDigestSynMessage.java:117)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:83)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:70)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:60)
        ... 4 more
{code}","03/Jan/13 22:25;brandon.williams;Here is a sad story of how multiple release cycles ended up causing a regression.

The cause of these exceptions is CASSANDRA-4576.  There, we added checks against VERSION_11 to prevent using the compatible mode with newer node that didn't need it. VERSION_11 has an actual value of 4.  We closed the ticket on Sept 18, and that was that.

Fast forward to November, where we closed CASSANDRA-4880.  To do this, we needed a protocol version bump, and created VERSION_117, which has an actual value of 5.  Unfortunately we used <= comparisons in CASSANDRA-4576, but now had created a version higher than VERSION_11 that still needed the compatibility, and we got our original bug back.

The effect of this is if you upgrade from nodes on 1.1.7 or later to 1.2.0, the 1.2.0 nodes won't be able to gossip with the 1.1.7 nodes and they won't be visible in ring output on the 1.2.0 node until they too are on 1.2.0.  The 1.1.7 nodes will still know about the 1.2.0 node, but they won't be able to successfully gossip with it, and keep it marked down.

Patch attached to go ahead and compare more explicitly against VERSION_12 to fix this, but I think it highlights a deeper problem, which is that if we ever do need to do another protocol bump in a minor, stable branch, we're out of luck because there's no space between VERSION_117 and VERSION_12.","04/Jan/13 08:37;slebresne;bq. if we ever do need to do another protocol bump in a minor, stable branch, we're out of luck

I'm sorry I did not follow CASSANDRA_4880 more closely but hadn't we decided that we should not change the protocol version in a minor version because it breaks streaming and we were only fine doing that for major upgrades?

Now I suppose what's done is done (though I wish some warning in the NEWS file for 1.1.7 had been added with CASSANDRA-4880 to explain that streaming would be broken during pre-1.1.7 to post-1.1.7 upgrades, and since it hadn't we should probably document it now), but as long as protocol bump means breaking streaming then I think we should maintain the rule ""no bump in minor version"" (not that I wouldn't be against lifting the ""protocol bump == break streaming"" limitation if possible but that's a different discussion).","04/Jan/13 12:43;brandon.williams;bq. hadn't we decided that we should not change the protocol version in a minor version because it breaks streaming and we were only fine doing that for major upgrades?

We had, but unfortunately it was the only way to fix what is hopefully the last of our schema problems in 1.1.  The impact from CASSANDRA-4880 is much worse than having to upgrade all nodes before streaming.

bq.  I wish some warning in the NEWS file for 1.1.7 had been added with CASSANDRA-4880 to explain that streaming would be broken during pre-1.1.7 to post-1.1.7 upgrades

Totally agree.","04/Jan/13 17:45;jbellis;The whole thing is a gotcha for assuming that we don't change messaging versions in minor releases.  The futureproof version would have been {{if (!(version >= MessagingService.VERSION_12))}} which looks kind of bizarre and might well have been ""fixed"" later on anyway.

So I'd file this under ""if we have to take a mulligan and add another version mid-release-cycle, make sure we validate usages of the initial major version.""

And if we want to be extra careful we should probably include an unused version before new major releases (i.e. make VERSION_20=8 instead of 7).","04/Jan/13 17:54;jbellis;bq. if (!(version >= MessagingService.VERSION_12))

Don't mind me, of course that simplifies to {{version < VERSION_12}}.

+1 on the patch, and please update NEWS retroactively.",04/Jan/13 18:15;brandon.williams;Committed; also updated 1.1 news to mention 4880,22/Mar/13 21:12;jjordan;Should that also get added to the 1.2.X NEWS.TXT in an UPGRADING section?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commitlog not replayed after restart,CASSANDRA-4782,12611021,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,frousseau,frousseau,09/Oct/12 16:51,12/Mar/19 14:07,13/Mar/19 22:27,11/Oct/12 13:59,1.1.6,,,,,0,,,,,,,"It seems that there are two corner cases where commitlog is not replayed after a restart :

 - After a reboot of a server + restart of cassandra (1.1.0 to 1.1.4)
 - After doing an upgrade from cassandra 1.1.X to cassandra 1.1.5

This is due to the fact that the commitlog segment id should always be an  incrementing number (see this condition : https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L247 )

But this assertion can be broken :
In the first case, it is generated by System.nanoTime() but it seems that System.nanoTime() is using the boot time as the base/reference (at least on java6 & linux), thus after a reboot, System.nanoTime() can return a lower number than before the reboot (and the javadoc says the reference is a relative point in time...)
In the second case, this was introduced by #4601 (which changes System.nanoTime() by System.currentTimeMillis() thus people starting with 1.1.5 are safe)

This could explain the following tickets : #4741 and #4481
",,,,,,,,,,,,,,,,,,,10/Oct/12 17:01;jbellis;4782.txt;https://issues.apache.org/jira/secure/attachment/12548588/4782.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-09 17:33:45.986,,,no_permission,,,,,,,,,,,,246195,,,Thu Oct 29 08:57:37 UTC 2015,,,,,,0|i07i2n:,41695,frousseau,frousseau,,,,,,,,,,09/Oct/12 17:33;brandon.williams;Thanks for the summary.  What are you proposing we do?,"10/Oct/12 08:44;frousseau;To solve the first case, it's probably better, when possible, to upgrade to a newer version, then rewrite all SSTables (or at least all SSTables metadata)
For the second case, just rewrite all SSTables (or at least all SSTables metadata)

Each SSTable metadata contains the ReplayPosition (and the max is taken to know which commitlog to replay), thus if System.nanoTime() returned a number which is a timestamp in the future, previous commitlogs will be ignored),
thus a drain should prevent from losing data (because there is no commitlog to replay).
And because SSTables are immutables, then rewriting them completely seems a better option (rather than modifying previously written metadata)

Maybe the simplest to do is to have a new option to nodetool (or a new option to nodetool upgradesstables) which only changes the metadata using the following rule :
if the replayPosition.segment of the SSTable is in the future, then reset it to NONE otherwise, let it to its current value. (NONE is valid value if node was drained and restarted)

By using this rule :
 - if a sstable was generated previously using a higher System.nanoTime() then it is reset
 - if a sstable was generated previously using a lower System.nanoTime() OR System.currentTimeMillis() then it is left as is
 - if a sstable is generated between the start & this rewriting process, then it is left as is

Thus the upgrade should be :
 - drain node
 - upgrade
 - start
 - run the process described above

What do you think ?
","10/Oct/12 16:52;jbellis;Thanks, Fabien.  I admit that I didn't realize at first the implications of the millis fix on existing sstable metadata.

Patch attached that bumps the sstable version to hf as a marker that we know metadata with that version has sane replay positions.  Replay positions from older metadata will be treated as NONE.

This will force a full replay the first restart on 1.1.6; afterwards, any newly flushed sstables will have the sane-replay-position marker and future restarts will not need to replay data unnecessarily.

What do you think?",10/Oct/12 17:01;jbellis;(patch revised to update CURRENT_VERSION as well),"11/Oct/12 12:08;frousseau;Thanks for the patch. This solution is more simple and elegant than the one I proposed.

I tested it and it worked like a charm.
Nevertheless, if there are counters CF, a drain is probably necessary to avoid replaying the full commitlog and avoid having overcounts. (I don't think it is a problem, just something to know before the upgrade...)","11/Oct/12 12:44;frousseau;By the way, I just noticed that the commitlog files were not replayed in the order of their ids.
It seems that they are sorted by ""last modification date"" before being replayed, but this does not corresponds to their ids.
Moreover, ""last modification date"" is changed when a file is copied, so, this could also change the order of archived commitlogs.

I suppose the sort order of commit log files is for schemas ?

Maybe it's safer to sort them using the id in the file name ?","11/Oct/12 13:59;jbellis;Committed with a warning in NEWS to drain.

Go ahead and open a separate ticket to fix sort order.","15/Oct/12 17:05;rcoli;If drain is required between versions to avoid this issue then CASSANDRA-4446, where drain sometimes doesn't actually drain, seems to have become more significant.",16/Oct/12 12:00;omid;CASSANDRA-4446 does not happen to me any more (so far) when restarting 1.1.6 into 1.1.6. I could observe CASSANDRA-4446 on upgrade from 1.1.3 to 1.1.6 though.,"18/Oct/12 16:35;arya;+1. I think the issue I had with data loss after node restart, actually had to do with this bug. Thanks for the fix.","21/Oct/15 07:53;htoulan;Hi,
I know this is bug is fixed now, but I got  a loss of data in production with several nodes in Cassandra 1.1.0 (java 6 + Red Hat)
I've been told that NTP and network issues occured, also the Cassandra servers have been restarted  probably due to power outage.
How can I identify that the loss of data are due to the bug described here? Is it reproducible ?

I can't decide to upgrade my servers in production without a solid evidence...

Thanks,

Hervé","26/Oct/15 09:37;htoulan;anyone can help ? how can I confirm I reproduced the bug ? 
something in the logs?
a specific cassandra-cli command ?

Thanks in advance.","28/Oct/15 17:59;rcoli;[~htoulan] :

1) JIRA is not a support forum, generally. You would get a better response from the #cassandra IRC channel on freenode or the cassandra-user@ mailing list.

2) Cassandra 1.1.0 is an extremely old and extremely broken version. You should update to at least the final released version of the 1.1 line. Much better would be (first 1.2.x and then) at least the most recent 2.0.x version.

3) No commit log replay bug, including this one, should be capable of losing data if you write with a sufficient ConsistencyLevel and Replication Factor and repair regularly.","29/Oct/15 08:57;htoulan;Hi Robert,

1) Thank you, I did know that, I almost never encountered issue with Cassandra (even with the 1.1.0)
2) That's what I thought. I plan to upgrade to 1.2.9.
3) Now, I lose data on all 1.1.0 platforms if I insert data and reboot servers (ring composed by 2 servers)... 
I can't believe we never saw that before. 
I am not sure I understand how Cassandra works with System.nanoTime() and replay position but couold we imagine current timestamp reaches a value for wich this bug is now reproducibke systematically?? 

Anyway you're right, and upgrade is mandatory.


Hervé
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure unique commit log file names,CASSANDRA-4601,12605927,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,amorton,amorton,amorton,02/Sep/12 22:37,12/Mar/19 14:07,13/Mar/19 22:27,04/Sep/12 20:09,1.1.5,,,,,0,,,,,,,"The commit log segment name uses System.nanoTime() as part of the file name. There is no guarantee that successive calls to nanoTime() will return different values. And on less than optimal hypervisors this happens a lot. 

I observed the following in the wild:

{code:java}
ERROR [COMMIT-LOG-ALLOCATOR] 2012-08-31 15:56:49,815 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.lang.AssertionError: attempted to delete non-existing file CommitLog-13926764209796414.log
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:68)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.discard(CommitLogSegment.java:172)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$4.run(CommitLogAllocator.java:223)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Unknown Source)
{code}

My _assumption_ is that it was because of duplicate file names. As this is on a hypervisor that is less than optimal. 
 
After a while (about 30 minutes) mutations stopped being processed and the pending count sky rocketed. I _think_ this was because log writing was blocked trying to get a new segment and writers could not submit to the commit log queue. The only way to stop the affected nodes was kill -9. 

Over about 24 hours this happened 5 times. I have deployed a patch that has been running for 12 hours without incident, will attach. 

The affected nodes could still read, and I'm checking logs to see how the other nodes handled the situation.",Sun JVM 1.6.33 / Ubuntu 10.04.4 LTS ,,,,,,,,,,,,,,,,,,02/Sep/12 23:13;amorton;cassandra-1.1-4601.patch;https://issues.apache.org/jira/secure/attachment/12543501/cassandra-1.1-4601.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-03 01:57:14.321,,,no_permission,,,,,,,,,,,,250701,,,Tue Sep 04 20:31:23 UTC 2012,,,,,,0|i0b09r:,62154,jbellis,jbellis,,,,,,,,,,"03/Sep/12 01:57;jbellis;LGTM overall.

- I'd go ahead and use time-in-millis as base instead of nanotime
- Looks like this diff is reversed from what it ""should"" be?  Old lines are +, new are - instead of the inverse.
- patch 1.0 as well?",04/Sep/12 20:02;jbellis;This actually doesn't affect < 1.1.0; it was introduced by CASSANDRA-3544.,04/Sep/12 20:09;jbellis;committed w/ millis as base.  (in a hurry since we're going to roll 1.1.5 RSN.),04/Sep/12 20:31;amorton;Thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Missing columns, errors when requesting specific columns from wide rows",CASSANDRA-5225,12631118,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,thobbs,thobbs,06/Feb/13 19:19,12/Mar/19 14:07,13/Mar/19 22:27,13/Feb/13 08:21,1.2.2,,,,,0,,,,,,,"With Cassandra 1.2.1 (and probably 1.2.0), I'm seeing some problems with Thrift queries that request a set of specific column names when the row is very wide.

To reproduce, I'm inserting 10 million columns into a single row and then randomly requesting three columns by name in a loop.  It's common for only one or two of the three columns to be returned.  I'm also seeing stack traces like the following in the Cassandra log:

{noformat}
ERROR 13:12:01,017 Exception in thread Thread[ReadStage:76,5,main]
java.lang.RuntimeException: org.apache.cassandra.io.sstable.CorruptSSTableException: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0 (/var/lib/cassandra/data/Keyspace1/CF1/Keyspace1-CF1-ib-5-Data.db, 14035168 bytes remaining)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1576)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.io.sstable.CorruptSSTableException: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0 (/var/lib/cassandra/data/Keyspace1/CF1/Keyspace1-CF1-ib-5-Data.db, 14035168 bytes remaining)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:69)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:81)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:133)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1358)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1215)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1127)
	at org.apache.cassandra.db.Table.getRow(Table.java:355)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:64)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1052)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1572)
	... 3 more
{noformat}

This doesn't seem to happen when the row is smaller, so it might have something to do with incremental large row compaction.",,,,,,,,,,,,CASSANDRA-5210,,,,,,,07/Feb/13 18:59;slebresne;5225.txt;https://issues.apache.org/jira/secure/attachment/12568441/5225.txt,18/Jun/13 22:58;dmeyer;corrected-pycassa-repro.py;https://issues.apache.org/jira/secure/attachment/12588474/corrected-pycassa-repro.py,06/Feb/13 19:20;thobbs;pycassa-repro.py;https://issues.apache.org/jira/secure/attachment/12568275/pycassa-repro.py,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-02-06 23:00:29.953,,,no_permission,,,,,,,,,,,,311614,,,Tue Jun 18 22:58:17 UTC 2013,,,,,,0|i1hrzb:,311960,brandon.williams,brandon.williams,,,,,,,,,dmeyer,06/Feb/13 19:20;thobbs;Attached python script reproduces the issue with pycassa.,"06/Feb/13 23:00;brandon.williams;Bisect says the winner is CASSANDRA-3885, but I never encountered the corrupt sstable exception.","07/Feb/13 17:24;yukim;It looks like cassandra is reading from wrong column index here(https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/columniterator/SSTableNamesIterator.java#L236).

Suppose we have col indexes of [[1..5][6..10][11..15][16..20]](numbers are column names), and we want to 'SELECT 2, 18 FROM CF';
First, we check '2' against indexes and get indexes[0]. Next, we check '18' against indexes with lastIndexIdx of 0.
Now, because we are limiting the second index check to the sublist of indexes[0, lastIndexIdx + 1] here(https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/io/sstable/IndexHelper.java#L186), it only checks against only first two indexes and gets wrong index position of indexes[2]. So it thinks '20' is not in the sstable.

In fact, if I removed sublisting part from IndexHelper.indexFor, SSTableNamesIterator started returning collect values. But I don't know that's the right way to do. [~slebresne]?","07/Feb/13 17:54;slebresne;With just eyeballing the code, I would say that the line at https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/io/sstable/IndexHelper.java#L179 should be:
{noformat}
if (!reversed)
{noformat}
i.e. both branch should be inverted. The goal of the lastIndex parameter is to ignore index block we know are ""behind"" us. So when we go forward (not reversed) you'd want to look at [lastIndex, index.size()], not the contrary.","07/Feb/13 18:59;slebresne;So attaching patch for this. Interestingly enough, the IndexHelperTest were broken too (which kind of make it obvious this was doing the wrong thing).

I haven't tried the pycassa repro script to validate this fixes thing though.",07/Feb/13 19:05;brandon.williams;It still doesn't pass :(,08/Feb/13 09:12;slebresne;Are you sure you applied the patch correctly? I just tested the pycassa-repro.py test above and it fails every time without the patch but haven't failed once with the patch.,"08/Feb/13 09:56;mukundneharkar;Yes..The patch works with my test code too..
","08/Feb/13 18:59;brandon.williams;I applied the patch correctly, but the bug is in the pycassa script itself... I was hitting an edge case where it asked for the same column twice.",12/Feb/13 19:17;eldenbishop;This patch also fixes CASSANDRA-5210. I'll mark that one as a dupe.,13/Feb/13 03:22;brandon.williams;+1,"13/Feb/13 08:21;slebresne;Committed, thanks.",07/Mar/13 19:09;abashir;This affects 1.1.x as well; will the fix be a part of 1.1.11?,"07/Mar/13 21:23;jbellis;If you have a test case that fails against 1.1, please post it.",08/Mar/13 23:10;abashir;The reporter for CASSANDRA-5210 (marked as a dupe of this) tested as far back as 0.8,"08/Mar/13 23:17;jbellis;Nevertheless, the bug fixed here was a regression introduced by CASSANDRA-3885 for 1.2.0.","18/Jun/13 19:41;dmeyer;Added a dtest to cover this scenario:
https://github.com/riptano/cassandra-dtest/blob/75bffeba0af410a41eb97b269ae1c94f4227c312/wide_rows_test.py",18/Jun/13 22:58;dmeyer;Fixed a small bug in the repro script.  Please use the 'corrected' version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Strange permament socket descriptors increasing leads to ""Too many open files""",CASSANDRA-4571,12604647,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,sergshne,sergshne,23/Aug/12 17:20,12/Mar/19 14:07,13/Mar/19 22:27,04/Sep/12 19:09,1.1.5,,,,,0,,,,,,,"On the two-node cluster there was found strange socket descriptors increasing. lsof -n | grep java shows many rows like""

java       8380 cassandra  113r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  114r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  115r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  116r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  117r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  118r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  119r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  120r     unix 0xffff8101a374a080            938348482 socket
"" And number of this rows constantly increasing. After about 24 hours this situation leads to error.
We use PHPCassa client. Load is not so high (aroud ~50kb/s on write). ","CentOS 5.8 Linux 2.6.18-308.13.1.el5 #1 SMP Tue Aug 21 17:10:18 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux. 

java version ""1.6.0_33""
Java(TM) SE Runtime Environment (build 1.6.0_33-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.8-b03, mixed mode)
",,,,,,,,,,,,,,,,,,04/Sep/12 16:19;jbellis;4571.txt;https://issues.apache.org/jira/secure/attachment/12543697/4571.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-23 18:21:37.192,,,no_permission,,,,,,,,,,,,248022,,,Wed Oct 17 14:51:32 UTC 2012,,,,,,0|i09ai7:,52135,beobal,beobal,,,,,,,,,,"23/Aug/12 18:21;brandon.williams;I've seen this a few times, but never found a cause/resolution, so I'll go ahead and dump what I know:

* All cases thus far seem to be upgrades, not new installations.

* 1.1 but less than 1.1.2 doesn't seem to exhibit

* Cassandra doesn't use unix sockets, at all

* This is fairly rare and only hits a handful of users

* some people have this happen on all nodes, some have it happen on only a portion

* going to such lengths as trying all kinds of different JVM versions and completely switching OSes has not helped

One user wrote a simple app to track the lost FDs here: http://pastebin.com/faBkJueB and it seemed to correlate with opening one sstable, and another user has corroborated that.  Both report heavy reads on that CF.

No way to reproduce is yet known, I've failed in all my attempts.","23/Aug/12 18:26;etobgra;I have seen the problem on cassandra 1.1.3 as well. Our 3 node cluster has the same issue. It's a blocker.
We are using Hector as client and FD increases up to 100K and keeps growing...

Java (build 1.6.0_32-b05)
Linux 2.6.32-220.el6.x86_64 #1 SMP Wed Nov 9 08:03:13 EST 2011 x86_64 x86_64 x86_64 GNU/Linux

After a fresh start cassandra uses one unix FD then we put some load on and it keeps growing.

lsof -p 14597 | grep -i unix
java    14597 root   43u  unix 0xffff88082a3acc80        0t0 42443166 socket

Put load on cassandra and then it increases

lsof -p 14597 | grep -i unix | wc -l
5678
7654
.....
98403










","23/Aug/12 18:29;etobgra;I can reproduce it every time we simulate a specific test case with load using many reads.
We have a new installation of cassandra 1.1.3.

So if you want some trace or dump whatever I can give it to you.","23/Aug/12 18:54;sergshne;It seems that bug is related to Java NIO internals (May be to Thrift framework). Please, read https://forums.oracle.com/forums/thread.jspa?threadID=1146235 for more details and give your thoughts about.
From topic: ""I am submitting this post to highlight a possible NIO ""gotcha"" in multithreaded applications and pose a couple of questions. We have observed file descriptor resource leakage (eventually leading to server failure) in a server process using NIO within the excellent framework written by Ronny Standtke (http://nioframework.sourceforge.net). Platform is JDK1.6.0_05 on RHEL4. I don't think that this is the same issue as that in connection with TCP CLOSED sockets reported elsewhere - What leaks here are descriptors connected to Unix domain sockets.

In the framework, SelectableChannels registered in a selector are select()-ed in a single thread that handles data transfer to clients of the selector channels, executing in different threads. When a client shuts down its connection (invoking key.cancel() and key.channel.close()) eventually we get to JRE AbstractInterruptibleChannel::close() and SocketChannelImpl::implCloseSelectableChannel() which does the preClose() - via JNI this dup2()s a statically maintained descriptor (attached to a dummy Unix domain socket) onto the underlying file descriptor (as discussed by Alan Bateman (http://mail.openjdk.java.net/pipermail/core-libs-dev/2008-January/000219.html)). The problem occurs when the select() thread runs at the same time and the cancelled key is seen by SelectorImpl::processDeregisterQueue(). Eventually (in our case) EPollSelectorImpl::implDereg() tests the ""channel closed"" flag set by AbstractInterruptibleChannel::close() (this is not read-protected by a lock) and executes channel.kill() which closes the underlying file descriptor. If this happens before the preClose() in the other thread, the out-of-sequence dup2() leaks the file descriptor, attached to the UNIX domain socket.

In the framework mentioned, we don't particularly want to add locking in the select() thread as this would impact other clients of the selector - alternatively a fix is to simply comment out the key.cancel(). channel.close() does the cancel() for us anyway, but after the close()/preClose() has completed, so the select() processing then occurs in the right sequence. (I am notifying Ronny Standtke of this issue independently).""

See also following links for more information:
http://stackoverflow.com/questions/7038688/java-nio-causes-file-descriptor-leak
http://mail-archives.apache.org/mod_mbox/tomcat-users/201201.mbox/%3CCAJkSUv-DDKTCQ-pD7W=QOVmPH1dXeXOvcr+3mCgu05cqpT7Zjg@mail.gmail.com%3E
http://www.apacheserver.net/HBase-Thrift-for-CDH3U3-leaking-file-descriptors-socket-at1580921.htm
",23/Aug/12 19:28;jeromatron;Tobias: is it possible to get the test case and the server setup to try to reproduce?  Heap dumps haven't proven very useful thus far.,23/Aug/12 21:49;sergshne;Bug is not recreating with one node cluster,"24/Aug/12 13:26;eperott;Adding few more observation from the 3-node cluster mentioned by Tobias above...

We have tried to reproduce the problem in a more isolated scenario without success.

I don't know whether this is of any use or not but we used strace to get a rough view on whats causing the leaked file descriptors. During a session with strace we placed a constant load of reads on the cluster. During this session strace collected ~2.400.000 rows of system calls. The first 2.000.000 have the following pattern:

# cat strace.cassandra.1* | sort | head -2000000 | grep open | wc -l
47
# cat strace.cassandra.1* | sort | head -2000000 | grep close | wc -l
48
# cat strace.cassandra.1* | sort | head -2000000 | grep dup2 | wc -l
20
# cat strace.cassandra.1* | sort | head -2000000 | grep fcntl | wc -l
86

The last 400.000 collected system calls reflect the time when FD leakage started:

# cat strace.cassandra.1* | sort | tail -390000 | grep open | wc -l
11344
# cat strace.cassandra.1* | sort | tail -390000 | grep close | wc -l
5718
# cat strace.cassandra.1* | sort | tail -390000 | grep dup2 | wc -l
16280
# cat strace.cassandra.1* | sort | tail -390000 | grep fcntl | wc -l
22670

As you can see, just a small part of the calls to open() and dup2() have a corresponding call to close().

A glimps of the system calls at the time when the problem starts:
# cat strace.cassandra.1* | sort -s -n -k 1 | less
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1260-Data.db"", O_RDONLY) = 2381
14:59:38 fstat(2381, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 fcntl(2381, F_GETFD) = 0
14:59:38 fcntl(2381, F_SETFD, FD_CLOEXEC) = 0
14:59:38 fstat(2381, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 lseek(2381, 0, SEEK_CUR) = 0
14:59:38 lseek(2381, 53072226, SEEK_SET) = 53072226
14:59:38 read(2381, ""\200\200\4tPREPARE\1\0\4\307\352\fi|J\0\0\0\4P5\337\257\0\3XI""..., 15790) = 15790
14:59:38 lseek(2381, 0, SEEK_CUR) = 53088016
14:59:38 read(2381, ""\330`\243\311"", 4) = 4
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1275-Data.db"", O_RDONLY)                  = 2387
14:59:38 fstat(2387, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 fcntl(2387, F_GETFD) = 0
14:59:38 fcntl(2387, F_SETFD, FD_CLOEXEC)                   = 0
14:59:38 fstat(2387, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 lseek(2387, 0, SEEK_CUR) = 0
14:59:38 lseek(2387, 1073276, SEEK_SET) = 1073276
14:59:38 read(2387, ""\200\200\4\360>nd\0\7batchId\0\0\4\307\3528\3760\240\0\0\0\34VOU""..., 18284) = 18284
14:59:38 lseek(2387, 0, SEEK_CUR)           = 1091560
14:59:38 read(2387, ""\301\320\321r"", 4) = 4
14:59:38 dup2(43, 2387)                  = 2387
14:59:38 close(2387)       = 0
14:59:38 futex(0x7fa968000eb4, FUTEX_WAKE_OP_PRIVATE, 1, 1, 0x7fa968000eb0, {FUTEX_OP_SET, 0, FUTEX_OP_CMP_GT, 1}) = 1
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1260-Data.db"", O_RDONLY) = 2389
14:59:38 fstat(2389, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 fcntl(2389, F_GETFD) = 0
14:59:38 fcntl(2389, F_SETFD, FD_CLOEXEC) = 0
14:59:38 fstat(2389, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 lseek(2389, 0, SEEK_CUR) = 0
14:59:38 lseek(2389, 16713707, SEEK_SET) = 16713707
14:59:38 read(2389, ""\200\200\4tgent\0\0\4\307\352\3\25\335\323\0\0\0\4bond\0\7batch""..., 15853) = 15853
14:59:38 lseek(2389, 0, SEEK_CUR) = 16729560
14:59:38 read(2389, ""pS\r\235"", 4)     = 4
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1275-Data.db"", O_RDONLY) = 2401
14:59:38 fstat(2401, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 fcntl(2401, F_GETFD) = 0
14:59:38 fcntl(2401, F_SETFD, FD_CLOEXEC)       = 0
14:59:38 fstat(2401, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 lseek(2401, 0, SEEK_CUR)                  = 0
14:59:38 lseek(2401, 325664, SEEK_SET) = 325664
14:59:38 read(2401, ""\200\200\4\01066\0\t\0010(\0\0\0\20\0\0\0\3\0\0\0\1\t\23\t\1(P5\353\317""..., 18359) = 18359
14:59:38 lseek(2401, 0, SEEK_CUR) = 344023
14:59:38 read(2401, ""\265.\321p"", 4)   = 4
14:59:38 dup2(43, 2401)       = 2401
14:59:38 close(2401) = 0
14:59:38 futex(0x41e1f804, FUTEX_WAKE_OP_PRIVATE, 1, 1, 0x41e1f800, {FUTEX_OP_SET, 0, FUTEX_OP_CMP_GT, 1})                   = 1
14:59:38 futex(0x7fae0c3278d0, FUTEX_WAKE_PRIVATE, 1) = 0
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1260-Data.db"", O_RDONLY)                   = 2416
14:59:38 fstat(2416, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 fcntl(2416, F_GETFD) = 0
14:59:38 fcntl(2416, F_SETFD, FD_CLOEXEC) = 0
14:59:38 fstat(2416, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 lseek(2416, 0, SEEK_CUR) = 0
14:59:38 lseek(2416, 275573213, SEEK_SET) = 275573213
14:59:38 read(2416, ""\200\200\4\0000n\1\0\310\0\16activationCode\0\0\4\307\351\361#""..., 15829) = 15829
14:59:38 lseek(2416, 0, SEEK_CUR)                  = 275589042
14:59:38 read(2416, ""\316\223j\235"", 4) = 4
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1275-Data.db"", O_RDONLY) = 2438
14:59:38 fstat(2438, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 fcntl(2438, F_GETFD) = 0
14:59:38 fcntl(2438, F_SETFD, FD_CLOEXEC) = 0
14:59:38 fstat(2438, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 lseek(2438, 0, SEEK_CUR) = 0
14:59:38 lseek(2438, 5538661, SEEK_SET) = 5538661
14:59:38 read(2438, ""\200\200\4\0\0\r\1,P5\354\""\0\4\307\352\312^\3450\5\23<\01755643264""..., 18465) = 18465
14:59:38 lseek(2438, 0, SEEK_CUR)                  = 5557126
14:59:38 read(2438, ""\232\327\216q"", 4) = 4
14:59:38 dup2(43, 2438) = 2438
14:59:38 close(2438)                   = 0

","24/Aug/12 16:23;jbellis;bq. https://forums.oracle.com/forums/thread.jspa?threadID=1146235

That thread says, ""the race condition with cancel is something we are currently working on  in http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6693490,"" which is marked fixed in java6u18.","24/Aug/12 16:25;jbellis;Tobias/Per, can you try your test case against 1.1.2 to verify that this was indeed introduce in 1.1.3 somehow?",24/Aug/12 18:15;joe.miller;We are seeing this fd socket leak issue on 1.1.2 (after upgrade from 1.0.10),24/Aug/12 20:19;jbellis;Thanks Joe.  Can anyone confirm/reject 1.1.1 then?,"24/Aug/12 22:33;tmeighen;Just wanted to add my experience with this. In our case we saw the issue when we upgraded from 1.0.x to 1.1.x. We are currently running 1.1.2 on Ubuntu 12.04 with Sun JDK 1.6.0_34. I'm pretty sure that we had the issue with 1.1.1 as well.

With the help of some kind folks on the irc channel I was able to track the vast majority of the leaks to one particular column family. Some of the characteristics of the CF:

- Heavy read load
- Data is deleted
- Accessed using Hector Object Mapping (the only CF in our system that uses this)
- Relatively small Column Family
- Row caching enabled *

Once I was able to pinpoint this table I started reviewing the schema and noticed that in the migration from 1.0.x to 1.1.x we had lost our row cache settings. The original schema defined row_cache_size=20000 but when we upgraded I found that caching=""KEYS_ONLY"" instead of ""ALL"". When I re-enabled the row cache we stopped leaking file descriptors. I'm assuming this is because the data was being pulled from memory instead of disk and so the underlying issue was no longer exposed.

Another thing to note is that our ring (4 nodes at the time, I've upped it to 6 since) was under fairly heavy load and doing a lot of garbage collection.",25/Aug/12 02:42;sergshne;I have checked 1.1.1 and have found the same issue.,29/Aug/12 16:21;swillcox;We are also seeing this bug and all nodes eventually run out of file descriptors and crash. It is a blocker for us.,"29/Aug/12 16:46;eperott;To verify, we started from scratch. A new installation on 3 servers. And the FD leak is still there. So, with our particular setup we are able to reproduce the bug.

These are the characteristics of our setup:
- We have one single CF.
- Rows are inserted in batches.
- Rows are red, updated and deleted in a random like pattern.
- The FD leak seem to start during heavy read load (but can appear during mixed read/write/delete operations as well).
- We are using Hector to access this single CF.
- Cassandra configuration is basically standard.

The FD leaks does not show immediately. It appears once there is ~60M rows in CF.
","29/Aug/12 16:53;jbellis;Are you sure you can't reproduce on a single-node cluster?

Because we're getting conflicting evidence here; on the one hand, strace indicates that the fd leakage is related to file i/o, but if so, you shouldn't need multiple nodes in the cluster to repro.","30/Aug/12 00:07;sergshne;bq.Are you sure you can't reproduce on a single-node cluster?

My mistake. I've checked it again. Bug also was reproduced with one-node cluster.","31/Aug/12 13:14;jbellis;bq. The FD leak seem to start during heavy read load (but can appear during mixed read/write/delete operations as well)

Does this mean that you can reproduce the leak if you stop doing inserts/updates entirely and just do reads?

What kind of reads are you doing?  index lookups?  seq scan?  named-columns-from-row?  slice-from-row?","31/Aug/12 14:25;etobgra;Yes. I have tried to run traffic for a couple of days which does mixed operations and cassandra is still running.
However, i just started to run a traffic testcase which does only reads and the issue is back directly.
I don't know if it matters but all my reads returns empty responses which is expected since these keys should be deleted :) 
The testcase does a lookup on the key only.

E.g I have file with a number of keys and then my testcase try to lookup a row using that key and the response is empty since these keys does not exists.





","31/Aug/12 14:52;swillcox;We can reliably reproduce this issue in our test environment every day. Start the servers up in the morning and by the end of the workday the number of open file descriptors reaches from 40-60K and the nodes stop responding. We have turned row caching off and it still has this problem. You can contact me if you think remotely debugging this issue will help in determining what is causing this.

We have reproduced this using just one node.","04/Sep/12 15:12;brandon.williams;Thanks to a reproducible example from Viktor Kuzmin, I've bisected this down to CASSANDRA-4116.",04/Sep/12 16:13;jbellis;Patch to fix 4116 sstable iterator leak.,04/Sep/12 16:25;brandon.williams;Issue does not repro with this patch.,04/Sep/12 18:49;beobal;LGTM +1,04/Sep/12 19:09;jbellis;committed!  thanks everyone for the help tracking this down.,15/Oct/12 17:24;j.casares;This can still be seen in 1.1.5 if the user is running Java 1.6.0_29. The current solution is to upgrade to 1.6.0_35.,"15/Oct/12 18:52;cherro;For anybody else encountering this unbounded socket growth problem on 1.1.5, note that while upgrading 1.6.0_35 seemed to help, a longer load test still reproduced the symptom. FWIW, upgradesstables ran for a period during this particular test - unclear if the increased compaction activity contributed.",16/Oct/12 01:57;jbellis;Related to CASSANDRA-4740?,"16/Oct/12 04:13;cherro;FYI was able to reproduce the symptom on Cassandra 1.1.6.
@[~jbellis] Re: CASSANDRA-4740 and whether it relates to this: 
* Haven't looked across all nodes for phantom connections yet
* Have searched across all logs - found a single instance of ""Timed out replaying hints"".
* Mina mentioned that ""Nodes running earlier kernels (2.6.39, 3.0, 3.1) haven't exhibited this"". We are seeing this on Linux kernel 2.6.35 with Java 1.6.0_35.
","16/Oct/12 16:15;cherro;We are also seeing errors similar to those reported in CASSANDRA-4687.
Could this be a side-effect of that problem? In {{SSTableSliceIterator}} as of commit {{e1b10590e84189b92af168e33a63c14c3ca1f5fa}}, if the constructor key equality assertion fails, {{fileToClose}} does not get closed.",16/Oct/12 16:45;jbellis;Are you then seeing that assertion failure logged?,"16/Oct/12 17:19;cherro;Yes, seeing the key equality AssertionErrors from two SSTable iterators: SSTableSliceIterator:60 and SSTableNamesIterator:72.
Also seeing same EOF error reported by [~tjake] in CASSANDRA-4687:
{code}
java.io.IOError: java.io.EOFException: unable to seek to position 61291844 in /redacted/cassandra/data/test1/redacted/test1-redacted-hf-1-Data.db (59874704 bytes) in read-only mode
        at org.apache.cassandra.io.util.CompressedSegmentedFile.getSegment(CompressedSegmentedFile.java:69)
        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:898)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:50)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:67)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:79)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:256)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:64)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1345)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1207)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1142)
        at org.apache.cassandra.db.Table.getRow(Table.java:378)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:69)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:51)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException: unable to seek to position 61291844 in /redacted/cassandra/data/test1/redacted/test1-redacted-hf-1-Data.db (59874704 bytes) in read-only mode
        at org.apache.cassandra.io.util.RandomAccessReader.seek(RandomAccessReader.java:253)
        at org.apache.cassandra.io.util.CompressedSegmentedFile.getSegment(CompressedSegmentedFile.java:64)
        ... 16 more
{code}
","17/Oct/12 03:36;cherro;Tested this patch: https://gist.github.com/2f10efd3922fab9a095e applied to a build from branch cassandra-1.1 at commit 4d2e5e73b127dc0b335176ddc1dec1f0244e7f6d.

This definitely reduced the growth of socket FD handles, but there must be other scenarios like this in the codebase because it did grow beyond 2 which is where I've seen it at steady state under normal conditions.

The AssertionErrors from CASSANDRA-4687 were so spurious that they were pegging disk IO. When I ran the same test again with assertions disabled for the org.apache.cassandra.db.columniterator package, I saw many errors like those described in CASSANDRA-4417 (""invalid counter shard detected""). See my comments in that issue.

Shouldn't CASSANDRA-4571 be re-opened?




","17/Oct/12 14:51;jbellis;If it's caused by 4687 assertion errors?  No.

Disabling key cache is a workaround for 4687 btw.",,,,,,,,,,,,,,,,,,,,,,,,,
"""Memory was freed"" AssertionError During Major Compaction",CASSANDRA-5256,12632473,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,cscotta,cscotta,14/Feb/13 21:17,12/Mar/19 14:07,13/Mar/19 22:27,20/Feb/13 13:39,1.2.2,,,,,1,compaction,,,,,,"When initiating a major compaction with `./nodetool -h localhost compact`, an AssertionError is thrown in the CompactionExecutor from o.a.c.io.util.Memory:

ERROR [CompactionExecutor:41495] 2013-02-14 14:38:35,720 CassandraDaemon.java (line 133) Exception in thread Thread[CompactionExecutor:41495,1,RMI Runtime]
java.lang.AssertionError: Memory was freed
  at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:146)
	at org.apache.cassandra.io.util.Memory.getLong(Memory.java:116)
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:176)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:88)
	at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:327)
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:755)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:792)
	at org.apache.cassandra.utils.BytesReadTracker.readLong(BytesReadTracker.java:114)
	at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:101)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumnsFromSSTable(ColumnFamilySerializer.java:149)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:235)
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:109)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:93)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:162)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:76)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:158)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:71)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:342)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

---

I've invoked the `nodetool compact` three times; this occurred after each. The node has been up for a couple days accepting writes and has not been restarted.

Here's the server's log since it was started a few days ago: https://gist.github.com/cscotta/4956472/raw/95e7cbc68de1aefaeca11812cbb98d5d46f534e8/cassandra.log

Here's the code being used to issue writes to the datastore: https://gist.github.com/cscotta/20cbd36c2503c71d06e9

---

Configuration: One node, one keyspace, one column family. ~60 writes/second of data with a TTL of 86400, zero reads. Stock cassandra.yaml.

Keyspace DDL:

create keyspace jetpack;
use jetpack;
create column family Metrics with key_validation_class = 'UTF8Type' and comparator = 'IntegerType';","Linux ashbdrytest01p 3.2.0-37-generic #58-Ubuntu SMP Thu Jan 24 15:28:10 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux

java version ""1.6.0_30""
Java(TM) SE Runtime Environment (build 1.6.0_30-b12)
Java HotSpot(TM) 64-Bit Server VM (build 20.5-b03, mixed mode)

Ubuntu 12.04.2 LTS",,,,,,,,,,,,,CASSANDRA-8134,,,,,15/Feb/13 05:00;jbellis;5256-v2.txt;https://issues.apache.org/jira/secure/attachment/12569459/5256-v2.txt,16/Feb/13 06:04;jbellis;5256-v4.txt;https://issues.apache.org/jira/secure/attachment/12569646/5256-v4.txt,19/Feb/13 14:54;jbellis;5256-v5.txt;https://issues.apache.org/jira/secure/attachment/12569947/5256-v5.txt,15/Feb/13 01:04;jbellis;5256.txt;https://issues.apache.org/jira/secure/attachment/12569431/5256.txt,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-02-14 21:23:25.572,,,no_permission,,,,,,,,,,,,312969,,,Wed Oct 29 15:16:08 UTC 2014,,,,,,0|i1i0cf:,313315,slebresne,slebresne,,,,,,,,,,"14/Feb/13 21:23;jbellis;Some race is allowing sstables to participate in multiple concurrent compactions; when the first finishes, it frees the compression metadata, which causes the above AssertionError in the other.  Look at sstable 2110 for example.",14/Feb/13 21:25;jbellis;Is this LeveledCompactionStrategy or SizeTiered?,14/Feb/13 21:56;brandon.williams;It's STS.,"15/Feb/13 00:59;jbellis;compactionLock is ugly and confusing but I think the logic is correct as far as it goes.  I think the only problem is that STCS.getMaximalTask doesn't mark its victims as compacting, so a minor compaction is free to attempt them as well.

There is a similar problem with STCS.getUserDefinedCompaction.  Both addressed in attached patch.

I've also created CASSANDRA-5259 to make getNextBackgroundTask try harder before deciding there is no work to do.

Finally, I have a patch to remove compactionLock entirely that I'll attach to CASSANDRA-3430.  I think the prudent course is to do that for trunk only.",15/Feb/13 04:58;jbellis;v2 does a little more cleanup.  removed unnecessary reference-keeping in submitUserDefined.  standardized concurrency control on markCompacting instead of mix of that + synchronized.,"15/Feb/13 21:57;yukim;Overall, looks good and simplifies things.
One problem I had was CompactionsTest was throwing AssertionError on testSingleSSTableCompactionWithLeveledCompaction, when trying to find overlapping sstables to determine droppable ones.

Patch for that pushed to https://github.com/yukim/cassandra/commits/5256-3.","15/Feb/13 22:48;slebresne;lgtm, +1.

Nits:
* we could also move the try \{ task.execute() \} finally \{ task.unmarkSSTables(); \} inside AbstractCompactionTask to 1) avoid repeating it 3 times and 2) making sure we don't forget about it.
* maybe the ctor of AbstractCompactionTask could assert that the sstable it gets are 'marked compacting' to avoid that kind of mistake in the future.","16/Feb/13 05:59;jbellis;bq.  One problem I had was CompactionsTest was throwing AssertionError on testSingleSSTableCompactionWithLeveledCompaction, when trying to find overlapping sstables to determine droppable ones.

I'm worried this patch fixes a symptom instead of a deeper bug:
# We already start with {{Set<SSTableReader> candidates = cfs.getUncompactingSSTables();}}; fixing a race by adding additional checks seems like it will just make the race less likely rather than eliminate it entirely
# I'm not sure why compacting status should affect getOverlappingSSTables

For the record, here's the stack trace in question:

{noformat}
    [junit] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    [junit] 	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
    [junit] Caused by: java.lang.AssertionError
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.getOverlappingSSTables(ColumnFamilyStore.java:882)
    [junit] 	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:184)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.findDroppableSSTable(LeveledCompactionStrategy.java:296)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getMaximalTask(LeveledCompactionStrategy.java:107)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:95)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:185)
{noformat}",16/Feb/13 06:04;jbellis;v4 attached that incorporates Sylvain's suggestions.  Still fails CompactionsTest since I'm not sure what's going on w/ the getOverlappingSSTables assert.,"19/Feb/13 14:07;jbellis;I think there are two problems here:

# I was wrong, we're not starting with non-compacting candidates (this is the case for STCS, but not LCS, which checks for compacting-ness later on).  
# The assert is broken in a concurrent compactions world; it's possible that the sstable we're checking for overlaps with, gets compacted away already by a different task before that assert fires.

v5 edits findDroppableSSTable to check for compacting status, and removes the getOverlappingSSTables assert.","20/Feb/13 02:56;slebresne;+1 on v5.

Nits: the comment of CompactionTask.execute is now outdated about the unmarking part (I would also probably have move the unmarking part in AbsractCompactionTask rather than CompactionTask as that felt ""safer"" to me in principle, but that doesn't really matter).",20/Feb/13 13:39;jbellis;Committed with nits addressed,"28/Oct/14 18:00;ngrigoriev@gmail.com;I have just got this problem on multiple nodes. Cassandra 2.0.10 (DSE 4.5.2). Should I reopen?

{code}
ERROR [CompactionExecutor:1196] 2014-10-28 17:14:50,124 CassandraDaemon.java (line 199) Exception in thread Thread[CompactionExecutor:1196,1,main]
java.lang.AssertionError: Memory was freed
        at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:259)
        at org.apache.cassandra.io.util.Memory.getInt(Memory.java:211)
        at org.apache.cassandra.io.sstable.IndexSummary.getIndex(IndexSummary.java:79)
        at org.apache.cassandra.io.sstable.IndexSummary.getKey(IndexSummary.java:84)
        at org.apache.cassandra.io.sstable.IndexSummary.binarySearch(IndexSummary.java:58)
        at org.apache.cassandra.io.sstable.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:692)
        at org.apache.cassandra.io.sstable.SSTableReader.estimatedKeysForRanges(SSTableReader.java:663)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:328)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.findDroppableSSTable(LeveledCompactionStrategy.java:354)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getMaximalTask(LeveledCompactionStrategy.java:125)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:113)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:192)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}",29/Oct/14 15:16;jbellis;That is a different error; please open a new ticket.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1.1 does not preserve compatibility w/ index queries against 1.0 nodes,CASSANDRA-4262,12556550,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,jbellis,jbellis,19/May/12 22:09,12/Mar/19 14:07,13/Mar/19 22:27,01/Jun/12 16:08,1.1.1,,,,,0,,,,,,,1.1 merged index + seq scan paths into RangeSliceCommand.  1.1 StorageProxy always sends a RSC for either scan type.  But 1.0 RSVH only does seq scans.,,,,,,,,,,,,,,,,,,,01/Jun/12 15:58;slebresne;4262-v2.txt;https://issues.apache.org/jira/secure/attachment/12530562/4262-v2.txt,25/May/12 17:56;slebresne;4262.txt;https://issues.apache.org/jira/secure/attachment/12529756/4262.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-25 17:56:04.61,,,no_permission,,,,,,,,,,,,256004,,,Fri Jun 01 16:08:04 UTC 2012,,,,,,0|i0gtvr:,96271,jbellis,jbellis,,,,,,,,,,"25/May/12 17:56;slebresne;Attached patch to generate IndexScanCommand message when appropriate.

I've pushed a test in dtests too (in rolling_upgrade_test.py). Fails before the patch, works after.","01/Jun/12 15:15;jbellis;Can we do something to make the problem more obvious when someone does a seq-scan-with-filters in a mixed-version cluster, than just send the RSC and have it error out?  Even just logging WARN would be better than nothing.","01/Jun/12 15:31;slebresne;Actually, I think we don't allow at all seq-scan-with-filters (even the storage engine can do it). The reason it's not allowed we haven't changed the validation of index expression, so that an invalid request will be return upfront to the user saying no columns are indexed. This is also true of CQL3.

On the front of ""should we allow those seq-scan-with-filters"", that would be a case of seemingly simple queries that could take *a lot* of time (I suppose you could do that with 2ndary indexes too, but you have to try a little bit more harder), so maybe this is worth giving it a though?","01/Jun/12 15:58;slebresne;Actually, I'm wrong. We do allow seq-scan-with-filters on the thrift side. So anyway, attaching a v2 that does throw an exception in that case on the sending node.

Still leave the question if we should allow those sequential queries with filters in CQL3 I guess.","01/Jun/12 16:02;jbellis;+1

Ultimately I'd like to have a CqlInputFormat for Hadoop, which is why we opened that up on the Thrift side.","01/Jun/12 16:08;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTables are not updated with max timestamp on upgradesstables/compaction leading to non-optimal performance.,CASSANDRA-4205,12553450,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,thorkild,thorkild,01/May/12 04:26,12/Mar/19 14:07,13/Mar/19 22:27,03/May/12 23:04,1.0.10,1.1.1,,,,0,,,,,,,"We upgraded from 0.7.9 to 1.0.7 on a cluster with a heavy update load. After converting all the reads to named column reads instead of get_slice calls, we noticed that we still weren't getting the performance improvements implemented in CASSANDRA-2498. A single named column read was still touching multiple SSTables according to nodetool cfhistograms. 

To verify whether or not this was a reporting issue or a real issue, we ran multiple tests with stress and noticed that it worked as expected. After changing stress so that it ran the read/write test directly in the CF having issues (3 times stress & flush), we noticed that stress also touched multiple SSTables (according to cfhistograms).

So, the root of the problem is ""something"" left over from our pre-1.0 days. All SSTables were upgraded with upgradesstables, and have been written and compacted many times since the upgrade (4 months ago). The usage pattern for this CF is that it is constantly read and updated (overwritten), but no deletes. 

After discussing the problem with Brandon Williams on #cassandra, it seems the problem might be because a max timestamp has never been written for the old SSTables that were upgraded from pre 1.0. They have only been compacted, and the max timestamp is not recorded during compactions. 

A suggested fix is to special case this in upgradesstables so that a max timestamp always exists for all SSTables. 

{panel}
06:08 < driftx> thorkild_: tx.  The thing is we don't record the max timestamp on compactions, but we can do it specially for upgradesstables.
06:08 < driftx> so, nothing in... nothing out.
06:10 < thorkild_> driftx: ah, so when you upgrade from before the metadata was written, and that data is only feed through upgradesstables and compactions -> never properly written?
06:10 < thorkild_> that makes sense.
06:11 < driftx> right, we never create it, we just reuse it :(
{panel}
",,,,,,,,,,,,,,,,,,,02/May/12 17:06;jbellis;4205.txt;https://issues.apache.org/jira/secure/attachment/12525327/4205.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-02 15:05:58.818,,,no_permission,,,,,,,,,,,,237612,,,Fri May 04 15:16:38 UTC 2012,,,,,,0|i0gt7z:,96164,yukim,yukim,,,,,,,,,,"02/May/12 15:05;jbellis;We have a related but more serious bug, as noted by Sam Tunnicliffe in CASSANDRA-4116: CF.maxTimestamp isn't including row tombstones.  This can result in incorrect results being returned by the collationcontroller code.","02/May/12 16:55;jbellis;bq. A suggested fix is to special case this in upgradesstables so that a max timestamp always exists for all SSTables. 

Looking at the code, scrub and upgradesstables and user-defined compactions all force deserialize + maxtimestamp computation.  The only operation that does not is cleanup.",02/May/12 17:06;jbellis;patch to add version hd indicating row tombstones have been computed correctly,"02/May/12 17:36;brandon.williams;I can reproduce, upgrading from 0.7 to 1.0 and running upgradesstables afterward.","02/May/12 19:38;jbellis;Checked Brandon's sstables with CASSANDRA-4211.  One had a max timestamp of 1335979912016 after upgradesstables, the other 1335979951175.  cfhistograms showed about 20% of reads hit both sstables.

This sounds about right; it's reasonable that the sstable w/ higher max timestamp, will contain *some* rows w/ actually an older version than the other sstable's max.  The CASSANDRA-2498 approach will always start with the newer sstable, but it will only skip the older one if the first-seen version of the columns requested have a timestamp newer than the max on the older sstable.

So, I think there is no bug here related to upgraded timestamps, it just isn't a magic bullet to prevent all multiple sstable reads.

The patch for creating a new version to represent ""we have *correct* timestamps including row tombstones"" is still relevant, though.","03/May/12 18:58;yukim;Jonathan,

Re: patch, you have to bump up CURRENT_VERSION to ""hd"" but otherwise looking good to me.","03/May/12 23:04;jbellis;good catch, committed w/ current version fix","04/May/12 04:24;thorkild;The original description is wrong (as we now know). Backporting the tool from 4211 and testing shows that the SSTables do indeed have the correct settings, and everything is updated correctly during the various compaction/upgradesstables etc.

After further debugging I found that the reason why I was hitting all the SSTables was due to the CollationController falling back to calling collectAllData(). This was due to the filter not being an instance of NamesQueryFilter:


CollationController.java:
{noformat}

public ColumnFamily getTopLevelColumns()
    {
        return filter.filter instanceof NamesQueryFilter
               && (cfs.metadata.cfType == ColumnFamilyType.Standard || filter.path.superColumnName != null)
               && cfs.metadata.getDefaultValidator() != CounterColumnType.instance
               ? collectTimeOrderedData()
               : collectAllData();
    }
{noformat}

This makes sense, but I struggled to find out why that happened on some CFs, and not on other. After tracing it, I found that the trigger is whether or not the CF has the row-cache enabled. If the row-cache is enabled, ColumnFamilyStore.cacheRow(..) is called, and if you then get a cache-miss, it'll end up calling CoumnFamilyStore.getTopLevelColumns(...), which again creates a new CollationController with an IdentityFilter filter as argument, and thus end up using collectAllData().

In our case, we have a lot of cache-misses since the update frequency is so high and often we only read an entry once before it is replaced. 

Testing with 'stress' before and after showed it moved from using all sstables for every query, to 100% reads satisfied with a single SSTable read (according to cfhistograms).

I tested a preliminary configuration with row-cache in production, and it looked like a lot more of the queries were served by a single sstable. We still have stragglers using more, but that can easily be because of the randomization size-tiered compaction will lead to in this case. 

I don't see an easy way to fix this properly, though, without changing the row cache to be a row+filter cache (covered by CASSANDRA-1956 it seems). Since you need to load the whole row into the row cache, you can't use named columns. ","04/May/12 15:16;jbellis;Thanks for following up, Thorkild.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool can't work at all !,CASSANDRA-4494,12601526,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,thepaul,sun74533,sun74533,06/Aug/12 03:28,12/Mar/19 14:07,13/Mar/19 22:27,13/Aug/12 14:29,1.1.4,,Tool/nodetool,,,0,,,,,,,"1. download cassandra 1.1.3 , then start with ""{cassandra}/bin/cassandra -pf &""
2. cd to bin , call nodetool as ""./nodetool -h localhost ring""
3. console returned : failed to connect to 'localhost:7199' : connection refused


BUT ,

at the same centos , all was ok before (1.1.2) .


PS: 

cassandra-cli/cqlsh works well (1.1.3)


--------------

update:

even if add the following in cassandra-env.sh , connection refused as well :
JVM_OPTS=""$JVM_OPTS -Djava.rmi.server.hostname=10.10.30.11
",centos 64bit,,,,,,,,,,,,,,,,,,07/Aug/12 22:04;thepaul;4494-sh-error.patch.txt;https://issues.apache.org/jira/secure/attachment/12539721/4494-sh-error.patch.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-06 10:50:08.451,,,no_permission,,,,,,,,,,,,256198,,,Mon Aug 13 14:29:05 UTC 2012,,,,,,0|i0gwfz:,96686,brandon.williams,brandon.williams,,,,,,,,,,"06/Aug/12 10:50;jdilloyd;Confirmed.

1.1.2 nodetool works perfectly
1.1.3 nodetool throws back 'connection refused' error

setting the JVM_OPTS hostname in cassandra-env.sh has no effect.

connection via 'localhost', server hostname, or server IP address all return the same result.

connection to remote 1.1.3 host from 1.1.3 host fails.
Connection to remote 1.1.2 host from 1.1.3 host works fine.
Connection from 1.1.2 host to remote 1.1.3 host fails.

restarting the node has no effect.
replacing nodetool with file from previous version (1.1.2) has no effect.



CentOS release 5.8 (Final) x86_64
","06/Aug/12 13:26;sun74533;there should be some official tips to tell us is it a bug or not ? if a bug , it should be fixed immediately , or show some resolutions officially . ","06/Aug/12 14:12;slebresne;I don't have a centos to test, but I do get the same on macosx (but not on ubuntu for instance). More precisely, the error is:
{noformat}
./bin/../conf/cassandra-env.sh: line 180: syntax error near unexpected token `['
./bin/../conf/cassandra-env.sh: line 180: `startswith () [ ""${1#$2}"" != ""$1"" ]'
{noformat}

The reason this breaks JMX is just that the JMX port is not set at that point of the script and the script likely exit on the error. It seems there is some shell incompatibility again.

While we fix that, a workaround could be to edit the cassandra-env.sh to comment the incriminated line.","07/Aug/12 16:06;pvelas;Version 1.1.3 don't listen on port 7199 .
Previous version 1.1.2 is OK. ","07/Aug/12 18:28;jbellis;""bash bin/nodetool"" should work.","07/Aug/12 18:53;jdilloyd;*EDIT*

Correction: that doesn't work either.","07/Aug/12 19:03;pvelas;When I try to use cassandra-env.sh from 1.1.2 version and restarted cassandra process then I can use nodetool without problems.
I can now see service sucessfully binded to port 7199.

{code}
[root@cass1 conf]# netstat -tunap | grep 7199
tcp        0      0 0.0.0.0:7199                0.0.0.0:*                   LISTEN      10904/java 
{code}",07/Aug/12 22:04;thepaul;Guess POSIX sh doesn't allow omission of curly braces around a function body of a single command. Dash fails at being a plain posix sh again :(,"08/Aug/12 03:48;sun74533;edit the cassandra-env.sh on line 180 :

original : startswith ()  [ ""${1#$2}"" != ""$1"" ] 

change to : startswith () { [ ""${1#$2}"" != ""$1"" ] ; }

then restart cassandra , every thing goes OK !","13/Aug/12 14:29;urandom;Sorry, I committed an (identical )fix to trunk for this on Friday when Jira was down.  That's been applied to 1.1.  Closing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Alter table' when it includes collections makes cqlsh hang,CASSANDRA-5064,12623780,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,enigmacurry,enigmacurry,13/Dec/12 16:48,12/Mar/19 14:07,13/Mar/19 22:27,17/Dec/12 11:05,1.2.0 rc2,,,,,0,,,,,,,"Having just installed 1.2.0-beta3 issue the following CQL into cqlsh:
{code}
drop keyspace test;

create keyspace test with replication = {
          'class': 'SimpleStrategy',
          'replication_factor': '1'
        };

use test;

create table users (
            user_id text PRIMARY KEY,
            first_name text,
            last_name text,
            email_addresses set<text>
        );

alter table users add mailing_address_lines list<text>;
{code}

As soon as you issue the alter table statement cqlsh hangs, and the java process hosting Cassandra consumes 100% of a single core's CPU.

If the alter table doesn't include a collection, it runs fine.","Ubuntu 12.04 LTS
3.2.0-23-virtual
",,,,,,,,,,,,,,,,,,14/Dec/12 09:51;slebresne;5064-v2.txt;https://issues.apache.org/jira/secure/attachment/12560939/5064-v2.txt,13/Dec/12 19:45;slebresne;5064.txt;https://issues.apache.org/jira/secure/attachment/12560838/5064.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-13 17:34:42.244,,,no_permission,,,,,,,,,,,,297480,,,Mon Dec 17 15:59:00 UTC 2012,,,,,,0|i14ozj:,235513,jbellis,jbellis,,,,,,,,,,13/Dec/12 17:34;jbellis;Can you verify against git 1.2.0 branch?,"13/Dec/12 18:04;enigmacurry;Yes, it's affected in git 1.2.0 as well.",13/Dec/12 18:11;brandon.williams;Repros on trunk as well.  Jstack indicates the commit log writer and memtable post flusher are consuming most of the cpu. Affects the entire cluster.,"13/Dec/12 19:48;slebresne;Patch attached.

This is due to the fix of CASSANDRA-4786 (and it happens that this example modifies the comparator of the CFS). The code from CASSANDRA-4786 is relying on the fact that maybeSwitchMemtable was either returning null or was really switching the memtable. That's not the case however if the memtable is clean (I could have swear I had checked for CASSANDRA-4786 but maybe I missed it, or the code has changed since then). In any case, the attached patch add a 'forceSwitch' flag to maybeSwitchMemtable that fixes the issue.","13/Dec/12 22:39;jbellis;This is starting to feel fragile, and I've never been a fan of Table.switchLock/maybeSwitchMemtable in the first place (even though I wrote it :).

Pushed a different approach to http://github.com/jbellis/cassandra/branches/5064.  The first commit is the important one: we replace Table.switchLock with reference-counted memtables.  We can then merge DataTracker.[renew|switch]Memtable, change maybeSwitchMemtable to unconditionally switch (but only flush if there were updates made), and this problem goes away along with a bunch of other switchLock-related complexity.

Also suspect that an atomic increment is a performance win over ReentrantReadWriteLock, which is one of the most heavyweight concurrency classes.",13/Dec/12 22:41;jbellis;NB: there may be a problem with flushing during CL replay since writeCommitLog is never passed as False since 1.0.  I've just removed it here for now.,"14/Dec/12 09:48;slebresne;I'm slightly confused by your branch. Do we agree that it doesn't do the ""change maybeSwitchMemtable to unconditionally switch""? Cause I don't see it.

Now about the memtable reference counting, I see at least a few problems:
# I see nothing that prevents flushing the same memtable multiple times.
# getting the commit log context and switching the memtable is not done atomically with respect to writes. So a write can be pushed in the commit log after the context we're getting but still reach the memtable we're about to flush. For normal update, this is mostly inefficient in that we'll kept commit logs around long than necessary and potentially replay some update unnecessarily, but for counter this is a bug.
# it's also possible that for postFlush tasks to not be scheduled in the order the commit log context were acquired. So we could discard a commitlog for which the data is not yet fully flushed.

Tbh, it may be possible to fix those problems (though for the 2nd one I don't have much idea), but I doubt we'll end up with something simpler than the current implementation. That might still be worth it ultimately for the performance improvement (though I'm not sure it's one of our bottleneck, so keeping the lock that is easier to reason about may be better), but I'm really doubtful that changing such an important piece of code just before a release is a good idea (again, supposing we even have a solution for the problems above).

I also don't think using reference counting really makes this issue simpler.  But I do agree that the looping in CFS.reload is a bit retarded. And more generally, the ""maybe"" part in maybeSwitchMemtable is probably not justified anymore. It was useful when this was called on the with path, where all we wanted was to avoid OOM and if some other thread was already flushing it was fine. But now that we don't do that, it probably does more bad than good. What I mean is that if you call forceFlush, you expect that any write that happens-before your call has been flushed. But currently, if some other thread flushes the memtable, you'll return immediately while some data may be currently under flush (but so not flushed yet). So attaching a v2 that remove the freeze business (thus getting rid of the CFS.reload loop). I've kept the forceSwitch flag of the first patch to avoid recreating a memtable object if not needed in the normal flush path, but we can also remove it and make forceSwitch the default if we prefer.
","14/Dec/12 14:31;jbellis;bq. Do we agree that it doesn't do the ""change maybeSwitchMemtable to unconditionally switch""?

Here's the new core of switchMemtable:

{code}
.       for (ColumnFamilyStore cfs : concatWithIndexes())
        {
            Memtable mt = cfs.data.switchMemtable();
            if ((!mt.isClean()))
                memtables.add(mt);
        }
{code}

bq. I see nothing that prevents flushing the same memtable multiple times.

What prevents it is that DataTracker.switchMemtable is atomic and will only return a given memtable once.  (So, if we have a whole bunch of threads calling forceFlush at once, we'll probably switch out a few empty ones, but that is the end of the extra work we do.)

bq. for counter this is a bug

Ugh, counters again.  Have to think about this.

bq. I also don't think using reference counting really makes this issue simpler.

I do. :)  We're net -120 lines of fairly tricky code.  We've had multiple bugs from not dealing with switchlock correctly (CASSANDRA-3712 and the flushing code from CASSANDRA-3411, off the top of my head).

bq. I'm really doubtful that changing such an important piece of code just before a release is a good idea

I'll buy that.",14/Dec/12 15:13;jbellis;v2 LGTM.,"14/Dec/12 15:46;jbellis;bq. getting the commit log context and switching the memtable is not done atomically with respect to writes

Isn't this easily fixed by moving the context request to after the wait-for-writes-to-finish loop?

bq. it's also possible that for postFlush tasks to not be scheduled in the order the commit log context were acquired

Bunch of ways we could fix this, including moving switchMemtable to its own single-threaded executor.  Or just synchronizing it, which is no worse performance-wise than the old writeLock.lock but much more localized.","17/Dec/12 11:05;slebresne;bq. v2 LGTM

Alright. For now I've committed v2 to 1.2.0 and closing this ticket.

bq. Ugh, counters again.

I agree about the counters being annoying once more. That being said, even counters aside, I do like the fact that we don't ever replay useless things. It gives me the confidence that ""we're doing it right"". Just to say that adding back the possibility to potentially hold commit logs longer than necessary doesn't feel like progress to me (it's probably a relatively minor downside compare to the performance benefits, a downside nonetheless).

bq. We're net -120 lines of fairly tricky code.

I do think it's just flushing that is tricky, because we have to somehow coordinate on-going writes, switching the memtable and the commit log and all that must be thread-safe. I'm abolutely convinced that our current stop-the-world does make it easier to reason about (granted it's not efficient and granted it's possible to forgot taking the read-lock at time, but that's still easy to understand errors). I do have a harder time convincing myself of the correctness of switchMemtable with the reference counting, because it allows for much more interleaving of events (that's what make concurrent programming hard after all). So I do buy ""more efficient"", I don't buy ""much less tricky"" so much, and what I really mean by that is that I would be mildly enthousiastic at committing such change before 2.0 (but I do think it would be a good change for performance overall).

bq. Isn't this easily fixed by moving the context request to after the wait-for-writes-to-finish loop?

If you do that, the context you'll get will include writes that went in the new memtable, so you'll discard commit log that have write not flushed yet, that's even worst.

bq. Bunch of ways we could fix this.

Sure. Just saying it ain't no child's play :)
",17/Dec/12 15:59;enigmacurry;Verified this is fixed on the 1.2.0 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Save EC2Snitch topology information in system table,CASSANDRA-5171,12628167,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,18/Jan/13 07:43,12/Mar/19 14:07,13/Mar/19 22:27,10/Jul/13 02:11,2.0 beta 2,,,,,1,,,,,,,EC2Snitch currently waits for the Gossip information to understand the cluster information every time we restart. It will be nice to use already available system table info similar to GPFS.,EC2,,,,,,,,,,,,,,,,,,07/Jul/13 08:06;vijay2win@yahoo.com;0001-CASSANDRA-5171-v2.patch;https://issues.apache.org/jira/secure/attachment/12591115/0001-CASSANDRA-5171-v2.patch,18/Jan/13 07:45;vijay2win@yahoo.com;0001-CASSANDRA-5171.patch;https://issues.apache.org/jira/secure/attachment/12565443/0001-CASSANDRA-5171.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-18 15:09:42.773,,,no_permission,,,,,,,,,,,,305041,,,Wed Jul 10 20:22:09 UTC 2013,,,,,,0|i186lr:,255872,brandon.williams,brandon.williams,,,,,,,,,,18/Jan/13 15:09;brandon.williams;+1,18/Jan/13 19:00;vijay2win@yahoo.com;Committed to 1.2 and trunk. Thanks!,"22/May/13 15:53;jbellis;This was reverted in CASSANDRA-5432, but I think the problem it solves is actually pretty severe, so I'm reopening it.

The problem is that pretty much everything from TokenMetadata to NetworkTopologyStrategy assumes that once we see a node, the snitch can tell us where it lives, and in particular that once the snitch tells us where a node lives it won't change its answer.

So this is problematic:

{code}
    public String getDatacenter(InetAddress endpoint)
    {
        if (endpoint.equals(FBUtilities.getBroadcastAddress()))
            return ec2region;
        EndpointState state = Gossiper.instance.getEndpointStateForEndpoint(endpoint);
        if (state == null || state.getApplicationState(ApplicationState.DC) == null)
            return DEFAULT_DC;
        return state.getApplicationState(ApplicationState.DC).value;
    }
{code}

That is, if we don't know where a node belongs (e.g., we just restarted and haven't been gosipped to yet), assume it's in {{DEFAULT_DC}}.

This can lead to data loss.  Consider node X in DC1, where keyspace KS is replicated.  Suddenly X is yanked out of DC1 and placed in DC2, where KS is not replicated.  Nobody will bother querying X for the data in KS that was formerly replicated to it.  Even repair will not see it.","07/Jul/13 08:06;vijay2win@yahoo.com;Attached patch brings the reverted patch back to life, in addition it saves the reseted_ip in system table so when a connection is reopened to the host we will use the reseted_ip instead of endpoint address.",07/Jul/13 17:16;vijay2win@yahoo.com;Looks like CASSANDRA-5669 forces the user to open public and private IP's for communication (and hence CASSANDRA-5432 no longer a problem) within a AZ and hence v2 is not needed and we can commit v1.,"09/Jul/13 12:20;jasobrown;While this patch (v1 actually) was reverted in CASANDRA-5432, it wasn't satisfactorily answered why the patch failed to work as expected. I'm adding details here so we can get this ticket done right :).

First it's helpful to explore how a node can start gossip in EC2MRS with inter-DC (inter-region) enabled (and a Priam-type setup).

# ec2 instance is started, Priam comes up first and adds publicIP/sslPort to the security group's ingress privileges (so this node can accept connections on it's publicIP/sslPort from anywhere). 
# c* starts, and gets seed node public hostnames from Priam
# gossip to one of the seeds - the public hostname will resolve to the node's public IP addr.
# When OTC goes to write the first message on the seed, it gets a socket from OTCP.newSocket(). newSocket() calls isEncryptedChannel() to determine if we need to encrypt the data on the wire. As we don't know anything yet about the seed node (remember we havn't started gossip yet with anyone), isEncryptedChannel() will always return true when the following are true:
## internode_encryption != none
## we don't know the DC or RACK info for the remote node (which is the case when using the EC2MRS). This step is a little funky as OTCP calls the snitch for the seed's DC/RACK, to which EC2MRS will return UNKNOWN-DC/UNKNOWN-RACK, which will just happen to not match a value like ""us-east-1"" (the current's node's DC). 
# create the socket using remote node's publicIP addr on the SSL port.
# create the connection from and send messages successfully, assuming you've opened the SSL port for public addresses on the security group (which Priam handles).

Thus, if we are connecting to a node in the same EC2 region, we connect on the publicIP (as expected) but use the SSL port.

After we learn, via gossip, about a remote node's DC/RACK/localIP, we can choose to reconnect to nodes in the same region on the localIP/nonSSLPort.

The reason why Vijay's patch had problems here was because on restart, we would already know the DC/RACK from the previous execution of c* on this node, and the check in OTCP.isEncryptedChannel() returns false (do not use encryption), so a we choose to use the non-SSL port when creating a connection to the publicIP. Thus the connection creation unltimately fails because the non-SSL port is not opened for traffic on the security group for the public IP (nor should it be). EDIT: The other part of the problem is that we start the connection on the publicIP rather than localIP (INTERNAL_IP) even if we already have the localIP.

To make this patch work then, I think getting the localIP address in the OTCP's ctor would work the best. Code would look something like this:

{code}
    OutboundTcpConnectionPool(InetAddress remoteEp)
    {
        EndpointState epState =  Gossiper.instance.getEndpointStateForEndpoint(remoteEp);
        if(epState != null && epState.getApplicationState(ApplicationState.INTERNAL_IP) != null
            && epState.getApplicationState(ApplicationState.DC).equals(snitch.getDatacenter(FBUtilities.getBroadcastAddress()))
        {
            id = epState.getApplicationState(ApplicationState.INTERNAL_IP);             
        }
        else
        {
            id = remoteEp;
        }
        
        cmdCon = new OutboundTcpConnection(this);
        cmdCon.start();
        ackCon = new OutboundTcpConnection(this);
        ackCon.start();

        metrics = new ConnectionMetrics(id, this);
    }
{code}

Then you would connect on the localIP addr with the correct port (SSL or non-SSL).","09/Jul/13 15:20;jasobrown;Ahh, didn't look at Vijay's second patch, but it more or less does what I suggested - however, I thought we were already keeping the resettedAddr in the system table, but the second patch adds that in.","09/Jul/13 16:44;jasobrown;I'm +1 on the v2 patch. As a minor nit, you might rename the ""communication_ip"" to something like ""preferred_ip"".

As an aside, I've never been thrilled with the default ""UNKNOWN-DC"" that can get returned when we don't know the DC. It's guaranteed to be wrong in almost all cases. However, I'm not sure what we'd do instead - returning null or empty string seems only slightly worse than the current default.","09/Jul/13 17:47;vijay2win@yahoo.com;{quote}
I've never been thrilled with the default ""UNKNOWN-DC"" 
{quote}

I am not a fan either, if we throw an exception node will never start the gossiping.... 
Back in the days, it made sense with the old way of versioning etc. We should probably open another ticket to discuses this if needed.

I will commit with the nit in few min. Thanks!","10/Jul/13 06:41;vijay2win@yahoo.com;PS: i only committed to 2.0 to be safe, let me know if you think otherwise. Thanks!","10/Jul/13 13:19;jasobrown;I thought the patch was reasonable enough for 1.2. If you give me a few days, I can test it out in our env, and let you know if it borks everything or not.

EDIT: Yeah, will definitely want to test to make sure it's cool with CASSANDRA-5669 (i.e. they don't collide to not connect at all).",10/Jul/13 17:26;vijay2win@yahoo.com;Thanks Jason!,"10/Jul/13 20:22;jasobrown;damn jira hotkeys - sorry this got reassigned (to me, and back), Vijay :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_paged_slices doesn't reset startColumn after first row,CASSANDRA-4136,12550441,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,jbellis,jbellis,10/Apr/12 21:15,12/Mar/19 14:07,13/Mar/19 22:27,11/Apr/12 14:34,1.1.0,,,,,0,,,,,,,"As an example, consider the WordCount example (see CASSANDRA-3883).  WordCountSetup inserts 1000 rows, each with three columns: text3, text4, int1.  (Some other miscellaneous columns are inserted in a few rows, but we can ignore them here.)

Paging through with get_paged_slice calls with a count of 99, CFRecordReader will first retrieve 33 rows, the last of which we will call K.  Then it will attempt to fetch 99 more columns, starting with row K column text4.

The bug is that it will only fetch text4 for *each* subsequent row K+i, instead of returning (K, text4), (K+1, int1), (K+1, int3), (K+1, text4), etc.",,,,,,,,,,,,,,,,,,,11/Apr/12 14:12;slebresne;4136.txt;https://issues.apache.org/jira/secure/attachment/12522249/4136.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-11 14:12:00.61,,,no_permission,,,,,,,,,,,,235305,,,Wed Apr 11 14:34:51 UTC 2012,,,,,,0|i0gsef:,96031,jbellis,jbellis,,,,,,,,,,"11/Apr/12 14:12;slebresne;Attached patch adds the ability to do paging through multiple rows. The support added by the patch is limited to what get_paged_slices requires (in particular using a SliceQueryFilter where finish != """" is not supported with that new option). The patch contains a unit test.","11/Apr/12 14:24;jbellis;With this and the 3883 patches I get

{noformat}
$ cat /tmp/word_count5/part-r-00000
0       250
1       250
2       250
3       250
word1   2002
word2   1
{noformat}

which is the expected result.

+1","11/Apr/12 14:34;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hector RetryService drop host,CASSANDRA-4055,12546662,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,,daningaddr,daningaddr,15/Mar/12 21:19,12/Mar/19 14:07,13/Mar/19 22:27,15/Mar/12 21:23,,,,,,0,,,,,,,,"This bug is in Hector code.

If there is exception in addCassandraHost() before adding host to hostPools, since addCassandraHost does not throw exception, the host will be removed from downedHostQueue, and the host will be gone forever.

        if(downedHostQueue.contains(cassandraHost) && verifyConnection(cassandraHost)) {
          connectionManager.addCassandraHost(cassandraHost);
          downedHostQueue.remove(cassandraHost);
          return;
        }",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-03-15 21:23:10.927,,,no_permission,,,,,,,,,,,,231820,,,Thu Mar 15 21:23:10 UTC 2012,,,,,,0|i0grfj:,95874,,,,,,,,,,,,15/Mar/12 21:23;jbellis;The Hector client is maintained at https://github.com/rantav/hector.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index CF tombstones can cause OOM,CASSANDRA-4314,12559612,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,wpoziombka,wpoziombka,06/Jun/12 22:16,12/Mar/19 14:07,13/Mar/19 22:27,08/Jun/12 07:26,1.0.11,1.1.2,,,,0,,,,,,,"My database (now at 1.0.10) is in a state in which it goes out of memory with hardly any activity at all.  A key slice nothing more.

The logs attached are this including verbose gc in stdout.  I started up cassandra and waited a bit to ensure that it was unperturbed.

Then (about 15:46) I ran this slice (using Pelops), which in this case should return NO data.  My client times out and the database goes OOM.

                  ConsistencyLevel cl = ConsistencyLevel.TWO;//TWO nodes in my cluster
                  Selector s = new Selector(this.pool);
                  List<IndexExpression> indexExpressions = new ArrayList<IndexExpression>();
                  IndexExpression e = new IndexExpression(
                              ByteBuffer.wrap(""encryptionSettingsID"".getBytes(ASCII)), IndexOperator.EQ,
                              ByteBuffer.wrap(encryptionSettingsID.getBytes(Utils.ASCII)));
                  indexExpressions.add(e);
                  IndexClause indexClause = new IndexClause(indexExpressions,
                              ByteBuffer.wrap(EMPTY_BYTE_ARRAY), 1);
                  SlicePredicate predicate = new SlicePredicate();
                  predicate.setColumn_names(Arrays.asList(new ByteBuffer[]
                        { ByteBuffer.wrap(COL_PAN_ENC_BYTES) }));
                  List<KeySlice> slices = s.getKeySlices(CF_TOKEN, indexClause, predicate, cl);

Note that “encryptionSettingsID” is an indexed column.  When this is executed there should be no columns with the supplied value.

I suppose I may have some kind of blatant error in this query but it is not obvious to me.  I’m relatively new to cassandra.

My key space is defined as follows:

KsDef(name:TB_UNIT, strategy_class:org.apache.cassandra.locator.SimpleStrategy, strategy_options:{replication_factor=3}, 
cf_defs:[

CfDef(keyspace:TB_UNIT, name:token, column_type:Standard, comparator_type:BytesType, column_metadata:[ColumnDef(name:70 61 6E 45 6E 63, validation_class:BytesType), ColumnDef(name:63 72 65 61 74 65 54 73, validation_class:DateType), ColumnDef(name:63 72 65 61 74 65 44 61 74 65, validation_class:DateType, index_type:KEYS, index_name:TokenCreateDate), ColumnDef(name:65 6E 63 72 79 70 74 69 6F 6E 53 65 74 74 69 6E 67 73 49 44, validation_class:UTF8Type, index_type:KEYS, index_name:EncryptionSettingsID)], caching:keys_only), 

CfDef(keyspace:TB_UNIT, name:pan_d721fd40fd9443aa81cc6f59c8e047c6, column_type:Standard, comparator_type:BytesType, caching:keys_only), 

CfDef(keyspace:TB_UNIT, name:counters, column_type:Standard, comparator_type:BytesType, column_metadata:[ColumnDef(name:75 73 65 43 6F 75 6E 74, validation_class:CounterColumnType)], default_validation_class:CounterColumnType, caching:keys_only)

])


tpstats show pending tasks many minutes after time out:


[root@r610-lb6 bin]# ../cassandra/bin/nodetool -h 127.0.0.1 tpstats
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
ReadStage                         3         3            107         0                 0
RequestResponseStage              0         0             56         0                 0
MutationStage                     0         0              6         0                 0
ReadRepairStage                   0         0              0         0                 0
ReplicateOnWriteStage             0         0              0         0                 0
GossipStage                       0         0           2231         0                 0
AntiEntropyStage                  0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
MemtablePostFlusher               0         0              3         0                 0
StreamStage                       0         0              0         0                 0
FlushWriter                       0         0              3         0                 0
MiscStage                         0         0              0         0                 0
InternalResponseStage             0         0              0         0                 0
HintedHandoff                     0         0              9         0                 0

Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  0
BINARY                       0
READ                         0
MUTATION                     0
REQUEST_RESPONSE             0

cfstats:

Keyspace: keyspace
        Read Count: 118
        Read Latency: 0.14722033898305084 ms.
        Write Count: 0
        Write Latency: NaN ms.
        Pending Tasks: 0
                Column Family: token
                SSTable count: 7
                Space used (live): 4745885584
                Space used (total): 4745885584
                Number of Keys (estimate): 18626048
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 118
                Read Latency: 0.147 ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 55058352
                Key cache: disabled
                Row cache: disabled
                Compacted row minimum size: 150
                Compacted row maximum size: 258
                Compacted row mean size: 201

                Column Family: pan_2fef6478b62242dd94aecaa049b9d7bb
                SSTable count: 7
                Space used (live): 1987147156
                Space used (total): 1987147156
                Number of Keys (estimate): 14955264
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 0
                Read Latency: NaN ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 28056224
                Key cache: disabled
                Row cache: disabled
                Compacted row minimum size: 104
                Compacted row maximum size: 124
                Compacted row mean size: 124

                Column Family: counters
                SSTable count: 11
                Space used (live): 3433469364
                Space used (total): 3433469364
                Number of Keys (estimate): 21475328
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 0
                Read Latency: NaN ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 40271696
                Key cache capacity: 4652
                Key cache size: 4652
                Key cache hit rate: NaN
                Row cache: disabled
                Compacted row minimum size: 125
                Compacted row maximum size: 179
                Compacted row mean size: 150

","AS5 64, 64 GB ram, 12 core, Intel SSD ",,,,,,,,,,,,,,,,,,07/Jun/12 17:22;wpoziombka;2012-06-07-compact.zip;https://issues.apache.org/jira/secure/attachment/12531274/2012-06-07-compact.zip,07/Jun/12 15:40;wpoziombka;2012-06-07.zip;https://issues.apache.org/jira/secure/attachment/12531270/2012-06-07.zip,08/Jun/12 02:48;jbellis;4314-1.0.txt;https://issues.apache.org/jira/secure/attachment/12531360/4314-1.0.txt,08/Jun/12 02:48;jbellis;4314-1.1.txt;https://issues.apache.org/jira/secure/attachment/12531361/4314-1.1.txt,06/Jun/12 22:16;wpoziombka;oom.zip;https://issues.apache.org/jira/secure/attachment/12531173/oom.zip,07/Jun/12 05:42;wpoziombka;yourkitsnapshot.png;https://issues.apache.org/jira/secure/attachment/12531220/yourkitsnapshot.png,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2012-06-06 22:26:56.394,,,no_permission,,,,,,,,,,,,256053,,,Sat Jun 09 15:29:23 UTC 2012,,,,,,0|i0guh3:,96367,slebresne,slebresne,,,,,,,,,,06/Jun/12 22:16;wpoziombka;log files.,"06/Jun/12 22:26;jbellis;Sounds like ""I'm using a row as a queue and building up a ton of tombstones"" to me, which is an antipattern: Cassandra has to send the tombstones back to the coordinator for read repair.","07/Jun/12 00:31;wpoziombka;I'm sorry but I don't understand the statement.  I have done no deletes and the rows are very small (max is like 285 bytes according to cfstats, which is inline with what I know about these data).  I did drop a column family before and I have updated many column values.  I don't know if that creates tombstones too.  

the model is this:

token - is the primary column family.  Has a column in it called ""pan"" which contains nearly unique binary values.  We need to be able to uniquely search pan so I have a pan_XXX family with pan as the key and token is a column name with a timestamp as a value.  pan_XXX is basically an index to the token column family.  

In the current scenario, there are very few token columns in the pan column family (indeed the largest row is 124 bytes by cfstat's measure).  At some point I need to essentially re-index (pan values change).  So I create a new dynamic column family (pan_YYY), modify the token's pan column and add new column to pan_YYY then when fully done I drop pan_XXX column family.

So at the end of it a new column family (an index) is populated and the old one is dropped.  All values in one column of the token column family are modified.

What is shown here in these logs is none of the above though.  I have restarted cassandra and done nothing but run the one query.

AND ONE MORE THING

I neglected to mention that during the update of the ""token"" column family it updates the indexed column too.  The indexed column essentially holds either XXX or YYY so we can resolve pan_XXX etc.  This may be important.  As it goes through each is eventually changed from XXX to YYY.  This index is the same that is used in the query above.","07/Jun/12 05:42;wpoziombka;A screenshot from Yourkit snapshot I took while it was growing.

Looks like 4 GB of DeletedColumn stuff.","07/Jun/12 14:02;jbellis;bq. I have done no deletes

Then either you're using TTLs -- expired columns are basically the same as deletes, in this respect -- or you're doing a lot of indexed column overwrites, which also generate deletes in the index CF.","07/Jun/12 15:33;wpoziombka;I am doing the latter.  However, I have run repair which I would expect to clear up the tombstones no?  I still observe the same problem after repair.",07/Jun/12 15:40;wpoziombka;Here are log files taken showing running repair then running the query in question demonstrating the out of memory condition.,"07/Jun/12 15:56;wpoziombka;I guess repair may not be the ticket... it should be done during compaction I guess.  But compaction should run automatically.  I have heeded the note on Tuning Compaction and not run a major compaction:

""Also, once you run a major compaction, automatic minor compactions are no longer triggered frequently forcing you to manually run major compactions on a routine basis. So while read performance will be good immediately following a major compaction, it will continually degrade until the next major compaction is manually invoked. For this reason, major compaction is NOT recommended by DataStax.""","07/Jun/12 16:55;brandon.williams;Repair doesn't remove tombstones, and compactions only remove them if they are older than gc_grace_seconds, so you're always holding however many occurred in that time period.",07/Jun/12 17:00;jbellis;Hmm.  Looks like we don't override index gcgs to 0.  I can't think of any reason to keep tombstones around on a purely local table.,07/Jun/12 17:22;wpoziombka;I have run nodetool compact on each node then reran the query.  Still run out of memory.  Attached is logs.,"08/Jun/12 02:48;jbellis;bq. Looks like we don't override index gcgs to 0

Patches to do this attached for 1.0 and 1.1.  If you compact the *index* CF with this patch applied, that should get rid of the tombstones. (compacting the data CF won't do anything.)",08/Jun/12 07:17;slebresne;+1,08/Jun/12 07:26;slebresne;(Committed),"08/Jun/12 16:17;wpoziombka;Jonathan, thanks a lot for this.  I was hedging my bets and duplicating column families instead of the column overwrites but this is WAY more preferrable.

A couple of quick questions: so I should do explicit compactions on the index CF only?  Once I do this explicit compaction I must do for all column families as part of routing maintenance?  Again I am speaking in reference to the comment in the doc: 

""Also, once you run a major compaction, automatic minor compactions are no longer triggered frequently forcing you to manually run major compactions on a routine basis. So while read performance will be good immediately following a major compaction, it will continually degrade until the next major compaction is manually invoked. For this reason, major compaction is NOT recommended by DataStax.""

","08/Jun/12 19:32;wpoziombka;How to compact the index CF?  What is its name?  I've tried a variety of things and can't seem to find the magic expression.
","09/Jun/12 15:29;jbellis;It's not exposed through nodetool, but you can invoke forceMajorCompaction on the index cfs mbean directly (o.a.c.db.IndexColumnFamilies), or you can drop and recreate the index.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing arrayOffset in FBUtilities.hash,CASSANDRA-4250,12555995,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,richardlow,richardlow,richardlow,16/May/12 09:26,12/Mar/19 14:07,13/Mar/19 22:27,16/May/12 09:58,1.1.1,,,,,0,,,,,,,"In CASSANDRA-3869, FBUtilities.hash was optimised to use the backing byte array if there is one.  However, there is a missing +arrayOffset in the offset parameter.

This can cause incorrect hashes resulting in data going to the wrong place, etc..  I haven't observed any errors directly attributable to this so maybe we are lucky and all backing arrays start at 0 but this could cause data loss in the worst case.",,,,,,,,,,,,,,,,,,,16/May/12 09:28;richardlow;CASSANDRA-4250-v1.patch;https://issues.apache.org/jira/secure/attachment/12527595/CASSANDRA-4250-v1.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-16 09:58:07.829,,,no_permission,,,,,,,,,,,,255992,,,Wed May 16 09:58:07 UTC 2012,,,,,,0|i0gtqf:,96247,slebresne,slebresne,,,,,,,,,,16/May/12 09:28;richardlow;Patch against 1.1.0,"16/May/12 09:58;slebresne;Good catch, +1. Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compactions don't work while node is bootstrapping,CASSANDRA-5244,12631944,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,jihartik,jihartik,12/Feb/13 13:41,12/Mar/19 14:07,13/Mar/19 22:27,14/Feb/13 04:56,1.2.2,,,,,0,gossip,,,,,,"It seems that there is a race condition in StorageService that prevents compactions from completing while node is in a bootstrap state.

I have been able to reproduce this multiple times by throttling streaming throughput to extend the bootstrap time while simultaneously inserting data to the cluster.

The problems lies in the synchronization of initServer(int delay) and reportSeverity(double incr) methods as they both try to acquire the instance lock of StorageService through the use of synchronized keyword. As initServer does not return until the bootstrap has completed, all calls to reportSeverity will block until that. However, reportSeverity is called when starting compactions in CompactionInfo and thus all compactions block until bootstrap completes. 

This might severely degrade node's performance after bootstrap as it might have lots of compactions pending while simultaneously starting to serve reads.

I have been able to solve the issue by adding a separate lock for reportSeverity and removing its class level synchronization. This of course is not a valid approach if we must assume that any of Gossiper's IEndpointStateChangeSubscribers could potentially end up calling back to StorageService's synchronized methods. However, at least at the moment, that does not seem to be the case.

Maybe somebody with more experience about the codebase comes up with a better solution?

(This might affect DynamicEndpointSnitch as well, as it also calls to reportSeverity in its setSeverity method)",,,,,,,,,,CASSANDRA-5129,,,,,,,,,14/Feb/13 04:51;brandon.williams;5244.txt;https://issues.apache.org/jira/secure/attachment/12569310/5244.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-12 15:37:57.733,,,no_permission,,,,,,,,,,,,312440,,,Thu Feb 14 05:47:17 UTC 2013,,,,,,0|i1hx2n:,312786,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"12/Feb/13 15:37;jbellis;Thanks for the detective work, Jouni.  I'll let Brandon comment on solutions; in the meantime, marking Minor since while inconvenient this does not compromise correctness.","14/Feb/13 03:43;brandon.williams;This is more severe than we originally thought, and causes CASSANDRA-5129 when there is a secondary index:

{noformat}
""CompactionExecutor:1"" daemon prio=10 tid=0x00007effbc03c800 nid=0x7abf waiting for monitor entry [0x00007effc843a000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.cassandra.service.StorageService.reportSeverity(StorageService.java:905)
    - waiting to lock <0x00000000ca576ac8> (a org.apache.cassandra.service.StorageService)
    at org.apache.cassandra.db.compaction.CompactionInfo$Holder.started(CompactionInfo.java:141)
    at org.apache.cassandra.metrics.CompactionMetrics.beginCompaction(CompactionMetrics.java:90)
    at org.apache.cassandra.db.compaction.CompactionManager$9.run(CompactionManager.java:813)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}","14/Feb/13 04:27;brandon.williams;It seems to me the only reason we're synchronizing here is for the increment, and we don't need to get our own severity out of gossip, so we can just track a local AtomicDouble instead.",14/Feb/13 04:36;vijay2win@yahoo.com;+1,14/Feb/13 04:56;brandon.williams;Committed.,14/Feb/13 05:47;mkjellman;this looks good.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RangeStreamer has no way to report failures, allowing bootstrap/move etc to complete without data",CASSANDRA-5009,12618363,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,brandon.williams,brandon.williams,30/Nov/12 18:59,12/Mar/19 14:07,13/Mar/19 22:27,03/Dec/12 20:33,1.1.8,1.2.0 beta 3,,,,1,,,,,,,"It looks like we fixed this for 1.2 by having RS.fetch() throw, but in 1.1 it does not and there doesn't appear to be a way to detect an RS failure, which among other things will cause bootstrap to succeed even though it failed to fetch any data.",,,,,,,,,,,,,,,,,,,03/Dec/12 19:01;brandon.williams;5009-v2.txt;https://issues.apache.org/jira/secure/attachment/12555808/5009-v2.txt,30/Nov/12 22:32;brandon.williams;5009.txt;https://issues.apache.org/jira/secure/attachment/12555577/5009.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-03 18:50:09.232,,,no_permission,,,,,,,,,,,,293028,,,Mon Dec 03 20:33:24 UTC 2012,,,,,,0|i0srn3:,165955,yukim,yukim,,,,,,,,,,"30/Nov/12 19:25;brandon.williams;Actually we haven't completely fixed this in 1.2, because we rely on the FD notification to fail but use phi * 2 to avoid a premature costly repair abortion, but the streaming can fail out before that and thus fetch() will never raise an exception.","30/Nov/12 22:32;brandon.williams;Patch to maintain a set of completed ranges, then check at the end that every range was successful or raise an exception.

This patch is against 1.1, but I'll note that when merged RangeStreamer should look exactly the same.  That is to say, all the FD business will be removed, as I can't figure out how it makes any sense to check the FD at all since we're going to block on the streaming latch first.  If any ranges (not necessarily streams) failed, then it's time to bail regardless of the FD, and if the FD does convict before that it won't break us out of the latch wait.
","03/Dec/12 18:50;yukim;two comments on the patch:

* looks like exceptionMessage is not used anywhere
* I think _completed_ needs to be synchronized and _completed.addAll(ranges)_ should be placed before _latch.countDown()_, otherwise we have chance to get incomplete completed set.","03/Dec/12 19:01;brandon.williams;Oops, exceptionMessage was a vestige from my initial attempt at backporting the 1.2 version before I discovered it was flawed too.

Updated patch removes that, moves the addAll call before the latch countdown, and initiates the set with a concurrent hash map.",03/Dec/12 19:52;yukim;+1,03/Dec/12 20:33;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in log related to Murmur3Partitioner,CASSANDRA-4567,12604460,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,tpatterson,tpatterson,22/Aug/12 17:41,12/Mar/19 14:17,13/Mar/19 22:27,30/Aug/12 16:56,1.2.0 beta 1,,,,,0,,,,,,,"Start a 2-node cluster on cassandra-1.1. Bring down one node, upgrade it to trunk, start it back up. The following error shows up in the log:
{code}
...
 INFO [main] 2012-08-22 10:44:40,012 CacheService.java (line 170) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [SSTableBatchOpen:1] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-2 (148 bytes)
 INFO [SSTableBatchOpen:2] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-1 (226 bytes)
 INFO [SSTableBatchOpen:3] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-3 (89 bytes)
ERROR [SSTableBatchOpen:3] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:3,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-3 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [SSTableBatchOpen:2] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:2,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-1 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [SSTableBatchOpen:1] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-2 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [main] 2012-08-22 10:44:40,486 DatabaseDescriptor.java (line 522) Couldn't detect any schema definitions in local storage.
 INFO [main] 2012-08-22 10:44:40,487 DatabaseDescriptor.java (line 525) Found table data in data directories. Consider using the CLI to define your schema.
...
{code}

Note that the error does not happen when upgrading from cassandra-1.0 to cassandra-1.1, or when ""upgrading"" from trunk to trunk.

This is the exact dtest I used:
{code}
from dtest import Tester, debug

FROM_VERSION = 'git:cassandra-1.1'
TO_VERSION = 'git:trunk'

class TestUpgradeOneNode(Tester):

    def upgrade_test(self):
        # Start a cluster
        cluster = self.cluster
        cluster.set_cassandra_dir(cassandra_version=FROM_VERSION)
        cluster.populate(2).start()
        node1 = cluster.nodelist()[0]
        node1.watch_log_for('Listening for thrift clients...')

        # Bring one node down and upgrade it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        node1.set_cassandra_dir(cassandra_version=TO_VERSION)
        node1.start(wait_other_notice=True)
{code}",Using ccm on ubuntu,,,,,,,,,,,,,,,,,,28/Aug/12 21:17;vijay2win@yahoo.com;0001-CASSANDRA-4567-v2.patch;https://issues.apache.org/jira/secure/attachment/12542830/0001-CASSANDRA-4567-v2.patch,28/Aug/12 21:39;vijay2win@yahoo.com;0001-CASSANDRA-4567-v3.patch;https://issues.apache.org/jira/secure/attachment/12542835/0001-CASSANDRA-4567-v3.patch,28/Aug/12 16:50;vijay2win@yahoo.com;0001-CASSANDRA-4567.patch;https://issues.apache.org/jira/secure/attachment/12542785/0001-CASSANDRA-4567.patch,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-08-22 22:44:56.706,,,no_permission,,,,,,,,,,,,256253,,,Thu Aug 30 16:56:31 UTC 2012,,,,,,0|i0gx7j:,96810,xedin,xedin,,,,,,,,,,"22/Aug/12 22:44;vijay2win@yahoo.com;CASSANDRA-3772 modified the default Paritioner to M3P (was RandomP). Users who upgrade the existing clusters need to change the yaml settings. 
May be updating NEWS.txt will help?","22/Aug/12 23:07;xedin;Agreed, I will update the NEWS as part of CASSANDRA-3772 when finished, I don't really see any user-friendly way to change default partitioner, users who use packages shouldn't be affected as configuration is separate but automated tools probably would have to add a check to reset partitioner when testing 1.1 -> 1.2 migration...",23/Aug/12 16:35;tpatterson;Would it be feasible to provide a more helpful error message for people that didn't read NEWS.txt?,"24/Aug/12 03:36;jbellis;Should we just ignore the partitioner after first startup, the way we do with initial_token?","24/Aug/12 16:21;vijay2win@yahoo.com;We can implement it like initial token, but the problem is that the User will not notice/fix it…. When a new node is bootstrapped then he/she will run into issues.",27/Aug/12 17:03;jbellis;We could check partitioner on bootstrap the way we do cluster name...,"28/Aug/12 16:50;vijay2win@yahoo.com;Didn't realize that System table requires SS.getPartitioner (Some how thought it uses local partitioner). 

Looks like we cannot store the partitioner information in system table like initial token (because to open the Table/CF we need a partitioner) and we have to store it as a cluster metadata (new file or something). IMHO, We can make the log messages pretty print and live with it?

BTW: checking the partitioner to be consistent with the cluster is a safe thing which we have to do, hence the attached patch.","28/Aug/12 18:38;jbellis;bq. We can make the log messages pretty print and live with it

+1","28/Aug/12 20:48;xedin;Two things: 

 - I think we can cache canonical name instead of calling StorageService.getPartitioner().getClass().getCanonicalName() all the time
 - In ""if (gDigestMessage.partioner != null && !gDigestMessage.partioner.equals(DatabaseDescriptor.getClusterName()))"" the second condition tries to match partitioner to cluster_name.","28/Aug/12 21:12;vijay2win@yahoo.com;Done! (not sure how i missed this with the tests which i did :( )

Attached patch also exits the VM if it detects the change in partitioner, earlier we where skiping the SST's and start the node without any data.",28/Aug/12 21:25;xedin;I see you added partitionerName to GossipDigestSynVerbHandler but I think better would be to add it to DatabaseDescriptor which would allow us to use it in GossipDigestSynVerbHandler and well as Gossiper :),28/Aug/12 21:39;vijay2win@yahoo.com;Done!,29/Aug/12 20:54;xedin;+1,"30/Aug/12 01:22;vijay2win@yahoo.com;Committed with regenerated test/data/serialization/1.2/gms.Gossip.bin, Thanks!",30/Aug/12 13:54;jbellis;looks like this broke SSTableReaderTest.testPersistantStatisticsWithSecondaryIndex,30/Aug/12 16:56;vijay2win@yahoo.com;Fixed in 9cd53fba648ae5a30a181f8a06786f33db95a0fe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update column family fails,CASSANDRA-4561,12604193,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,zenek_kraweznik0,zenek_kraweznik0,21/Aug/12 09:24,12/Mar/19 14:17,13/Mar/19 22:27,04/Sep/12 21:17,1.1.5,,,,,2,,,,,,,"[default@test] show schema;
create column family Messages
  with column_type = 'Standard'
  and comparator = 'AsciiType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'AsciiType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 2
  and max_compaction_threshold = 4
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and compaction_strategy_options = {'sstable_size_in_mb' : '1024'}
  and compression_options = {'chunk_length_kb' : '64', 'sstable_compression' : 'org.apache.cassandra.io.compress.DeflateCompressor'};


[default@test] update column family Messages with min_compaction_threshold = 4 and  max_compaction_threshold = 32;
a5b7544e-1ef5-3bfd-8770-c09594e37ec2
Waiting for schema agreement...
... schemas agree across the cluster

[default@test] show schema;
create column family Messages
  with column_type = 'Standard'
  and comparator = 'AsciiType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'AsciiType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 2
  and max_compaction_threshold = 4
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and compaction_strategy_options = {'sstable_size_in_mb' : '1024'}
  and compression_options = {'chunk_length_kb' : '64', 'sstable_compression' : 'org.apache.cassandra.io.compress.DeflateCompressor'};",,,,,,,,,,,,,,,,,,,07/Sep/12 11:45;xedin;CASSANDRA-4561-CS.patch;https://issues.apache.org/jira/secure/attachment/12544204/CASSANDRA-4561-CS.patch,29/Aug/12 13:22;xedin;CASSANDRA-4561.patch;https://issues.apache.org/jira/secure/attachment/12542918/CASSANDRA-4561.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-22 03:04:14.026,,,no_permission,,,,,,,,,,,,256247,,,Tue Sep 11 00:32:47 UTC 2012,,,,,,0|i0gx4v:,96798,jbellis,jbellis,,,,,,,,,,"21/Aug/12 09:28;zenek_kraweznik0;in logfile I see only this:
 INFO [MigrationStage:1] 2012-08-21 11:27:55,560 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-schema_columnfamilies@970905946(1266/1582 serialized/live bytes, 20 ops)
 INFO [FlushWriter:5] 2012-08-21 11:27:55,561 Memtable.java (line 264) Writing Memtable-schema_columnfamilies@970905946(1266/1582 serialized/live bytes, 20 ops)
 INFO [FlushWriter:5] 2012-08-21 11:27:55,587 Memtable.java (line 305) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-he-196-Data.db (1336 bytes) for commitlog position ReplayPosition(segmentId=4914817711083622, position=333055)","21/Aug/12 12:58;zenek_kraweznik0;oh, and it obviously happens when I'm trying to change anything else, not only compaction treshhold, ex: sstable_size_in_mb","22/Aug/12 03:04;suguru;I saw a similar case. In my case, columns of schema_keyspaces and schema_columnfamilies in the system keyspace had future timestamp such as 2052-08-01 11:22:33. All operations of changing schemas were not affected. I do not know why timestamps had future values. I fixed it with rebuilding system keyspace.","22/Aug/12 07:12;zenek_kraweznik0;Hmmm, it's possible. How did you rebuilt system keyspace?","22/Aug/12 09:16;xedin;Zenek, what version are you using? It's possible if you are on version less than 1.1.3 or if you have created your schema before 1.1.3 because of CASSANDRA-4432...","22/Aug/12 10:21;zenek_kraweznik0;I'm using 1.1.4, but schema was created in 0.8.5

Could you help me solve my problem step by step?",22/Aug/12 10:27;xedin;Did you upgrade directly to 1.1.4 or one of the previous 1.1 versions first? Please check timestamps in your schema_* ColumnFamilies and see if they have any future dates. The easiest way I see to fix this would be to re-create your schema if you have timestamp problems from CASSANDRA-4432 ,"22/Aug/12 12:40;zenek_kraweznik0;I've upgraded cassandra from 8.7 to 1.0, then to 1.0.5 -> 1.0.6 -> 1.1.0 -> 1.1.1 -> 1.1.2 -> 1.1.0 -> 1.1.3 -> 1.1.4.
","22/Aug/12 15:16;xedin;aha, so check system.schema_* CFs column's timestamp values I bet they are from the future. The simplest fix for you would be to delete system/schema_* SSTables and re-create a schema after restart.","22/Aug/12 15:20;jbellis;bq. check system.schema_* CFs column's timestamp values I bet they are from the future

Can we fix that on startup for other upgraders?","22/Aug/12 15:23;xedin;Yes, I was about too :)",23/Aug/12 23:40;arya;+1 I just ended up seeing this problem in our cluster which was upgraded from 1.1.2 to 1.1.3. Will probably have to find a workaround since I have to change schema now.,"27/Aug/12 18:17;arya;I took my cluster down for 10 minutes. I took a snapshot of 'show schema' into a text file and removed system KS from it so I will only have my KS schema definition in it. Then I stop cassandra on all nodes. On each node, I removed system/schema_* folders from system's keyspace folder in cassandra data dir. I started all cassandra nodes. When I tried to reload the schema file using cli to recreate my CFs, I kept getting the message that CFs already exist. When I listed schema_columnfamilies in one of the node, I saws the same long timestamps like 2705487066780774 on columns of that CF. So, the procedure didn't quiet work out for me. What could have gone wrong here?

Please advice.",27/Aug/12 18:35;xedin;What cassandra version are you running? Did you try to recreate a schema on all of the nodes or just one of them?,"27/Aug/12 21:07;arya;I am running 1.1.3 which was upgraded from 1.1.2. When nodes came up, I created the schemas in one node only.","27/Aug/12 21:08;arya;As part of my process, I also removed saved_caches/system-*.",27/Aug/12 21:14;xedin;if you still have such timestamps and messages that CF already exists means that schema wasn't empty on restart and was reloaded from one of the nodes. I'm currently working on the patch which would fix timestamp situation on node's start up.,27/Aug/12 21:44;arya;Do you have an ETA? I'll be happy to monkey apply it on my cluster and be the guinea pig for it.,27/Aug/12 23:40;xedin;Not yet but I'm working on it and soon as it's ready I will attach it here.,04/Sep/12 19:38;jbellis;Don't you want to call cfs.truncate().get() to block for the truncate to finish?  Rest LGTM.,"04/Sep/12 19:59;xedin;good point, I would add that and commit, thanks!",04/Sep/12 21:17;xedin;Committed.,"04/Sep/12 22:25;hudson;Integrated in Cassandra #2035 (See [https://builds.apache.org/job/Cassandra/2035/])
    change SYSTEM_TABLE to SYSTEM_KS related to CASSANDRA-4561 merge (Revision cf23bd0a0fd192d991e971bcae94c2447126f873)

     Result = FAILURE
xedin : 
Files : 
* src/java/org/apache/cassandra/db/DefsTable.java
","07/Sep/12 02:49;awinter;After applying this patch (1.1.5-tentative) I see the future timestamps being detected and fixed:

{code}
INFO [main] 2012-09-07 02:24:00,090 DefsTable.java (line 202) Fixing timestamps of schema ColumnFamily schema_keyspaces...
INFO [main] 2012-09-07 02:24:00,168 DefsTable.java (line 202) Fixing timestamps of schema ColumnFamily schema_columnfamilies...
{code}

If I list schema_keyspaces I still see the old (future) timestamps.

Given the timestamps weren't actually updated I did go back and found the following error shortly after the restarted node joined the ring (this may or not be related):

{code}ERROR [InternalResponseStage:1] 2012-09-07 02:24:16,555 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[InternalResponseStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:468)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:346)
        at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:324)
        at org.apache.cassandra.service.MigrationManager$MigrationTask$1.response(MigrationManager.java:416)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:45)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
{code}

I have however assumed that the patch automatically fixes this across all servers in a multi dc ring once this patch is applied.  If there is a specific series of steps required to fix this, such as the full ring outage, deletion of system/schema_* etc can it please be documented here, the wiki or in the NEWS.txt file under upgrading notes?","07/Sep/12 09:01;xedin;Yes it does, but it assumes that you have schema in agreement across the nodes before it tries to fix timestamps. I can put that into NEWS.txt","07/Sep/12 09:09;awinter;My schema is in agreement, but the timestamps aren't fixed, even though the log suggests otherwise (is that previously mentioned NPE related?).  

{code}
[default@unknown] describe cluster;
Cluster Information:
   Snitch: org.apache.cassandra.locator.PropertyFileSnitch
   Partitioner: org.apache.cassandra.dht.RandomPartitioner
   Schema versions: 
        89b22434-5e34-381d-83d1-2a3cde1482fe: [x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x]

[default@unknown]
{code}

Schema updates continue to silently fail.","07/Sep/12 09:23;xedin;Interesting, why does it try to merge remote schema if all of your nodes are in agreement, can you run in debug mode as attach the log? The thing is timestamps are fixed long before the storage server is initialized and the process involves truncate of the system.schema_* CFs so I don't see any other reason why your schema still has old timestamps except some other node sent it migration request... 

NPE that you experience is also interesting, cfMetaData() is always not null as CFs are copied from one map to another on KS initialization, and KS itself could not be null or you would have the assertion error instead of NPE.",07/Sep/12 10:36;slebresne;Maybe we should reopen this if that didn't fully fixed the problem? I'd better hold on a bit on 1.1.5 if this is not fixed yet as this seem to hit quite a few people.,07/Sep/12 11:10;xedin;We are not yet sure what is the problem so I think we should hold on reopening for a bit.,"07/Sep/12 11:19;xedin;I just thought that a good addition to existing patch could be change in ColumnSerializer.deserialize to fix timestamps from the future, that way migrations from the remote locations would be deserialized with correct timestamp even if they were sent with the wrong one...","07/Sep/12 11:21;slebresne;Well, Anton still sees timestamp in the future and is still unable to do schema updates, it does sound a lot like there is a problem and it's related to this ticket. I just meant that I prefer keeping that in mind before releasing 1.1.5 into the wild too quickly.","07/Sep/12 11:27;xedin;I understand, but we don't know if that is caused by the fix not working or by something else as exception indicates that something was send to the node, this could be a separate issue. I will work on the ColumnSerializer change I mentioned and we'll see if it helps.  ",07/Sep/12 11:35;awinter;Debug log sent to [~xedin] privately.,"07/Sep/12 11:45;xedin;adding a patch that fixes column timestamp on deserialization. Anton, can you please apply and see if that fixes the problem?","07/Sep/12 11:59;xedin;As I see from the log - timestamps were actually fixed to ""7 september 2012 cl 10:58 GMT"" but then overriten by remote migration, latest patch should help with that.",07/Sep/12 12:08;zenek_kraweznik0;When cassandra 1.1.5 will be available?,"07/Sep/12 13:16;awinter;I've patched & deployed to a couple of nodes where I now see the corrected timestamps.  The NPE appears to also be gone.

It will take me some time to deploy to the entire ring but it looks promising so far.",07/Sep/12 14:42;awinter;Ring upgraded with the second patch and I am now able to perform schema updates.  Thanks!,07/Sep/12 15:52;jbellis;+1 lgtm,07/Sep/12 16:08;xedin;Committed to cassandra-1.1 branch.,"11/Sep/12 00:32;hudson;Integrated in Cassandra #2084 (See [https://builds.apache.org/job/Cassandra/2084/])
    change ColumnSerializer.deserialize to be able fix timestamps from future, related to CASSANDRA-4561 (Revision 2c69e2ea757be9492a095aa22b5d51234c4b4102)
change ColumnSerializer.deserialize to be able fix timestamps from future, related to CASSANDRA-4561 (Revision 429fa7a80e22757a55c03e99c27c157824a666af)

     Result = ABORTED
xedin : 
Files : 
* src/java/org/apache/cassandra/db/ColumnSerializer.java

xedin : 
Files : 
* src/java/org/apache/cassandra/db/ColumnSerializer.java
",,,,,,,,,,,,,,,,,,
Error in non-upgraded node's log when upgrading another node to trunk,CASSANDRA-4576,12604898,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,24/Aug/12 18:17,12/Mar/19 14:17,13/Mar/19 22:27,18/Sep/12 22:25,1.2.0,,,,,0,,,,,,,"I'm upgrading a 2-node cluster from cassandra-1.1 to trunk. On node1 I flush, stop the node, upgrade it to trunk, and start it. The following error gets written once a second in the log for node2:

{code}
ERROR [GossipStage:10] 2012-08-24 11:03:36,293 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[GossipStage:10,5,main]
java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:89)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.UnknownHostException: addr is of illegal length
	at java.net.InetAddress.getByAddress(InetAddress.java:935)
	at java.net.InetAddress.getByAddress(InetAddress.java:1318)
	at org.apache.cassandra.net.CompactEndpointSerializationHelper.deserialize(CompactEndpointSerializationHelper.java:39)
	at org.apache.cassandra.gms.EndpointStatesSerializationHelper.deserialize(GossipDigestSynMessage.java:117)
	at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:83)
	at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:70)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:60)
	... 4 more
{code}


Here is the exact code I ran to produce the error:
{code}
from dtest import Tester, debug

FROM_VERSION = 'git:cassandra-1.1'
TO_VERSION = 'git:trunk'

class TestUpgradeOneNode(Tester):

    def upgrade_test(self):
        # Start a cluster
        cluster = self.cluster
        cluster.partitioner = 'org.apache.cassandra.dht.RandomPartitioner'
        cluster.set_cassandra_dir(cassandra_version=FROM_VERSION)
        cluster.populate(2).start()
        (node1, node2) = cluster.nodelist()
        node1.watch_log_for('Listening for thrift clients...')
        node2.watch_log_for('Listening for thrift clients...')

        # Bring one node down and upgrade it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        node1.set_cassandra_dir(cassandra_version=TO_VERSION)
        node1.start(wait_other_notice=True)
        import pdb; pdb.set_trace()  # <-- pause here and tail -f the node2.logfilename()
{code}","ubuntu, ccm cluster with dtests",,,,,,,,,,,,,,,,,,18/Sep/12 15:35;xedin;CASSANDRA-4576.patch;https://issues.apache.org/jira/secure/attachment/12545576/CASSANDRA-4576.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-24 20:58:21.96,,,no_permission,,,,,,,,,,,,256261,,,Tue Sep 18 22:25:44 UTC 2012,,,,,,0|i0gxb3:,96826,brandon.williams,brandon.williams,,,,,,,,,,"24/Aug/12 20:58;jbellis;Brandon says, ""Something is wrong w/ the upgrade of the system KS, not gossip.""",18/Sep/12 15:35;xedin;Actually it's a problem with Gossiper :),"18/Sep/12 16:47;brandon.williams;Can you explain what happened here?  I'm confused by the version 11 change, given that trunk is version 12.","18/Sep/12 18:48;xedin;Sure, GossipDigestAckSerializer always expects boolean on deserialization, that would only changed in 1.2 which doesn't write that boolean.","18/Sep/12 22:19;brandon.williams;I see, thanks. +1",18/Sep/12 22:25;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
select statement with indexed column causes node to OOM,CASSANDRA-4555,12603936,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,dr-alves,dr-alves,18/Aug/12 01:02,12/Mar/19 14:17,13/Mar/19 22:27,18/Aug/12 04:49,1.2.0 beta 1,,,,,0,,,,,,,"After creating a keyspace, table and index on a clean ccm 3 node cluster based on trunk, when a select statement with an index expression is executed in cqlsh one of the nodes OOM's and goes down.

The steps to reproduce the problem are:

create a 3 node cluster from trunk (I used ccm)

execute the following statements in cqlsh:

{noformat}
CREATE KEYSPACE trace WITH strategy_class = 'SimpleStrategy'
  AND strategy_options:replication_factor = '1';

CREATE TABLE trace.trace_events(sessionId  timeuuid,
  coordinator       inet,
  eventId           timeuuid,
  description       text,
  duration          bigint,
  happened_at       timestamp,
  name              text,
  payload_types     map<text, text>,
  payload           map<text, blob>,
  source            inet,
  type              text,
  PRIMARY KEY (sessionId, coordinator, eventId));

CREATE INDEX idx_name ON trace.trace_events (name);
{noformat}

Executing the following statement causes node2 to OOM:

select * from trace_events where name = 'batch_mutate';

In my case node2 goes down with:

{noformat}
ERROR [Thread-9] 2012-08-17 19:42:55,741 CassandraDaemon.java (line 131) Exception in thread Thread[Thread-9,5,main]
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.dht.Token$TokenSerializer.deserialize(Token.java:97)
	at org.apache.cassandra.dht.AbstractBounds$AbstractBoundsSerializer.deserialize(AbstractBounds.java:161)
	at org.apache.cassandra.db.RangeSliceCommandSerializer.deserialize(RangeSliceCommand.java:299)
	at org.apache.cassandra.db.RangeSliceCommandSerializer.deserialize(RangeSliceCommand.java:181)
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:94)
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:181)
	at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:69)
 INFO [StorageServiceShutdownHook] 2012-08-17 19:42:55,746 ThriftServer.java (line 221) Stop listening to thrift clients
 INFO [StorageServiceShutdownHook] 2012-08-17 19:42:55,748 Gossiper.java (line 1054) Announcing shutdown
 INFO [StorageServiceShutdownHook] 2012-08-17 19:42:56,749 MessagingService.java (line 657) Waiting for messaging service to quiesce
 INFO [ACCEPT-/127.0.0.2] 2012-08-17 19:42:56,751 MessagingService.java (line 849) MessagingService shutting down server thread.
{noformat}
",MAC OSx,,,,,,,,,,,,,,,,,,18/Aug/12 04:16;jbellis;4555.txt;https://issues.apache.org/jira/secure/attachment/12541468/4555.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-18 03:33:42.682,,,no_permission,,,,,,,,,,,,256241,,,Sat Aug 18 04:49:55 UTC 2012,,,,,,0|i0gx2f:,96787,dr-alves,dr-alves,,,,,,,,,,18/Aug/12 03:33;jbellis;It OOMs with no data in it at all?,18/Aug/12 03:55;dr-alves;With and without data. It OOMs because it tries to allocate a huge byte[] on TokenSerializer.deserialize(). ,"18/Aug/12 04:16;jbellis;patch to make de/serializers agree to use a short length for index expression value, and to add validation that client-provided expressions do conform to this limitation",18/Aug/12 04:30;dr-alves;+1 applies cleanly and solves the problem.,18/Aug/12 04:49;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh COPY TO and COPY FROM don't work with cql3,CASSANDRA-4674,12607992,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,18/Sep/12 01:06,12/Mar/19 14:17,13/Mar/19 22:27,15/Oct/12 17:33,1.2.0 beta 2,,,,,0,cqlsh,,,,,,cqlsh COPY TO and COPY FROM don't work with cql3 due to previous cql3 changes.,,,,,,,,,,,,,,,,,,,15/Oct/12 16:50;iamaleksey;CASSANDRA-4674.txt;https://issues.apache.org/jira/secure/attachment/12549164/CASSANDRA-4674.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-15 10:47:16.687,,,no_permission,,,,,,,,,,,,248677,,,Mon Oct 15 17:33:13 UTC 2012,,,,,,0|i09xpr:,55903,brandon.williams,brandon.williams,,,,,,,,,,18/Sep/12 13:07;iamaleksey;Mostly irrelevant now.,"15/Oct/12 10:47;marco.matarazzo;Well, We would love to have it fixed.","15/Oct/12 15:28;iamaleksey;[~marco.matarazzo] It's a new issue, caused by a different reason. Affects cql3 in trunk, not cql2.
I'm looking into it.",15/Oct/12 16:03;iamaleksey;Though COPY TO and COPY FROM still work in CQL3 as long as you don't provide any optional parameters to it.,"15/Oct/12 16:16;marco.matarazzo;I realized I hijacked another ticket thinking that mine was a well-known problem, when it's obvious it's not. I'm very sorry about that.

I have some specific column families on which COPY FROM does not work from cqlsh -3, and works perfectly with cqlsh -2. 

If it's ok with you, I'll hunt down the condition that makes it not work and, as soon as I have them pinned down, I will open a new bug.

Again, I'm sorry about that.
","15/Oct/12 16:25;iamaleksey;Well, you've accidentally uncovered a COPY TO/COPY FROM bug, so there is nothing to apologize for. Thank you (:
Turns out COPY TO/COPY FROM has had broken completion for a really long time, which in combination with a recent commit (2f979ed60fc4f9dab2db7ce9921ff2953acd714c for CASSANDRA-4488) broke COPY TO/COPY FROM with non-default parameters.",15/Oct/12 17:33;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql error with ORDER BY when using IN,CASSANDRA-4612,12606123,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,04/Sep/12 18:20,12/Mar/19 14:17,13/Mar/19 22:27,07/Sep/12 16:01,1.1.5,,,,,0,,,,,,,"{code}
            CREATE TABLE test(
                my_id varchar, 
                col1 int, 
                value varchar, 
                PRIMARY KEY (my_id, col1)
            );

INSERT INTO test(my_id, col1, value) VALUES ( 'key1', 1, 'a');
INSERT INTO test(my_id, col1, value) VALUES ( 'key2', 3, 'c');
INSERT INTO test(my_id, col1, value) VALUES ( 'key3', 2, 'b');
INSERT INTO test(my_id, col1, value) VALUES ( 'key4', 4, 'd');
SELECT col1 FROM test WHERE my_id in('key1', 'key2', 'key3') ORDER BY col1;
{code}

The following error results: TSocket read 0 bytes
The log gives a traceback:
{code}
ERROR [Thrift:8] 2012-09-04 12:02:15,894 CustomTThreadPoolServer.java (line 202) Error occurred during processing of message.
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1356)
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1343)
	at java.util.Arrays.mergeSort(Arrays.java:1270)
	at java.util.Arrays.sort(Arrays.java:1210)
	at java.util.Collections.sort(Collections.java:159)
	at org.apache.cassandra.cql3.statements.SelectStatement.orderResults(SelectStatement.java:821)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:793)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:136)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:118)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:107)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:115)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1521)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3618)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3606)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}","ubuntu, cassandra trunk (commit 769fe895a36868c47101f681f5fdd721bee1ad62 )",,,,,,,,,,,,,,,,,,06/Sep/12 20:55;xedin;CASSANDRA-4612-v2.patch;https://issues.apache.org/jira/secure/attachment/12544107/CASSANDRA-4612-v2.patch,06/Sep/12 15:15;xedin;CASSANDRA-4612.patch;https://issues.apache.org/jira/secure/attachment/12544056/CASSANDRA-4612.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-06 11:28:44.11,,,no_permission,,,,,,,,,,,,244027,,,Fri Sep 07 16:01:38 UTC 2012,,,,,,0|i05ajr:,28806,slebresne,slebresne,,,,,,,,,,06/Sep/12 11:28;xedin;fixed problem related to determining position when single 'order by' condition is used and added a check to prevent users from ordering by columns which are not included in the select clause.,06/Sep/12 14:32;slebresne;The fix for the case where there is only one ordered column lgtm but this doesn't handle the CompositeComparator case.,"06/Sep/12 14:34;slebresne;Also, I believe this affect 1.1 as well, so I would suggest fixing it there.",06/Sep/12 15:15;xedin;ported to 1.1 and added composite support.,"06/Sep/12 17:16;slebresne;I believe that removing the ""Order by currently only support the ordering of columns following their declared order in the PRIMARY KEY"" is buggy because it still doesn't work in the case where the key restriction is an EQ (i.e. not an IN). That is, we can remove it but in that case we must change the condition at the top of orderResults() so that we go through the 'CompositeComparator' path if the keyRestriction is an EQ and the requested orderings are not in the same order that in the PK definition. Not a bad idea though.

Some minor nits:
* We can provide the size when allocating the typesWithPositions list. I would have split typesWithPositions in two lists as this reduce the number of allocation needed (because you avoid all the Pair but also because you can use int[] for the positions) and in that case wouldn't really complicate the code anyway. 
* In getColumnPositionInSelect, we could use a by-index for loop and directly return the current index when we find the name. That way we can throw an assertion if we didn't find the name at all since that shouldn't happen anyway (tiny bonus: iteration by index on ArrayList is slightly cheaper as it avoids the iterator allocation).
","06/Sep/12 18:17;xedin;Looks like its's just not my day today, I will fix keyRestriction thing and nits asap.",06/Sep/12 20:55;xedin;v2 with nits fixed and order restriction reverted to original state after discussion with Jonathan.,"07/Sep/12 10:04;slebresne;lgtm, +1",07/Sep/12 16:01;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in StorageProxy.getRestrictedRange,CASSANDRA-4621,12606245,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,pyritschard,pyritschard,05/Sep/12 13:35,12/Mar/19 14:17,13/Mar/19 22:27,18/Sep/12 14:42,1.2.0 beta 1,,,,,0,,,,,,,"On a freshly built cassandra from trunk, I can create a column family with a composite row key using the syntax:

for instance a standard eventlog CF:

     CREATE TABLE events (
       facility text,
       prio int,
       message text,
       PRIMARY KEY ( (facility, prio) )
     );

A simple query will then generate exceptions:

SELECT * FROM events; will yield:

ERROR 15:33:40,383 Exception in thread Thread[Thrift:1,5,main]
java.lang.AssertionError: [min(0),max(-8021625467324731134)]
	at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:41)
	at org.apache.cassandra.dht.Bounds.split(Bounds.java:59)
	at org.apache.cassandra.service.StorageProxy.getRestrictedRanges(StorageProxy.java:1073)
	at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:879)
	at org.apache.cassandra.cql3.statements.SelectStatement.multiRangeSlice(SelectStatement.java:209)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:128)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:118)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:107)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:115)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1521)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3618)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3606)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

","archlinux, openjdk7",,,,,,,,,,,,,,,,,,18/Sep/12 09:28;slebresne;4621-followup.txt;https://issues.apache.org/jira/secure/attachment/12545544/4621-followup.txt,11/Sep/12 09:00;slebresne;4621.txt;https://issues.apache.org/jira/secure/attachment/12544611/4621.txt,10/Sep/12 18:05;slebresne;4621.txt;https://issues.apache.org/jira/secure/attachment/12544496/4621.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-09-10 18:05:15.667,,,no_permission,,,,,,,,,,,,256292,,,Tue Sep 18 14:42:39 UTC 2012,,,,,,0|i0gxrr:,96901,jbellis,jbellis,,,,,,,,,,"10/Sep/12 18:05;slebresne;Pretty sure this has nothing to do with CQL. But the trace shows a token that is negative, which should not happen. So my guess is that it's because the new Murmur3Partitioner can generate negative token while it shouldn't. Patch attached to fix (there was 2 places where a negative could be generated, one was because the special case of Long.MIN_VALUE wasn't handled, the other was that we weren't taking the absolute value at all for random token. Both should be fixed though this is probably the latter that produced the error here since it's way more likely to happen).",10/Sep/12 18:10;jbellis;What if we just made the minimum token for M3P Long.MIN_VALUE instead?,"10/Sep/12 18:17;slebresne;A priori, I don't think that would be a problem (and if it's fine we should). We could also avoid using BigInteger to generate the middle of two longs :)",10/Sep/12 18:18;slebresne;(I'll have a shot at both),"10/Sep/12 18:37;slebresne;I also think that it's a problem that Murmur3Partitioner has a MINIMUM token that is a ""valid"" token (one that correspond to actual keys). Indeed (min, X] is suppose to include everything smaller than token X, but if min is a actual token, it is not contained in (min, X]. ",10/Sep/12 18:38;jbellis;Hmm.  Forgot that we changed MINIMUM to < ZERO for RP.,"11/Sep/12 09:00;slebresne;Attaching patch that:
* Don't force positive tokens in Murmur3Partitioner (thus using all the bits of the LongToken).
* Exclude Long.MIN_VALUE from the possible tokens of a key (since it's now the MINIMUM token).

I note that this means tokens can be negative and that we'll have to update a number of documentation relating to computing tokens. Though this was already the case before this patch (but maybe to a slightly lesser extend). And hopefully vnodes will make (manual) token computations a thing of the past.",11/Sep/12 15:26;jbellis;LGTM.,"11/Sep/12 15:44;slebresne;Committed, thanks","18/Sep/12 09:28;slebresne;Reopening this since the midpoint calculation for murmur3Partitioner is actually broken by having negative tokens (concretely, Murmur3PartitionerTest is failing). Attaching patch to fix it.",18/Sep/12 14:42;jbellis;committed w/ additional comments,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support inet data type,CASSANDRA-4627,12606532,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thepaul,thepaul,thepaul,07/Sep/12 05:12,12/Mar/19 14:17,13/Mar/19 22:27,13/Sep/12 19:23,1.2.0 beta 1,,Legacy/Tools,,,0,cql,cqlsh,,,,,"CASSANDRA-4018 introduced a new cassandra data type with a cql name ""inet"", which is not yet supported in cqlsh. Add support for decoding and formatting.",,,,,,,,,,,,,,,,,,,10/Sep/12 09:34;slebresne;4627.txt;https://issues.apache.org/jira/secure/attachment/12544445/4627.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-10 09:34:29.484,,,no_permission,,,,,,,,,,,,256297,,,Thu Sep 13 19:23:01 UTC 2012,,,,,,0|i0gxu7:,96912,brandon.williams,brandon.williams,,,,,,,,,,09/Sep/12 04:16;thepaul;Turns out this only required an update to the python-cql driver. Once a new release is out I'll update the bundled version in lib/.,"10/Sep/12 09:34;slebresne;Since this ticket was open with a generic enough title, attaching a trivial patch that adds ""support"" in a few places where it was missing, namely the binary protocol and the CQL documentation.","10/Sep/12 15:58;thepaul;Oops, I intended this to be just for adding the support to cqlsh. But your patch does add useful support, so I'm in favor of including that as well. I just ask that we not commit it or close the ticket until I can add in the driver update.","11/Sep/12 10:12;slebresne;I know that was for cqlsh :)

Anyway, I've committed the patch but leaving that open until the driver is updated","12/Sep/12 21:01;thepaul;My additional changes committed into the 4627 branch in my github clone:

http://github.com/thepaul/cassandra/tree/4627

Notice that this includes an update of the internal CQL lib, so there's a zipfile to add and one to remove.",13/Sep/12 19:23;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lost+found directory in the data dir causes problems again,CASSANDRA-4572,12604655,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,23/Aug/12 18:08,12/Mar/19 14:17,13/Mar/19 22:27,30/Aug/12 19:02,1.1.5,,,,,0,,,,,,,"Looks like we've regressed from CASSANDRA-1547 and mounting a fs directly on the data dir is a problem again.

{noformat}
INFO [main] 2012-08-22 23:30:03,710 Directories.java (line 475) Upgrade from pre-1.1 version detected: migrating sstables to new directory layout ERROR [main] 2012-08-22 23:30:03,712 AbstractCassandraDaemon.java (line 370) Exception encountered during startup 
                java.lang.NullPointerException         at org.apache.cassandra.db.Directories.migrateSSTables(Directories.java:487)
{noformat}",,,,,,,,,,,,,,,,,,,29/Aug/12 16:20;yukim;4572-1.1.txt;https://issues.apache.org/jira/secure/attachment/12542935/4572-1.1.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-29 16:20:38.408,,,no_permission,,,,,,,,,,,,256257,,,Thu Aug 30 19:02:06 UTC 2012,,,,,,0|i0gx9b:,96818,jbellis,jbellis,,,,,,,,,,"29/Aug/12 16:20;yukim;When you do File#listFiles on lost+found directory, it returns null. I believe there are other cases that it returns null, so attached patch just checks null after File#listFiles is performed.",30/Aug/12 17:44;jbellis;+1,30/Aug/12 19:02;yukim;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Impossible to set LeveledCompactionStrategy to a column family.,CASSANDRA-4597,12605651,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jal06,jal06,31/Aug/12 07:51,12/Mar/19 14:17,13/Mar/19 22:27,04/Sep/12 11:40,1.1.5,,,,,0,,,,,,,"CFPropDefs.applyToCFMetadata() does not set the compaction class on CFM

When altering the compaction strategy of a column family to LeveledCompactionStrategy, the compaction strategy is not changed (the describe command shows that the SizeTieredCompactionStrategy is still set to the CF)
When creating a column family WITH compaction_strategy_class='LeveledCompactionStrategy', the compaction strategy class used is  SizeTieredCompactionStrategy

Ex : 
jal@jal-VirtualBox:~/cassandra/apache-cassandra-1.1.1/bin$ ./cqlsh -3
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.1 | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> use test1;
cqlsh:test1> describe table pns_credentials;

CREATE TABLE pns_credentials (
  ise text PRIMARY KEY,
  isnew int,
  ts timestamp,
  mergestatus int,
  infranetaccount text,
  user_level int,
  msisdn bigint,
  mergeusertype int
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

I want to set the LeveledCompaction strategy for this table, so I execute the following ALTER TABLE :

cqlsh:test1> alter table pns_credentials 
         ... WITH compaction_strategy_class='LeveledCompactionStrategy'
         ... AND compaction_strategy_options:sstable_size_in_mb=10;

In Cassandra logs, I see some informations :
 INFO 10:23:52,532 Enqueuing flush of Memtable-schema_columnfamilies@965212657(1391/1738 serialized/live bytes, 20 ops)
 INFO 10:23:52,533 Writing Memtable-schema_columnfamilies@965212657(1391/1738 serialized/live bytes, 20 ops)
 INFO 10:23:52,629 Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-94-Data.db (1442 bytes) for commitlog position ReplayPosition(segmentId=3556583843054, position=1987)


However, when I look at the description of the table, the table is still with the SizeTieredCompactionStrategy
cqlsh:test1> describe table pns_credentials ;

CREATE TABLE pns_credentials (
  ise text PRIMARY KEY,
  isnew int,
  ts timestamp,
  mergestatus int,
  infranetaccount text,
  user_level int,
  msisdn bigint,
  mergeusertype int
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';
 
In the schema_columnfamilies table (in system keyspace), the table pns_credentials is still using the SizeTieredCompactionStrategy
cqlsh:test1> use system;
cqlsh:system> select * from schema_columnfamilies ;
...
         test1 |   pns_credentials |                   null | KEYS_ONLY |                        [] |         | org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy |                          {} |                                                                                                                                                                           org.apache.cassandra.db.marshal.UTF8Type | {""sstable_compression"":""org.apache.cassandra.io.compress.SnappyCompressor""} |          org.apache.cassandra.db.marshal.UTF8Type |           864000 | 1029 |       ise |     org.apache.cassandra.db.marshal.UTF8Type |                        0 |                       32 |                        4 |                0.1 |               True |          null | Standard |        null
... 


Same behaviour using cqlsh or command-cli.
","Ubuntu 12.04
cqlsh 2.2.0 | Cassandra 1.1.1 | CQL spec 3.0.0 | Thrift protocol 19.32.0
Cluster with only 1 node",,,,,,,,,,,,,,,,,,03/Sep/12 09:20;xedin;CASSANDRA-4597.patch;https://issues.apache.org/jira/secure/attachment/12543524/CASSANDRA-4597.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-31 12:16:10.719,,,no_permission,,,,,,,,,,,,256277,,,Tue Nov 27 07:44:24 UTC 2012,,,,,,0|i0gxin:,96860,jbellis,jbellis,,,,,,,,,,31/Aug/12 12:16;jbellis;Several schema bugs have been fixed since 1.1.1.  Let us know if you can reproduce in 1.1.4.  You may need to recreate the schema because 1.1.1 used an incorrectly-high timestamp on the original creation.,"02/Sep/12 08:23;jal06;I tried to alter the table with the folowing CQL command :
alter table pns_credentials6
WITH compaction_strategy_class='LeveledCompactionStrategy'
AND compaction_strategy_options:sstable_size_in_mb=15;

Using cassandra 1.1.1 : it doesn't work (neither compaction_strategy_class nor compaction_strategy_options is modified)
Using cassandra 1.1.2 : same as 1.1.1
Using cassandra 1.1.3 : neither compaction_strategy_class is not modified, but compaction_strategy_options is modified)
Using cassandra 1.1.4 : same as 1.1.3


Below are the details with cassandra 1.1.4 :
At first, I create a table pns_credentials6 (with compaction_strategy_class='SizeTieredCompactionStrategy')
Then, I describe the table
Then, I alter the table
Then, I describe again the table :  the line compaction_strategy_options:sstable_size_in_mb='15' (since 1.1.3) appears but compaction_strategy_class is still 'SizeTieredCompactionStrategy'

Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.4 | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> use test1;
cqlsh:test1> CREATE TABLE pns_credentials6 (
         ...    ise text PRIMARY KEY,
         ...    isnew int,
         ...    ts timestamp,
         ...    mergestatus int,
         ...    infranetaccount text,
         ...    user_level int,
         ...    msisdn bigint,
         ...    mergeusertype int
         ...  ) WITH
         ...    comment='' AND
         ...    read_repair_chance=0.100000 AND
         ...   gc_grace_seconds=864000 AND
         ...    replicate_on_write='true' AND
         ...    compaction_strategy_class='SizeTieredCompactionStrategy' AND
         ...    compression_parameters:sstable_compression='SnappyCompressor';
cqlsh:test1> describe table pns_credentials6;

CREATE TABLE pns_credentials6 (
  ise text PRIMARY KEY,
  infranetaccount text,
  isnew int,
  mergestatus int,
  mergeusertype int,
  msisdn bigint,
  ts timestamp,
  user_level int
) WITH
  comment='' AND
  caching='KEYS_ONLY' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

cqlsh:test1> alter table pns_credentials6
         ... WITH compaction_strategy_class='LeveledCompactionStrategy'
         ... AND compaction_strategy_options:sstable_size_in_mb=15;
cqlsh:test1> describe table pns_credentials6;

CREATE TABLE pns_credentials6 (
  ise text PRIMARY KEY,
  infranetaccount text,
  isnew int,
  mergestatus int,
  mergeusertype int,
  msisdn bigint,
  ts timestamp,
  user_level int
) WITH
  comment='' AND
  caching='KEYS_ONLY' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='15' AND
  compression_parameters:sstable_compression='SnappyCompressor';

As you can see, I did the check with a new table (pns_credentials6), created in 1.1.4.
When you say that I need to recreate the schema, is it what you were expecting ?",02/Sep/12 14:05;jal06;Still happening in 1.1.4,"03/Sep/12 01:51;jbellis;Thanks, we'll have a second look at that.","03/Sep/12 08:53;xedin;I have noticed and included a fix for this in patch for CASSANDRA-4497 for 1.2.0, will add patch for 1.1 here soon.",03/Sep/12 11:59;jbellis;+1,04/Sep/12 11:40;xedin;Committed.,"26/Nov/12 12:52;shamim_ru;still happening in 1.1.5

Connected to SMEVCLUSTER at 192.168.157.94:9160.
[cqlsh 2.2.0 | Cassandra 1.1.5 | CQL spec 2.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh > describe columnfamily auditlog_01;
CREATE TABLE auditlog_01 (
  lid text PRIMARY KEY,
  dscn text,
  asid text,
  soapa text,
  sysn text,
  msgs double,
  leid bigint,
  prc text,
  aeid bigint,
  adt timestamp,
  name text,
  asn text,
  msg text,
  msgid text,
  msgt text
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='5' AND
  compression_parameters:sstable_compression='SnappyCompressor';
cqlsh> alter table auditlog_01 with compaction_strategy_class='LeveledCompactionStrategy' AND compaction_strategy_options:sstable_size_in_mb=5;
cqlsh> describe columnfamily auditlog_01;
CREATE TABLE auditlog_01 (
  lid text PRIMARY KEY,
  dscn text,
  asid text,
  soapa text,
  sysn text,
  msgs double,
  leid bigint,
  prc text,
  aeid bigint,
  adt timestamp,
  name text,
  asn text,
  msg text,
  msgid text,
  msgt text
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='5' AND
  compression_parameters:sstable_compression='SnappyCompressor';
","26/Nov/12 17:42;jeromatron;Shamim,

Your case may be a product of CASSANDRA-4965.  Can you try to update the column family metadata in the cassandra-cli such as: 
{code}update column family auditlog_01 with compaction_strategy=LeveledCompactionStrategy;{code}

See if that works for you.","27/Nov/12 07:44;shamim_ru;Thank'x Jeremy,
  yes it's works through cli. It's bug on cql 2.* which defined on issue CASSANDRA-4965",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy slow-down and memory leak,CASSANDRA-4708,12608822,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,danielnorberg,danielnorberg,danielnorberg,24/Sep/12 07:09,12/Mar/19 14:17,13/Mar/19 22:27,24/Sep/12 15:11,1.0.12,1.1.6,,,,0,,,,,,,"I am consistently observing slow-downs in StorageProxy caused by the NonBlockingHashMap used indirectly by MessagingService via the callbacks ExpiringMap.

This seems do be due to NBHM having unbounded memory usage in the face of workloads with high key churn. As monotonically increasing integers are used as callback id's by MessagingService, the backing NBHM eventually ends up growing the backing store unboundedly. This causes it to also do very large and expensive backing store reallocation and migrations, causing throughput to drop to tens of operations per second, lasting seconds or even minutes. 

This behavior is especially noticable for high throughput workloads where the dataset is completely in ram and I'm doing up to a hundred thousand reads per second.

Replacing NBHM in ExpiringMap with the java standard library ConcurrentHashMap resolved the issue and allowed me to keep a consistent high throughput.

An open issue on NBHM can be seen here: http://sourceforge.net/tracker/?func=detail&aid=3563980&group_id=194172&atid=948362",,,,,,,,,,,,,,,,,,,24/Sep/12 07:28;danielnorberg;0001-MessagingService-don-t-use-NBHM-in-ExpiringMap.patch;https://issues.apache.org/jira/secure/attachment/12546260/0001-MessagingService-don-t-use-NBHM-in-ExpiringMap.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-24 15:11:19.935,,,no_permission,,,,,,,,,,,,256348,,,Tue Sep 25 04:07:59 UTC 2012,,,,,,0|i0gynr:,97045,jbellis,jbellis,,,,,,,,,,24/Sep/12 07:28;danielnorberg;Attached patch with the proposed fix; replaces NBHM with CHM in ExpiringMap.,"24/Sep/12 15:11;jbellis;committed, thanks!

(also commented on NBHM SF issue.)","24/Sep/12 15:19;jbellis;did a quick audit of NBHM usage elsewhere.  think we're okay, everything else has a pretty tiny amount of possible keys.  the main ""large"" maps are in the column containers where NBHM is a non-candidate since we need sorting.",24/Sep/12 15:44;jbellis;also committed to 1.0 branch,"25/Sep/12 04:07;scode;Nice catch!

FWIW, ConcurrentSkipListMap should probably be considered to avoid locking (though I believe it's generally slower and it'll be less memory compact).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate column names with DynamicCompositeType,CASSANDRA-4711,12608897,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,thrykol,thrykol,24/Sep/12 16:35,12/Mar/19 14:17,13/Mar/19 22:27,29/Oct/12 10:14,1.1.7,,,,,0,,,,,,,"I have a column family whose comparator is DynamicCompositeType and validation is CounterColumnType.  During automated testing, there have been occasions where a counter column is created twice, throwing off the query results for the column.

Doing a 'get' via the cli, I see the following output for the row:

=> (counter=s@language:b@00000001:s@pt_BR, value=198)
=> (counter=s@language:s@possible, value=200)
=> (counter=s@language:b@00000001:s@pt_BR, value=0)

If I print out the byte value of the column names along with their MD5 sum I see:

Name: [language, java.nio.HeapByteBuffer[pos=0 lim=4 cap=4], pt_BR]
Byte array: ffffff8073086c616e67756167650ffffff80620400010ffffff80730570745f42520
MD5: 2db353a9a72a0d7cb6cb277ac5125653
Name: [language, possible]
Byte array: ffffff8073086c616e67756167650ffffff807308706f737369626c650
MD5: 82cad9b6a65c794e97cf1d4613e2e367
Name: [language, java.nio.HeapByteBuffer[pos=0 lim=4 cap=4], pt_BR]
Byte array: ffffff8073086c616e67756167650ffffff80620400010ffffff80730570745f42520
MD5: 2db353a9a72a0d7cb6cb277ac5125653

Unfortunately, I have been unable to duplicate this manually or via a generic test script and our QA department can only duplicate ~25% of the time.","CentOS 5.8 x86_64
Cassandra 1.1.5
Hector 1.1-2",,,,,,,,,,,,,,,,,,26/Oct/12 09:45;slebresne;4711.txt;https://issues.apache.org/jira/secure/attachment/12550944/4711.txt,25/Oct/12 20:47;thrykol;DuplicateColumnName.java;https://issues.apache.org/jira/secure/attachment/12550858/DuplicateColumnName.java,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-26 09:45:53.323,,,no_permission,,,,,,,,,,,,251121,,,Wed Nov 14 18:41:44 UTC 2012,,,,,,0|i0b4vr:,62903,jbellis,jbellis,,,,,,,,,,24/Sep/12 18:20;thrykol;Rough example of how counter column is being used.,"25/Oct/12 20:47;thrykol;Java class which demonstrates unexpected behavior with counter columns.

Sample output from the CLI is included in inline comment blocks.  Each time I ran the script, the column ordering changed slightly.",26/Oct/12 09:45;slebresne;This is a bug in the patch of CASSANDRA-3625. We are mistakenly using ReversedType with our FixedValueComparator but that doesn't correctly reverse the output. Attaching simple patch to fix.,26/Oct/12 09:47;slebresne;Note that the patch is against 1.1 but 1.0 would also be affected so we can commit there if we want (the patch probably apply there without modifications).,"26/Oct/12 10:02;slebresne;Oh, and the use of counters has nothing to do with anything so I've update the title to reflect that.","26/Oct/12 14:36;thrykol;Thanks for the quick response on this.  Do you have a time frame for when 1.1.7 is expected to be released?  I'm seeing this issue in a production environment and would like to get the fix in place as quickly as possible.  If the release won't be happening in the very near future, can you patch this into 1.1.6 as well.  While I can download the source and apply the patch locally, it'd be my preference not to.","26/Oct/12 18:38;jbellis;+1 on the fix.

The next release will be 1.1.7, probably in a couple weeks.  If you want a hotfix sooner than that, then rolling your own patched 1.1.6 is your best option.","29/Oct/12 10:14;slebresne;Committed, thanks.","14/Nov/12 18:41;thrykol;As and FYI, after patching 1.1.6 in my environment I've found that some of my unit tests failed (I'm assuming the same would be true for 1.1.7).  Going through the failing tests I found that the logic for my slice ranges had to be modified.  It may be good to put a warning along those lines in the change log.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log error when using IN together with ORDER BY,CASSANDRA-4689,12608254,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,19/Sep/12 14:57,12/Mar/19 14:17,13/Mar/19 22:27,20/Sep/12 13:59,1.1.6,,,,,0,,,,,,,"{code}
$ bin/cqlsh -3
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.5-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> use ks;
cqlsh:ks> drop TABLE test;
cqlsh:ks> CREATE TABLE test (my_id varchar, time_id uuid, value int, PRIMARY KEY (my_id, time_id));
cqlsh:ks> INSERT INTO test (my_id, time_id, value) VALUES ('key1', 1, 1);
cqlsh:ks> INSERT INTO test (my_id, time_id, value) VALUES ('key2', 2, 2);
cqlsh:ks> select * from test where my_id in('key1', 'key2') order by time_id;
TSocket read 0 bytes
{code}

The log shows this:
{code}
ERROR [Thrift:5] 2012-09-19 08:44:59,859 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.IllegalArgumentException: Column time_id wasn't found in select clause.
	at org.apache.cassandra.cql3.statements.SelectStatement.getColumnPositionInSelect(SelectStatement.java:866)
	at org.apache.cassandra.cql3.statements.SelectStatement.orderResults(SelectStatement.java:836)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:807)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:137)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1242)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

NOTE: This issue appears similar to https://issues.apache.org/jira/browse/CASSANDRA-4612 from the user perspective, even though 4612 was verified as fixed.","built from source on cassandra-1.1 (b43cc362aa568a46bc53e545c68518b0bd350b76)
os: ubuntu",,,,,,,,,,,,,,,,,,19/Sep/12 22:13;xedin;CASSANDRA-4689.patch;https://issues.apache.org/jira/secure/attachment/12545814/CASSANDRA-4689.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-19 17:16:34.196,,,no_permission,,,,,,,,,,,,256338,,,Thu Sep 20 13:59:07 UTC 2012,,,,,,0|i0gyhb:,97016,slebresne,slebresne,,,,,,,,,,"19/Sep/12 17:16;xedin;Jonathan, I can easily handle this one if Sylvain has a lot on his plate.",19/Sep/12 22:13;xedin;changes from CASSANDRA-4612 aren't covering extended selections properly.,20/Sep/12 11:05;slebresne;+1,20/Sep/12 13:59;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix scrubbing of CQL3 created tables,CASSANDRA-4685,12608233,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,19/Sep/12 13:00,12/Mar/19 14:17,13/Mar/19 22:27,26/Sep/12 12:11,1.2.0 beta 2,,,,,0,,,,,,,"{noformat}
 INFO 12:20:42,822 Scrubbing SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ia-1-Data.db')
 WARN 12:20:42,826 Non-fatal error reading row (stacktrace follows)
java.lang.RuntimeException: Error validating row DecoratedKey(61935297886570031978600740763604084078, 4b6579737061636531)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:244)
        at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:111)
        at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:95)
        at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:151)
        at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:157)
        at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:173)
        at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:495)
        at org.apache.cassandra.db.compaction.CompactionManager.doScrub(CompactionManager.java:484)
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:66)
        at org.apache.cassandra.db.compaction.CompactionManager$3.perform(CompactionManager.java:223)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:193)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.db.marshal.MarshalException: String didn't validate.
        at org.apache.cassandra.db.marshal.UTF8Type.validate(UTF8Type.java:65)
        at org.apache.cassandra.db.Column.validateFields(Column.java:287)
        at org.apache.cassandra.db.ColumnFamily.validateColumnFields(ColumnFamily.java:378)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:240)
        ... 15 more
 WARN 12:20:42,826 Row at 19 is unreadable; skipping to next
 WARN 12:20:42,827 No valid rows found while scrubbing SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ia-1-Data.db'); it is marked for deletion now. If you want to attempt manual recovery, you can find a copy in the pre-scrub snapshot
{noformat}",,,,,,,,,,,,,,,,,,,25/Sep/12 12:29;slebresne;4685.txt;https://issues.apache.org/jira/secure/attachment/12546502/4685.txt,24/Sep/12 20:40;jbellis;scrub-transparency.txt;https://issues.apache.org/jira/secure/attachment/12546374/scrub-transparency.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-24 20:40:53.466,,,no_permission,,,,,,,,,,,,256336,,,Wed Sep 26 12:11:11 UTC 2012,,,,,,0|i0gyfz:,97010,jbellis,jbellis,,,,,,,,,,"24/Sep/12 20:40;jbellis;attached patch demonstrates that it's trying to validate gossip_generation as utf8, when it's declared int.  bug in schema compiling?

{noformat}
Caused by: org.apache.cassandra.db.marshal.MarshalException: Failed to validate column gossip_generation
        at org.apache.cassandra.db.Column.validateFields(Column.java:295)
        at org.apache.cassandra.db.ColumnFamily.validateColumnFields(ColumnFamily.java:378)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTabl
eIdentityIterator.java:240)
        ... 15 more
Caused by: org.apache.cassandra.db.marshal.MarshalException: String didn't validate.
        at org.apache.cassandra.db.marshal.UTF8Type.validate(UTF8Type.java:65)
{noformat}","24/Sep/12 20:41;jbellis;(patch not intended for inclusion, since we can't trust wide row names to not blow up our log file)","25/Sep/12 12:29;slebresne;The problem was that scrub was correctly handling the CQL3 ColumnDefinition.  Basically it needs the same thing that we've introduced in CASSANDRA-4377, so attaching patch that move said code to CFMetadata to be used more generally.
",25/Sep/12 16:51;jbellis;+1,"26/Sep/12 12:11;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stack Size on Sun JVM 1.6.0_33 must be at least 160k,CASSANDRA-4602,12605931,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,amorton,amorton,02/Sep/12 23:34,12/Mar/19 14:17,13/Mar/19 22:27,24/Sep/12 19:50,1.0.12,1.1.5,,,,1,,,,,,,"I started a fresh Cassandra 1.1.4 install with Sun JVM 1.6.35.

On startup I got this in output.log

{noformat}
The stack size specified is too small, Specify at least 160k
Cannot create Java VM
Service exit with a return value of 1
{noformat}

Remembering CASSANDRA-4275 I monkeyed around and started the JVM with -Xss160k the same as Java 7. I then got

{code:java}
ERROR [WRITE-/192.168.1.12] 2012-08-31 01:43:29,865 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[WRITE-/192.168.1.12,5,main]
java.lang.StackOverflowError
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(Unknown Source)
	at java.net.SocketOutputStream.write(Unknown Source)
	at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
	at java.io.BufferedOutputStream.flush(Unknown Source)
	at java.io.DataOutputStream.flush(Unknown Source)
	at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:156)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:126)
{code}

Same as CASSANDRA-4442

At which point I dropped back to Java 6.33. 

CASSANDRA-4457 bumped the stack size to 180 for java 7, should we also do this for Java 6.33+ ?","Ubuntu 10.04 
java version ""1.6.0_35""
Java(TM) SE Runtime Environment (build 1.6.0_35-b10)
Java HotSpot(TM) 64-Bit Server VM (build 20.10-b01, mixed mode)",,,,,,,,,,,,,,,,,,03/Sep/12 12:08;jbellis;4602.txt;https://issues.apache.org/jira/secure/attachment/12543538/4602.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-03 12:05:59.369,,,no_permission,,,,,,,,,,,,251082,,,Thu Sep 27 05:12:02 UTC 2012,,,,,,0|i0b41j:,62767,brandon.williams,brandon.williams,,,,,,,,,,03/Sep/12 12:05;jbellis;Were you able to find anything in the release notes on this?  I couldn't.,03/Sep/12 12:08;jbellis;patch against 1.0 to change default to 160,04/Sep/12 19:40;brandon.williams;+1,04/Sep/12 19:55;jbellis;committed,"06/Sep/12 10:30;omid;Apparently this is due to HotSpot fix 7059899 [1] on 1.6.0_34 that increased ""StackShadowPages""'s default to 20, since a change in socketWrite's native implementation required more stack space. Increased StackShadowPages might require increased stack size (-Xss) [2] so that upon a call to a native method, there would be at least ""StackShadowPages"" stack space available.

[1] http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7059899
[2] http://www.oracle.com/technetwork/java/javase/crashes-137240.html ","06/Sep/12 13:23;jbellis;Thanks, Omid!","13/Sep/12 07:53;zenek_kraweznik0;cassandra 1.1.5

# java -version
java version ""1.6.0_35""
Java(TM) SE Runtime Environment (build 1.6.0_35-b10)
Java HotSpot(TM) 64-Bit Server VM (build 20.10-b01, mixed mode)
#

ERROR 17:39:53,363 Exception in thread Thread[Thrift:4040,5,main]
java.lang.StackOverflowError
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(Unknown Source)
        at java.io.BufferedInputStream.fill(Unknown Source)
        at java.io.BufferedInputStream.read1(Unknown Source)
        at java.io.BufferedInputStream.read(Unknown Source)
        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
        at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
        at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
        at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
        at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)

stack shoud be set to minimum 192k","24/Sep/12 19:50;jbellis;it looks like you are sending large-ish Thrift messages.  I'm fine with saying that if you're going to do that, you need to push up the stack size manually.","27/Sep/12 05:12;pmcfadin;+1 on greater than 160. Just fixed a re-occurring StackOverflowError problem by bumping the stack size. Added this line to cassandra-env.sh:

JVM_OPTS=""$JVM_OPTS -Xss194k""

Same JRE as above. build 1.6.0_35-b10

It was set to 160k but still threw exceptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid special characters which might yield to a build fail,CASSANDRA-4620,12606243,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,pyritschard,pyritschard,05/Sep/12 13:29,12/Mar/19 14:17,13/Mar/19 22:27,05/Sep/12 14:43,1.2.0,,,,,0,,,,,,,on jdk7 ant will fail on StreamingHistogram.java due to the special characters used line 130 and 133.,linux i386,,,,,,,,,,,,,,,,,,05/Sep/12 13:30;pyritschard;0001-Avoid-special-characters-which-might-confuse-ant.patch;https://issues.apache.org/jira/secure/attachment/12543850/0001-Avoid-special-characters-which-might-confuse-ant.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-05 14:43:42.674,,,no_permission,,,,,,,,,,,,255193,,,Wed Sep 05 14:43:42 UTC 2012,,,,,,0|i0epz3:,83970,yukim,yukim,,,,,,,,,,"05/Sep/12 13:30;pyritschard;this simple patch removes special characters and fixes the build on all versions of java i tested (jdk7, openjdk7, jdk6 and openjdk6)","05/Sep/12 14:43;yukim;+1 and committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflowError in CompactionExecutor thread,CASSANDRA-4765,12610368,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,abashir,abashir,04/Oct/12 18:17,12/Mar/19 14:17,13/Mar/19 22:27,18/Oct/12 20:20,1.1.7,1.2.0 beta 2,,,,0,compaction,,,,,,"Seeing the following error:


Exception in thread Thread[CompactionExecutor:21,1,RMI Runtime]
java.lang.StackOverflowError
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)

",,,,,,,,,,,,,,,,,,,04/Oct/12 18:59;jbellis;4765.txt;https://issues.apache.org/jira/secure/attachment/12547800/4765.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-04 18:34:28.987,,,no_permission,,,,,,,,,,,,240690,,,Thu Oct 18 20:20:39 UTC 2012,,,,,,0|i014ov:,4529,yukim,yukim,,,,,,,,,,04/Oct/12 18:34;brandon.williams;Which java version is this on?  You might be able to increase the Xss flag as a workaround.,"04/Oct/12 18:50;jbellis;This is from the SetView returned by Sets.union:

{code}
.     @Override public Iterator<E> iterator() {
        return Iterators.unmodifiableIterator(
            Iterators.concat(set1.iterator(), set2minus1.iterator()));
      }
{code}

I don't think we're taking 100s of unions so increasing Xss is unlikely to help.",04/Oct/12 18:59;jbellis;Actually we do take union in a loop in getOverlappingSSTables.  Give this patch a try.,"17/Oct/12 17:31;omid;I just got a similar StackOverflowError but on Iterators.java:

{code}
2012-10-17_14:35:09.95258 ERROR 14:35:09,942 Exception in thread Thread[CompactionExecutor:2773,1,main]
2012-10-17_14:35:09.95260 java.lang.StackOverflowError
2012-10-17_14:35:09.95261 	at java.util.AbstractList$Itr.hasNext(Unknown Source)
2012-10-17_14:35:09.95269 	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
2012-10-17_14:35:09.95281 	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
2012-10-17_14:35:09.95293 	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
2012-10-17_14:35:09.95305 	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
2012-10-17_14:35:09.95320 	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
2012-10-17_14:35:09.95331 	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
{code}

And iterators.java:517 and :514 belong to concat call that appears on the SetView returned by Sets::union.",18/Oct/12 13:00;omid;I could only reproduce the error by mistakenly running size tiered compaction in test environment on originally LCS-compacted data and the patch fixes the stack overflow. ,18/Oct/12 15:31;yukim;+1,18/Oct/12 20:20;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HintedHandoff fails to deliver hints after first repaired node,CASSANDRA-4772,12610635,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,azotcsit,azotcsit,azotcsit,05/Oct/12 21:01,12/Mar/19 14:17,13/Mar/19 22:27,09/Oct/12 13:55,1.1.6,,,,,1,hintedhandoff,,,,,,"If some node has hints for a few nodes it will deliver hints only for the first one of them. After all hints delivery for the first node compaction process is started. After compaction all data from hints cf is removed.

target fix for 1.2 version:
{code}
diff --git a/src/java/org/apache/cassandra/db/HintedHandOffManager.java b/src/java/org/apache/cassandra/db/HintedHandOffManager.java
index e5ff163..c02997e 100644
--- a/src/java/org/apache/cassandra/db/HintedHandOffManager.java
+++ b/src/java/org/apache/cassandra/db/HintedHandOffManager.java
@@ -189,7 +189,7 @@ public class HintedHandOffManager implements HintedHandOffManagerMBean
         ArrayList<Descriptor> descriptors = new ArrayList<Descriptor>();
         for (SSTable sstable : hintStore.getSSTables())
             descriptors.add(sstable.descriptor);
-        return CompactionManager.instance.submitUserDefined(hintStore, descriptors, Integer.MAX_VALUE);
+        return CompactionManager.instance.submitUserDefined(hintStore, descriptors, (int) System.currentTimeMillis() / 1000);
     }

 
     private static boolean pagingFinished(ColumnFamily hintColumnFamily, ByteBuffer startColumn)
{code}

Can I expect to see that fix in 1.1.6 version?",,,,,,,,,,,,,,CASSANDRA-3972,,,,,09/Oct/12 04:32;jbellis;4772-1.0.txt;https://issues.apache.org/jira/secure/attachment/12548358/4772-1.0.txt,08/Oct/12 11:00;azotcsit;cassandra-1.2-4772-hh_compact.txt;https://issues.apache.org/jira/secure/attachment/12548223/cassandra-1.2-4772-hh_compact.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-05 21:15:50.451,,,no_permission,,,,,,,,,,,,244077,,,Tue Oct 09 13:55:08 UTC 2012,,,,,,0|i05gtr:,29823,jbellis,jbellis,,,,,,,,,,"05/Oct/12 21:15;jbellis;the timestamp is ""time before which we can throw away deleted columns.""  columns that have not already been deleted are unaffected.","08/Oct/12 11:00;azotcsit;That timestamp is passed to ColumnFamilyStore.removeDeleted() method. That method removes columns with tombstones and expired columns. All columns in ""hints"" column family are ""expiring columns"" and they should be removed if gcBefore is greater than localExpirationTime. We pass gcBefore as Integer.MAX_VALUE, so all columns should be deleted because of Integer.MAX_VALUE >> localExpirationTime.
I've attached some test, that fails without the fix. Please take a look on it. 
","09/Oct/12 04:31;jbellis;You're right, Alexey.  Thanks for the followup.

Thought at first this would affect 1.0 as well -- compaction code is the same in 1.0.x -- but the bug needs on both compaction w/ MAX_TIMESTAMP *and* CASSANDRA-3716.",09/Oct/12 04:32;jbellis;attaching backport of test to 1.0,09/Oct/12 13:55;jbellis;committed fix to 1.1 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage.getNextWide produces corrupt data,CASSANDRA-4789,12611242,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,oberman,oberman,oberman,10/Oct/12 20:48,12/Mar/19 14:17,13/Mar/19 22:27,11/Oct/12 19:01,1.1.6,,,,,0,,,,,,,"This took me a while to track down.  I'm seeing the problem when the ""key changes"" case happens.  The intended behavior (as far as I can tell) when the key changes is the method returns the current tuple, and picks up where it left off on the next call to getNextWide().  The problem I'm seeing is the sometimes the current key advances between method calls, sometimes not.  ""Not"" being the correct behavior, since the code is saving the value into an instance variable, but when the key advances there is a key/value mismatch (the result being the values for two different keys are being glued together).  I think the problem might be related to keys that only have a single column???  I'm still trying to track that down to help assist in solving this case...

Maybe this will be clearer from me pasting a bunch of logging I added to the class.  The log messages are fairly self documenting (I hope):  

...lots of previous logging...
enter getNextWide
hasNext = true
set key = dVNhbXAxMzQ3ODM1OA%3D%3D
lastRow != null
added 1 items to bag from lastRow
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
key changed, new key = 669392df09572d0045b964bc65f86a2c
exit getNextWide
enter getNextWide
hasNext = true
//!!!THIS IS THE PROBLEM HERE I THINK!!!
//!!!Usually the key here == key before ""exit getNextWide""!!!
set key = 5f900ee4bb1850f8cf387cc3d5fc23ca
//!!! lastRow is data for 669392df09572d0045b964bc65f86a2c !!! 
//!!! but it's being added to key 5f900ee4bb1850f8cf387cc3d5fc23ca !!!
lastRow != null
added 1 items to bag from lastRow
//!!! Here are the real values for 5f900ee4bb1850f8cf387cc3d5fc23ca !!!
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
key changed, new key = 50438549-cdb6-8c44-f93a-d18d7daeffd8
exit getNextWide
enter getNextWide
hasNext = true
set key = 50438549-cdb6-8c44-f93a-d18d7daeffd8",,,,,,,,,,,,,,,,,,,11/Oct/12 15:22;oberman;patch.txt;https://issues.apache.org/jira/secure/attachment/12548752/patch.txt,11/Oct/12 14:55;oberman;patch.txt;https://issues.apache.org/jira/secure/attachment/12548748/patch.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-10 21:33:06.92,,,no_permission,,,,,,,,,,,,247116,,,Thu Oct 11 19:01:22 UTC 2012,,,,,,0|i07z87:,44474,brandon.williams,brandon.williams,,,,,,,,,,"10/Oct/12 20:50;oberman;Apparently using !!! is some kind of special syntax, sorry.  I was just trying to distinguish my comments from my logging.","10/Oct/12 21:05;oberman;I'm now 99% sure the problem is keys that map to a single column are being skipped over, and their values glued into the key after them.  But I'm not sure the most elegant fix...",10/Oct/12 21:33;jbellis;Are you mutating the ByteBuffers returned by CFIF?  That could definitely produce this kind of problem.,"11/Oct/12 14:04;oberman;It's definitely a problem using widerows=true and having keys that only map to one column.  Here is my patch on 1.1.5 (I also have a one line patch to fix the wide rows bug):

112a113
>     private ByteBuffer lastKey;
116d116
< 
153a154
> 			    //check key == lastKey?
159a161
> 			lastKey = null;
177a180
> 		    lastKey = (ByteBuffer)reader.getCurrentKey();
185a189,200
> 		    if(lastKey != null && !(key.equals(lastKey))) // last key only had one value
> 		    {
> 			tuple.append(new DataByteArray(lastKey.array(), lastKey.position()+lastKey.arrayOffset(), lastKey.limit()+lastKey.arrayOffset()));
> 			for (Map.Entry<ByteBuffer, IColumn> entry : lastRow.entrySet())
> 			{
> 			    bag.add(columnToTuple(entry.getValue(), cfDef, parseType(cfDef.getComparator_type())));
> 			}
> 			tuple.append(bag);
> 			lastKey = key;
> 			lastRow = (SortedMap<ByteBuffer,IColumn>)reader.getCurrentValue();
> 			return tuple;
> 		    }
194a210
> 		    lastKey = null;
551c567
<             widerows = Boolean.valueOf(System.getProperty(PIG_WIDEROW_INPUT));
---
>             widerows = Boolean.valueOf(System.getenv(PIG_WIDEROW_INPUT));",11/Oct/12 14:51;brandon.williams;Could you post your patches as attachments?,"11/Oct/12 14:55;oberman;Sorry about that, new to this.","11/Oct/12 14:59;brandon.williams;Thanks, but this doesn't appear to be in patch format:

patch: **** Only garbage was found in the patch input.
","11/Oct/12 15:02;oberman;I don't really have a proper development environment for cassandra, so it's a diff of the file...  I'll see how to get something better going.",11/Oct/12 15:22;oberman;Does this work?,"11/Oct/12 15:27;brandon.williams;It applies, thanks.","11/Oct/12 15:30;oberman;Good!  At some point I'll actually try to figure out how to setup a real development environment, as I feel really uncomfortable showing code without unit tests.","11/Oct/12 19:01;brandon.williams;Committed the relevant portion of this with formatting fixes.  I'll leave the PIG_WIDEROW_INPUT fix to CASSANDRA-4749, though it's needed to test this patch.  Thanks, Will!  I know this function is especially tricky.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in migration stage after creating an index,CASSANDRA-4786,12611102,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,10/Oct/12 01:58,12/Mar/19 14:17,13/Mar/19 22:27,16/Oct/12 06:41,1.2.0 beta 2,,,,,0,,,,,,,"The dtests are generating this error after trying to create an index in cql2:

{noformat}

ERROR [MigrationStage:1] 2012-10-09 20:54:12,796 CassandraDaemon.java (line 132) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:162)
    at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:549)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:479)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:256)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
ERROR [Thrift:1] 2012-10-09 20:54:12,797 CustomTThreadPoolServer.java (line 214) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:348)
    at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:238)
    at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:209)
    at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:714)
    at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:816)
    at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1656)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:196)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:344)
    ... 13 more
Caused by: java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:162)
    at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:549)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:479)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:256)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    ... 3 more
{noformat}",,,,,,,,,,,,,,,,,,,15/Oct/12 08:50;slebresne;4786.txt;https://issues.apache.org/jira/secure/attachment/12549121/4786.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-10 20:04:13.87,,,no_permission,,,,,,,,,,,,246571,,,Thu Jan 24 01:54:43 UTC 2013,,,,,,0|i07rqn:,43260,xedin,xedin,,,,,,,,,,10/Oct/12 20:04;xedin;[~brandon.williams] Can you please elaborate which of dtests indicates that? I tried cloning riptano/cassandra-dtests and running secondary_indexes_test.py on trunk but it can't even start a node because of cassandra.yaml changes...,10/Oct/12 20:06;brandon.williams;It's actually snapshot_test in concurrent_schema_changes_test.py that does it about 50% of the time.,"10/Oct/12 20:09;xedin;Interesting, I will try to deal with cassandra.yaml problem and run those tests then.",10/Oct/12 20:09;brandon.williams;I had to comment out the native_transport_address line in ccmlib/node.py to get it to run.,"11/Oct/12 20:15;brandon.williams;Here is a similar error I've seen when creating a CF:

{noformat}
ERROR [InternalResponseStage:1] 2012-10-11 15:12:16,695 CassandraDaemon.java (line 132) Exception in thread Thread[InternalResponseStage:1,5,main]
java.lang.NullPointerException
    at org.apache.cassandra.db.DefsTable.addColumnFamily(DefsTable.java:500)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:434)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationTask$1.response(MigrationTask.java:66)
    at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:44)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}","11/Oct/12 22:14;brandon.williams;Also doesn't appear to be tied to cql2, it happens with cql3 index creation too sometimes:

{noformat}

ERROR [Thrift:1] 2012-10-11 17:09:39,616 CustomTThreadPoolServer.java (line 214) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:348)
    at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:238)
    at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:209)
    at org.apache.cassandra.cql3.statements.CreateIndexStatement.announceMigration(CreateIndexStatement.java:113)
    at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:82)
    at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:130)
    at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:138)
    at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1658)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:196)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:344)
    ... 15 more
Caused by: java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:162)
    at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:549)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:479)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:256)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    ... 3 more
{noformat}","12/Oct/12 01:11;xedin;It looks like it's not a good idea to try to switch memtables as part of the CF reload process as NPE happens in CFS.reload():162 which is related to ability to change CF comparators feature (introduced in CQL3?), we can probably minimize the effect by actually somehow checking if comparator was changed before requesting memtable switch...","12/Oct/12 15:02;brandon.williams;If what we're seeing here is racy memtable switch behavior, then this is only being seen by schema changes since they flush two tables so close together, so we have a greater overall problem.  [~slebresne], can you take a look?","15/Oct/12 08:50;slebresne;So the NPE is because when we switch the memtable during reload, there could be a race where some other thread flush the current memtable first and thus maybeSwitchMemtable returns null.

This can be fixed by storing the comparator at the time a memtable is created, and when we try to change the memtable in reload, try switching the memtable until we know it has the right comparator (this also has the advantage that we won't switch the memtable unless there has been a comparator change). Patch attached to implement that.

Note that the patch also switch all the non-final variables from CFMetadata to volatile as they are definitively accessed from multiple threads.",15/Oct/12 21:19;xedin;+1,"16/Oct/12 06:41;slebresne;Committed, thanks","23/Jan/13 23:23;mharris;Do you guys know what version this bug was introduced in?  This is affecting us, but we're hesitant to move to 1.2 so soon after release.  Do you have a recommendation for a version of cassandra to use that would not be affected by this?",23/Jan/13 23:59;jbellis;This bug was introduced in 1.2.0 beta1.,"24/Jan/13 00:21;mharris;Weird, we're seeing this on 1.1.6.  Do you know of a fix we could try for this on that version?  We saw a similar bug had been fixed for 1.1.1, so it's surprising to see it again in 1.1.6.  I'll double check our deployment setup, but I'm pretty sure that's the case.

","24/Jan/13 01:54;mharris;Apologies, I think I was barking up the wrong tree here.  The stack trace I'm seeing actually matches CASSANDRA-4219, which looks similar but isn't quite the same thing, so I'll start being obnoxious on that thread instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in cql3 select,CASSANDRA-4783,12611037,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,09/Oct/12 18:39,12/Mar/19 14:17,13/Mar/19 22:27,23/Oct/12 08:31,1.2.0 beta 2,,,,,0,,,,,,,"Caused by 'select * from foo where key='blah' and column in (...)

{noformat}
ERROR 18:35:46,169 Exception in thread Thread[Thrift:11,5,main]
java.lang.AssertionError
        at org.apache.cassandra.cql3.statements.SelectStatement.getRequestedColumns(SelectStatement.java:443)
        at org.apache.cassandra.cql3.statements.SelectStatement.makeFilter(SelectStatement.java:312)
        at org.apache.cassandra.cql3.statements.SelectStatement.getSliceCommands(SelectStatement.java:200)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:125)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:61)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:130)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:138)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1658)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:196)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

Causes cqlsh to hang forever.",,,,,,,,,,,,,,,,,,,15/Oct/12 08:22;slebresne;0001-Fix-validation-of-IN-queries.txt;https://issues.apache.org/jira/secure/attachment/12549119/0001-Fix-validation-of-IN-queries.txt,15/Oct/12 08:22;slebresne;0002-Fix-mixing-list-set-operation-and-regular-updates.txt;https://issues.apache.org/jira/secure/attachment/12549120/0002-Fix-mixing-list-set-operation-and-regular-updates.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-13 00:28:33.254,,,no_permission,,,,,,,,,,,,246240,,,Tue Oct 23 08:31:07 UTC 2012,,,,,,0|i07j8f:,41883,jbellis,jbellis,,,,,,,,,,"13/Oct/12 00:28;iflatness;I have also found this issue when running update or insert on an existing column with both list and noncollection modifications. 
For example ""update my_table insert set list_col = [...], noncollection_col = 'val' where key='row_key';""

{noformat}
ERROR [Thrift:1] 2012-10-12 17:08:10,911 CassandraDaemon.java (line 132) Exception in thread Thread[Thrift:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.cql3.statements.ColumnGroupMap.getCollection(ColumnGroupMap.java:85)
	at org.apache.cassandra.cql3.statements.UpdateStatement.mutationForKey(UpdateStatement.java:251)
	at org.apache.cassandra.cql3.statements.UpdateStatement.getMutations(UpdateStatement.java:134)
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:83)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:130)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:138)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1677)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:193)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{noformat}","15/Oct/12 08:22;slebresne;The reason for the assertionError in select is that we were not validating correctly IN queries. But basically, we don't support yet the kind of IN queries that was attempted. We certainly should, and now that we can do multi-slice queries we should, but that's on the todo list. So attaching a patch that just fix the validation for now, and let's leave lifting the limitation to another ticket.

Ian's assertion error is unrelated but is fairly easy to fix so attaching a second patch for that too (not sure it's worth the trouble of spawning a separate ticket but we can if someone prefers it).",22/Oct/12 21:21;jbellis;+1,"23/Oct/12 08:31;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh fails to format timeuuid values,CASSANDRA-4720,12609124,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thepaul,thepaul,thepaul,25/Sep/12 20:54,12/Mar/19 14:17,13/Mar/19 22:27,25/Sep/12 22:39,1.2.0 beta 2,,Legacy/Tools,,,0,cqlsh,,,,,,"If a user has a table with a timeuuid column, and the user tries to select some rows containing timeuuid values, a weird error results:

{noformat}
global name 'unix_time_from_uuid1' is not defined
{noformat}

Not very helpful. It should probably display the timeuuid somehow.",,,,,,,,,,,,,,,,,,,25/Sep/12 20:56;thepaul;4720.patch.txt;https://issues.apache.org/jira/secure/attachment/12546580/4720.patch.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-25 22:36:14.079,,,no_permission,,,,,,,,,,,,255208,,,Tue Sep 25 23:50:23 UTC 2012,,,,,,0|i0eq2f:,83985,brandon.williams,brandon.williams,,,,,,,,,,25/Sep/12 20:56;thepaul;fixes the problem.,"25/Sep/12 22:36;iamaleksey;It does. (why is it assigned to me? I made sure it works, though, just in case).",25/Sep/12 22:39;brandon.williams;Committed.,"25/Sep/12 23:45;thepaul;sorry Aleksey, my fault, I guessed you were the one to automatically get new cqlsh tickets. But then I went and fixed it myself anyway, because my memory sucks. :)","25/Sep/12 23:50;iamaleksey;np, I don't mind getting issues with working patches attached to them (:",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 Predicate logic bug when using composite columns,CASSANDRA-4759,12610142,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,tjake,tjake,03/Oct/12 21:23,12/Mar/19 14:17,13/Mar/19 22:27,04/Oct/12 15:03,1.1.6,,,,,0,cql3,,,,,,"Looks like a predicate logic bug that only happens when you have > 2 primary keys and use COMPACT STORAGE (meaning its using composite columns under the hood)

First I'll show it works with just 2 
{code}
cqlsh:dev> CREATE TABLE testrev (
       ...          key text,
       ...          rdate timestamp,
       ...          num double,
       ...          PRIMARY KEY(key,rdate)
       ...          ) WITH COMPACT STORAGE
       ...            AND CLUSTERING ORDER BY(rdate DESC);

cqlsh:dev> INSERT INTO testrev(key,rdate,num) VALUES ('foo','2012-01-01',10.5);
cqlsh:dev> select * from testrev where key='foo' and rdate > '2012-01-01';
cqlsh:dev> select * from testrev where key='foo' and rdate >= '2012-01-01';
 key | rdate                    | num
-----+--------------------------+------
 foo | 2012-01-01 00:00:00-0500 | 10.5
{code}

Now we create with 3 parts to the PRIMARY KEY
{code}
cqlsh:dev> drop TABLE testrev ;
cqlsh:dev> CREATE TABLE testrev (
       ...          key text,
       ...          rdate timestamp,
       ...          rdate2 timestamp,
       ...          num double,
       ...          PRIMARY KEY(key,rdate,rdate2)
       ...          ) WITH COMPACT STORAGE
       ...          AND CLUSTERING ORDER BY(rdate DESC);

cqlsh:dev> INSERT INTO testrev(key,rdate,rdate2,num) VALUES ('foo','2012-01-01','2012-01-01',10.5);
cqlsh:dev> select * from testrev where key='foo' and rdate > '2012-01-01';
 key | rdate                    | rdate2                   | num
-----+--------------------------+--------------------------+------
 foo | 2012-01-01 00:00:00-0500 | 2012-01-01 00:00:00-0500 | 10.5

cqlsh:dev> select * from testrev where key='foo' and rdate >= '2012-01-01';
{code}

The last query should return the row...
",,,,,,,,,,,,,,,,,,,04/Oct/12 09:08;slebresne;4759.txt;https://issues.apache.org/jira/secure/attachment/12547701/4759.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-03 21:32:23.068,,,no_permission,,,,,,,,,,,,239428,,,Thu Oct 04 15:03:20 UTC 2012,,,,,,0|i00h53:,710,tjake,tjake,,,,,,,,,,"03/Oct/12 21:25;tjake;Also the first query in the second example is wrong, it should return nothing.",03/Oct/12 21:32;jbellis;Is this 1.1 or 1.2?,"04/Oct/12 09:08;slebresne;Seems like the fix for CASSANDRA-4716 didn't fix the whole problem. The (hopefully) last problem is in the handling of the compositeType 'end-of-component'. Namely, ReversedType don't apply to that eoc so we should take that into account. Patch attached to fix (the patch is against 1.1 because this does affect 1.1). It also fixes CASSANDRA-4760 (because that was in fact the same problem).",04/Oct/12 14:55;tjake;Tested +1,"04/Oct/12 15:03;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Estimated Row Cache Entry size incorrect (always 24?),CASSANDRA-4860,12613379,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,cburroughs,cburroughs,24/Oct/12 22:11,12/Mar/19 14:17,13/Mar/19 22:27,13/May/13 20:04,1.2.0 beta 3,,,,,0,qa-resolved,,,,,,"After running for several hours the RowCacheSize was suspicious low (ie 70 something MB)  I used  CASSANDRA-4859 to measure the size and number of entries on a node:

In [3]: 1560504./65021
Out[3]: 24.0

In [4]: 2149464./89561
Out[4]: 24.0

In [6]: 7216096./300785
Out[6]: 23.990877204647838


That's RowCacheSize/RowCacheNumEntires  .  Just to prove I don't have crazy small rows the mean size of the row *keys* in the saved cache is 67 and Compacted row mean size: 355.  No jamm errors in the log

Config notes:
row_cache_provider: ConcurrentLinkedHashCacheProvider
row_cache_size_in_mb: 2048

Version info:
 * C*: 1.1.6
 * centos 2.6.32-220.13.1.el6.x86_64
 * java 6u31 Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)",,,,,,,,,,,,,,,,,,,28/Mar/13 19:51;vijay2win@yahoo.com;0001-4860-v2.patch;https://issues.apache.org/jira/secure/attachment/12575941/0001-4860-v2.patch,30/Mar/13 03:55;vijay2win@yahoo.com;0001-4860-v3.patch;https://issues.apache.org/jira/secure/attachment/12576219/0001-4860-v3.patch,13/Nov/12 04:00;vijay2win@yahoo.com;0001-CASSANDRA-4860-for-11.patch;https://issues.apache.org/jira/secure/attachment/12553261/0001-CASSANDRA-4860-for-11.patch,13/Nov/12 04:00;vijay2win@yahoo.com;0001-CASSANDRA-4860.patch;https://issues.apache.org/jira/secure/attachment/12553262/0001-CASSANDRA-4860.patch,13/May/13 19:37;carlyeks;4860-fix-row-index-entry.patch;https://issues.apache.org/jira/secure/attachment/12582969/4860-fix-row-index-entry.patch,02/Apr/13 16:50;vijay2win@yahoo.com;4860-perf-test.zip;https://issues.apache.org/jira/secure/attachment/12576601/4860-perf-test.zip,07/May/13 16:38;carlyeks;4860-tests.patch;https://issues.apache.org/jira/secure/attachment/12582117/4860-tests.patch,28/Mar/13 04:54;enigmacurry;trunk-4860-revert.patch;https://issues.apache.org/jira/secure/attachment/12575829/trunk-4860-revert.patch,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2012-11-13 04:00:31.59,,,no_permission,,,,,,,,,,,,250881,,,Mon May 13 20:04:48 UTC 2013,,,,,,0|i0b27z:,62472,jbellis,jbellis,,,,,,,,,enigmacurry,"13/Nov/12 04:00;vijay2win@yahoo.com;Simple patch to fix the issue....

Looks like the issue is that we do Mesure (instead of MeasureDeep) which will not measure the byte[] attached to the RK.

*-for-11.patch is for 1.1 and *.patch is for 1.2.",13/Nov/12 14:06;jbellis;+1,13/Nov/12 18:48;vijay2win@yahoo.com;Committed Thanks!,"28/Mar/13 04:53;enigmacurry;I've been working on a 2.0/1.2 read performance regression analysis ([report is here|http://goo.gl/KHZfL]) and it brought me back to this ticket.

The patch as it was applied in revision 94fa82558f198489e0eefb0c14392607d5d23224 introduces a read performance penalty of about ~20%.

I've tested reverting this patch and applying on top of the latest 2.0 branch and it greatly improves read performance. 

I don't claim to know what this patch really does, so not sure if that's an appropriate action to take or not, but I've attached my patch here regardless (trunk-4860-revert.patch).","28/Mar/13 05:52;vijay2win@yahoo.com;Ryan, so the results has row caching enabled? (For the record this shows up only when caching is enabled and using InHeap RowCache).
20% over head is a lot more than i initially anticipated.

I could think of one alternative: I have to dig my history, but i had a alternative to measureDeep() which was very similar to http://goo.gl/oqD89, but it doesn't cover all the platforms and was lot more complicated so it was never committed. Should we resurrect it?","28/Mar/13 06:31;enigmacurry;Hi [~vijay2win@yahoo.com], I have not tweaked any row cache settings from the default. This is in my cassandra.yaml:

{code}
# Maximum size of the row cache in memory.
# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.
#
# Default value is 0, to disable row caching.
row_cache_size_in_mb: 0
{code}

From the description, it sounds like I should have row caching turned off, and yet this code is still being run as evidenced by the change in performance by reverting your patch. I don't yet have a deep understanding of the features involved here, so if you have any other suggestions for things to test here, please let me know. Thanks!","28/Mar/13 16:46;vijay2win@yahoo.com;Ahaa missed it, it is the key cache which is slowing down the performance.","28/Mar/13 19:51;vijay2win@yahoo.com;Hi Ryan, 
Since you have the environment do you mind testing -v2?
It is not a final patch, I have to verify the accuracy of the estimate though.","28/Mar/13 20:59;enigmacurry;With your v2 patch applied I get an average read rate of 14276. That's much worse actually than the first patch. To make sure something is not amiss, I re-ran the 2.0 baseline and got comparable results as before (22524). The number we're hoping to back to is ~28000.","30/Mar/13 03:55;vijay2win@yahoo.com;Hi Ryan, can you try this one? 

I am really optimistic that this patch should improve the performance without sacrificing the accuracy of measurement of the memory footprint.
Attached patch doesn't use any reflection, Micro benchmark shows a better performance than any other approach.

{code}
Completed warmup!, Number of Iteratoions: 1000000
Using reflection took: 8113
Using 4860-v3 took: 95
Using MemoryMeter meter.measure(key) took: 190
Using MemoryMeter meter.measureDeep(key) took: 982
{code}

Note: We don't have this optimization when we have a range tombstone in KeyCache (coz the code becomes really complex), and while using RowCache.

Let me know if you want me to publish the accuracy test and perf test code.",02/Apr/13 15:46;enigmacurry;This one (v3 patch) is much better. Average read op rate is 26484. That's the best run I've seen on 1.2+ except for the run with the trunk-4860-revert.patch applied which has an average read op rate of 27809 (4.8% faster).,"02/Apr/13 16:50;vijay2win@yahoo.com;Ryan, Micro benchmark shows v3 is better, is there any chance you are hitting the same key in the key cache often? The reason for asking is that you might have a smaller key cache since the calculation is more accurate. If yes then i would increase the keycache and try. 

To be clear the Meter.measure() is bug and cannot be used in production and can cause OOM.","11/Apr/13 20:14;enigmacurry;I'm hitting the same key multiple times in the write, but not the read:

stress -F 2000000 -n 20000000 -i 1
stress -n 2000000 -o read -i 1
","12/Apr/13 04:00;vijay2win@yahoo.com;This is my theory:

2M KV with Measure.measure() will take 96,000,000 or 96M (2 *24 * 2000000 bytes) will fit in key cache.
2M KV with measureDeep() will take 96M + 48M (48 * 2000000 + 24 * 2000000) where 48 is the index min size and 24 is the key size.

Hence there is a eviction overhead on the keycache which you dont have in Measure.measure().
Give the above if you have the key cache of 300M and re test both v3 should show a better performance.

{code}
Completed warmup!, Number of Iteratoions: 1000000
Using reflection took: 8037
Using 4860-v3 took: 90
Using MemoryMeter meter.measure(key) took: 190
Using MemoryMeter meter.measureDeep(key) took: 1002
Using 4860-v3 RowIndexEntry took: 14
Using MemoryMeter meter.measure(RowIndexEntry(i)) took: 104
Using MemoryMeter meter.measureDeep(RowIndexEntry(i)) took: 459
Size of Meter.measure key: 24
Size of Meter.measure index: 24
Size of Meter.measureDeep key: 48
Size of Meter.measureDeep index: 24
Size of key: 48
Size of index: 24
{code}","12/Apr/13 04:39;vijay2win@yahoo.com;Added unit test and pushed to https://github.com/Vijay2win/cassandra/commits/4860-v4, Thanks!","16/Apr/13 16:25;enigmacurry;I increased my key_cache_size_in_mb and did in fact get better results:

{code}
Averages from the middle 80% of values:
interval_op_rate          : 27848
interval_key_rate         : 27848
latency median            : 0.7
latency 95th percentile   : 1.4
latency 99.9th percentile : 22.4
Total operation time      : 00:01:20
{code}

The old measure() way with 300M key cache:
{code}
Running stress : -n 2000000 -o read -i 1
output is hidden while collecting stats...
Averages from the middle 80% of values:
interval_op_rate          : 27877
interval_key_rate         : 27877
latency median            : 0.7
latency 95th percentile   : 1.5
latency 99.9th percentile : 23.4
Total operation time      : 00:01:20
{code}

This is roughly equal to the original messure() method in read performance, I'm happy with it!","16/Apr/13 16:38;jbellis;{quote}
2M KV with Measure.measure() will take 96,000,000 or 96M (2 *24 * 2000000 bytes) will fit in key cache.
2M KV with measureDeep() will take 96M + 48M (48 * 2000000 + 24 * 2000000) where 48 is the index min size and 24 is the key size.
{quote}

Clarifying for my own benefit: Vijay is saying that before the original fix, the key cache underestimated the real entry size by 1/3, so a configured size of 2M would actually allow caching 3M worth of entries.  So to compare performance apples-to-apples, we need to allow the fixed code to use an equivalent size.","16/Apr/13 16:44;jbellis;Suggest standardizing on {{memorySize}} instead of {{size}} in IMeasureableMemory, to avoid confusion with size = number of contained elements.  Otherwise +1.","17/Apr/13 01:44;vijay2win@yahoo.com;Wow that sounds lot better than my cryptic explanation :)

Committed to 1.2 and trunk, Thanks Ryan and Jonathan!","07/May/13 15:02;tjake;I just rolled this patch and am hitting this exception:

{code}
) liveRatio is 7.8818923401518655 (just-counted was 7.8818923401518655).  calculation took 825ms for 78373 columns
ERROR [ReadStage:14] 2013-05-07 11:01:44,555 CassandraDaemon.java (line 175) Exception in thread Thread[ReadStage:14,5,main]
java.lang.AssertionError: Serialized size cannot be more than 2GB/Integer.MAX_VALUE
	at org.apache.cassandra.cache.ConcurrentLinkedHashCache$1.weightOf(ConcurrentLinkedHashCache.java:58)
	at org.apache.cassandra.cache.ConcurrentLinkedHashCache$1.weightOf(ConcurrentLinkedHashCache.java:54)
	at com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$BoundedEntryWeigher.weightOf(ConcurrentLinkedHashMap.java:1447)
	at com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:764)
	at com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:743)
	at org.apache.cassandra.cache.ConcurrentLinkedHashCache.put(ConcurrentLinkedHashCache.java:101)
	at org.apache.cassandra.cache.ConcurrentLinkedHashCache.put(ConcurrentLinkedHashCache.java:27)
	at org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:44)
	at org.apache.cassandra.io.sstable.SSTableReader.cacheKey(SSTableReader.java:696)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:834)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:717)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:43)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:101)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:274)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1357)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)
	at org.apache.cassandra.db.Table.getRow(Table.java:347)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}","07/May/13 15:09;jbellis;I don't suppose you can turn that into a failing unit test?  It's not obvious to me what could be different about what you're doing, and what we're already doing in the [passing] tests.",07/May/13 15:19;tjake;[~carlyeks] is on the case... ,"07/May/13 15:38;tjake;Looking at the code this line looks suspect....

https://github.com/apache/cassandra/blob/da93a1cfe483a1522b2c149d287279a74e43a8a9/src/java/org/apache/cassandra/utils/ObjectSizes.java#L65

If you pass in a mmapped bytebuffer I don't think capacity is the right thing to use...","07/May/13 15:50;jbellis;Agreed, the way we use BB it should be .remaining",07/May/13 16:13;tjake;Also do we even care about direct byte buffers? it's not on heap so we should just skip those?,07/May/13 16:16;jbellis;Don't we make a copy for the row cache entry?  If we don't we probably should.,"07/May/13 16:38;carlyeks;Providing a couple of tests which show the broken byte buffer. Don't think that we should be comparing against the meter for these, but since it appears to be compared against that in the other parts, I followed the same specification.","07/May/13 17:27;vijay2win@yahoo.com;We should probably check BB isDirect and skip adding the BB overhead, honestly it was an oversight. 
We should probably just switch to remaining() for the rest to be safe.

{quote}
Don't we make a copy for the row cache entry? If we don't we probably should.
{quote}

Do you want to open a separate ticket on this? Since this happens on CLHM (in-heap cache) i am not sure if it will benefit from bytes copying.","07/May/13 17:50;jbellis;I'm more worried about safety issues after a mapped buffer like Jake's gets unmapped.

Edit: but if he's using 1.2.4 safely then I guess the ""clean out row cache after compaction"" code is working pretty well.","07/May/13 18:08;tjake;I'm not using 1.2.4. We went from 1.2.3 -> 1.2.5

Our workaround is to disable the keycache","07/May/13 19:55;jbellis;I did some code diving but just got more confused.  Here's the KeyKacheKey relevant parts:

{code}
    public final byte[] key;

    public KeyCacheKey(Descriptor desc, ByteBuffer key)
    {
        this.desc = desc;
        this.key = ByteBufferUtil.getArray(key);
        assert this.key != null;
    }
{code}

# we are indeed making a defensive copy
# we don't even have a ByteBuffer involved here, so while {{.capacity}} could sometimes not be what we want, it's not causing the problem here.  (Other times, {{capacity}} *could* be what we want; I don't think there's a one-size-fits-all answer here.  JAMM has an {{omitSharedBufferOverhead}} setting to configure this behavior.) 

I think we need to look elsewhere for our smoking gun.","07/May/13 21:43;tjake;I think the issue is the RowIndexEntry size. If you look at IndexInfo.memorySize() those bytebuffers would have the wrong capacity since we mmap them...

{code}
        public long memorySize()
        {
            long fields = ObjectSizes.getSize(firstName) + ObjectSizes.getSize(lastName) + 8 + 8; 
            return ObjectSizes.getFieldSize(fields);
        }
{code}
","07/May/13 21:44;vijay2win@yahoo.com;{quote}
Providing a couple of tests which show the broken byte buffer. 
{quote}
No it is not broken, MeasureDeep was omitting the shared buffer size... which is kind of wrong... try the following.

{code}
        ByteBuffer bb = ByteBuffer.allocate(1000);
        long objectSize = ObjectSizes.getSize(bb);
        MemoryMeter meter2 = new MemoryMeter();
        long meterSize = meter2.measureDeep(bb);
        Assert.assertEquals(meterSize, objectSize);
{code}

{quote}
Here's the KeyKacheKey relevant parts
{quote}

Agreed! Looks like error is from CLHC, i am still not sure where its broken MappedFileDataInput.readBytes copies anyways.... [~tjake]?","07/May/13 22:03;jbellis;bq. i am still not sure where its broken MappedFileDataInput.readBytes copies anyways

Also a good point.  For reference:

{code}
        // we have to copy the data in case we unreference the underlying sstable.  See CASSANDRA-3179
        ByteBuffer clone = ByteBuffer.allocate(bytes.remaining());
        clone.put(bytes);
        clone.flip();
        return clone;
{code}","13/May/13 19:37;carlyeks;The issue is in the following code from RowIndexEntry:208

{code}
long internal = 0;
            for (IndexHelper.IndexInfo idx : columnsIndex)
                internal += idx.memorySize();
            long listSize = ObjectSizes.getFieldSize(ObjectSizes.getArraySize(columnsIndex.size(), internal) + 4);
            return ObjectSizes.getFieldSize(deletionTime.memorySize() + listSize);
{code}

The list size is *not* getArraySize(columnsIndex.size(), internal). That multiplies the number of elements by the size that was just calculated.

It should instead be:
{code}
ObjectSizes.getFieldSize(internal + 4)
{code}

There are no other suspicious usages of getArraySize, the only other ones are from the ObjectSizes file for byte array sizes (those make sense).",13/May/13 20:04;jbellis;Moved to CASSANDRA-5564,,,,,,,,,,,,,,,,,,,,,,,
get_count with 'count' param between 1024 and ~actual column count fails,CASSANDRA-4833,12612449,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,thobbs,thobbs,18/Oct/12 16:56,12/Mar/19 14:17,13/Mar/19 22:27,22/Oct/12 18:56,1.1.7,1.2.0 beta 2,,,,0,,,,,,,"If you run get_count() with the 'count' param of the SliceRange set to a number between 1024 and (approximately) the actual number of columns in the row, something seems to silently fail internally, resulting in a client side timeout.  Using a 'count' param outside of this range (lower or much higher) works just fine.

This seems to affect all of 1.1 and 1.2.0-beta1, but not 1.0.",,,,,,,,,,,,,,,,,,,22/Oct/12 16:08;yukim;4833-1.1-v2.txt;https://issues.apache.org/jira/secure/attachment/12550308/4833-1.1-v2.txt,19/Oct/12 17:05;yukim;4833-1.1.txt;https://issues.apache.org/jira/secure/attachment/12550008/4833-1.1.txt,18/Oct/12 16:58;thobbs;4833-get-count-repro.py;https://issues.apache.org/jira/secure/attachment/12549708/4833-get-count-repro.py,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-10-18 20:36:24.716,,,no_permission,,,,,,,,,,,,249596,,,Thu Jan 10 17:50:23 UTC 2013,,,,,,0|i0aft3:,58834,thobbs,thobbs,,,,,,,,,,18/Oct/12 16:58;thobbs;Attached script reproduces the issue with pycassa.,"18/Oct/12 20:36;yukim;get_count runs into infinite loop when requesting with count param around a multiple of page size(1024).
Patch attached with unit test.","18/Oct/12 21:06;thobbs;I tested out the patch, and although the infinite loop isn't hit, the resulting count numbers are off.  For example, the repro script has 3050 columns, and when run produces these counts (manually edited for clarity):

{noformat}
specified count=1024: 1024 expected, got 1024
specified count=2^31: 3050 expected, got 2047
specified count=4000: 3050 expected, got 2047
specified count=3051: 3050 expected, got 2047
specified count=1025: 1025 expected, got 1024
{noformat}","19/Oct/12 17:05;yukim;You are right.
New version attached. I also modified test to match yours.

get_count pages when requesting count more than page size (determined by average column size but max at 1024). Paging starts with the last column of previously fetched page, so newly fetched page may contains one overlapped column.
When page size is 1024, and we have more than 1024 columns in a row, counting with limit of 1025 columns always fails because we fetch 1 (1025 - 1024 page size) column on 2nd page and it contains only already fetched column. Same thing can happen around the actual number of columns in a row.

Attached patch modified so that paging will fetch at least two columns.","19/Oct/12 23:43;thobbs;The latest patch fixes the issue and passes all of the pycassa tests.

One comment on this conditional:
{code}
    if (requestedCount == 0 || columns.size() < predicate.slice_range.count)
        break;
{code}

Since you're no longer decrementing requestedCount, the first half of the disjunction isn't needed.  If the user actually set a requestedCount of 0, the first column slice would be empty, so we wouldn't get this far.

Other than that, I'm +1 on the changes","22/Oct/12 16:08;yukim;Thanks for review, Tyler.

What we want here is to fetch last page with remainder, not whole page size. So we still need requestedCount -= newColumns.

Attaching v2 for this.","22/Oct/12 16:28;thobbs;The patch needs to be rebased, but I'm +1 on the code changes","22/Oct/12 18:56;yukim;Committed, thanks!","10/Jan/13 17:50;jbellis;Updating affects-version to 1.1.0 since ""this seems to affect all of 1.1,"" and since the fix caused a regression with TTL data (CASSANDRA-5099).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assertion failure in leveled compaction test,CASSANDRA-4799,12611533,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,12/Oct/12 14:47,12/Mar/19 14:17,13/Mar/19 22:27,16/Oct/12 14:41,1.1.7,1.2.0,,,,0,lcs,,,,,,"It's somewhat rare, but I'm regularly seeing this failure on trunk:

{noformat}
    [junit] Testcase: testValidationMultipleSSTablePerLevel(org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest.testValidationMultipleSSTablePerLevel(LeveledCompactionStrategyTest.java:78)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest FAILED
{noformat}

I suspect there's a deeper problem, since this is a pretty fundamental assertion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-10-15 18:25:03.487,,,no_permission,,,,,,,,,,,,247970,,,Tue Oct 16 14:41:51 UTC 2012,,,,,,0|i095zb:,51401,jbellis,jbellis,,,,,,,,,,"15/Oct/12 18:25;yukim;I found two problems here.

1) Test does not completely setup necessary SSTables and causes above AssertionError.

LCS test uses CFS#forceFlush to generate enough SSTables at the beginning of the test, but since the method is asynchronous call, there is a chance that test proceeds without sufficient SSTables to fill up L2 and causes AE at strat.getLevelSize(2) > 0.

Fix for this is to change forceFlush to forceBlockingFlush.
(diff: https://github.com/yukim/cassandra/commit/0d16efc6d7592e61f15598d5a4e3cc81d2760007)

2) Repair sometimes causes AssertionError with LCS

During the test, I encountered below error several times.

{code}
ERROR [ValidationExecutor:1] 2012-10-12 14:39:18,660 SchemaLoader.java (line 73) Fatal exception in thread Thread[ValidationExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getScanners(LeveledCompactionStrategy.java:183)
        at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:879)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:743)
        at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:71)
        at org.apache.cassandra.db.compaction.CompactionManager$7.call(CompactionManager.java:481)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

AssertionError comes from following assert code in LCS#getScanners:

{code}
for (SSTableReader sstable : sstables)
{
    int level = manifest.levelOf(sstable);
    assert level >= 0;
    byLevel.get(level).add(sstable);
}
{code}

LeveledManifest#levelOf method returns level of SSTable or -1 if SSTable is not yet added to LeveledManifest. Here, each SSTable comes from CF's DataTracker.
Every time SSTable is written by flush/compaction, it gets added to DataTracker and  after that, added to LeveledManifest if you are using LCS.
If above repair code is executed between those two, you will get AssertionError.

My proposed fix is to remove assertion and treat SSTables that do not belong LeveledManifest yet as L0 SSTables.
(diff: https://github.com/yukim/cassandra/commit/c8a0fb9a9128e47ec3d07926eb26f6fd93664f52)

Problem (2) also exists in 1.1 branch, but without assertion. We may want to fix this in 1.1 too.","16/Oct/12 02:16;jbellis;+1

(I wish we could keep that assert but I don't have a better solution.)",16/Oct/12 14:41;yukim;Committed to trunk as well as 1.1 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
composite indexes don't always return results they should,CASSANDRA-4796,12611438,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,11/Oct/12 21:37,12/Mar/19 14:17,13/Mar/19 22:27,16/Oct/12 06:41,1.2.0 beta 2,,,,,0,,,,,,,"composite_index_with_pk_test in the dtests is failing and it reproduces manually.

{noformat}
cqlsh:foo>            CREATE TABLE blogs (                 blog_id int,                 time1 int,                 time2 int,                 author text,                 content text,                 PRIMARY KEY (blog_id, time1, time2)             ) ;
cqlsh:foo> create index on blogs(author);
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (1, 0, 0, 'foo', 'bar1');
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (1, 0, 1, 'foo', 'bar2');
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (2, 1, 0, 'foo', 'baz');
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (3, 0, 1, 'gux', 'qux');
cqlsh:foo> SELECT blog_id, content FROM blogs WHERE time1 = 1 AND author='foo';
cqlsh:foo>
{noformat}

The expected result is:
{noformat}

 blog_id | time1 | time2 | author | content
---------+-------+-------+--------+---------
       2 |     1 |     0 |    foo |     baz
{noformat}",,,,,,,,,,,,,,,,,,,15/Oct/12 13:37;slebresne;4726.txt;https://issues.apache.org/jira/secure/attachment/12549145/4726.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-15 13:37:44.119,,,no_permission,,,,,,,,,,,,247803,,,Tue Oct 16 06:41:24 UTC 2012,,,,,,0|i08pzr:,48810,jbellis,jbellis,,,,,,,,,,"15/Oct/12 13:37;slebresne;Hum, I think that is due to some bad merge or something along that way. Basically we were using a column value instead of a key because SelectStatement.buildBound() was used on keys but was (wrongfully) using columns internally instead.

Patch attached to change that and that makes buildBound static to make it harder to do that kind of mistake again.",16/Oct/12 01:30;jbellis;+1,"16/Oct/12 06:41;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepared Statements don't support collections,CASSANDRA-4739,12609767,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,titanous,titanous,01/Oct/12 16:32,12/Mar/19 14:17,13/Mar/19 22:27,05/Oct/12 07:33,1.2.0 beta 2,,,,,0,,,,,,,"I'm putting a collection onto the wire in an EXECUTE request with exactly the same bytes that Cassandra encodes the same data in a response:

""Can't apply operation on column with org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type) type""

Here's the full trace log from cassandra:

{noformat}
DEBUG 19:24:15,414 Received: PREPARE INSERT INTO things (id, set_text) VALUES (?, ?);
TRACE 19:24:15,414 CQL QUERY: INSERT INTO things (id, set_text) VALUES (?, ?);
TRACE 19:24:15,415 Stored prepared statement #413587006 with 2 bind markers
DEBUG 19:24:15,415 Responding: RESULT PREPARED 413587006 [id(gocql_collections, things), org.apache.cassandra.db.marshal.TimeUUIDType][set_text(gocql_collections, things), org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type)]
DEBUG 19:24:15,416 Received: EXECUTE 413587006 with 2 values
TRACE 19:24:15,416 [1] 'java.nio.HeapByteBuffer[pos=18 lim=34 cap=53]'
TRACE 19:24:15,416 [2] 'java.nio.HeapByteBuffer[pos=38 lim=51 cap=53]'
DEBUG 19:24:15,417 Responding: ERROR INVALID: Can't apply operation on column with org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type) type.
{noformat}


The prepared statement is:

{noformat}
INSERT INTO things (id, set_text) VALUES (?, ?);
{noformat}

and the collection value ({'asdf', 'sdf'}) is encoded as:

{noformat}
00 02 00 04 61 73 64 66 00 03 73 64 66
{noformat}

which is byte-for-byte exactly the same as what I get from Cassandra off the wire when I do a query for the same data.

I already have the driver working with other queries/inserts with all other types, so this is just a collection encoding problem.","Cassandra 937f15e1
OS X 10.8.2
java version ""1.6.0_35""
Java(TM) SE Runtime Environment (build 1.6.0_35-b10-428-11M3811)
Java HotSpot(TM) 64-Bit Server VM (build 20.10-b01-428, mixed mode)",,,,,,,,,,,,,,,,,,04/Oct/12 10:14;slebresne;4739.txt;https://issues.apache.org/jira/secure/attachment/12547704/4739.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-02 10:26:22.468,,,no_permission,,,,,,,,,,,,239432,,,Fri Oct 05 07:33:02 UTC 2012,,,,,,0|i00h6n:,717,jbellis,jbellis,,,,,,,,,,"02/Oct/12 10:26;slebresne;This is actually not related to the binary protocol; collections are just not supported correctly in prepared statements (I've updated the title to reflect that).

And this require a little bit of specific code because we must ""deserialize"" the sent list to be able to insert internally one column per element. Attaching patch to fix that.","02/Oct/12 16:08;jbellis;bq. this require a little bit of specific code because we must ""deserialize"" the sent list to be able to insert internally one column per element

How is this different from Collection support in non-prepared statements?","02/Oct/12 16:35;slebresne;bq. How is this different from Collection support in non-prepared statements?

That's dealt with by the parser.","03/Oct/12 19:19;titanous;I've been testing this patch out a bit, and it's mostly working, but I ran into an issue when using a map<timestamp,int> ({1349286846012: 2}):

""Expected 8 or 0 byte long for date (4)""

The map is encoded in the exact same format that I get from the server when querying the same value:

{noformat}
00 01 00 08 00 00 01 3a 27 c3 5e 3c 00 04 00 00 00 02
{noformat}","04/Oct/12 10:14;slebresne;Oups, that last one is just a copy-paste type (MapType was trying to validate values with the keys comparator). Patch updated.","04/Oct/12 17:51;jbellis;+1, although it seems weird to have partly-prepared statements to me.  Weird in the ""I hope this doesn't get a lot more complicated and cause us grief in the future"" kind of way. :)","05/Oct/12 07:33;slebresne;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
o.a.c.service.StorageProxy - compilation issue,CASSANDRA-4703,12608756,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,kaykay.unique,kaykay.unique,kaykay.unique,22/Sep/12 16:38,12/Mar/19 14:17,13/Mar/19 22:27,22/Sep/12 20:20,1.2.0 beta 2,,,,,0,,,,,,,"Running ant on master yields the following compilation error. 



build-project:
     [echo] apache-cassandra: /home/user/workspace/cassandra/build.xml
    [javac] Compiling 1 source file to /home/user/workspace/cassandra/build/classes/main
    [javac] /home/user/workspace/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java:803: local variable remotes is accessed from within inner class; needs to be declared final
    [javac]                             sendToHintedEndpoints(cm.makeReplicationMutation(), remotes, responseHandler, localDataCenter, consistency_level);
    [javac]                                                                                 ^
    [javac] Note: /home/user/workspace/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java uses or overrides a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: /home/user/workspace/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java uses unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 error




","$ java -version
java version ""1.6.0_33""
Java(TM) SE Runtime Environment (build 1.6.0_33-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.8-b03, mixed mode)




$ uname -a
Linux user-System-Product-Name 3.5.0-13-generic #14-Ubuntu SMP Wed Aug 29 16:48:44 UTC 2012 x86_64 x86_64 x86_64 GNU/Linux


Ubuntu 12.10",,,,,,,,,,,,,,,,,,22/Sep/12 16:39;kaykay.unique;CASSANDRA-4703.patch;https://issues.apache.org/jira/secure/attachment/12546169/CASSANDRA-4703.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-22 20:20:05.763,,,no_permission,,,,,,,,,,,,256346,,,Sat Sep 22 20:20:05 UTC 2012,,,,,,0|i0gym7:,97038,jbellis,jbellis,,,,,,,,,,"22/Sep/12 20:20;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug when composite index is created in a table having collections,CASSANDRA-4909,12614717,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,04/Nov/12 17:44,12/Mar/19 14:17,13/Mar/19 22:27,05/Nov/12 16:18,1.2.0 beta 2,,,,,0,,,,,,,"CASSANDRA-4511 is open to add proper indexing of collection, but currently indexing doesn't work correctly if we index a value in a table having collection, even if that value is not a collection itself.

We also don't refuse creating index on collections, even though we don't support it. Attaching patch to fix both.",,,,,,,,,,,,,,,,,,,04/Nov/12 17:44;slebresne;4909.txt;https://issues.apache.org/jira/secure/attachment/12552029/4909.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-05 16:02:13.001,,,no_permission,,,,,,,,,,,,255140,,,Mon Nov 05 22:49:01 UTC 2012,,,,,,0|i0eplr:,83910,jbellis,jbellis,,,,,,,,,,"05/Nov/12 16:02;henrikring;Working at the ApacheConEU Hackathon. Suggestions for testing the patch.
Below is my understanding of the issue and the solution as well as a suggestion for how to test it.

The patch changes the code so the exception: InvalidRequestException(""Indexes on collections are no yet supported"")
is raised if an attempt is made to create an index on an collection type column.

Creation of indexes on tables that have collection column values is allowed as long as 
the collection columns are NOT part of the index. 

Tests:
In CQL3 create CF with 2 collection type columns ""A1"" and ""A2"" and 2 noncollection type columns ""B1"" and ""B2"".
Create index on A1 or A2 should fail. 
Create an index on B1 or B2 should succeed.
Insert data.
Select on column with index to ensure index is in place.

-- Should succeed
CREATE KEYSPACE TEST_CASSANDRA_4909_KS;

-- Should succeed
USE TEST_CASSANDRA_4909_KS; 

-- Should succeed
CREATE TABLE TEST_CASSANDRA_4909_TLB1
   (A1 set<text>,
    A2 set<text>,
    B1 text,
    B2 text,
    PRIMARY KEY B1);

-- Should fail: InvalidRequestException(""Indexes on collections are no yet supported"")
CREATE INDEX TEST_CASSANDRA_4909_TLB1_INX1 ON TEST_CASSANDRA_4909_TLB1 (A1);

-- Should Succeed:
CREATE INDEX TEST_CASSANDRA_4909_TLB1_INX2 ON TEST_CASSANDRA_4909_TLB1 (B2);

-- Wait for schema agreement.

-- Should succeed:
INSERT INTO TEST_CASSANDRA_4909_TLB1 (A1, A2, B1, B2)
       VALUES({'A1-ROW1-ELM-1', 'A1-ROW1-ELM2'}, {'A2-ROW1-ELM-1', 'A2-ROW1-ELM2'}, 'B1-ROW1', 'B2-ROW1' );

-- Should succeed:
INSERT INTO TEST_CASSANDRA_4909_TLB1 (A1, A2, B1, B2)
       VALUES({'A1-ROW2-ELM-1', 'A1-ROW2-ELM2'}, {'A2-ROW2-ELM-1', 'A2-ROW2-ELM2'}, 'B1-ROW2', 'B2-ROW2' );

-- Should succeed (ensure index is in place):
SELECT * FROM TEST_CASSANDRA_4909_TLB1 WHERE B2 = 'B2-ROW2';

-- Should succeed:
DROP TABLE TEST_CASSANDRA_4909_TLB1;
",05/Nov/12 16:03;jbellis;+1,"05/Nov/12 16:18;slebresne;Committed, thanks","05/Nov/12 16:21;slebresne;bq. Working at the ApacheConEU Hackathon

I'm not sure what that means, but let me recall that there is a fair amount of tests here: https://github.com/riptano/cassandra-dtest/blob/master/cql_tests.py, including a test for this ticket. It's not necessarily perfect, but in any case it would be nice to avoid duplication of efforts.","05/Nov/12 17:18;henrikring;Just to clarify: Comments above are my understanding of the issue and a suggestion for test. Since you agree I will next try and actually run the test to see if it does as expected. I have not done that yet. I need to get the development environment up and running first. I am new to all of this so it is taking a bit of time to get it all going :-). I'm not a Python developer, so I was hoping someone else with better skills that area could include the test in the automated tests as appropriate. ","05/Nov/12 22:49;henrikring;Now I got it running and ran the test - it passed:

{code:title=CASSANDRA_4909.java|borderStyle=solid}
package org.apache.cassandra;

import org.apache.cassandra.thrift.*;
import org.apache.thrift.protocol.TProtocol;
import org.apache.thrift.transport.TFramedTransport;
import org.apache.thrift.transport.TSocket;
import org.apache.thrift.transport.TTransport;
import org.junit.AfterClass;
import org.junit.BeforeClass;

import java.io.UnsupportedEncodingException;
import java.nio.ByteBuffer;
import java.nio.charset.Charset;
import java.nio.charset.CharsetDecoder;
import java.nio.charset.CharsetEncoder;
import java.sql.Connection;
import java.util.List;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertNotNull;

public class CASSANDRA_4909 {
    public static final Charset charset = Charset.forName(""UTF-8"");
    public static final CharsetEncoder encoder = charset.newEncoder();
    public static final CharsetDecoder decoder = charset.newDecoder();

    private static final String KEYSPACE_NAME = ""test_cassandra_4909_ks"";
    private static final String TABLE_NAME = ""test_cassandra_4909_tlb1"";
    private static Connection _con = null;
    private static Cassandra.Client _client = null;


    @BeforeClass
    public static void setUp() throws Exception {
        TTransport tr = new TFramedTransport(new TSocket(""localhost"", 9160));
        TProtocol proto = new TBinaryProtocol(tr);
        _client = new Cassandra.Client(proto);
        tr.open();
        _client.set_cql_version(""3.0.0"");
        _client.set_keyspace(""system"");

        // Create Keyspace
        executeCQL_legacy(""CREATE KEYSPACE "" + KEYSPACE_NAME + "" WITH strategy_class = SimpleStrategy AND strategy_options:replication_factor = 1"");

        // Switch default KS
        _client.set_keyspace(KEYSPACE_NAME);
    }

    @AfterClass
    public static void tearDown() throws Exception {
        if (_client != null) {
            executeCQL3(""DROP KEYSPACE "" + KEYSPACE_NAME);
        }
    }

    @org.junit.Test
    public void test1() throws Exception {
        assertNotNull(""Expected a connection to be defined?"", _client);

        executeCQL3(""CREATE TABLE "" + TABLE_NAME + ""\n"" +
                ""(A1 set<text>,\n"" +
                ""A2 set<text>,\n"" +
                ""B1 text PRIMARY KEY,\n"" +
                ""B2 text);"");

        //Should fail: InvalidRequestException(""Indexes on collections are no yet supported"")
        try {
            executeCQL3(""CREATE INDEX "" + TABLE_NAME + ""_INX1 ON "" + TABLE_NAME + "" (A1)"");
        } catch (InvalidRequestException e) {
            assertEquals(""Unexpected message in exception"", ""Indexes on collections are no yet supported"", e.getWhy());
        }

        //Should succeed
        executeCQL3(""CREATE INDEX "" + TABLE_NAME + ""_INX2 ON "" + TABLE_NAME + "" (B2)"");

        executeCQL3(""INSERT INTO "" + TABLE_NAME + "" (A1, A2, B1, B2)\n"" +
                ""VALUES({'A1-ROW1-ELM-1', 'A1-ROW1-ELM2'}, {'A2-ROW1-ELM-1', 'A2-ROW1-ELM2'}, 'B1-ROW1', 'B2-ROW1' );\n"");

        executeCQL3(""INSERT INTO "" + TABLE_NAME + "" (A1, A2, B1, B2)\n"" +
                ""VALUES({'A1-ROW2-ELM-1', 'A1-ROW2-ELM2'}, {'A2-ROW2-ELM-1', 'A2-ROW2-ELM2'}, 'B1-ROW2', 'B2-ROW2' );\n"");

        // The select would fail with InvalidRequestException(why:No indexed columns present in by-columns clause with Equal operator)
        // if the index is not in effect.
        CqlResult r = executeCQL3(""SELECT B1 FROM "" + TABLE_NAME + "" WHERE B2 = 'B2-ROW2';"");
        List<CqlRow> rows = r.getRows();
        CqlRow row = rows.get(0);
        Column c_b1 = row.getColumns().get(0); // B1
        String B1_value = bb2str(c_b1.bufferForValue());
        assertEquals(""Expected other value for B1"", ""B1-ROW2"", B1_value);
        int dummy = 0;
    }

    public static String bb2str(ByteBuffer buffer) {
        String data = """";
        try {
            int old_position = buffer.position();
            data = decoder.decode(buffer).toString();
            buffer.position(old_position);
        } catch (Exception e) {
            e.printStackTrace();
            return """";
        }
        return data;
    }

    private static ByteBuffer str2bb(String str) {
        try {
            return ByteBuffer.wrap(str.getBytes(""UTF-8""));
        } catch (UnsupportedEncodingException e) {
            throw new RuntimeException(""UTF-8 is unavailable?"", e);
        }
    }

    private static CqlResult executeCQL_legacy(String cql) throws Exception {
        return _client.execute_cql_query(str2bb(cql), Compression.NONE);
    }

    private static CqlResult executeCQL3(String cql) throws Exception {
        return _client.execute_cql3_query(str2bb(cql), Compression.NONE, ConsistencyLevel.ALL);
    }
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Short read protection don't count columns correctly for CQL3,CASSANDRA-4882,12614100,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,pmcfadin,pmcfadin,30/Oct/12 23:08,12/Mar/19 14:17,13/Mar/19 22:27,31/Oct/12 17:50,1.2.0 beta 2,,,,,0,,,,,,,"Using the same schema defined in https://issues.apache.org/jira/browse/CASSANDRA-4881

select * from video_event;
 videoid_username                           | event | event_timestamp | video_timestamp
--------------------------------------------+-------+-----------------+--------------------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start |    1346636100.0 | 2012-09-02 18:35:00+0000
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start |    1346634300.0 | 2012-09-02 18:05:00+0000
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd |  stop |    1346636250.0 | 2012-09-02 18:37:30+0000
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd |  stop |    1346634330.0 | 2012-09-02 18:05:30+0000

And this:

select * from video_event where videoid_username = '99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd' limit 1;
 videoid_username                           | event | event_timestamp | video_timestamp
--------------------------------------------+-------+-----------------+-----------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start |    1346636100.0 |            null

As you can see, we have some different output based on the select statement. The timestamp in both is formatted as a float or a double. ",,,,,,,,,,,,,,,,,,,31/Oct/12 14:15;slebresne;4882.txt;https://issues.apache.org/jira/secure/attachment/12551542/4882.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-30 23:10:49.292,,,no_permission,,,,,,,,,,,,253228,,,Wed Oct 31 17:50:05 UTC 2012,,,,,,0|i0dclj:,75970,jbellis,jbellis,,,,,,,,,,"30/Oct/12 23:10;brandon.williams;Formatting appears fixed in cqlsh trunk, but the null is strange.","31/Oct/12 14:15;slebresne;So I'm ignoring the formatting issue since that has been fixed.

Howe the returned null is a legit bug. The problem is that the StorageProxy short read protection hasn't been updated after the changed made by CASSANDRA-3647, namely the fact that SliceQueryFilter now group columns by prefix before counting them.

Attaching a patch that fixes that.","31/Oct/12 17:19;jbellis;LGTM.

Nit: would prefer avoiding allocating unnecessary HashSet in trim when the common case is not to need to trim.","31/Oct/12 17:50;slebresne;Committed with nit fixed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Endless loop flushing+compacting system/schema_keyspaces and system/schema_columnfamilies,CASSANDRA-4880,12614077,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,minaguib,minaguib,30/Oct/12 20:18,12/Mar/19 14:17,13/Mar/19 22:27,24/Jan/14 18:47,1.1.7,1.2.0 beta 3,,,,2,,,,,,,"After upgrading a node from 1.1.2 to 1.1.6, the startup sequence entered a loop as seen here:

http://mina.naguib.ca/misc/cassandra_116_startup_loop.txt

Stopping and starting the node entered the same loop.

Reverting back to 1.1.2 started successfully.","Linux x86_64 3.4.9, sun-jdk 1.6.0_33",,,,,,,,,,,,,,,,,,03/Dec/13 12:12;tiberiusteng;131203-schema-1.txt;https://issues.apache.org/jira/secure/attachment/12616759/131203-schema-1.txt,03/Dec/13 12:12;tiberiusteng;131203-schema-2.txt;https://issues.apache.org/jira/secure/attachment/12616760/131203-schema-2.txt,20/Nov/12 22:30;xedin;CASSANDRA-4880-fix.patch;https://issues.apache.org/jira/secure/attachment/12554418/CASSANDRA-4880-fix.patch,14/Nov/12 22:14;xedin;CASSANDRA-4880.patch;https://issues.apache.org/jira/secure/attachment/12553575/CASSANDRA-4880.patch,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-11-08 18:09:58.73,,,no_permission,,,,,,,,,,,,253202,,,Fri Jan 24 14:07:06 UTC 2014,,,,,,0|i0dcen:,75939,brandon.williams,brandon.williams,,,,,,,,,,"08/Nov/12 18:09;mheffner;We also see this after upgrading a node from 1.1.0 -> 1.1.6 in our ring. We also saw increased flush writes begin continuously on the other 1.1.0 nodes in the ring after the 1.1.6 was started, but not as rapidly as on the 1.1.6 node. There were also periodic HintedHandoffs occurring around the ring after the 1.1.6 node came up.",08/Nov/12 18:34;jbellis;Is schema synchronization confused?  I can't think of any other reason for continuous flush here.,"08/Nov/12 19:20;brandon.williams;I'm pretty sure that's what's happening, but it's not clear why.  You can artificially reproduce this by manually injecting a conflict, such as a ""Column family ID mismatch"" though that error isn't present here.",08/Nov/12 21:04;brandon.williams;Bisects to CASSANDRA-4561,08/Nov/12 21:14;xedin;can you try 1.1.6 + CASSANDRA-4837 ?,08/Nov/12 21:42;brandon.williams;No help :(,08/Nov/12 21:45;xedin;[~brandon.williams] Can you attach schema_* sstables that could trigger this behavior to the ticket please (if you have them)?,08/Nov/12 21:50;brandon.williams;Sent to Pavel privately.,"09/Nov/12 02:03;xedin;[~mheffner] and [~minaguib] Can you please execute following commands in CLI and post output here - ""use system;"" and ""list schema_keyspaces""?","09/Nov/12 02:37;minaguib;[~xedin] Here's mine (after some very minor name obfuscation):

{code}
[default@system] list schema_keyspaces;
Using default limit of 100
Using default column limit of 100
-------------------
RowKey: AdKS
=> (column=durable_writes, value=true, timestamp=1336233003898)
=> (column=name, value=AdKS, timestamp=1336233003898)
=> (column=strategy_class, value=org.apache.cassandra.locator.NetworkTopologyStrategy, timestamp=1336233003898)
=> (column=strategy_options, value={""DCMTL"":""2"",""DCLA"":""2"",""DCLON"":""2""}, timestamp=1336233003898)

1 Row Returned.
Elapsed time: 3 msec(s).
{code}

Tomorrow I'll run a test with cassandra 1.1.6 + CASSANDRA-4837 just to add a second vote to Brandon's test whether it helps or not.","09/Nov/12 03:00;mheffner;This is after we removed the 1.1.6 node, but:

{code}
Using default limit of 100
Using default column limit of 100
-------------------
RowKey: KSName
=> (column=durable_writes, value=true, timestamp=5397847727343)
=> (column=name, value=KSName, timestamp=5397847727343)
=> (column=strategy_class, value=org.apache.cassandra.locator.NetworkTopologyStrategy, timestamp=5397847727343)
=> (column=strategy_options, value={""us-east"":""3""}, timestamp=5397847727343)

1 Row Returned.
Elapsed time: 132 msec(s).
{code}","09/Nov/12 05:09;xedin;it looks like it wasn't a good idea to involve timestamps in computing schema version (was super easy to do), I will work on improving things to make a migration from older versions (which System.nanoTime() bug) as smooth as possible.","14/Nov/12 22:14;xedin;I made start timestamp schema timestamp repair account and fix any timestamps that are grater than current timestamp and introduced new version in MessagingVersion as it's the only way to fix infinite loop in schema due to broken nano timestamps that we can't properly fix in <= 1.1.6.

Edit: Also note that once this is committed we would have to increase VERSION number for cassandra-1.2.",14/Nov/12 22:15;xedin;Assigning Brandon as reviewer because he was initially involved.,"15/Nov/12 17:24;brandon.williams;Hmm, this unfortunate but I don't see any way around it, either.  +1",16/Nov/12 22:36;xedin;Committed.,"20/Nov/12 21:30;brandon.williams;I think something's still wrong in 1.1, concurrent schema changes aren't settling: http://buildbot.datastax.com:8010/builders/cassandra-1.1/builds/567/steps/shell/logs/stdio

Everything works in 1.2.",20/Nov/12 21:35;xedin;Could that be that those two stub nodes running on different versions - one on recent and one on previous? That would explain why they have disagreement. It's tested on rolling restart by CASSANDRA-4837.,"20/Nov/12 21:39;brandon.williams;Nope, those tests all run with the same version.","20/Nov/12 21:42;xedin;Interesting... Ok, I will try to figure it out asap.","20/Nov/12 22:30;xedin;We don't need special RM deserialization handler any more, verified that tests are now passing.",21/Nov/12 19:37;brandon.williams;+1,22/Nov/12 00:02;xedin;Committed.,"03/Dec/13 12:11;tiberiusteng;I have encountered similar bug after I upgrade one of my 1.2.2 node to 1.2.12.
Was using FreeBSD 8.2 + diablo-jre-1.6.0.07.02_18.

Before I upgrading the node to 1.2.12, I changed JVM to openjdk-7.25.15_2 (in retrospective probably not a good idea ...), saw flipping Memtable flushes, changes JVM back to diablo-jre-1.6.0.07.02_18, and still seeing rapid flushes. Now I've taken the 1.2.12 node offline (but not decommissioned it).

After that I see tens of Memtable flushes on the 1.2.12 per second, while once or twice a second on nodes with 1.2.2.

Attached files are the flipping schema versions.","06/Dec/13 08:11;tiberiusteng;Run the node with 1.2.2 again, and the schema converged. Further upgrading JVM to openjdk-7.25.15_2 and Cassandra to 1.2.12 didn't reproduce the problem. Strange, but at least my cluster now upgrading fine ...","22/Jan/14 01:08;cscetbon;I can easily reproduce it by having one DC with 1.2.2 and another with 1.2.12/1.2.13. see http://pastebin.com/YZKUQLXz
If I grep for only InternalResponseStage logs I get http://pastebin.com/htnXZCiT which always displays same account of ops and serialized/live bytes per column family. Column family system/schema_columns is also concerned by this issue.","24/Jan/14 08:28;cscetbon;When I upgrade one node from 1.2.2 to 1.2.13 for 2h I get the previous messages with a raise of CPU(as it flush and compact continually) on all nodes http://picpaste.com/pics/Screen_Shot_2014-01-24_at_09.18.50-ggcCDVqd.1390551670.png
After that, everything is fine and I can upgrade other nodes without any important raise of cpus load. when I start the upgrade, the more nodes I upgrade at the same time (at the beginning), the higher the cpu load is http://picpaste.com/pics/Screen_Shot_2014-01-23_at_17.45.56-I3fdEQ2T.1390552036.png","24/Jan/14 13:46;brandon.williams;This ticket was closed more than a year ago.  Please open a new ticket if you encounter the issue, because it's probably not related to this one.",24/Jan/14 14:07;cscetbon;Done. see [CASSANDRA-6614|https://issues.apache.org/jira/browse/CASSANDRA-6614],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't throw internal exceptions over JMX,CASSANDRA-4893,12614410,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,nickmbailey,nickmbailey,01/Nov/12 18:33,12/Mar/19 14:17,13/Mar/19 22:27,21/Nov/12 23:13,1.2.0 beta 3,,,,,0,,,,,,,"Similarly to how we don't return internal objects over JMX we shouldn't throw internal exceptions over jmx as well.

The one example I encountered was throwing ConfigurationException for the move() command. We should check the rest of our jmx as well.",,,,,,,,,,,,,,,,,,,19/Nov/12 17:31;yukim;4893-1.2.txt;https://issues.apache.org/jira/secure/attachment/12554198/4893-1.2.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-16 22:16:12.841,,,no_permission,,,,,,,,,,,,253738,,,Wed Nov 21 23:13:10 UTC 2012,,,,,,0|i0e4wv:,80558,dbrosius,dbrosius,,,,,,,,,,"16/Nov/12 22:16;yukim;Patch is against 1.2 branch. It removes or cassandra related exceptions or replace them with standard ones.
It may be better to put this into 1.2.0 since it changes method signatures which we don't want to do it in minor versions.","17/Nov/12 22:59;jbellis;I want to get rc1 out really, really badly.  If you can expedite review of this [~nickmbailey] that would be great.  Otherwise, we've waited this long, it won't kill us to push it to 1.3.","18/Nov/12 03:41;dbrosius;doesn't doing

throw new IllegalArgumentException(e);

still exhibit the problem of needing the underlying exception class of 'e' in the classpath of the caller?

it seems the thrown exception needs to lose it's exception history.","19/Nov/12 17:31;yukim;Dave, thanks for review.
Updated patch not to wrap cause and instead use just error message.","20/Nov/12 01:59;dbrosius;Cassandra throws a few custom exceptions that are non-checked. Not sure if any of these places can have these exceptions, but if the were, then we would still have bad exception types going back to jmx. To be paranoid, catching just 'Exception' in these circumstances would be needed. Don't know if this is a real issue tho.

Some of these methods are used as 'real code' outside of jmx, and thus we are weakening the exception model in these spots. Most of these look like client/tool code so i guess it's ok.

nit: not a fan of dangling statements after a try/catch->exit, as in setCompactionStrategyClass, i'd move maybeReloadCompactionStrategy into the try.


otherwise, if the above first issue, isn't an issue... lgtm.","21/Nov/12 23:13;yukim;Committed to 1.2.0 and above with nit fixed.
Thanks for review!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL help in trunk/doc/cql3/CQL.textile outdated,CASSANDRA-4879,12614076,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,khahn,khahn,30/Oct/12 20:07,12/Mar/19 14:17,13/Mar/19 22:27,06/Nov/12 14:14,1.2.0 beta 3,,,,,0,documentation,,,,,,"https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile doesn't include the new create keyspace syntax or the collections. Last time, I updated the CQL.textile for Paul Cannon to review. Want me to do it again? 

BNR-like formatting needs to be replaced, right?, because the brackets now have literal meaning. I test-applied this custom formatting to commands and it seems ok: Uppercase means literal (lowercase nonliteral), italics mean optional, the | symbol means OR, ... means repeatable. The ... in italics doesn't strictly explain things like nested [...] does, but it's easier on the eyes and loosely understandable. Any doubt could be erased by examples, I think. ",,,,,,,,,,,,,,,,,,,05/Nov/12 16:53;urandom;v2-0001-CASSANDRA-4879-update-CQL-doc-for-collections.txt;https://issues.apache.org/jira/secure/attachment/12552125/v2-0001-CASSANDRA-4879-update-CQL-doc-for-collections.txt,05/Nov/12 16:53;urandom;v2-0002-update-CREATE-KEYSPACE-for-map-syntax.txt;https://issues.apache.org/jira/secure/attachment/12552126/v2-0002-update-CREATE-KEYSPACE-for-map-syntax.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-31 14:25:39.782,,,no_permission,,,,,,,,,,,,253201,,,Tue Nov 06 14:14:27 UTC 2012,,,,,,0|i0dce7:,75937,slebresne,slebresne,,,,,,,,,,"31/Oct/12 14:25;slebresne;That's on my todo list and I'll make sure that's ready for 1.2 release. However:

bq. BNR-like formatting needs to be replaced, right?

I'm not sure about that. The fact that we use '<' and '>' for collection types is not a big deal in the sense that we can quote those characters in the grammar (much like we do when we have other signs like '=', ':', etc...). Since that's a reference documentation, I'd rather have the grammar be as precise as can be (nested [..] do matter) and not rely on things like italics (that don't translate if you say copy-paste the grammar into a text file). Yes, it might be slightly less readable, but that's what the examples are for. Of course the datastax documentation should feel free to use a syntax that is easier on the eye.",05/Nov/12 16:21;urandom;Patch attached to document collection types,"05/Nov/12 16:54;urandom;bq. Patch attached to document collection types

And another to fix {{CREATE KEYSPACE}} for map vs. properties","06/Nov/12 14:14;slebresne;Alright, I've committed those patches with a number of additions:
* I've added a few missing collections operation and I've included the collections to the insert/update/delete syntax
* I've also updated the new map syntax for create table statements.
* I've added the new boolean support
* I've removed the consistency level bits (since it's now in the protocol)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range queries return fewer result after a lot of delete,CASSANDRA-4877,12614009,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,julien_campan,julien_campan,30/Oct/12 09:55,12/Mar/19 14:17,13/Mar/19 22:27,21/Nov/12 13:04,1.2.0 beta 3,,,,,0,,,,,,,"Hi, I'm testing on the trunk version
I'm using : [cqlsh 2.3.0 | Cassandra 1.2.0-beta1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.35.0]

My use case is :
I create a table
CREATE TABLE premier (
id int PRIMARY KEY,
value int
) WITH
comment='' AND
caching='KEYS_ONLY' AND
read_repair_chance=0.100000 AND
dclocal_read_repair_chance=0.000000 AND
gc_grace_seconds=864000 AND
replicate_on_write='true' AND
compression={'sstable_compression': 'SnappyCompressor'};

1) I insert 10 000 000 rows (they are like id = 1 and value =1)
2) I delete 2 000 000 rows (i use random method to choose the key value)
3) I do select * from premier ; and my result is 7944 instead of 10 000.
4) if if do select * from premier limit 20000 ; my result is 15839 .

So after a lot of delete, the range operator is not working.",,,,,,,,,,,,,,,,,,,20/Nov/12 17:31;slebresne;0001-4877.patch;https://issues.apache.org/jira/secure/attachment/12554370/0001-4877.patch,20/Nov/12 17:31;slebresne;0002-Rename-maxIsColumns-to-countCQL3Rows.patch;https://issues.apache.org/jira/secure/attachment/12554371/0002-Rename-maxIsColumns-to-countCQL3Rows.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-11-20 17:31:02.05,,,no_permission,,,,,,,,,,,,253123,,,Wed Nov 21 13:04:45 UTC 2012,,,,,,0|i0dbp3:,75824,jbellis,jbellis,,,,,,,,,,"20/Nov/12 17:31;slebresne;Attaching patch to fix this. The problem is that our handling of LIMIT was still not correct, in particular when NamesQueryFilter where used, as delete rows were wrongfully counted.

One problem with that patch is that we may still under-count in a mixed 1.1/1.2 cluster because 1.1 nodes won't know how to count correctly. That's sad, but at the same time changing this in 1.1 would be hard and dangerous and CQL3 is beta in 1.1 after all.

Note that I'm attaching 2 patches. The first one is the bulk of the fix. The second one is mostly a renaming of the 'maxIsColumn' parameters that is used in a number of place to 'countCQL3Rows' because that describe more faithfully what this parameter actually do.",21/Nov/12 12:07;jbellis;+1,"21/Nov/12 13:04;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assertion error in offheap bloom filter,CASSANDRA-4934,12615341,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,08/Nov/12 15:57,12/Mar/19 14:17,13/Mar/19 22:27,09/Nov/12 21:17,1.2.0 beta 3,,,,,0,,,,,,,"Saw this while running the dtests:

{noformat}
 INFO [CompactionExecutor:2] 2012-11-08 09:35:18,206 CompactionTask.java (line 116) Compacting [SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-9-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-11-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-10-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-13-Data.db')]
ERROR [CompactionExecutor:2] 2012-11-08 09:35:18,257 CassandraDaemon.java (line 132) Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.AssertionError: Memory was freed
    at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:134)
    at org.apache.cassandra.io.util.Memory.getByte(Memory.java:104)
    at org.apache.cassandra.utils.obs.OffHeapBitSet.get(OffHeapBitSet.java:60)
    at org.apache.cassandra.utils.BloomFilter.isPresent(BloomFilter.java:71)
    at org.apache.cassandra.db.compaction.CompactionController.shouldPurge(CompactionController.java:106)
    at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:64)
    at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:95)
    at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:151)
    at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:72)
    at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
    at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
    at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
    at com.google.common.collect.Iterators$8.computeNext(Iterators.java:734)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
    at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:146)
    at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:69)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:179)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,08/Nov/12 19:38;brandon.williams;log.txt;https://issues.apache.org/jira/secure/attachment/12552695/log.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-08 19:35:26.257,,,no_permission,,,,,,,,,,,,255974,,,Fri Nov 09 21:17:55 UTC 2012,,,,,,0|i0g6jz:,92492,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"08/Nov/12 19:35;vijay2win@yahoo.com;Looks like the compaction task is not referencing the SSTR which is understandable, not sure which part of the code is closing more than it is openings.... I can see one, but it is rare... any other error message in the logs (It will speedup the search for bug) :) ? Thanks!","08/Nov/12 19:38;brandon.williams;I didn't see anything else useful, but here's the whole thing.","09/Nov/12 05:41;vijay2win@yahoo.com;From the logs i see the following events: 

at, 2012-11-08 09:35:18,257 compaction failed on 

SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-9-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-11-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-10-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-13-Data.db')]

at, 2012-11-08 09:35:18,290 Compaction completed sucessfully on

SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-9-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-11-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-10-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-12-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-13-Data.db')]

only thing which i can think of is to make Memory peer variable volatile but not sure i am looking at right places.","09/Nov/12 16:12;jbellis;I think the problem is that compaction never bothered to mark overlapped sstables (that it uses for tombstone purge checking) as referenced, so those can get free'd out from under an ongoing compaction.  Before OHBS this was not a problem because the contents of those sstables was never referenced, only the bloom filter, which the GC kept around as long as it was needed.  But now we need to manually manage that.

Patchset pushed to http://github.com/jbellis/cassandra/branches/4934","09/Nov/12 18:41;vijay2win@yahoo.com;+1 (ignore my first comment getOverlappingSSTables will return a diffrent set).
Honestly, i did think about this somehow missed the overlapping SST code path earlier :)",09/Nov/12 21:17;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESC KEYSPACE <ks> from cqlsh won't show cql3 cfs,CASSANDRA-4913,12614835,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,nickmbailey,nickmbailey,05/Nov/12 19:05,12/Mar/19 14:17,13/Mar/19 22:27,12/Nov/12 22:37,1.2.0,,,,,0,,,,,,,I'm assuming because we made 'describe_keyspaces' from thrift not return cql3 cfs.,,,,,,,,,,,,CASSANDRA-4907,CASSANDRA-4924,,,,,,12/Nov/12 04:30;iamaleksey;4913.txt;https://issues.apache.org/jira/secure/attachment/12553070/4913.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-12 04:31:44.685,,,no_permission,,,,,,,,,,,,255329,,,Mon Nov 12 20:49:18 UTC 2012,,,,,,0|i0er1z:,84145,brandon.williams,brandon.williams,,,,,,,,,,12/Nov/12 04:31;iamaleksey;Fixes every issue with DESCRIBE * commands.,"12/Nov/12 20:49;brandon.williams;lgtm, +1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy.getRangeSlice sometimes returns incorrect number of columns,CASSANDRA-4919,12614979,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,pkolaczk,pkolaczk,pkolaczk,06/Nov/12 15:55,12/Mar/19 14:17,13/Mar/19 22:27,12/Nov/12 22:00,1.1.8,1.2.0 beta 3,,,,0,,,,,,,"When deployed on a single node, number of columns is correct.
When deployed on a cluster, total number of returned columns is slightly lower than desired. ",,,,,,,,,,,,,,,,,,,06/Nov/12 15:59;pkolaczk;0001-Fix-getRangeSlice-paging-reset-predicate-after-fetch.patch;https://issues.apache.org/jira/secure/attachment/12552302/0001-Fix-getRangeSlice-paging-reset-predicate-after-fetch.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-06 16:05:45.137,,,no_permission,,,,,,,,,,,,255497,,,Tue Nov 27 20:00:27 UTC 2012,,,,,,0|i0esc7:,84355,jbellis,jbellis,,,,,,,,,,06/Nov/12 15:59;pkolaczk;Attaching a patch fixing paged column iteration.,06/Nov/12 16:05;jbellis;Is there a dtest for this?,"06/Nov/12 16:58;brandon.williams;There's a wide row test and a range slice test, but not a combination of the two.","12/Nov/12 22:00;jbellis;committed, and create https://github.com/riptano/cassandra-dtest/issues/5 to follow up w/ dtest.","20/Nov/12 08:49;slebresne;I note for the testing part that this only concern getRangeSlice with the isPaging option (i.e. for thrift, get_paged_slice) and that currently CQL never calls that so this is not reproducible with CQL. We may end up using the isPaging thing in CQL with CASSANDRA-4851 however.",27/Nov/12 20:00;tpatterson;Just added wide_slice_test to putget_test.py in the dtests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove CQL3 arbitrary select limit,CASSANDRA-4918,12614919,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,06/Nov/12 08:39,12/Mar/19 14:17,13/Mar/19 22:27,19/Nov/12 16:57,1.2.0 beta 3,,,,,0,,,,,,,"Let it be clear however that until CASSANDRA-4415 is resolved, it will put us in a situation where it will be easy to write queries that timeout (and potentially OOM the server). That being said, even with the auto-limit it's not too hard to write queries that timeout if you're not at least a bit careful and so far we've always answer that by saying 'you have to be mindful of how much data your query is asking for'. And while I'm all for adding protection against OOMing the server like suggested by Jonathan on CASSANDRA-4304, I think the arbitrary auto-limit is the worst possible solution to this problem.

Note that until CASSANDRA-4415 is resolved I wouldn't be totally opposed to force people to provide a LIMIT to select queries if we're really thing it will avoids lots of surprise, though tbh I do think it would be enough to just continue to be vocal about the fact that 'you have to be mindful of how much data your query is asking for' and its follow-up 'you should use an explicit LIMIT if in doubt about how much data will be returned'.

But I am *strongly opposed* in keeping the current arbitrary limit because it makes very little sense imo, and the little sense it makes will completely vanish once CASSANDRA-4415 is here, and I don't want to break the API and do a CQL4 to be able to remove that limit later.
",,,,,,,,,,,,,,,,,,,06/Nov/12 08:43;slebresne;4918.txt;https://issues.apache.org/jira/secure/attachment/12552245/4918.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-15 17:55:21.779,,,no_permission,,,,,,,,,,,,255431,,,Mon Nov 19 16:57:02 UTC 2012,,,,,,0|i0erwn:,84285,jbellis,jbellis,,,,,,,,,,06/Nov/12 08:43;slebresne;Trivial patch attached,"15/Nov/12 17:55;jbellis;How about we remove this server-side, but add it to cqlsh?  (Maybe even cut it down to 1K rows there.)  That would address my worries about OOMing the server adequately.
","15/Nov/12 17:56;jbellis;(that is: if querystring does not contain {{limit}}, cqlsh could add on {{LIMIT 1000}} as a default.)","16/Nov/12 11:36;slebresne;I'm clearly fine with that, though while the server-side patch is attached, I'd rather let the cqlsh part to someone that knows the cqlsh code better if possible.",19/Nov/12 10:25;jbellis;+1 then.  Created CASSANDRA-4972 for the cqlsh change.  (Would like to get that into 1.2.0 but it's not critical.),"19/Nov/12 16:57;slebresne;Alright, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""AssertionError: Wrong class type"" at CounterColumn.diff()",CASSANDRA-4976,12616849,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jblangston@datastax.com,jblangston@datastax.com,19/Nov/12 22:30,12/Mar/19 14:17,13/Mar/19 22:27,23/Nov/12 09:03,1.1.7,,,,,0,counters,,,,,,"Thousands of the following errors are getting logged to system.log:

ERROR [ReadRepairStage:150152] 2012-11-15 12:31:02,815 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[ReadRepairStage:150152,5,main]
java.lang.AssertionError: Wrong class type.
        at org.apache.cassandra.db.CounterColumn.diff(CounterColumn.java:110)
        at org.apache.cassandra.db.ColumnFamily.diff(ColumnFamily.java:248)
        at org.apache.cassandra.db.ColumnFamily.diff(ColumnFamily.java:342)
        at org.apache.cassandra.service.RowRepairResolver.scheduleRepairs(RowRepairResolver.java:117)
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:94)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

There are also many of the following errors intermingled with the above:

ERROR [ReadRepairStage:150158] 2012-11-15 12:30:34,148 CounterContext.java (line 381) invalid counter shard detected; (b29a5480-e911-11e1-0000-ce481d6d2aff, 3, 916) and (b29a5480-e911-11e1-0000-ce481d6d2aff, 3, -1590) differ only in count; will pick highest to self-heal; this indicates a bug or corruption generated a bad counter shard

I am not 100% sure whether they are related.",,,,,,,,,,,,,,,,,,,22/Nov/12 10:48;slebresne;4976.txt;https://issues.apache.org/jira/secure/attachment/12554666/4976.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-22 10:48:02.733,,,no_permission,,,,,,,,,,,,258721,,,Fri Nov 23 09:03:43 UTC 2012,,,,,,0|i0l2m7:,121074,jbellis,jbellis,,,,,,,,,,"22/Nov/12 10:48;slebresne;I think that assertion is wrong and has always been (it's definitively possible that diff is called on a tombstone). Attaching patch to fix.

For the second type of errors, this is CASSANDRA-4417 and I'm almost certain this is not related (or rather if it is, I have no clue how).",23/Nov/12 02:34;jbellis;+1,"23/Nov/12 09:03;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in Bounds when running word count,CASSANDRA-4975,12616796,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,19/Nov/12 20:02,12/Mar/19 14:17,13/Mar/19 22:27,26/Nov/12 23:07,1.2.0 beta 3,,,,,0,,,,,,,"Just run the word count in examples:

{noformat}
ERROR 20:01:24,693 Exception in thread Thread[Thrift:16,5,main]
java.lang.AssertionError: [DecoratedKey(663380155395974698, 6b6579333137),max(-8100212023555812159)]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:41)
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:34)
        at org.apache.cassandra.thrift.CassandraServer.get_paged_slice(CassandraServer.java:1041)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_paged_slice.getResult(Cassandra.java:3478)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_paged_slice.getResult(Cassandra.java:3466)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,26/Nov/12 18:24;brandon.williams;4975.txt;https://issues.apache.org/jira/secure/attachment/12554891/4975.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-26 22:18:13.546,,,no_permission,,,,,,,,,,,,258666,,,Mon Nov 26 23:07:06 UTC 2012,,,,,,0|i0l23b:,120989,yukim,yukim,,,,,,,,,,"26/Nov/12 18:24;brandon.williams;WordCount is using RP, causing invalid bounds for M3P.  Something seems too fragile here since the job hangs forever after the AE, but for rc1 let's just change the partitioner to M3P.  Patch to do so.","26/Nov/12 22:18;yukim;+1 for patch, but you missed WordCountCounters.java. You need to change that too.","26/Nov/12 23:07;brandon.williams;Oops, thanks.  Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error opening data file at startup,CASSANDRA-4984,12617298,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,carlyeks,zenek_kraweznik0,zenek_kraweznik0,22/Nov/12 09:17,12/Mar/19 14:17,13/Mar/19 22:27,10/Dec/12 15:00,1.1.8,,,,,0,,,,,,,"I've found this in logfile, this happens at cassandra startup:

INFO 10:06:13,670 Opening /var/lib/cassandra/data/MYKSPC/MYCF/MYKSPC-MYCF-hf-5547 (1073761823 bytes)
ERROR 10:06:13,670 Exception in thread Thread[SSTableBatchOpen:3,5,main]
java.lang.AssertionError
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:166)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:153)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:242)
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)


Every CF in this Keyspace has cashing set to 'NONE'",Oracle Java 1.6u37,,,,,,,,,,,,,,,,,,08/Dec/12 23:03;carlyeks;4984.patch;https://issues.apache.org/jira/secure/attachment/12560050/4984.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-08 23:05:28.394,,,no_permission,,,,,,,,,,,,259511,,,Mon Dec 10 15:00:44 UTC 2012,,,,,,0|i0lq53:,124886,jbellis,jbellis,,,,,,,,,,23/Nov/12 11:00;zenek_kraweznik0;It happens after offline scrub too.,08/Dec/12 23:05;carlyeks;The assertion is being raised because the primary index of the file is missing. Attaching trivial patch to make the assertion error more descriptive.,"10/Dec/12 15:00;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: alter table add column with table that has collection fails,CASSANDRA-4982,12617225,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,dbrosius,dbrosius,21/Nov/12 22:03,12/Mar/19 14:17,13/Mar/19 22:27,26/Nov/12 09:25,1.2.0 beta 3,,,,,0,,,,,,,"create keyspace collections with replication = {'class':'SimpleStrategy', 'replication_factor':1};
use collections;
create table collections (key int primary key, aset set<text>);
insert into collections (key, aset) values (1, {'fee', 'fi'});
alter table collections add aaa text;
 
 
ERROR 16:52:33,792 Error occurred during processing of message.
java.lang.UnsupportedOperationException: ColumnToCollectionType should only be used in composite types, never alone
        at org.apache.cassandra.db.marshal.ColumnToCollectionType.validate(ColumnToCollectionType.java:103)
        at org.apache.cassandra.config.CFMetaData.validate(CFMetaData.java:1094)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:202)
        at org.apache.cassandra.cql3.statements.AlterTableStatement.announceMigration(AlterTableStatement.java:217)
        at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:73)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:132)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:140)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1706)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)",,,,,,,,,,,,,,,,,,,23/Nov/12 10:42;slebresne;4982.txt;https://issues.apache.org/jira/secure/attachment/12554721/4982.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-23 10:42:43.823,,,no_permission,,,,,,,,,,,,259429,,,Mon Nov 26 09:25:32 UTC 2012,,,,,,0|i0lp33:,124714,dbrosius,dbrosius,,,,,,,,,,23/Nov/12 10:42;slebresne;Trivial patch attached.,24/Nov/12 04:50;dbrosius;fixed... LGTM.,"26/Nov/12 09:25;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Less than operator when comparing timeuuids behaves as less than equal.,CASSANDRA-4936,12615408,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,cesar.nataren,cesar.nataren,09/Nov/12 01:13,12/Mar/19 14:17,13/Mar/19 22:27,18/Jan/13 13:02,1.2.1,,,,,0,,,,,,,"If we define the following column family using CQL3:

CREATE TABLE useractivity (
  user_id int,
  activity_id 'TimeUUIDType',
  data text,
  PRIMARY KEY (user_id, activity_id)
);

Add some values to it.

And then query it like:

SELECT * FROM useractivity WHERE user_id = '3' AND activity_id < '2012-11-07 18:18:22-0800' ORDER BY activity_id DESC LIMIT 1;

the record with timeuuid '2012-11-07 18:18:22-0800' returns in the results.

According to the documentation, on CQL3 the '<' and '>' operators are strict, meaning not inclusive, so this seems to be a bug.","Linux CentOS.

Linux localhost.localdomain 2.6.18-308.16.1.el5 #1 SMP Tue Oct 2 22:01:37 EDT 2012 i686 i686 i386 GNU/Linux",,,,,,,,,,,,,,,,,,11/Jan/13 13:30;slebresne;4936.txt;https://issues.apache.org/jira/secure/attachment/12564408/4936.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-09 01:58:28.665,,,no_permission,,,,,,,,,,,,256686,,,Fri Jan 18 13:02:55 UTC 2013,,,,,,0|i0iqen:,107372,thobbs,thobbs,,,,,,,,,,09/Nov/12 01:58;jbellis;Are you sure the UUIDs do not differ in their non-time-based component?  See CASSANDRA-4284.,"09/Nov/12 07:12;slebresne;This reminds me, I think that at the minimum we should provide a cqlsh option to display timeuuid as UUID rather than date. Otherwise it's painful to even check that the actual UUID differs. Though tbh I'm still not sure using dates by default is the best idea given this is a lossy representation and I'm afraid will confuse more people not used to type 1 UUID than anything else. Unless of course we adopt of the the format discussed in CASSANDRA-4284 (alternatively, we could just display the date *and* the uuid in parenthesis, though it'll be verbose). ",09/Nov/12 19:23;cesar.nataren;Jonathan: What's the easiest way to find that out?,"10/Nov/12 00:06;thobbs;bq. Are you sure the UUIDs do not differ in their non-time-based component?

(Note: I suggested that Cesar open this ticket.)  It almost certainly is a problem with the non-time-based component.  However, the intention of the user is pretty clear for this query: select anything where the timestamp component is less than the provided one.

I've been handling this specially in pycassa for quite a while.  If the comparator is TimeUUID, and a datetime/timestamp is passed as a slice end, it will create a TimeUUID with special non-timestamp components.  For example, since slice ends are inclusive in the thrift api, when creating the TimeUUID for the slice 'finish', the highest possible TimeUUID with the given timestamp will be used.

We could do something similar with CQL, it would just require passing along information about whether to create the highest or lowest TimeUUID representation for a given datestamp based on the comparison operator that's used.

If details about what non-timestamp components pycassa uses would be useful, let me know.","11/Jan/13 08:53;slebresne;I really think the bug here is that TimeUUIDType.fromString() accepts a date as input. But a date is *not* a valid representation of a timeuuid, and the fromString method does *arbitrarily* pick some 0's for parts of the resulting UUID.

In other words, the SELECT query above should be invalid.

Now don't get me wrong, selecting timeuuid based on dates is useful but that is a slightly different problem. So what I think we should do is:
# refuse dates as valid timeuuid values because they just are not.
# add convenience methods to translate dates to precise timeuuid. For querying we would have 'startOf()' and 'endOf()' (where 'startOf(<date> )' (resp. 'endOf(<date> )') would return the *smallest* (resp. *biggest*) possible timeuuid at time <date> ). And for insertion we could optionally add 'random(<date> )' that would return a random timeuuid at time '<date>' (we could even accept 'now' as syntactic sugar for 'random(now)' if we feel like it).

That would also mean that cqlsh should stop this non-sense of displaying timeuuid like date. Again, I understand the intention of making it more readable but this will confuse generations of CQL3 users. I do am in favor of finding a non confusing way to make it readable for users. In fact one solution could be to handle that on the CQL side and to allow 'SELECT dateOf(x ) FROM ...' that would return a date string from timeuuid x (but now it's clear, you've explicitly asked for a lossy representation of x).

I note that this suggestion pretty much fixes the problem discussed in CASSANDRA-4284 too.

I note that Tyler's solution of basically automatically generating the startOf() and endOf() method under the cover based on whether we've use an inclusive of exclusive operation may appear seductive but I don't think we should do that because:
# if you do that, what about SELECT ... WHERE activity_id = '2012-11-07 18:18:22-0800'. You still have no solution for that and by doing magic under the carpet for < and >, you've in fact blurred what = really does.
# ""it would just require passing along information about whether to create the highest or lowest TimeUUID representation for a given datestamp based on the comparison operator that's used"" <- while this seem simple on principle, this will yield very *very* ugly special cases internally. This is *not* 2 lines of code.
# more generally, this doesn't solve the fact that date *are not* valid representation of timeuuid. For example, I still think the first point mentioned in CASSANDRA-4284 is a bug in its own right.

Allowing dates as valid representation of timeuuid is a bug, let's fix it.
","11/Jan/13 13:29;slebresne;Attaching a patch that implements what my previous comment describes.

A few precisions:
* the patch allows {{startOf('2012-11-07 18:18:22-0800')}} or even {{startOf(12432343423)}} but not {{startOf(? )}}. It's just that it's simpler to not support prepared marker for now. We can add it later, but I'd rather leave it for later. It's not excessively useful anyway since any good library will provide an equivalent to the {{startOf()}} method anyway (and so you can use that client side for prepared statements).
* the patch change the fact that dates are accepted as valid TimeUUID representation because, as arged previously, this is bogus. However, CQL2 has done that bogus thing to, and I'm not sure it's worth fixing there as there might be people relying on that buggy behavior. So the patch maintain the buggy behavior fro CQL2.
* I talks about adding a 'random(date)' method for insertions sake in my previous comment, but thinking about that a bit more, I'm not sure it's a good idea. Namely, the only way to generate a version 1 UUID according to the spec is based on the current time. Generating one from a timestamp is kind of not safe. Now I admit that if you use the timestamp but randomize all other bits, you probably end up with something having virtually no chance of collision, but still, I'm slightly reluctant to do that in Cassandra. I'd rather let people do that client side (and provide a UUID string) if they really want to. So instead the patch only provides a {{now()}} method that generate a new unique timeuuid based on the current time.
* The patch also adds the conversion for select statement I mention in my previous comment. In fact it adds 2 methods {{dateOf()}} and {{unixTimestampOf()}}. This part is kind of optional and I can rip it out if there is objections (I meant to separate in 2 patches but scewed up and got lazy). That being said, I kind of like it and with that I think we can just have cqlsh stop printing timeUUID as dates (which the patch doesn't include however).

Let's not shy away from the fact that this patch kinds of break backward compatibility. I say ""kinds of"" because as I've said, I really think allowing date litterals as timeuuid values is a bug so I really think this patch is a bug fix. And if we get that in 1.2.1, I really don't think they'll be any arm done. I also note that there isn't any way to fix this issue that don't ""break backward compatibility"". So I'm really sorry I didn't got that fix up before 1.2.0, but I still really think we should do it nonetheless.","11/Jan/13 18:19;thobbs;I agree with all of your points.  The one thing I might change are the function names {{startOf()}} and {{endOf()}}.  Since these functions are dealing with dates and times, I think the names suggest they might be altering the time component.  Perhaps {{minTimeUUID()}} and {{maxTimeUUID()}}?

Regarding backwards compatibility: are we carefully tracking changes to CQL by version and documenting them somewhere?  These kinds of changes need to be easily discoverable, even if they are only considered bugfixes.","14/Jan/13 09:43;slebresne;bq. Perhaps minTimeUUID() and maxTimeUUID()?

Agreed, that's probably less confusing. That's trivial to change though, I'll change before committing if it comes to that.

bq. are we carefully tracking changes to CQL by version and documenting them somewhere?

Not yet, but I do intend to do it now that CQL3 is final. It'll probably be listed in http://cassandra.apache.org/doc/cql3/CQL.html. Though if that patch is committed, I'll also mention it clearly in the NEWS file.","16/Jan/13 08:27;jbellis;Tyler, can you review the patch?","16/Jan/13 16:33;thobbs;Yes, I can review it.","17/Jan/13 22:45;thobbs;+1 on the patch

I'd like to include the {{dateOf()}} and {{unixTimestampOf()}} functions; do you want update cqlsh here to not automatically convert timeUUIDs to a date format?","18/Jan/13 13:02;slebresne;Alright, committed, thanks

bq. I'd like to include the dateOf() and unixTimestampOf() functions

I've left them in the commit. I've also renamed startOf/endOf to minTimeUUID/maxTimeUUID as suggested above.

bq. do you want update cqlsh here to not automatically convert timeUUIDs to a date format?

As that was simple, I did that too with the commit. Though if someone more knowledgeable of cqlsh wants to have a look at the commit and make sure I didn't forget something, that'd be appreciated.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgraded cassandra loses all cfs on restart,CASSANDRA-5061,12623752,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,13/Dec/12 14:02,12/Mar/19 14:17,13/Mar/19 22:27,13/Dec/12 16:53,1.1.8,,,,,0,,,,,,,"A bit dramatic summary, but hey;

If you upgrade cassandra and then restart it, you lose all your CFs, but they come back if you restart again.

This is due to fixSchemaNanoTimestamp not flushing the new data after truncating the CF and re-doing the mutations.",,,,,,,,,,,,,,,,,,,13/Dec/12 14:03;krummas;0001-flush-data-after-fixing-nano-timestamps.patch;https://issues.apache.org/jira/secure/attachment/12560779/0001-flush-data-after-fixing-nano-timestamps.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-13 16:53:32.118,,,no_permission,,,,,,,,,,,,297451,,,Thu Dec 13 16:53:32 UTC 2012,,,,,,0|i14osf:,235481,jbellis,jbellis,,,,,,,,,,13/Dec/12 14:09;krummas;ah! it comes back due to the commitlog being replayed after reading up the schema,13/Dec/12 16:53;jbellis;added comment + committed.  thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
debian packaging should include shuffle,CASSANDRA-5058,12623485,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,12/Dec/12 04:49,12/Mar/19 14:17,13/Mar/19 22:27,18/Dec/12 17:19,1.2.0 rc2,,Packaging,,,0,,,,,,,"Our debian packaging doesn't currently include shuffle, but we should add it so people have a way of upgrading to vnodes.  This might also be a good time to consider a different name for shuffle, though I don't believe it currently conflicts with anything else.",,,,,,,,,,,,,,,,,,,18/Dec/12 16:32;slebresne;5058.txt;https://issues.apache.org/jira/secure/attachment/12561509/5058.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-17 14:09:25.629,,,no_permission,,,,,,,,,,,,297193,,,Tue Dec 18 17:19:56 UTC 2012,,,,,,0|i14mzb:,235187,urandom,urandom,,,,,,,,,,"17/Dec/12 14:09;slebresne;bq. I don't believe it currently conflicts with anything else

Nothing come to mind either, but I'd be fine with something like shuffle-tokens just for the sake of avoiding a too general name.","17/Dec/12 15:39;urandom;bq. Nothing come to mind either, but I'd be fine with something like shuffle-tokens just for the sake of avoiding a too general name.

Or {{cassandra-shuffle}}.  Somewhat long for a command name, but considering how seldom it will be used...",17/Dec/12 21:15;jbellis;+1 for cassandra-shuffle (following precedent for cassandra-cli),18/Dec/12 16:32;slebresne;Rename to cassandra-shuffle and add it to the debian packaging.,"18/Dec/12 17:16;urandom;bq. Rename to cassandra-shuffle and add it to the debian packaging.

+1","18/Dec/12 17:19;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Downed node loses its host-id,CASSANDRA-5032,12618938,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,tjake,tjake,05/Dec/12 20:22,12/Mar/19 14:17,13/Mar/19 22:27,06/Dec/12 14:26,1.2.0 rc1,,,,,0,vnodes,,,,,,"We took down one of our nodes for maintenance and during that time it seems the other nodes have lost the downed nodes node id

We also see lots of hint assertion exceptions ""Missing host ID for 10.6.27.98""

{code}
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address           Load       Tokens  Owns (effective)  Host ID                               Rack
UN  10.6.27.96        129.37 GB  256     140.0%            59f3df94-e551-45ce-a3b0-51462f3ea868  27
UN  10.6.27.97        125.24 GB  256     133.7%            f5bb146c-db51-475c-a44f-9facf2f1ad6e  27
DN  10.6.27.98        ?          256     126.3%            null                                  27
{code}

We restarted c* on the two other nodes that are up, my guess is the host id was lost on restart of those.
",,,,,,,,,,,,,,,,,,,05/Dec/12 23:59;brandon.williams;0001-rename-ring_id-to-host_id-for-clarity.txt;https://issues.apache.org/jira/secure/attachment/12556174/0001-rename-ring_id-to-host_id-for-clarity.txt,05/Dec/12 23:59;brandon.williams;0002-Load-host_ids-when-adding-saved-endpoints.txt;https://issues.apache.org/jira/secure/attachment/12556175/0002-Load-host_ids-when-adding-saved-endpoints.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-05 23:59:07.979,,,no_permission,,,,,,,,,,,,296204,,,Thu Dec 06 14:26:21 UTC 2012,,,,,,0|i146xj:,232586,tjake,tjake,,,,,,,,,,"05/Dec/12 23:59;brandon.williams;First patch renames 'ring_id' in peers to 'host_id' because that's what we call it everywhere else including nodetool, so we should be consistent.

Second patch updates the hostid in TMD when it updates the tokens.",06/Dec/12 00:53;tjake;Looks straight fwd. I can test but this looks like I'll need to recreate the cluster/system table?,"06/Dec/12 00:57;brandon.williams;I tested it and it works, but yeah, you'll have to wipe out the system table, or I think starting once with -Dcassandra.load_ring=false will work.",06/Dec/12 13:01;tjake;+1,06/Dec/12 14:26;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stock example for using pig throws InvalidRequestException(why:Start token sorts after end token),CASSANDRA-5106,12625917,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jdinata,jdinata,03/Jan/13 21:43,12/Mar/19 14:17,13/Mar/19 22:27,10/Jan/13 18:52,1.1.9,1.2.1,,,,0,,,,,,,"The setup:
This is from printenv
HADOOP_HOME=/home/Downloads/hadoop-1.1.1
PIG_HOME=/home/Downloads/pig-0.10.0
PIG_INITIAL_ADDRESS=localhost
PIG_RPC_PORT=9160
PIG_PARTITIONER=org.apache.cassandra.dht.Murmur3Partitioner

This is from cassandra-cli
[default@PigTest] describe cluster;
Cluster Information:
   Snitch: org.apache.cassandra.locator.SimpleSnitch
   Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
   Schema versions:
        b5fc9a82-fbdd-3cf5-af16-9c498c9f9a5c: [127.0.0.1]

Running test_storage.pig as
bin/pig_cassandra -x local test/test_storage.pig
after populating the cf as
cassandra-cli --host localhost --port 9160 < populate-cli.txt
throws 
2013-01-03 02:20:47,626 [Thread-4] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current spli
t being processed ColumnFamilySplit((-1, '-1728690256123413808] @[localhost.localdomain])
2013-01-03 02:20:47,758 [Thread-4] WARN  org.apache.hadoop.mapred.FileOutputCommitter - Output path is null in cleanup
2013-01-03 02:20:47,760 [Thread-4] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local_0001
java.lang.RuntimeException: InvalidRequestException(why:Start token sorts after end token)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:384)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:390)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:313)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.nextKeyValue(ColumnFamilyRecordReader.java:184)
        at org.apache.cassandra.hadoop.pig.CassandraStorage.getNext(CassandraStorage.java:226)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:194)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
Caused by: InvalidRequestException(why:Start token sorts after end token)
        at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12916)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:734)
        at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:718)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:346)
        ... 13 more",1 node cluster with default cassandra.yaml,,,,,,,,,,,,,,,,,,09/Jan/13 22:45;jbellis;5106.txt;https://issues.apache.org/jira/secure/attachment/12564051/5106.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-04 13:45:12.276,,,no_permission,,,,,,,,,,,,302499,,,Thu Jan 10 18:52:21 UTC 2013,,,,,,0|i173kv:,249547,brandon.williams,brandon.williams,,,,,,,,,,"04/Jan/13 13:45;brandon.williams;You probably have exported PIG_PARTITIONER=org.apache.cassandra.dht.RandomPartitioner, but 1.2 is using the default partitioner.  I've updated the README.txt instructions in examples/pig to use the m3 partitioner instead.","07/Jan/13 23:12;jdinata;Hi,
Still having the same exception thrown even with PIG_PARTITIONER=org.apache.cassandra.dht.Murmur3Partitioner",08/Jan/13 15:23;brandon.williams;Regression from CASSANDRA-5076,"08/Jan/13 19:25;yukim;This issue indeed comes from CASSANDRA-5089.
WordCount does fail in latest cassandra-1.1 branch with the same exception.",08/Jan/13 19:31;brandon.williams;Closing in favor of CASSANDRA-5089,08/Jan/13 21:41;jbellis;Reopening to avoid polluting fixver on 5089.,09/Jan/13 22:45;jbellis;patch to re-allow wrapping ranges for start_token/end_token pairing,10/Jan/13 18:09;brandon.williams;+1,10/Jan/13 18:52;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-shuffle fails as system keyspace is not user-modifiable,CASSANDRA-5097,12625687,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jihartik,jihartik,jihartik,02/Jan/13 15:04,12/Mar/19 14:17,13/Mar/19 22:27,02/Jan/13 18:20,1.2.1,,Legacy/Tools,,,0,,,,,,,"cassandra-shuffle tool fails to insert calculated relocations into the system keyspace as it is not user-modifiable. When run, the following exception is thrown after printing out the list of relocations for the first node in ring:

Exception in thread ""main"" java.lang.RuntimeException: InvalidRequestException(why:system keyspace is not user-modifiable.)
        at org.apache.cassandra.tools.Shuffle.executeCqlQuery(Shuffle.java:516)
        at org.apache.cassandra.tools.Shuffle.shuffle(Shuffle.java:359)
        at org.apache.cassandra.tools.Shuffle.main(Shuffle.java:678)
Caused by: InvalidRequestException(why:system keyspace is not user-modifiable.)
        at org.apache.cassandra.thrift.Cassandra$execute_cql3_query_result.read(Cassandra.java:37849)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_execute_cql3_query(Cassandra.java:1562)
        at org.apache.cassandra.thrift.Cassandra$Client.execute_cql3_query(Cassandra.java:1547)
        at org.apache.cassandra.tools.CassandraClient.execute_cql_query(Shuffle.java:733)
        at org.apache.cassandra.tools.Shuffle.executeCqlQuery(Shuffle.java:502)
        ... 2 more

By quickly checking the code it seems that the patch set for CASSANDRA-4874 disallows modifications to system keyspace again (they were previously allowed by CASSANDRA-4664) thus rendering cassandra-shuffle unable to do its job.",,,,,,,,,,,,,,,,,,,02/Jan/13 15:52;jihartik;CASSANDRA-5097.patch;https://issues.apache.org/jira/secure/attachment/12562917/CASSANDRA-5097.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-02 15:24:14.609,,,no_permission,,,,,,,,,,,,302234,,,Wed Jan 02 18:19:28 UTC 2013,,,,,,0|i16zlb:,248901,iamaleksey,iamaleksey,,,,,,,,,,"02/Jan/13 15:24;jbellis;We should allow modifying system table contents, but not changing the schema.",02/Jan/13 15:53;jihartik;Added patch that fixes the problem by allowing also MODIFY permission for system KS operations (applied against 1.2.0).,"02/Jan/13 18:19;iamaleksey;Can't switch assignee/reviewer for some reason..
anyway, +1 and committed Jouni's patch - thanks, Jouni.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Major compaction IOException in 1.1.8,CASSANDRA-5088,12625118,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,kmueller,kmueller,22/Dec/12 18:38,12/Mar/19 14:17,13/Mar/19 22:27,07/Jan/13 22:55,1.1.9,1.2.1,,,,0,compression,,,,,,"Upgraded 1.1.6 to 1.1.8.

Now I'm trying to do a major compaction, and seeing this:

ERROR [CompactionExecutor:129] 2012-12-22 10:33:44,217 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[CompactionExecutor:129,1,RMI Runtime]
java.io.IOError: java.io.IOException: Bad file descriptor
        at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:65)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:195)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:298)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Bad file descriptor
        at sun.nio.ch.FileDispatcher.preClose0(Native Method)
        at sun.nio.ch.FileDispatcher.preClose(FileDispatcher.java:59)
        at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:96)
        at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
        at java.io.FileInputStream.close(FileInputStream.java:258)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:131)
        at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:121)
        at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
        at java.io.RandomAccessFile.close(RandomAccessFile.java:541)
        at org.apache.cassandra.io.util.RandomAccessReader.close(RandomAccessReader.java:224)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:130)
        at org.apache.cassandra.io.sstable.SSTableScanner.close(SSTableScanner.java:89)
        at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:61)
        ... 9 more
",,,,,,,,,,,,,,,,,,,26/Dec/12 21:18;jbellis;5088-v2.txt;https://issues.apache.org/jira/secure/attachment/12562407/5088-v2.txt,26/Dec/12 22:04;jbellis;5088-v3.txt;https://issues.apache.org/jira/secure/attachment/12562411/5088-v3.txt,26/Dec/12 16:01;jbellis;5088.txt;https://issues.apache.org/jira/secure/attachment/12562383/5088.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-12-22 20:48:31.2,,,no_permission,,,,,,,,,,,,301654,,,Mon Jan 07 22:56:42 UTC 2013,,,,,,0|i16vbb:,248205,iamaleksey,iamaleksey,,,,,,,,,,22/Dec/12 20:48;jbellis;Jason reported something similar in CASSANDRA-5059.  Looks like an environment problem though since we can't reproduce on the same sstables.,"22/Dec/12 20:54;kmueller;What would be helpful to debug?  The same environment worked fine for 1.1.6 - I didn't try 1.1.7

I'm running a bit old of a JDK - I could try upgrading it.","26/Dec/12 04:41;cdaw;I am able to consistently reproduce this running upgrade scenarios for DataStax Enterprise (basically C* 1.1.6 to C* 1.1.8).
* I can't reproduce this going from vanilla C* 1.1.6 to C* 1.1.8 using cassandra-stress
* I can reproduce this on my mac using DSE.  Java version is: 1.6.0_24
* I can't reproduce this on ubuntu precise 64-bit using Java 1.6.0_31

*Pre-Upgrade: run on DSE 2.2.1 / Cassandra 1.1.6*
{code}
~/dse-2.2.1/demos/portfolio_manager/bin/pricer -o INSERT_PRICES
~/dse-2.2.1/demos/portfolio_manager/bin/pricer -o UPDATE_PORTFOLIOS
~/dse-2.2.1/demos/portfolio_manager/bin/pricer -o INSERT_HISTORICAL_PRICES -n 100
~/dse-2.2.1/bin/dse  hive -f ~/dse-2.2.1/demos/portfolio_manager/10_day_loss.q
~/dse-2.2.1/bin/nodetool drain
sudo pkill -9 java

# then restart using C* 1.1.8
{code}

+Below are the different related errors+


*Post-Upgrade: read CF created pre-upgrade*
{code}
ERROR [Thrift:3] 2012-12-25 18:53:22,139 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[Thrift:3,5,main]
java.io.IOError: java.io.IOException: Bad file descriptor
	at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore$2.close(ColumnFamilyStore.java:1411)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1490)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1435)
	at org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:50)
	at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:876)
	at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:705)
	at com.datastax.bdp.server.DseServer.get_range_slices(DseServer.java:1087)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3083)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3071)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at com.datastax.bdp.transport.server.ClientSocketAwareProcessor.process(ClientSocketAwareProcessor.java:43)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:192)
{code}

*Post-Upgrade: running upgradesstables*
{code}
Error occured while upgrading the sstables for keyspace HiveMetaStore
java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Bad file descriptor
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:226)
	at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:242)
	at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:983)
	at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1789)
{code}

*Post-Upgrade: running nodetool scrub*
{code}
WARN [CompactionExecutor:23] 2012-12-25 14:42:50,024 FileUtils.java (line 116) Failed closing /var/lib/cassandra/data/cfs/inode/cfs-inode-hf-1-Data.db - chunk length 65536, data length 48193.
java.io.IOException: Bad file descriptor
	at sun.nio.ch.FileDispatcher.preClose0(Native Method)
	at sun.nio.ch.FileDispatcher.preClose(FileDispatcher.java:59)
	at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:96)
	at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
	at java.io.FileInputStream.close(FileInputStream.java:258)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:131)
	at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:121)
	at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
	at java.io.RandomAccessFile.close(RandomAccessFile.java:541)
	at org.apache.cassandra.io.util.RandomAccessReader.close(RandomAccessReader.java:224)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:130)
	at org.apache.cassandra.io.util.FileUtils.closeQuietly(FileUtils.java:112)
	at org.apache.cassandra.db.compaction.Scrubber.close(Scrubber.java:306)
	at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:500)
	at org.apache.cassandra.db.compaction.CompactionManager.doScrub(CompactionManager.java:485)
	at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:69)
	at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:235)
	at org.apache.cassandra.db.compaction.CompactionManager$3.call(CompactionManager.java:205)
{code}
",26/Dec/12 15:17;jbellis;Can you try with compression disabled?,"26/Dec/12 16:01;jbellis;... Testing w/o compression won't be necessary, this is clearly a problem with the compression code only.

This looks like the following JDK bug: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6322678

It is marked fixed in JDK7, but inspection of JDK6u38 source looks like the fix is there as well.  Not sure how early the fix was made in JDK6.

So upgrading to the latest jdk6 or jdk7 should fix the problem.  I've also attached a workaround for platforms (basically: Mac OS users running DSE) where that isn't an option.",26/Dec/12 21:18;jbellis;v2 removes a redundant channel creation which might also help.,"26/Dec/12 21:49;cdaw;Results from testing v2:
* We originally reproduced this with a forked/modified version of C* 1.1.8 dropped in to DSE.  When we dropped in the C* 1.1.8 jar file from the apache download, we were also to reproduce this as well.
* Post-upgrade exception running:  list Stocks
* Post-upgrade exception running: nodetool upgradesstables
* Post-upgrade no errors running: nodetool compact or nodetool scrub

{code}
ERROR [Thrift:3] 2012-12-26 13:42:25,343 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[Thrift:3,5,main]
java.io.IOError: java.io.IOException: Bad file descriptor
	at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore$2.close(ColumnFamilyStore.java:1411)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1490)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1435)
	at org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:50)
	at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:876)
	at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:703)
	at com.datastax.bdp.server.DseServer.get_range_slices(DseServer.java:1087)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3083)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3071)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at com.datastax.bdp.transport.server.ClientSocketAwareProcessor.process(ClientSocketAwareProcessor.java:43)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:192)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.IOException: Bad file descriptor
	at sun.nio.ch.FileDispatcher.preClose0(Native Method)
	at sun.nio.ch.FileDispatcher.preClose(FileDispatcher.java:59)
	at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:96)
	at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
	at java.io.RandomAccessFile.close(RandomAccessFile.java:541)
	at org.apache.cassandra.io.util.RandomAccessReader.close(RandomAccessReader.java:224)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:131)
	at org.apache.cassandra.io.sstable.SSTableScanner.close(SSTableScanner.java:89)
	at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:61)
{code}
",26/Dec/12 22:04;jbellis;v3 removes CRAR.source entirely in favor of using the FileChannel handle we already have in RAR,"26/Dec/12 22:23;cdaw;[~jbellis]
three is the charm. the last patch worked.",07/Jan/13 17:35;iamaleksey;+1,07/Jan/13 22:56;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction deletes ExpiringColumns in Secondary Indexes,CASSANDRA-5079,12624763,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,amorton,amorton,amorton,20/Dec/12 04:05,12/Mar/19 14:17,13/Mar/19 22:27,20/Dec/12 09:12,1.1.9,1.2.0 rc2,Feature/2i Index,,,0,,,,,,,"From this discussion http://www.mail-archive.com/user@cassandra.apache.org/msg26599.html

CompactionManager.getDefaultGcBefore() set's the gc_before to be Integer.MAX_VALUE. 

In the example all entries in the secondary index have a TTL. In PreCompactedRow.removeDeletedAndOldShards() the CF is determined to have irrelevant data, the call to CFS.removeDeleted() results in the ExpiringColumns being removed and the row is treated as empty. CompactionTask.execute() exits at the {{if (!nni.hasNext())}} test, so the sstables are marked as compacted and soon deleted. 

In the example the localDeletionTime was Thu, 21 Mar 2013 08:25:22 GMT and should not have been purged. 

In the example when the first compaction on the secondary index runs the on disk data for the index is deleted. The logs show a compaction starting and no associated ""Compacted to"" message from that compaction thread. 

The impact is incorrect results from secondary indexes queries.",,,,,,,,,,,,CASSANDRA-5024,,,,,,,20/Dec/12 04:32;amorton;5079.txt;https://issues.apache.org/jira/secure/attachment/12561842/5079.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-20 05:09:50.044,,,no_permission,,,,,,,,,,,,301269,,,Fri Dec 21 08:05:39 UTC 2012,,,,,,0|i16r1b:,247512,slebresne,slebresne,,,,,,,,,,20/Dec/12 04:32;amorton;first version uses the current time for gc_before for secondary indexes.,"20/Dec/12 05:09;jbellis;Thanks Aaron, that looks like good detective work.

(Setting affects-version to when 2I were added.  Please correct if this was a more recent regression.)","20/Dec/12 08:07;amorton;Yup, was a fun few hours.

bq. (Setting affects-version to when 2I were added. Please correct if this was a more recent regression.)
Yes it has always been there. 

Will need to patch 0.8 onwards. Do we still release 0.7?","20/Dec/12 08:31;slebresne;That patch is reversed, right? (i.e. it adds what it is supposed to remove and remove what it is supposed to add)

bq. Will need to patch 0.8 onwards. Do we still release 0.7?

We don't release 0.7 and we don't release 0.8 either. I don't even want to release a 1.0 anymore. If you haven't complained about this, you probably don't use 2i and TTL. Even if you do, then that's a good excuse to upgrade to 1.1 at last.","20/Dec/12 09:12;slebresne;Alright, I've committed assuming the patch was indeed reversed (didn't make any sense otherwise anyway). Thanks.","21/Dec/12 02:27;amorton;bq. assuming the patch was indeed reversed

Not sure what you mean. Did i get the diff wrong?","21/Dec/12 08:05;slebresne;Yeah. If you look at the patch, the '+' should be '-' and vice-versa. You probably did a {{git diff patched HEAD}} instead of {{git diff HEAD patch}}. But anyway, that's ok, no harm done.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Since 1.1, get_count sometimes returns value smaller than actual column count",CASSANDRA-5099,12625743,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,alienth,alienth,02/Jan/13 22:51,12/Mar/19 14:17,13/Mar/19 22:27,11/Jan/13 17:06,1.1.9,1.2.1,,,,0,,,,,,,"We have a CF where rows have thousands of TTLd columns. The columns are continually added at a regular rate, and TTL out after 15 minutes. We continually run a 'get_count' on these keys to get a count of the number of live columns.

Since we upgrade from 1.0 to 1.1.7, ""get_count"" regularly returns much smaller values than are possible. For example, with  roughly 15,000 columns that have well-distributed TTLs, running a get_count 10 times will result in 1 or 2 results that are up to half the actual column count. Using a normal 'get' to count those columns always results in proper values. 

For example:

(all of these counts were ran within a second or less of eachother)
{code}
[default@reddit] count  AccountsActiveBySR['2qh0u'];
13665 columns
[default@reddit] count  AccountsActiveBySR['2qh0u'];
13665 columns
[default@reddit] count  AccountsActiveBySR['2qh0u'];
13666 columns
[default@reddit] count  AccountsActiveBySR['2qh0u'];
3069 columns
[default@reddit] count  AccountsActiveBySR['2qh0u'];
13660 columns
[default@reddit] count  AccountsActiveBySR['2qh0u'];
13661 columns
{code}

I should note that this issue happens much more frequently with larger (>10k columns) rows than smaller rows. It never seems to happen with rows having fewer than 1k columns.

There are no supercolumns in use. The key names and column names are very short, and there are no column values. The CF is LCS, and due to the TTL only hovers around a few MB in size. GC grace is normally at zero, but the problem is consistent with non-zero gc grace times.


It appears that there was an issue (CASSANDRA-4833) fixed in 1.1.7 regarding get_count. Some logic was added to prevent an infinite loop case. Could that change have resulted in this problem somehow? I can't find any other relevant 1.1 changes that might explain this issue.",,,,,,,,,,,,,,,,,,,10/Jan/13 00:29;yukim;5099-1.1.txt;https://issues.apache.org/jira/secure/attachment/12564063/5099-1.1.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-10 00:29:22.038,,,no_permission,,,,,,,,,,,,302294,,,Fri Jan 11 17:06:25 UTC 2013,,,,,,0|i16zyn:,248961,slebresne,slebresne,,,,,,,,,,02/Jan/13 22:56;alienth;Also found CASSANDRA-3798 which altered the get_count logic between 1.0 and 1.1.,05/Jan/13 00:06;alienth;Confirmed that this problem is consistent on both LCS and SizeTiered.,07/Jan/13 08:26;alienth;I've ported the 1.0.11 get_count code from CassandraServer.java to 1.1.7. Doing so resolves this issue.,"10/Jan/13 00:29;yukim;Fix for CASSANDRA-4833 assumes that only last page from get_slice has columns less than requested, and if that happens, returns the column count collected.
But in case of ttl, it is possible to have less columns at the middle of the pagenation.

Attached patch changes loop termination check so that it does not depend on column count, which is what we had in 1.0.","10/Jan/13 10:53;slebresne;In fact, I think that this point out to a more serious problem with TTL.

Because even get_count aside, if I do a get_slice with a particular count n, I expect that if I get less than n values in my result, it means there is no more value matching whatever replica I've provided. I'd say that any other behavior is a bug. But with TTL, if a column expires while the query is processed, we may fail that count argument (which is exactly what hits us here).

In other words, I'm not sure reintroducing the inefficiency of always doing one last almost always useless query to make sure the paging is indeed over is the right fix, because I doubt people that do manual row paging using get_slice actually implement that ""do one last query just in case the previous query lied on the count returned"".

What I would suggest instead would be to have CassandraServer save the current time before doing a get_slice, and use that time in thriftifyColumns when deciding if a column is considered expired or not. That way we won't skip an expiring column from the result set if it had been counted as live during the actual internal query.

 
","10/Jan/13 16:36;slebresne;I'll note that the solution I'm suggesting is 1) slightly painful to implement because deserialization change expiring columns into tombstones so we would have have to pass the expireBefore all the way down the stack and 2) this only really work correctly if we actually send that expireBefore to other nodes along with the read request to use as reference. In particular 2) makes such solution pretty much out of scope for 1.1 and even 1.2 (since it's a protocol change). That being said, I do think this is probably the ""right"" solution in the long run (unless someone has a better idea of course).

Note that I don't oppose committing the ""let's do one more request"" to 1.1 to fix get_count if we have nothing better, but that doesn't change the fact that the get_slice count is potentially broken when TTLs are involved. I just don't see a quick fix right off the bat.","10/Jan/13 17:52;jbellis;Sounds to me like we should commit the ""one more request"" fix and open a new ticket to address mid-request ttl expiration for 1.2 only.","10/Jan/13 21:52;yukim;I'm +1 to commit this patch for 1.1.9 and open new ticket for 1.2.
So, is the patch looking good?","11/Jan/13 15:32;slebresne;bq. So, is the patch looking good?

Yes, +1. I've created CASSANDRA-5149 for the follow up.","11/Jan/13 17:06;yukim;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
node fails to start because host id is missing,CASSANDRA-5107,12625922,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,03/Jan/13 22:12,12/Mar/19 14:17,13/Mar/19 22:27,22/Feb/13 21:00,1.2.2,,,,,1,,,,,,,"I saw this once on dtestbot but couldn't figure it out, but now I've encountered it myself:

{noformat}
ERROR 22:04:45,949 Exception encountered during startup
java.lang.AssertionError
   at org.apache.cassandra.locator.TokenMetadata.updateHostId(TokenMetadata.java:219)
   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:442)
   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:397)
   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:309)
   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:397)
   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:440)
java.lang.AssertionError
   at org.apache.cassandra.locator.TokenMetadata.updateHostId(TokenMetadata.java:219)
   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:442)
   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:397)
   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:309)
   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:397)
   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:440)
Exception encountered during startup: null
{noformat}

Somehow our own hostid is null sometimes.",,,,,,,,,,,,,,,,,,,18/Jan/13 17:30;brandon.williams;0001-Preload-gossip-application-states-before-starting.txt;https://issues.apache.org/jira/secure/attachment/12565509/0001-Preload-gossip-application-states-before-starting.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-11 12:50:20.66,,,no_permission,,,,,,,,,,,,302504,,,Fri Feb 22 21:00:17 UTC 2013,,,,,,0|i173lz:,249552,slebresne,slebresne,,,,,,,,,,"10/Jan/13 23:25;brandon.williams;Another trace that may be related:

{noformat}
ERROR [main] 2013-01-10 17:17:20,150 CassandraDaemon.java (line 387) Exception encountered during startup
java.lang.AssertionError
    at org.apache.cassandra.service.StorageService.getLocalTokens(StorageService.java:1969)
    at org.apache.cassandra.service.StorageService.setTokens(StorageService.java:209)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:740)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:508)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:406)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:282)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
{noformat}

We must be wiping our own tokens out so that there's nothing left to retrieve subsequently.","11/Jan/13 12:50;slebresne;That second trace is CASSANDRA-5121 (which has a patch attached). Maybe the patch there fixes the first trace too, not sure.",17/Jan/13 17:00;brandon.williams;I think CASSANDRA-5121 solved this.,"18/Jan/13 15:26;brandon.williams;It didn't :(  Here's another slightly different stacktrace where a remote node's hostId is missing:

{noformat}
 INFO [main] 2013-01-18 09:24:30,576 StorageService.java (line 1288) Node /127.0.0.3 state jump to normal
 INFO [main] 2013-01-18 09:24:30,583 StorageService.java (line 744) Startup completed! Now serving reads.
 WARN [main] 2013-01-18 09:24:30,597 Auth.java (line 131) Skipping default superuser setup: some nodes are not ready
 INFO [GossipStage:1] 2013-01-18 09:24:30,637 Gossiper.java (line 782) Node /127.0.0.1 has restarted, now UP
 INFO [GossipStage:1] 2013-01-18 09:24:30,637 Gossiper.java (line 750) InetAddress /127.0.0.1 is now UP
 INFO [GossipStage:1] 2013-01-18 09:24:30,639 StorageService.java (line 1288) Node /127.0.0.1 state jump to normal
 INFO [GossipStage:2] 2013-01-18 09:24:30,640 Gossiper.java (line 782) Node /127.0.0.2 has restarted, now UP
ERROR [GossipStage:1] 2013-01-18 09:24:30,641 CassandraDaemon.java (line 133) Exception in thread Thread[GossipStage:1,5,main]
java.lang.NullPointerException
    at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:637)
    at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1304)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)
    at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1899)
    at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:802)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:853)
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:43)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
 INFO [GossipStage:2] 2013-01-18 09:24:30,642 Gossiper.java (line 750) InetAddress /127.0.0.2 is now UP
{noformat}","18/Jan/13 16:01;slebresne;Can't that be because we start gossiper before setting the applicationState (including the host_id)? Granted we have a 1 second delay before really starting the gossipTask, but if those stack traces come from the test bot, maybe sometime the machine is slow enough that you can have a whole second interruption of the thread initializing the server.

In any case, this feel dangerous to me. Maybe we could split Gossiper.start into a Gossiper.init(), followed by the addition of the applicationstate that we expect to be always there, and then we start it.","18/Jan/13 17:25;brandon.williams;I'm not convinced this is the cause, but you're right that this seems dangerous regardless.  Patch to preload essential gossip states before starting the gossiper.","18/Jan/13 17:42;slebresne;Nit: the gossip info are not ""pre-loaded"", maybe they good for good measure (splitting start() into init() and start() would have allowed to reuse the gossipSnitchInfo as is :), though I'm fine with duplicating the 2 lines of that function if you prefer). Also, currently the ""gossip snitch infos"" comment are misplaced.

But that's details, +1 in any case.

bq. I'm not convinced this is the cause

I'm not either, but I don't see anything else so far. Besides, I've seen dtest getting to a bit of crawl for some tests that were involving many nodes, so it's far fetched, but not impossible.","18/Jan/13 17:50;brandon.williams;bq. Nit: the gossip info are not ""pre-loaded"", maybe they good for good measure (splitting start() into init() and start() would have allowed to reuse the gossipSnitchInfo as is

The problem with start/init is propagating the passed in generation, and we can't add states until we've added our local state, which is why I added a new start() signature instead.  I get what you're saying about gossipSnitchInfo, but it's also called from PFS.reloadConfiguration, and since addLocalApplicationState does notifications where this 'preload' does not, I didn't want to risk breaking anything there.

bq. Also, currently the ""gossip snitch infos"" comment are misplaced.

Actually I fixed that in a ninja-reupload. :)

Committed 0001, leaving this ticket open to see what happens.",22/Feb/13 21:00;brandon.williams;Closing since I haven't seen this repro since we committed the fix.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli doesn't support JMX authentication.,CASSANDRA-5080,12624803,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,michalm,solf,solf,20/Dec/12 11:17,12/Mar/19 14:17,13/Mar/19 22:27,15/Feb/13 19:55,1.1.11,1.2.2,Legacy/Tools,,,0,,,,,,,"It seems that cassandra-cli doesn't support JMX user authentication.

Specifically I went about securing our Cassandra cluster slightly -- I've added cassandra-level authentication (which cassandra-cli does support), but then I discovered that nodetool is still completely unprotected. So I went ahead and secured JMX (via -Dcom.sun.management.jmxremote.password.file and -Dcom.sun.management.jmxremote.access.file). Nodetool supports JMX authentication via -u and -pw options.

However it seems that cassandra-cli doesn't support JMX authentication, e.g.:
{quote}
apache-cassandra-1.1.6\bin>cassandra-cli -h hostname -u experiment -pw password
Starting Cassandra Client
Connected to: ""db"" on hostname/9160
Welcome to Cassandra CLI version 1.1.6

[experiment@unknown] show keyspaces;
WARNING: Could not connect to the JMX on hostname:7199, information won't be shown.

Keyspace: system:
  Replication Strategy: org.apache.cassandra.locator.LocalStrategy
  Durable Writes: true
    Options: [replication_factor:1]
... (rest of keyspace output snipped)
{quote}

help connect; and cassandra-cli --help do not seem to indicate that there's any way to specify JMX login information.",,,,,,,,,,,,,,,,,,,01/Feb/13 18:41;michalm;5080-v1.patch;https://issues.apache.org/jira/secure/attachment/12567615/5080-v1.patch,01/Feb/13 18:41;michalm;enable-jmx-authentication.patch;https://issues.apache.org/jira/secure/attachment/12567616/enable-jmx-authentication.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-02-01 18:41:22.557,,,no_permission,,,,,,,,,,,,301311,,,Fri Feb 15 19:55:19 UTC 2013,,,,,,0|i16rbr:,247559,iamaleksey,iamaleksey,,,,,,,,,,"01/Feb/13 18:41;michalm;Attaching patch (5080-v1.patch) that adds JMX authentication support for cassandra-cli. 

For easier testing apply the second patch (enable-jmx-authentication.patch) - it enables JMX authentication for Cassandra. With this patch you will have two roles available for test (username / password / permissions):
adminRole / admin / readwrite
userRole / user / readonly

Then start Cassandra and launch:

{noformat}./bin/cassandra-cli --jmxusername adminRole --jmxpassword admin{noformat}

or 

{noformat}./bin/cassandra-cli --jmxusername userRole --jmxpassword user{noformat}
","15/Feb/13 19:55;iamaleksey;Committed. Thanks, Michał.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RepairCallback breaks CL guarantees,CASSANDRA-5113,12626061,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,04/Jan/13 16:25,12/Mar/19 14:17,13/Mar/19 22:27,07/Jan/13 10:13,1.2.1,,,,,0,,,,,,,"RepairCallback does not validate the consistency level of the query. It seems that this was done on purpose as the comments there says:
{noformat}
/**
 * The main difference between this and ReadCallback is, ReadCallback has a ConsistencyLevel
 * it needs to achieve.  Repair on the other hand is happy to repair whoever replies within the timeout.
 */
{noformat}
Concretely, the get() method of RepairCallback:
* waits for all endpoints, even if there is more than strictly required by the CL.
* if it timeouts, doesn't check it and always return a response.
* for some reason, it returns null unless there is strictly more than 1 response.

All of that seems wrong to me. The result of RepairCallback is what is returned to the client in case of a digest mismatch on the first read. So we must ensure that the CL has been reached. Also, returning null where there is 1 response (or none) seems clearly wrong.

In fact I don't think we need a special callback for this ""read all data"" phase as it is a ""normal"" read (the fact we do a first read with digests is just an ""optimization""). The only difference between the two phases should be how we resolve the responses (in the first case we have digest and in the 2nd we don't) but that's handled by the resolver.

So attaching a patch that removes RepairCallback and use ReadCallback instead.  I'm also attaching a 2nd trivial patch that renames RowRepairResolver to RowDataResolver because I think it describe better what this actually do (i.e.  the main goal is to resolve a full data read to answer the right value to the client, repairing inconsistent nodes is secondary).

The patch is against 1.1, because I think breaking CL guarantees is probably serious enough to warrant pushing this to 1.1.
",,,,,,,,,,,,,,,,,,,04/Jan/13 18:18;slebresne;0001-Always-ensure-CL-after-a-digest-mismatch-1.2.txt;https://issues.apache.org/jira/secure/attachment/12563333/0001-Always-ensure-CL-after-a-digest-mismatch-1.2.txt,04/Jan/13 16:25;slebresne;0001-Always-ensure-CL-after-a-digest-mismatch.txt;https://issues.apache.org/jira/secure/attachment/12563308/0001-Always-ensure-CL-after-a-digest-mismatch.txt,04/Jan/13 18:18;slebresne;0002-Rename-RowRepairResolver-to-RowDataResolver-1.2.txt;https://issues.apache.org/jira/secure/attachment/12563334/0002-Rename-RowRepairResolver-to-RowDataResolver-1.2.txt,04/Jan/13 16:25;slebresne;0002-Rename-RowRepairResolver-to-RowDataResolver.txt;https://issues.apache.org/jira/secure/attachment/12563309/0002-Rename-RowRepairResolver-to-RowDataResolver.txt,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-01-04 17:27:55.174,,,no_permission,,,,,,,,,,,,302644,,,Mon Jan 07 10:13:49 UTC 2013,,,,,,0|i174hb:,249694,jbellis,jbellis,,,,,,,,,,"04/Jan/13 17:27;jbellis;+1 on the fix.

-0 on the rename; the only time we do full-data reads is for repairs so I don't really see an improvement there.

-1 on committing to 1.1; this behavior has been present since at least 1.0 and probably longer, and changes to core StorageProxy code like this have a long track record of causing subtle regressions.","04/Jan/13 17:49;slebresne;bq. the only time we do full-data reads is for repairs

You kind of thouch my point, ""the only time we do full-data reads is for repairs"" is imho largely false. We do full-data reads in 2 places: # on a digest mismatch of an initial read (i.e. in SP.fetchRows) # on a digest mismatch in ReadCallback.maybeResolveForRepair, i.e.  asynchronously if we've already responded to the client but have more response due to read_repair_chance kicking in.

While in the 2nd case this is indeed ""for repairs"", in the first case we first and foremost do the full-data read so we can compute the right/up-to-date value to return to the client. That's by far the most important use of the full-data read because that's where correctness is involved. The fact we use that occasion to compute diff and repair inconcistent nodes is secondary as it's after ""an optimization"".

So RowRepairResolver suggests repairing is the most important thing it does, but it's not.

bq. -1 on committing to 1.1; this behavior has been present since at least 1.0

I can buy that. And for the record, the risk of breaking CL guarantees is actually pretty low as you need a digetst mistmatch and have a node die between the digest phase and the full-data read phase.

One thing that has more chance to occurs with the current implementation is to get requests that take rpc_timeout seconds to return sometimes if read_repair_change > 0. Because in that case, and in the even of a mismatch, RepairCallback.get() will wait for all replica (even at QUORUM), so there is a real change that it'll wait until the timeout. But after that timeout it will almost always return the correct value, so it's just an inefficiency.

I'll rebase the patches to 1.2.
","04/Jan/13 18:18;slebresne;Alright, 1.2 rebased version attached (including the rename since as said above I do really like it :))",04/Jan/13 19:11;jbellis;+1 on both,"07/Jan/13 10:13;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a close() method to CRAR to prevent leaking file descriptors.,CASSANDRA-4820,12612139,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,xedin,xedin,17/Oct/12 01:31,12/Mar/19 14:17,13/Mar/19 22:27,17/Oct/12 18:08,1.1.7,,,,,0,,,,,,,"The problem is that under heavy load Finalizer daemon is unable to keep up with number of ""source"" and ""channel"" fields from CRAR to finalize (as FileInputStream has custom finalize() which calls close inside) which creates memory/cpu pressure on the machine.",,,,,,,,,,,,,,,,,,,17/Oct/12 03:38;xedin;CASSANDRA-4820.patch;https://issues.apache.org/jira/secure/attachment/12549437/CASSANDRA-4820.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-17 03:11:09.377,,,no_permission,,,,,,,,,,,,249186,,,Wed Oct 17 18:08:50 UTC 2012,,,,,,0|i0a5l3:,57178,jbellis,jbellis,,,,,,,,,,"17/Oct/12 03:11;jbellis;FIS.close says ""If this stream has an associated channel then the channel is closed as well.""  So closing both source and channel is unnecessary.","17/Oct/12 03:38;xedin;Right, sorry, they share the same fd. Re-attached the patch.",17/Oct/12 14:52;jbellis;+1,17/Oct/12 18:08;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair -pr triggers an Assertion,CASSANDRA-5136,12626795,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,carlyeks,mkjellman,mkjellman,09/Jan/13 16:51,12/Mar/19 14:17,13/Mar/19 22:27,09/Jan/13 21:59,1.2.1,,,,,0,compaction,,,,,,"{code}
ERROR [ValidationExecutor:1] 2013-01-09 08:47:29,232 CassandraDaemon.java (line 133) Exception in thread Thread[ValidationExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.PrecompactedRow.update(PrecompactedRow.java:149)
        at org.apache.cassandra.service.AntiEntropyService$Validator.rowHash(AntiEntropyService.java:375)
        at org.apache.cassandra.service.AntiEntropyService$Validator.add(AntiEntropyService.java:367)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:780)
        at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:75)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:495)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{code}",Ubuntu 12.04,,,,,,,,,,,,,CASSANDRA-5077,,,,,09/Jan/13 19:21;carlyeks;5136-repair-assertion.patch;https://issues.apache.org/jira/secure/attachment/12563999/5136-repair-assertion.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-09 19:24:06.793,,,no_permission,,,,,,,,,,,,303413,,,Wed Jan 09 21:59:04 UTC 2013,,,,,,0|i17b0v:,250754,jbellis,jbellis,,,,,,,,,,09/Jan/13 16:56;mkjellman;assert compactedCf != null;,"09/Jan/13 19:24;carlyeks;It appears that the row is empty, but that doesn't get checked when doing a validation. I've added a check, as well as closing the row if it is empty (as this was the cause of CASSANDRA-4492).",09/Jan/13 21:46;mkjellman;+1 on the patch and also verified this fixes the bug,09/Jan/13 21:59;jbellis;committed (and removed the not-null filter too),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unsafeAssassinateEndpoint throws NullPointerException and fails to remove node from gossip,CASSANDRA-5127,12626481,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mkjellman,mkjellman,08/Jan/13 03:28,12/Mar/19 14:17,13/Mar/19 22:27,08/Jan/13 19:26,1.2.1,,,,,0,,,,,,,"unsafeAssassinateEndpoint() throws NullPointerException and the node still seems to be in gossip. 

gossip info for node in question:

{code}
/10.8.30.15
  HOST_ID:d84a5632-d6d5-4b06-8e1b-ae39ab185ca1
  RPC_ADDRESS:0.0.0.0
  RACK:RAC1
  DC:DC1
  REMOVAL_COORDINATOR:REMOVER,b63fe173-5d13-4905-a59f-a78790f4f980
  RELEASE_VERSION:1.2.0
  NET_VERSION:6
  LOAD:2.64185473948E11
  STATUS:removed,d84a5632-d6d5-4b06-8e1b-ae39ab185ca1,1357874470406
  SCHEMA:5cd8420d-ce3c-3625-8293-67558a24816b
{code}

{code}
ERROR 19:26:20,078 Exception in thread Thread[GossipStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.getApplicationStateValue(StorageService.java:1192)
	at org.apache.cassandra.service.StorageService.getTokensFor(StorageService.java:1200)
	at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:1452)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1163)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1895)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:805)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:856)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:57)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)

{code}",Ubuntu 12.04,,,,,,,,,,,,,,,,,,08/Jan/13 18:32;brandon.williams;5127-v2.txt;https://issues.apache.org/jira/secure/attachment/12563790/5127-v2.txt,08/Jan/13 15:53;brandon.williams;5127.txt;https://issues.apache.org/jira/secure/attachment/12563766/5127.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-08 06:16:31.498,,,no_permission,,,,,,,,,,,,303088,,,Tue Jan 08 23:04:01 UTC 2013,,,,,,0|i177uv:,250241,slebresne,slebresne,,,,,,,,,,08/Jan/13 06:16;brandon.williams;This is because there is no TOKENS state for a non-vnode host in 1.2.  Mostly saying this for myself so I don't forget what to do tomorrow.,"08/Jan/13 15:37;brandon.williams;Hmm, to clarify, the stacktrace you're posting is from a node receiving the assassinate attempt? Because that stack doesn't point at uAE itself.",08/Jan/13 15:41;mkjellman;node i'm trying to assassinate is down. Stacktrace is from the node I executed uAE on.,"08/Jan/13 15:44;mkjellman;{code}
 WARN [RMI TCP Connection(3166)-10.8.240.170] 2013-01-08 07:42:02,143 Gossiper.java (line 419) Assassinating /10.8.30.15 via gossip
 INFO [RMI TCP Connection(3166)-10.8.240.170] 2013-01-08 07:42:02,144 Gossiper.java (line 434) Sleeping for 30000ms to ensure /10.8.30.15 does not change
 INFO [RMI TCP Connection(3166)-10.8.240.170] 2013-01-08 07:42:32,416 Gossiper.java (line 767) InetAddress /10.8.30.15 is now dead.
{code}

so that looks okay.

jconsole threw ""Problem invoking unsafeAssassinateEndpoint: java.lang.NullPointerException""

{code}
/10.8.30.15
  DC:DC1
  SCHEMA:5cd8420d-ce3c-3625-8293-67558a24816b
  RELEASE_VERSION:1.2.0
  REMOVAL_COORDINATOR:REMOVER,b63fe173-5d13-4905-a59f-a78790f4f980
  LOAD:2.64185473948E11
  STATUS:LEFT,1357918952416,21210815907080254728491917739023519863
  RPC_ADDRESS:0.0.0.0
  NET_VERSION:6
  HOST_ID:d84a5632-d6d5-4b06-8e1b-ae39ab185ca1
  RACK:RAC1
{code}


node is still in gossipinfo though..

I assumed that stack trace which happened at the same time correlated to the npe.","08/Jan/13 15:48;mkjellman;logs from another node in the cluster (aka didn't execute the jmx call and is not the downed node.

{code}
 INFO [GossipStage:1] 2013-01-08 07:42:33,258 Gossiper.java (line 767) InetAddress /10.8.30.15 is now dead.
ERROR [GossipStage:1] 2013-01-08 07:42:33,267 CassandraDaemon.java (line 133) Exception in thread Thread[GossipStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.getApplicationStateValue(StorageService.java:1192)
        at org.apache.cassandra.service.StorageService.getTokensFor(StorageService.java:1200)
        at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:1452)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1163)
        at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1895)
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:805)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:856)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:57)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [GossipStage:2] 2013-01-08 07:42:35,274 CassandraDaemon.java (line 133) Exception in thread Thread[GossipStage:2,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.getApplicationStateValue(StorageService.java:1192)
        at org.apache.cassandra.service.StorageService.getTokensFor(StorageService.java:1200)
        at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:1452)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1163)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:917)
        at org.apache.cassandra.gms.Gossiper.applyNewStates(Gossiper.java:908)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:866)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:57)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}","08/Jan/13 15:53;brandon.williams;There are a few cases where we assume that if a host has a HOST_ID, then it must use vnodes, but that is not the case.  Patch attached to refactor these cases into a usesVnodes method which checks for both HOST_ID and TOKENS.","08/Jan/13 17:52;slebresne;Agreed on the analysis, but I'd rather hide this directly in getTokenFor (i.e. replace the '{{if vnodes then getTokensFor else TokenFactory.fromString(pieces[1])}}' by '{{getTokenFor(endpoing, pieces[1])}}' and handle the backward compatibility inside getTokenFor.

In particular, and though that's a nit, I'm not of fan of have a usesVnodes method because if I'm not mistaken any upgraded would have this method return true even if only one token is used, right? If so, I'm afraid the method would be misleading. ","08/Jan/13 18:33;brandon.williams;v2 consolidates everything into getTokensFor.

bq.  I'm not of fan of have a usesVnodes method because if I'm not mistaken any upgraded would have this method return true even if only one token is used

Not quite, to have the TOKENS app state you must be on vnodes.  If there's only one token it's handled the old, ugly way by splitting the STATUS state.
","08/Jan/13 19:21;slebresne;+1 on v2, though I still don't like the usesVnodes method name (basically I don't want anyone starting to use it for anything else than what getTokenFor is doing, which for me qualify as 'it shouldn't exist') but that's a nit.

bq. If there's only one token it's handled the old, ugly way by splitting the STATUS state.

I would suggest handling the one token case by feeding both the STATUS state *and* the new TOKENS state so that in some future version we can just remove the STATUS old ugly way, but that's work for another ticket in any case.",08/Jan/13 19:26;brandon.williams;Committed,"08/Jan/13 23:04;mkjellman;+1, confirmed that the patch resolves the issue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 BATCH authorization caching bug,CASSANDRA-5145,12627102,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,11/Jan/13 02:04,12/Mar/19 14:17,13/Mar/19 22:27,11/Jan/13 16:43,1.1.9,1.2.1,,,,0,,,,,,,"cql3.BatchStatement:

{noformat}
    public void checkAccess(ClientState state) throws InvalidRequestException
    {
        Set<String> cfamsSeen = new HashSet<String>();
        for (ModificationStatement statement : statements)
        {
            // Avoid unnecessary authorizations.
            if (!(cfamsSeen.contains(statement.columnFamily())))
            {
                state.hasColumnFamilyAccess(statement.keyspace(), statement.columnFamily(), Permission.MODIFY);
                cfamsSeen.add(statement.columnFamily());
            }
        }
    }
{noformat}

In CQL3 we can use fully-qualified name of the cf and so a batch can contain mutations for different keyspaces. And when caching cfamsSeen, we ignore the keyspace. This can be exploited to modify any CF in any keyspace so long as the malicious user has CREATE+MODIFY permissions on some keyspace (any keyspace). All you need is to create a table in your ks with the same name as the table you want to modify and perform a batch update.

Example: an attacker doesn't have permissions, but wants to modify k1.demo table. The attacker controls k2 keyspace. The attacker creates k2.demo table and then does the following request:

{noformat}
cqlsh:k2> begin batch
      ... insert into k2.demo ..
      ... insert into k1.demo ..
      ... apply batch;
cqlsh:k2>
{noformat}

.. and successfully modifies k1.demo table since 'demo' cfname will be cached.

Thrift's batch_mutate and atomic_batch_mutate are not affected since the only allow mutations to a single ks. CQL2 batches are not affected since they don't do any caching.

We should either get rid of caching here or switch cfamsSeen to a Map<String, Set<String>>.

Personally, I'd rather do the latter now, and get rid of caching here completely once CASSANDRA-4295 is resolved. ",,,,,,,,,,,,,,,,,,,11/Jan/13 16:09;iamaleksey;5145.txt;https://issues.apache.org/jira/secure/attachment/12564432/5145.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-11 15:04:01.612,,,no_permission,,,,,,,,,,,,303860,,,Fri Jan 11 16:43:39 UTC 2013,,,,,,0|i17g3r:,251578,slebresne,slebresne,,,,,,,,,,"11/Jan/13 15:04;slebresne;bq. I'd rather do the latter now

I'm good with that, though I would not even bother with a map of sets, but just add {{statement.keyspace() + "":"" + statement.columnFamily()}} to cfamsSeen.","11/Jan/13 15:23;iamaleksey;Thought about that, but if we later allow ':' in ks/cf names, for example, this would bite us again, since there would be now way to distinguish between (ks: ""ks:1"", cf: ""demo"") and (ks: ""ks"", cf: ""1:demo"") and a similar attack would happen.

Now, this may not be a valid concern, but I'd rather not risk by depending on cql grammar here.","11/Jan/13 15:38;slebresne;I estimate the change of allowing ':' in ks/cf names before CASSANDRA-4295 to 0 :). In fact I kind of doubt we'll ever allow it (there's no real use and there's no point in potentially screwing up tools that rely on those name not containing ':'). But really, I'm perfectly fine with a map of sets.",11/Jan/13 16:29;slebresne;+1 (nit: we can alias the Set to avoid 2 get()) ,"11/Jan/13 16:43;iamaleksey;Thanks, committed (with the nit taken care of).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement better way of eliminating compaction left overs.,CASSANDRA-5151,12627235,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,yukim,yukim,11/Jan/13 19:17,12/Mar/19 14:17,13/Mar/19 22:27,01/Jul/13 20:29,2.0 beta 1,,,,,0,,,,,,,"This is from discussion in CASSANDRA-5137. Currently we skip loading SSTables that are left over from incomplete compaction to not over-count counter, but the way we track compaction completion is not secure.

One possible solution is to create system CF like:

{code}
create table compaction_log (
  id uuid primary key,
  inputs set<int>,
  outputs set<int>
);
{code}

to track incomplete compaction.",,,,,,,,,,,,,,,,,,,11/Feb/13 21:02;yukim;0001-move-scheduling-MeteredFlusher-to-CassandraDaemon.patch;https://issues.apache.org/jira/secure/attachment/12568889/0001-move-scheduling-MeteredFlusher-to-CassandraDaemon.patch,17/Jan/13 21:16;yukim;5151-1.2.txt;https://issues.apache.org/jira/secure/attachment/12565372/5151-1.2.txt,18/Jan/13 21:10;yukim;5151-v2.txt;https://issues.apache.org/jira/secure/attachment/12565555/5151-v2.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-01-18 00:38:06.511,,,no_permission,,,,,,,,,,,,304005,,,Mon Jul 01 14:30:09 UTC 2013,,,,,,0|i17hfb:,251792,jbellis,jbellis,,,,,,,,,,"17/Jan/13 17:31;yukim;We want to remove left-overs before opening CFs by reading compaction log system CF. The problem is, commit log has to be recovered before reading from any CF, but CF initialization should be completed before commit log recovery. So there is no chance to scrub off left-overs before CF init. Obviously we don't want to flush every time we update compaction log.

Alternative way is to write compaction log file with inputs each time we start compaction, and remove it when complete.
If CF sees that compaction log files in its data directory, that means there are unfinished compactions and those sstable files with ancestors that are in compaction log files should be deleted. Does this sound likely to work?","17/Jan/13 21:16;yukim;Initial patch attached to get feedback. It writes compaction log file to track incomplete compactions described in my previous comment.

SSTable left-overs are deleted during node start up with CFS.scrubDataDirectories. In order to get ancestors of SSTable files, it has to deserialize each Stats.db file so slows down node startup, but this only happens when incomplete compaction left compaction log file.

The patch does not rely on CQL3 collection at all, so it is possible to port into 1.1.x.","18/Jan/13 00:38;jbellis;bq. commit log has to be recovered before reading from any CF

That can't be true, can it?  At the least we have to special case schema.","18/Jan/13 01:52;yukim;It is. Except schema_* are flushed at the end of migration, other system cf used before commit log recovery like local also flushes every time after its data updated. If we go using system cf for this, we need to do the same, and doing so requires 2 flushes during one compaction, one for inserting log and one for removing at the end.

I think plain file under cf data dir is easy to implement both for writing log and recovery.","18/Jan/13 02:27;jbellis;Flushing is roughly the same cost as the fsync you need to make sure the plain file is safely written, so I don't see a performance penalty.  And I'm nervous about more one-off files (manifest was bad enough).  Storing in the system table does seem cleaner to me.  Am I missing something still?","18/Jan/13 17:43;yukim;If we use CQL, we cannot clean up left-overs in system keyspace before opening it since columnfamilies under system keyspace is open at the time we issue CQL.","18/Jan/13 17:48;jbellis;You're saying there's a problem with storing data about compacting system tables, in a system table?

I think you're right, but can you spell it out with an example?","18/Jan/13 18:09;yukim;Yes.

Storing compaction log in system keyspace can be done without problem.
But when the node starts up and tries to eliminate left overs based on query result from compaction log, there is a problem.

For example, the node goes down leaving system.schema_keyspaces' compaction unfinished and both original sstable(A) and produced sstable(A') remained in data dir. What we want when node restarts is that schema_keyspaces only references A and discards A'.
So we query compaction log from system ks to determine which sstables are left overs. But when we query system ks, cassandra opens whole columnfamilies under system ks, and schema_keyspaces already references both A and A' at this time. So even though we get the query result, we cannot do anything further.","18/Jan/13 18:20;slebresne;That's true, but we don't store counters in the system tables (and I take on myself to veto whomever suggests to change that fact), so it's safe to always pick all sstables without consulting the compaction log. In other words I suggest special casing the system keyspace on startup so it don't bother with the compaction log. As far as I'm prefer, I prefer that many times to relying on an external file.","18/Jan/13 21:10;yukim;Alright, CQL3 based v2 patch attached.

Below is newly introduced compaction_log schema:

{code}
CREATE TABLE compaction_log (
  id uuid PRIMARY KEY,
  keyspace_name text,
  columnfamily_name text,
  inputs set<int>
) WITH COMMENT='unfinished compactions'
{code}

data in compaction_log gets created at the beginning of compaction and removed when finished. It only tracks compaction inputs and the node uses them combined with each sstable's ancestors to remove leftovers.

compaction_log does not store logs from system columnfamilies(should we skip auth and trace also?).","31/Jan/13 20:30;jbellis;Posted a patch to https://github.com/jbellis/cassandra/branches/5151 that adds removal of obsolete sstables for completed compactions, as well as some cosmetic changes.","31/Jan/13 21:41;yukim;patch lgtm, +1.",01/Feb/13 04:51;jbellis;committed,"06/Feb/13 01:15;iamaleksey;After this commit (aa90c88be14b337714739cd857c12cad2a9fedeb) two tests started failing from time to time (not 100% consistently, bit noticeably often):

1. ColumnFamilyStoreTest

{noformat}
    [junit] Testsuite: org.apache.cassandra.db.ColumnFamilyStoreTest
    [junit] Tests run: 28, Failures: 0, Errors: 0, Time elapsed: 5.919 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit]  WARN 04:11:00,492 setting live ratio to maximum of 64.0 instead of 102.56071964017991
    [junit]  WARN 04:11:00,610 setting live ratio to maximum of 64.0 instead of 303.2307692307692
    [junit] ERROR 04:11:01,182 Fatal exception in thread Thread[CompactionExecutor:2,1,main]
    [junit] java.lang.AssertionError: Incoherent new size -3 replacing [SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-4-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-3-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-6-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-5-Data.db')] by [SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-7-Data.db')] in View(pending_count=1, sstables=[], compacting=[])
    [junit] 	at org.apache.cassandra.db.DataTracker$View.newSSTables(DataTracker.java:545)
    [junit] 	at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:517)
    [junit] 	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
    [junit] 	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:227)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:1030)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:251)
    [junit] 	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:72)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:193)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
    [junit] ------------- ---------------- ---------------
{noformat}

2. CompactionsTest

{noformat}
   [junit] Testsuite: org.apache.cassandra.db.compaction.CompactionsTest
    [junit] Tests run: 9, Failures: 1, Errors: 0, Time elapsed: 20.298 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit]  WARN 04:14:16,985 setting live ratio to maximum of 64.0 instead of 360.7111111111111
    [junit]  WARN 04:14:17,121 setting live ratio to maximum of 64.0 instead of 365.15555555555557
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testCompactionLog(org.apache.cassandra.db.compaction.CompactionsTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.compaction.CompactionsTest.testCompactionLog(CompactionsTest.java:311)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.CompactionsTest FAILED
{noformat}","06/Feb/13 04:33;yukim;I need more time for #1, but for #2, I committed fix in f3091835e67ebdc0171991b2ca283e6cd357cda3.
","09/Feb/13 02:06;mkjellman;[~yukim] looks like there might still be an unexpected condition here

{code}
java.lang.IllegalStateException: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.
        at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:444)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:213)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:324)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:188)
Cannot load daemon
Service exit with a return value of 3
{code}","09/Feb/13 02:47;mkjellman;wondering if this is related...

{code}
 INFO 18:43:25,360 Completed flushing /data/cassandra/system/compactions_in_progress/system-compactions_in_progress-ib-60-Data.db (192 bytes) for commitlog position ReplayPosition(segmentId=1360374628064, position=29215408)
 INFO 18:43:25,364 Compacting [SSTableReader(path='/var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2182-Data.db'), SSTableReader(path='/var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2180-Data.db'), SSTableReader(path='/var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2185-Data.db'), SSTableReader(path='/data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-2181-Data.db'), SSTableReader(path='/data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-2184-Data.db'), SSTableReader(path='/var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2183-Data.db'), SSTableReader(path='/var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2179-Data.db')]
ERROR 18:43:25,367 Exception in thread Thread[CompactionExecutor:85,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2183-Data.db (No such file or directory)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:61)
        at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1121)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:51)
        at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:954)
        at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:966)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:147)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:124)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:63)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:193)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2183-Data.db (No such file or directory)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:78)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:57)
        ... 17 more
{code}","11/Feb/13 17:33;yukim;[~mkjellman] What else do you see about the file ""evidence-fingerprints-ib-2183-Data.db"" in your log file? Can you see the file is compacted elsewhere?","11/Feb/13 20:33;yukim;I think there is concurrency problem between opening ColumnFamilyStore and deleting/scrubbing SSTables.

This static block(https://github.com/apache/cassandra/blob/cassandra-1.2.1/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L88) schedules MeteredFlusher which accesses all ColumnFamilyStore when it runs every 1 sec. Scheduling is done when JVM first load ColumnFamilyStore class, so after that, there is always a chance to open SSTables before doing scrub directory/remove compaction left overs.
We should move the content of static block at the end of CassandraDaemon setup.","11/Feb/13 21:02;yukim;Patch for v1.2 attached to schedule MeteredFlusher in CassandraDaemon.
Probably it's better to patch 1.1 as well.","13/Feb/13 16:22;mkjellman;It appears the patch seems to have resolved the FileNotFoundException but I'm still able to reproduce the IllegalStateException.

Also, it seems that once a node throws this exception, even after deleting the sstables in system/compactions_in_process that node will now throw the Exception on startup every time.","13/Feb/13 17:01;brandon.williams;I move to revert the non-bugfix portion of this patch from 1.2 and push it to trunk, given the fallout we've seen thus far.",13/Feb/13 17:06;mkjellman;[~brandon.williams] is the fallout due to bugs in the patch/new implementation or is it exposing unrelated bugs that were just being skipped before?,"13/Feb/13 17:11;brandon.williams;I'll let Yuki decide, but the fact that we failed a dtest, a utest, and most importantly the [~mkjellman] test is worrisome. ;)","13/Feb/13 17:22;yukim;I'm thinking of the cause of this can be CASSANDRA-5241, since this function relies on a lot of concurrent forceBlockingFlush. So there is a chance that compaction_in_progress flush is not complete at the end of the compaction.

If that is the case, we should wait until we fix CASSANDRA-5241.","13/Feb/13 17:34;brandon.williams;bq. If that is the case, we should wait until we fix CASSANDRA-5241.

+1, that is a troublesome bug in many ways.","14/Feb/13 16:30;yukim;OK, I will revert this from 1.2 branch but leave it in trunk.","14/Feb/13 17:33;yukim;I reverted change in 1.2 and changed fixver to 2.0.
However, we still need to fix conflict in opening CFS with the patch attached before.
Should I create and move to the new ticket for that?",13/Mar/13 19:50;yukim;We should also remove ancestors from sstable metadata because those could consume heap when you are using LCS(CASSANDRA-5342).,"24/May/13 14:41;jbellis;bq. However, we still need to fix conflict in opening CFS with the patch attached before.

(This was done in CASSANDRA-5350.)",26/Jun/13 16:29;slebresne;What's the status of this? Is there something more to do?,"27/Jun/13 16:47;slebresne;So let me see if I can sum what's going on here.

As far as trunk is concerned, the initial patch is still there but we suspect it may cause problems, namely:
# The AssertionError ""Incoherent new size -3 replacing ..."" error in ColumnFamilyStoreTest.
# The IllegalStateException ""Unfinished compactions reference missing sstables"" that Michael experienced.

As far as I can tell from reading the history, the other problems have been fixed, right?

Now, concerning 1) (the AssertionError), I ""think"" the problem is just with CFS.clearUnsafe(). Namely, it butcher the DataTracker for test purposes, but if a compaction runs concurrently, at the end of said compaction it will try to replace sstables that are not there anymore, hence the exception. I've committed a simple workaround in commit 3935587 that make clearUnsafe() stop compactions first. As far as I can tell, this did fixed the error (I've been able to run ColumnFamilyStoreTest a number of time without getting the stack while I had it consistently before).

Remains 2), the IllegalStateException. Yuki suggests that it may have been caused by CASSANDRA-5241 that is resolved now. So should we close this until further notice?  [~mkjellman] Do you remember if you were able to easily reproduce that error back when you tested this patch?
",28/Jun/13 21:02;mkjellman;[~slebresne] man this was a while back but if I remember it was fairly easy to reproduce.,"01/Jul/13 07:26;slebresne;Ok, so any opposition to closing this now and re-opening if it turns Michael's bug hasn't been fix by CASSANDRA-5241?",01/Jul/13 14:30;jbellis;SGTM.,,,,,,,,,,,,,,,,,,,,,,,,
streaming from another node results in a bogus % streamed on that sstable,CASSANDRA-5130,12626504,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,mkjellman,mkjellman,08/Jan/13 06:37,12/Mar/19 14:17,13/Mar/19 22:27,15/Jan/13 18:53,1.2.1,,,,,0,,,,,,,"reproducible thru a repair -pr

5689557% from node 10.8.25.132 ??

{code}
root@scl-cas06:~# nodetool netstats
Mode: NORMAL
Streaming to: /10.8.25.132
   /data2/cassandra/evidence/messages/evidence-messages-ia-2378-Data.db sections=1504 progress=10877426741/11376970910 - 95%
   /data/cassandra/evidence/messages/evidence-messages-ia-2396-Data.db sections=1326 progress=0/747963031 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2412-Data.db sections=342 progress=0/29791995 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2413-Data.db sections=307 progress=0/42375841 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2375-Data.db sections=702 progress=0/5138101708 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2416-Data.db sections=220 progress=0/21729661 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-2382-Data.db sections=1498 progress=0/7731727694 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-2388-Data.db sections=698 progress=0/2097813910 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-2401-Data.db sections=889 progress=0/154218729 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2411-Data.db sections=290 progress=0/45457960 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-2414-Data.db sections=22 progress=0/1809989 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2407-Data.db sections=423 progress=0/55302683 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2409-Data.db sections=73 progress=0/6366250 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-2377-Data.db sections=1298 progress=0/815617310 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2389-Data.db sections=167 progress=0/7307457 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2415-Data.db sections=126 progress=0/9260301 - 0%
 Nothing streaming to /10.8.30.16
Streaming from: /10.8.25.132
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-1091-Data.db sections=328 progress=0/52507334 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-1084-Data.db sections=1029 progress=0/198699737 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-1097-Data.db sections=35 progress=0/731705 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-1079-Data.db sections=1504 progress=1126955908333568/19807443126 - 5689557%
Pool Name                    Active   Pending      Completed
Commands                        n/a         0         459801
Responses                       n/a         0        1061826
{code}",Ubuntu 12.04,,,,,,,,,,,,,,,,,,15/Jan/13 17:55;yukim;5130.txt;https://issues.apache.org/jira/secure/attachment/12564959/5130.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-15 13:22:02.584,,,no_permission,,,,,,,,,,,,303111,,,Tue Jan 15 18:53:39 UTC 2013,,,,,,0|i17807:,250265,mkjellman,mkjellman,,,,,,,,,,"08/Jan/13 06:38;mkjellman;1141023% on this node?

{code}
root@scl-cas10:~# nodetool netstats
Mode: NORMAL
Streaming to: /10.8.30.103
   /data/cassandra/evidence/messages/evidence-messages-ia-1079-Data.db sections=1504 progress=12737080525/19807443126 - 64%
   /data2/cassandra/evidence/messages/evidence-messages-ia-1097-Data.db sections=35 progress=0/731705 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-1084-Data.db sections=1029 progress=0/198699737 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-1091-Data.db sections=328 progress=0/52507334 - 0%
Streaming from: /10.8.30.103
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2375-Data.db sections=702 progress=58626960326656/5138101708 - 1141023%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2389-Data.db sections=167 progress=0/7307457 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-2388-Data.db sections=698 progress=0/2097813910 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2416-Data.db sections=220 progress=0/21729661 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-2377-Data.db sections=1298 progress=0/815617310 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2409-Data.db sections=73 progress=0/6366250 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-2414-Data.db sections=22 progress=0/1809989 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-2382-Data.db sections=1498 progress=0/7731727694 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2407-Data.db sections=423 progress=0/55302683 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2411-Data.db sections=290 progress=0/45457960 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2415-Data.db sections=126 progress=0/9260301 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-2401-Data.db sections=889 progress=0/154218729 - 0%
Pool Name                    Active   Pending      Completed
Commands                        n/a         0         252046
Responses                       n/a         3         746069
{code}",08/Jan/13 06:39;mkjellman;related to https://issues.apache.org/jira/browse/CASSANDRA-4687 ? streaming wrong part of file?,15/Jan/13 13:22;brandon.williams;Do you have key cache on?,"15/Jan/13 16:40;mkjellman;key_cache_size_in_mb: 0
",15/Jan/13 17:55;yukim;I somehow screwed calculating progress of compressed file transferred. Fix attached.,15/Jan/13 18:02;mkjellman;+1 on patch and i'll make a build to actually test it to be extra sure in a moment,15/Jan/13 18:44;mkjellman;confirmed stream progress is being correctly calculated,"15/Jan/13 18:53;yukim;Committed, thanks for the review.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
newly bootstrapping nodes hang indefinitely in STATUS:BOOT while JOINING cluster,CASSANDRA-5129,12626499,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mkjellman,mkjellman,08/Jan/13 06:27,12/Mar/19 14:17,13/Mar/19 22:27,14/Feb/13 05:48,1.2.2,,,,,1,,,,,,,"bootstrapping a new node causes it to hang indefinitely in STATUS:BOOT

Nodes streaming to the new node report 

{code}
Mode: NORMAL
 Nothing streaming to /10.8.30.16
Not receiving any streams.
Pool Name                    Active   Pending      Completed
Commands                        n/a         0        1843990
Responses                       n/a         2         661750
{code}

the node being streamed to stuck in the JOINING state reports:

{code}
Mode: JOINING
Not sending any streams.
 Nothing streaming from /10.8.30.103
 Nothing streaming from /10.8.30.102
Pool Name                    Active   Pending      Completed
Commands                        n/a         0             10
Responses                       n/a         0         613577
{code}

it appears that the nodes in the ""nothing streaming"" state never sends a ""finished streaming"" to the joining node.

no exceptions are thrown during the streaming on either node while the node is in this state.

{code:name=""full gossip state of bootstrapping node""}
/10.8.30.16
  NET_VERSION:6
  RELEASE_VERSION:1.2.0
  STATUS:BOOT,127605887595351923798765477786913079289
  RACK:RAC1
  RPC_ADDRESS:0.0.0.0
  DC:DC1
  SCHEMA:5cd8420d-ce3c-3625-8293-67558a24816b
  HOST_ID:e20817ce-7454-4dc4-a1c6-b1dec35c4491
  LOAD:1.11824041581E11
{code}",Ubuntu 12.04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-01-18 22:47:39.035,,,no_permission,,,,,,,,,,,,303107,,,Wed Mar 13 11:27:41 UTC 2013,,,,,,0|i177zb:,250261,mkjellman,mkjellman,,,,,,,,,,18/Jan/13 22:47;joeyi;I can confirm this bug on CentOS 6.3 as well,"14/Feb/13 03:36;mkjellman;it appears this is related to secondary indexes. after the bootstrapping node finishes streaming it submits an index build. This gets submitted but never makes any progress and hangs indefinitely.

{code}
 INFO [Thread-382] 2013-02-13 18:02:57,205 StreamInSession.java (line 199) Finished streaming session 4ae0be23-75fb-11e2-ba65-8f73c0b9d93d from /10.138.12.10
 INFO [Thread-540] 2013-02-13 18:17:42,526 SecondaryIndexManager.java (line 137) Submitting index build of [domain_metadata.classificationIdx, domain_metadata.domaintypeIdx] for data in SSTableReader(path='/data/cassandra/brts/domain_metadata/brts-domain_metadata-ib-1-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-2-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-3-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-4-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-5-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-6-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-7-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-8-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-9-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-10-Data.db')
{code}

{code}
#nodetool compactionstats
pending tasks: 23
Active compaction remaining time :        n/a
{code}

also when C* is killed, the node hung with nothing streaming logs:
{code}
ERROR 19:37:42,274 Exception in thread Thread[Streaming to /10.138.12.11:1,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:101)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more
{code}","14/Feb/13 03:38;brandon.williams;Easily repros with toy data from stress:

{noformat}
 INFO 03:30:47,313 JOINING: Starting to bootstrap...
 INFO 03:30:48,522 Submitting index build of [Standard1.Idx1] for data in SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-4-Data.db')
 INFO 03:30:48,526 Enqueuing flush of Memtable-compactions_in_progress@893461718(177/177 serialized/live bytes, 7 ops)
 INFO 03:30:48,527 Writing Memtable-compactions_in_progress@893461718(177/177 serialized/live bytes, 7 ops)
 INFO 03:30:48,546 Completed flushing /var/lib/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-ib-1-Data.db (176 bytes) for commitlog position ReplayPosition(segmentId=1360812614633, position=75619)
 INFO 03:30:48,547 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-4-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-2-Data.db')]
{noformat}

and stays like that forever.","14/Feb/13 03:41;brandon.williams;Thread dump indicates this is actually CASSANDRA-5244 which has a good analysis, but is more severe than we thought.",14/Feb/13 05:48;mkjellman;fixed with 5244.,13/Mar/13 11:27;kcarlson;I'm still experiencing this on 1.2.2. Same exception and same result. I took a thread dump and it didn't have anything interesting in it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't prepare an INSERT query,CASSANDRA-5016,12618585,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,iamaleksey,iamaleksey,03/Dec/12 16:29,12/Mar/19 14:17,13/Mar/19 22:27,05/Dec/12 08:04,1.2.0 rc1,,,,,0,,,,,,,"Preparing an INSERT query fails with CQL3+binary protocol (maybe thrift as well, haven't checked). Preparing an equivalent UPDATE query works just fine.

demo (id int primary key, value text)
preparing ""INSERT INTO test.demo (id, value) VALUES (?, ?)"" -> 8704, Invalid definition for id, not a collection type
prparing ""UPDATE test.demo SET value = ? WHERE id = ?"" -> ok",,,,,,,,,,,,,,,,,,,03/Dec/12 18:22;slebresne;5016.patch;https://issues.apache.org/jira/secure/attachment/12555796/5016.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-05 08:04:37.913,,,no_permission,,,,,,,,,,,,293422,,,Wed Dec 05 08:04:37 UTC 2012,,,,,,0|i0szof:,167257,iamaleksey,iamaleksey,,,,,,,,,,04/Dec/12 23:33;iamaleksey;+1,"05/Dec/12 08:04;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL 3 ALTER TABLE ... ADD causes OOB,CASSANDRA-5117,12626142,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,khahn,khahn,04/Jan/13 23:38,12/Mar/19 14:17,13/Mar/19 22:27,08/Jan/13 08:49,1.2.1,,,,,0,,,,,,,"To reproduce:

./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 2.3.0 | Cassandra 1.2.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.35.0]
Use HELP for help.
cqlsh> create keyspace music with replication = {'CLASS' : 
   ... 'SimpleStrategy', 'replication_factor' : 3};
cqlsh> use music
   ... ;
cqlsh:music> CREATE TABLE songs (
         ... id uuid PRIMARY KEY,
         ... title text,
         ... album text,
         ... artist text,
         ... data blob
         ... );
cqlsh:music> insert into songs (id, title, artist, album) values ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', 'La Grange', 'ZZ Top', 'Tres Hombres');
cqlsh:music> insert into songs (id, title, artist, album) values ('8a172618-b121-4136-bb10-f665cfc469eb', 'Moving in Stereo', 'Fu Manchu', 'We Must Obey');
cqlsh:music> insert into songs (id, title, artist, album) values ('62c36092-82a1-3a00-93d1-46196ee77204', 'Outside Woman Blues', 'Back Door Slam', 'Roll Away');
cqlsh:music> CREATE TABLE song_tags (
         ... id uuid,
         ... tag_name text,
         ... PRIMARY KEY (id, tag_name)
         ... );
cqlsh:music> select * from song_tags;
cqlsh:music> INSERT INTO song_tags (id, tag_name) VALUES ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', 'blues');
cqlsh:music> INSERT INTO song_tags (id, tag_name) VALUES ('8a172618-b121-4136-bb10-f665cfc469eb', 'covers');
cqlsh:music> INSERT INTO song_tags (id, tag_name) VALUES ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', '1973');
cqlsh:music> INSERT INTO song_tags (id, tag_name) VALUES ('8a172618-b121-4136-bb10-f665cfc469eb', '2007');
cqlsh:music> select * from song_tags;

 id                                   | tag_name
--------------------------------------+----------
 a3e64f8f-bd44-4f28-b8d9-6938726e34d4 |     1973
 a3e64f8f-bd44-4f28-b8d9-6938726e34d4 |    blues
 8a172618-b121-4136-bb10-f665cfc469eb |     2007
 8a172618-b121-4136-bb10-f665cfc469eb |   covers

cqlsh:music> drop table song_tags;
cqlsh:music> ALTER TABLE songs ADD tags set<text>;
TSocket read 0 bytes
","Mac OSX, DS Java Driver, apache-cassandra-1.2.0-src downloaded from project Jan 2, 2013",,,,,,,,,,,,,,,,,,07/Jan/13 10:28;slebresne;5117-v2.txt;https://issues.apache.org/jira/secure/attachment/12563547/5117-v2.txt,06/Jan/13 01:16;dbrosius@apache.org;5117.txt;https://issues.apache.org/jira/secure/attachment/12563449/5117.txt,05/Jan/13 00:00;khahn;OOB.txt;https://issues.apache.org/jira/secure/attachment/12563380/OOB.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-01-04 23:55:59.026,,,no_permission,,,,,,,,,,,,302725,,,Tue Jan 08 08:49:45 UTC 2013,,,,,,0|i174zb:,249775,dbrosius,dbrosius,,,,,,,,,,04/Jan/13 23:55;iamaleksey;Looks like it could be related to CASSANDRA-5064.,05/Jan/13 00:00;khahn;system.log,"07/Jan/13 10:28;slebresne;I don't think the attached patch is correct (that is, it fixes the fact that componentIndex be set to a negative value, but it's not the correct fix). The problem is that in the case where we add a collection but we had not collection previous, we should not do the {{component--}} at all. Attaching a v2 to fix that.",08/Jan/13 05:34;dbrosius;+1,"08/Jan/13 08:49;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Migrating Clusters with gossip tables that have old dead nodes causes NPE, inability to join cluster",CASSANDRA-5211,12630218,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,rbranson,rbranson,31/Jan/13 21:18,12/Mar/19 14:17,13/Mar/19 22:27,12/Feb/13 22:55,1.2.2,,,,,0,,,,,,,"I had done a removetoken on this cluster when it was 1.1.x, and it had a ""ghost"" entry for the removed node still in the stored ring data. When the nodes loaded the table up after conversion to 1.2 and attempting to migrate to VNodes, I got the following traceback:

ERROR [WRITE-/10.0.0.0] 2013-01-31 18:35:44,788 CassandraDaemon.java (line 133) Exception in thread Thread[WRITE-/10.0.0.0,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:93)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:32)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:96)
	at org.apache.cassandra.db.SystemTable.loadDcRackInfo(SystemTable.java:402)
	at org.apache.cassandra.locator.Ec2Snitch.getDatacenter(Ec2Snitch.java:117)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:127)
	at org.apache.cassandra.net.OutboundTcpConnection.isLocalDC(OutboundTcpConnection.java:74)
	at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:270)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:142)

This is because these ghost nodes had a NULL tokens list in the system/peers table. A workaround was to delete the offending row in the system/peers table and restart the node.",,,,,,,,,,,,,,,,,,,06/Feb/13 14:20;brandon.williams;5211.txt;https://issues.apache.org/jira/secure/attachment/12568226/5211.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-06 13:56:10.972,,,no_permission,,,,,,,,,,,,310714,,,Tue Feb 12 22:55:49 UTC 2013,,,,,,0|i1hmfb:,311059,jbellis,jbellis,,,,,,,,,,"06/Feb/13 13:56;brandon.williams;Rick, I don't suppose you might happen to have one of these problem system tables left anywhere?  The trace indicates that the rack was missing but the dc wasn't, and we only write those together in a single insert.","06/Feb/13 14:20;brandon.williams;Confirmed that a null tokens list won't cause this.  Regardless of how we got here, it's more correct to confirm the existence of the dc and rack than just the dc, so patch to do so.",12/Feb/13 22:17;jbellis;+1,12/Feb/13 22:55;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table created through CQL3 are not accessble to Pig 0.10,CASSANDRA-5234,12631394,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alexliu68,shamim_ru,shamim_ru,08/Feb/13 08:13,12/Mar/19 14:17,13/Mar/19 22:27,28/Jun/13 23:09,1.2.7,,,,,4,,,,,,,"Hi,
  i have faced a bug when creating table through CQL3 and trying to load data through pig 0.10 as follows:
java.lang.RuntimeException: Column family 'abc' not found in keyspace 'XYZ'
	at org.apache.cassandra.hadoop.pig.CassandraStorage.initSchema(CassandraStorage.java:1112)
	at org.apache.cassandra.hadoop.pig.CassandraStorage.setLocation(CassandraStorage.java:615).
This effects from Simple table to table with compound key. ",Red hat linux 5,,,,,,,,,,,,,,,,,,18/Jun/13 16:13;alexliu68;5234-1-1.2-patch.txt;https://issues.apache.org/jira/secure/attachment/12588405/5234-1-1.2-patch.txt,14/Jun/13 05:57;alexliu68;5234-1.2-patch.txt;https://issues.apache.org/jira/secure/attachment/12587782/5234-1.2-patch.txt,20/Jun/13 22:28;alexliu68;5234-2-1.2branch.txt;https://issues.apache.org/jira/secure/attachment/12588946/5234-2-1.2branch.txt,25/Jun/13 22:19;alexliu68;5234-3-1.2branch.txt;https://issues.apache.org/jira/secure/attachment/12589677/5234-3-1.2branch.txt,26/Jun/13 23:06;alexliu68;5234-3-trunk.txt;https://issues.apache.org/jira/secure/attachment/12589811/5234-3-trunk.txt,30/May/13 18:24;alexliu68;5234.tx;https://issues.apache.org/jira/secure/attachment/12585444/5234.tx,10/Jul/13 08:24;shamim_ru;fix_where_clause.patch;https://issues.apache.org/jira/secure/attachment/12591607/fix_where_clause.patch,29/Jul/13 14:33;xcbsmith;pigCounter-patch.txt;https://issues.apache.org/jira/secure/attachment/12594708/pigCounter-patch.txt,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2013-02-08 16:12:14.497,,,no_permission,,,,,,,,,,,,311890,,,Thu Sep 05 18:35:48 UTC 2013,,,,,,0|i1hton:,312236,brandon.williams,brandon.williams,,,,,,,,,,08/Feb/13 16:12;iamaleksey;This is not a bug - CQL3 tables are intentionally not included in thrift describe_keyspace(s) (CASSANDRA-4377).,08/Feb/13 19:03;brandon.williams;See CASSANDRA-4421,"27/Mar/13 13:15;cscetbon;It means that CQL3 column families are not accessible through thrift and for me it's an issue (I do not agree with your Resolution label). That's why Pig 0.11 cannot use it. Is there a way to solve it ?
I can help you if necessary",13/May/13 08:48;cscetbon;It should be fixed after [CASSANDRA-4421|https://issues.apache.org/jira/browse/CASSANDRA-4421],16/May/13 04:13;alexliu68;I will work on it once I am done with other assignments soon.,16/May/13 05:44;cscetbon;thanks Alex !,"16/May/13 22:03;alexliu68;To fix it, we need modify CassandraStorage to get CF meta data from system table instead of thrift describe_keyspace because of the CQL3 table doesn't show up in thrift describe_keyspace call.",30/May/13 18:24;alexliu68;Patch is attached. It's on top of the 4421 patch,"30/May/13 18:32;alexliu68;pull @ https://github.com/alexliu68/cassandra/pull/3

Use CassandraStorage for any cql3 tables, you will have composite columns in ""columns"" bag

Use CqlStorage for any cql3 table.
{code}
cassandra://[username:password@]<keyspace>/<columnfamily>[?[page_size=<size>]
[&columns=<col1,col2>][&output_query=<prepared_statement>]
[&where_clause=<clause>][&split_size=<size>][&partitioner=<partitioner>]]
{code}

where 
  page_size is the number of cql3 rows per page (the default is 1000, it's optional)

  columns is the column names for the cql3 select query, it's optional
 
  where_clause is the user defined where clause on the indexed column, it's optional

  split_size is the number of C* rows per split which can be used to tune the number of mappers

  output_query is the prepared query for inserting data to cql3 table (replace the = by @ and ? by #,
      because Pig can't take = and ? as url parameter values)

Output row are in the following format
{code}
(((name, value), (name, value)), (value ... value), (value...value))
{code}

where the name and value tuples are key name and value pairs.


The input schema: ((name, value), (name, value), (name, value)) where keys are in the front.",14/Jun/13 05:51;alexliu68;I attach 5234-1.2-patch.txt to patch 1.2 branch. It update the last patch with the latest 4421 changes.,"14/Jun/13 05:53;alexliu68;The patch resolves the following issues.

1. allow access to cql3 type table through CassandraStorage.

2. create new CqlStorage to easy access cql3 tables.",14/Jun/13 09:23;cscetbon;Do you think I can test them now ?,"14/Jun/13 15:34;alexliu68;yes, I have done some testing.","14/Jun/13 17:46;cscetbon;Okay, I'll give it a try :)
thanks","17/Jun/13 13:54;cscetbon;My Pig script doesn't work anymore. I suppose you changed the input format ? 
I get :
Projected field [filtre] does not exist in schema: key:chararray,columns:bag{:tuple(name:tuple(),value:byte array)}
when column family is created without COMPACT STORAGE

Something weird is that I get no values when I should get some :
cqlsh:k1> SELECT * FROM cf1 WHERE ISE='XXXX';

 ise  | filtre | value_1
------+--------+---------
 XXXX |      1 |   81056

2013-06-17 13:13:01,372 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1
(XXXX,{((),),((filtre),),((value_1),<?)}) <-- value not printable for filtre ?","17/Jun/13 15:26;alexliu68;Can you post your schema for the table and pig script, so I can test it.","17/Jun/13 15:31;cscetbon;http://pastebin.com/Fub9t6j9 <-- my column family
http://pastebin.com/HwKxsC4f <-- my pig script",17/Jun/13 15:48;alexliu68;Can you just dump(data) to check whether you have all the data? Then do the following filter scripts.,"17/Jun/13 15:57;cscetbon;You definitely broke something as I get different results when dumping data for cql2 and cql3 tables (same structure).
CQL2 : 
{code}
* format is 
x: {key: chararray,value_1: (name: chararray,value: int),filtre: (name: chararray,value: int),columns: {(name: chararray,value: bytearray)}}
* rows are
(XXXX,(value_1,18584),(filtre,0),{})
(YYYY,(value_1,49926),(filtre,2),{})
{code}
CQL3 : 
{code}
* format is 
x: {key: chararray,columns: {(name: (),value: bytearray)}}
* rows are
(XXXX,{((),),((filtre),),((value_1),??)})
(YYYY,{((),),((filtre),),((value_1),??)})
{code}

","17/Jun/13 16:25;alexliu68;CQL2 has different structure from CQL3, CQL2 is more to the legacy thrift type CF(Check the developer blog from datastax). You should follow the cql3 format to change your pig script. or use CqlStorage for easy data mapping for cql3 type table.","17/Jun/13 16:28;alexliu68;Use the following script to find the structure of cql3 table, then change your pig script

{code}
   rows = LOAD 'cassandra://MyKeyspace/MyColumnFamily' USING CassandraStorage();
   dump(rows);
{code}","17/Jun/13 16:54;cscetbon;Great, CqlStorage() helps a lot to get same input.

Thanks",17/Jun/13 20:55;alexliu68;5234-1-1.2-patch.txt is attached to allow CassandraStorage to pass partitioner as parameter.,"20/Jun/13 22:28;alexliu68;5234-2-1.2branch.txt is attached to use ""cql://"" instead of ""cassandra://"" for CqlStorage","24/Jun/13 16:21;brandon.williams;Hmm, I'm seeing errors when running the examples/pig/test tests that don't use cql3.",24/Jun/13 20:17;alexliu68;I will fix it today.,"25/Jun/13 21:08;alexliu68;The failed test is where there is filter for COUNT(columns)

{code}
-- filter to fully visible rows (no uuid columns) and dump
visible = FILTER rows BY COUNT(columns) == 0;
dump visible;
{code}","25/Jun/13 22:21;alexliu68;It turns out to wide row schema issue which could be an error from previous version.

5234-3-1.2branch.txt is attached to fixd failing to run examples/pig/test ",26/Jun/13 20:05;tharigopla;@Alex I pulled the branch CASSANDRA-5234 and tried building it. But the build fails when compiling.,"26/Jun/13 20:16;alexliu68;It works for me.

try the commands

{code}
git checkout cassandra-1.2
patch -p1 < 5234-3-1.2branch.txt 
{code}","26/Jun/13 21:45;tharigopla;@Alex: I did check out cassandra-1.2 and tried to apply the patch using ""patch -p1 < 5234-3-1.2branch.txt"" But it didnt work it threw and exception < is reserveed for future use.
So, I downloaded the patch 5234-3-1.2branch.txt and did a git apply and then committed the unstaged changes. Now when I browsed the file location cassandra ""/ src / java / org / apache / cassandra / hadoop / pig /"" the patch is not applied. Sorry for my naive questions, I am very new to git and cassandra.","26/Jun/13 21:49;tharigopla;The patch is applied. But do I have to worry about the 

5234-3-1.2branch.txt:22: trailing whitespace.
                partitioner = FBUtilities.newPartitioner(client.describe_partitioner());
5234-3-1.2branch.txt:735: trailing whitespace.

5234-3-1.2branch.txt:925: trailing whitespace.

5234-3-1.2branch.txt:1297: trailing whitespace.
        }
5234-3-1.2branch.txt:1321: trailing whitespace.

","26/Jun/13 21:56;brandon.williams;Committed, and created CASSANDRA-5709 for example follow-up.  Thanks!","26/Jun/13 21:57;brandon.williams;[~tharigopla] it's in the 1.2 branch now, just do a git pull.",26/Jun/13 22:12;tharigopla;Thanks Brandon,26/Jun/13 22:58;alexliu68;5234-3-trunk.txt patch for trunk branch is attached.,27/Jun/13 15:55;tharigopla;@Brandon. Thanks for your patience. I did a git pull on 1.2 and ran the build. It fails again.,"27/Jun/13 16:09;brandon.williams;I recommend sending an email to user list with the details, as the 1.2 branch is obviously compiling for everyone and JIRA isn't a support forum.","09/Jul/13 15:07;shamim_ru;Hello Alex!
  I have got error when trying to send where_clause in cqlStorage URL as follows:
rows = LOAD 'cql://keyspace1/test?page_size=1&columns=title,age&split_size=4&where_clause=age=41' USING CqlStorage();
and haven't find any way to escape the equal operator.
  It will be better to send the where_clause with URL encoding, for example where_clause=age%3D41.
However for a quick fix i have slightly modify the method getQueryMap as follows:
    /** decompose the query to store the parameters in a map*/
    public static Map<String, String> getQueryMap(String query) throws Exception
    {
        String[] params = query.split(""&"");
        Map<String, String> map = new HashMap<String, String>();
        for (String param : params)
        {
            String[] keyValue = param.split(""="");
            map.put(keyValue[0], URLDecoder.decode(keyValue[1],""UTF-8""));
			
        }
        return map;
    }
and now i can send the query with URL-Encoding character.
","09/Jul/13 15:55;alexliu68;Thx, I will update the patch for it.",10/Jul/13 08:25;shamim_ru;add the patch as a temporary fix,"19/Jul/13 14:00;kkonrad;I was trying it locally and I'm not sure whether it fully works. My stacktrace and cassandra table schema is here: http://pastebin.com/uPUAs9T2 (i tried using old cassandra:// ... - it worked for other table) and output when I used cql://.. is here: http://pastebin.com/b0bKd7G3   . I have worked on this with Pig in local mode. It might be important or not: this table contained counters.

I was working on cassandra-1.2 with latest commit 27efded38d855b24f41e5332ffb29cd13d98f8da","29/Jul/13 03:41;xcbsmith;Just adding that I'm getting the same result as Konrad Kurdej. I'm using DSE 3.1, but I think this is the same bug. I was able to isolate the problem specifically to counter fields. Here's a simple set up and test case:

cqlsh> create keyspace pigtest with REPLICATION = { 'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> use pigtest;
cqlsh:pigtest> create table foo ( key_1 text primary key, value_1 bigint );
cqlsh:pigtest> create table foo2 ( key_1 text primary key, value_1 counter );
cqlsh:pigtest> update foo set value_1 = 1 where key_1 = 'foo';
cqlsh:pigtest> update foo2 set value_1 = value_1 + 2 where key_1 = 'foo2';
cqlsh:pigtest> select * from foo;

 key_1 | value_1
-------+---------
   foo |       1

cqlsh:pigtest> select * from foo2;

 key_1 | value_1
-------+---------
  foo2 |       2

Now, the following grunt commands:

counts = LOAD 'cassandra://pigtest/foo' USING CassandraStorage();
dump counts;

Will work, but:

counts = LOAD 'cassandra://pigtest/foo2' USING CassandraStorage();
dump counts;

Will fail with the same stack trace that Konrad mentioned.","29/Jul/13 14:33;xcbsmith;This patch against the head appears to have fixed the problem for me. I applied it to DSE 3.1 and it also worked for me.

Basic deal is to use LongType for validation too.",29/Jul/13 17:47;alexliu68;Use CqlStorage for your use case. CassandraStorage has some drawbacks.,29/Jul/13 18:06;alexliu68;CassandraStorage is legacy for any none-CQL3 tables. Use CqlStorage for CQL3 tables.,"30/Jul/13 01:55;marcostrama;Cassandra 1.2.8 was released (after regression in 1.2.7), but this last patch for counters appears to be left. Anyone can confirm this?

I downloaded the .deb from http://people.apache.org/~eevans/ but when in grunt i dump a table with counter column, get the error:

java.lang.IndexOutOfBoundsException
	at java.nio.Buffer.checkIndex(Buffer.java:537)
	at java.nio.HeapByteBuffer.getLong(HeapByteBuffer.java:410)
	at org.apache.cassandra.db.context.CounterContext.total(CounterContext.java:477)
	at org.apache.cassandra.db.marshal.AbstractCommutativeType.compose(AbstractCommutativeType.java:34)
	at org.apache.cassandra.db.marshal.AbstractCommutativeType.compose(AbstractCommutativeType.java:25)
	at org.apache.cassandra.hadoop.pig.AbstractCassandraStorage.columnToTuple(AbstractCassandraStorage.java:137)
	at org.apache.cassandra.hadoop.pig.CqlStorage.getNext(CqlStorage.java:110)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:211)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
	at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)

My table:
cqlsh:test> desc table votes_count_period3;

CREATE TABLE votes_count_period3 (
  period text,
  poll timeuuid,
  votes counter,
  PRIMARY KEY (period, poll)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

","30/Jul/13 02:20;marcostrama;I confirmed this issue. I downloaded the src, applied the patch and after build the cassandra jar, pig works with counter. This will be patched in 1.2.9?",30/Jul/13 03:31;alexliu68;+1,30/Jul/13 04:40;xcbsmith;[~alexliu68] The patch is to the base class shared by CqlStorage and CassandraStorage. They were both broken and they are now both fixed.,"05/Sep/13 18:35;jlemire;The last patch for counters to AbstractCassandraStorage.java has not been applied either to cassandra-1.2 or to the trunk. Consequently, the problem still exists in 1.2.9, as I could verify myself today. I found another recent bug report on SO for the same problem: http://stackoverflow.com/questions/18553230/error-with-cassandra-pig-cql-counter-column.

Should I open a new bug report for the counters bug even though we have a working patch or will you reopen the current issue?",,,,,,,,
Cassandra/Hadoop does not support current Hadoop releases,CASSANDRA-5201,12629942,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bcoverston,jeltema,jeltema,30/Jan/13 17:11,12/Mar/19 14:17,13/Mar/19 22:27,05/Mar/14 19:00,2.0.6,2.1 beta2,,,,10,,,,,,,"Using Hadoop 0.22.0 with Cassandra results in the stack trace below.
It appears that version 0.21+ changed org.apache.hadoop.mapreduce.JobContext
from a class to an interface.


Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.JobContext, but class was expected
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSplits(ColumnFamilyInputFormat.java:103)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:445)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:462)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1045)
	at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1042)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1153)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1042)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1062)
	at MyHadoopApp.run(MyHadoopApp.java:163)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
	at MyHadoopApp.main(MyHadoopApp.java:82)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:192)
",,,,,,,,,,,,,,,,,,,08/Mar/13 05:53;dbrosius;5201_a.txt;https://issues.apache.org/jira/secure/attachment/12572704/5201_a.txt,05/Mar/14 18:55;bcoverston;hadoop-compat-2.1-merge.patch;https://issues.apache.org/jira/secure/attachment/12632877/hadoop-compat-2.1-merge.patch,03/Dec/13 22:58;bcoverston;hadoopCompat.patch;https://issues.apache.org/jira/secure/attachment/12616870/hadoopCompat.patch,11/Feb/14 21:26;bcoverston;hadoopcompat-trunk.patch;https://issues.apache.org/jira/secure/attachment/12628339/hadoopcompat-trunk.patch,28/Feb/14 16:57;bcoverston;progressable-fix.patch;https://issues.apache.org/jira/secure/attachment/12631782/progressable-fix.patch,04/Mar/14 22:33;bcoverston;progressable-wrapper.patch;https://issues.apache.org/jira/secure/attachment/12632687/progressable-wrapper.patch,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2013-01-30 17:29:56.738,,,no_permission,,,,,,,,,,,,310438,,,Wed Mar 05 19:00:44 UTC 2014,,,,,,0|i1hkq7:,310783,brandon.williams,brandon.williams,,,,,,,,,,"30/Jan/13 17:29;jeromatron;The current stable hadoop version is still the 1.0.x line according to http://hadoop.apache.org/releases.html#Download As long as the 1.0.x support isn't adversely affected, I would think a patch to provide support for all hadoop versions would be welcome.","31/Jan/13 00:47;brandon.williams;I think the last time I looked into this I couldn't find a way to support both versions, so we're either stuck with 0.21+ being broken, or breaking it for everyone that's on a lower version already.",02/Feb/13 19:42;genx7up;Do you have a patch?,"05/Mar/13 02:11;dbrosius;what if the hadoop directory was split out into two separate sub-projects which produced two separate jars, thus a jar for old hadoop support and one for new hadoop. then folks could choose which jar to add to the cassandra classpath?","05/Mar/13 03:43;brandon.williams;That's not a bad idea, but I'm not sure how it'd work entirely, care to formulate a patch? :)","05/Mar/13 06:40;dbrosius;Actually, i only see 0.20.*, 1.0.* (both of which have JobContext as a class) on the central maven repository, and on apache's download page... 

Does the 0.21+ jars still exist, (supported)?

",07/Mar/13 20:23;genx7up;We are using the newer 0.23.x & are facing this integration issue. Any workaround?,"08/Mar/13 05:04;dbrosius;ah ok, here it is:

groupid: org.apache.hadoop
artifactid: hadoop-mapreduce-client-core
","08/Mar/13 05:53;dbrosius;initial patch against 1.2
  - pulls hadoop code into src/hadoop/hadoop-legacy
  - replicates that into src/hadoop/hadoop-new with changes for api changes.

pulls new hadoop dependencies and builds separate jars for each.

which jar you run against is still manual... command line switch?

posted version a so i didn't lose stuff.

This uses 0.23.6 versions of jars, which i assume is compatible with the 2.0a versions of the api.","08/Mar/13 10:35;jeromatron;I wonder if it would be preferable to have all of the code in tree in the same jar still, but have different package paths, like org.apache.cassandra.hadoop (for backwards compatibility) and org.apache.cassandra.hadoop2.  Would that work?","08/Mar/13 12:06;brian.jeltema@digitalenvoy.net;No workaround that I'm aware of. We don't actually have this problem; I just became aware of it by
change while looking into a problem report, and filed an issue.

Brian



","09/Mar/13 01:12;dbrosius;{quote} wonder if it would be preferable to have all of the code in tree in the same jar still, but have different package paths, like org.apache.cassandra.hadoop (for backwards compatibility) and org.apache.cassandra.hadoop2. Would that work?{quote}

probably could be done. here's some 'issues' with that approach.

1) external code won't work against both versions with the same code base. Obviously the modifications are insignificant, (packages) but still.
2) Makes the build.xml file marginally more complicated as you have to use <exclude> elements in the build
3) Building in IDEs is complicated as you need to add resource excludes, since these packages need to be built as a separate project (at least one of them does).
4) If you have two IDE projects targetting the same classes dir, cleaning one project cleans the other.

i could do it either way... whatever people think.
","19/May/13 07:49;michaelsembwever;What about simply putting the hadoop2 package into a github project?
it would become available for others to use, and c* can switch to it when they feel ready to drop support for hadoop-0.20

otherwise i'm in favour of separate jar files (apache-cassandra-hadoop-legacy-XXX.jar and apache-cassandra-hadoop-XXX.jar). c* already bundles too much into the one jar file IMHO.","19/May/13 08:37;michaelsembwever;{quote}What about simply putting the hadoop2 package into a github project?{quote}

Done @ https://github.com/michaelsembwever/cassandra-hadoop
 (i refactor the new package to hadoop1 instead of hadoop2, to better match the hadoop version we are actually supporting).","11/Oct/13 12:27;michaelsembwever;I've updated the github project so to be a [patch|https://github.com/michaelsembwever/cassandra-hadoop/commit/6d7555ea205354a606907e40c16db35072004594] off the InputFormat and OutputFormat classes as found in cassandra-1.2.10
It works against hadoop-0.22.0",13/Oct/13 16:55;jbellis;How common is hadoop2 usage now?  Can we drop hadoop1 for 2.1?  /cc [~bcoverston],"25/Oct/13 00:55;kkrugler;Re Hadoop 1.x vs. Hadoop 2.x - most companies I see are using a 1.x version of Hadoop. Usage of Hadoop 2.x is ramping up fast, but I'm guessing it will be at least another year (probably more) before you'd want to start thinking about phasing out support for Hadoop 1.x",25/Oct/13 07:07;michaelsembwever;Hadoop-2 only just came out of alpha/beta with hadoop-2.2.0,"25/Oct/13 08:34;nyo;Why dropping support for old hadoop versions while you can just use a different package name (e.g  org.apache.cassandra.hadoop2.<SomeClass>)? Imo, the code in here:
https://github.com/michaelsembwever/cassandra-hadoop/ should just be merged with mainstream implementation, so both 1.x and 2.x users can  be happy.

Or you can indeed make it a lot more complex by merging the 1.x and 2.x compatible implementations and deciding which one to use based on metadata obtained by polling the cluster, but that is a bit too sophisticated. For most people the first option, where they can choose which package to use, would be sufficient.","29/Oct/13 16:18;claudio.romo.otto;[~michaelsembwever] How could be used your patch to make work Cassandra + Hadoop 2.2 + Pig? I have your library compiled but cannot figure how to apply to Cassandra or Pig to make it work, because I still hit the exception from Cassandra:
java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.JobContext, but class was expected
	at org.apache.cassandra.hadoop.AbstractColumnFamilyInputFormat.getSplits(AbstractColumnFamilyInputFormat.java:115)","29/Oct/13 16:32;nyo;[~claudio.romo.otto] 
It is a bit complex but basically you need to use the org.apache.cassandra.hadoop2.ColumnFamilyInputFormat stuff from https://github.com/michaelsembwever/cassandra-hadoop/
As far as hadoop goes, I ran succesfully with a cdh 4.4.1 cloudera hadoop lib, and with a cassandra 1.2.10. ","29/Oct/13 19:36;claudio.romo.otto;So the correct way is to update cassandra 1.2.10 using org.apache.cassandra.hadoop2.ColumnFamilyInputFormat, right?","03/Dec/13 22:58;bcoverston;Poking around at other projects this generally gets solved in one of two ways: Ship two versions of their Hadoop integrations (one compiled for the old, and one compiled for the new), or use a little reflection to make things work across the board.

I'm attaching a patch that uses the hadoopCompat subproject of elephantbird. This will allow us to compile a single binary and run with the new and old context objects.

I've tested this patch with HDP 2.0, and Apache Hadoop 1.0.4 and it works fine with both (including Hive in DSE). With Pig I needed to compile our (optional) pig dependency with:

bq. ant clean jar-withouthadoop -Dhadoopversion=23

Only really needed if you're using one of the current versions of thrift with the new JobContext.
",03/Dec/13 23:36;bcoverston;These changes also depend on CASSANDRA-6309 for anything to work.,04/Dec/13 22:33;jbellis;WDYT [~dbrosius]?,17/Dec/13 03:31;jbellis;[~michaelsembwever]?  [~jeromatron]?,"04/Jan/14 23:29;jeromatron;Seems reasonable if they're keeping their code up to date with all of the releases.  The code appears lightweight to make to use it as well.  I've also tried to reach out to [~dvryaboy] via twitter to see if he has any feedback about the approach of having elephant bird as a dependency, e.g. any hidden costs or pitfalls being more familiar with the project.","05/Jan/14 22:10;dvryaboy;The EB HadoopCompat is what we use in production at Twitter, and plan to keep up to date in the foreseeable future. Glad you are finding it useful.

Maybe you can send that Reporter implementation as a pull request for hadoop-compat? Seems generally applicable.

Thanks to this ping, I looked around and noticed that our Parquet project uses a slightly different version of the same code -- we'll take a look and merge things. Shouldn't change anything significantly for this patch.

Also note that Tom White has a handy findbugs plugin to look for Hadoop incompatibility problems: https://github.com/tomwhite/hadoop-incompatibility-findbugs-detector 

Here's how you'd use it: https://github.com/Parquet/parquet-mr/pull/77/files","06/Jan/14 13:30;jeromatron;Thanks [~dvryaboy]!

Just for completeness the twitter thread is https://twitter.com/jeromatron/status/419607697588510721

[~bcoverston] [~jbellis] what do you think?

As for me, it sounds like the EB (or if it makes more sense Parquet) dependency makes sense.  The hadoop incompatibility findbugs detector also sounds great to include to catch anything before it is committed.  So I'm +1 on this approach.","06/Jan/14 18:02;bcoverston;I'll submit the reporter impl upstream. Thanks [~dvryaboy]!
",10/Feb/14 19:19;jbellis;Are we good here [~brandon.williams]?,11/Feb/14 15:39;brandon.williams;Committed to 2.0.  [~bcoverston] can you rebase a patch for trunk?,11/Feb/14 21:26;bcoverston;Attached trunk-rebased patch.,11/Feb/14 22:10;brandon.williams;Committed.,"27/Feb/14 13:37;moliware;Hi [~brandon.williams]

I'm using this patch and I keep having compatibility problems while trying to write something with pig using CqlStorage.
I'm using Hadoop 2.2 and Cassandra 2.0 branch.

This is the full stack trace:

{noformat}
java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
        at org.apache.cassandra.hadoop.Progressable.progress(Progressable.java:45)
        at org.apache.cassandra.hadoop.cql3.CqlRecordWriter.write(CqlRecordWriter.java:183)        
        at org.apache.cassandra.hadoop.cql3.CqlRecordWriter.write(CqlRecordWriter.java:63)        
        at org.apache.cassandra.hadoop.pig.CqlStorage.sendCqlQuery(CqlStorage.java:440)        
        at org.apache.cassandra.hadoop.pig.CqlStorage.cqlQueryFromTuple(CqlStorage.java:414)        
        at org.apache.cassandra.hadoop.pig.CqlStorage.putNext(CqlStorage.java:362)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)        
        at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:576)        
        at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)        
        at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:467)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:432)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:412)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:256)        
        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)        
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:645)        
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:405)        
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162)        
        at java.security.AccessController.doPrivileged(Native Method)        
        at javax.security.auth.Subject.doAs(Subject.java:415)        
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)        
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)
{noformat}

","28/Feb/14 16:57;bcoverston;[~moliware],  attaching a patch that will fix the issue with progressable when used on the jobcontext object. I've tested this with the latest HDP release, and it works with pig. [~brandon.williams] take a look.",28/Feb/14 17:51;moliware;Thanks [~bcoverston] I will test the patch probably on monday and will confirm you but it looks good. Thanks!,03/Mar/14 08:24;moliware;[~bcoverston] Tested and It worked! Thanks!,"03/Mar/14 08:44;hkropp;Does this patch include deployment with maven classifiers, so that hadoop1 or hadoop2 are referenced dependencies?","03/Mar/14 20:20;hkropp;I also was able to make this patch work with HDP 2.0. Nevertheless before this ticket is closed I would like to make 2 suggestions:
1. Use maven classifiers for deployment. If this is used in a hadoop2 project you don't want hadoop1 dependencies and vice versa. This approach is used by Parquet, Avro and EB. For detailed discussion please read [here|https://github.com/Parquet/parquet-mr/pull/32].
2. Change {{hadoop-core}} to {{hadoop-client}} dependency. {{hadoop-client}} is the preferred dependency. Read [here|http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH-Version-and-Packaging-Information/cdhvd_topic_8_1.html] and the link above.","03/Mar/14 23:33;bcoverston;[~hkropp] I agree that we need to do the second change, but I don't see a need to add deployment classifiers. Because we're using Hadoop-Compat (from EB) a single binary will work for both.

There's a niggle right now with Progressable that I'm working on, but a single binary will work for new and legacy deployments.","04/Mar/14 07:54;hkropp;I might be missing an important point, but Hadoop-Compat is nothing other than ContextUtil of hadoop2? It uses reflection to test what's on the classpath to decide how or better what to return. I made it work with just using ContextUtil before I found this patch here.

Therefor the binary works fine for both (btw I would not consider it legacy!), if you manage the dependencies on your own.

But if you would start a new project you would import Cassandra for example like this:
{code}
dependencies {
  ...
  compile 'org.apache.cassandra:cassandra:2.0.6'
  ...
}
{code}

What will happen now is, that this will load hadoop1 dependencies into my hadoop2 project for example, or not? To avoid this maven classifiers could be used, to call explicitly for hadoop2 dependencies:

{code}
dependencies {
  ....
  compile 'org.apache.cassandra:cassandra:2.0.6:hadoop2'
  ...
}
{code}","04/Mar/14 16:36;bcoverston;Take a look at the code, it works for Hadoop1 and Hadoop2 without recompliation, and without shipping two sets of dependencies. Basically it detects the current version of Hadoop that you're running and dynamically determines which TaskAttemptContext to use, the Class, or the Interface. There's no need to use deployment classifiers to solve this particular problem.","04/Mar/14 21:31;bcoverston;Attaching a patch that brings in hadoop-compat, and adds the progressable wrapper to it. I'm going to submit this upstream to the elephant bird project, so we should be able to remove this code and add the dependency in the future.",04/Mar/14 21:46;bcoverston;Updated progressable-wrapper.patch to conform to current namespaces.,"04/Mar/14 22:33;bcoverston;I removed the hadoop-compat dependency, as maven couldn't resolve the dependencies, reverted to the old version. This won't stop our input/output formats from being compatible with past and future releases.",05/Mar/14 18:55;bcoverston;Patch for 2.1 branch,05/Mar/14 19:00;brandon.williams;I had the same maven resolution problem with hadoop-compat.  Committed.,,,,,,,,,,,
Loading persisted ring state in a mixed cluster can throw AE,CASSANDRA-5197,12629825,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,30/Jan/13 02:23,12/Mar/19 14:17,13/Mar/19 22:27,13/Feb/13 23:01,1.2.2,,,,,0,qa-resolved,,,,,,"{noformat}
 INFO 02:07:16,263 Loading persisted ring state
java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.updateHostId(TokenMetadata.java:221)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:406)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:282)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:315)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
{noformat}

We assume every host has a hostid, but this is not always true.",,,,,,,,,,,,,,,,,,,31/Jan/13 19:31;brandon.williams;5197.txt;https://issues.apache.org/jira/secure/attachment/12567420/5197.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-13 22:43:33.302,,,no_permission,,,,,,,,,,,,310321,,,Wed Feb 13 23:01:20 UTC 2013,,,,,,0|i1hk07:,310666,iamaleksey,iamaleksey,,,,,,,,,enigmacurry,"31/Jan/13 19:31;brandon.williams;If you restart a 1.2 node in a mixed cluster, it assumes all hosts have a hostId, but the 1.1 nodes will not.",13/Feb/13 22:43;iamaleksey;+1,13/Feb/13 23:01;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memtable flushwriter can pick a blacklisted directory,CASSANDRA-5193,12629594,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,28/Jan/13 22:17,12/Mar/19 14:17,13/Mar/19 22:27,03/Feb/13 14:34,1.2.2,,,,,0,,,,,,,"The top-level data directory will be picked by DiskAwareRunnable (directory = Directories.getLocationCapableOfSize(writeSize)), and the top-level data directory itself might not be blacklisted (most likely won't be).

For the same reason we can't just add a blacklist-check in the middle of Directories#getLocationCapableOfSize - most often it's the sstable directory that gets blacklisted.

The issue seems to be caused by/related to CASSANDRA-4292, which was committed just two days prior 2116-2118 and undid some blacklist-aware directory-picking logic.

Anyway, DiskAwareRunnable should be altered to respect directory blacklist.",,,,,,,,,,,,,,,,,,,03/Feb/13 02:37;iamaleksey;5193.txt;https://issues.apache.org/jira/secure/attachment/12567753/5193.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-03 10:41:52.519,,,no_permission,,,,,,,,,,,,310090,,,Sun Feb 03 14:34:04 UTC 2013,,,,,,0|i1hikv:,310435,jbellis,jbellis,,,,,,,,,,03/Feb/13 01:47;iamaleksey;The patch makes DiskAwareRunnable blacklist-aware and makes best_effort policy work properly again.,03/Feb/13 10:41;jbellis;+1,"03/Feb/13 14:34;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 shouldn't lowercase DC names for NTS,CASSANDRA-5292,12634202,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,26/Feb/13 17:16,12/Mar/19 14:17,13/Mar/19 22:27,27/Feb/13 14:46,1.2.3,,,,,0,,,,,,,"In CREATE and ALTER statements, when a property map is given (replication, compaction and compression options), CQL3 lowercase the map keys to provide case insensitivity. The goal is to allow things like:
{noformat}
  replication = { 'Class' : 'SimpleStrategy', 'Replication_factor' : '1' }
{noformat}

However, this messes up with NTS, as it ends up lower-casing the datacenter names. As a consequence,
{noformat}
  replication = { 'class' : 'NetworkTopologyStrategy', 'DC1' : '1' }
{noformat}
will currently create a DC that is really called 'dc1', which is a problem because DC names are interpreted case sensitively otherwise (at least in PropertyFileSnitch).

That's the problem. Now I'm kind of hesitant on what is the right fix. I see the following possibilities:
# Remove the CQL3 lower-casing completely. I'll admit that providing case insensitivy for property map keys may not have been such a good idea in the first place. After all, those map keys are string literal, which rather suggest case sensitivity. However, making that change would be a break strictly speaking.
# Make DC name case insensitive. As much as I think DC names ought to be case insensitive, I'm not sure that's very doable in practice because that would imply storing DC names lower-cased internally, but DC names are exchanged over gossip and whatnot, so that would probably break all hell loose.
# Keep CQL3 case insensitivity for property map keys in general but special case internally for NTS. The problems I see with that is that 1) this will be ugly and 2) if we special case too much, we might break potential custom strategies inspired by NTS. I also had the idea of changing strategy options internally from Map<String, String> to some custom object that would be essentially a case insensitive string map (general case), but that would also hold the original case of keys so NTS (and any other likely-minded strategy) can do its stuff. This happens to not be a small patch however (I'm attaching the patch for reference because I wrote it, but I'm seriously wondering if it's not too overkill).
",,,,,,,,,,,,,,,,,,,26/Feb/13 17:17;slebresne;5292-option3.txt;https://issues.apache.org/jira/secure/attachment/12571005/5292-option3.txt,26/Feb/13 18:06;slebresne;5292.txt;https://issues.apache.org/jira/secure/attachment/12571012/5292.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-02-26 17:38:53.699,,,no_permission,,,,,,,,,,,,314695,,,Wed Feb 27 14:46:50 UTC 2013,,,,,,0|i1iazj:,315039,jbellis,jbellis,,,,,,,,,,"26/Feb/13 17:38;jbellis;IMO there are two principles we should be following here:

# schema options should be ""just a Map.""  This is the most straightforward and trying to be clever here will surprise people more than it will help.  Nor do I want to maintain ""almost-Map"" structures in the code.
# CQL should not ""helpfully"" lowercase Map keys.

My understanding is that we observe #2 elsewhere, but schema options are not-quite-a-Map in this respect.  In that case I think we should suck it up and take the backwards incompatibility to fix this.

If we violate #2 in general, that is a bitter pill to swallow, but it's still a bug that should be fixed.","26/Feb/13 18:06;slebresne;bq. My understanding is that we observe #2 elsewhere

Yes, we do.

bq. In that case I think we should suck it up and take the backwards incompatibility to fix this.

Yeah, I think I agree really. I'm attaching the trivial patch to do that.

bq. Nor do I want to maintain ""almost-Map"" structures in the code.

For the record, schema options and ""normal"" map don't really share any code outside the parser. But I agree on the rest, just saying :)
","26/Feb/13 19:10;jbellis;bq. For the record, schema options and ""normal"" map don't really share any code outside the parser. But I agree on the rest, just saying

Right.  But we're going to fix that, right? :)

+1 on the trivial patch.","27/Feb/13 14:46;slebresne;Alright, committed the trivial fix then. Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Composite comparators for super CFs broken in 1.2,CASSANDRA-5287,12633993,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,thobbs,thobbs,25/Feb/13 21:03,12/Mar/19 14:17,13/Mar/19 22:27,28/Feb/13 13:11,1.2.3,,,,,0,,,,,,,"In Cassandra 1.2.0 through 1.2.2, attempting to insert data into a super column family with a CompositeType comparator results in the following stacktrace:

{noformat}
ERROR 14:56:49,920 Error occurred during processing of message.
java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:249)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.split(AbstractCompositeType.java:126)
	at org.apache.cassandra.config.CFMetaData.getColumnDefinitionFromColumnName(CFMetaData.java:920)
	at org.apache.cassandra.thrift.ThriftValidation.validateColumnData(ThriftValidation.java:404)
	at org.apache.cassandra.thrift.ThriftValidation.validateColumnOrSuperColumn(ThriftValidation.java:287)
	at org.apache.cassandra.thrift.ThriftValidation.validateMutation(ThriftValidation.java:343)
	at org.apache.cassandra.thrift.CassandraServer.createMutationList(CassandraServer.java:704)
	at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:752)
	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResult(Cassandra.java:3622)
	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResult(Cassandra.java:3610)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

Client-side, you'll just see ""{{TTransportException: TSocket read 0 bytes}}"".

Cassandra 1.1 doesn't have the same problem.",,,,,,,,,,,,,,,,,,,25/Feb/13 21:04;thobbs;5287-pycassa-repro.py;https://issues.apache.org/jira/secure/attachment/12570859/5287-pycassa-repro.py,27/Feb/13 18:45;slebresne;5287.txt;https://issues.apache.org/jira/secure/attachment/12571242/5287.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-02-26 18:20:22.069,,,no_permission,,,,,,,,,,,,314487,,,Thu Feb 28 13:11:29 UTC 2013,,,,,,0|i1i9pb:,314831,thobbs,thobbs,,,,,,,,,,25/Feb/13 21:04;thobbs;Attached script reproduces the issue with pycassa.,26/Feb/13 18:20;slebresne;Patch attached (I haven't tested it yet tbh but pretty sure that's *a* problem if not *the* problem).,27/Feb/13 18:40;thobbs;Is the patch supposed to be against the 1.2 branch? It doesn't compile there.,"27/Feb/13 18:46;slebresne;Hum, that's what you get for not even compiling before attaching a patch I suppose. The newly attached version should work better. ","27/Feb/13 19:29;thobbs;The patch seems fine to me, but I'm not terribly familiar with this area of the code.  With it applied, all of the pycassa and phpcassa tests pass, and they cover this pretty well.","28/Feb/13 13:11;slebresne;bq. The patch seems fine to me, but I'm not terribly familiar with this area of the code.

For the record, what happens is that in CQL3 you can have a ColumnDefinition that applies to only a component of the column name, hence the special code for composite in CFMetadata.getColumnDefinitionFromColumnName. However, for super columns we pass to this method the subcolumn name, so using the comparator is bogus. But since super columns don't exist in CQL3, and since thrift don't allow to define ColumnDefinition that applies to only sub-component of composites, the patch just always use the 'else' branch that does the right thing.

bq. all of the pycassa and phpcassa tests pass, and they cover this pretty well

Alright, I'll consider that +1 enough :). So committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql3 doesn't support multiple clauses on primary key components,CASSANDRA-5230,12631343,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,07/Feb/13 21:52,12/Mar/19 14:17,13/Mar/19 22:27,20/Feb/13 16:53,1.2.2,,,,,0,,,,,,,"In trying to write a dtest for CASSANDRA-5225, I noticed that given a table such as:

{noformat}
CREATE TABLE foo (
  key text,
  c text,
  v text,
  PRIMARY KEY (key, c)
)
{noformat}

It is possible to slice the values of 1 or 2 for c:
{noformat}
select c from foo where key = 'foo' and c > '0' and c < '3';
{noformat}

However, there is no way to get these explicitly by name, even though it should be possible:
{noformat}
cqlsh:Keyspace1> select c from foo where key = 'foo' and c in ('1', '2');
Bad Request: PRIMARY KEY part c cannot be restricted by IN relation
{noformat}",,,,,,,,,,,,,,CASSANDRA-5376,,,,,20/Feb/13 06:59;slebresne;5230.patch;https://issues.apache.org/jira/secure/attachment/12570092/5230.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-20 06:59:31.14,,,no_permission,,,,,,,,,,,,311839,,,Thu Apr 11 20:18:37 UTC 2013,,,,,,0|i1htdb:,312185,jbellis,jbellis,,,,,,,,,,"20/Feb/13 06:59;slebresne;Agreed that we could allow that, especially since we allow it for COMPACT
STORAGE (the reason we were not allowing it previously is that it wasn't so
simple in the original code of CQL3 for reason that are not relevant anymore).
",20/Feb/13 13:26;jbellis;+1,"20/Feb/13 16:53;slebresne;Committed, thanks","11/Apr/13 19:34;happyoli;If I add another key like:

CREATE TABLE foo (
  key text,
  c text,
  d text,
  v text,
  PRIMARY KEY (key, c, d)
);

select c from foo where key = 'foo' and c in ('1', '2') will still result in a Bad Request error.

Is this the expected behavior?","11/Apr/13 20:18;iamaleksey;bq. Is this the expected behavior?

Yes, only the last part of the key can be restricted by IN. In your case it's 'd'.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 has error with Compund row keys when secondray index involved,CASSANDRA-5240,12631721,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,shsedghi,shsedghi,11/Feb/13 13:25,12/Mar/19 14:17,13/Mar/19 22:27,20/Feb/13 16:54,1.2.2,,,,,0,,,,,,,"CREATE TABLE  test(
    interval text,
    seq int,
    id int,
    severity int,
    PRIMARY KEY ((interval, seq), id))
    WITH CLUSTERING ORDER BY (id DESC);
--
CREATE INDEX ON test(severity);

insert into test(interval, seq, id , severity) values('t',1, 1, 1);
insert into test(interval, seq, id , severity) values('t',1, 2, 1);
insert into test(interval, seq, id , severity) values('t',1, 3, 2);
insert into test(interval, seq, id , severity) values('t',1, 4, 3);
insert into test(interval, seq, id , severity) values('t',2, 1, 3);
insert into test(interval, seq, id , severity) values('t',2, 2, 3);
insert into test(interval, seq, id , severity) values('t',2, 3, 1);
insert into test(interval, seq, id , severity) values('t',2, 4, 2);

select * from test where severity = 3 and  interval = 't' and seq =1;

Bad Request: Start key sorts after end key. This is not allowed; you probably should not specify end key at all under random partitioner


The following works fine

CREATE TABLE  test(
    interval text,
    id int,
    severity int,
    PRIMARY KEY (interval, id))
    WITH CLUSTERING ORDER BY (id DESC);
--
CREATE INDEX ON test(severity);

insert into test(interval, id , severity) values('t1', 4, 1);
insert into test(interval, id , severity) values('t1', 1, 3);
insert into test(interval, id , severity) values('t1', 2, 2);
insert into test(interval, id , severity) values('t1', 3, 3);
insert into test(interval, id , severity) values('t2', 3, 3);
 insert into test(interval, id , severity) values('t2', 1, 3);
 insert into test(interval, id , severity) values('t2', 2, 1);

select * from test where severity = 3 and  interval = 't1';
interval | id | severity
----------+----+----------
       t1 |  3 |        3
       t1 |  1 |        3


",Linux centos 6.3,,,,,,,,,,,,,,,,,,20/Feb/13 06:28;slebresne;5240.patch;https://issues.apache.org/jira/secure/attachment/12570086/5240.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-20 06:28:29.222,,,no_permission,,,,,,,,,,,,312217,,,Wed Feb 20 16:54:08 UTC 2013,,,,,,0|i1hvp3:,312563,jbellis,jbellis,,,,,,,,,,"20/Feb/13 06:28;slebresne;The underlying code, that uses a range slice since a 2ndary index is involved, was using an end-of-component for the end key of the range, which outside of being useless in that case, is actually wrong with random partitioner. Patch attached.",20/Feb/13 13:23;jbellis;+1 (send to Testing for unit test after commit),"20/Feb/13 16:54;slebresne;Committed, thanks.

I did pushed a dtest for this as far as testing is concerned.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix timestamp-based tomstone removal logic,CASSANDRA-5248,12632187,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,13/Feb/13 11:03,12/Mar/19 14:17,13/Mar/19 22:27,15/Feb/13 18:31,1.2.2,,,,,0,,,,,,,"Quoting the description of CASSANDRA-4671:
{quote}
In other words, we should force CompactionController.shouldPurge() to return true if min_timestamp(non-compacted-overlapping-sstables) > max_timestamp(compacted-sstables)
{quote}
but somehow this was translating in the code to:
{noformat}
if (sstable.getBloomFilter().isPresent(key.key) && sstable.getMinTimestamp() >= maxDeletionTimestamp)
    return false;
{noformat}
which, well, is reversed.

Attaching the trivial patch to fix. I note that we already had a test that catched this (CompactionsTest.testDontPurgeAccidentaly), but that test was racy in that most of the time the compaction was done in the same second than the removal done prior to that and thus the compaction wasn't considering the tombstone gcable even though gcgrace was 0. I've already pushed the addition of a 1 second delay to make sure the patch reliably catch this bug.
",,,,,,,,,,,,,,,,,,,13/Feb/13 11:04;slebresne;5248.txt;https://issues.apache.org/jira/secure/attachment/12569176/5248.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-13 14:25:54.902,,,no_permission,,,,,,,,,,,,312683,,,Fri Feb 15 18:31:15 UTC 2013,,,,,,0|i1hykn:,313029,jbellis,jbellis,,,,,,,,,,13/Feb/13 14:25;jbellis;+1,"13/Feb/13 15:16;yukim;Wait, we do want to delete even if key exits in other sstable but the sstable timestamp was older than max deletion time, don't we?
The patch changes the behavior to not delete even if such key exists.","13/Feb/13 20:25;yukim;Sorry, I was wrong. I just got confused. :(
You're right that the comparison should be opposite.
","15/Feb/13 18:31;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't announce migrations to pre-1.2 nodes,CASSANDRA-5334,12636457,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,enigmacurry,enigmacurry,11/Mar/13 22:23,12/Mar/19 14:17,13/Mar/19 22:27,14/Mar/13 00:11,1.2.3,,,,,0,qa-resolved,,,,,,"I have a mixed version cluster consisting of two 1.1.9 nodes and one 1.2.2 node upgraded from 1.1.9. 

The upgrade works, and I don't see any end user problems, however I see this exception in the logs on the non-upgraded nodes:

{code}
ERROR [MigrationStage:1] 2013-03-11 18:09:09,001 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:256)
	at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:397)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:373)
	at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:352)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}


Steps to reproduce:
{code}
ccm create -v 1.1.9 1.1.9
ccm populate -n 3
ccm start
ccm node1 stress
ccm node1 stop
{code}

edit ~/.ccm/1.1.9/cluster.conf and configure cassandra_dir to point to 1.2.2. Edit node1's cassandra.yaml to be 1.2 compliant.

{code}
ccm node1 start
{code}

The cluster is now a mixed version, and works for user queries, but with the exception above.",,,,,,,,,,CASSANDRA-5321,,,,,,,,,13/Mar/13 22:43;iamaleksey;5334-extra.txt;https://issues.apache.org/jira/secure/attachment/12573604/5334-extra.txt,13/Mar/13 20:18;iamaleksey;5334-v2.txt;https://issues.apache.org/jira/secure/attachment/12573574/5334-v2.txt,13/Mar/13 18:18;slebresne;5334.patch;https://issues.apache.org/jira/secure/attachment/12573550/5334.patch,13/Mar/13 17:00;enigmacurry;cassandra.trunk.yaml;https://issues.apache.org/jira/secure/attachment/12573533/cassandra.trunk.yaml,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-03-12 18:02:33.426,,,no_permission,,,,,,,,,,,,316949,,,Thu Mar 14 00:11:50 UTC 2013,,,,,,0|i1iovr:,317291,jbellis,jbellis,,,,,,,,,enigmacurry,11/Mar/13 22:29;enigmacurry;I've added a [dtest|https://github.com/riptano/cassandra-dtest/commit/4432c02cc09fb61adce3121a72429132bcc29451] to test this scenario as well.,12/Mar/13 18:02;brandon.williams;Do you know which step is causing the exception?,"12/Mar/13 18:06;enigmacurry;It only happens once you bring node1 up on the new version, the error appears at that moment on the other nodes.","13/Mar/13 09:54;slebresne;This was introduced by CASSANDRA-4433, that removed the ""name"" column of the schema_keyspaces table, but that one, while redundant, is used by 1.1 nodes.

This boils down to the problem I described in CASSANDRA-4603: we cannot easily remove anything in the schema tables without breaking rolling upgrades.

So attaching a patch that reintroduce the ""name"" column in 1.2 (we don't use it but we write it for compatibility sake). 
","13/Mar/13 16:00;enigmacurry;[~slebresne], applying your patch and reproducing the steps above I get this in node2's log (running 1.1.9). The first exception is similar but different, perhaps another field needs migrating? The second exception I'm not sure if it's related or needs another ticket.

{code}
ERROR [MigrationStage:1] 2013-03-13 11:57:13,722 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.toInt(ByteBufferUtil.java:414)
	at org.apache.cassandra.cql.jdbc.JdbcInt32.compose(JdbcInt32.java:98)
	at org.apache.cassandra.db.marshal.Int32Type.compose(Int32Type.java:37)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getInt(UntypedResultSet.java:97)
	at org.apache.cassandra.config.CFMetaData.fromSchemaNoColumns(CFMetaData.java:1202)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1247)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:299)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:462)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:374)
	at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:352)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [ReadStage:33] 2013-03-13 11:57:23,794 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[ReadStage:33,5,main]
java.lang.AssertionError: Unknown keyspace system_auth
	at org.apache.cassandra.db.Table.<init>(Table.java:287)
	at org.apache.cassandra.db.Table.open(Table.java:119)
	at org.apache.cassandra.db.Table.open(Table.java:97)
	at org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:44)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:63)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}","13/Mar/13 16:43;slebresne;Quick question: does the dtest create new tables in the mixed version cluster? The last NPE seems related to the old CF id removed by CASSANDRA-3794, however we still write the old id for column families that have one in 1.2, so in theory this should only happen if a new column family is created. If that is the case (that the test create a column family in the mixed version cluster), then I'm afraid this is not supported between 1.1 and 1.2 (due to CASSANDRA-3794).

For the 2nd error (the system_auth one), is the dtest doing anything related to authorization in the mixed version cluster? ","13/Mar/13 16:53;enigmacurry;I thought maybe the dtest was doing something I didn't want to have happen, so I'm not using it for this bug report. The two exceptions in my previous comment were from issuing ccm commands directly on the command line exactly as I have in the original report (no dtest used.)

Both exceptions appear in the logs directly after starting the upgraded node. The only queries issued in this test is the stress command before the upgrade, no queries were issued after the upgrade.","13/Mar/13 17:00;enigmacurry;The cassandra.yaml for the 1.1.9 nodes are the default that ccm creates.

Attached is my cassandra.yaml that I use for the upgraded node. This is basically the default one that ccm creates for 1.2.2 modified to use the RandomPartitioner that the node is using and some path fixes.","13/Mar/13 18:17;slebresne;Alright. The problem is that due to CASSANDRA-3794 (and to a lower extend CASSANDRA-4433), new keyspaces/column families cannot be created in a mixed 1.1/1.2 cluster (keyspace lacks the ""name"" column and column families lack an old-style CfId).

This is a limitation we accepted in CASSANDRA-3794, but unfortunately, 1.2 creates the system_auth keyspace/column families on startup if they don't exist which triggers pretty much all the stacktrace on this issue.

One hacky solution I can see is to 1) reintroduce the ""name"" column for keyspace, and 2) we manually provide an old-style cfId for the Auth.USERS_CF. Attaching an updated patch that does both. This is ugly however.
","13/Mar/13 18:37;jbellis;Alternatively, could we hack 1.1.x to ignore the auth CFs?","13/Mar/13 19:37;enigmacurry;I've tested [~slebresne]'s patch; it works to resolve all the NPEs, assuming that's the way we want to go. I'm seeing some other issues after the upgrade, but I'll break those out into separate bugs as I'm pretty sure they're unrelated to this report.

","13/Mar/13 20:05;iamaleksey;Should MigrationManager.announce() send migrations to 1.1 nodes at all?

{noformat}
            // don't send migrations to the nodes with the versions older than < 1.1
            if (MessagingService.instance().getVersion(endpoint) < MessagingService.VERSION_11)
                continue;
{noformat}

Should change it to '< MessagingService.VERSION_12'. Any downsides to it?","13/Mar/13 20:11;jbellis;That sounds simplest to me, and should fix CASSANDRA-5343.  (IIRC we already have code to prevent 1.1 from pulling the schema from 1.2 nodes, so we just need to stop 1.2 from pushing it as well.)",13/Mar/13 20:18;iamaleksey;v2 prevents 1.2 nodes from pushing migrations to 1.1 nodes.,"13/Mar/13 20:55;enigmacurry;Tested [~iamaleksey]'s patch fixes all NPEs and resolves this issue. It mostly fixes CASSANDRA-5343, too, but there is a lingering AsseritionError.","13/Mar/13 21:47;enigmacurry;This is intermittently reoccurring for me, I'll assign it to me until I can come up with a dependable way to reproduce it.","13/Mar/13 21:51;enigmacurry;hmm, looks like the same NPE is coming from a different place than before:

{code}
ERROR [InternalResponseStage:1] 2013-03-13 17:41:01,836 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[InternalResponseStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
        at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
        at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
        at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
        at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
        at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:256)
        at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:397)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:373)
        at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:352)
        at org.apache.cassandra.service.MigrationManager$MigrationTask$1.response(MigrationManager.java:453)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:45)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}","13/Mar/13 22:07;enigmacurry;giving this back to [~iamaleksey] to revisit ""IIRC we already have code to prevent 1.1 from pulling the schema from 1.2 nodes.""",13/Mar/13 22:43;iamaleksey;5334-extra patch modifies 1.2 MigrationRequestVerbHandler to return an empty collection to 1.1 nodes' schema requests.,"13/Mar/13 23:07;enigmacurry;Great, 5334-extra seems to clear up the intermittent failures. At least, I ran my dtest 10 times before and it failed half the time, and now I ran it 10 times with no failures.",14/Mar/13 00:05;jbellis;+1,"14/Mar/13 00:11;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insufficient validation of UPDATE queries against counter cfs,CASSANDRA-5300,12634654,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,iamaleksey,iamaleksey,28/Feb/13 18:19,12/Mar/19 14:17,13/Mar/19 22:27,01/Mar/13 13:30,1.2.3,,,,,0,,,,,,,"{noformat}
CREATE TABLE demo (
  id int PRIMARY KEY,
  c counter
)
{noformat}

This is expected:
{noformat}
insert into demo (id, c) VALUES ( 0, 20);
Bad Request: INSERT statement are not allowed on counter tables, use UPDATE instead
{noformat}

This should also be forbidden, but it is not:
{noformat}
update demo set c = 20 where id = 0;
select * from demo;

 id | c
----+----
  0 | 20
{noformat}",,,,,,,,,,,,,,,,,,,01/Mar/13 10:28;slebresne;5300-v2.txt;https://issues.apache.org/jira/secure/attachment/12571561/5300-v2.txt,28/Feb/13 19:41;iamaleksey;5300.txt;https://issues.apache.org/jira/secure/attachment/12571453/5300.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-01 10:00:53.528,,,no_permission,,,,,,,,,,,,315147,,,Fri Mar 01 13:30:26 UTC 2013,,,,,,0|i1idrz:,315491,iamaleksey,iamaleksey,,,,,,,,,,"28/Feb/13 18:24;iamaleksey;1.1 is not affected - this is a 1.2 regression.

{noformat}
update demo set c = 20 where id = 0;
Bad Request: invalid operation for commutative columnfamily demo
{noformat}","01/Mar/13 10:00;marco.matarazzo;I was hoping this was a feature, and was about to be extended to INSERT too, it can be very useful during table initialization/import using COPY FROM on cqlsh or just a bunch of saved CQL.
","01/Mar/13 10:04;ggargani;I welcomed it as nice feature, too. Forcing a defined value in a counter does have uses (i.e., when you have a cap to the value a counter can reach)","01/Mar/13 10:28;slebresne;I'd prefer doing the validation in Operation.SetValue directly because that where we have all the other type validation now (and it's cleaner that way imo), as done in the attached v2.

bq. it can be very useful during table initialization/import

For the record, nobody contesting being able to set a counter would be useful.  But it doesn't work and I'm pretty sure it cannot work with the current implementation of counters. Don't be fooled by the example above, it's not complete. If you try an increment after the last update, you will end up with a server side exception because you've basically corrupted your data.
","01/Mar/13 12:39;iamaleksey;bq. I'd prefer doing the validation in Operation.SetValue directly because that where we have all the other type validation now (and it's cleaner that way imo), as done in the attached v2.

wfm, +1","01/Mar/13 13:30;slebresne;v2 commmitted, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair -pr with vnodes incompatibilty,CASSANDRA-5329,12636355,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,julien_campan,julien_campan,11/Mar/13 12:38,12/Mar/19 14:17,13/Mar/19 22:27,11/Mar/13 21:24,1.2.3,,,,,2,,,,,,,"Hi,

I have a cluster on 1.2.2 .

This cluster is composed of 16 nodes in two datacenters (8 and 8) with an RF 3 :3.

 I used virtual nodes, 256 on each node.

When I do “repair –pr"" on a node, I can see that  it’s doing repair only on the first vnode :

[2013-03-07 14:42:56,922] Starting repair command #7, repairing 1 ranges for keyspace pns_fr

[2013-03-07 14:42:57,835] Repair session eb38dfa0-872c-11e2-af2d-f36fae36bab1 for range (-9064588501660224828,-9063047458265491057] finished

[2013-03-07 14:42:57,835] Repair command #7 finished

[2013-03-07 14:42:57,852] Starting repair command #8, repairing 1 ranges for keyspace hbxtest

[2013-03-07 14:42:59,307] Repair session ebc6c7c0-872c-11e2-af2d-f36fae36bab1 for range (-9064588501660224828,-9063047458265491057] finished

So if  I understand well, when I do a ""repair –pr""  on each node, I will repair only the first vnode on each node. (16 token ranges on 4096 ranges). 

This method doesn’t guarantee the consistency of the dataset.

It seems to me that the ""repair –pr"" is not compatible with vnode cluster.  ",,,,,,,,,,,,,,,,,,,11/Mar/13 19:42;yukim;5329.txt;https://issues.apache.org/jira/secure/attachment/12573156/5329.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-11 14:28:27.659,,,no_permission,,,,,,,,,,,,316847,,,Mon Mar 11 21:24:32 UTC 2013,,,,,,0|i1io93:,317189,brandon.williams,brandon.williams,,,,,,,,,,"11/Mar/13 14:28;marco.matarazzo;I second this, we see exactly the same behaviour.",11/Mar/13 19:42;yukim;We should have used getLocalPrimaryRanges instead of getLocalPrimaryRange for 1.2 and above. Fix attached.,11/Mar/13 20:46;brandon.williams;+1,11/Mar/13 21:24;yukim;Committed. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted handoff sends over 1000 rows for one column change,CASSANDRA-5179,12628677,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,anttiko,anttiko,22/Jan/13 08:15,12/Mar/19 14:17,13/Mar/19 22:27,20/Mar/13 17:29,1.2.4,,,,,2,,,,,,,"We have a small test environment with two datacenters (DC1 and DC2) running on Windows 7 laptops.
Both datacenters have one node. We use network topology strategy to replicate all data to both datacenters.

We started with empty db. 
1. Created a keyspace with strategy options [DC1:1, DC2:1]
2. Added one row to a column family with CLI to DC1. Change was replicated to DC2.
3. Disconnected network cable from DC2.
4. Gossiper noticed, that other DC is dead.
5. Added another row to DC1.
6. Reconnected cable on DC2.
7. DC1 started hinted handoff for DC2.
8. Hinted handoff is finished with message: ""Finished hinted handoff of 1969 rows to endpoint <DC2 ip>""

We repeated test with same results on Linux cluster with Cassandra 1.2.0. 

On Cassandra 1.1.5 Linux cluster, only one row was sent to endpoint. ""Finished hinted handoff of 1 rows to endpoint <DC2 ip>""","Windows 7
Java 1.6u38",,,,,,,,,,,,,,,,,,25/Jan/13 07:59;anttiko;cassandra_receiver.log;https://issues.apache.org/jira/secure/attachment/12566481/cassandra_receiver.log,25/Jan/13 07:59;anttiko;cassandra_sender.log;https://issues.apache.org/jira/secure/attachment/12566480/cassandra_sender.log,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-23 17:29:20.415,,,no_permission,,,,,,,,,,,,307202,,,Wed Mar 20 17:29:47 UTC 2013,,,,,,0|i1ahvr:,269363,jbellis,jbellis,,,,,,,,,,23/Jan/13 17:29;jbellis;Can you give us a debug log?,"04/Feb/13 16:02;cscetbon;It's written that it's fixed in 1.2.2
Do you know if a bugfix corrected it ? ",04/Feb/13 16:23;jbellis;Fix-for is target fix version.  The issue is not fixed until resolved.,04/Feb/13 16:25;cscetbon;Ok thanks for this information …,"12/Mar/13 04:14;iamaleksey;4881221363f984ab6610756cab38e1a016b79e15 (CASSANDRA-4761) broke it. Current pagination logic doesn't deal well with the fact that deleteHint is now being called from a response handler callback and can go through the same hint again and again and again until it's finally replaced with a tombstone. This becomes really visible in multi-dc setups, where it can take a while to complete the write, but I was able to trigger it with ccm as well (the same hint got sent up to 3 times).

Should we reopen CASSANDRA-4761 or deal with it here?",12/Mar/13 04:33;jbellis;Let's fix it here.,12/Mar/13 04:40;iamaleksey;k. This is the problematic branch btw: https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/HintedHandOffManager.java#L336,12/Mar/13 05:41;iamaleksey;https://github.com/iamaleksey/cassandra/compare/5179,18/Mar/13 15:20;mkjellman;[~jbellis] patch looks good to me,"18/Mar/13 15:44;jbellis;If we time out we should probably cease further delivery attempts, to avoid hammering a node that is behind.","18/Mar/13 17:47;iamaleksey;bq. If we time out we should probably cease further delivery attempts, to avoid hammering a node that is behind.

Probably. But that's not what causing this particular issue - the patch only fixes that faulty pagination logic.

I don't see an easy way to stop it on a timeout as we did in 1.1 now (yet), but that's a problem for another ticket anyway.",18/Mar/13 18:17;jbellis;We can't put that code in the empty catch block here?,18/Mar/13 18:20;iamaleksey;It'll be of no use - all the requests have been sent at that point. It's waiting for responses before forcing compaction.,"18/Mar/13 20:50;jbellis;What if we made it wait per page, instead of all at once?

Seems like that would be a good way to put a bound on how many callbacks we need to keep around too.","18/Mar/13 23:46;iamaleksey;bq. What if we made it wait per page, instead of all at once?

This goes against CASSANDRA-4761 somewhat, but I think it's a good compromise between sending hints one at a time and pouring everything out. Updated https://github.com/iamaleksey/cassandra/compare/5179","19/Mar/13 01:08;jbellis;You could use break-to-label instead of a bool in the get() loop, but shouldn't it just be a return instead of break?","19/Mar/13 01:20;iamaleksey;bq. You could use break-to-label instead of a bool in the get() loop, but shouldn't it just be a return instead of break?

Yes I could. But some requests after the timed-out one could actually have succeeded - with the ratelimiter delay and all, and since we sent them, we might as well wait for the replies before triggering compaction (hints are deleted in that callback). And with return instead of break 1) compaction wouldn't be triggered, event if it's the last page of many and 2) ""Finished hinted handoff .."" message wouldn't be logged.","19/Mar/13 01:24;jbellis;I think the reasoning for return instead of break in the FD block was that, if we haven't finished sending hints then we probably don't want to force a major compaction that will rewrite a bunch of undelivered hints.",19/Mar/13 01:26;iamaleksey;Maybe. It was break in 1.1 though - https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/db/HintedHandOffManager.java#L375,"19/Mar/13 01:33;jbellis;True...  and 1.0 had a ""if rowsReplayed > 0"" around it.  Not sure why we removed that, except maybe that it would almost always be true.

How about we make the 1.0 check smarter and compact if we delivered over half the hints?","19/Mar/13 01:34;iamaleksey;bq. How about we make the 1.0 check smarter and compact if we delivered over half the hints?

wfm","19/Mar/13 05:43;iamaleksey;Actually, it doesn't wfm. HHM is already complicated enough. I think returning completely is just fine there - as long as the log message mentions how many hints have been delivered.

There will be many more opportunities for compaction - when hints to another node are all sent, or after another attempt in <= 10 minutes.

Updated the branch (also did some *very* minor refactoring so that the paging logic would at least fit in a single screen, plus some even minorer changes that I just couldn't resist).",20/Mar/13 15:50;jbellis;+1,"20/Mar/13 17:29;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 SelectStatement should not share slice query filters amongst ReadCommand,CASSANDRA-4928,12615193,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,07/Nov/12 17:11,12/Mar/19 14:16,13/Mar/19 22:27,07/Nov/12 18:33,1.2.0 beta 3,,,,,0,,,,,,,"In 1.2, SliceQueryFilter is stateful and should thus not be shared but SelectStatement is doing some sharing in the case of an IN query. This is the reason why cql_test:TestCQL.limit_multiget_test in the dtests is failing intermittently.

Let's be clear that the fact that SliceQueryFilter is stateful is ugly, but that is a concession made for performance until we can refactor all this more cleanly (which still being efficient). Such refactor being, as far as I can tell, far from trivial.",,,,,,,,,,,,,,,,,,,07/Nov/12 17:12;slebresne;4928.txt;https://issues.apache.org/jira/secure/attachment/12552494/4928.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-07 17:55:23.868,,,no_permission,,,,,,,,,,,,255791,,,Wed Nov 07 18:33:50 UTC 2012,,,,,,0|i0fux3:,90607,jbellis,jbellis,,,,,,,,,,07/Nov/12 17:55;jbellis;+1,"07/Nov/12 18:33;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
schema_* CFs do not respect column comparator which leads to CLI commands failure.,CASSANDRA-4093,12548388,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,dbrosius@apache.org,dbrosius@apache.org,27/Mar/12 20:59,12/Mar/19 14:16,13/Mar/19 22:27,09/Apr/12 16:58,1.1.0,,Legacy/Tools,,,0,,,,,,,"ColumnDefinition.{ascii, utf8, bool, ...} static methods used to initialize schema_* CFs column_metadata do not respect CF comparator and use ByteBufferUtil.bytes(...) for column names which creates problems in CLI and probably in other places.

The CompositeType validator throws exception on first column

String columnName = columnNameValidator.getString(columnDef.name);

Because it appears the composite type length header is wrong (25455)

AbstractCompositeType.getWithShortLength

java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:247)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:50)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:59)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getString(AbstractCompositeType.java:139)
	at org.apache.cassandra.cli.CliClient.describeColumnFamily(CliClient.java:2046)
	at org.apache.cassandra.cli.CliClient.describeKeySpace(CliClient.java:1969)
	at org.apache.cassandra.cli.CliClient.executeShowKeySpaces(CliClient.java:1574)

(seen in trunk)",,,,,,,,,,,,,,,,,,,29/Mar/12 19:02;slebresne;4093.txt;https://issues.apache.org/jira/secure/attachment/12520482/4093.txt,06/Apr/12 16:15;slebresne;4093_v2.txt;https://issues.apache.org/jira/secure/attachment/12521691/4093_v2.txt,28/Mar/12 00:26;xedin;CASSANDRA-4093-CD-changes.patch;https://issues.apache.org/jira/secure/attachment/12520216/CASSANDRA-4093-CD-changes.patch,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-03-27 21:11:15.475,,,no_permission,,,,,,,,,,,,233485,,,Mon Apr 09 16:58:41 UTC 2012,,,,,,0|i0grvz:,95948,jbellis,jbellis,,,,,,,,,,27/Mar/12 21:11;xedin;Can you please provide a simple test-case for this problem? It doesn't seem to be CLI related but rather CompositeType related problem...,"27/Mar/12 23:28;dbrosius@apache.org;I just created a brand new database directory, went into cli and did

show keyspaces;

the output finishes with this: Notice the trailing 'null'. That is caused by the exception.

    ColumnFamily: schema_columnfamilies
    ""ColumnFamily definitions""
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.AsciiType,org.apache.cassandra.db.marshal.AsciiType)
      GC grace seconds: 10368000
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.0
      DC Local Read repair chance: 0.0
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: []
      Column Metadata:
null


","28/Mar/12 00:26;xedin;patch makes changes to ColumnDefinition to make it CF comparator aware, CLI `show` and `describe` commands works just as expected but it breaks ColumnFamily related migrations with some weird NPE exceptions.","29/Mar/12 18:37;jbellis;Hmm, I'm pretty sure that giving CFMetdata column names a composite type header is the wrong solution. I think instead we need to teach cliclient that when you have a composite type comparator, you need to look at column aliases as well as column names to form the composite in question.","29/Mar/12 18:45;xedin;I don't get it, if the column names in column_metadata were not serialized using given CF comparator shouldn't we fix that instead of changing CLI?","29/Mar/12 18:56;jbellis;No, because when you have column aliases the name in the metadata is a ""leaf"" name, it's not supposed to be a full composite name. In other words,

composite name = (alias name, metadata name)","29/Mar/12 19:02;slebresne;So the problem is the following: for CQL3, we've introduced the fact that when we have a composite comparator, the 'name' of a ColumnDefinition refers to the last component of the composite comparator. In other words, one should use the last comparator of the composite comparator to parse the ColumnDefinition name, but the CLI doesn't know that. So we do need to fix the CLI.

However, now that I think about this, I realize we may have been a little bit careless when doing that, because the current code basically *assumes* that if you have a composite comparator, then ColumnDefinition refers to the last component of those. That may break compatibility for user using composite type today. I'm not sure how many people use composite comparators *and* have ColumnDefinition on them, but we probably shouldn't assume nobody does that.

So I think to fix that we need to add a new information to ColumnDefinition, which if the comparator of the CF is a composite type, to which component the definition apply (or if it apply to the column name as a whole). Attached a patch that does just that. I'll note that while we could probably do with a boolean in that case (since currently we only have 2 cases: either the definition apply to the full column name or it applies to the last component), but the full generality of specifying which component exactly the definition apply to will likely be useful if we allow defining secondary index of other parts of the composite (CASSANDRA-3680). Note that the patch does fix the CLI following that change.
","29/Mar/12 19:16;jbellis;bq. I'm not sure how many people use composite comparators and have ColumnDefinition on them, but we probably shouldn't assume nobody does that

given that you can't do that with the cli (as shown here) I think that's actually a pretty safe assumption.","29/Mar/12 19:40;slebresne;bq. given that you can't do that with the cli

Yes you can. What the cli doesn't support is the new-in-1.1 way of interpreting ColumnDefinition for CF having a composite comparator as refering to the last component. It is perfectly possible in pre-1.1 to have a composite comparator and declare a ColumnDefinition. What I'm saying is that currently (i.e without the attached patch), 1.1 breaks compatibility with pre-1.1 for that use case, which is a bad thing.",29/Mar/12 20:05;xedin;I confirm that CLI works as expected with the patch but I'm not sure that adding new option to the ColumnDef especially because users can simply ignore it without even given it a thought. Feels like we are adding complexity from pure air...,"29/Mar/12 21:21;jbellis;I think we'll regret the precedent in trying to preserve all the arbitrarily complex things you could do with the old CFMetadata.  It really doesn't make sense to have a CompositeType as a column name.  Given that, and given that CFMetadata names under <= 1.0 were only useful for validation (and indexes, but we don't index CTs yet either), I'd rather say ""we don't support that in 1.1+, and if you were doing that, drop the column definition before you upgrade.""

Optionally, we could auto-drop it for them and log a warning.","29/Mar/12 23:19;xedin;I like that more than adding ""component_index"" option which would be easily misconfigured and create a confusion. Right now we should change comparators of the schema_* CFs for that to work and add a check to the ThriftValidation and CQL validations to prevent users from setting CompositeType comparators to the new CFs along side with check when schema is converted to 1.1...","30/Mar/12 07:51;slebresne;bq. but we don't index CTs yet either

That's not true. We do support indexes on full CT currently (by the mere fact that CompositeType is a fully functional AbstractType). What we don't support (yet) is indexing on a specific component of a CT.

Now, we agreed on CASSANDRA-3782 that it could sometimes be usefull to index a specific column in a wide row as a way of 'tagging' such wide rows (and there is probably other useful use cases). Why would that suddently stop making sense because the name is a composite one?

So this is supported and this does have some reasonable use cases. As such, I *strongly* think that it would be unreasonable to break it (and in that case, auto-dropping the index for people (or even forcing them to do it) would be the worst possible user experience ever). And I even think we should seriously give some though about maintaining that possibility (the ability to index on a full column name) even in CQL3 (which will likely be a simple and logical extension of CASSANDRA-3792).

bq. trying to preserve all the arbitrarily complex things you could do with the old CFMetadata

I disagree that this is arbitrary complex. The patch adds the ability to specify for a ColumnDefinition to which component it refers to. If we want to support secondary indexes on specific component of a composite column (which I think is the goal of CASSANDRA-3680), we'll have to add that exact information to ColumnDefinition anyway. So I think it's really adding new and useful flexibility.
","30/Mar/12 14:17;jbellis;bq. we agreed on CASSANDRA-3782 that it could sometimes be usefull to index a specific column in a wide row as a way of 'tagging' such wide rows 

But you don't need to support indexing the CT column itself, to support that use case.  More consistent to have a ""tag"" sparse column and index that instead.

bq. really adding new and useful flexibility

New, yes; useful? dubious.

We've barely educated people on CQL3 wide rows as it is, now is not a good time to complicate things further.  Let's take an incremental approach and add such things in response to user demand, should it materialize, instead of adding features ""because we can.""","30/Mar/12 14:40;slebresne;bq. But you don't need to support indexing the CT column itself, to support that use case. More consistent to have a ""tag"" sparse column and index that instead.

But my concern is for backward compatibility. Currently (with 0.8-1.0), you *cannot* support this with a tag sparse column that you index because we *don't* support indexing on a component of a CT (even 1.1 doesn't support it yet). I'm not talking of adding some new possibility that may or may not be useful. I'm saying that if someone is indexing on the CT column itself, which again is currently the *only* solution and btw wcan be done with pure thrift that we have swear not to break, then we'll completely break these users application (and as if that wasn't bad enough, they won't even have a correct replacement at the moment).","30/Mar/12 15:50;jbellis;The thing about backwards-compatibility hacks is you tend to get stuck with them.  I'm okay with any of these alternatives:

# Add a big huge warning to NEWS that this corner case is not supported to 1.1, but an alternative solution for ""tagging"" will be supported eventually.  Fail 1.1 startup if such a schema startup is detected and explain the new limitation
# Wait until we actually have indexes on composite PKs supported to release 1.1
# Ask users@ if anyone actually has such a use case in production, and go with 1 or 2 based on the responses

Here is why I'm willing to go scorched earth on this:

- Schema design is *the* most difficult thing to explain to new Cassandra users. We've made *huge* strides towards simplifying this in CQL3 and composite PKs. I don't think we can afford to dilute this with footnotes about how you can escape hatch back to the old world, dragging in all our old legacy baggage again. Put another way: it's time to bury ""wtf is a supercolumn"" once and for all.
- Of our hundreds of deployments of 0.8 and 1.0, I can live with one or two needing to wait for 1.1.2 or whatever to upgrade.  We have a pretty good feel at this point for commonly used features and this isn't one of those.

TLDR: backwards compatibility in this specific case is high cost, low benefit.","30/Mar/12 16:59;slebresne;bq. The thing about backwards-compatibility hacks is you tend to get stuck with them.

I suppose it depends of ones own definition of a hack, but I really don't consider this patch as a hack. As soon as we consider CT to be first class citizen, adding the ability to specify to which component a ColumnDefinition applies (which is all the patch does) feels like a rather natural extension to me. And as said, if we decide to allow secondary indexes on any component of a CT, which I though was something we wanted ultimately, we'll have to add that componentIndex information. So breaking compatibility to avoid adding a simple info to ColumnDefinition that we'll very likely end up adding anyway later feels like the wrong choice to me.

bq. We've made huge strides towards simplifying this in CQL3 and composite PKs. I don't think we can afford to dilute this with footnotes about how you can escape hatch back to the old world

I don't understand. As of the attached patch, there is 0 footnotes to add to CQL3. Currently it's pretty much *exclusively* a thrift backward-compatibility patch. If you're referring to me saying that we could later consider allowing index on a full composite in CQL3, then I'm sorry, let's forget about that for now. This patch does not force us at all to do that.

bq. dragging in all our old legacy baggage again. Put another way: it's time to bury ""wtf is a supercolumn"" once and for all.

I don't understand what you're referring to. Again, what this patch does is recognizing that CT are now first class citizen and thus that it makes sense that a ColumnDefinition may refer to any of the component. And while it is true that we don't *yet* use that generalization, it *trivially* allows us to support backward compatibility for thrift, hence the idea of doing that generalization now. I don't see where this patch drag *any* old legacy baggage so I'd love to get more precision on what you're referring to exactly. 

bq. Of our hundreds of deployments of 0.8 and 1.0, I can live with one or two needing to wait for 1.1.2 or whatever to upgrade.

But that is *not* what will happen. Anyone that use an index on a CT currently won't be able to upgrade *ever*. Because their index will just be not be supported anymore. If they upgrade, C* will crash. Seriously, they will be completely screwed. Their only option will be some complex manual data model migration which will be impossible to do without any downtime for the application (since no version will both support their existing index *and* the new possibly-introduced-in-1.2 indexing of the the last component of a CT).

bq. TLDR: backwards compatibility in this specific case is high cost, low benefit.

I couldn't disagree more. I still don't see what this high cost is. I wrote the patch and I just don't see at all the high cost. The patch is fairly trivial (it's not small in size but for a good part because it include thrift generated code) and has barely any user side visibility. More precisely, the only visibility it has is on the thrift side, and even then it's an optional field whose default is the right one for the thrift side. On the other side, the benefit is huge imo because it means we don't screw up the people that use C* in original ways.","30/Mar/12 17:35;dbrosius@apache.org;>> Ask users@ if anyone actually has such a use case in production, and go with 1 or 2 based on the responses

This makes sense to me... it's possible that this isn't really an issue.","30/Mar/12 23:07;xedin;Would users be required to manually set ""component_index"" at every metadata column they have after update to 1.1 or to rephrase that - when users are going to be *required* to set ""composite_index"" field?","31/Mar/12 12:07;slebresne;bq. Meaning there will be C* versions which support both?

Only if 1) they use thrift and 2) they ever want to define a ColumnDef that apply to one of the component of a CT. Currently, 2) doesn't have any interest on the thrift side, except maybe as a preparation if you're going to switch to cql3.

In other word, you will never be *required* to set the field. ","31/Mar/12 13:23;xedin;If users are never required to set it for everything to work, what is the benefit of adding new field at the first place? ","31/Mar/12 13:47;slebresne;Because the system CFs are using CQL3 style metadata so that we can display them nicely when queried with CQL3. So typically the CLI (or anyone else) needs that info to be able to correctly decode and display the system table definitions.

Besides, when we add the ability to index the components of a CT, the fact that component_index is exposed through thrift means that the thrift user will be able to use that new functionality, which while probably not a priori is still nice.

Lastly, the field is useful internally in ColumnDefinition so not adding it to ColumnDef (to say avoid confusing thrift user) would imo be a bit dangerous (it would make using both thrift and CQL3 at the same time difficult, which may be useful for people transitioning to CQL3). Overall I really don't think adding the new field is such a big deal. It is true that is is currently mildly useful for pure thrift user, but it's not like it's something new. All the aliases (key_alias, column_aliases, value_alias) are only useful to CQL3 but yet have been added on the thrift side. This is not really different.",31/Mar/12 14:17;xedin;Sounds like it wasn't a good time to make schema_* CFs to use CQL3 style metadata which breaks all other parties and causes half-hacky field (dispute the fact that it is not even useful yet) to be added to the thrift structure just to support correct data display even if that is ambiguous for users how/when to correctly use it...,"31/Mar/12 14:30;jbellis;bq. Sounds like it wasn't a good time to make schema_* CFs to use CQL3 style metadata 

I'm still convinced that part is worth it to be able to query schema information without thrift describe_ methods.","31/Mar/12 14:32;xedin;bq. I'm still convinced that part is worth it to be able to query schema information without thrift describe_ methods.

Ok but it's only useful with CQL3 because all others wouldn't be able to query that correctly like CLI because of the way column metadata is stored right now.","31/Mar/12 14:33;jbellis;bq. the field is useful internally in ColumnDefinition so not adding it to ColumnDef (to say avoid confusing thrift user) would imo be a bit dangerous 

Here's what I don't get.  We're adding this to support backwards compatibility, and yet we need to change the interface to do it?  Clearly a 0.8 or 1.0 client isn't going to be aware of this new field.

bq. All the aliases (key_alias, column_aliases, value_alias) are only useful to CQL3 but yet have been added on the thrift side

Because we added those before we had cqlsh, so the cli was the only way to configure them.  In retrospect, not a great idea.","31/Mar/12 14:35;jbellis;bq. Ok but it's only useful with CQL3 because all others wouldn't be able to query that correctly like CLI because of the way column metadata is stored right now.

Granted, which is why we need to make the cli aware of column aliases.","31/Mar/12 14:43;xedin;bq. Granted, which is why we need to make the cli aware of column aliases.

The problem with that it's not only about CLI it is also about all other possible clients too because users expect comparator to be able to {de-}serialize column names correctly. So we should make it very clear how to work with this type of situation without making any special cases (e.g. for CT).

bq. Because we added those before we had cqlsh, so the cli was the only way to configure them. In retrospect, not a great idea.

I also don't think that having aliases in Thrift really justifies this situation.","31/Mar/12 14:52;jbellis;bq. If you're referring to me saying that we could later consider allowing index on a full composite in CQL3, then I'm sorry, let's forget about that for now. This patch does not force us at all to do that.

My problem is that by supporting this misfeature from thrift in the name of backwards compatibility, we're only pushing the problem into the future: the next step is for someone to say, ""you support this in Thrift, so I should be able to do it in CQL too.""  I'd rather just draw a line and say, ""allowing this was a bad idea and we don't support it anymore.""

bq. while it is true that we don't yet use that generalization

My point is that I'm pretty sure this is a generalization I don't want to support at all.  This is a case where exposing every detail of your storage engine is a bad idea.

bq. Anyone that use an index on a CT currently won't be able to upgrade ever

The upgrade path requires some effort but is conceptually simple:

# update your application to no longer use the CT column index
# upgrade

(If you respond that updating your application is not acceptable, then why are you bothering to upgrade?  Stay on the stable version that you built against in the first place, there is nothing wrong with that.)

My claim is that inflicting this on a small handful of users (possibly as small as zero, and I would bet money not more than one) is worth the upside of getting to a clean data model in 1.2 without the distractions of legacy features like this.  The danger is that the longer we preserve features like this, the more potential there is for new users to start using them which makes it more difficult for us to drop them later.  So I'd rather make a clean break now, than drag it out.

That said, if you are fundamentally opposed to not dropping any feature no matter how obscure without some warning, which I admit is at least a consistent position, I would be okay with supporting this in 1.1 with a clear intention to drop it in 1.2.  (Which would imply to me that we leave the thrift interface alone.)  This would give us a full release to make sure we have alternatives to whatever use cases people may have for the CT index (e.g. indexing the sparse columns for the tag scenario).","31/Mar/12 15:06;jbellis;bq. The problem with that it's not only about CLI it is also about all other possible clients too because users expect comparator to be able to {de-}serialize column names correctly

But this is only a problem if you're trying to show the schema for the System keyspace.  (Presumably if you're using column aliases in your own CFs, you're using a sufficiently up to date client already.)

I'm willing to accept that ""to introspect the system keyspace, you need to be using a 1.1-aware cli.""",31/Mar/12 15:13;xedin;Would that assumption still hold when users are going to start creating CFs using CQL3?,"31/Mar/12 15:42;jbellis;""Presumably if you're using column aliases in your own CFs, you're using a sufficiently up to date client already.""

In other words, your upgrade path is

# upgrade C* to 1.1
# upgrade cli to 1.1
# start creating CQL3 CFs

In particular, 3 doesn't work before 1, at which point 2 is trivial.",31/Mar/12 15:53;xedin;I'm fine fixing it just for CLI then.,"31/Mar/12 17:41;slebresne;Wow, that seems to get so out of proportion and I so don't understand why.

What happpened is this: currently (i.e. in all released version), a ColumnDefinition name refers to the CF column name, i.e. one must use the column comparator to decode it. In all currently released version, CompositeType is a normal comparator and that rule applies to it, and for anyone that will use thrift in 1.1+, for all intent and purposes CompositeType will still be a normal comparator like all the other ones and so the natural thing will still be that the ColumDefinition name applies to CT comparator entirely.

Now when I wrote the CQL3 patch (the initial one), I realised that we needed a new feature, the need to be able to have ColumnDefinitions whose name refers to only one of the component of the CT comparator. And while  we currently only need to refer to the last component (because we don't yet support secondary indexes on CT), we *will* need to have ColumnDefinition whose name refer to *any* of the CT components as soon as we support secondary indexes on CT (CASSANDRA-3680). In other words, the introduction of composite_index (this patch) is just the first part CASSANDRA-3680. We *will* have to add it soon enough. It is *hardly* something added *just* for backward compatibility.

Anyway while I was writting the CQL3 patch, instead of properly handling that new case, I introduced a *bug* consisting of changing the behavior of ColumnDefinition for CT so that by default they refer to the last component of the CT, not the comparator itself anymore. I.e. I changed the default behavior to an incompatible one.  I'm sorry I did that, I shouldn't have and the goal of that patch is to restore the default behavior and introduce a new info to ColumnDefinition to allow switching to the new behavior. Again, I think it is incorrect to say that this patch is just 'in the name of backward compatibility'. If I had mistakefully changed the subcolumn comparator of super column to apply to column names instead of super columns names, we wouldn't call fixing that 'supporting a misfeature in the name of backward compatibilty'. But yes, this bug happens to break backward compatibility and I do have a big problem with that part.

On the thrift side. Yes this does add a new field to ColumnDef. But:
# It's untrue that column_aliases and value_alias were 'added before we had cqlsh'. They would be added in the exact same version as this patch, so I think that using that argument against component_index is unfair.
# I'm not sure this will be so very confusing to users. Again, for thrift users, CT is a comparator like any other. The confusing thing would be for a ColumnDef to apply to the last component rather than the full CT. Why would CT be suddenly special (again, on the thrift side) and why the last component? It would random and thus confusing. On the contrary, if you have component_index, then you can say that the default is the one you'd expect, i.e the one that apply to all other comparator, but that we've added the new ability to make ColumnDef apply to other component.
# As soon as CASSANDRA-3680 is done, it will be usefull to allow creating secondary indexes on CT component on the thrift side. Why wouldn't we allow CASSANDRA-3680 on the thrift if it only cost us the addition of a simple int field in ColumnDef?

{quote}
The upgrade path requires some effort but is conceptually simple:

# update your application to no longer use the CT column index
# upgrade
{quote}

My problem is that step 1 will be *super* painfull. Because before upgrading, you don't have any reasonable alternative for whatever you were doing with the CT column index (tagging rows for instance). So you have to write manual code to do the same manual indexing. And then you have to write a map reduce job to reconstruct this new manual index for existing rows. In real life situations where you must do that without downtime, it's a pain in the ass.

And yes I know, you don't believe anyone uses that. But I have the weakness to think that we do not know what everyone user is doing. And yes, I have a *big* problem with screwing up even 1 user (especially after we've said we were serious about not breaking the thrift API). And yes, even if that force us to write a little bit more code now and/or later.

bq. is worth the upside of getting to a clean data model in 1.2 without the distractions of legacy features like this. The danger is that the longer we preserve features like this, the more potential there is for new users to start using them which makes it more difficult for us to drop them later

Ok, I understand the argument in principle but in practice what is it we're talking about?  There is 0 line of specific code to support index on a full CT column name, this is all handled by the current secondary index code transparently.  So worst case scenario, more people use the feature (which btw would suggest it's vaguely useful). But then adding support in CQL3 would be fairly trivial. The only thing we'd have to add is way to declare those index (again the rest is already handled by generic code), and that would likely be a nature extension of whatever we come up for CASSANDRA-3782. Typically, to reuse one of the proposed syntax on that issue, on top of supporting:
{noformat}
CREATE INDEX index_name ON timeline(0);
{noformat}
we would have to also add support for
{noformat}
CREATE INDEX index_name ON timeline(0, 'foobar', 4);
{noformat}
Again, this is hardly making the data mode in 1.2 dirty. Actually I'm not even sure I understand in what way that would hinder the compression of the data model one bit. And again, we don't *have* to support it if we don't want to. But at least, instead of forcing a painful upgrade on those that were using index on full CT from thrift when *we* decide, they would have the option to stay with thrift until they are ready to make the migration ().

Overall, I completely disagree that this patch will force anything on us in the future and I just don't see where it make the data model for 1.2 less clean.  I also strongly think we should be more considerate about breaking use cases that people may be using, even if we're talking very few people.
","05/Apr/12 19:50;jbellis;bq. I'm not sure this will be so very confusing to users.

What I'm worried about is that it forces us to distinguish between ""logical"" and ""physical"" columns again.  I *love* that with CQL3 all I have to talk about is CQL columns and not have to dig out my diagrams of mapping CT to logical columns until someone starts to actually dig into the engine code.

bq. the introduction of composite_index (this patch) is just the first part CASSANDRA-3680. We will have to add it soon enough. It is hardly something added just for backward compatibility.

I don't follow at all.  3680 just means we want to be able to create an index on a logical column that is part of a CT under the hood: i.e., exactly the same thing we can already represent with the current-as-of-today CFMetadata.

I have virtually zero interest in supporting partial indexes as discussed in 3782; RDBMSes have shown pretty conclusively that this is a very niche feature.  Very much in the category of ""let's take our time and add it if it makes sense, not just because we know how to do it.""

bq. It's untrue that column_aliases and value_alias were 'added before we had cqlsh'. 

You left out key_alias, which is what I was referring to having added in 0.8 well before we had cqlsh.  I can only guess that we added column_aliases and value_aliases to thrift as well for the sake of consistency with that precedent.  As you point out, though, it's not too late to rip those out and we probably should.

bq. As soon as CASSANDRA-3680 is done, it will be usefull to allow creating secondary indexes on CT component on the thrift side. Why wouldn't we allow CASSANDRA-3680 on the thrift if it only cost us the addition of a simple int field in ColumnDef?

Sounds like you're getting a little ahead of yourself.  Why not add it as part of 3680, should that be something we want to do?  (But as I described above, I don't think it is.)","05/Apr/12 20:00;jbellis;bq. What I'm worried about is that it forces us to distinguish between ""logical"" and ""physical"" columns again. 

That said, you're probably right that for CQL users this is an implementation detail that they won't care about.  And Thrift users can carry on with CT definitions to the degree they have or haven't previously, I suppose.

But, that doesn't mean we should gratuitously blur the lines between the two.

So here is what I propose:

- Change ColumnDefinition back to its old behavior.  I guess we'll need to add the int to allow us to support CQL3 internally, but we don't need to expose it to thrift yet, if at all.  (Alternatively we could add a cql_column_metadata that supports the new semantics.)
- Remove column_aliases and value_aliases from Thrift.  They serve no purpose there than to give users a gun with which to shoot themselves in the foot.
","06/Apr/12 14:31;slebresne;bq. I don't follow at all. 3680 just means we want to be able to create an index on a logical column that is part of a CT under the hood

Hum, I though that parts of CASSANDRA-3680 was that say you have
{noformat}
CREATE TABLE t (
  rk text,
  k1 text,
  k2 text,
  c1 text,
  c2 text,
  PRIMARY KEY (rk, k1, k2)
);
{noformat}
then we could allow defining something like:
{noformat}
CREATE INDEX ON t(k1);
{noformat}
(or the same with k2).

Now truth is I haven't given a tons of though to this and if that is what you call partial indexes (it's not exactly the same than CASSANDRA-3782), then maybe it does have little convincing use cases. In which case, fair enough.

bq. for CQL users this is an implementation detail that they won't care about. And Thrift users can carry on with CT definitions to the degree they have or haven't previously

That's exactly what I meant :)

bq. Alternatively we could add a cql_column_metadata that supports the new semantics

Not necessarily against the idea but it feels like the integer makes for less special casing internally.

bq. Remove column_aliases and value_aliases from Thrift. They serve no purpose there than to give users a gun with which to shoot themselves in the foot

I'm good with that. There may just be 2 things to be careful with when doing that:
* we probably cannot assume just yet that no user will do a mix of access through thrift and through CQL. We must make sure one doesn't destroy the CQL metadata while doing a simple column family update from thrift. Shouldn't be too hard to do though.
* exposing everything through thrift have the slight advantage that a use may say set a column alias to allow transitioning to CQL3. There is likely better way to probide that though.

Anyway, I'll update the patch so that it doesn't expose those and component_index through thrift, and to not return ColumnDefinition when component_index is not null through thrift (so that the cli don't get confused).
","06/Apr/12 14:43;jbellis;bq. we could allow defining something like {{create index on t(k1)}}

Ah, right.  Yes, I suppose there's no reason not to expose that to Thrift as well.","06/Apr/12 16:15;slebresne;Attaching 4093_v2. Internally, this is roughtly the same as the previous patch (using componentIndex). However, this patch doesn't not expose it through thrift and remove column_aliases and value_alias (from thrift) too.

Note that it completely skip ColumnDefinition whose componentIndex is not null (i.e those created by CQL3) when translating to thrift, which fixes the CLI problem (basically the CLI don't get back the parts he don't know how to interpret).

An additional change is that CFMetaData.apply() make sure that a thrift update won't wrongfully remove CQL3 only metadata (ie. columnAliases, valueAliases and ColumnDefinition with a non-null componentIndex). In other words, it's safe to create a column family through CQL3 and later update it with the cli (to avoid foot shooting for those that would be in the middle of transitioning to CQL3 for instance).

The only small detail is that we *need* to backport CASSANDRA-4037 to 1.1.0 for this to work (since currently CQL do a toThrift(fromThrift()) dance for schema update (in order to use ThriftValidation)).

bq. Yes, I suppose there's no reason not to expose that to Thrift as well.

I'm actually happy with the idea of v2 of not exposing to thrift what's not usefull right now. I guess we can go with v2 and expose componentIndex later when it makes sense on the thrift.
","06/Apr/12 19:10;jbellis;bq. I guess we can go with v2 and expose componentIndex later when it makes sense on the thrift.

That's what I had in mind.

We should keep dclocal_r_r as parameter 37 in the thrift idl to make it compatible w/ beta clients.
Otherwsise, +1 on v2 and backport of 4037.",09/Apr/12 16:58;slebresne;Backported CASSANDRA-4037 and committed v2 (with dc_local_rr back at 37). Thanks.,,,,,,,,,,,,,,,,,,
"Issue with cassandra-cli ""assume"" command and custom types",CASSANDRA-4081,12547942,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,drew_kutchar,drew_kutchar,24/Mar/12 07:05,12/Mar/19 14:16,13/Mar/19 22:27,30/Mar/12 08:36,1.0.9,,,,,1,,,,,,,"There seems to be an issue with cassandra-cli's assume command with a custom type. I get ""Syntax error at position 35: missing EOF at '.'""

To make sure the issue is not with my custom type, I tried it with the built-in BytesType and got the same error:

[default@test] assume UserDetails validator as org.apache.cassandra.db.marshal.BytesType;
Syntax error at position 35: missing EOF at '.'

I also tried it with single and double quotes with no success:
[default@test] assume UserDetails validator as 'org.apache.cassandra.db.marshal.BytesType';
Syntax error at position 32: mismatched input ''org.apache.cassandra.db.marshal.BytesType'' expecting Identifier


Based on the output of ""help assume"" I should be able to just pass a fqn of a class.

> It is also valid to specify the fully-qualified class name to a class that
> extends org.apache.Cassandra.db.marshal.AbstractType.
",Cassandra 1.0.7 on Mac OSX Lion,,,,,,,,,,,,,,,,,,29/Mar/12 13:22;xedin;CASSANDRA-4081.patch;https://issues.apache.org/jira/secure/attachment/12520419/CASSANDRA-4081.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-24 19:04:41.665,,,no_permission,,,,,,,,,,,,233039,,,Fri Mar 30 08:36:18 UTC 2012,,,,,,0|i0grqn:,95924,slebresne,slebresne,,,,,,,,,,"24/Mar/12 19:04;dbrosius@apache.org;It appears you need to supply one of these constants from the CliClient.Function

BYTES, INTEGER, LONG, INT, LEXICALUUID, TIMEUUID, UTF8, ASCII, COUNTERCOLUMN

and not class names. Given that it would appear to me that the doc is wrong about being able to specify custom class types.","24/Mar/12 20:40;drew_kutchar;Seems like it, but I'd really prefer if I can pass in a fqn of a class. We are storing Jackson Smile encoded blobs as bytes in Cassandra but we would like to be able to get a human friendly output from cassandra-cli.","30/Mar/12 08:36;slebresne;+1, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScrubTest failing on current 1.1.0 branch,CASSANDRA-4077,12547767,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,23/Mar/12 09:03,12/Mar/19 14:16,13/Mar/19 22:27,30/Mar/12 15:32,1.1.0,,Legacy/Testing,,,0,,,,,,,"I get the following error:
{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 09:53:05,979 Corrupt sstable build/test/cassandra/data/Keyspace1/Super5/Keyspace1-Super5-f-2; skipped
    [junit] java.io.EOFException
    [junit] 	at java.io.DataInputStream.readFully(DataInputStream.java:180)
    [junit] 	at java.io.DataInputStream.readLong(DataInputStream.java:399)
    [junit] 	at org.apache.cassandra.db.commitlog.ReplayPosition$ReplayPositionSerializer.deserialize(ReplayPosition.java:133)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableMetadata$SSTableMetadataSerializer.deserialize(SSTableMetadata.java:206)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableMetadata$SSTableMetadataSerializer.deserialize(SSTableMetadata.java:194)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:155)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:483)
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScrubFile(ScrubTest.java:86)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)
    [junit] 	at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testScrubFile(org.apache.cassandra.db.ScrubTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScrubFile(ScrubTest.java:87)
{noformat}",,,,,,,,,,,,,,,,,,,30/Mar/12 15:16;slebresne;4077.txt;https://issues.apache.org/jira/secure/attachment/12520611/4077.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-30 15:28:40.66,,,no_permission,,,,,,,,,,,,232864,,,Fri Mar 30 15:32:58 UTC 2012,,,,,,0|i0grp3:,95917,jbellis,jbellis,,,,,,,,,,30/Mar/12 15:16;slebresne;This is because loadNewSSTable was incorrectly bumping the version of the sstables to the current version when renaming them. Trivial patch attached.,30/Mar/12 15:28;jbellis;+1,"30/Mar/12 15:32;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
check most recent TS values in SSTables when a row tombstone has already been encountered,CASSANDRA-4116,12549563,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,mdennis,mdennis,04/Apr/12 20:27,12/Mar/19 14:16,13/Mar/19 22:27,02/May/12 15:02,1.1.1,,,,,0,,,,,,,"once C* comes across a row tombstone, C* should check the TS on the tombstone against all SSTables.  If the most recent TS in an SST is older than the row tombstone, that entire SST (or the remainder of it) can be safely ignored.

There are two drivers for this.

* avoid checking column values that could not possibly be in the result set

* avoid OOMing because all the tombstones are temporarily kept in memory.",,,,,,,,,,,,,,,,,,,01/May/12 16:03;beobal;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4116-After-Row-tombstone-skip-earlier-SSTabl.txt;https://issues.apache.org/jira/secure/attachment/12525178/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4116-After-Row-tombstone-skip-earlier-SSTabl.txt,01/May/12 16:03;beobal;ASF.LICENSE.NOT.GRANTED--v1-0002-CASSANDRA-4116-Set-SSTable-maxTimestamp-correctly-for-.txt;https://issues.apache.org/jira/secure/attachment/12525179/ASF.LICENSE.NOT.GRANTED--v1-0002-CASSANDRA-4116-Set-SSTable-maxTimestamp-correctly-for-.txt,02/May/12 09:32;beobal;CASSANDRA-4116-Set-SSTable-maxTimestamp-correctly-1.0.txt;https://issues.apache.org/jira/secure/attachment/12525270/CASSANDRA-4116-Set-SSTable-maxTimestamp-correctly-1.0.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-04-28 19:37:14.267,,,no_permission,,,,,,,,,,,,234554,,,Wed May 02 15:02:10 UTC 2012,,,,,,0|i0gs6f:,95995,jbellis,jbellis,,,,,,,,,,"28/Apr/12 19:37;beobal;While I was working through this & testing with tiny amounts of data, I found that if the last (or only) update to a CF before a flush is a Row level delete then the maxTimeStamp on the resulting SSTable is incorrect. The second patch includes a fix + test for that - I included it as a separate patch as I was a bit less confident about screwing around with the SSTable metadata although all existing tests are still passing.",29/Apr/12 12:12;beobal;Updated patches because I realised the license header was missing from the new test source,"01/May/12 04:03;jbellis;{code}
.               if (sstable.getMaxTimestamp() < mostRecentRowTombstone)
                    continue;
{code}

For {{collectTimeOrderedData}}, I think that can be a ""break"" since we're scanning in reverse chronological order.

For {{collectAllData}}, I think it might be worth making (potentially) two passes: if we find a deletion marker on the first pass, we should do a second pass, so we can strip out sstables that we saw the first time before we knew about the marker.  This would improve the deletion case, without imposing the overhead of a sort or two passes on workloads with no deletes.","01/May/12 16:11;beobal;Updated patches as per comments. In {{collectAllData}}, collect the eligible iterators in a multimap with key = maxTimestamp (I guess that its pretty unlikely for two sstables to have the exact same timestamp, but using a Multimap doesn't really add any cost). Then, if we did find a row tombstone, only keep the iterators from younger sstables. Otherwise, keep all that we collected.","02/May/12 04:30;jbellis;Committed patch 0001 to 1.1.1, with some cleanup.  (Main one was to use a Map<iterator, timestamp> instead of a multimap the other direction.)

I suspect 0002 is needed for earlier releases as well, can you post a version against the 1.0 branch?",02/May/12 09:32;beobal;Added version of patch 0002 with fix to sstable maxTimeStamp against 1.0 branch,"02/May/12 15:02;jbellis;thanks, committed.  (moved the fix into CF.maxTimestamp so the other code paths using that will benefit as well.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool cleanup giving exception,CASSANDRA-4112,12549262,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,shoaibmir,shoaibmir,03/Apr/12 00:18,12/Mar/19 14:16,13/Mar/19 22:27,03/Apr/12 14:35,1.0.9,,,,,0,compaction,,,,,,"We just recently started using version 1.0.9, previously we were using tiered compaction because of a bug in 1.0.8 (not letting us use leveled compaction) and now since moving to 1.0.9 we have started using leveled compaction.

Trying to do a cleanup we are getting the following exception:

root@test:~# nodetool -h localhost cleanup 
Error occured during cleanup
java.util.concurrent.ExecutionException: java.util.NoSuchElementException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:204)
        at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:240)
        at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:988)
        at org.apache.cassandra.service.StorageService.forceTableCleanup(StorageService.java:1639)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.util.NoSuchElementException
        at java.util.ArrayList$Itr.next(ArrayList.java:757)
        at org.apache.cassandra.db.compaction.LeveledManifest.replace(LeveledManifest.java:196)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:147)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:495)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:1010)
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:802)
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:64)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:244)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:183)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)

cheers,
Shoaib","Ubuntu LTS 10.04, OpenJDK 1.6.0_20",,,,,,,,,,,,,,,,,,03/Apr/12 02:23;jbellis;4112.txt;https://issues.apache.org/jira/secure/attachment/12521096/4112.txt,03/Apr/12 07:46;slebresne;4112_v2.txt;https://issues.apache.org/jira/secure/attachment/12521114/4112_v2.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-03 00:55:45.213,,,no_permission,,,,,,,,,,,,234253,,,Tue Apr 03 14:35:38 UTC 2012,,,,,,0|i0gs4n:,95987,slebresne,slebresne,,,,,,,,,,03/Apr/12 00:55;jbellis;Are you running with assertions disabled?,"03/Apr/12 01:04;shoaibmir;Yes, we have assertions disabled.","03/Apr/12 02:23;jbellis;LCS incorrectly assumed that cleanup would always result in the same number of sstables as before, which is not the case (if no keys in an sstable belong post-cleanup, it will be left out entirely).

fix attached.","03/Apr/12 07:46;slebresne;Agreed on the diagnostic, but the patch changes the behavior a bit in that it adds the new sstable to level 0 (which is probably ok in the sense of not introducing a bug, but is less efficient for no good reason that I can see). Attaching v2 that correct that.",03/Apr/12 14:23;jbellis;+1 on v2,"03/Apr/12 14:35;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh does not work with piping,CASSANDRA-4113,12549357,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thepaul,pavel1,pavel1,03/Apr/12 16:18,12/Mar/19 14:16,13/Mar/19 22:27,15/May/12 22:25,1.0.11,,,,,0,cqlsh,,,,,,"When I run cqlsh under ant like this
	<target name=""create-test-db"">
		<exec executable=""../dsc-cassandra-1.0.8/bin/cqlsh"" input=""./resource/test/test.cql"">  
		</exec>
	</target>
I got: 
     [exec] Traceback (most recent call last):
     [exec]   File ""../dsc-cassandra-1.0.8/bin/cqlsh"", line 1891, in <module>
     [exec]     main(*read_options(sys.argv[1:], os.environ))
     [exec]   File ""../dsc-cassandra-1.0.8/bin/cqlsh"", line 1863, in main
     [exec]     completekey=options.completekey)
     [exec]   File ""../dsc-cassandra-1.0.8/bin/cqlsh"", line 384, in __init__
     [exec]     self.output_codec = codecs.lookup(encoding)
     [exec] TypeError: must be string, not None
     [exec] Result: 1

I suggest replacement of:
        if encoding is None:
            encoding = sys.stdout.encoding
to: 
       if encoding is None:
           encoding=locale.getpreferredencoding()
",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-04-23 21:31:57.399,,,no_permission,,,,,,,,,,,,234348,,,Tue May 15 22:25:29 UTC 2012,,,,,,0|i0gs53:,95989,brandon.williams,brandon.williams,,,,,,,,,,"23/Apr/12 21:31;thepaul;Fixed in my 4113 branch (current tag pending/4113) in github:

https://github.com/thepaul/cassandra/tree/pending/4113 . Defaults encoding to ascii when python doesn't provide any useful sys.stdout.encoding (as happens when stdout is not a tty).

1.1 branch already had this fix as part of CASSANDRA-3479.","23/Apr/12 21:36;thepaul;Oops, I didn't even read the last part of the report. Didn't know about locale.getpreferredencoding()- tested it out, and you're right, that's a perfect fit. Redoing fix.","23/Apr/12 21:47;thepaul;New fix in same branch (4113), now tagged pending/4113-2:

https://github.com/thepaul/cassandra/tree/pending/4113-2

And adjusted for the 1.1 branch, since it doesn't cherry-pick or merge cleanly:

https://github.com/thepaul/cassandra/tree/pending/4113-for-1.1",15/May/12 22:25;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serializing cache can cause Segfault in 1.1,CASSANDRA-4111,12549251,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,02/Apr/12 23:29,12/Mar/19 14:16,13/Mar/19 22:27,03/Apr/12 19:35,1.1.0,,,,,0,,,,,,,"Rare but this can happen per sure, looks like this issue is after CASSANDRA-3862 hence affectes only 1.1

        FreeableMemory old = map.get(key);
        if (old == null)
            return false;

        // see if the old value matches the one we want to replace
        FreeableMemory mem = serialize(value);
        if (mem == null)
            return false; // out of memory.  never mind.
        V oldValue = deserialize(old);
        boolean success = oldValue.equals(oldToReplace) && map.replace(key, old, mem);

        if (success)
            old.unreference();
        else
            mem.unreference();
        return success;

in the above code block we deserialize(old) without taking reference to the old memory, this can case seg faults when the old is reclaimed (free is called)
Fix is to get the reference just for deserialization

        V oldValue;
        // reference old guy before de-serializing
        old.reference();
        try
        {
             oldValue = deserialize(old);
        }
        finally
        {
            old.unreference();
        }",,,,,,,,,,,,,,,,,,,03/Apr/12 17:06;vijay2win@yahoo.com;0001-CASSANDRA-4111-v2.patch;https://issues.apache.org/jira/secure/attachment/12521171/0001-CASSANDRA-4111-v2.patch,02/Apr/12 23:42;vijay2win@yahoo.com;0001-CASSANDRA-4111.patch;https://issues.apache.org/jira/secure/attachment/12521079/0001-CASSANDRA-4111.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-03 02:09:27.319,,,no_permission,,,,,,,,,,,,234242,,,Tue Apr 03 19:35:35 UTC 2012,,,,,,0|i0gs47:,95985,jbellis,jbellis,,,,,,,,,,02/Apr/12 23:47;vijay2win@yahoo.com;Noticed it when compaction was serializing it and update was freeing it.,"03/Apr/12 02:09;jbellis;good catch.  also need to handle old.reference() returning false tho.
",03/Apr/12 17:06;vijay2win@yahoo.com;ahaaa missed that.... V2 fixes it. Thanks!,03/Apr/12 17:28;jbellis;+1,"03/Apr/12 19:35;vijay2win@yahoo.com;Committed to 1.1.0, 1.1 and trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkRecordWriter constructor crashes on wrong default parameter in conf.get() which is letter instead of number,CASSANDRA-4166,12551454,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,michalm,michalm,18/Apr/12 11:42,12/Mar/19 14:16,13/Mar/19 22:27,18/Apr/12 14:11,1.1.1,,,,,0,,,,,,,"I've pulled the newest code and my bulkloading started to crash on:

{noformat}2012-04-18 12:34:09,620 WARN org.apache.hadoop.mapred.Child: Error running child
java.lang.NumberFormatException: For input string: ""O""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48){noformat}

The problem is this line and default parameter of conf.get() which was accidentally set to ""O"" (letter O) instead of 0 (zero).

{noformat}maxFailures = Integer.valueOf(conf.get(MAX_FAILED_HOSTS, ""O""));{noformat}

",,,,,,,,,,,,,,,,,,,18/Apr/12 11:44;michalm;trunk-4166.txt;https://issues.apache.org/jira/secure/attachment/12523177/trunk-4166.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-18 14:11:11.107,,,no_permission,,,,,,,,,,,,236262,,,Wed Apr 18 14:11:11 UTC 2012,,,,,,0|i0gsrb:,96089,brandon.williams,brandon.williams,,,,,,,,,,18/Apr/12 11:44;michalm;Patch. Kind of obvious ones ;),"18/Apr/12 11:45;michalm;Changing ""O"" to ""0"", proudly called ""a patch"" ;)","18/Apr/12 14:11;brandon.williams;Good catch.  Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cqlsh should support DESCRIBE on cql3-style composite CFs,CASSANDRA-4164,12551347,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thepaul,nickmbailey,nickmbailey,18/Apr/12 03:50,12/Mar/19 14:16,13/Mar/19 22:27,04/May/12 17:47,1.1.1,,,,,0,cql3,,,,,,"There is a discrepancy between create column family commands and then the output of the describe command:

{noformat}
cqlsh:test> CREATE TABLE timeline (
        ...     user_id varchar,
        ...     tweet_id uuid,
        ...     author varchar,
        ...     body varchar,
        ...     PRIMARY KEY (user_id, tweet_id)
        ... );
cqlsh:test> describe columnfamily timeline;

CREATE COLUMNFAMILY timeline (
  user_id text PRIMARY KEY
) WITH
  comment='' AND
  comparator='CompositeType(org.apache.cassandra.db.marshal.UUIDType,org.apache.cassandra.db.marshal.UTF8Type)' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write=True AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='org.apache.cassandra.io.compress.SnappyCompressor';
{noformat}",,,,,,,,,,,,,,,,,,,04/May/12 17:38;thepaul;4164-fix.patch.txt;https://issues.apache.org/jira/secure/attachment/12525652/4164-fix.patch.txt,27/Apr/12 18:25;thepaul;4164.patch-2.txt;https://issues.apache.org/jira/secure/attachment/12524901/4164.patch-2.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-25 20:18:17.941,,,no_permission,,,,,,,,,,,,236211,,,Fri May 04 19:05:55 UTC 2012,,,,,,0|i0gsqf:,96085,brandon.williams,brandon.williams,,,,,,,,,,"25/Apr/12 20:18;thepaul;teach cqlsh how to understand system.schema_columnfamilies so that it can get information on composite primary key tables.

patch attached, or, as usual, in my 4164 branch on my github. tagged here:

https://github.com/thepaul/cassandra/tree/pending/4164

Note that this still won't work for describing composite CFs in the system keyspace; system CFs don't show up in schema_columnfamilies, and there's no other way to get key-component column info.","26/Apr/12 12:54;thepaul;Was just pointed out to me that this doesn't identify tables with COMPACT STORAGE, and it should. I'll add a fix (assuming its's possible to distinguish them over thrift or through system.schema_columnfamilies).",26/Apr/12 13:11;slebresne;The current way the code distinguishes COMPACT STORAGE is by checking if column_metadata is empty (i.e. if system.schema_columns is empty).,"27/Apr/12 18:25;thepaul;Thanks, Sylvain. Branch is updated; new tag is here:

https://github.com/thepaul/cassandra/tree/pending/4164-2

New comprehensive patch attached.","27/Apr/12 21:11;brandon.williams;+1, committed.","04/May/12 17:38;thepaul;The rebased updated fix here introduced a bug; DESCRIBE COLUMNFAMILY does not work with this change (user only sees ""global name 'ksname' is not defined"").",04/May/12 17:38;thepaul;One-line fix for that attached.,"04/May/12 17:47;brandon.williams;Committed, but I'm wondering if it's time that cqlsh had some kind of tests?  Even something ghetto like we do for pig could catch this sort of thing.","04/May/12 19:05;thepaul;Absolutely it's time. That's CASSANDRA-3920, which is in progress. Just need cycles..",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql delete does not delete,CASSANDRA-4193,12552933,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,cywjackson,cywjackson,26/Apr/12 19:11,12/Mar/19 14:16,13/Mar/19 22:27,29/Jun/12 08:07,1.1.2,,,,,0,cql3,,,,,,"tested in 1.1 and trunk branch on a single node:
{panel}
cqlsh:test> create table testcf_old ( username varchar , id int , name varchar , stuff varchar, primary key(username,id,name)) with compact storage;
cqlsh:test> insert into testcf_old ( username , id , name , stuff ) values ('abc', 2, 'rst', 'some other bunch of craps');
cqlsh:test> select * from testcf_old;
 username | id | name | stuff
----------+----+------+---------------------------
      abc |  2 |  rst | some other bunch of craps
      abc |  4 |  xyz |          a bunch of craps

cqlsh:test> delete from testcf_old where username = 'abc' and id =2;
cqlsh:test> select * from testcf_old;
 username | id | name | stuff
----------+----+------+---------------------------
      abc |  2 |  rst | some other bunch of craps
      abc |  4 |  xyz |          a bunch of craps
{panel}

same also when not using compact:
{panel}
cqlsh:test> create table testcf ( username varchar , id int , name varchar , stuff varchar, primary key(username,id));
cqlsh:test> select * from testcf;
 username | id | name                      | stuff
----------+----+---------------------------+------------------
      abc |  2 | some other bunch of craps |              rst
      abc |  4 |                       xyz | a bunch of craps

cqlsh:test> delete from testcf where username = 'abc' and id =2;
cqlsh:test> select * from testcf;
 username | id | name                      | stuff
----------+----+---------------------------+------------------
      abc |  2 | some other bunch of craps |              rst
      abc |  4 |                       xyz | a bunch of craps
{panel}",,,,,,,,,,,,,,,,,,,27/Apr/12 13:15;slebresne;4193.txt;https://issues.apache.org/jira/secure/attachment/12524854/4193.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-27 13:14:51.317,,,no_permission,,,,,,,,,,,,236704,,,Fri Jun 29 08:07:12 UTC 2012,,,,,,0|i0gt2v:,96141,jbellis,jbellis,,,,,,,,,,"27/Apr/12 13:14;slebresne;So for the compact case, this is a dupe of CASSANDRA-3708, it in fact requires a range tombstone (they could be millions of record having the 'abc' and 2 as first components). The reason for the non-compact case is very similar, this amount internally to remove multiple columns. However in that second we could implement a workaround as we know which columns are defined for the table. However, for this too CASSANDRA-3708 will offer a better fix, as it will be more efficient to have 1 (range) tombstone rather than n where n is the number of columns in the table and less special code once CASSANDRA-3708 is in.

So what I propose is for now to throw an error on the compact case and support the second one by deleting each column individually. Once CASSANDRA-3708 is in, we'll use it to replace the second part. Patch attached to do that.","27/Jun/12 16:56;jbellis;With CASSANDRA-3708 committed, is this still relevant?",28/Jun/12 09:59;slebresne;It is relevant for the 1.1 branch (for which the patch is targeted) since CASSANDRA-3708 is 1.2 only.,"28/Jun/12 22:16;jbellis;+1

nit: comment above ""cfDef.isComposite && builder.componentCount() != 0"" needs to be updated",29/Jun/12 08:07;slebresne;Committed (fix comment fixed) to 1.1. Since it's not need for trunk I've merge the commit but with '-s ours'.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apparent data loss using super columns and row cache via ConcurrentLinkedHashCacheProvider,CASSANDRA-4190,12552706,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,minaguib,minaguib,25/Apr/12 19:01,12/Mar/19 14:16,13/Mar/19 22:27,26/Apr/12 14:35,1.0.10,1.1.1,,,,0,cache,ConcurrentLinkedHashCacheProvider,supercolumns,,,,"Tested on a vanilla single-node cassandra 1.0.9 installation.

When using super columns along with row caching via ConcurrentLinkedHashCacheProvider (default if no JNA available, or explicitly configured even if JNA available), there's what appears as transient data loss.

Given this script executed in cassandra-cli:
{quote}
create keyspace Test;
use Test;

create column family Users with column_type='Super' and key_validation_class='UTF8Type' and comparator='UTF8Type' and subcomparator='UTF8Type' and default_validation_class='UTF8Type' and rows_cached=75000 and row_cache_provider='ConcurrentLinkedHashCacheProvider';

set Users['mina']['attrs']['name'] = 'Mina';
get Users['mina'];

set Users['mina']['attrs']['country'] = 'Canada';
get Users['mina'];

set Users['mina']['attrs']['region'] = 'Quebec';
get Users['mina'];
{quote}

The output from the 3 gets above is as follows:

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

It's clear that the second and third set commands (country, region) are missing in the returned results.

If the row cache is explicitly invalidated (in a second terminal, via `nodetool -h localhost invalidaterowcache Test Users`), the missing data springs to life on next 'get':
{quote}
[default@Test] get Users['mina'];
=> (super_column=attrs,
     (column=country, value=Canada, timestamp=1335377839592000)
     (column=name, value=Mina, timestamp=1335377788441000)
     (column=region, value=Quebec, timestamp=1335377871353000))
Returned 1 results.
{quote}

From cursory checks, this does not to appear to happen with regular columns, nor with JNA enabled + SerializingCacheProvider.

",Linux 2.6.27,,,,,,,,,,,,,,,,,,26/Apr/12 13:05;slebresne;4190.txt;https://issues.apache.org/jira/secure/attachment/12524435/4190.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-26 13:05:58.653,,,no_permission,,,,,,,,,,,,236784,,,Thu Apr 26 14:35:30 UTC 2012,,,,,,0|i0gt1j:,96135,jbellis,jbellis,,,,,,,,,,"25/Apr/12 19:52;minaguib;I can confirm that testing with cassandra 1.1.0 has the same conclusion.

To reproduce against cassandra 1.1.0, edit cassandra.yaml and set:
{quote}
row_cache_provider: ConcurrentLinkedHashCacheProvider                                                                                                                                                     
row_cache_size_in_mb: 200                                                                                                                                                                                 
{quote}

And use this slightly updated script to accomodate for 1.1.0 changes:
{quote}
create keyspace Test;
use Test;

create column family Users with column_type='Super' and key_validation_class='UTF8Type' and comparator='UTF8Type' and subcomparator='UTF8Type' and default_validation_class='UTF8Type' and caching='ALL';

set Users['mina']['attrs']['name'] = 'Mina';
get Users['mina'];

set Users['mina']['attrs']['country'] = 'Canada';
get Users['mina'];

set Users['mina']['attrs']['region'] = 'Quebec';
get Users['mina'];
{quote}

The rest of the observations are the same as with the cassandra 1.0.9 test.",26/Apr/12 13:05;slebresne;Turns out the patch for CASSANDRA-3957 had a stupid typo. Attaching patch to fix and I've pushed a test in the dtests.,"26/Apr/12 14:26;jbellis;Javadoc also needs update, otherwise +1","26/Apr/12 14:35;slebresne;Committed (with javadoc update), thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 ALTER TABLE command causes NPE,CASSANDRA-4163,12551325,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thepaul,khahn,khahn,17/Apr/12 23:43,12/Mar/19 14:16,13/Mar/19 22:27,19/Apr/12 20:54,1.1.0,,,,,0,cql3,,,,,,"To reproduce the problem:

./cqlsh --cql3
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-rc1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.30.0]
Use HELP for help.

cqlsh> CREATE KEYSPACE test34 WITH strategy_class = 'org.apache.cassandra.locator.SimpleStrategy' AND strategy_options:replication_factor='1';

cqlsh> USE test34;

cqlsh:test34> CREATE TABLE users (
          ... password varchar,
          ... gender varchar,
          ... session_token varchar,
          ... state varchar,
          ... birth_year bigint,
          ... pk varchar,
          ... PRIMARY KEY (pk)
          ... );

cqlsh:test34> ALTER TABLE users ADD coupon_code varchar;
TSocket read 0 bytes
","INFO 16:07:11,757 Cassandra version: 1.1.0-rc1-SNAPSHOT
INFO 16:07:11,757 Thrift API version: 19.30.0
INFO 16:07:11,758 CQL supported versions: 2.0.0,3.0.0-beta1 (default: 2.0.0)
",,,,,,,,,,,,,,,,,,19/Apr/12 18:36;thepaul;4163.patch-2.txt;https://issues.apache.org/jira/secure/attachment/12523384/4163.patch-2.txt,18/Apr/12 20:51;thepaul;4163.patch.txt;https://issues.apache.org/jira/secure/attachment/12523254/4163.patch.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-18 02:55:34.365,,,no_permission,,,,,,,,,,,,236189,,,Thu Apr 19 20:54:22 UTC 2012,,,,,,0|i0gspz:,96083,jbellis,jbellis,,,,,,,,,,"18/Apr/12 02:55;thepaul;Sorry, should have been more specific. This is not cqlsh related; it causes an NPE in the Cassandra server:

{noformat}
ERROR [Thrift:2] 2012-04-17 15:13:28,048 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.cassandra.cql3.CFPropDefs.addAll(CFPropDefs.java:192)
        at org.apache.cassandra.cql3.statements.AlterTableStatement.<init>(AlterTableStatement.java:52)
        at org.apache.cassandra.cql3.CqlParser.alterTableStatement(CqlParser.java:2286)
        at org.apache.cassandra.cql3.CqlParser.cqlStatement(CqlParser.java:428)
        at org.apache.cassandra.cql3.CqlParser.query(CqlParser.java:183)
        at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:219)
        at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:201)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{noformat}","18/Apr/12 20:51;thepaul;Fix attached here; also broken up into 3 more concise chunks on the 4163 branch in my github. Link:

http://github.com/thepaul/cassandra/tree/pending/4163

However, in testing, I found that ALTER TABLE  ALTER TYPE still doesn't work, and ALTER TABLE WITH only works in some cases. Will need to make new tickets for those.","18/Apr/12 20:58;jbellis;LGTM.

Nit: could we initialize properties to an empty map to avoid having to null-check it?","18/Apr/12 21:03;thepaul;We could, in the grammar definition, but that attribute is only ever updated with an assignment, so the empty map would be consed for nothing. Hardly a big deal either way though.","18/Apr/12 21:17;jbellis;Yeah, I don't see this as performance critical so I'd rather go for cleanliness.","19/Apr/12 18:36;thepaul;K, attached and updated in github.","19/Apr/12 18:37;thepaul;Link for the new tag (it's on the same branch, though):

http://github.com/thepaul/cassandra/tree/pending/4163-2",19/Apr/12 20:54;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFRR wide row iterator does not handle tombstones well,CASSANDRA-4154,12551062,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,16/Apr/12 13:35,12/Mar/19 14:16,13/Mar/19 22:27,16/Apr/12 21:24,1.1.0,,,,,0,,,,,,,"If the last row is a tombstone, CFRR's wide row iterator will throw an exception:

{noformat}

java.util.NoSuchElementException
        at com.google.common.collect.Iterables.getLast(Iterables.java:663)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:441)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:467)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:413)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:137)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:132)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.nextKeyValue(ColumnFamilyRecordReader.java:188)
        at org.apache.cassandra.hadoop.pig.CassandraStorage.getNextWide(CassandraStorage.java:140)
        at org.apache.cassandra.hadoop.pig.CassandraStorage.getNext(CassandraStorage.java:199)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:187)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:423)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
{noformat}",,,,,,,,,,,,,,,,,,,16/Apr/12 20:05;jbellis;4154.txt;https://issues.apache.org/jira/secure/attachment/12522850/4154.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-16 21:24:38.121,,,no_permission,,,,,,,,,,,,235926,,,Mon Apr 16 21:24:38 UTC 2012,,,,,,0|i0gsm7:,96066,brandon.williams,brandon.williams,,,,,,,,,,16/Apr/12 20:31;brandon.williams;+1,16/Apr/12 21:24;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decommission should take a token,CASSANDRA-4061,12546908,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,17/Mar/12 17:52,12/Mar/19 14:16,13/Mar/19 22:27,09/May/12 16:38,1.0.11,1.1.1,,,,0,,,,,,,"Like removetoken, decom should take a token parameter.  This is a bit easier said than done because it changes gossip, but I've seen enough people burned by this (as I have myself.)  In the short term though *decommission still accepts a token parameter* which I thought we had fixed.",,,,,,,,,,,,,,,,,,,09/May/12 15:56;brandon.williams;4061.txt;https://issues.apache.org/jira/secure/attachment/12526175/4061.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-20 12:24:34.573,,,no_permission,,,,,,,,,,,,232066,,,Wed May 09 16:38:13 UTC 2012,,,,,,0|i0gri7:,95886,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"20/Mar/12 12:24;jonma;I think using ""./nodetool -h host -p port decommission "" is  more simple than using ""./nodetool -h host -p port decommision <the node's token>"" . If ""./nodetool -h host -p port decommision <the node's token>""  could be available ,it  can't be better.","09/May/12 15:56;brandon.williams;To the dismay of my OCD, after looking at the options I think calling decommission on the host you want to decommission is the cleanest option we have.

However, as stated, nodetool should definitely not allow miscellaneous args to get passed, allow users to think it's doing something other than what it is.  Patch to fix this and clarify the help message, against 1.0 in the unlikely case we ever have reason to roll a 1.0.11.",09/May/12 16:31;vijay2win@yahoo.com;+1,09/May/12 16:38;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql3 ORDER BY not ordering,CASSANDRA-4246,12555858,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,thepaul,thepaul,15/May/12 19:05,12/Mar/19 14:16,13/Mar/19 22:27,17/May/12 17:38,1.1.1,,,,,0,cql3,,,,,,"Creating the simplest composite-key cql3 table I can think of, populating it with a few rows of data, then trying to do a query with an ORDER BY does not yield ordered results.

Here's a cql script:

{noformat}
create keyspace test with strategy_class = 'SimpleStrategy'
   and strategy_options:replication_factor = 1;
use test;
create table moo (a int, b int, c int, primary key (a, b));

insert into moo (a, b, c) values (123, 12, 3400);
insert into moo (a, b, c) values (122, 13, 3500);
insert into moo (a, b, c) values (124, 10, 3600);
insert into moo (a, b, c) values (121, 11, 3700);

select * from moo;
select * from moo order by b;
{noformat}

Here is the output of those two queries:

{noformat}
 a   | b  | c
-----+----+------
 121 | 11 | 3700
 122 | 13 | 3500
 124 | 10 | 3600
 123 | 12 | 3400

 a   | b  | c
-----+----+------
 121 | 11 | 3700
 122 | 13 | 3500
 124 | 10 | 3600
 123 | 12 | 3400
{noformat}

I also tried these using the bare thrift interface, to make sure it wasn't python-cql or cqlsh doing something stupid. Same results. Am I totally missing something important here about how this is supposed to work?",,,,,,,,,,,,,,,,,,,17/May/12 17:18;slebresne;4246-2.txt;https://issues.apache.org/jira/secure/attachment/12527859/4246-2.txt,17/May/12 12:45;slebresne;4246.txt;https://issues.apache.org/jira/secure/attachment/12527832/4246.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-15 19:31:56.037,,,no_permission,,,,,,,,,,,,255988,,,Thu May 17 17:38:21 UTC 2012,,,,,,0|i0gton:,96239,jbellis,jbellis,,,,,,,,,,"15/May/12 19:31;slebresne;Hum, I though we were correctly validating this but apparently not. In any case, the fix will be to disallow the second query. We don't know how to do ordering across multiple C* rows (and probably won't before quite some time). So, we should only allow order by when we know the query only select one physical C* row, which I though we were already doing but I'll fix.","17/May/12 12:45;slebresne;We were doing some validation that order by was not used when multiple internal rows are queried, but only for the reversed case for some reason. Patch attached to fix (it also refuse order by when a IN is used on the row key as this would require a bit more work to have it work correctly).","17/May/12 16:30;jbellis;Is this right?

{code}
                if (stmt.keyRestriction == null || !(stmt.keyRestriction.isEquality() && stmt.keyRestriction.eqValues.size() == 1))
{code}

I would have expected
{code}
                if (stmt.keyRestriction == null || !stmt.keyRestriction.isEquality() || stmt.keyRestriction.eqValues.size() != 1)
{code}","17/May/12 16:39;slebresne;Given that !(A&&B) == (!A)||(!B), I'd suggest that both are fine :)
But I'm fine going with the second one if you find it more readable.","17/May/12 17:05;jbellis;doh, demorgan fail :)

i do think it's a little clearer to have a chain of || than a mix of || and &&, though",17/May/12 17:18;slebresne;I'm good with that. Attaching v2 with that change.,17/May/12 17:34;jbellis;+1,"17/May/12 17:38;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in SSTableReader.getSampleIndexesForRanges(...) causes uneven InputSplits generation for Hadoop mappers,CASSANDRA-4259,12556422,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,br1985,br1985,br1985,18/May/12 17:07,12/Mar/19 14:16,13/Mar/19 22:27,19/May/12 22:35,1.1.1,,,,,0,,,,,,,"Running a simple mapreduce job on cassandra column family results in creating multiple small mappers for one half of the ring and one big mapper for the other half. Upper part (85... - 0) is cut into smaller slices. Lower part (0 - 85...) generates one big input slice. One mapper processing half of the ring causes huge inefficiency. Also the progress meter for this mapper is incorrect - it goes to 100% in a couple of seconds, than stays at 100% for an hour or two.

I've investigated the problem a bit. I think it is related to incorrect output of 'nodetool rangekeysample'. On the node resposible for part (0 - 85...) the output is empty! On the other node it works fine.

I think the bug is in SSTableReader.getSampleIndexesForRanges(...). These two lines:

   RowPosition leftPosition = range.left.maxKeyBound();
   RowPosition rightPosition = range.left.maxKeyBound();

should be changed to:

   RowPosition leftPosition = range.left.maxKeyBound();
   RowPosition rightPosition = range.right.maxKeyBound();

After that fix the output of nodetool is correct and the whole ring is split into small mappers.

The other half of the ring works fine because of extra 'if' in the code:

   int right = Range.isWrapAround(range.left, range.right)...

This causes that the bug does not show up in one-node cluster or in the ""last"" ring partition in muli-node clusters.

Can anyone look at it and verify my thoughts? I'm rather new to Cassandra.
","Small cassandra cluster with 2 nodes. Version 1.1.0. 

Tokens: 0, 85070591730234615865843651857942052864

Hadoop 1.0.1 and Pig 0.10.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-19 22:35:25.39,,,no_permission,,,,,,,,,,,,256001,,,Mon Jun 18 15:48:26 UTC 2012,,,,,,0|i0gtuf:,96265,jbellis,jbellis,,,,,,,,,,"19/May/12 22:35;jbellis;Good detective work! Committed, thanks.","18/Jun/12 15:48;jbellis;To clarify: this was a regression introduced in 1.1.0, it should not affect 1.0.x.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig does not work on DateType,CASSANDRA-4204,12553443,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,cywjackson,cywjackson,01/May/12 00:20,12/Mar/19 14:16,13/Mar/19 22:27,01/May/12 17:07,1.0.10,,,,,0,,,,,,,"cqlsh:PigDemo> describe columnfamily test1897;

CREATE COLUMNFAMILY test1897 (
KEY text PRIMARY KEY,
testcol timestamp
) WITH
comment='' AND
comparator=text AND
row_cache_provider='SerializingCacheProvider' AND
key_cache_size=200000.000000 AND
row_cache_size=0.000000 AND
read_repair_chance=1.000000 AND
gc_grace_seconds=864000 AND
default_validation=blob AND
min_compaction_threshold=4 AND
max_compaction_threshold=32 AND
row_cache_save_period_in_seconds=0 AND
key_cache_save_period_in_seconds=14400 AND
replicate_on_write=True;

cqlsh:PigDemo> select * from test1897;
KEY | testcol
-----+-------------------------
akey | 2012-01-21 00:14:12+0000

$ cat test1897.pig
cassandra_data = LOAD 'cassandra://PigDemo/test1897' USING CassandraStorage() AS (name, columns: bag {T: tuple()});
dump cassandra_data;

there seems problem with the DateType. the above simple pig script fail with the attached err
",,,,,,,,,,,,,,,,,,,01/May/12 16:54;brandon.williams;4204.txt;https://issues.apache.org/jira/secure/attachment/12525185/4204.txt,01/May/12 00:21;cywjackson;pig_1335816404547.log;https://issues.apache.org/jira/secure/attachment/12525144/pig_1335816404547.log,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-01 16:54:44.598,,,no_permission,,,,,,,,,,,,237605,,,Tue May 01 19:58:37 UTC 2012,,,,,,0|i0gt7j:,96162,rbranson,rbranson,,,,,,,,,,"01/May/12 16:54;brandon.williams;Sad patch w/test to cast DateType to a long, since pig has no native date type, and DateType is just a long under the hood anyway.","01/May/12 17:02;rbranson;Reviewed, looks good.",01/May/12 17:07;brandon.williams;Committed.,"01/May/12 19:58;brandon.williams;Just noticed we're actually returning a byte array here, so I changed this to a long in 2b7672fa11efa6e8c3c5cbc6fa857b3bc8947092",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move error on 1 node cluster,CASSANDRA-4200,12553413,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,nickmbailey,nickmbailey,30/Apr/12 20:27,12/Mar/19 14:16,13/Mar/19 22:27,04/May/12 08:19,1.1.1,,,,,0,,,,,,,"Attempting to move a node in a 1 node cluster with a keyspace using NTS produces an error:

{noformat}
bin/nodetool -h localhost move 0
Exception in thread ""main"" java.lang.IllegalStateException: unable to find sufficient sources for streaming range (0,129685538820263942208828358218513421652]
at org.apache.cassandra.dht.RangeStreamer.getRangeFetchMap(RangeStreamer.java:197)
	at org.apache.cassandra.dht.RangeStreamer.getWorkMap(RangeStreamer.java:205)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:2419)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:2327)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
...
{noformat}",,,,,,,,,,,,,,,,,,,03/May/12 09:40;slebresne;4200-v2.txt;https://issues.apache.org/jira/secure/attachment/12525422/4200-v2.txt,02/May/12 10:08;slebresne;4200.txt;https://issues.apache.org/jira/secure/attachment/12525271/4200.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-02 10:08:37.312,,,no_permission,,,,,,,,,,,,237572,,,Fri May 04 08:19:30 UTC 2012,,,,,,0|i0gt5z:,96155,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"02/May/12 10:08;slebresne;The problem is that RangeStreamer excludes localhost by default, but it shouldn't do that for move.

Attached patch to fix (it moves the locahost exclusion out of RangeStreamer, but I think that is cleaner like that).",02/May/12 14:16;slebresne;Note: I've also added a test for this in the dtests,"02/May/12 17:57;vijay2win@yahoo.com;attached patch works but it streams the ranges back to the localhost, which i am not sure is intended.


 INFO [RMI TCP Connection(8)-10.2.179.47] 2012-05-02 10:50:17,366 StorageService.java (line 752) MOVING: fetching new ranges and streaming old ranges
 INFO [StreamStage:1] 2012-05-02 10:50:17,367 StreamOut.java (line 113) Beginning transfer to localhost/127.0.0.1
 INFO [StreamStage:1] 2012-05-02 10:50:17,367 StreamOut.java (line 94) Flushing memtables for [CFS(Keyspace='Keyspace1', ColumnFamily='Counter1'), CFS(Keyspace='Keyspace1', ColumnFamily='Super1'), CFS(Keyspace='Keyspace1', ColumnFamily='SuperCounter1'), CFS(Keyspace='Keyspace1', ColumnFamily='Standard1')]...
 INFO [StreamStage:1] 2012-05-02 10:50:17,368 StreamOut.java (line 159) Stream context metadata [/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-69-Data.db sections=1 progress=0/279349056 - 0%, /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-86-Data.db sections=1 progress=0/279349056 - 0%, /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-89-Data.db sections=1 progress=0/59903136 - 0%], 3 sstables.
 INFO [StreamStage:1] 2012-05-02 10:50:17,368 StreamOutSession.java (line 161) Streaming to localhost/127.0.0.1
 INFO [Streaming to localhost/127.0.0.1:2] 2012-05-02 10:50:25,211 StreamReplyVerbHandler.java (line 55) Successfully sent /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-69-Data.db to /127.0.0.1
 INFO [Streaming to localhost/127.0.0.1:2] 2012-05-02 10:50:32,983 StreamReplyVerbHandler.java (line 55) Successfully sent /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-86-Data.db to /127.0.0.1
 INFO [Streaming to localhost/127.0.0.1:2] 2012-05-02 10:50:34,760 StreamReplyVerbHandler.java (line 55) Successfully sent /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-89-Data.db to /127.0.0.1
 INFO [Thread-15] 2012-05-02 10:50:34,761 StreamInSession.java (line 190) Finished streaming session 1335981017366948000 from localhost/127.0.0.1
","03/May/12 09:40;slebresne;Oups, you're right, I don't know what I was thinking. Attaching v2 that simply recognize that localhost is a valid source (but don't stream from it).",03/May/12 19:15;vijay2win@yahoo.com;+1,"04/May/12 08:19;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix descriptor versioning for bloom filter changes,CASSANDRA-4203,12553435,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,jbellis,jbellis,30/Apr/12 22:33,12/Mar/19 14:16,13/Mar/19 22:27,02/May/12 05:27,1.2.0 beta 1,,,,,0,,,,,,,"CASSANDRA-2975 introduced changes to the Data component, breaking stream compatibility",,,,,,,,,,,,,,,,,,,01/May/12 01:17;vijay2win@yahoo.com;0001-CASSANDRA-4203-v2.patch;https://issues.apache.org/jira/secure/attachment/12525148/0001-CASSANDRA-4203-v2.patch,30/Apr/12 22:37;jbellis;4203.txt;https://issues.apache.org/jira/secure/attachment/12525128/4203.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-30 23:00:35.659,,,no_permission,,,,,,,,,,,,237595,,,Wed May 02 07:36:16 UTC 2012,,,,,,0|i0gt73:,96160,yukim,yukim,,,,,,,,,,"30/Apr/12 22:37;jbellis;2975 introduced version ""ib"", but as noted in Descriptor comments, ""Minor versions must be forward-compatible"" (which in practice means, ""only additions to the metadata are allowed"").

We'll run out of version letters if we keep bumping the major version in trunk, so rather than turn this into version j, I suggest we stick with ia until 1.2.0 is released.

This patch also updates the streaming compatibility version to i.","30/Apr/12 23:00;yukim;I think you mean CASSANDRA-2319 broke compatibility. +1 on change in {{isStreamingCompatible}}.

And I'm ok with sticking with ""ia"", but you also need to fix here: https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/Descriptor.java#L113","01/May/12 01:17;vijay2win@yahoo.com;Attached patch adds yuki's comments and fixes the test accordingly. 
Attachement also makes streaming to be always be the latest version.
One thing to note is that folks who build from trunk are out of luck they have to wipe the data... 

Not sure if we need CASSANDRA-4196 anymore.","01/May/12 02:06;jbellis;bq. One thing to note is that folks who build from trunk are out of luck they have to wipe the data... 

That would be a problem, but hopefully it's blazingly obvious that running trunk in production is insane.  Especially at the beginning of a dev cycle.

I don't see a good alternative, if we continue chewing through major revisions the way we did in 0.7 we're kind of screwed.  We started at the wrong end of the ascii table; we don't even have the luxury of using upper case characters. :)  (""A"" < ""a"")",02/May/12 04:41;jbellis;+1 on v2,02/May/12 05:13;vijay2win@yahoo.com;Committed Thanks!,02/May/12 07:36;slebresne;Agreed on not burning versions during development. If we start trying to keep compatibility between changes to trunk it'll be a mess. I'm fine with starting being careful about such things once we release a first beta (even though the point of doing betas and rcs is that this shouldn't really be a requirement even then) but before that all bets are off imo.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql3 ALTER TABLE ALTER TYPE has no effect,CASSANDRA-4170,12551529,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,thepaul,thepaul,18/Apr/12 20:57,12/Mar/19 14:16,13/Mar/19 22:27,20/Apr/12 15:27,1.1.0,,Legacy/CQL,,,0,cql3,,,,,,"running the following with cql3:

{noformat}
CREATE TABLE test (foo text PRIMARY KEY, bar int);
ALTER TABLE test ALTER bar TYPE float;
{noformat}

does not actually change the column type of bar. It does under cql2.

Note that on the current cassandra-1.1.0 HEAD, this causes an NPE, fixed by CASSANDRA-4163. But even with that applied, the ALTER shown here has no effect.",,,,,,,,,,,,,,,,,,,20/Apr/12 10:15;slebresne;4170.txt;https://issues.apache.org/jira/secure/attachment/12523474/4170.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-20 10:15:53.904,,,no_permission,,,,,,,,,,,,236337,,,Fri Apr 20 15:27:46 UTC 2012,,,,,,0|i0gstb:,96098,jbellis,jbellis,,,,,,,,,,20/Apr/12 10:15;slebresne;Due to a small typo. Patch attached. I've push a simple test for that in the dtests.,20/Apr/12 14:00;jbellis;+1,"20/Apr/12 15:27;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3: fix index dropping and assign default name if none provided at index creation,CASSANDRA-4192,12552832,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,26/Apr/12 10:35,12/Mar/19 14:16,13/Mar/19 22:27,14/May/12 06:52,1.1.1,,Legacy/CQL,,,0,cql3,,,,,,"This ticket proposes to fix two problems of CQL3 index handling:
# DROP INDEX is broken (because the code forgot to clone the metadata before doing modification which break the schema update path)
# If an index is created with a name (which CREATE INDEX allow), there is no way to drop the index (note that we will internally assign a name to the index ColumnFamilyStore, but we don't assign a name in the ColumnDefinition object, which is the only one checked by DROP INDEX).",,,,,,,,,,,,,,,,,,,03/May/12 08:48;slebresne;4192-v2.txt;https://issues.apache.org/jira/secure/attachment/12525415/4192-v2.txt,26/Apr/12 10:40;slebresne;4192.txt;https://issues.apache.org/jira/secure/attachment/12524420/4192.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-02 17:40:38.976,,,no_permission,,,,,,,,,,,,236785,,,Mon May 14 06:52:34 UTC 2012,,,,,,0|i0gt2f:,96139,jbellis,jbellis,,,,,,,,,,"26/Apr/12 10:40;slebresne;Patch attached fixes the two issues above. For the second one, it makes sure CREATE INDEX always assign a name to the index. If none is provided, it assign a default one based on the column family name and column name (making sure that 1) the index name is valid and 2) the name is unique). The patch also make sure one cannot create 2 index with the same name, as DROP would have a random behavior in that case.

Note that is probably other way to deal with those problems, yet I'm not sure it's worth getting much more fancy than that.",02/May/12 17:40;jbellis;Can we leverage CFMetadata.validate instead of rewriting the code here?,"03/May/12 08:48;slebresne;Right, I looked too quickly at that code and missed that we already validate the name is not unique and we already do assign a name to index that don't have one, sorry. What we could do though is to make sure we pick a name that is unique when choosing a default one (it's not a big deal to fail in that case, the user can always pick a name itself, but it's not very nice).

Anyway, attaching v2, that only includes the fix to DROP INDEX and make sure we pick a unique name.",11/May/12 22:49;jbellis;+1,"14/May/12 06:52;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Looks like Serializing cache broken in 1.1,CASSANDRA-4141,12550607,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,12/Apr/12 00:00,12/Mar/19 14:16,13/Mar/19 22:27,12/Apr/12 15:39,1.1.0,1.2.0 beta 1,,,,0,,,,,,,"I get the following error while setting the row cache to be 1500 MB

INFO 23:27:25,416 Initializing row cache with capacity of 1500 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
java.lang.OutOfMemoryError: Java heap space
Dumping heap to java_pid26402.hprof ...

havent spend a lot of time looking into the issue but looks like SC constructor has 

.initialCapacity(capacity)
.maximumWeightedCapacity(capacity)

 which 1500Mb",,,,,,,,,,,,,,,,,,,12/Apr/12 00:22;vijay2win@yahoo.com;0001-CASSANDRA-4141.patch;https://issues.apache.org/jira/secure/attachment/12522351/0001-CASSANDRA-4141.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-12 09:51:57.146,,,no_permission,,,,,,,,,,,,235471,,,Thu Apr 12 15:39:24 UTC 2012,,,,,,0|i0gsgn:,96041,xedin,xedin,,,,,,,,,,12/Apr/12 00:22;vijay2win@yahoo.com;attached fixes this issue... changes to ConcurrentLinkedHashCache is not needed but thought the default was good instead of setting it to 0.,12/Apr/12 09:51;xedin;Wouldn't that instead crash with OOM later in some unpredictable moment (e.g. when cache is read from disk) or when cache resizing is done?,"12/Apr/12 14:23;vijay2win@yahoo.com;initialCapacity here is the number of elements in the hashmap and not the size. Size should be controlled by maximumWeightedCapacity (yes the names are confusing through)
We are running OOM because we where trying to allocate 1500 * 1024 *1024 elements.

the intial capacity is used to create the ConcurrentHashMap
{code}
new ConcurrentHashMap<K, Node>(builder.initialCapacity, 0.75f, concurrencyLevel);
{code}",12/Apr/12 14:37;xedin;+1,12/Apr/12 15:39;vijay2win@yahoo.com;Committed thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
isMarkedForDelete can return false if it is a few seconds in the future,CASSANDRA-4307,12559327,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,wadey,wadey,05/Jun/12 06:09,12/Mar/19 14:16,13/Mar/19 22:27,06/Jun/12 08:06,1.1.2,,,,,0,,,,,,,"The patch in CASSANDRA-3716 causes some weird issues to arrise when server times don't exactly match up (and since the resolution is seconds, it is easy to be off by just enough to see it).

I am seeing a case where during schema propagation .isMarkedForDelete() is checked, but the timestamp is a few seconds in the future because the schema was sent from a different node. The code then happily tries to interpret the value of the column as a String, but it is actually the Int encoded deletion time.

Here is an example in the code that does this check and will do the wrong thing if the deletion timestamp is even just a few seconds in the future: https://github.com/apache/cassandra/blob/47f0cc5d38d272ec9f7d6179eb3ffa28c6f74107/src/java/org/apache/cassandra/cql3/statements/SelectStatement.java#L607-609

To prove that this is a problem, here is a stack trace of a machine trying to interpret the ""localDeletionTime"" value of a DeletedColumn as UTF-8 because the .isMarkedForDeletion() check failed:

https://gist.github.com/deb064d4377d206368d3",,,,,,,,,,,,,,,,,,,05/Jun/12 17:40;wadey;4307-test.txt;https://issues.apache.org/jira/secure/attachment/12530986/4307-test.txt,05/Jun/12 14:20;slebresne;4307.txt;https://issues.apache.org/jira/secure/attachment/12530959/4307.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-06-05 14:20:12.598,,,no_permission,,,,,,,,,,,,256047,,,Wed Jun 06 08:06:42 UTC 2012,,,,,,0|i0guev:,96357,jbellis,jbellis,,,,,,,,,,"05/Jun/12 14:20;slebresne;I believer you're right, good catch!
Attaching patch to re-add the overwrite of isMarkedForDelete in DeletedColumn. We shouldn't have removed it since DeletedColumn should never ever be interpreted as live columns, even with lack of node synchronization (it's ok for expiring columns though).",05/Jun/12 15:49;jbellis;+1,05/Jun/12 17:40;wadey;Test case to ensure no future regressions. Fails if 4307 patch is not applied.,"06/Jun/12 08:06;slebresne;Committed. I included the unit test, thanks Wade.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing host ID results in NPE when delivering hints,CASSANDRA-4300,12558773,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,31/May/12 12:48,12/Mar/19 14:16,13/Mar/19 22:27,31/May/12 16:11,,,,,,0,vnodes,,,,,,"
In {{StorageService.handledStateNormal()}} the token-to-endpoint map is updated before the id-to-endpoint map, creating a small window where {{TokenMetadata.isMember()}} can return true before a host ID is available.

Trivial patch forthcoming.

{noformat}
[...]
 INFO [GossipStage:1] 2012-05-30 21:59:10,683 Gossiper.java (line 833) Node /10.2.131.32 has restarted, now UP
 INFO [GossipStage:1] 2012-05-30 21:59:10,684 Gossiper.java (line 799) InetAddress /10.2.131.32 is now UP
 INFO [HintedHandoff:1] 2012-05-30 21:59:10,697 HintedHandOffManager.java (line 304) Started hinted handoff for host: null with IP: /10.2.131.32
ERROR [HintedHandoff:1] 2012-05-30 21:59:10,698 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[HintedHandoff:1,1,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.UUIDGen.decompose(UUIDGen.java:112)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:305)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:265)
	at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:86)
	at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:439)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
 INFO [GossipStage:1] 2012-05-30 21:59:10,700 Events.java (line 130) Node 10.2.131.32 now available: true
 INFO [GossipStage:1] 2012-05-30 21:59:10,700 StorageService.java (line 1218) Node /10.2.131.32 state jump to normal
 INFO [GossipStage:1] 2012-05-30 21:59:10,701 Events.java (line 120) Node 10.2.131.32 is now in state NORMAL
[...]
{noformat}",,,,,,,,,,,,,,,,,,,31/May/12 12:54;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4300-update-host-ID-before-token.txt;https://issues.apache.org/jira/secure/attachment/12530384/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4300-update-host-ID-before-token.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-31 14:34:13.496,,,no_permission,,,,,,,,,,,,256040,,,Thu May 31 16:11:06 UTC 2012,,,,,,0|i0gubz:,96344,brandon.williams,brandon.williams,,,,,,,,,,31/May/12 14:34;brandon.williams;+1,31/May/12 16:11;urandom;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary Indexes fail following a system restart,CASSANDRA-4289,12558250,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,beobal,beobal,26/May/12 16:56,12/Mar/19 14:16,13/Mar/19 22:27,01/Jun/12 15:57,1.2.0 beta 1,,Feature/2i Index,,,0,,,,,,,"Create a new cf with a secondary index, and queries with indexes predicates work fine until the server is restarted, after which they error and the following stacktrace is output to the log:

{code}
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:44)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:88)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:1)
	at org.apache.cassandra.utils.IntervalTree.comparePoints(IntervalTree.java:191)
	at org.apache.cassandra.utils.IntervalTree.contains(IntervalTree.java:203)
	at org.apache.cassandra.utils.IntervalTree.access$3(IntervalTree.java:201)
	at org.apache.cassandra.utils.IntervalTree$IntervalNode.searchInternal(IntervalTree.java:293)
	at org.apache.cassandra.utils.IntervalTree.search(IntervalTree.java:140)
	at org.apache.cassandra.utils.IntervalTree.search(IntervalTree.java:146)
	at org.apache.cassandra.db.ColumnFamilyStore.markReferenced(ColumnFamilyStore.java:1259)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:229)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1300)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1174)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1104)
	at org.apache.cassandra.db.index.keys.KeysSearcher$1.computeNext(KeysSearcher.java:144)
	at org.apache.cassandra.db.index.keys.KeysSearcher$1.computeNext(KeysSearcher.java:1)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1409)
	at org.apache.cassandra.db.index.keys.KeysSearcher.search(KeysSearcher.java:88)
	at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:595)
	at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1398)
	at org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:47)
	at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:870)
	at org.apache.cassandra.cql3.statements.SelectStatement.multiRangeSlice(SelectStatement.java:259)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:134)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1236)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:1)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

Tested with a single node setup & verified that this behaviour is only present in trunk, cassandra-1.0.10 works as expected.
","Single node, trunk",,,,,,,,,,,,,CASSANDRA-4331,,,,,01/Jun/12 03:27;jbellis;4289-keycache.txt;https://issues.apache.org/jira/secure/attachment/12530503/4289-keycache.txt,01/Jun/12 14:18;beobal;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4289-Fix-errors-with-secondary-index-caused-.txt;https://issues.apache.org/jira/secure/attachment/12530550/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4289-Fix-errors-with-secondary-index-caused-.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-27 05:02:06.905,,,no_permission,,,,,,,,,,,,256030,,,Fri Jun 01 15:57:26 UTC 2012,,,,,,0|i0gu7b:,96323,xedin,xedin,,,,,,,,,,27/May/12 05:02;dbrosius@apache.org;easily reproducable. The index's column family's IntervalTree's head BigIntegerToken compared to the searched against column value.,"31/May/12 05:26;dbrosius@apache.org;It appears this commit introduced this regression

Save IndexSummary into new SSTable 'Summary' component for CASSANDRA-2392

http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=04874186892c86a20181a2f64c5dc24285021b2c","31/May/12 10:12;xedin;The commit you mentioned doesn't touch CFS it merely saves/loads primary index into separate component, but anyway - what do you suggest as work around?","01/Jun/12 02:48;jbellis;What am I doing wrong?

{noformat}
cqlsh> create schema ks with strategy_class ='SimpleStrategy' and strategy_options:replication_factor=1;
cqlsh> use ks;
cqlsh:ks> create table foo(key text primary key, i int);
cqlsh:ks> create index i_idx on foo(i);
cqlsh:ks> insert into foo (key, i) values('asdf', 1);
cqlsh:ks> select * from foo where i = 1;
  KEY | i
------+---
 asdf | 1

[restart Cassandra server and cqlsh]

cqlsh> use ks;
cqlsh:ks> select * from foo where i = 1;
 KEY  | i 
------+---
 asdf | 1
{noformat}","01/Jun/12 03:06;dbrosius@apache.org;before stopping the server do

nodetool flush","01/Jun/12 03:27;jbellis;Still works for me.

I did fix a bug in the key cache for index CFs (attached) but I don't think that's what you're seeing.","01/Jun/12 14:21;beobal;SSTableReader.loadSummary wasn't taking the specific IPartitioner for the sstable into consideration, so when the IndexSummary for a secondary index cf was loaded from disk, the keys in the summary and the first & last keys of the sstable were decorated incorrectly. 

jbellis: I guess you weren't seeing this as the IndexSummary wasn't being written down to disk, but I don't why that would be",01/Jun/12 15:57;xedin;Committed combination of Sam's patch and changes to CacheService so row/key cache serializer methods would use per-CF partitioner instead of global. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrading encounters: 'SimpleStrategy requires a replication_factor strategy option.' and refuses to start,CASSANDRA-4294,12558481,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,29/May/12 19:48,12/Mar/19 14:16,13/Mar/19 22:27,01/Jun/12 15:41,1.1.1,,,,,0,,,,,,,"I've seen this reported quite a few times now:

{noformat}
ERROR [main] 2012-05-29 19:33:40,589 AbstractCassandraDaemon.java (line 370) Exception encountered during startup
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
  at org.apache.cassandra.db.Table.<init>(Table.java:275)
  at org.apache.cassandra.db.Table.open(Table.java:114)
  at org.apache.cassandra.db.Table.open(Table.java:97)
  at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
  at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
  at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
  at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:71)
  at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:218)
  at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:295)
  at org.apache.cassandra.db.Table.<init>(Table.java:271)
  ... 5 more
{noformat}

The common thread seems to be old lineage, from at least 0.7.  1.0.x works fine, but upgrading to 1.1 causes the problem.",,,,,,,,,,,,,,,,,,,01/Jun/12 08:51;slebresne;4294.txt;https://issues.apache.org/jira/secure/attachment/12530524/4294.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-31 05:03:41.704,,,no_permission,,,,,,,,,,,,256035,,,Fri Jun 01 15:41:25 UTC 2012,,,,,,0|i0gu9j:,96333,jbellis,jbellis,,,,,,,,,,29/May/12 19:57;brandon.williams;A common workaround for this is to delete the schema (Schema* and Migrations* in the system keyspace) and then recreate it.,"31/May/12 05:03;jbellis;To clarify:

0.7 -> 1.0: fine
1.0 -> 1.1: fine
0.7 -> 1.1: not fine

?","31/May/12 05:10;brandon.williams;I strongly believe the path originates from 0.7, but 1.1 is the culprit.","01/Jun/12 08:45;slebresne;Well, apparently we did broke compatibility of 0.7 -> 1.1. In 1.0, in {{KSMetadata.fromAvro()}}, we have the following line:
{noformat}
maybeAddReplicationFactor(strategyOptions, ks.strategy_class.toString(), ks.replication_factor);
{noformat}
which allows compatibility with the old ways where the replication factor was part of the KsDef directly. In 1.1, we've removed that line (the fromAvro method has been moved to config/Avro.java).

I think the idea of removing this was that we were fine not supporting direct upgrade from 0.7 -> 1.1 and force upgrade to 1.0 first. Unfortunately, since maybeAddReplicationFactor() was called by fromAvro, it means that one would have to upgrade from 0.7 to 1.0 *and* do some schema upgrade to every keyspace before upgrading to 1.1. Otherwise, I don't think that the fix of maybeAddReplicationFactor is persisted (I haven't tested so I'm not 100% sure but I think that's the case).

So I think we may want to add back the maybeAddReplicationFactor to fromAvro for now. We can drop that in 1.2 if we want however, since 1.1 will guarantee that this is persisted since the first thing 1.1 does is to save schema in the new format.",01/Jun/12 08:51;slebresne;Attaching patch that adds back {{maybeAddReplicationFactor}} for compatibility sake as suggested in previous comment.,"01/Jun/12 12:39;pasthelod;Upgrading from 1.0.2 to 1.1.0, I've encountered this problem.

Also, I wouldn't mind the KeySpace upgrade provided there's a guide for it, which I haven't been able to locate so far.","01/Jun/12 14:02;jbellis;There was a bunch more code using mARF in 1.0, mainly from forwardsCompatibleOptions.  Do we need that too?","01/Jun/12 14:10;slebresne;I don't think we do, {{forwardsCompatibleOptions}} was used on the thrift side (so not on the upgrade path), to ""support"" KsDef.replication_factor. But it's marked deprecated so people shouldn't use it (if they do, we will ignore it and likely trigger an exception since the replication factor will be missing).","01/Jun/12 14:42;jbellis;+1 from me, did you want to test Brandon?","01/Jun/12 15:41;slebresne;Committed, thanks.

Brandon: feel free to complain if you do test and this doesn't fix it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple SLF4J bindings warning when running stress,CASSANDRA-4276,12557021,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,nickmbailey,mbulman,mbulman,23/May/12 15:38,12/Mar/19 14:16,13/Mar/19 22:27,29/May/12 22:37,1.1.1,,Legacy/Tools,,,0,,,,,,,"{noformat}
> ./tools/bin/stress
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/path/apache-cassandra-1.1.0/tools/lib/stress.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/path/apache-cassandra-1.1.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
{noformat}",,,,,,,,,,,,,,,,,,,23/May/12 22:29;nickmbailey;0001-Allow-stress-to-run-from-binary-tar.patch;https://issues.apache.org/jira/secure/attachment/12528803/0001-Allow-stress-to-run-from-binary-tar.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-23 18:41:10.698,,,no_permission,,,,,,,,,,,,256018,,,Tue May 29 22:37:37 UTC 2012,,,,,,0|i0gu1r:,96298,kirktrue,kirktrue,,,,,,,,,,23/May/12 18:41;brandon.williams;Also looks like stress no longer works in the binary dist due to a classpath problem.,23/May/12 22:29;nickmbailey;Fixes the slf4j warning as well as allowing running from the tarball dist.,"29/May/12 22:35;kirktrue;+1, patch applies and fixes issue against current trunk.",29/May/12 22:37;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Oracle Java 1.7 u4 does not allow Xss128k,CASSANDRA-4275,12557011,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,appodictic,appodictic,23/May/12 14:38,12/Mar/19 14:16,13/Mar/19 22:27,19/Nov/13 22:44,1.1.2,,,,,0,,,,,,,"Problem: This happens when you try to start it with default Xss setting of 128k
=======
The stack size specified is too small, Specify at least 160k
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

Solution
=======
Set -Xss to 256k

Problem: This happens when you try to start it with Xss = 160k
========
ERROR [Thrift:14] 2012-05-22 14:42:40,479 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thrift:14,5,main]
java.lang.StackOverflowError

Solution
=======
Set -Xss to 256k",,,,,,,,,,,,,,,,,,,12/Jun/12 09:34;slebresne;4275.txt;https://issues.apache.org/jira/secure/attachment/12531799/4275.txt,25/Jun/12 22:48;scurrilous;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4275-Use-JVM-s-reported-minimum-stack-size-o.txt;https://issues.apache.org/jira/secure/attachment/12533394/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4275-Use-JVM-s-reported-minimum-stack-size-o.txt,23/May/12 14:46;appodictic;trunk-cassandra-4275.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12528739/trunk-cassandra-4275.1.patch.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-05-23 14:48:18.189,,,no_permission,,,,,,,,,,,,256017,,,Tue Nov 19 22:44:01 UTC 2013,,,,,,0|i0gu1j:,96297,jbellis,jbellis,,,,,,,,,,23/May/12 14:46;appodictic;Ups Xss to a value that Java 7 will accept.,23/May/12 14:48;jbellis;I could go for increasing to 160 but doubling is a big hit.  What is the full SOE stack trace?  Let's figure out what's doing a lot of stack allocation and make it play nice.,"23/May/12 14:53;appodictic;{noformat}
ERROR [Thrift:41] 2012-05-22 14:02:08,685 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thrift:41,5,main]
java.lang.StackOverflowError
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:150)
	at java.net.SocketInputStream.read(SocketInputStream.java:121)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2877)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{noformat}","23/May/12 15:23;brandon.williams;How many clients are connecting to each node, socket-wise?","23/May/12 15:38;appodictic;Port 9160 is given to apani in /etc/services :)

{noformat}
[root@cdbsd28 ~]# netstat -a | grep ESTABLISHED | grep apani | wc -l
672
{noformat}
","23/May/12 16:24;scode;Just wanted to quickly chime in and say that I saw this too at 160k (used 512k and was done with it). I don't remember whether the stack trace matches exactly or not, but it was definitely one of the I/O paths - either thrift or internode. This was with a build of openjdk7, so not specific to the Oracle release.","23/May/12 20:30;appodictic;I search for answers on stackoverflow.com but found none. This is interesting reading that suggests 512 is the default.
http://www.onkarjoshi.com/blog/209/using-xss-to-adjust-java-default-thread-stack-size-to-save-memory-and-prevent-stackoverflowerror/ ",23/May/12 20:59;jbellis;Wouldn't surprise me; defaults are *way* too high for us...  we go out of our way to avoid recursion and other stack-eating constructs so we don't chew up a lot of memory per thread.  Our goal is to run w/ the minimum allowed by the jvm.,"25/May/12 14:37;appodictic;I understand wanting to keep this setting low. But there must be some fairly major underlying factor that made the JVM developers decided to raise the minimum allowable from whatever it was to 160. While the increase is large 125K->250K the net effect on 1000 connections is not a big deal. 125MB vs 250MB  This is stack space not heap correct? Will using 100MB more stack be a problem in the grand schema of Cassandra memory management? Also we do not want some minimum setting that is likely to blow up on someone at the first sign of load. 

I do not think it is a bad idea to dig in and determine if we can safely code/tune cassandra to lower the setting. But I will un-assign myself if that is the road we are going to take because that type of analysis is not my bread and butter skill set.","25/May/12 15:21;jbellis;bq. there must be some fairly major underlying factor that made the JVM developers decided to raise the minimum allowable from whatever it was to 160

Doubt it.  Old minimum was 128, so it's not *that* big a change.

I've asked Jake Farrell to have a look at Thrift/Java7 to see if anything odd is going on here.","12/Jun/12 09:34;slebresne;Since currently C* doesn't start with Oracle java7 u4, I suggest we fix this. On the other side, we know -Xss128 works fine for java 6 so why take any risk? Attaching patch that keeps 128 for java 6 but use 160 for java 7.

I suggest we start with that, and if it proves that indeed java 7 puts so much more on the stack that 160 is too low, it will still be time to up it later.","12/Jun/12 12:23;jbellis;SGTM, +1","12/Jun/12 13:06;slebresne;Committed, thanks","25/Jun/12 16:45;scurrilous;If the goal is to run with the JVM minimum stack size, how about attempting to query it directly? Going to 160k on 32-bit/ARM systems with Java 7 is way too much.

On amd64 with 7u4:

\# java -Xss32k

The stack size specified is too small, Specify at least 160k
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

On amd64 with 6u32 and 7u2:

\# java -Xss32k

The stack size specified is too small, Specify at least 104k
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

On ARM with 7u4:

\# java -Xss32k

The stack size specified is too small, Specify at least 64k
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

I can provide a patch and/or open a new issue if this sounds reasonable.","25/Jun/12 17:13;jbellis;That sounds like a good improvement to have, we'd appreciate the patch.","25/Jun/12 22:53;scurrilous;The patch is attached. Tested with Oracle 7u4 and OpenJDK 6u24 on amd64, and Oracle 7u4 on ARMv7. Reported stack sizes of 160k, 104k, and 64k, respectively. Just tested startup on amd64, but ran 'stress -o insert' on ARM.

Note that the patch also fixes an issue with CASSANDRA-4366 (UseCondCardMark) where JDK 1.7 was only detected on Linux.
",26/Jun/12 18:15;brandon.williams;It seems like we should generalize this a bit more to cover FreeBSD et al as well since I imagine the same problem exists there.,04/Jul/12 18:11;shawn_;Will this fix be back ported to 1.0.x?,"04/Jul/12 19:43;jbellis;No.  You can tweak your 1.0.x startup script manually if you really want to run it on java 7 (not really recommended, they're still shaking the bugs out).","18/Jul/12 00:27;scurrilous;So regarding ARM, etc., should this be reopened or a new issue created?

As for FreeBSD, I have no experience at all with it. Unless someone else volunteers to help with that, I'd like to fix ARM/Linux now and keep the FreeBSD issue separate (as I could envision that languishing indefinitely).",18/Jul/12 07:38;jbellis;Should probably open a new ticket either way since we committed the original for 1.1.2.,"20/Jul/12 21:53;jeromatron;Should this be reopened to set it to -Xss256k?  Both Ed Capriolo and another person posting in IRC have found 160 insufficient.  Ed is running in production with 256 and the other person is changing to 256.
<Rav|2> how should I set -Xss for oracle java 7? 160k causes java.lang.StackOverflowError :(
<ecapriolo> 256 or higher
<Rav|2> ecapriolo: everything is back to normal with 256. big thanks :)",20/Jul/12 21:57;scurrilous;My proposed fix for CASSANDRA-4442 would set it to 200k (160k * 1.25). Would that be large enough?,"21/Jul/12 03:39;jbellis;I'm -1 on just shrugging and throwing numbers at it until it appears to work.  I want to find out *why* 160 isn't enough, because there isn't supposed to be anything allocated on the stack anywhere near that size.  Let's either find the bug that is the root cause or advance our understanding of where that memory is going.","21/Jul/12 14:59;jeromatron;I'll create another ticket then.  The problem is that out of the box it sounds like it just doesn't work for people.  I agree that it's better to find the root cause, but leaving it at a lower level and having everyone who uses it with jdk 7 have the same error until they go into IRC and ask and get the workaround is also undesirable in the meantime.

Could 256k be committed for now until the new ticket is resolved so people can have it start up properly in the meantime?

Created CASSANDRA-4457.","21/Jul/12 17:28;jbellis;You do realize that JDK7 is still pretty buggy, right?  If anything I'd support a modification to echo ""JDK7 is not supported, use at your own risk.  You can do this by making the following edit to the stack size.""","21/Jul/12 17:53;jeromatron;Fair enough.  A clear notification would be nice so that they know to update the value without checking the user list or IRC.  It sounds like Ed and others are starting to use JDK7 in production based on the needs of their organization, so it would be nice to have something documented about support for JDK7 and tweaking as you've mentioned.","21/Jul/12 17:57;scurrilous;""JDK7, get off my lawn!"" :p

Buggy or not though, Java 6 was originally scheduled for EOL this month, and is now EOL in November: https://blogs.oracle.com/henrik/entry/updated_java_6_eol_date

I'd argue that it's time to start supporting Java 7 at least experimentally so that any remaining bugs can be fixed before we're forced to rely on it.

(I'll also admit my bias toward Java 7 since the server VM for ARM is not available for Java 6.)",24/Sep/12 16:41;chengas123;I'd like to see Java 7 supported as well. It's going to be the default in the Ubuntu release next month and all my other software uses it just fine now.,24/Sep/12 17:22;jbellis;This was addressed in CASSANDRA-4602,"31/Oct/13 19:24;enigmacurry;cassandra-1.1 and cassandra-1.2 both currently default to -Xss180k. Using Java 1.7.0_40 this setting is too small:

{code}
xss =  -ea -javaagent:/home/ryan/.ccm/repository/git_cassandra-1.2/bin/../lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1982M -Xmx1982M -Xmn400M -XX:+HeapDumpOnOutOfMemoryError -Xss180k

The stack size specified is too small, Specify at least 228k
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
{code}",19/Nov/13 22:44;jbellis;Bumped Xss to 256k in 1.2 as well.  Not touching fixver here since it's really talking about a different issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows tools don't work and litter the environment,CASSANDRA-4344,12560755,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,h2o,h2o,h2o,15/Jun/12 11:49,12/Mar/19 14:16,13/Mar/19 22:27,16/Jun/12 15:05,1.1.2,,Legacy/Tools,,,0,,,,,,,"On Windows the tools either don't work at all (cassandra-stress) and/or litter the shell environment (cassandra-stress & sstablemetadata) by repeatedly appending the same information to variables, eventually running out of space.
","any Windows, any JDK",,,,,,,,,,,,,,,,,,15/Jun/12 11:52;h2o;sstablemeta.patch;https://issues.apache.org/jira/secure/attachment/12532187/sstablemeta.patch,15/Jun/12 11:52;h2o;stress.patch;https://issues.apache.org/jira/secure/attachment/12532188/stress.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-06-16 13:29:37.236,,,no_permission,,,,,,,,,,,,256079,,,Sat Jun 16 15:05:54 UTC 2012,,,,,,0|i0gutb:,96422,jbellis,jbellis,,,,,,,,,,15/Jun/12 11:52;h2o;Trivial fixes for tools to consistently find the right classes and not litter the environment.,"16/Jun/12 13:29;jbellis;Does stress work for you with these changes?  I still get ""Could not find the main class: org.apache.cassandra.stress.Stress.""","16/Jun/12 14:21;h2o;Yes, everything runs for me (I just re-tested): start cassandra node on localhost, open second shell in ../tools/bin, run cassandra-stress.bat, it goes to work. I made these changes directly in the unpacked 1.1.1 distro tree. The main problem was that resolving ""*.jar"" as single string did not work for the stress.jar, so it was never found. The other .bat scripts do the same find-append loop, as well as the setlocal.
","16/Jun/12 15:05;jbellis;Ah, I see, this works if you have the tarball extracted with stress.jar in tools/lib ...  Added the build/ directories so it will work from a dev environment as well, and committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in handleStateNormal in a mixed cluster,CASSANDRA-4317,12559719,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,07/Jun/12 16:51,12/Mar/19 14:16,13/Mar/19 22:27,20/Jun/12 02:12,1.2.0 beta 1,,,,,0,,,,,,,"In a 3 node cluster with one seed on trunk, a member on trunk, and another member on a previous version, the following occurs only on the non-seed trunk member:

{noformat}

ERROR 16:44:18,708 Exception in thread Thread[GossipStage:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1072)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:995)
        at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1568)
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:819)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:897)
        at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:43)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:57)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

This doesn't repro if a non-trunk member is the seed, however upgrading the seed first should still be valid.",,,,,,,,,,,,,,CASSANDRA-4311,,,CASSANDRA-4101,,19/Jun/12 16:22;brandon.williams;0001-Gossip-current-network-version.txt;https://issues.apache.org/jira/secure/attachment/12532575/0001-Gossip-current-network-version.txt,19/Jun/12 16:22;brandon.williams;0002-Check-both-ms-and-gossip-for-version-when-handling-sta.txt;https://issues.apache.org/jira/secure/attachment/12532576/0002-Check-both-ms-and-gossip-for-version-when-handling-sta.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-06-20 01:59:25.567,,,no_permission,,,,,,,,,,,,256055,,,Wed Jun 20 02:12:47 UTC 2012,,,,,,0|i0gui7:,96372,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"14/Jun/12 15:26;brandon.williams;The problem here is similar to what CASSANDRA-4311 is trying to solve, but with a small twist.

Consider node X, Y, and Z.  Both X and Y are on the newer version v2, but Z is on the older version v1.  X is the seed, and Z is the first to talk to X, and they complete a gossip round and both speak v1 to each other.  Now Y contacts X and they complete a gossip round, where Y learns about Z.  The problem, however, is that Y has never talked directly to Z, so it doesn't actually know Z's version yet but needs to handle events for it.  StorageService then does version checks to determine how to handle the events for Z, but getVersion assumes the current version (v2 in this example) when it doesn't actually know what the version is, and thus ends up processing old-style information as new-style and fails.  This isn't limited to handleStateNormal, but likely any event called from onChange.

Solving this is somewhat tricky.  Reviving CASSANDRA-4101 would do it, but then we have two sources of version information and I'm not sure how clean that will be.  getVersion would have to check for null, and if found check for a gossip version, and if that too is not found assume the *lowest* version.

","19/Jun/12 16:22;brandon.williams;The first patch is CASSANDRA-4101 rebased, which gossips the net version.

The second adds a way to see is MS _really_ knows the version (so that we don't have to mess with its current behavior) and encapsulates the logic to check both it and gossip to determine if hostids should be used inside SS, replacing all the current checks there as well.",20/Jun/12 01:59;vijay2win@yahoo.com;+1,20/Jun/12 02:12;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clean up messagingservice protocol limitations,CASSANDRA-4311,12559479,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,06/Jun/12 02:18,12/Mar/19 14:16,13/Mar/19 22:27,14/Jun/12 23:03,1.2.0 beta 1,,,,,0,jmx,,,,,,"Weaknesses of the existing protocol:

- information asymmetry: node A can know what version node B expects, but not vice versa (see CASSANDRA-4101)
- delayed information: node A will often not know what version node B expects, until after first contacting node B -- forcing it to throw that first message away and retry for the next one
- protocol cannot handle both cross-dc forwarding and broadcast_address != socket address (see bottom of CASSANDRA-4099)
- version is partly global, partly per-connection, and partly per-message, resulting in some interesting hacks (CASSANDRA-3166) and difficulty layering more sophisticated OutputStreams on the socket (CASSANDRA-3127, CASSANDRA-4139)",,,,,,,,,,,,,,,,,,,06/Jun/12 03:11;jbellis;4311-skeleton.txt;https://issues.apache.org/jira/secure/attachment/12531058/4311-skeleton.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-06 16:52:52.607,,,no_permission,,,,,,,,,,,,256050,,,Thu Jun 14 23:03:46 UTC 2012,,,,,,0|i0gug7:,96363,brandon.williams,brandon.williams,,,,,,,,,,"06/Jun/12 02:22;jbellis;I think we can address all of these by making these changes to VERSION_12 protocol:

- send version and connection metadata just once per connection.  This will give the other side enough information to know whether to expect a compressed or varint stream
- *both* sides exchange CURRENT_VERSION when connection is established; each side will then have version information immediately upon contacting another node

As a consequence,
- When version changes, we need to drop existing connections and reconnect (already implied but not yet implemented by CASSANDRA-3127)",06/Jun/12 03:11;jbellis;I think the changes to ITC demonstrate what I have in mind pretty clearly (attached).  I'll flesh the rest of this out shortly.,"06/Jun/12 16:52;brandon.williams;This looks like a good solution so far.

bq. When version changes, we need to drop existing connections and reconnect

Let me take this opportunity to suggest that set/getVersion be moved out of Gossiper (which does nothing but expose a Map) and into MS.",06/Jun/12 18:17;jbellis;Pushed to https://github.com/jbellis/cassandra/branches/4311-2,"07/Jun/12 12:36;krummas;looks good to me, just a bit confusing calling it a 'header' when it is essentially a handshake packet

could we negotiate SSL the way we do compression? nothing 'secret' is shared during the handshake phase anyway, after that we could upgrade sockets","07/Jun/12 17:49;brandon.williams;MS.setVersion needs a minor fix to prevent NPE:

{noformat}
     public Integer setVersion(InetAddress address, int version)
     {
         logger.debug(""Setting version {} for {}"", version, address);
         Integer v = versions.put(address, version);
         return v == null ? version : v;
     }
{noformat}

+1 otherwise.","14/Jun/12 23:03;jbellis;bq. MS.setVersion needs a minor fix to prevent NPE

Done and committed.

bq. could we negotiate SSL the way we do compression

Sure, can you open a separate ticket for that?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsha server may stop responding and will not close selectors,CASSANDRA-4370,12595782,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,kvaster,kvaster,kvaster,25/Jun/12 12:20,12/Mar/19 14:16,13/Mar/19 22:27,26/Jun/12 01:42,1.1.2,,,,,0,,,,,,,"Cassandra launches several threads to listen on selectors. There can be CancelledKeyException and cassandra will log ""Unexpected exception"". In that case there two problems:
1) listener thread will be closed and cassandra will stop after all listener threads will stop.
2) selector will be not closed",,,,,,,,,,,,,,,,,,,25/Jun/12 12:21;kvaster;hsha.patch;https://issues.apache.org/jira/secure/attachment/12533285/hsha.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-25 16:57:42.219,,,no_permission,,,,,,,,,,,,256102,,,Tue Jun 26 01:42:23 UTC 2012,,,,,,0|i0gv4f:,96472,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,25/Jun/12 12:21;kvaster;Patch,"25/Jun/12 16:57;vijay2win@yahoo.com;1) I am wondering when will we get the CancelledKeyException? 
We might get it when the client decides to exit out, isn't it safe to ignore it (may ignore the log as well) and let the following code deal with it?

{code}
                    if (!key.isValid())
                    {
                        // if invalid cleanup.
                        cleanupSelectionkey(key);
                        continue;
                    }
{code}

2) Why should we close the selector when one of the key is invalid?","25/Jun/12 22:14;kvaster;1) This code is not enough. Even if key is valid in that expression, it may be invalid in next statement - I was able to reproduce it on my boxes with cassandra 1.1.1.
2) We should not close selector at all until server shutdown.","26/Jun/12 01:42;vijay2win@yahoo.com;+1, Committed to 1.1 and trunk, (Misread the patch earlier)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bulkLoad() method in StorageService throws AssertionError,CASSANDRA-4368,12595636,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,dinoscottie,dinoscottie,22/Jun/12 21:19,12/Mar/19 14:16,13/Mar/19 22:27,25/Jun/12 17:39,1.1.2,,Legacy/Tools,,,0,,,,,,,"Firstly, I apologize if this is a duplicate, as I cannot find a bug related to that.

We tried to stream some data to our Cassandra cluster by using JMX bulkLoad method. However, jmx reports a AssertionError since 1.1.0. I haven't really debugged into Cassandra, but by eyeballing the code it seems the AssertionError is thrown from SSTableReader.open() method with the line:
{code}
assert practitioner != null;
{code}
and tracing the code backwards, it seems the code in SSTableLoader.openSSTables() method has been changed to get the partitioner from the impl of inner class SSTableLoader.Client:
{code}
sstables.add(SSTableReader.open(desc, components, null, client.getPartitioner()));
{code}
This is different than 1.0.x codebase, when the partitioner is retrieved from StorageService:
{code}
sstables.add(SSTableReader.open(desc, components, null, StorageService.getPartitioner()));
{code}
The problem seems to me is when StorageService.bulkLoad instantiaties an impl of SSTableLoader.Client() it never does anything with the partitioner, resulting in the call 'client.getPartitioner()' returning null, thus the AssertionError.

(Note: this is me eyeballing the code only without debugging into it).
",,,,,,,,,,,,,,,,,,,25/Jun/12 16:49;brandon.williams;4368.txt;https://issues.apache.org/jira/secure/attachment/12533332/4368.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-25 16:49:40.518,,,no_permission,,,,,,,,,,,,256100,,,Mon Jun 25 22:04:58 UTC 2012,,,,,,0|i0gv3j:,96468,jbellis,jbellis,,,,,,,,,,"25/Jun/12 16:49;brandon.williams;Your analysis was completely correct (and thanks for that, since errors from jmx are stacktraceless!)

Patch to specify the partitioner in bulkLoad()","25/Jun/12 17:34;jbellis;LGTM, +1.

Nit: could make the ""setPartitioner(String partclass)"" overload chain to this new one.",25/Jun/12 17:39;brandon.williams;Committed with setPartitioner chained.,"25/Jun/12 22:04;hudson;Integrated in Cassandra #1589 (See [https://builds.apache.org/job/Cassandra/1589/])
    removed duplicate SSTableLoader.setPartitioner(IPartitioner) method introduced by patch for CASSANDRA-4368 (Revision 112ce0c69b940d6b663c1e44d2c765bbfc526e8f)

     Result = ABORTED
xedin : 
Files : 
* src/java/org/apache/cassandra/io/sstable/SSTableLoader.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL client timeout when inserting data after creating index,CASSANDRA-4328,12560155,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,evilezh,evilezh,11/Jun/12 11:44,12/Mar/19 14:16,13/Mar/19 22:27,12/Jun/12 13:07,1.1.2,,,,,0,,,,,,,"After creating index on table inserts fails.
steps (from cqlsh -3)
create table myapp (pidh text, cn text, tn text, s text, m text, ts bigint, PRIMARY KEY (pidh, ts));
INSERT INTO myapp(pidh, cn, tn, s, m, ts) VALUES ('4274@localhost','Test.tests','main','text','bzzzzz',2231897614162493);
create index idx_cn on myapp(cn);

Next insert from cql client time outs without showing error.
Each insert in systemlog gives ERROR [MutationStage:xx] ....
from log file:

 INFO [MigrationStage:1] 2012-06-11 12:28:35,715 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-schema_columnfamilies@1502301540(1259/1573 serialized/live bytes, 20 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:35,716 Memtable.java (line 266) Writing Memtable-schema_columnfamilies@1502301540(1259/1573 serialized/live bytes, 20 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:35,868 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-143-Data.db (1312 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [MigrationStage:1] 2012-06-11 12:28:35,869 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-schema_columns@1756291746(280/350 serialized/live bytes, 5 ops)
 INFO [CompactionExecutor:26] 2012-06-11 12:28:35,869 CompactionTask.java (line 109) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-141-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-142-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-140-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-143-Data.db')]
 INFO [FlushWriter:4] 2012-06-11 12:28:35,869 Memtable.java (line 266) Writing Memtable-schema_columns@1756291746(280/350 serialized/live bytes, 5 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,104 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-65-Data.db (325 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [CompactionExecutor:26] 2012-06-11 12:28:36,130 CompactionTask.java (line 221) Compacted to [/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-144-Data.db,].  42,461 to 38,525 (~90% of original) bytes for 3 keys at 0.140767MB/s.  Time: 261ms.
 INFO [MigrationStage:1] 2012-06-11 12:28:36,140 SecondaryIndexManager.java (line 208) Creating new index : ColumnDefinition{name=636e, validator=org.apache.cassandra.db.marshal.UTF8Type, index_type=KEYS, index_name='idx_cn', component_index=1}
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,141 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-myapp@207814912(171/213 serialized/live bytes, 4 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,141 Memtable.java (line 266) Writing Memtable-myapp@207814912(171/213 serialized/live bytes, 4 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,255 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-2-Data.db (170 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,256 SecondaryIndex.java (line 159) Submitting index build of myapp.idx_cn for data in SSTableReader(path='/var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-2-Data.db')
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,258 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-IndexInfo@409882274(39/48 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,258 Memtable.java (line 266) Writing Memtable-IndexInfo@409882274(39/48 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,390 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/IndexInfo/system-IndexInfo-hd-14-Data.db (84 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8744)
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,390 SecondaryIndex.java (line 200) Index build of myapp.idx_cn complete
ERROR [MutationStage:37] 2012-06-11 12:28:39,657 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:37,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:267)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:369)
	at java.util.TreeMap.getEntry(TreeMap.java:340)
	at java.util.TreeMap.containsKey(TreeMap.java:227)
	at java.util.TreeMap$KeySet.contains(TreeMap.java:1045)
	at org.apache.cassandra.db.Table.apply(Table.java:415)
	at org.apache.cassandra.db.Table.apply(Table.java:380)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:294)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:453)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1250)
	... 3 more


ERROR [MutationStage:39] 2012-06-11 12:29:39,876 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:39,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:267)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:369)
	at java.util.TreeMap.getEntry(TreeMap.java:340)
	at java.util.TreeMap.containsKey(TreeMap.java:227)
	at java.util.TreeMap$KeySet.contains(TreeMap.java:1045)
	at org.apache.cassandra.db.Table.apply(Table.java:415)
	at org.apache.cassandra.db.Table.apply(Table.java:380)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:294)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:453)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1250)
	... 3 more
",Linux emka 3.3.4-gentoo #4 SMP Mon May 14 17:03:02 BST 2012 x86_64 Intel(R) Core(TM) i7-3820 CPU @ 3.60GHz GenuineIntel GNU/Linux,,,,,,,,,,,,,,,,,,12/Jun/12 12:05;slebresne;4328.txt;https://issues.apache.org/jira/secure/attachment/12531814/4328.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-11 12:41:56.226,,,no_permission,,,,,,,,,,,,256064,,,Tue Jun 12 13:07:00 UTC 2012,,,,,,0|i0gumn:,96392,jbellis,jbellis,,,,,,,,,,"11/Jun/12 12:41;slebresne;For information secondary indexes are not supported on composite primary keys. We have a ticket for that (CASSANDRA-3680) but this won't be before Cassandra 1.2.

That being said, there is a clear lack of validation here and we will fix that (in other words, the 'create index' should return an meaningful error message).",12/Jun/12 12:05;slebresne;Attaching patch to reject index creation on table with composite primary keys.,12/Jun/12 12:24;jbellis;+1,"12/Jun/12 13:07;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error in log when upgrading multi-node cluster to 1.1,CASSANDRA-4195,12553369,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,30/Apr/12 14:17,12/Mar/19 14:16,13/Mar/19 22:27,11/Jul/12 22:00,1.0.11,,,,,0,,,,,,,"I upgraded a cluster from 1.0.9 to 1.1.0. The following message shows up in the logs for all but the first node.
{code}
ERROR [GossipStage:1] 2012-04-30 07:37:06,986 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:1,5,main]
java.lang.UnsupportedOperationException: Not a time-based UUID                  
    at java.util.UUID.timestamp(UUID.java:331)                                  
    at org.apache.cassandra.service.MigrationManager.updateHighestKnown(MigrationManager.java:121)
    at org.apache.cassandra.service.MigrationManager.rectify(MigrationManager.java:99)
    at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:83)
    at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:806)           
    at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:849)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:908)   
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:62)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
{code}

this dtest demonstrates the issue. It was added to the cassandra-dtest repository as upgrade_to_11_test:
{code}
from dtest import Tester, debug 
from tools import * 
 
class TestUpgradeTo1_1(Tester): 
 
    def upgrade_test(self): 
        self.num_rows = 0 
        cluster = self.cluster 
 
        # Forcing cluster version on purpose 
        cluster.set_cassandra_dir(cassandra_version='1.0.9') 
 
        cluster.populate(3).start() 
        time.sleep(1) 
 
        for node in cluster.nodelist():     
            node.flush() 
            time.sleep(.5) 
            node.stop(wait_other_notice=True) 
            node.set_cassandra_dir(cassandra_version='1.1.0') 
            node.start(wait_other_notice=True) 
            time.sleep(.5)
{code}","ccm, dtest. Ubuntu",,,,,,,,,,,,,,,,,,09/Jul/12 20:33;xedin;CASSANDRA-4195.patch;https://issues.apache.org/jira/secure/attachment/12535737/CASSANDRA-4195.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-30 14:34:02.346,,,no_permission,,,,,,,,,,,,237523,,,Wed Jul 11 22:00:53 UTC 2012,,,,,,0|i0gt3r:,96145,jbellis,jbellis,,,,,,,,,,30/Apr/12 14:34;jbellis;I thought we added logic to not send incompatible migration messages to 1.0.x nodes?,"01/May/12 00:12;xedin;Yes, we don't send it and the problem is not about it - here we see the same problem I was talking about at CASSANDRA-3804 - as we no longer use time-based UUID to represent schema version, the ""old"" nodes (version < 1.1) would fail to join into the ring because they would be stuck on the ""highest"" version resolution as they assume that all nodes have time-based UUID for version which is no longer the case. We should make a strict regulation about mixed version C* ring otherwise things would continue to break in the different ways, probably to forbid the older node to join if it detects nodes with version >= 1.1, because we can't rely just skip schema agreement checks on startup or on ring changes...","01/May/12 03:06;jbellis;bq. probably to forbid the older node to join if it detects nodes with version >= 1.1, because we can't rely just skip schema agreement checks on startup or on ring changes

Doesn't that kill node-at-a-time upgrading dead in the water?","01/May/12 10:48;xedin;It would, but on the other hand one can't upgrade node-at-a-time to 1.1 because of incompatible schema changes... We of course can report something like ""Nodes with C* version >= 1.1 detected, please upgrade!"" instead of failing in MM.rectify(...) but if we continue to operate on that node we can't guarantee stability anyway... ",09/Jul/12 16:23;jbellis;We *need* to support read/write ops in a mixed cluster.  We don't need to support schema changes.,"09/Jul/12 16:31;xedin;bq. We need to support read/write ops in a mixed cluster. We don't need to support schema changes.

See my first comment, old nodes wouldn't be able to join the ring because they can't resolve the schema version correctly and as we can't know what is the difference between old/new schemas they wouldn't be able to properly handle read/write workloads. We can probably add special mechanism to the 1.0 that would ask for schema in Avro format if it detects that schema version is not a TimeUUID, load it and try to operate but that would require from uses to update their nodes to the latest 1.0 version before they can do upgrade to 1.1.","09/Jul/12 16:35;jbellis;Ugh, how did we miss this in testing?

How I think it should work:

# if node can't parse schema version, it should join ring w/ whatever schema it currently has
# nodes should make best-effort to fulfil requests w/ current schema instead of blocking for schema reconciliation

That way as long as users don't try to modify schema during the upgrade they will be fine.

I think #2 works as desired, which leaves #1 as a fairly easy update to make in 1.0.11.","09/Jul/12 16:49;xedin;Ok, I can do #1 and it would get ring into disagreement until upgrade to 1.1 is finished with is a good thing in our case because users won't be able to mutate it. I was warning about from the time we have decided to change the way version is handled... ",09/Jul/12 20:33;xedin;patch against cassandra-1.0 to make it skip trying to migrate the schema if it detects that it's running in mixed node cluster and verbose warning.,09/Jul/12 21:59;brandon.williams;Worked for me.,11/Jul/12 21:39;jbellis;+1,11/Jul/12 22:00;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORDER BY ... DESC reverses comparrison predicates in WHERE,CASSANDRA-4160,12551206,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,dohse,dohse,17/Apr/12 12:06,12/Mar/19 14:16,13/Mar/19 22:27,02/May/12 15:47,1.1.1,,Legacy/CQL,,,1,cql3,,,,,,"When issuing a cql select statement with an ORDER BY ... DESC clause the comparison predicates in the WHERE clause gets reversed. 

Example: (see also attached)

SELECT number FROM test WHERE number < 3 ORDER BY number DESC

returns the results expected of WHERE number > 3",cqlsh,,,,,,,,,,,,,,,,,,02/May/12 14:44;slebresne;4160.txt;https://issues.apache.org/jira/secure/attachment/12525298/4160.txt,17/Apr/12 12:08;dohse;test.cql;https://issues.apache.org/jira/secure/attachment/12522947/test.cql,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-02 14:44:39.631,,,no_permission,,,,,,,,,,,,236070,,,Wed May 02 15:47:57 UTC 2012,,,,,,0|i0gson:,96077,jbellis,jbellis,,,,,,,,,,17/Apr/12 12:08;dohse;Example schema and data to demonstrate bug,"02/May/12 14:44;slebresne;Yep, we were forgetting to change the start and end when giving them to getSlice() on reversed queries. Fix attached.",02/May/12 15:03;jbellis;+1,02/May/12 15:31;dohse;Works,"02/May/12 15:47;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 Range Query contains unwanted results with composite columns,CASSANDRA-4372,12595805,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,grinser,grinser,25/Jun/12 16:33,12/Mar/19 14:16,13/Mar/19 22:27,27/Jun/12 07:53,1.1.2,,Legacy/CQL,,,1,bug,composite,compositeColumns,cql3,query,range,"Here is a CQL3 range query sample where I get wrong results (tested using cqlsh --cql3) from my perspective:

CREATE KEYSPACE testing WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;

USE testing;

CREATE TABLE bug_test (a int, b int, c int, d int, e int, f text, PRIMARY KEY (a, b, c, d, e) );

INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 2, '2');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 1, '1');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 2, 1, '1');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 3, '3');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 5, '5');

----------

Normal select everything query:

SELECT * FROM bug_test;

Results:

 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 1 | 1
 1 | 1 | 1 | 1 | 2 | 2
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Everything fine so far.

----------

Select with greater equal comparison for last column of composite key:

SELECT a, b, c, d, e, f FROM bug_test WHERE a = 1 AND b = 1 AND c = 1 AND d = 1 AND e >= 2;

Results:

 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 2 | 2
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Bug:
Why was the last row returned? It shouldn't be there, right?

----------

Select with greater comparison for last column of composite key:

SELECT a, b, c, d, e, f FROM bug_test WHERE a = 1 AND b = 1 AND c = 1 AND d = 1 AND e > 2;

Results:
 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Bug:
Why was the last row returned? It shouldn't be there, right?

The same issue is also present with between ranges (e >= 1 AND e <= 2)...","Mac OS 10.7.4, Cassandra 1.1.1, single node for testing",,,,,,,,,,,,,,,,,,26/Jun/12 16:34;slebresne;4372.txt;https://issues.apache.org/jira/secure/attachment/12533498/4372.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-26 16:34:59.245,,,no_permission,,,,,,,,,,,,256104,,,Wed Jun 27 07:53:55 UTC 2012,,,,,,0|i0gv5b:,96476,jbellis,jbellis,,,,,,,,,,26/Jun/12 16:34;slebresne;Fix attached (and I've pushed the test to the distributed tests).,"26/Jun/12 17:10;jbellis;LGTM, although if you want to leave the logging in, it should be converted to .debug w/o string concatenation","27/Jun/12 07:53;slebresne;Oups, didn't meant to leave the logging in. Committed with it removed. Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HintedHandoff can begin before SS knows the hostID,CASSANDRA-4384,12596137,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,27/Jun/12 19:00,12/Mar/19 14:16,13/Mar/19 22:27,15/Nov/12 17:58,1.2.0 beta 3,,,,,0,,,,,,,"Since HH fires from the FD, SS won't quite have the hostId yet:

{noformat}
 INFO 18:58:04,196 Started hinted handoff for host: null with IP: /10.179.65.102
 INFO 18:58:04,197 Node /10.179.65.102 state jump to normal
ERROR 18:58:04,197 Exception in thread Thread[HintedHandoff:1,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.utils.UUIDGen.decompose(UUIDGen.java:120)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:304)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:250)
        at org.apache.cassandra.db.HintedHandOffManager.access$400(HintedHandOffManager.java:87)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:433)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

Simple solution seems to be getting the hostId from gossip instead.",,,,,,,,,,,,,,,,,,,19/Sep/12 11:41;brandon.williams;4384.txt;https://issues.apache.org/jira/secure/attachment/12545711/4384.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-27 22:32:31.707,,,no_permission,,,,,,,,,,,,239484,,,Thu Nov 15 17:58:48 UTC 2012,,,,,,0|i00hsf:,814,jbellis,jbellis,,,,,,,,,,"27/Jun/12 22:32;urandom;This is a corner-case that only happens to a node that is restarted (and lost its hostId map as a result).  It is the result of a hint delivery being triggered (by the _reception_ of a gossip message) before the hostId could be processed from the message.

I see two ways to fix:

# Skip hint delivery when we don't (yet) have a hostId.
# Persist hostIds the way we do tokens.

(1) has the benefit of being a one-liner.  Presumably this code exists to schedule hint delivery for a remote host that was dead and is now alive.  Since in this case it is _us_ that was down, I don't think skipping would be Evil.

(2) adds complexity, but guards against any future cases of an {{IEndpointStateChangeSubscriber.onAlive()}} relying on {{TokenMetadata.isMember()}} before looking up a hostId.

","28/Jun/12 14:28;soverton;When a node first starts up without a hostId map, it may be asked to store a hint for a node which is down and it won't know the hostId of that node since it never saw it alive.

We should persist the hostIds to prevent both of these cases where we are missing required information when we first start up.

","28/Jun/12 18:03;brandon.williams;I'm inclined to agree with Sam, since this is how the problem is solved with tokens.","28/Jun/12 18:09;slebresne;CASSANDRA-4351 is relevant (or in other words, I agree too since we wanted to persist them anyway for that issue).","29/Jun/12 15:23;brandon.williams;I'll note that while CASSANDRA-4351 is similar in spirit, that is information we do NOT want to use, so I would prefer this be in LocationInfo with the tokens, and have everything else in a different CF, so the old method of blowing away LI when something goes bad is still valid.","29/Jun/12 16:13;urandom;bq. ...so I would prefer this be in LocationInfo with the tokens...

Tokens are now in the {{peers}} columnfamily, keyed by the token with one column for the peer (inet address).  With vnodes, that will become many-to-one (the endpoints will not be unique).  Host IDs are one-to-one.  The most straightforward approach seemed to be to create another columnfamily (peer_ids?), similar to {{peers}}, but keyed by host ID.",29/Jun/12 16:25;jbellis;or we could replace the existing {{peers}} entirely (I'd prefer to retain the table name) and add an index on the token column for that lookup.,"29/Jun/12 16:30;slebresne;bq. or we could replace the existing peers entirely

I would agree with that, but going a bit further I would imagine that a natural schema for that peers table could be something like:
{noformat}
CREATE TABLE peers (
  host_id UUID PRIMARY KEY,
  peer inet,
  tokens set<blob>,
)
{noformat}
with maybe even more info for each host but that's for CASSANDRA-4351. Of course that specific schema means waiting for CASSANDRA-3647, but I don't see that as a big problem. This wouldn't let use query by token but I don't think that really matter because we'll read the whole table content at startup anyway.","11/Jul/12 15:19;soverton;I have a branch at https://github.com/acunu/cassandra/tree/CASSANDRA-4384 which
* gossips host ID on startup
* persists host IDs to the peers CF and loads them on startup

I ended up changing the peers schema to be keyed off IP address instead of UUID:
{noformat}
CREATE TABLE peers (
  peer inet PRIMARY KEY,
  token_bytes blob,
  ring_id uuid,
)
{noformat}

I agree with Sylvain that it seems more natural to use UUID as the key rather than by IP, but that didn't seem as straightforward to update when tokens or hostIds change.

There is some overlap with CASSANDRA-4122 so it would be best to wait for that to go in before changing the SystemTable again for this.

","11/Jul/12 15:57;brandon.williams;I like making HOST_ID a full app state, that seems much cleaner.  Unsure on the ip vs uuid change at the moment.",04/Sep/12 18:48;brandon.williams;HOST_ID is a full app state after CASSANDRA-4383 now so we can revisit this.,"19/Sep/12 11:41;brandon.williams;Not sure where we stand on the schema changes, but we can trivially prevent this exception by checking the gossiper instead of SS now.","16/Oct/12 12:15;slebresne;We do persist the ring_id for all nodes now, so maybe we should use that.",15/Nov/12 17:53;jbellis;+1 on Brandon's fix for 1.2.0.,15/Nov/12 17:58;brandon.williams;Committed.  Let's leave future changes to a new ticket if needed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FD incorrectly using RPC timeout to ignore gossip heartbeats,CASSANDRA-4375,12595863,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,scode,scode,26/Jun/12 04:41,12/Mar/19 14:16,13/Mar/19 22:27,22/Jan/14 19:45,1.2.14,2.0.5,,,,0,gossip,,,,,,"Short version: You can't run a cluster with short RPC timeouts because nodes just constantly flap up/down.

Long version:

CASSANDRA-3273 tried to fix a problem resulting from the way the failure detector works, but did so by introducing a much more sever bug: With low RPC timeouts, that are lower than the typical gossip propagation time, a cluster will just constantly have all nodes flapping other nodes up and down.

The cause is this:

{code}
+    // in the event of a long partition, never record an interval longer than the rpc timeout,
+    // since if a host is regularly experiencing connectivity problems lasting this long we'd
+    // rather mark it down quickly instead of adapting
+    private final double MAX_INTERVAL_IN_MS = DatabaseDescriptor.getRpcTimeout();
{code}

And then:

{code}
-        tLast_ = value;            
-        arrivalIntervals_.add(interArrivalTime);        
+        if (interArrivalTime <= MAX_INTERVAL_IN_MS)
+            arrivalIntervals_.add(interArrivalTime);
+        else
+            logger_.debug(""Ignoring interval time of {}"", interArrivalTime);
{code}

Using the RPC timeout to ignore unreasonably long intervals is not correct, as the RPC timeout is completely orthogonal to gossip propagation delay (see CASSANDRA-3927 for a quick description of how the FD works).

In practice, the propagation delay ends up being in the 0-3 second range on a cluster with good local latency. With a low RPC timeout of say 200 ms, very few heartbeat updates come in fast enough that it doesn't get ignored by the failure detector. This in turn means that the FD records a completely skewed average heartbeat interval, which in turn means that nodes almost always get flapped on interpret() unless they happen to *just* have had their heartbeat updated. Then they flap back up whenever the next heartbeat comes in (since it gets brought up immediately).

In our build, we are replacing the FD with an implementation that simply uses a fixed {{N}} second time to convict, because this is just one of many ways in which the current FD hurts, while we still haven't found a way it actually helps relative to the trivial fixed-second conviction policy.

For upstream, assuming people won't agree on changing it to a fixed timeout, I suggest, at minimum, never using a value lower than something like 10 seconds or something, when determining whether to ignore. Slightly better is to make it a config option.

(I should note that if propagation delays are significantly off from the expected level, other things than the FD already breaks - such as the whole concept of {{RING_DELAY}}, which assumes the propagation time is roughly constant with e.g. cluster size.)",,,,,,,,,,,,,,,,,,,08/Jan/14 15:40;brandon.williams;4375.txt;https://issues.apache.org/jira/secure/attachment/12621991/4375.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-26 14:21:03.421,,,no_permission,,,,,,,,,,,,256107,,,Wed Jan 22 19:45:06 UTC 2014,,,,,,0|i0gv6f:,96481,jbellis,jbellis,,,,,,,,,,26/Jun/12 14:21;brandon.williams;What if we set MAX_INTERVAL_IN_MS to the greater of DD.getRpcTimeout() or the gossip interval * 2?,"26/Jun/12 17:00;scode;Why do we believe gossip interval * 2 is a good value? Empirically, that seems low given the 0-3x prop delay empirically observed. Remember that the gossip interval does not imply that the expected propagation delay is equal to the interval, since you're only gossiping to 1-2 random hosts.
","26/Jun/12 17:07;brandon.williams;bq. since you're only gossiping to 1-2 random hosts

You're always gossiping to a seed, though I suppose with enough seeds that too isn't reliable.  I'd be fine with setting it to something static like 10s to match the default rpc timeout, we just need some kind of reasonable bound to prevent CASSANDRA-3273.","26/Jun/12 22:54;scode;Don't rely on seeds, unless we want to say that seeds are required to be up for gossip propagation to happen in a supported fashion (beyond just acting as a common set fo avoid partitions).

Also, in my previous testing the seed:s ended up not impacting gossip propagation delay anyway; at least with a handful of seeds. With a single seed it might be different.","20/Nov/13 00:06;jbellis;bq.  the RPC timeout to ignore unreasonably long intervals is not correct

I agree; the two are unrelated.

Suggest either hardcoding it to 10s or making it configurable.","21/Nov/13 21:22;brandon.williams;Well, let's just set it to 10s.  Probably what most people have been using anyway, and I don't think it's quite yet worth making configurable.","21/Nov/13 22:38;jbellis;Is that long enough for a Really Big Cluster?  It's only 10 gossip rounds.  Seems like RING_DELAY would be a better choice for ""this is the longest I should go w/o hearing from any given node.""","22/Nov/13 22:38;brandon.williams;10s seemed 'most compatible' but I think you're right, that RING_DELAY is more semantically correct.  That said, I tend to think we should just set it to 30s instead of ring delay itself, since people often inflate ring delay for various things that might not translate well to this, and since we already seed the initial value at 30s.",22/Nov/13 22:56;jbellis;Allowing people to tune it could be a feature. :),22/Nov/13 22:58;brandon.williams;Shouldn't they be able to tune what we seed the FD with then too? ;),22/Nov/13 23:08;jbellis;I look forward to your patch. :),"08/Jan/14 15:40;brandon.williams;I've thought about this a bit, and still think it should default to ring delay, but not be coupled to it.  However, in normal operation, I do think it makes send to couple the initial value we seed the FD with and the max interval we accept.  I don't think most people should be tweaking these though, so I've made them system properties (as ring delay is.)

Patch adds cassandra.fd_initial_value_ms to control the value the FD is seeded with, which the max interval will also default to, but also adds cassandra.fd_max_interval_ms if you really need them to be disjoint (most likely for testing like CASSANDRA-6558 where you want the seed ridiculously low, but the max interval reasonable.)

I will note that I changed the max interval from a double to an int, because a double just didn't make any sense.","21/Jan/14 06:37;jbellis;Shouldn't it actually default to StorageService.getRingDelay then?

+1 otherwise, modulo whitespace issues","22/Jan/14 18:47;brandon.williams;bq. Shouldn't it actually default to StorageService.getRingDelay then?

This goes back to what I was saying about coupling RING_DELAY (which is set by getRingDelay) too closely with FD_INITIAL_VALUE.  Someone unaware of this ticket, who is changing ring delay for some reason, could ,get a nasty surprise, which would be pretty bad, especially in a minor since we're targeting 1.2.14 for this.  On the other hand, with the patch as it is if they *do* want them to move in lockstep for some odd reason, they can just define both properties to be the same.

bq. +1 otherwise, modulo whitespace issues

I couldn't see any whitespace issues.
","22/Jan/14 19:33;jbellis;bq. if they do want them to move in lockstep for some odd reason, they can just define both properties to be the same.

Good reasoning, +1

bq. I couldn't see any whitespace issues.

{{if (newvalue!= null)}}",22/Jan/14 19:45;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQLSH: describe command doesn't output valid CQL command.,CASSANDRA-4380,12595967,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thepaul,nickmbailey,nickmbailey,26/Jun/12 19:20,12/Mar/19 14:16,13/Mar/19 22:27,11/Jul/12 14:44,1.1.3,,,,,1,cqlsh,,,,,,"{noformat}
cqlsh:test> describe columnfamily stats;

CREATE TABLE stats (
  gid blob PRIMARY KEY
) WITH
  comment='' AND
  comparator='CompositeType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.UTF8Type)' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';
{noformat}

I can create a cf in cql3 and then use describe cf to get the above output. However trying to run that create statement says that all of the following are invalid options:

* default_validation
* min_compaction_threshold
* max_compaction_threshold
* comparator",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-06-27 08:00:45.796,,,no_permission,,,,,,,,,,,,256110,,,Wed Sep 19 16:28:35 UTC 2012,,,,,,0|i0gv8f:,96490,brandon.williams,brandon.williams,,,,,,,,,,"27/Jun/12 08:00;slebresne;I'll not that at lest for CQL3, as explained in CASSANDRA-4377, it will be much easier to fix on trunk than on 1.1 because of CASSANDRA-4018. As far as my personal opinion is involved, I would be fine in saying that cqlsh describe command is broken with CQL3 until C* 1.2 (since CQL3 is beta until then anyway).",27/Jun/12 14:42;nickmbailey;My issue was more with the fact that it seems impossible to set any of those four properties on a cf via cql. ,"29/Jun/12 17:35;thepaul;This was actually intended behavior- since it's possible to create tables in cql2 which can't be duplicated in cql3, I wanted to still note the possibly-relevant options on certain tables even if the syntax wasn't valid cql3.

I've since changed my mind, however, since it is confusing and makes it harder to do automated tests on DESCRIBE output, so I've already made this change in a personal branch. I'll put that up here.","09/Jul/12 23:14;thepaul;Nick, can you try with the bin/cqlsh from my 4380 branch, here:

https://github.com/thepaul/cassandra/tree/4380

This should fix the cases where cql2-style DESCRIBE was used sometimes, even when the connection was cql3, but looking closer at your case, it might not actually fix that.",10/Jul/12 16:16;nickmbailey;Seems to work fine for me.,11/Jul/12 14:44;brandon.williams;Committed.,"10/Sep/12 14:30;mdione;what about the rest of the output? if I run «cqlsh --cqlversion=3», I expect that the output is CQLv3, but for a particular CF that was created with a CQLv2 command, I get:

CREATE TABLE logs (
  day timestamp PRIMARY KEY
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

which is only valid in CQLv2; with CQLv3 I get this message:

<stdin>:17:Bad Request: No definition found that is not part of the PRIMARY KEY","10/Sep/12 16:01;thepaul;Marcos- there isn't any way to recreate some CQL 2 tables in CQL 3. CQL 3 can model the same data, but the structure of the table needs to be different. Cqlsh does the best that it can here, but if you want a better guarantee of valid reconstruction of CQL 2 tables, use cqlsh in CQL 2 mode.",10/Sep/12 16:31;jbellis;Doesn't COMPACT STORAGE provide the ability to model thrift/cql2-style tables?,"10/Sep/12 16:46;thepaul;bq. Doesn't COMPACT STORAGE provide the ability to model thrift/cql2-style tables?

It does for some. It probably would in this case. It doesn't for the cases where there are multiple columns with metadata in addition to undefined/wide-row columns.

Maybe that's not a case worth worrying about, though. We could open a new ticket for recreating thrift/cql2-style tables in CQL 3 mode with COMPACT STORAGE and some made-up default column names like ""colname"" and ""value"" or something.","10/Sep/12 17:07;slebresne;bq. It doesn't for the cases where there are multiple columns with metadata in addition to undefined/wide-row columns

That's true on 1.1 but I'll note that for 1.2, provided you use the system.schema_columnfamilies table, it should (almost) always be possible to create a valid CQL3 definition from an existing CF (potentially, and almost always in the case of CF created from thrift/CQL2, with COMPACT STORAGE). The only thing that comes in mind that makes me say ""almost"" is the case where people have created on the thrift side a column_metadata on a CompositeType. In that last case we can create a CQL3 query to recreate the CF, but not the column_metadata. I think it's safe to say that we can ignore that.

bq. some made-up default column names

The Cassandra already does that (so that you can always access thrift CF from CQL3), see CFDefinition.java. It could be worth using the same made-up names here.","10/Sep/12 18:34;thepaul;bq. for 1.2, provided you use the system.schema_columnfamilies table, it should (almost) always be possible to create a valid CQL3 definition from an existing CF

Could you elaborate on how that should be done? Say I made the following in CQL2:

{noformat}
CREATE TABLE foo (t text PRIMARY KEY, f float, x uuid) WITH default_validation=timestamp;
{noformat}

If that can be recreated in CQL 3, then I've missed a development somewhere.

bq. The Cassandra already does that (so that you can always access thrift CF from CQL3), see CFDefinition.java. It could be worth using the same made-up names here.

That's good to know. We should probably make that new ticket then.","10/Sep/12 18:45;slebresne;bq. If that can be recreated in CQL 3, then I've missed a development somewhere

Good point, I think too much in term of CQL3 now I guess, forgot about those. I suppose we could generate a valid CQL3 definition that is ""compatible"" with that (the same thing without the default_validation_validation part), but it's unclear it's a better idea.","19/Sep/12 15:58;dyrby76;Using CQL3, how are you now expected to create a column family like:

CREATE TABLE UserProducts
(
  userId uuid PRIMARY KEY
);

Where I would then dynamically add the product ids in the column name for each product, and leaving the column value null for each. The type of the product id is uuid.

When I try to create this column family I am told: No definition found that is not part of the PRIMARY KEY

Any help is greatly appreciated.

It works with CQL2, so is it a bug that CQL3 cannot? 






","19/Sep/12 16:28;jbellis;CREATE TABLE UserProducts (
  user_id uuid,
  product_id uuid,
  PRIMARY KEY (user_id, product_id)
);

More details at http://www.datastax.com/dev/blog/schema-in-cassandra-1-1.

Let's take any further discussion on usage to the mailing list.  Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json error,CASSANDRA-4331,12560299,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,gang0713,gang0713,12/Jun/12 06:30,12/Mar/19 14:16,13/Mar/19 22:27,27/Jun/12 07:20,1.1.2,,,,,1,,,,,,,"/apache-cassandra-1.1.1/bin> ./sstable2json  /home/cassandra/data/pimda/CF_bookmark/pimda-CF_bookmark-hd-48-Data.db > test.json


ERROR 22:27:14,215 Error in ThreadPoolExecutor
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
	at org.apache.cassandra.db.marshal.UTF8Type.compare(UTF8Type.java:27)
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:89)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)
	at java.util.TreeMap.getEntry(TreeMap.java:328)
	at java.util.TreeMap.containsKey(TreeMap.java:209)
	at java.util.TreeSet.contains(TreeSet.java:217)
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:396)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:187)
	at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:225)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR 22:27:14,219 Error in ThreadPoolExecutor
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
	at org.apache.cassandra.db.marshal.UTF8Type.compare(UTF8Type.java:27)
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:89)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)",,,,,,,,,,,,,,,,,,,12/Jun/12 18:34;jbellis;4331.txt;https://issues.apache.org/jira/secure/attachment/12531862/4331.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-12 18:34:19.065,,,no_permission,,,,,,,,,,,,256067,,,Wed Jun 27 07:20:30 UTC 2012,,,,,,0|i0gunz:,96398,xedin,xedin,,,,,,,,,,12/Jun/12 18:34;jbellis;Same patch I attached to CASSANDRA-4289.,"12/Jun/12 18:35;jbellis;Note that as a workaround, deleting the key cache files for your index CFs should fix this.",18/Jun/12 11:35;xedin;Duplicates CASSANDRA-4289,"18/Jun/12 20:29;jbellis;Not a duplicate, 4289 was for a 1.2-only issue; this is against 1.1","18/Jun/12 20:31;xedin;Oh, I'm sorry, +1 then.",27/Jun/12 07:20;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hints compaction loop over same sstable,CASSANDRA-4435,12598461,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,11/Jul/12 21:48,12/Mar/19 14:16,13/Mar/19 22:27,19/Jul/12 16:53,1.2.0 beta 1,,,,,0,compaction,hintedhandoff,,,,,"Noticed the following while testing something else:

{noformat}
INFO 22:14:48,496 Completed flushing /var/lib/cassandra/data/system/hints/system-hints-ia-1-Data.db (109645 bytes) for commitlog position ReplayPosition(segmentId=9372773011543415, position=30358488)
 INFO 22:14:48,498 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-1-Data.db')]
 INFO 22:14:48,500 SSTables for user defined compaction are already being compacted.
 INFO 22:14:48,500 Finished hinted handoff of 16893 rows to endpoint /10.179.64.227
 INFO 22:14:48,658 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-2-Data.db,].  109,645 to 899 (~0% of original) bytes for 1 keys at 0.005392MB/s.  Time: 159ms.
 INFO 22:14:48,660 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-2-Data.db')]
 INFO 22:14:48,668 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-3-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.107169MB/s.  Time: 8ms.
 INFO 22:14:48,669 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-3-Data.db')]
 INFO 22:14:48,679 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-4-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.095261MB/s.  Time: 9ms.
 INFO 22:14:48,680 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-4-Data.db')]
 INFO 22:14:48,697 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-5-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.050433MB/s.  Time: 17ms.
 INFO 22:14:48,698 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-5-Data.db')]
 INFO 22:14:48,714 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-6-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,715 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-6-Data.db')]
 INFO 22:14:48,722 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-7-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,723 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-7-Data.db')]
 INFO 22:14:48,736 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-8-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.065950MB/s.  Time: 13ms.
 INFO 22:14:48,737 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-8-Data.db')]
 INFO 22:14:48,744 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-9-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,745 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-9-Data.db')]
 INFO 22:14:48,753 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-10-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,754 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-10-Data.db')]
 INFO 22:14:48,761 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-11-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,762 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-11-Data.db')]
 INFO 22:14:48,775 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-12-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.065950MB/s.  Time: 13ms.
 INFO 22:14:48,776 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-12-Data.db')]
 INFO 22:14:48,783 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-13-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,784 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-13-Data.db')]
 INFO 22:14:48,792 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-14-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,793 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-14-Data.db')]
 INFO 22:14:48,800 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-15-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,802 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-15-Data.db')]
 INFO 22:14:48,809 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-16-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,810 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-16-Data.db')]
 INFO 22:14:48,826 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-17-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,827 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-17-Data.db')]
 INFO 22:14:48,834 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-18-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,835 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-18-Data.db')]
 INFO 22:14:48,842 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-19-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,842 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-19-Data.db')]
 INFO 22:14:48,850 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-20-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,850 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-20-Data.db')]
 INFO 22:14:48,867 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-21-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,868 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-21-Data.db')]
 INFO 22:14:48,876 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-22-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,877 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-22-Data.db')]
 INFO 22:14:48,884 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-23-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,885 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-23-Data.db')]
 INFO 22:14:48,893 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-24-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.107169MB/s.  Time: 8ms.
 INFO 22:14:48,895 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-24-Data.db')]
 INFO 22:14:48,901 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-25-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,902 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-25-Data.db')]
 INFO 22:14:48,910 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-26-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,910 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-26-Data.db')]
 INFO 22:14:48,926 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-27-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.057157MB/s.  Time: 15ms.
 INFO 22:14:48,930 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-27-Data.db')]
 INFO 22:14:48,938 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-28-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.107169MB/s.  Time: 8ms.
 INFO 22:14:48,943 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-28-Data.db')]
 INFO 22:14:48,949 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-29-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,950 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-29-Data.db')]
 INFO 22:14:48,966 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-30-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,967 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-30-Data.db')]
 INFO 22:14:48,974 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-31-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,974 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-31-Data.db')]
 INFO 22:14:48,980 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-32-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,981 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-32-Data.db')]
 INFO 22:14:48,988 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-33-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,989 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-33-Data.db')]
 INFO 22:14:48,995 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-34-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.171471MB/s.  Time: 5ms.
 INFO 22:14:48,995 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-34-Data.db')]
 INFO 22:14:49,002 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-35-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.171471MB/s.  Time: 5ms.
 INFO 22:14:49,002 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-35-Data.db')]
{noformat}

After this, the loop stopped.  It also did not occur on another member which delivered hints, so it may not be easy to replicate.  I suspect something related to CASSANDRA-3442 caused this, though the odd thing is there shouldn't even be a tombstone left.",,,,,,,,,,,,,,,,,,,18/Jul/12 07:36;jbellis;4435-v2.txt;https://issues.apache.org/jira/secure/attachment/12536963/4435-v2.txt,18/Jul/12 15:22;yukim;4435-v3.txt;https://issues.apache.org/jira/secure/attachment/12537003/4435-v3.txt,17/Jul/12 15:28;yukim;4435.txt;https://issues.apache.org/jira/secure/attachment/12536829/4435.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-07-16 15:23:54.981,,,no_permission,,,,,,,,,,,,256156,,,Thu Jul 19 16:53:49 UTC 2012,,,,,,0|i0gvuf:,96589,jbellis,jbellis,,,,,,,,,,"16/Jul/12 15:23;yukim;So far I have not able to reproduce this(If you still have one of those SSTables, it may help me figure out easier).

But I noticed from the log and the HHOM code, HHOM tries user defined compaction to compact all SSTables at once after forcing memtable to flush.
Flushing triggers background compaction, so sometimes, it competes with user defined one. You may notice from the line in above log,

bq.  INFO 22:14:48,500 SSTables for user defined compaction are already being compacted.

shows this situation that SSTables are already marked for compaction when running user defined.

It may be not the right solution here, but since recursive compaction only occurs when doing background compaction, I think it is better to add an option not to submit background compaction after flushing and use it here to make sure you only run user defined compaction.","16/Jul/12 15:27;jbellis;That would actually make a lot of sense, since hints are otherwise append-only, so there's no real benefit from normal background compaction.",17/Jul/12 15:28;yukim;Patch to not submit background compaction after flush. HHOM uses this to only perform user defined compaction after delivering hints.,18/Jul/12 07:36;jbellis;Can we just use the existing isCompactionDisabled logic?  v2 attached,"18/Jul/12 15:22;yukim;Yes it works, except your syntax doesn't work. Attaching v3.",18/Jul/12 17:26;jbellis;+1,"19/Jul/12 16:53;yukim;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restarting a failed bootstrap instajoins the ring,CASSANDRA-4427,12597963,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,08/Jul/12 23:06,12/Mar/19 14:16,13/Mar/19 22:27,31/Jul/12 20:57,1.1.3,,,,,0,,,,,,,"I think when we made auto_bootstrap = true the default, we broke the check for the bootstrap flag, creating a dangerous situation.",,,,,,,,,,,,,,,,,,,31/Jul/12 17:48;jbellis;4427-4.txt;https://issues.apache.org/jira/secure/attachment/12538579/4427-4.txt,31/Jul/12 18:51;brandon.williams;4427-5.txt;https://issues.apache.org/jira/secure/attachment/12538597/4427-5.txt,13/Jul/12 18:14;brandon.williams;4427-v2.txt;https://issues.apache.org/jira/secure/attachment/12536426/4427-v2.txt,13/Jul/12 18:27;brandon.williams;4427-v3.txt;https://issues.apache.org/jira/secure/attachment/12536429/4427-v3.txt,12/Jul/12 19:48;brandon.williams;4427.txt;https://issues.apache.org/jira/secure/attachment/12536264/4427.txt,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-07-12 20:10:36.853,,,no_permission,,,,,,,,,,,,256148,,,Tue Jul 31 20:57:53 UTC 2012,,,,,,0|i0gvr3:,96574,slebresne,slebresne,,,,,,,,,,"12/Jul/12 19:20;brandon.williams;Patch to remove the check for non-system tables.  As far as I can tell, it makes no sense to have this condition, since you can partially stream some data during bootstrap, then die.  If you try again without clearing the machine, instajoin.

It looks like this condition was added as part of a fix for CASSANDRA-3219 in commit de829f17, but the check was inverted and later fixed in CASSANDRA-3285.  However, the reasoning behind having such a check at all is never explained and I can't see why it should be there.","12/Jul/12 19:48;brandon.williams;Updated to also remove strange seed check (if seed, then bootstrap?) which is impossible anyway due to previous check.","12/Jul/12 20:10;jbellis;Here's what we were trying to address there:

bq. Now there is a actual new problem with 1.0.0. That problem is that when you start an initial cluster, i.e, when in 0.8 you would start node with auto-boostrap=false, you do often end up starting nodes simultaneously. That is why older version were using random token when auto-bootstrap was false. This problem does need to be fix for 1.0.0 because that is a serious regression. However, my argument is that even though we now default to auto-boostrap=true, that doesn't mean that there is no difference between setting up the initial nodes of a cluster and the latter bootstrapping of nodes to add capacity to an existing cluster. Indeed, in 1.0.0 we decided to draw this line based on whether a schema had been created or not (we call the bootstrap() method based on that). Imho, this means that we have no boostrap option and the ""I have no schema"" is the old auto-boostrap=false. So we should use random token in that case and balanced one otherwise the same way we are doing it in 0.8.","12/Jul/12 21:30;brandon.williams;bq. Indeed, in 1.0.0 we decided to draw this line based on whether a schema had been created or not

This seems more dangerous than it was worth, since you can easily receive even partial schema within a couple of seconds, realize you made some sort of mistake (forgot to mount the data dir, etc) and restart it, possibly wrecking your production app.

(The seed check still seems strange regardless)","13/Jul/12 18:14;brandon.williams;v2 takes a different approach that Sylvain kindly suggested, and changes the bootstrap flag from a boolean to a 3-way value so that we can detect previous attempts that failed, and try again.",13/Jul/12 18:19;jbellis;I think you're misreading the original seed logic...  !(isBootstrapped || isSeed) expands to !isBootstrapped && !isSeed.  Still need that so that single-node clusters don't try to bootstrap.,"13/Jul/12 18:27;brandon.williams;Oh, I see now.  v3 restores the seed check.","16/Jul/12 21:06;jbellis;+1

nit: worth adding a comment to explain wtf all the clauses of that if statement are, so we don't have to dig through ticket history next time",16/Jul/12 21:19;brandon.williams;Committed with comments added.,"16/Jul/12 22:18;jbellis;Started trying to improve the comments and got stuck on the schema check: it's basically a no-op (except for the purposes of screwing up a partial bootstrap like this), since we perform the check before waiting for gossip to fill in the schema.

Simpler fix at https://github.com/jbellis/cassandra/tree/4427-4 to move the schema check into getBootstrapToken.","17/Jul/12 09:39;slebresne;I believe this simpler fix doesn't handle the case of boostrapping multiple nodes into an existing cluster. Namely, in that case, that will have a schema and so the node will have a system table by the time it checks for it and we'll end up picking the same token for multiple nodes.

Also, I think checking system tables existence is fairly fragile and I would prefer moving away from it. It is way too easy to screw that up by having something (anything) written to those system tables. Typically, I don't know if that fix works for multiple nodes started in a brand new cluster (with not all being seeds), because without careful checking I don't know if we can end up writing some info in the system tables before checking for getBootstrapToken.

Overall I do like the idea of registering that the bootstrap is in process, because on top of (I think) fixing the problem in a non-fragile way, it also allows us better reporting. Even outside of the problem of generating tokens, I think it is reassuring for a user that restart a node that failed to boostrap to have the software acknowledge that it understand and handle correctly the situation.","18/Jul/12 07:35;jbellis;bq. I believe this simpler fix doesn't handle the case of boostrapping multiple nodes into an existing cluster.

We've never tried to prevent this in the existing cluster case, except by saying ""thou shalt space bootstraps apart two minutes,"" because the only way to stop it is to drop the ""balanced"" token picking altogether.  Adding ""bootstrap in progress"" concept does nothing for this one way or the other.

bq. Namely, in that case, that will have a schema and so the node will have a system table by the time it checks for it and we'll end up picking the same token for multiple nodes.

This is exactly how it's supposed to work: if there's a schema, we use ""existing cluster mode"" and pick a token to divide the range of the heaviest node (and cross our fingers that the user is spacing things out enough between node additions).  If there's no schema, we use ""new cluster mode"" and pick a random token.

Let the record show that back in CASSANDRA-3219 I said this was confusing behavior and we should add explicit initial_token modes instead of trying to make it magical. :)
","18/Jul/12 08:44;slebresne;bq. Adding ""bootstrap in progress"" concept does nothing for this one way or the other.

You're right, brain fart, sorry.

Anyway, there is still one behavior that the patch changes, that is it will always boobstrap non seeds node, while previously the system table check was making sure we never bootstrapped a node in a new cluster, independently of whether it was a seed or not.

It is clearly not a bad idea when you start a new cluster to set all those nodes as seeds, but I just want to point out that the behavior is changed and I'm not sure everyone always set all of its initial node as seeds today. I'll also note that boostrapping some of the node in an initial cluster don't break anything, it just makes the node start much less quickly that they would otherwise.

I'm not sure how I feel about changing that behavior, especially in a minor release. The fact is that recording that bootstrap is in progress (along with the system table check) would allow to fix the instajoin while keeping the current behavior unchanged otherwise, and I do feel that recording the info is not a bad idea in itself, so that would have my preference. But that is not an extremely strong preference either.","18/Jul/12 11:51;brandon.williams;bq. The fact is that recording that bootstrap is in progress (along with the system table check) would allow to fix the instajoin while keeping the current behavior unchanged otherwise, and I do feel that recording the info is not a bad idea in itself, so that would have my preference.

I tend to agree that having an explicit, persisted flag feels a lot less fragile than the current logic, and being able to indicate a failure to the user seems like a good improvement.","23/Jul/12 16:06;jbellis;bq. there is still one behavior that the patch changes, that is it will always boobstrap non seeds node

You're right.  Okay, take five: https://github.com/jbellis/cassandra/tree/4427-5

4 patches here on top of Brandon's work.  The main ones are the 1st and 4th.  In the first, I remove the seed special case since it's a subset of the empty schema case.  (Unless you're Doing It Wrong and adding seed nodes directly to an active cluster, which always surprises people when it burns them.  So I say good riddance.)

The first also adds a 2-gossip-round sleep so that (always assuming seeds are set correctly) we eliminate the risk of thinking schema is empty incorrectly due to a race w/ gossip.  The fourth patch follows this up by making the schema check based on other peers' schema uuids instead of local data.  Which is unlikely to be a problem today, but is is still a race-y approach and the correct alternative was straightforward.","27/Jul/12 10:06;slebresne;In the check for bootstrap:
{noformat}
if (DatabaseDescriptor.isAutoBootstrap()
    && (SystemTable.bootstrapInProgress() || (!SystemTable.bootstrapComplete() && !schemaPresent)))
{noformat}
I believe the schemaPresent condition shouldn't be negated. We want to skip boostrap is there is no schema, but bootstrap if there is one.

Even with that fixed, this breaks some of the unit tests (BoostrapperTest, EmbeddedCassandraServiceTest, StreamingTransferTest and AntiEntropyServiceStandardTest). Namely:
{noformat}
junit] java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
junit] 	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:127)
junit] 	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:109)
junit] 	at org.apache.cassandra.dht.BootStrapper.getBootstrapToken(BootStrapper.java:104)
junit] 	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:629)
junit] 	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:526)
junit] 	at org.apache.cassandra.dht.BootStrapperTest.testTokenRoundtrip(BootStrapperTest.java:50)
{noformat}

On committing to 1.0, I'm not sure what was the intention, but this feels a bit bigger than what I'm plainly confortable pushing in 1.0 at this point, and it feels we can tell people on 1.0 to wipe the data dir on a failed boostrap before retrying. That's not a strong opposition though, more an opinion.

Nits:
* Instead of calculateEmptySchema(), I would have put the initialization fo Schema.emptyVersion in a static block to make it explicit that it's a one time initialization. Though if you made that on purpose because you don't like static blocks, that's good enough for me.
* We log when we detect a boostrap failure, but it could be nice to also log whether we're going to boostrap or not and why in the other case.
","27/Jul/12 14:48;slebresne;I suspect the test failures are due to the removal of the seeds special case and because our tests are not fully realistic. Namely, in the tests, while localhost is a seed, it gets a schema loaded before joinTokenRing is called, and so it ends up with schemaPresent = true and tries to bootstrap (even though it's the only node). That shouldn't happen in real life but at least on the short term fixing the tests themselves is more work than is worth it, so maybe we can:
* Either we back the isSeed test
* Or exclude ourselves when we check for schemaPresent

Some Preference?","27/Jul/12 22:02;jbellis;bq. I believe the schemaPresent condition shouldn't be negated

Right, fix pushed to same github branch.

bq. I would have put the initialization fo Schema.emptyVersion in a static block to make it explicit that it's a one time initialization

I thought you couldn't declare emptyVersion final that way...  I was wrong, the compiler is smart enough to recognize the static block.  Also fixed.

bq. it could be nice to also log whether we're going to boostrap or not and why in the other case.

Added a debug line.

bq. exclude ourselves when we check for schemaPresent

Done.  (Since we can't have one ourselves unless another does too -- or unless we already joined the ring successfully -- there is no loss of correctness.)

bq. this feels a bit bigger than what I'm plainly confortable pushing in 1.0 at this point

+1, let's leave it as a known issue in 1.0.","28/Jul/12 09:54;slebresne;lgtm, +1","29/Jul/12 14:59;brandon.williams;This doesn't quite work, because we're looking for the SCHEMA app state, which at startup won't always exist:

{noformat}
ERROR [main] 2012-07-29 01:08:28,476 CassandraDaemon.java (line 335) Exception encountered during startup
java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:527)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:475)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:366)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:228)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:318)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:361)
{noformat}
","29/Jul/12 16:24;brandon.williams;Note: that was with autobootstrap disabled.  But, I'm also not convinced that waiting two gossiper rounds is sufficient either (alert the ring_delay police!)

It's possible that you could have 3 seeds and all but one could be down, thus 2 gossip rounds doesn't guarantee you'll have any appstates.","29/Jul/12 17:27;jbellis;bq. This doesn't quite work, because we're looking for the SCHEMA app state, which at startup won't always exist

Added a quick fix for this case.  If the cluster is so new that there is no SCHEMA state, then there's no actual schema info either.

bq. It's possible that you could have 3 seeds and all but one could be down, thus 2 gossip rounds doesn't guarantee you'll have any appstates

Granted, but surely two rounds is a better measure than the zero we had before.  (Which apparently worked most of the time...)  Remember, our goal is to avoid the full RING_DELAY sleep when we don't need to bootstrap.","29/Jul/12 17:39;brandon.williams;bq. Added a quick fix for this case. If the cluster is so new that there is no SCHEMA state, then there's no actual schema info either.

LGTM.

bq. Granted, but surely two rounds is a better measure than the zero we had before. (Which apparently worked most of the time...) Remember, our goal is to avoid the full RING_DELAY sleep when we don't need to bootstrap.

I know.  It's a situation with no perfect solution unfortunately (but I agree 2 > 0 ;)","31/Jul/12 16:13;brandon.williams;Well, bad news, something here is still broken:

{noformat}
DEBUG [main] 2012-07-31 10:52:40,564 MigrationManager.java (line 240) Gossiping my schema version 59adb24e-f3cd-3e02-97f0-5b395827453f
...
DEBUG [main] 2012-07-31 10:52:42,584 StorageService.java (line 591) Bootstrap variables: true false false false
{noformat}

and it joins the ring, where it should have bootstrapped.  I'm not sure why the schema check is failing, but it's causing problems for the dtests and happens in a very reproducible manner.","31/Jul/12 16:35;jbellis;59adb24e-f3cd-3e02-97f0-5b395827453f is emptyVersion, so from that snippet it looks like it's working as designed.","31/Jul/12 16:56;brandon.williams;Here's the real problem:

{noformat}

 INFO 16:49:57,531 Starting up server gossip
 INFO 16:49:57,547 Enqueuing flush of Memtable-LocationInfo@1547338589(126/157 serialized/live bytes, 3 ops)
 INFO 16:49:57,548 Writing Memtable-LocationInfo@1547338589(126/157 serialized/live bytes, 3 ops)
 INFO 16:49:57,586 Completed flushing /var/lib/cassandra/data/system/LocationInfo/system-LocationInfo-he-1-Data.db (234 bytes) for commitlog position ReplayPosition(segmentId=10938112371080118, position=595)
 INFO 16:49:57,616 Starting Messaging Service on port 7000
 INFO 16:49:59,634 Saved token not found. Using 113427455640312821154458202477256070484 from configuration
 INFO 16:49:59,636 Enqueuing flush of Memtable-LocationInfo@1088940267(53/66 serialized/live bytes, 2 ops)
 INFO 16:49:59,636 Writing Memtable-LocationInfo@1088940267(53/66 serialized/live bytes, 2 ops)
 INFO 16:49:59,652 Completed flushing /var/lib/cassandra/data/system/LocationInfo/system-LocationInfo-he-2-Data.db (163 bytes) for commitlog position ReplayPosition(segmentId=10938112371080118, position=776)
 INFO 16:49:59,655 Node cassandra-3/10.179.111.137 state jump to normal
 INFO 16:49:59,656 Bootstrap/Replace/Move completed! Now serving reads.
 INFO 16:49:59,690 Binding thrift service to cassandra-3/10.179.111.137:9160
 INFO 16:49:59,694 Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO 16:49:59,698 Using synchronous/threadpool thrift server on cassandra-3/10.179.111.137 : 9160
 INFO 16:49:59,699 Listening for thrift clients...
 INFO 16:49:59,873 Node /10.179.64.227 is now part of the cluster
 INFO 16:49:59,874 InetAddress /10.179.64.227 is now UP
 INFO 16:49:59,876 Enqueuing flush of Memtable-LocationInfo@1301257077(35/43 serialized/live bytes, 1 ops)
 INFO 16:49:59,877 Writing Memtable-LocationInfo@1301257077(35/43 serialized/live bytes, 1 ops)
 INFO 16:49:59,892 Completed flushing /var/lib/cassandra/data/system/LocationInfo/system-LocationInfo-he-3-Data.db (89 bytes) for commitlog position ReplayPosition(segmentId=10938112371080118, position=874)
 INFO 16:49:59,894 Node /10.179.65.102 is now part of the cluster
 INFO 16:49:59,894 InetAddress /10.179.65.102 is now UP
{noformat}

Gossip hasn't quite discovered any other nodes yet when the schema check fires.",31/Jul/12 17:48;jbellis;Patch attached to add back seed logic and move the schema check into getBootstrapToken.,31/Jul/12 18:25;brandon.williams;+1,"31/Jul/12 18:42;brandon.williams;We can save an appreciable amount of time by checking for schema during the delay, and then short circuiting to the isReadyForBootstrap check.","31/Jul/12 20:01;jbellis;Don't you still want the full ring delay to make sure you know about everyone in the cluster (so if you are picking a ""balanced"" token it does the Right Thing)?","31/Jul/12 20:04;brandon.williams;bq. Don't you still want the full ring delay to make sure you know about everyone in the cluster (so if you are picking a ""balanced"" token it does the Right Thing)?

Well, if we got any non-empty schema, a full gossip round has occurred so we should be good to go at that point, since it will have also populated our knowledge of the ring.","31/Jul/12 20:22;jbellis;+1 then.

nit: i'd also change sleep(delay) in the MigrationManager loop to sleep(1000), or even sleep(100)","31/Jul/12 20:57;brandon.williams;Committed w/nit fixed to sleep(1000), since if it's not complete logging every 100ms would be a bit annoying.",,,,,,,,,,,,,,,,,,,,,,,,,,
Possible schema corruption with cql 3.0,CASSANDRA-4420,12597764,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,bertpassek,bertpassek,06/Jul/12 12:53,12/Mar/19 14:16,13/Mar/19 22:27,08/Jul/12 11:11,1.1.3,,,,,0,,,,,,,"Hi,

i've got some problems while creating schemas with cql 3.0. After that i can't even start cassandra anymore.

Following steps for reproduction were done on a new installation of cassandra:

1. simply create a keyspace test via ""cqlsh -3""

create keyspace test with strategy_class = 'SimpleStrategy' and strategy_options:replication_factor = 1;

2. add cf with composite columns via ""cqlsh -3""

create table test1 (
    a int,
    b int,
    c int,
    d int,
    primary key (a, b, c)
);

3. drop column family 

drop columnfamily test1;

So until now everything went fine. Now i'm trying to insert a slightly modified column family with the same name above.

4. create new cf via ""cqlsh -3""

create table test1 (
    a int,
    b int,
    c int,
    primary key (a, b)
);

This creation fails with following exception:


java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.cassandra.config.CFMetaData.getColumnDefinitionComparator(CFMetaData.java:1280)
        at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
        at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
        at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
        at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
        at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
        at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)


Now at this point you can't do anything anymore via cql or cli. Shutting down and starting cassandra again throws same exceptions:


ERROR 14:48:41,705 Exception encountered during startup
java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.config.CFMetaData.getColumnDefinitionComparator(CFMetaData.java:1280)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
	at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
java.lang.IndexOutOfBoundsException: Index: 2, Size: 2Exception encountered during startup: Index: 2, Size: 2

	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.config.CFMetaData.getColumnDefinitionComparator(CFMetaData.java:1280)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
	at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)


Actually it's the result of a slightly different problem in combination with composite columns, but i will describe this later.

I've got no idea, what the problem is, there might be some corruption in table schemas, even after dropping tables.

I have to delete cassandra data in order to get cassandra running again.

Best Regards 

Bert Passek",Lenny Squeeze,,,,,,,,,,,,,,,,,,07/Jul/12 15:58;xedin;CASSANDRA-4420.patch;https://issues.apache.org/jira/secure/attachment/12535534/CASSANDRA-4420.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-07 15:58:40.731,,,no_permission,,,,,,,,,,,,256142,,,Sun Jul 08 11:11:43 UTC 2012,,,,,,0|i0gvo7:,96561,jbellis,jbellis,,,,,,,,,,"07/Jul/12 15:58;xedin;DROP TABLE wasn't deleting ""component_index"" column.",08/Jul/12 02:46;jbellis;+1,08/Jul/12 11:11;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counters in columns don't preserve correct values after cluster restart,CASSANDRA-4436,12598552,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,pvelas,pvelas,12/Jul/12 13:32,12/Mar/19 14:16,13/Mar/19 22:27,26/Jul/12 16:07,1.1.3,,,,,0,,,,,,,"Similar to #3821. but affecting normal columns. 


Set up a 2-node cluster with rf=2.
1. Create a counter column family and increment a 100 keys in loop 5000 times. 
2. Then make a rolling restart to cluster. 
3. Again increment another 5000 times.
4. Make a rolling restart to cluster.
5. Again increment another 5000 times.
6. Make a rolling restart to cluster.


After step 6 we were able to reproduce bug with bad counter values. 
Expected values were 15 000. Values returned from cluster are higher then 15000 + some random number.
Rolling restarts are done with nodetool drain. Always waiting until second node discover its down then kill java process. ",,,,,,,,,,,,,,,,,,,26/Jul/12 07:22;slebresne;4436-1.0-2.txt;https://issues.apache.org/jira/secure/attachment/12537966/4436-1.0-2.txt,24/Jul/12 08:59;slebresne;4436-1.0-2.txt;https://issues.apache.org/jira/secure/attachment/12537669/4436-1.0-2.txt,18/Jul/12 18:31;slebresne;4436-1.0.txt;https://issues.apache.org/jira/secure/attachment/12537037/4436-1.0.txt,26/Jul/12 07:22;slebresne;4436-1.1-2.txt;https://issues.apache.org/jira/secure/attachment/12537967/4436-1.1-2.txt,24/Jul/12 08:59;slebresne;4436-1.1-2.txt;https://issues.apache.org/jira/secure/attachment/12537670/4436-1.1-2.txt,18/Jul/12 18:31;slebresne;4436-1.1.txt;https://issues.apache.org/jira/secure/attachment/12537038/4436-1.1.txt,17/Jul/12 15:14;pvelas;increments.cql.gz;https://issues.apache.org/jira/secure/attachment/12536828/increments.cql.gz,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2012-07-13 17:15:19.992,,,no_permission,,,,,,,,,,,,245636,,,Fri Aug 03 23:07:20 UTC 2012,,,,,,0|i06cbj:,34930,jbellis,jbellis,,,,,,,,,,13/Jul/12 17:15;slebresne;Can you reproduce every time with those steps? I tried reproducing with those exact steps (as far as I can tell) a few times on both 1.0 and 1.1 (the counter code didn't change much between 1.0 and 1.1) and wasn't able to reproduce.,"14/Jul/12 07:55;pvelas;

create keyspace test_old
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = {replication_factor : 2}
  and durable_writes = true;

use test_old;

create column family cf1_increment
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'CounterColumnType'
  and key_validation_class = 'BytesType'
  and read_repair_chance = 1.0
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.SnappyCompressor'};



In version 1.0.10 am always able to reproduce with this steps.. but its not reproducible in 1.1.2 .

When I stop writing and shutdown node with ""nodetool drain"" there are some small commitlog files, but I don't bother to delete them just restart cassandra process. Maybe this is case ?","17/Jul/12 10:58;slebresne;The only difference I could see with the test I ran previously was the use of compression. So while I strongly doubt compression can have anything to do with that in any way, I rerun the test against 1.0 a bunch of time but I was still not able to reproduce any error.

Since you seem to be able to reproduce easily, would you mind sharing the scripts you use to reproduce? I.e. mainly the code you use for insertion, preferably in plain thrift or CQL2 as this would eliminate the possibility of a client library bug.","17/Jul/12 15:40;pvelas;You are right its not affected by compression.
I was just curious if its problem with our python code using pycassa ... 
So I created increments.cql containing 100k lines with 1000 increments for each of 100 key values.
{code}
cassandra-cli -h $HOSTNAME -p 9160 -f increments.cql -B >/dev/null 
{code}

after 3 rolling restarts each value was correct with value 3000 
after 4 rolling restart values are incorrect see bellow

{code}
col1	5479
col10	5507
col100	5531
col11	5480
col12	5501
col13	5499
col14	5516
{code}

Its 2 node cluster with replication=2. 




{code}
[root@cass-bug1 ~]# /opt/apache-cassandra-1.0.10/bin/cassandra-cli -h $HOSTNAME -p 9160 -f increments.cql -B >/dev/null 
[root@cass-bug1 ~]# /opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME drain

[root@cass-bug2 ~]# /opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME ring
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
10.20.30.160    datacenter1 rack1       Down   Normal  97.67 KB        50.00%  0                                           
10.20.30.161    datacenter1 rack1       Up     Normal  113.45 KB       50.00%  85070591730234615865843651857942052864  

[root@cass-bug1 ~]# killall java
[root@cass-bug1 ~]# /opt/apache-cassandra-1.0.10/bin/cassandra

[root@cass-bug2 ~]# /opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME drain

[root@cass-bug1 ~]# /opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME ring
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
10.20.30.160    datacenter1 rack1       Up     Normal  97.67 KB        50.00%  0                                           
10.20.30.161    datacenter1 rack1       Down   Normal  86.13 KB        50.00%  85070591730234615865843651857942052864 

[root@cass-bug2 ~]# killall java
[root@cass-bug2 ~]# /opt/apache-cassandra-1.0.10/bin/cassandra


{code}



Here is dump of keyspace and CF 


{code}
create keyspace inc_test
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = {replication_factor : 2}
  and durable_writes = true;

use inc_test;

create column family cf1_increment
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'CounterColumnType'
  and key_validation_class = 'BytesType'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and row_cache_keys_to_save = 2147483647
  and keys_cached = 200000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'SerializingCacheProvider'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy';
{code}


Hope that helps you reproduce ..",17/Jul/12 15:41;pvelas;Increment for batch loading through cassandra-cli.,"18/Jul/12 18:31;slebresne;Thanks a lot Peter for helping out reproducing this issue.

The problem is that when a node stops (or is drained for that matter, we don't wait for all compaction to end during drain as this could mean waiting for a very long time, at least with SizeTieredCompaction) just when a compaction is finishing, it is possible for some of the compacted file to not have -Compacted components even if the compacted file is not temporary anymore. In other words, it is possible that when the node is restart, it will load both the compacted files and some of the file used to compact it. While this is harmless (though inefficient) for normal column family, this means overcounting for counters.

I'll note that even though I can't reproduce the counter bug on 1.1 with the test case above, it is just ""luck"" as 1.1 is affected as well.

What we need to guarantee is that we will never use both a compacted file and one of it's ancestor. One way to ensure that is to keep in the metadata of the compacted file, the list of it's ancestors (we only need to keep the generation). Then when a node start, it can gather all the ancestors of all the sstable in the data dir, and delete all those sstable that are in this ancestor set. Since we don't want to keep ever going list of ancestors however, a newly compacted sstable only need to keep the list of it's still live ancestor (which 99% of the time means keeping only the generation of the file that were compacted to obtain it). I note that if we do that, we don't need to generate -Compacted components.

Attaching patch to implement this. Attaching a patch for 1.0 and 1.1 (which aren't very different). I wrote the 1.0 version because it's on this version that I knew how to reproduce the counter bug reliably, and I've checked that this patch does fix the issue. However, this patch doesn't only affect counter code and is not trivial per se, so I don't know how I feel about risking to breaking things on 1.0 for non-counter user at this point. I think it might me wiser to put this in 1.1.3 only and say that counter users should either apply the attached patch at their own risk or upgrade to 1.1.3.
",20/Jul/12 10:02;pvelas;Thanks for your interest and time to fix it. We currently move to 1.1.2 version to avoid some random aws failure and patiently waiting for 1.1.3 release. ,"23/Jul/12 20:12;jbellis;Looks like skipCompacted in Directories.SSTableLister can be removed (since we scrubDataDirectories on startup and no new compacted components will be created).

Using a List means we can add an ancestor multiple times.  Suggest using a Set instead.

Nits:
- would prefer Ancestor to LiveAncestor, since we only check liveness at creation time, so ""Live"" is misleading when iterating over them later.
- the deleting code feels more at home in CFS constructor than addInitialSSTables.
- tracker parameter is unused now in SSTR.open","24/Jul/12 08:59;slebresne;bq. Looks like skipCompacted in Directories.SSTableLister can be removed (since we scrubDataDirectories on startup and no new compacted components will be created).

True, though there is the (arguably remote) possibility that people call loadNewSSTables() (or the offline scrub from CASSANDRA-4441) on sstables having some -Compacted components. So I would prefer leaving it in 1.1 and removing it during the merge to trunk, just to be sure minor upgrade are as little disrupting as can be.

bq. Using a List means we can add an ancestor multiple times. Suggest using a Set instead.

But we won't have the same ancestor multiple times. Otherwise that would be a bug (and at least for counters, a particularly bad one). But for sanity I've added an assertion to check this doesn't happen (I've a list however, I figured that since the list will be small, the difference between List.contains() and Set.contains() will be negligeable, and it's checked in an assertion and only once a the sstable creation. On the other Lists have a smaller memory footprint. Though I admit in either case we're talked minor differences).

bq. would prefer Ancestor to LiveAncestor, since we only check liveness at creation time, so ""Live"" is misleading when iterating over them later.

Renamed.

bq. the deleting code feels more at home in CFS constructor than addInitialSSTables.

Moved.

bq. tracker parameter is unused now in SSTR.open

Removed. I realized that setTrackedBy was already always call through the DataTracker.addNewSSTablesSize, so I also removed the call duplication.
","25/Jul/12 19:17;jbellis;bq. But we won't have the same ancestor multiple times

I don't think that's true.  Suppose for instance we have leveled compaction with A and B in L0.  They are larger than 5MB so we split the result into X, Y, and Z.  Next we flush C to L0.  It overlaps with Y and Z, so we're compacting C, Y, and Z.  Now we have Y and Z both with A and B as ancestors.

(Switching from LCS back to STCS is another way you could get duplicate ancestors.)","26/Jul/12 07:22;slebresne;You're completely right, I'm still thinking too much in terms of SizeTieredCompaction.

Updated patches to use a Set.",26/Jul/12 14:34;jbellis;+1,"26/Jul/12 16:07;slebresne;Committed (to >= 1.1 as per my earlier comment), thanks.

I've also removed Directories.skipCompacted() while merging to trunk.","03/Aug/12 23:07;hudson;Integrated in Cassandra #1861 (See [https://builds.apache.org/job/Cassandra/1861/])
    Fix ScrubTest after file format change in CASSANDRA-4436 (Revision a075385d05c3e1d26475f448363958bad4645f17)

     Result = ABORTED
yukim : 
Files : 
* test/data/corrupt-sstables/Keyspace1-Standard3-ia-1-Statistics.db
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig driver casts ints as bytearray,CASSANDRA-4459,12599883,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,cdaw,cdaw,23/Jul/12 18:51,12/Mar/19 14:16,13/Mar/19 22:27,25/Jul/12 17:09,1.1.3,,,,,0,,,,,,,"we seem to be auto-mapping C* int columns to bytearray in Pig, and farther down I can't seem to find a way to cast that to int and do an average.  

{code}

grunt> cassandra_users = LOAD 'cassandra://cqldb/users' USING CassandraStorage();
grunt> dump cassandra_users;
(bobhatter,(act,22),(fname,bob),(gender,m),(highSchool,Cal High),(lname,hatter),(sat,500),(state,CA),{})
(alicesmith,(act,27),(fname,alice),(gender,f),(highSchool,Tuscon High),(lname,smith),(sat,650),(state,AZ),{})
 
// notice sat and act columns are bytearray values 
grunt> describe cassandra_users;
cassandra_users: {key: chararray,act: (name: chararray,value: bytearray),fname: (name: chararray,value: chararray),
gender: (name: chararray,value: chararray),highSchool: (name: chararray,value: chararray),lname: (name: chararray,value: chararray),
sat: (name: chararray,value: bytearray),state: (name: chararray,value: chararray),columns: {(name: chararray,value: chararray)}}

grunt> users_by_state = GROUP cassandra_users BY state;
grunt> dump users_by_state;
((state,AX),{(aoakley,(highSchool,Phoenix High),(lname,Oakley),state,(act,22),(sat,500),(gender,m),(fname,Anne),{})})
((state,AZ),{(gjames,(highSchool,Tuscon High),(lname,James),state,(act,24),(sat,650),(gender,f),(fname,Geronomo),{})})
((state,CA),{(philton,(highSchool,Beverly High),(lname,Hilton),state,(act,37),(sat,220),(gender,m),(fname,Paris),{}),(jbrown,(highSchool,Cal High),(lname,Brown),state,(act,20),(sat,700),(gender,m),(fname,Jerry),{})})

// Error - use explicit cast
grunt> user_avg = FOREACH users_by_state GENERATE cassandra_users.state, AVG(cassandra_users.sat);
grunt> dump user_avg;
2012-07-22 17:15:04,361 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.AVG as multiple or none of them fit. Please use an explicit cast.

// Unable to cast as int
grunt> user_avg = FOREACH users_by_state GENERATE cassandra_users.state, AVG((int)cassandra_users.sat);
grunt> dump user_avg;
2012-07-22 17:07:39,217 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1052: Cannot cast bag with schema sat: bag({name: chararray,value: bytearray}) to int
{code}

*Seed data in CQL*
{code}
CREATE KEYSPACE cqldb with 
  strategy_class = 'org.apache.cassandra.locator.SimpleStrategy' 
  and strategy_options:replication_factor=3;	


use cqldb;

CREATE COLUMNFAMILY users (
  KEY text PRIMARY KEY, 
  fname text, lname text, gender varchar, 
  act int, sat int, highSchool text, state varchar);

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (gjames, Geronomo, James, f, 24, 650, 'Tuscon High', 'AZ');

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (aoakley, Anne, Oakley, m , 22, 500, 'Phoenix High', 'AX');

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (jbrown, Jerry, Brown, m , 20, 700, 'Cal High', 'CA');

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (philton, Paris, Hilton, m , 37, 220, 'Beverly High', 'CA');

select * from users;
{code}",C* 1.1.2 embedded in DSE,,,,,,,,,,,,,,,,,,25/Jul/12 16:16;brandon.williams;4459-v2.txt;https://issues.apache.org/jira/secure/attachment/12537848/4459-v2.txt,23/Jul/12 19:19;brandon.williams;4459.txt;https://issues.apache.org/jira/secure/attachment/12537596/4459.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-07-23 19:19:29.719,,,no_permission,,,,,,,,,,,,256175,,,Wed Jul 25 17:09:56 UTC 2012,,,,,,0|i0gw3z:,96632,xedin,xedin,,,,,,,,,,"23/Jul/12 19:19;brandon.williams;We actually have tests in examples/pig/test that explicitly cover this, but we populate that data with the cli which uses IntegerType, but cql uses Int32Type.  Trivial patch to cast both to pig's integer type.",23/Jul/12 19:40;jbellis;casting IntegerType to pig's [32bit] int sounds broken to me.  shouldn't we fix the population script to use int32 as well?,"24/Jul/12 11:41;xedin;I agree with Jonathan on this, IntegerType could be larger than int32.","24/Jul/12 16:04;jeromatron;fwiw - see https://issues.apache.org/jira/browse/PIG-2764 for the addition of BigInteger and BigDecimal as built-in pig data types.  Also, I'm not sure how much of an issue it is for users to use pig ints for now because I don't know how many users know that the cassandra IntegerType is actually a BigInteger and not just a regular Integer.  That's not to say that it's not dangerous to try to put a BigInteger value into an Integer type.  It's just that I don't know if it's common knowledge that Cassandra uses a BigInteger underneath.","24/Jul/12 16:10;brandon.williams;bq. casting IntegerType to pig's [32bit] int sounds broken to me.

I agree, but the conundrum we're in now is, I'm almost certain someone is relying on the current behavior to work and always using ints under 2**31, so changing it now would break things for them, and really the only danger is exceeding that limit, which I suspect people who create int columns never intend to do (or they'd make them longs.)

So, I propose that when pig has a BigInteger, we switch to that, allowing a smooth transition (unless you're exceeding 2**31 already, which to my knowledge no one is.)","24/Jul/12 16:12;jbellis;That's reasonable, but we should still use int32type for cli population :)","25/Jul/12 16:16;brandon.williams;Update with a comment explaining that IntegerType is wrong, but we're doing it anyway.  Also switched all the IntegerTypes to Int32Types in the tests, which pass.  I don't see any point in explicitly testing IntegerType as well until pig has a BigInteger.",25/Jul/12 16:40;jbellis;+1,25/Jul/12 16:43;xedin;+1,25/Jul/12 17:09;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion with LCS compaction,CASSANDRA-4411,12597524,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,awinter,awinter,05/Jul/12 04:52,12/Mar/19 14:16,13/Mar/19 22:27,16/Jul/12 15:33,1.1.3,1.1.4,,,,2,lcs,,,,,,"As instructed in CASSANDRA-4321 I have raised this issue as a continuation of that issue as it appears the problem still exists.

I have repeatedly run sstablescrub across all my nodes after the 1.1.2 upgrade until sstablescrub shows no errors.  The exceptions described in CASSANDRA-4321 do not occur as frequently now but the integrity check still throws exceptions on a number of nodes.  Once those exceptions occur compactionstats shows a large number of pending tasks with no progression afterwards.

{code}
ERROR [CompactionExecutor:150] 2012-07-05 04:26:15,570 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:150,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}
",,,,,,,,,,,,,,,,,,,13/Jul/12 13:17;slebresne;0001-Add-debugging-info-for-LCS.txt;https://issues.apache.org/jira/secure/attachment/12536382/0001-Add-debugging-info-for-LCS.txt,09/Aug/12 16:27;omid;0001-Fix-off-by-one-for-out-of-order-and-overlapping-ssta.patch;https://issues.apache.org/jira/secure/attachment/12540058/0001-Fix-off-by-one-for-out-of-order-and-overlapping-ssta.patch,16/Jul/12 15:15;slebresne;4411-followup.txt;https://issues.apache.org/jira/secure/attachment/12536645/4411-followup.txt,16/Jul/12 11:27;slebresne;4411.txt;https://issues.apache.org/jira/secure/attachment/12536617/4411.txt,15/Jul/12 13:51;omid;assertion-w-more-debugging-info-omid.log;https://issues.apache.org/jira/secure/attachment/12536552/assertion-w-more-debugging-info-omid.log,14/Jul/12 15:04;rvanderleeden;assertion.moreinfo.system.log;https://issues.apache.org/jira/secure/attachment/12536509/assertion.moreinfo.system.log,11/Jul/12 12:44;schnidrig;system.log;https://issues.apache.org/jira/secure/attachment/12536021/system.log,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2012-07-10 06:24:17.435,,,no_permission,,,,,,,,,,,,249306,,,Tue Aug 28 17:09:49 UTC 2012,,,,,,0|i0a6qn:,57365,jbellis,jbellis,,,,,,,,,,10/Jul/12 06:24;schnidrig;I got the same AssertionError on a 1.1.2 version cluster which I did not upgrade from an earlier version.,"10/Jul/12 14:10;slebresne;Would one of your guys have the log leading to that exception? If you have it at DEBUG, even better.","11/Jul/12 11:42;rvanderleeden;I could reproduce the problem on a 3-node testcluster with 1.1.2 and LCS.
Replication factor is 3 and number of total keys is 24m.
I added SSTables from a previous backup to node1.
Then running on node1:  nodetool repair -pr 
Result: 

 INFO [CompactionExecutor:7] 2012-07-11 10:06:57,632 CompactionTask.java (line 109) Compacting [SSTableReader(path='/mnt/cassandra/data/highscores/highscore/highscores-highscore-hd-4937-Data.db')]
 INFO [CompactionExecutor:7] 2012-07-11 10:06:58,601 CompactionTask.java (line 221) Compacted to [/mnt/cassandra/data/highscores/highscore/highscores-highscore-hd-5591-Data.db,].  5,252,617 to 5,252,617 (~100% of original) bytes for 51,419 keys at 5.174882MB/s.  Time: 968ms.
 INFO [CompactionExecutor:6] 2012-07-11 10:06:58,602 CompactionTask.java (line 109) Compacting [SSTableReader(path='/mnt/cassandra/data/highscores/highscore/highscores-highscore-hd-5590-Data.db'), SSTableReader(path='/mnt/cassandra/data/highscores/highscore/highscores-highscore-hd-5571-Data.db')]
ERROR [CompactionExecutor:6] 2012-07-11 10:06:59,655 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:6,1,main]
java.lang.AssertionError
	at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)


The next repair command throws the following assertion:

ERROR [ValidationExecutor:2] 2012-07-11 10:31:28,020 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.AssertionError: row DecoratedKey(162957119114255422766928006879345246467, c9e91cfb77634f32b9399dd4ad6b784e93dec9d0b11f431dad58a35e9f623de9) received out of order wrt DecoratedKey(165755005851296361665897424577644629314, ac63200da3fb452ca0b57a648b90c8a427a3d45b2d2146e089c6d04b959bb207)
	at org.apache.cassandra.service.AntiEntropyService$Validator.add(AntiEntropyService.java:349)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:712)
         ...etc...

Let me know if you need more from the log.
Thanks, -Rudolf.


","11/Jul/12 12:44;schnidrig;Didn't have the old log, but I created a new keyspace and started using it until I hit the error. See attached file.

","11/Jul/12 15:03;omid;I could also reproduce it from the data I had:

{code}
DEBUG 12:47:37,353 adding /home/omid/data/KSP/CF/KSP-CF-hd-121136 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121137 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121138 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121139 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121140 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121141 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121142 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121143 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121144 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121145 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121146 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121147 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121148 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121149 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121150 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121151 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121152 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121153 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121154 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121155 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121156 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121157 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121158 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121159 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121160 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121161 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 GC for ParNew: 14 ms for 1 collections, 5438330152 used; max is 8506048512
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121162 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121163 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121164 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121165 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121166 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121167 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121168 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121169 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121170 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121171 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121172 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121173 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121174 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121175 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 removing /home/omid/data/KSP/CF/KSP-CF-hd-121068 from list of files tracked for KSP.CF
DEBUG 12:47:37,360 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121068-Data.db compacted
DEBUG 12:47:37,360 All segments have been unmapped successfully
DEBUG 12:47:37,360 removing /home/omid/data/KSP/CF/KSP-CF-hd-120844 from list of files tracked for KSP.CF
DEBUG 12:47:37,360 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120844-Data.db compacted
DEBUG 12:47:37,360 All segments have been unmapped successfully
DEBUG 12:47:37,360 removing /home/omid/data/KSP/CF/KSP-CF-hd-121051 from list of files tracked for KSP.CF
DEBUG 12:47:37,361 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121051-Data.db compacted
DEBUG 12:47:37,361 All segments have been unmapped successfully
DEBUG 12:47:37,361 removing /home/omid/data/KSP/CF/KSP-CF-hd-120830 from list of files tracked for KSP.CF
DEBUG 12:47:37,361 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120830-Data.db compacted
DEBUG 12:47:37,361 All segments have been unmapped successfully
DEBUG 12:47:37,361 removing /home/omid/data/KSP/CF/KSP-CF-hd-121050 from list of files tracked for KSP.CF
DEBUG 12:47:37,362 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121050-Data.db compacted
DEBUG 12:47:37,362 All segments have been unmapped successfully
DEBUG 12:47:37,362 removing /home/omid/data/KSP/CF/KSP-CF-hd-120843 from list of files tracked for KSP.CF
DEBUG 12:47:37,362 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120843-Data.db compacted
DEBUG 12:47:37,362 All segments have been unmapped successfully
DEBUG 12:47:37,362 removing /home/omid/data/KSP/CF/KSP-CF-hd-120847 from list of files tracked for KSP.CF
DEBUG 12:47:37,362 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120847-Data.db compacted
DEBUG 12:47:37,363 Deleting KSP-CF-hd-121068-CompressionInfo.db
DEBUG 12:47:37,363 All segments have been unmapped successfully
DEBUG 12:47:37,363 Deleting KSP-CF-hd-121068-Statistics.db
DEBUG 12:47:37,363 removing /home/omid/data/KSP/CF/KSP-CF-hd-120834 from list of files tracked for KSP.CF
DEBUG 12:47:37,363 Deleting KSP-CF-hd-121068-Index.db
DEBUG 12:47:37,363 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120834-Data.db compacted
DEBUG 12:47:37,363 Deleting KSP-CF-hd-121068-Filter.db
DEBUG 12:47:37,363 All segments have been unmapped successfully
DEBUG 12:47:37,363 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121068
DEBUG 12:47:37,364 removing /home/omid/data/KSP/CF/KSP-CF-hd-120840 from list of files tracked for KSP.CF
DEBUG 12:47:37,364 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120840-Data.db compacted
DEBUG 12:47:37,364 All segments have been unmapped successfully
DEBUG 12:47:37,364 removing /home/omid/data/KSP/CF/KSP-CF-hd-121047 from list of files tracked for KSP.CF
DEBUG 12:47:37,364 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121047-Data.db compacted
DEBUG 12:47:37,364 All segments have been unmapped successfully
DEBUG 12:47:37,364 removing /home/omid/data/KSP/CF/KSP-CF-hd-120816 from list of files tracked for KSP.CF
DEBUG 12:47:37,365 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120816-Data.db compacted
DEBUG 12:47:37,365 All segments have been unmapped successfully
DEBUG 12:47:37,365 removing /home/omid/data/KSP/CF/KSP-CF-hd-121133 from list of files tracked for KSP.CF
DEBUG 12:47:37,365 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121133-Data.db compacted
DEBUG 12:47:37,365 All segments have been unmapped successfully
DEBUG 12:47:37,365 removing /home/omid/data/KSP/CF/KSP-CF-hd-120821 from list of files tracked for KSP.CF
DEBUG 12:47:37,366 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120821-Data.db compacted
DEBUG 12:47:37,366 All segments have been unmapped successfully
DEBUG 12:47:37,366 Deleting KSP-CF-hd-120844-CompressionInfo.db
DEBUG 12:47:37,366 removing /home/omid/data/KSP/CF/KSP-CF-hd-120831 from list of files tracked for KSP.CF
DEBUG 12:47:37,366 Deleting KSP-CF-hd-120844-Statistics.db
DEBUG 12:47:37,366 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120831-Data.db compacted
DEBUG 12:47:37,366 Deleting KSP-CF-hd-120844-Index.db
DEBUG 12:47:37,366 All segments have been unmapped successfully
DEBUG 12:47:37,366 Deleting KSP-CF-hd-120844-Filter.db
DEBUG 12:47:37,367 removing /home/omid/data/KSP/CF/KSP-CF-hd-120856 from list of files tracked for KSP.CF
DEBUG 12:47:37,367 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120844
DEBUG 12:47:37,367 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120856-Data.db compacted
DEBUG 12:47:37,367 All segments have been unmapped successfully
DEBUG 12:47:37,367 removing /home/omid/data/KSP/CF/KSP-CF-hd-120842 from list of files tracked for KSP.CF
DEBUG 12:47:37,367 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120842-Data.db compacted
DEBUG 12:47:37,367 All segments have been unmapped successfully
DEBUG 12:47:37,368 removing /home/omid/data/KSP/CF/KSP-CF-hd-120849 from list of files tracked for KSP.CF
DEBUG 12:47:37,368 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120849-Data.db compacted
DEBUG 12:47:37,368 All segments have been unmapped successfully
DEBUG 12:47:37,368 Deleting KSP-CF-hd-121051-CompressionInfo.db
DEBUG 12:47:37,368 removing /home/omid/data/KSP/CF/KSP-CF-hd-120848 from list of files tracked for KSP.CF
DEBUG 12:47:37,368 Deleting KSP-CF-hd-121051-Statistics.db
DEBUG 12:47:37,368 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120848-Data.db compacted
DEBUG 12:47:37,368 Deleting KSP-CF-hd-121051-Index.db
DEBUG 12:47:37,369 All segments have been unmapped successfully
DEBUG 12:47:37,369 Deleting KSP-CF-hd-121051-Filter.db
DEBUG 12:47:37,369 removing /home/omid/data/KSP/CF/KSP-CF-hd-120851 from list of files tracked for KSP.CF
DEBUG 12:47:37,369 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121051
DEBUG 12:47:37,369 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120851-Data.db compacted
DEBUG 12:47:37,369 All segments have been unmapped successfully
DEBUG 12:47:37,369 removing /home/omid/data/KSP/CF/KSP-CF-hd-120855 from list of files tracked for KSP.CF
DEBUG 12:47:37,369 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120855-Data.db compacted
DEBUG 12:47:37,370 All segments have been unmapped successfully
DEBUG 12:47:37,370 removing /home/omid/data/KSP/CF/KSP-CF-hd-120815 from list of files tracked for KSP.CF
DEBUG 12:47:37,370 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120815-Data.db compacted
DEBUG 12:47:37,370 All segments have been unmapped successfully
DEBUG 12:47:37,370 removing /home/omid/data/KSP/CF/KSP-CF-hd-120836 from list of files tracked for KSP.CF
DEBUG 12:47:37,370 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120836-Data.db compacted
DEBUG 12:47:37,371 All segments have been unmapped successfully
DEBUG 12:47:37,371 removing /home/omid/data/KSP/CF/KSP-CF-hd-120841 from list of files tracked for KSP.CF
DEBUG 12:47:37,371 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120841-Data.db compacted
DEBUG 12:47:37,371 All segments have been unmapped successfully
DEBUG 12:47:37,371 removing /home/omid/data/KSP/CF/KSP-CF-hd-120814 from list of files tracked for KSP.CF
DEBUG 12:47:37,371 Deleting KSP-CF-hd-120830-CompressionInfo.db
DEBUG 12:47:37,371 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120814-Data.db compacted
DEBUG 12:47:37,371 Deleting KSP-CF-hd-120830-Statistics.db
DEBUG 12:47:37,372 All segments have been unmapped successfully
DEBUG 12:47:37,372 Deleting KSP-CF-hd-120830-Index.db
DEBUG 12:47:37,372 removing /home/omid/data/KSP/CF/KSP-CF-hd-120819 from list of files tracked for KSP.CF
DEBUG 12:47:37,372 Deleting KSP-CF-hd-120830-Filter.db
DEBUG 12:47:37,372 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120819-Data.db compacted
DEBUG 12:47:37,372 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120830
DEBUG 12:47:37,372 All segments have been unmapped successfully
DEBUG 12:47:37,372 removing /home/omid/data/KSP/CF/KSP-CF-hd-121048 from list of files tracked for KSP.CF
DEBUG 12:47:37,372 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121048-Data.db compacted
DEBUG 12:47:37,373 All segments have been unmapped successfully
DEBUG 12:47:37,373 removing /home/omid/data/KSP/CF/KSP-CF-hd-120860 from list of files tracked for KSP.CF
DEBUG 12:47:37,373 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120860-Data.db compacted
DEBUG 12:47:37,373 All segments have been unmapped successfully
DEBUG 12:47:37,373 removing /home/omid/data/KSP/CF/KSP-CF-hd-120818 from list of files tracked for KSP.CF
DEBUG 12:47:37,373 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120818-Data.db compacted
DEBUG 12:47:37,374 All segments have been unmapped successfully
DEBUG 12:47:37,374 removing /home/omid/data/KSP/CF/KSP-CF-hd-120832 from list of files tracked for KSP.CF
DEBUG 12:47:37,374 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120832-Data.db compacted
DEBUG 12:47:37,374 All segments have been unmapped successfully
DEBUG 12:47:37,374 removing /home/omid/data/KSP/CF/KSP-CF-hd-120846 from list of files tracked for KSP.CF
DEBUG 12:47:37,374 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120846-Data.db compacted
DEBUG 12:47:37,374 Deleting KSP-CF-hd-121050-CompressionInfo.db
DEBUG 12:47:37,374 All segments have been unmapped successfully
DEBUG 12:47:37,375 Deleting KSP-CF-hd-121050-Statistics.db
DEBUG 12:47:37,375 removing /home/omid/data/KSP/CF/KSP-CF-hd-120853 from list of files tracked for KSP.CF
DEBUG 12:47:37,375 Deleting KSP-CF-hd-121050-Index.db
DEBUG 12:47:37,375 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120853-Data.db compacted
DEBUG 12:47:37,375 Deleting KSP-CF-hd-121050-Filter.db
DEBUG 12:47:37,375 All segments have been unmapped successfully
DEBUG 12:47:37,375 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121050
DEBUG 12:47:37,375 removing /home/omid/data/KSP/CF/KSP-CF-hd-120857 from list of files tracked for KSP.CF
DEBUG 12:47:37,376 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120857-Data.db compacted
DEBUG 12:47:37,376 All segments have been unmapped successfully
DEBUG 12:47:37,376 removing /home/omid/data/KSP/CF/KSP-CF-hd-120854 from list of files tracked for KSP.CF
DEBUG 12:47:37,376 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120854-Data.db compacted
DEBUG 12:47:37,376 All segments have been unmapped successfully
DEBUG 12:47:37,376 removing /home/omid/data/KSP/CF/KSP-CF-hd-120850 from list of files tracked for KSP.CF
DEBUG 12:47:37,376 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120850-Data.db compacted
DEBUG 12:47:37,377 All segments have been unmapped successfully
DEBUG 12:47:37,377 removing /home/omid/data/KSP/CF/KSP-CF-hd-120820 from list of files tracked for KSP.CF
DEBUG 12:47:37,377 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120820-Data.db compacted
DEBUG 12:47:37,377 All segments have been unmapped successfully
DEBUG 12:47:37,377 removing /home/omid/data/KSP/CF/KSP-CF-hd-121049 from list of files tracked for KSP.CF
DEBUG 12:47:37,377 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121049-Data.db compacted
DEBUG 12:47:37,377 All segments have been unmapped successfully
DEBUG 12:47:37,378 removing /home/omid/data/KSP/CF/KSP-CF-hd-120817 from list of files tracked for KSP.CF
DEBUG 12:47:37,378 Deleting KSP-CF-hd-120843-CompressionInfo.db
DEBUG 12:47:37,378 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120817-Data.db compacted
DEBUG 12:47:37,378 Deleting KSP-CF-hd-120843-Statistics.db
DEBUG 12:47:37,378 All segments have been unmapped successfully
DEBUG 12:47:37,378 Deleting KSP-CF-hd-120843-Index.db
DEBUG 12:47:37,378 removing /home/omid/data/KSP/CF/KSP-CF-hd-121123 from list of files tracked for KSP.CF
DEBUG 12:47:37,378 Deleting KSP-CF-hd-120843-Filter.db
DEBUG 12:47:37,378 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121123-Data.db compacted
DEBUG 12:47:37,379 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120843
DEBUG 12:47:37,379 All segments have been unmapped successfully
DEBUG 12:47:37,379 removing /home/omid/data/KSP/CF/KSP-CF-hd-120845 from list of files tracked for KSP.CF
DEBUG 12:47:37,379 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120845-Data.db compacted
DEBUG 12:47:37,379 All segments have been unmapped successfully
DEBUG 12:47:37,379 removing /home/omid/data/KSP/CF/KSP-CF-hd-120823 from list of files tracked for KSP.CF
DEBUG 12:47:37,379 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120823-Data.db compacted
DEBUG 12:47:37,380 All segments have been unmapped successfully
DEBUG 12:47:37,380 removing /home/omid/data/KSP/CF/KSP-CF-hd-120838 from list of files tracked for KSP.CF
DEBUG 12:47:37,380 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120838-Data.db compacted
DEBUG 12:47:37,380 All segments have been unmapped successfully
DEBUG 12:47:37,381 L0 contains 6413 SSTables (12319161290 bytes) in Manifest@1395029065
DEBUG 12:47:37,381 Deleting KSP-CF-hd-120847-CompressionInfo.db
DEBUG 12:47:37,381 L1 contains 15 SSTables (145544851 bytes) in Manifest@1395029065
DEBUG 12:47:37,381 Deleting KSP-CF-hd-120847-Statistics.db
DEBUG 12:47:37,381 L2 contains 108 SSTables (1045242822 bytes) in Manifest@1395029065
DEBUG 12:47:37,381 Deleting KSP-CF-hd-120847-Index.db
DEBUG 12:47:37,381 L3 contains 212 SSTables (2066259940 bytes) in Manifest@1395029065
DEBUG 12:47:37,382 Replacing [CF-121068(L2), CF-120844(L2), CF-121051(L2), CF-120830(L2), CF-121050(L2), CF-120843(L2), CF-120847(L2), CF-120834(L2), CF-120840(L2), CF-121047(L2), CF-120816(L2), CF-121133(L2), CF-120821(L2), CF-120831(L2), CF-120856(L2), CF-120842(L2), CF-120849(L2), CF-120848(L2), CF-120851(L2), CF-120855(L2), CF-120815(L2), CF-120836(L2), CF-120841(L2), CF-120814(L2), CF-120819(L2), CF-121048(L2), CF-120860(L2), CF-120818(L2), CF-120832(L2), CF-120846(L2), CF-120853(L2), CF-120857(L2), CF-120854(L2), CF-120850(L2), CF-120820(L2), CF-121049(L2), CF-120817(L2), CF-121123(L1), CF-120845(L2), CF-120823(L2), CF-120838(L2), ]
DEBUG 12:47:37,382 Deleting KSP-CF-hd-120847-Filter.db
DEBUG 12:47:37,382 Adding [CF-121136(L-1), CF-121137(L-1), CF-121138(L-1), CF-121139(L-1), CF-121140(L-1), CF-121141(L-1), CF-121142(L-1), CF-121143(L-1), CF-121144(L-1), CF-121145(L-1), CF-121146(L-1), CF-121147(L-1), CF-121148(L-1), CF-121149(L-1), CF-121150(L-1), CF-121151(L-1), CF-121152(L-1), CF-121153(L-1), CF-121154(L-1), CF-121155(L-1), CF-121156(L-1), CF-121157(L-1), CF-121158(L-1), CF-121159(L-1), CF-121160(L-1), CF-121161(L-1), CF-121162(L-1), CF-121163(L-1), CF-121164(L-1), CF-121165(L-1), CF-121166(L-1), CF-121167(L-1), CF-121168(L-1), CF-121169(L-1), CF-121170(L-1), CF-121171(L-1), CF-121172(L-1), CF-121173(L-1), CF-121174(L-1), CF-121175(L-1), ] at L2
DEBUG 12:47:37,382 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120847
ERROR 12:47:37,383 Exception in thread Thread[CompactionExecutor:577,1,main]
java.lang.AssertionError
	at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
DEBUG 12:47:37,384 Deleting KSP-CF-hd-120834-CompressionInfo.db
DEBUG 12:47:37,385 Deleting KSP-CF-hd-120834-Statistics.db
DEBUG 12:47:37,385 Deleting KSP-CF-hd-120834-Index.db
DEBUG 12:47:37,386 Deleting KSP-CF-hd-120834-Filter.db
DEBUG 12:47:37,386 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120834
DEBUG 12:47:37,388 Deleting KSP-CF-hd-120840-CompressionInfo.db
DEBUG 12:47:37,388 Deleting KSP-CF-hd-120840-Statistics.db
DEBUG 12:47:37,388 Deleting KSP-CF-hd-120840-Index.db
DEBUG 12:47:37,389 Deleting KSP-CF-hd-120840-Filter.db
DEBUG 12:47:37,389 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120840
DEBUG 12:47:37,391 Deleting KSP-CF-hd-121047-CompressionInfo.db
DEBUG 12:47:37,391 Deleting KSP-CF-hd-121047-Statistics.db
DEBUG 12:47:37,392 Deleting KSP-CF-hd-121047-Index.db
DEBUG 12:47:37,392 Deleting KSP-CF-hd-121047-Filter.db
DEBUG 12:47:37,392 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121047
DEBUG 12:47:37,394 Deleting KSP-CF-hd-120816-CompressionInfo.db
DEBUG 12:47:37,395 Deleting KSP-CF-hd-120816-Statistics.db
DEBUG 12:47:37,395 Deleting KSP-CF-hd-120816-Index.db
DEBUG 12:47:37,395 Deleting KSP-CF-hd-120816-Filter.db
DEBUG 12:47:37,395 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120816
DEBUG 12:47:37,395 Deleting KSP-CF-hd-121133-CompressionInfo.db
DEBUG 12:47:37,396 Deleting KSP-CF-hd-121133-Statistics.db
DEBUG 12:47:37,396 Deleting KSP-CF-hd-121133-Index.db
DEBUG 12:47:37,396 Deleting KSP-CF-hd-121133-Filter.db
DEBUG 12:47:37,396 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121133
DEBUG 12:47:37,399 Deleting KSP-CF-hd-120821-CompressionInfo.db
DEBUG 12:47:37,399 Deleting KSP-CF-hd-120821-Statistics.db
DEBUG 12:47:37,399 Deleting KSP-CF-hd-120821-Index.db
DEBUG 12:47:37,399 Deleting KSP-CF-hd-120821-Filter.db
DEBUG 12:47:37,399 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120821
DEBUG 12:47:37,402 Deleting KSP-CF-hd-120831-CompressionInfo.db
DEBUG 12:47:37,402 Deleting KSP-CF-hd-120831-Statistics.db
DEBUG 12:47:37,402 Deleting KSP-CF-hd-120831-Index.db
DEBUG 12:47:37,402 Deleting KSP-CF-hd-120831-Filter.db
DEBUG 12:47:37,402 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120831
DEBUG 12:47:37,405 Deleting KSP-CF-hd-120856-CompressionInfo.db
DEBUG 12:47:37,405 Deleting KSP-CF-hd-120856-Statistics.db
DEBUG 12:47:37,405 Deleting KSP-CF-hd-120856-Index.db
DEBUG 12:47:37,405 Deleting KSP-CF-hd-120856-Filter.db
DEBUG 12:47:37,406 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120856
DEBUG 12:47:37,408 Deleting KSP-CF-hd-120842-CompressionInfo.db
DEBUG 12:47:37,408 Deleting KSP-CF-hd-120842-Statistics.db
DEBUG 12:47:37,408 Deleting KSP-CF-hd-120842-Index.db
DEBUG 12:47:37,408 Deleting KSP-CF-hd-120842-Filter.db
DEBUG 12:47:37,409 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120842
DEBUG 12:47:37,411 Deleting KSP-CF-hd-120849-CompressionInfo.db
DEBUG 12:47:37,411 Deleting KSP-CF-hd-120849-Statistics.db
DEBUG 12:47:37,411 Deleting KSP-CF-hd-120849-Index.db
DEBUG 12:47:37,412 Deleting KSP-CF-hd-120849-Filter.db
DEBUG 12:47:37,412 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120849
DEBUG 12:47:37,414 Deleting KSP-CF-hd-120848-CompressionInfo.db
DEBUG 12:47:37,414 Deleting KSP-CF-hd-120848-Statistics.db
DEBUG 12:47:37,414 Deleting KSP-CF-hd-120848-Index.db
DEBUG 12:47:37,415 Deleting KSP-CF-hd-120848-Filter.db
DEBUG 12:47:37,415 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120848
DEBUG 12:47:37,417 Deleting KSP-CF-hd-120851-CompressionInfo.db
DEBUG 12:47:37,417 Deleting KSP-CF-hd-120851-Statistics.db
DEBUG 12:47:37,418 Deleting KSP-CF-hd-120851-Index.db
DEBUG 12:47:37,418 Deleting KSP-CF-hd-120851-Filter.db
DEBUG 12:47:37,418 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120851
DEBUG 12:47:37,420 Deleting KSP-CF-hd-120855-CompressionInfo.db
DEBUG 12:47:37,421 Deleting KSP-CF-hd-120855-Statistics.db
DEBUG 12:47:37,421 Deleting KSP-CF-hd-120855-Index.db
DEBUG 12:47:37,421 Deleting KSP-CF-hd-120855-Filter.db
DEBUG 12:47:37,421 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120855
DEBUG 12:47:37,424 Deleting KSP-CF-hd-120815-CompressionInfo.db
DEBUG 12:47:37,424 Deleting KSP-CF-hd-120815-Statistics.db
DEBUG 12:47:37,424 Deleting KSP-CF-hd-120815-Index.db
DEBUG 12:47:37,424 Deleting KSP-CF-hd-120815-Filter.db
DEBUG 12:47:37,424 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120815
DEBUG 12:47:37,427 Deleting KSP-CF-hd-120836-CompressionInfo.db
DEBUG 12:47:37,427 Deleting KSP-CF-hd-120836-Statistics.db
DEBUG 12:47:37,427 Deleting KSP-CF-hd-120836-Index.db
DEBUG 12:47:37,427 Deleting KSP-CF-hd-120836-Filter.db
DEBUG 12:47:37,427 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120836
DEBUG 12:47:37,430 Deleting KSP-CF-hd-120841-CompressionInfo.db
DEBUG 12:47:37,430 Deleting KSP-CF-hd-120841-Statistics.db
DEBUG 12:47:37,430 Deleting KSP-CF-hd-120841-Index.db
DEBUG 12:47:37,430 Deleting KSP-CF-hd-120841-Filter.db
DEBUG 12:47:37,430 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120841
DEBUG 12:47:37,433 Deleting KSP-CF-hd-120814-CompressionInfo.db
DEBUG 12:47:37,433 Deleting KSP-CF-hd-120814-Statistics.db
DEBUG 12:47:37,433 Deleting KSP-CF-hd-120814-Index.db
DEBUG 12:47:37,433 Deleting KSP-CF-hd-120814-Filter.db
DEBUG 12:47:37,433 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120814
DEBUG 12:47:37,436 Deleting KSP-CF-hd-120819-CompressionInfo.db
DEBUG 12:47:37,436 Deleting KSP-CF-hd-120819-Statistics.db
DEBUG 12:47:37,436 Deleting KSP-CF-hd-120819-Index.db
DEBUG 12:47:37,437 Deleting KSP-CF-hd-120819-Filter.db
DEBUG 12:47:37,437 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120819
DEBUG 12:47:37,439 Deleting KSP-CF-hd-121048-CompressionInfo.db
DEBUG 12:47:37,439 Deleting KSP-CF-hd-121048-Statistics.db
DEBUG 12:47:37,440 Deleting KSP-CF-hd-121048-Index.db
DEBUG 12:47:37,440 Deleting KSP-CF-hd-121048-Filter.db
DEBUG 12:47:37,440 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121048
DEBUG 12:47:37,442 Deleting KSP-CF-hd-120860-CompressionInfo.db
DEBUG 12:47:37,442 Deleting KSP-CF-hd-120860-Statistics.db
DEBUG 12:47:37,443 Deleting KSP-CF-hd-120860-Index.db
DEBUG 12:47:37,443 Deleting KSP-CF-hd-120860-Filter.db
DEBUG 12:47:37,443 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120860
DEBUG 12:47:37,445 Deleting KSP-CF-hd-120818-CompressionInfo.db
DEBUG 12:47:37,446 Deleting KSP-CF-hd-120818-Statistics.db
DEBUG 12:47:37,446 Deleting KSP-CF-hd-120818-Index.db
DEBUG 12:47:37,446 Deleting KSP-CF-hd-120818-Filter.db
DEBUG 12:47:37,446 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120818
DEBUG 12:47:37,448 Deleting KSP-CF-hd-120832-CompressionInfo.db
DEBUG 12:47:37,449 Deleting KSP-CF-hd-120832-Statistics.db
DEBUG 12:47:37,449 Deleting KSP-CF-hd-120832-Index.db
DEBUG 12:47:37,449 Deleting KSP-CF-hd-120832-Filter.db
DEBUG 12:47:37,449 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120832
DEBUG 12:47:37,452 Deleting KSP-CF-hd-120846-CompressionInfo.db
DEBUG 12:47:37,452 Deleting KSP-CF-hd-120846-Statistics.db
DEBUG 12:47:37,452 Deleting KSP-CF-hd-120846-Index.db
DEBUG 12:47:37,452 Deleting KSP-CF-hd-120846-Filter.db
DEBUG 12:47:37,452 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120846
DEBUG 12:47:37,455 Deleting KSP-CF-hd-120853-CompressionInfo.db
DEBUG 12:47:37,455 Deleting KSP-CF-hd-120853-Statistics.db
DEBUG 12:47:37,455 Deleting KSP-CF-hd-120853-Index.db
DEBUG 12:47:37,455 Deleting KSP-CF-hd-120853-Filter.db
DEBUG 12:47:37,456 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120853
DEBUG 12:47:37,458 Deleting KSP-CF-hd-120857-CompressionInfo.db
DEBUG 12:47:37,458 Deleting KSP-CF-hd-120857-Statistics.db
DEBUG 12:47:37,459 Deleting KSP-CF-hd-120857-Index.db
DEBUG 12:47:37,459 Deleting KSP-CF-hd-120857-Filter.db
DEBUG 12:47:37,459 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120857
DEBUG 12:47:37,461 Deleting KSP-CF-hd-120854-CompressionInfo.db
DEBUG 12:47:37,462 Deleting KSP-CF-hd-120854-Statistics.db
DEBUG 12:47:37,462 Deleting KSP-CF-hd-120854-Index.db
DEBUG 12:47:37,462 Deleting KSP-CF-hd-120854-Filter.db
DEBUG 12:47:37,462 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120854
DEBUG 12:47:37,465 Deleting KSP-CF-hd-120850-CompressionInfo.db
DEBUG 12:47:37,465 Deleting KSP-CF-hd-120850-Statistics.db
DEBUG 12:47:37,465 Deleting KSP-CF-hd-120850-Index.db
DEBUG 12:47:37,465 Deleting KSP-CF-hd-120850-Filter.db
DEBUG 12:47:37,465 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120850
DEBUG 12:47:37,468 Deleting KSP-CF-hd-120820-CompressionInfo.db
DEBUG 12:47:37,468 Deleting KSP-CF-hd-120820-Statistics.db
DEBUG 12:47:37,468 Deleting KSP-CF-hd-120820-Index.db
DEBUG 12:47:37,468 Deleting KSP-CF-hd-120820-Filter.db
DEBUG 12:47:37,469 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120820
DEBUG 12:47:37,471 Deleting KSP-CF-hd-121049-CompressionInfo.db
DEBUG 12:47:37,471 Deleting KSP-CF-hd-121049-Statistics.db
DEBUG 12:47:37,471 Deleting KSP-CF-hd-121049-Index.db
DEBUG 12:47:37,471 Deleting KSP-CF-hd-121049-Filter.db
DEBUG 12:47:37,472 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121049
DEBUG 12:47:37,474 Deleting KSP-CF-hd-120817-CompressionInfo.db
DEBUG 12:47:37,474 Deleting KSP-CF-hd-120817-Statistics.db
DEBUG 12:47:37,474 Deleting KSP-CF-hd-120817-Index.db
DEBUG 12:47:37,474 Deleting KSP-CF-hd-120817-Filter.db
DEBUG 12:47:37,475 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120817
DEBUG 12:47:37,477 Deleting KSP-CF-hd-121123-CompressionInfo.db
DEBUG 12:47:37,477 Deleting KSP-CF-hd-121123-Statistics.db
DEBUG 12:47:37,477 Deleting KSP-CF-hd-121123-Index.db
DEBUG 12:47:37,478 Deleting KSP-CF-hd-121123-Filter.db
DEBUG 12:47:37,478 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121123
DEBUG 12:47:37,480 Deleting KSP-CF-hd-120845-CompressionInfo.db
DEBUG 12:47:37,480 Deleting KSP-CF-hd-120845-Statistics.db
DEBUG 12:47:37,480 Deleting KSP-CF-hd-120845-Index.db
DEBUG 12:47:37,481 Deleting KSP-CF-hd-120845-Filter.db
DEBUG 12:47:37,481 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120845
DEBUG 12:47:37,483 Deleting KSP-CF-hd-120823-CompressionInfo.db
DEBUG 12:47:37,483 Deleting KSP-CF-hd-120823-Statistics.db
DEBUG 12:47:37,484 Deleting KSP-CF-hd-120823-Index.db
DEBUG 12:47:37,484 Deleting KSP-CF-hd-120823-Filter.db
DEBUG 12:47:37,484 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120823
DEBUG 12:47:37,487 Deleting KSP-CF-hd-120838-CompressionInfo.db
DEBUG 12:47:37,487 Deleting KSP-CF-hd-120838-Statistics.db
DEBUG 12:47:37,487 Deleting KSP-CF-hd-120838-Index.db
DEBUG 12:47:37,487 Deleting KSP-CF-hd-120838-Filter.db
DEBUG 12:47:37,487 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120838
{code}","12/Jul/12 10:50;rvanderleeden;I attached the file assertion.system.log with DEBUG lines around the assertion ERROR.
This is just an excerpt from system.log which is much larger.","13/Jul/12 13:17;slebresne;Unfortunately the log doesn't give us much to chew on.

I've actually be able to reproduce this once (using stress). Unfortunately, I hadn't added more debugging yet and since I added more debug info I haven't been able to reproduce (despite having retried from scratch like 3 times letting it run for multiple hours each time).

So I'm attaching a simple patch that adds more debugging. If you guys can try applying the patch and see if you can reproduce. If so, the log file produced should be helpful. I'll note that it is preferable to *not* turn DEBUG logging with this patch as this is not useful and would only generate awfully large logs.
",14/Jul/12 15:04;rvanderleeden;includes extended INFO lines from using 0001 patch,"14/Jul/12 15:18;rvanderleeden;The file assertion.moreinfo.system.log has been attached. 

The assertion ERROR could be reproduced by doing the following:
(1) Bulk load 200 SSTables from a snapshot into a new 3-node cluster
(2) Run nodetool compact and repair
(3) Add ~500 SSTables from the same snapshot
(4) Run nodetool repair

RESULT: 
- we see immediately a StackOverflowError (from the repair command)
- after 2 minutes compaction starts
- after 11 minutes there is the AssertionError
",15/Jul/12 13:51;omid;Attached the log (assertion-w-more-debugging-info-omid.log) with more debugging info that leads to AssertionError on LeveledManifest.promote.,"16/Jul/12 11:27;slebresne;Thanks a lot for the logs everyone.

The problem was with the handling of sstables having the same first and last token. More precisely, Bounds.contains uses Range.contains(), but while the Range (a, a] selects the whole ring, the Bounds [a,a] only selects a. This means sstable with the same first and last on level N+1 were included in compaction of any sstable of level N even when there wasn't any intersection.

Attaching simple patch to fix.",16/Jul/12 12:33;jbellis;+1,"16/Jul/12 13:35;slebresne;Committed, thanks.","16/Jul/12 15:15;slebresne;Alright, the patch did fix the issue but also introduce a small bug in that the Bounds.contains does not handle correctly the special Bounds(min, min) that should include everything (rather than nothing). Patch attached to fix that.",16/Jul/12 15:23;jbellis;+1,"16/Jul/12 15:33;slebresne;Alright, committed, thanks.","18/Jul/12 12:14;omid;Awesome. Thanks for the patch. Tested it and it works.

Sylvain, regarding your earlier comment on CASSANDRA-4321:

{quote} This is not really a new bug, but I believe that prior to CASSANDRA-4142, this had less consequences. {quote}

Does it mean LC-compacted SSTables created by 1.1.0 or earlier are as well affected and need to be scrubbed?","18/Jul/12 16:42;slebresne;bq. Does it mean LC-compacted SSTables created by 1.1.0 or earlier are as well affected and need to be scrubbed?

Potentially, yes, unfortunately.","23/Jul/12 18:14;minaguib;Hi

I'm running cassandra 1.1.2 + the 2 patches in this ticket applied.

On one node in my cluster I just got the same assertion error - shortly after the node started up and a compaction was attempted:

    ERROR [CompactionExecutor:19] 2012-07-23 14:05:43,312 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:19,1,main]
    java.lang.AssertionError
            at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
            at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
            at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
            at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
            at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
            at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
            at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
            at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
            at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
            at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
            at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
            at java.util.concurrent.FutureTask.run(FutureTask.java:138)
            at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
            at java.lang.Thread.run(Thread.java:662)


Does this indicate a problem with a bad sstable that's fixable with scrubbing ?  Or does this ticket + patches deserve a second look ?",23/Jul/12 18:18;jbellis;The former.,"24/Jul/12 00:14;minaguib;
Unfortunately the problem did not go away after scrubbing.

I scrubbed 2 of the problematic nodes.  Immediately after the scrub (5 hours) finished, a compaction was attempted and again failed:

{code}
ERROR [CompactionExecutor:47] 2012-07-23 19:48:52,500 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:47,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

I've verified (using ls and the timestamps of *-Data.db sstables) that there are no old sstables and all of the sstables in the CF are the ones generated during the 5 hours of scrubbing.

I've also stopped and restarted one of these nodes, and again shortly after restart the compaction failed with a different stack trace:

{code}
java.lang.RuntimeException: Last written key DecoratedKey(225595347341523546110318866012608496, 64313635626665302d333764372d313165302d393933622d303032366239333763386531) >= current key DecoratedKey(221078382620949716286900834756484795, 37303538643361662d616362662d343030312d313565382d633662303030303030336131) writing into /var/lib/cassandra/data/MYKS/MYCF/MYKS-MYCF-tmp-hd-520277-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

Is the fix for this ticket contained in some other code beyond the 2 posted patches (4411.txt, 4411-followup.txt) ?  That's what I'm running with on top of 1.1.2.
","24/Jul/12 00:37;awinter;I can also confirm that after multiple offline sstablescrubs across all nodes that I still had several nodes (but not all) spread across multiple DC's still exhibiting this problem as described above by Mina.  

In an attempt to work around the problem I shut down the affected instances, deleted all data and re-bootstrapped them as if they were dead nodes.  Since doing so I haven't had the problem return however it is still early days.","24/Jul/12 07:14;slebresne;@Mina Did you run the offline scrub introduced with CASSANDRA-4321. Otherwise, it won't fix the problem. So you need to 1) shut down the node (this is important before running the offline scrub) and 2) run ./bin/sstablescrub. That last step should print some lines indicating having corrected some problems (otherwise, something is wrong with the scrubbing).

If after that you still get an exception, it might be helpful if you could run with 0001-Add-debugging-info-for-LCS.txt applied.","24/Jul/12 12:20;minaguib;I ran the scrub in online mode.

I just took down a node and am now running it in offline mode.  Will report back.

BTW, the default ""sstablescrub"" does not respect the memory limits set in cassandra.in.sh, so it failed for me with:
{code}
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at sun.security.provider.DigestBase.engineDigest(DigestBase.java:146)
        at java.security.MessageDigest$Delegate.engineDigest(MessageDigest.java:546)
        at java.security.MessageDigest.digest(MessageDigest.java:323)
        at org.apache.cassandra.utils.FBUtilities.hash(FBUtilities.java:229)
        at org.apache.cassandra.utils.FBUtilities.hashToBigInteger(FBUtilities.java:213)
        at org.apache.cassandra.dht.RandomPartitioner.getToken(RandomPartitioner.java:154)
        at org.apache.cassandra.dht.RandomPartitioner.decorateKey(RandomPartitioner.java:47)
        at org.apache.cassandra.cache.AutoSavingCache.readSaved(AutoSavingCache.java:118)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:230)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:341)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:313)
        at org.apache.cassandra.db.Table.initCf(Table.java:371)
        at org.apache.cassandra.db.Table.<init>(Table.java:304)
        at org.apache.cassandra.db.Table.open(Table.java:119)
        at org.apache.cassandra.db.Table.openWithoutSSTables(Table.java:102)
        at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:65)
{code}

I edited it to update the hardocded limit of 256MB to a more reasonable value (the same as my cassandra.in.sh) to allow it to run without crashing.

","24/Jul/12 15:43;minaguib;Things appear better after an offline scrub.  While the scrubbing itself was uneventful, at the very end it did ""Checking leveled manifest"", found 14 sstables in level 3 and level 4 that were problematic and moved them back to level 0.

I started the node back up and all (+/- 10) compactions ran successfully.

I'll keep an eye on it and if it stays well I'll do the same to the other nodes.  Perhaps I'll try my luck with sstablescrub --manifest-check to see if I can keep the downtime to a minimum.","26/Jul/12 14:33;minaguib;Quick follow-up

All the problematic nodes have been offline scrubbed (successfully using --manifest-check to speed things up).  There are no more compaction errors / pending compactions.

Like Anton, I'm a bit weary and keeping an eye on things - but so far so good.

On a tangent, it occurred to me that the amount of time it takes to run ( sstablescrub --manifest-check ) is mostly reading the sstables - the check itself and demoting the bad sstables to L0 appears very cheap - would it be a good idea to perform that check automatically on cassandra startup (after the sstables have been read) ?  It *may* be a quick fix for 1.1.3 to help people out who have been bitten by this but don't know it yet.

",06/Aug/12 16:55;omid;@Mina were the sstables created after CASSANDRA-4321 patch? Otherwise offline-scrub with --manifest-check is unlikely to solve the problem (or at least I don't understand how) since there would still be out-of-order sstables existing.,"08/Aug/12 18:31;omid;I can confirm that the problem is still there. I offline-scrubbed using 1.1.3 (sstables were generated by 1.1.0) , but the scrubber did not report any out-of-order sstables, but sent some sstables back to L0. On compaction though, I get the exception:

{quote}
2012-08-08_18:15:41.85260 java.lang.RuntimeException: Last written key DecoratedKey(135076574692378869287086649376333921820, SOME_KEY_1) >= current key DecoratedKey(135076574692378869287086649376333921820, SOME_KEY_1) writing into /var/lib/cassandra/abcd/data/KSP/CF1/KSP-CF1-tmp-he-178793-Data.db
2012-08-08_18:15:41.85303 	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
2012-08-08_18:15:41.85314 	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
2012-08-08_18:15:41.85326 	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
2012-08-08_18:15:41.85338 	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
2012-08-08_18:15:41.85351 	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:288)
2012-08-08_18:15:41.85364 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2012-08-08_18:15:41.85375 	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-08-08_18:15:41.85385 	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
2012-08-08_18:15:41.85395 	at java.util.concurrent.FutureTask.run(Unknown Source)
2012-08-08_18:15:41.85403 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-08-08_18:15:41.85414 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-08-08_18:15:41.85424 	at java.lang.Thread.run(Unknown Source)
{quote}

","08/Aug/12 18:56;omid;Not sure, but from the fact that the two keys are identical and the ""Last written key""-check checks for greater-or-equal, shouldn't the "">"" be "">="" in:

https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/db/compaction/Scrubber.java#L178

?",08/Aug/12 21:53;jbellis;I think you're right.  I'll push a fix for that shortly.,09/Aug/12 14:43;jbellis;done in 115f380a86912e5918f534db2ec2935253909fad,"09/Aug/12 16:27;omid;Thanks!

There are two more off-by-ones. 

One is the Scrubber to detect out-of-order keys (similar to the one already patched). 

The other one is in manifestCheck to send overlapping sstables back to L0 (which has causes assertion errors in LeveledManifest::promote)

Patch is attached.","09/Aug/12 16:40;jbellis;committed, thanks!","28/Aug/12 17:09;jbellis;Updating to note that this was ""mostly"" fixed in 1.1.3, with Omid's extra >= fixes in 1.1.4.",,,,,,,,,,,,,,,,,,,,,,,,,
Change nanoTime() to currentTimeInMillis() in schema related code.,CASSANDRA-4432,12598217,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,xedin,xedin,10/Jul/12 16:51,12/Mar/19 14:16,13/Mar/19 22:27,10/Jul/12 17:17,1.1.3,,,,,0,,,,,,,"From nanoTime() description:

""The value returned represents nanoseconds since some fixed but arbitrary time (perhaps in the future, so values may be negative). This method provides nanosecond precision, but not necessarily nanosecond accuracy. No guarantees are made about how frequently values change.""

Also see http://www.mail-archive.com/dev@cassandra.apache.org/msg04992.html
",,,,,,,,,,,,,,,,,,,10/Jul/12 17:00;xedin;CASSANDRA-4432.patch;https://issues.apache.org/jira/secure/attachment/12535872/CASSANDRA-4432.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-10 17:08:31.393,,,no_permission,,,,,,,,,,,,256153,,,Tue Jul 10 22:08:22 UTC 2012,,,,,,0|i0gvt3:,96583,jbellis,jbellis,,,,,,,,,,10/Jul/12 17:08;jbellis;+1,"10/Jul/12 17:09;mtheroux2;In order to be consistent with timestamps elsewhere, shouldn't the changes read System.currentTimeMills * 1000?",10/Jul/12 17:14;xedin;We have millis * 1000 for columns but I don't see a reason to do that for schema as all columns would be created with the same millis resolution by all migrations.,"10/Jul/12 17:17;mtheroux2;Its a consideration for existing users.  Any user who would have created a keyspace before this fix, will have timestamps in whatever System.nanoTime() returns.  The bigger you make the timestamp, the less users will be stuck attempting to fix their timestamps so they can update their schema (I guess using sstable2json?).  Plus, its consistent with columns :)  ",10/Jul/12 17:17;xedin;Committed.,"10/Jul/12 17:18;slebresne;I suppose that since we'll access the system table (including the schema) directly in CQL3, it might be worth using micro-seconds.",10/Jul/12 17:23;jbellis;Good point.,"10/Jul/12 17:26;xedin;Ok, I will add * 1000 there.",10/Jul/12 17:39;slebresne;nit: we have a FBUtilities.timestampMicros(),"10/Jul/12 17:42;xedin;Right, this is what I was thinking about :)",10/Jul/12 20:46;xedin;Committed change to timestampMicros().,"10/Jul/12 22:08;hudson;Integrated in Cassandra #1673 (See [https://builds.apache.org/job/Cassandra/1673/])
    change System.currentTimeMillis() to FBUtilities.timestampMicros(), related to CASSANDRA-4432 (Revision 346ac03c29cd1fe763bd01077a5e0c59f12181b3)

     Result = ABORTED
xedin : 
Files : 
* src/java/org/apache/cassandra/service/MigrationManager.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in ColumnFamilyStore.getOverlappingSSTables() during repair,CASSANDRA-4456,12599726,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,mheffner,mheffner,21/Jul/12 14:07,12/Mar/19 14:16,13/Mar/19 22:27,23/Jul/12 19:36,1.1.3,,,,,0,,,,,,,"We have hit the following exception on several nodes while running repairs across our 1.1.2 ring. We've observed it happen on either the node executing the repair or a participating replica in the repair operation. The result in either case is that the repair hangs.


ERROR [ValidationExecutor:9] 2012-07-21 01:54:03,019 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[ValidationExecutor:9,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.ColumnFamilyStore.getOverlappingSSTables(ColumnFamilyStore.java:874)
        at org.apache.cassandra.db.compaction.CompactionController.<init>(CompactionController.java:69)
        at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:834)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:698)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:68)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:438)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


In building this ring we migrated sstables from an identical 0.8.8 ring by:

 1. Creating the schema on our new 1.1.2 ring.
 2. Rsyncing over sstables from 0.8.8 ring.
 3. Renaming the sstables to match the directory and file naming structure of 1.1.x.
 4. Ran nodetool refresh <keyspace> <cf> for each CF across each node.
 5. Ran nodetool upgradesstables for each CF across each node.

When those steps had completed, we began rolling repairs. Not all of the repair operations have hit the exception -- some of the repairs have completed successfully.
",Ubuntu 11.04 64-bit,,,,,,,,,,,,,,,,,,23/Jul/12 16:16;slebresne;4456.txt;https://issues.apache.org/jira/secure/attachment/12537576/4456.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-21 19:15:17.07,,,no_permission,,,,,,,,,,,,256172,,,Mon Jul 23 19:36:36 UTC 2012,,,,,,0|i0gw2v:,96627,jbellis,jbellis,,,,,,,,,,"21/Jul/12 19:15;mtheroux2;I just hit this problem myself today, on a single node in a six node cluster.  I was running nodetool repair, and it halted with this exception in the log.  I was monitoring the repair pretty closely.  A couple of observations:

1) It happened while compaction of the same column family was happening simultaneously
2) When I re-ran it, it worked.

Note: I am not a cassandra developer, but I looked at the code.  A highly uneducated guess is that an sstable was compacted and deleted while validation was expecting it to be there?  ",21/Jul/12 19:15;mtheroux2;I am also on 1.1.2.,"23/Jul/12 14:46;jbellis;I think this was introduced by CASSANDRA-3721: getOverlappingSSTables assumes that the sstables we check for overlaps are part of the live set, but now we can validate over a snapshot instead.","23/Jul/12 16:16;slebresne;Actually I think this can happen even when snapshots are not used since a sstable can finish to be compacted just between when we chose sstable for repair and when we create the CompactionController for the validation compaction. In particular, I wonder if Michael and Mike have used -snapshot for their compaction. Though it's true that repair on snapshot will make that way more likely to happen.

But actually I don't think we need to call getOverlappingSStables at all in the first place for repair, since this is used only to decide if we can purge but repair does not do purging. Attaching a simple patch to skip the call entirely.
","23/Jul/12 16:29;jbellis;You need to wire VCC in to ValidationCompactionIterable, but otherwise +1.",23/Jul/12 19:36;slebresne;Oups indeed. Committed with the thing wired up. Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql3: defining more than one pk should be invalid,CASSANDRA-4477,12600923,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,31/Jul/12 22:17,12/Mar/19 14:16,13/Mar/19 22:27,15/Aug/12 10:13,1.2.0 beta 1,,,,,0,cql3,,,,,,"dtests caught this on trunk:

{noformat}
  File ""/var/lib/buildbot/cassandra-dtest/cql_tests.py"", line 277, in create_invalid_test
    assert_invalid(cursor, ""CREATE TABLE test (key1 text PRIMARY KEY, key2 text PRIMARY KEY)"")
  File ""/var/lib/buildbot/cassandra-dtest/assertions.py"", line 31, in assert_invalid
    assert False, ""Expecting query to be invalid""
AssertionError: Expecting query to be invalid
{noformat}",,,,,,,,,,,,,,,,,,,01/Aug/12 07:31;slebresne;4477.txt;https://issues.apache.org/jira/secure/attachment/12538653/4477.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-01 07:31:17.529,,,no_permission,,,,,,,,,,,,256188,,,Wed Aug 15 10:13:47 UTC 2012,,,,,,0|i0gwaf:,96661,xedin,xedin,,,,,,,,,,01/Aug/12 07:31;slebresne;Attached patch to fix the issue and correctly refuse invalid PK definitions.,"15/Aug/12 10:13;xedin;+1, Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh COPY FROM without explicit column names is broken,CASSANDRA-4470,12600572,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thepaul,thepaul,thepaul,28/Jul/12 20:32,12/Mar/19 14:16,13/Mar/19 22:27,29/Jul/12 16:50,1.1.3,,Legacy/Tools,,,0,cqlsh,,,,,,"When trying to do a COPY FROM command in cqlsh without an explicit list of column names, an error results:

{noformat}
cqlsh:a> copy blah from stdin;
[Use \. on a line by itself to end input]
[copy] a,b,c

object of type 'NoneType' has no len()
{noformat}

Broken by the fix for CASSANDRA-4434.",,,,,,,,,,,,,,,,,,,28/Jul/12 20:35;thepaul;4470.txt;https://issues.apache.org/jira/secure/attachment/12538253/4470.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-29 16:50:31.401,,,no_permission,,,,,,,,,,,,256182,,,Sun Jul 29 16:50:31 UTC 2012,,,,,,0|i0gw7j:,96648,brandon.williams,brandon.williams,,,,,,,,,,"28/Jul/12 20:35;thepaul;Fixed in attached patch, and in my github, in the 4470 branch. Current version tagged pending/4470:

https://github.com/thepaul/cassandra/tree/4470",29/Jul/12 16:50;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix update of CF comparator (including adding new collections),CASSANDRA-4493,12601508,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,05/Aug/12 19:22,12/Mar/19 14:16,13/Mar/19 22:27,07/Sep/12 09:59,1.2.0 beta 1,,,,,0,,,,,,,"Updating the comparator of a column family (which is authorized if the new comparator is ""compatible"" with the old one, and that includes adding a new component to a CompositeType, or adding a new collection to a CQL3 table) doesn't completely work. The problem is that even if we change the comparator in CFMetada, the old comparator will still be aliased by the current memtable. This means updates (that expect the new comparator) will fail until a new memtable is created.",,,,,,,,,,,,,,,,,,,05/Aug/12 19:29;slebresne;0001-Flush-after-CFStore-reload.txt;https://issues.apache.org/jira/secure/attachment/12539217/0001-Flush-after-CFStore-reload.txt,05/Aug/12 19:29;slebresne;0002-Fix-ALTER-logic.txt;https://issues.apache.org/jira/secure/attachment/12539218/0002-Fix-ALTER-logic.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-06 20:11:05.453,,,no_permission,,,,,,,,,,,,256197,,,Fri Sep 07 09:59:09 UTC 2012,,,,,,0|i0gwfr:,96685,jbellis,jbellis,,,,,,,,,,"05/Aug/12 19:29;slebresne;Attached a fix (first patch) that adds a flush to the reload of CFS (since it's called when the CFMetadata a the cf is modified).

I note that this affect at least 1.1 and maybe some of 1.0 (I don't remember exactly when we started allowing to change the comparator). However, there was little reason to use that feature so far. This change however in 1.2 since adding a new collection actually change the comparator.

I'm also attaching a 2nd patch that fixes a bit of broken logic in ALTER TABLE for collections. This is not directly related expect that I discovered both bug at the same time, and that second one is fairly simple so I figured this might not warrant a specific ticket.",06/Sep/12 20:11;jbellis;+1,"07/Sep/12 09:59;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update CQL pseudo-maps to real map syntax,CASSANDRA-4497,12602033,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,07/Aug/12 16:23,12/Mar/19 14:16,13/Mar/19 22:27,04/Sep/12 17:43,1.2.0 beta 1,,,,,0,cql3,,,,,,"- compression_parameters
- replication_parameters (combine strategy + options like we did compression)
- anything else?",,,,,,,,,,,,,,,,,,,04/Sep/12 16:26;slebresne;4497-v2.txt;https://issues.apache.org/jira/secure/attachment/12543699/4497-v2.txt,04/Sep/12 11:46;xedin;CASSANDRA-4497.patch;https://issues.apache.org/jira/secure/attachment/12543657/CASSANDRA-4497.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-07 16:36:05.341,,,no_permission,,,,,,,,,,,,256200,,,Tue Sep 04 17:43:32 UTC 2012,,,,,,0|i0gwgv:,96690,slebresne,slebresne,,,,,,,,,,"07/Aug/12 16:36;slebresne;There's also compaction (same than replication, we want to merge the class and the options).

I would also be in favor of using shortish name, something like:
{noformat}
CREATE KEYSPACE foo WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }
{noformat}
(we could even shorten replication_factor to factor I guess, though I'm personally good like that.

Talking of the replication strategy, I'd be in favor of having some default for the replication strategy as we do when using the cassandra-cli, as this make it easier to do quick tests.","07/Aug/12 18:35;jbellis;bq. I would also be in favor of using shortish name


+1, although we already accept short classnames everywhere (?) we accept long ones, for built-in classes.  Unsure why docs continue to use fully-qualified names.

bq. I'd be in favor of having some default for the replication strategy as we do when using the cassandra-cli

I'd like to, but I think the reasoning in https://issues.apache.org/jira/browse/CASSANDRA-2529?focusedCommentId=13022925&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13022925 still applies.","07/Aug/12 19:16;slebresne;bq. although we already accept short classnames everywhere

Yes, but I was mostly taking about using 'replication' instead of 'replication_parameters'. That's a detail of course.

bq. I'd like to, but I think the reasoning in https://issues.apache.org/jira/browse/CASSANDRA-2529?focusedCommentId=13022925&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13022925 still applies

That's true. Maybe we could at least make it so that ""replication = { 'replication_factor' : 1 }"" infers SimpleStrategy? ","07/Aug/12 19:33;jbellis;At that point, I don't think the characters saved justify the ambiguity over what's happening.","07/Aug/12 20:20;slebresne;I guess it's not so much about saving keystrokes (though as someone that does a lot a quick, one-shot CQL tests, I wouldn't mind the shortcut), but the fact that the create keyspace statement is the first statement beginner will see. At that time, it feels like even having the strategy class means you need to either say ""ignore that for now"" (which is always bad and frustrate people) or have to explain that it's because there is multiple strategies and yada yada yada. And given we don't allow 'replication_factor' for NTS, I don't think this adds much ambiguity. Not a huge deal though, but I do feel this minimal create keyspace statement is not minimal enough.","07/Aug/12 20:31;jbellis;I agree with your problem statement, but I haven't seen a sufficiently attractive solution yet. :)","10/Aug/12 12:01;slebresne;Actually, while we're at using collections in system table, it would be to also
use collections for the columns_aliases, key_aliases and the index_options for
in ColumnDefinition. For the column_aliases and key_aliases, it's indirectly
visible to user if they query System.schema_columnfamilies, and since that will
be the new way to do a describe, it's worth removing the use of json.","30/Aug/12 16:07;xedin;{replication, compression, compaction}_parameters are made to be set using <key> = { <k> : <v>, ... } syntax.","31/Aug/12 17:55;thepaul;From the patch, it looks like we won't be supporting the old syntax anymore. This, and the option name changes, is going to make all our docs more complicated, and also non-trivially complicate the smarter clients (in particular, cqlsh, opscenter, etc). Also, it looks like there isn't any way to change one option and leave the other intact (like ""WITH replication['replication_factor'] = 2"", which it seems like ought to be implied by the use of the map syntax. The whole thing doesn't seem much cleaner than before. What's the main motivation?","31/Aug/12 23:51;jbellis;How is it more complicated to document that replication_parameters is a map like any other, than replication_parameters has its own one-off syntax that only exists for historical reasons?  I don't buy it.

bq. Also, it looks like there isn't any way to change one option and leave the other intact (like ""WITH replication['replication_factor'] = 2"", which it seems like ought to be implied by the use of the map syntax

Agreed, if we can fix this easily we should.

bq. What's the main motivation

Two motivations:
- reduce the special case weirdnesses exposed to users, as above
- reduce special case weirdnesses in our internal implementation (which is also exposed to ""power users"" in the system KS).  so much more pleasant to ""select replication_options from schema_keyspaces where keyspace_name = 'foo'"" and get a Map back.","01/Sep/12 03:12;thepaul;bq. How is it more complicated to document that replication_parameters is a map like any other, than replication_parameters has its own one-off syntax that only exists for historical reasons? I don't buy it.

It's not, but it is more complicated when we need to say ""if you're using CQL 3 with Cassandra 1.0 or 1.1, then you need to do this. If you're using CQL 3 with 1.2 or higher, then you need to do this totally different thing. If you're using CQL 2, then you do something else."" I know, ideally we would have fully separate documentation sets and fully separate cqlsh versions per major Cassandra version, but people don't seem to be very good at referring only to the right one.

bq. reduce the special case weirdnesses exposed to users, as above

I'm admittedly thick, but which weirdnesses are reduced here?

bq. reduce special case weirdnesses in our internal implementation (which is also exposed to ""power users"" in the system KS). so much more pleasant to ""select replication_options from schema_keyspaces where keyspace_name = 'foo'"" and get a Map back.

I agree with that 100%, but this patch doesn't do that. It only changes the dedicated syntax for setting keyspace and columnfamily options. I'd much prefer getting real maps inside the system.schema* tables and leaving the cql syntax alone when we can. It has been a real moving target, and swiping out the ""first command you do"" from under everyone's feet seems like it might be discouraging.","01/Sep/12 15:16;xedin;Ok, let make it clear what do me want to be the out come of the ticket: I thought that we want to change CQL3 parameter behavior from pseudo-maps to real maps for commands like ""create keyspace/table"" and ""alter table""? I'm not a fan of keeping old syntax because that could lead to confusing mixing problems where users would use old syntax in one place and new one later in the same statement... If we also want to modify how schema handles maps or namely change json to native maps that would require a separate ticket because of work front which would have to be done. ","03/Sep/12 01:49;jbellis;bq.  I'm not a fan of keeping old syntax because that could lead to confusing mixing problems where users would use old syntax in one place and new one later in the same statement

+1

bq. If we also want to modify how schema handles maps or namely change json to native maps that would require a separate ticket 

Fine, CASSANDRA-4603.",04/Sep/12 11:46;xedin;rebased patch after changes in CASSANDRA-4597,"04/Sep/12 16:26;slebresne;
Since we're breaking from earlier version of CQL3 anyway (and we've been clear that CQL3 has been beta so far), I would be in favor of shortening some options names (some were in Pavel's patch already but not all) as said previous.

I'm attaching a v2 based on Pavel's patch that does that. So that the exact syntax is:
{noformat}
CREATE KEYSPACE foo WITH replication = { 'class' : 'SimpleStrategy',  'replication_factor' : 1 }
CREATE TABLE .... WITH compression = { ... }
                   AND compaction = { 'class' : 'SizeTieredCompactionStrategy', 'min_threshold' : 2, 'max_threshold' : 4 }
{noformat}

v2 also slightly update the grammar to better reuse code between create and alter table, and it fixes a potential NPE in the grammar convertMap method (due to antlr being a smart-ass). It also throw a ConfiguarionException in the case were compaction options are given but not the 'class' itself (since that feels more consistent with the semantic of having a map literal).

bq. it looks like there isn't any way to change one option and leave the other intact (like ""WITH replication['replication_factor'] = 2""

I agree, but that's a bit of a pain to add (in particular things like ""WITH replication = { 'class' : 'SimpleStrategy} AND replication['replication_factor'] = 2"" are a bit annoying to handle) so I would suggest leaving that to later.
","04/Sep/12 16:51;xedin;+1, with nit: I think we would be should make naming consistent and change COMPACTION_OPTIONS to COMPACTION_PARAMETERS.","04/Sep/12 17:43;slebresne;Committed with nits fixed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 column value validation bug,CASSANDRA-4377,12595934,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,nickmbailey,nickmbailey,26/Jun/12 16:44,12/Mar/19 14:16,13/Mar/19 22:27,11/Sep/12 09:03,1.2.0 beta 1,,Legacy/CQL,,,2,cql3,,,,,,"{noformat}
cqlsh> create keyspace test with strategy_class = 'SimpleStrategy' and strategy_options:replication_factor = 1;
cqlsh> use test;
cqlsh:test> CREATE TABLE stats (
        ...   gid          blob,
        ...   period     int,
        ...   tid          blob, 
        ...   sum        int,
        ...   uniques           blob,
        ...   PRIMARY KEY(gid, period, tid)
        ... );
cqlsh:test> describe columnfamily stats;

CREATE TABLE stats (
  gid blob PRIMARY KEY
) WITH
  comment='' AND
  comparator='CompositeType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.UTF8Type)' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';
{noformat}

You can see in the above output that the stats cf is created with the column validator set to text, but neither of the non primary key columns defined are text. It should either be setting metadata for those columns or not setting a default validator or some combination of the two.",,,,,,,,,,,,,,CASSANDRA-4907,,,,,06/Sep/12 11:16;slebresne;4377-2.txt;https://issues.apache.org/jira/secure/attachment/12544028/4377-2.txt,10/Jul/12 15:40;slebresne;4377.txt;https://issues.apache.org/jira/secure/attachment/12535860/4377.txt,22/Aug/12 09:50;xedin;CASSANDRA-4377.patch;https://issues.apache.org/jira/secure/attachment/12541883/CASSANDRA-4377.patch,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-06-27 07:00:03.317,,,no_permission,,,,,,,,,,,,255103,,,Tue Sep 11 09:03:56 UTC 2012,,,,,,0|i0ep2n:,83824,slebresne,slebresne,,,,,,,,,,"27/Jun/12 07:00;slebresne;What happens is that columns metadata for composite CQL3 columns refers to only one of the component of the column name. So internally they have a 'componentIndex' parameter which allow to handle them correctly (otherwise you don't know which comparator to use to display those column metadata). However, we decided that thrift users shouldn't have to care about this, so we don't expose those metadata to thrift at all. In other words, the CF does have metadata for 'sum' and 'uniques' internally, but they are not exposed to thrift.

So I guess it is more of a cqlsh problem that shouldn't use the thrift describe call for CQL3. Instead, it should directly query the system.keyspace, system.columnfamilies and system.columns table. However, it'd be much more easier to do that with CASSANDRA-4018, but that is not in 1.1.","27/Jun/12 14:45;nickmbailey;bq. So I guess it is more of a cqlsh problem that shouldn't use the thrift describe call for CQL3.

The main problem here from my perspective is that it was impossible to insert data into the column family except with cqlsh. Using a basic thrift batch mutate failed on the validation step because it tried to validate the value in the sum column as text (default_validation_class).","27/Jun/12 22:01;hubaghdadi;True, I'm unable to save any data to Cassandra.
Trying to save with Hector (uses Mutators):
#<HInvalidRequestException me.prettyprint.hector.api.exceptions.HInvalidRequestException: InvalidRequestException(why:(String didn't validate.) [mks][stats][1234:7461726765742d3131:sum] failed validation)>","28/Jun/12 11:51;slebresne;bq. it was impossible to insert data into the column family except with cqlsh. Using a basic thrift batch mutate failed on the validation step because it tried to validate the value

Alright, that part is not expected and is likely a bug.

Though I note that even if we fix that, since we don't expose on the thrift side everything needed to interpret the value correctly, thrift client won't be able to work with those kind of table very well. In particular they won't know how to interpret the value of a get. So I guess we need to either say clearly that CQL3 table are not meant to be accessed from thrift, or we should probably start exposing all the column metatada on the thrift side (but we need to expose the componentIndex part of the metadata in particular and the discussion on CASSANDRA-4093 is relevant for that).","10/Jul/12 15:40;slebresne;Attaching simple patch to allow the insertion in the case above. Truth is, I'm not really satisfied (though I don't have a clearly better option) by such a patch for 2 reasons:
# it will break the case for thrift where people were using compositeType and column_metadata on them. That might be 0 people we're talking about but it's still a bit annoying. An alternative here would be that for each composite column, we iterater over all column_metadata and check where one apply. This would work but this feels butt ugly.
# it feels to me we supporting either not enough or too much in the thrift side. Even if we support this, we still don't expose the CQL3 metadata to thrift, so one has to know the CQL schema definition to be able to create the column in the first place (or deserialize it on read). In particular, most advanced thrift client will likely still break at one point or another.

Overall I see two reasonable approaches:
* Either we start exposing enough on the thirft side so that thrift client can work correctly with CQL3. While this may be doable reasonably easily now, this will be increasingly difficult with things like CASSANDRA-4179, CASSANDRA-3647, ... Besides, even if we make it possible to work with CQL3 table, it doesn't mean it will be convenient since thrift won't do the grouping of columns in sparse table.
* Be clear that you cannot work with CQL3 created table from thrift.

But imo the in-the-middle approach that this patch would start takes the risk of polluting the thrift side without adding much.
","10/Jul/12 15:48;slebresne;To be clear, if this issue is for instance a blocker for map-reduce for CQL3, I'm not against committing it, but on a more long term/general level, I do want to note that accessing CQL3 tables to thrift is imho a larger problem and we should be clear on what we want to guarantee.","10/Jul/12 16:32;nickmbailey;It doesn't sound like its going to be possible to make things work well in the case of accessing cql3 data from thrift.

If thats the case I'm in favor of making that incompatibility *very* explicit. I would say explicitly throwing an exception when trying to access a cql3 column family from thrift would be an acceptable solution.

The fact that it took me quite a bit of time as well as digging around in both client code and cassandra source code to figure this bug out makes me worried for other users hitting similar problems.","10/Jul/12 17:11;thepaul;bq. I would say explicitly throwing an exception when trying to access a cql3 column family from thrift would be an acceptable solution.

-1 on that; it's very useful to have a lower-level access method to look at the ""storage engine"" layer instead of the logical CQL3 rows at times.","10/Jul/12 17:14;nickmbailey;so disable write access to cql3 cfs from thrift? And leave read access with the qualification that everything will be returned as BytesType?

The only other option I see is to say that we are going to identify and fix all bugs like this one.","10/Jul/12 17:22;jbellis;bq. disable write access to cql3 cfs from thrift? And leave read access with the qualification that everything will be returned as BytesType?

Sounds reasonable to me.","10/Jul/12 17:44;slebresne;bq. disable write access to cql3 cfs from thrift? And leave read access with the qualification that everything will be returned as BytesType?

A variation in the same spirit could be to disable access to CQL3 table by default but add a thrift debug mode (either per-connection through or globally through JMX if we don't want to add a new thrift method) that would make thrift disable every validation. That way, by default the message that CQL3 tables should be access through CQL3 would be clear but we would have low-level read/write for debugging.  ","10/Jul/12 17:46;thepaul;bq. disable access to CQL3 table by default but add a thrift debug mode

+lots.",10/Jul/12 17:53;nickmbailey;sgtm,"10/Jul/12 18:15;jbellis;+1

(and I'd prefer JMX otherwise I foresee clients using this as an escape hatch and fubarring things up.)","10/Jul/12 22:38;thepaul;So, from an outside conversation, it sounds like there may be some confusion on exactly what is being discussed here. Are we talking about disabling Thrift access entirely to columnfamilies which use named metadata with composites, or are we just talking about not supporting Thrift addressing columns inside composites by their CQL3 names?

The second seems eminently reasonable. The former sounds crazy.","11/Jul/12 07:36;slebresne;bq. are we just talking about not supporting Thrift addressing columns inside composites by their CQL3 names

That is not what I was talking about, but I don't even understand how we could ever support that.

bq. disabling Thrift access entirely to columnfamilies which use named metadata with composites

That is what I'm talking about (though to be precise, it would be for composites using named metadata *created through CQL3*). Anyway, I don't know if that's so crazy but in any case calling it crazy doesn't help solving the problem.

","30/Jul/12 23:26;nickmbailey;So here is where I ended up on this.

Assuming we aren't extremely interested in fixing bugs like these when the come up then I'm all for disabling thrift access by default to cql3 cfs. From what I can tell there isn't an immediately easy way to know a cf was created through cql3 at the moment but we could add that I guess.

On the other hand if we want to just go ahead and fix these bugs when they happen, I don't see much reason to go out of our way to disable thrift access to cql3 cfs.","09/Aug/12 15:17;jbellis;bq. one has to know the CQL schema definition to be able to create the column in the first place (or deserialize it on read)

Can you remind me why we don't translate {{PRIMARY KEY(gid, period, tid)}} into a comparator of {{CompositeType(Int32Type, BytesType, UTF8Type)}}?  (period -> int, tid -> bytes, sparse columns -> utf8)","09/Aug/12 15:41;slebresne;bq. Can you remind me why we don't translate PRIMARY KEY(gid, period, tid) into a comparator of CompositeType(Int32Type, BytesType, UTF8Type)

We do. The problem is actually with columns that are not part of the key. Let me try to sum that up:
* Pre-CQL3, the ColumnDefinition name was always a full column name.
* In CQL3, the ColumnDefinition name only correspond to one of the component of the column name, i.e. to the UTF8Type component above.
* To be able to distinguish both case internally, we've introduce the componentIndex field in ColumnDefinition. However, we decided that we didn't wanted to expose this field to thrift, and so we don't expose to thrift the ColumnDefinition from CQL3 table.

The net result is that as far as thrift is concerned, the CQL3 tables have no columns_metadata whatsoever. It follows that thrift clients don't know what are the correct value for the last UTF8Type component, and don't know what is the type of the corresponding value (and thus cannot serialize/deserialize said value correctly). ","09/Aug/12 15:53;jbellis;So basically, the problem is we're trying to maintain compatibility with (non-cql3) wide-row named columns?   If we're willing to break that scenario, does the problem go away?

I still think that naming wide row columns is nonsensical, but we could add an extra layer of protection by warning on startup in 1.2 that you need to update your schema:

- if you have named columns and non-utf8-or-bytes comparator
- if you have named columns but metadata shows more columns than names","09/Aug/12 16:41;slebresne;I'm not sure I understand what's a named columns above to be honest.

There is basically two informations from CFMetadata you need to know to insert a column correctly in a table (CQL3 or no CQL3): the comparator and *all* of column_metadata. The comparator is necessary to know what is a valid column name and the column_metadata is necessary to know what is a valid column value (I'm simplifying a bit, I'm assuming that the key_validation and default_validator are BytesType but that doesn't matter for the problem at hand).

Now the problem is that for any table created through CQL3 that doesn't use COMPACT STORAGE (let's call those CQL3 tables), all the ColumnDefinition of column_metada will have a componentIndex. So none of those ColumnDefinition are exposed in thrift. In practice it means that if I do:
{noformat}
CREATE TABLE user {
    user_id blob PRIMARY KEY,
    name text,
    age int
}
{noformat}
then if a thrift client do a describe, it will basically get:
{noformat}
comparator = CompositeType(UTF8Type) // it's a composite so that we can add collection later on
column_metadata = []
{noformat}

At that point we have two slightly separate problems:
# Even if a user produces a valid column, with say a composite name being ""age"" and a value being an int, then currently the code throw an exception. Fixing that exception is the goal of the attached patch (though it would have to be updated to work with collections in 1.2). I'm fine fixing that, though I'm pointing that there is a second, more general problem.
# Since the thrift client doesn't know about the actual column_metadata, how can we expect it to correctly insert data. In particular I'm pretty sure higher level clients like pycassa or astyanax will serialize data incorrectly if they don't know the right value validator. Besides, there is many way to be confused if you use a CQL3 table from thrift. For instance if you create the wrong column (i'ts enough to mess up the case), you'll be surprised to not be able to access it when you go back to CQL3. So be clear, I do am suggesting that we don't allow accessing table created from CQL3 *without* COMPACT STORAGE from thrift, because I think it will be more sane, even if it does mean that you're not coming back from CQL3 once you've start really using it.
","09/Aug/12 17:07;jbellis;Right, so what I was saying was, if we're willing to say that columndefinition name is always the cql3 column name, then we don't need componentIndex, at the price of potentially breaking a corner case in non-cql3 schemas.","09/Aug/12 17:33;slebresne;But what the componentIndex give us is which of the composite component is the cql3 column name. Typically, with collections, it's not even necessarily the last of the component. Now, if you know the table is a CQL3 one, you could try to infer which component it is by saying that it's the last component, except if the last is a collection type, in which case that's the previous one, but that feels a bit messy. And besides, thrift client libraries don't have a simple automatic way to know if the table is a CQL3 one in the first place. I guess you could say that if you have a composite comparator *and* some column_metadata then you are likely a CQL3 table, but again, not very clean imo.


At least internally I would be in favor of keeping the componentIndex as it is cleaner. I guess we could start returning the ColumnDefinition from CQL3 table without the componentIndex and let thrift client infer what they can. As said, I still think using CQL3 table from thrift has other way to be confusing, but why not.","17/Aug/12 16:00;jbellis;Here's what I think our goals are, in order of importance:

# Existing ""high level"" clients should have a reasonable upgrade path to read and update collections and cql3 CFs.
# CLI and other tools that don't speak cql should have a way to tell that they can't cope with CQL3 CF definitions.  This is the problem Nick described originally in this ticket description.  Actually making such tools able to manipulate CQL3 definitions is NOT a goal, but we should, as Nick says, make that more obvious.
# We should allow updates to CQL3 CFs from Thrift, if someone manually composes the correct CompositeType bytes.  This is what most of the rest of the discussion here involves.

Analysis:

# This we have done--they will have to use cql-over-thrift, but IMO this is reasonable.  Thrift RPC methods to deal with collections have never been on the roadmap.
# This is tough since if this involves new information (like exposing component_index, or even adding a cql3 boolean to CfDef) then old tools by definition won't know about it.  *Proposal:* what if we omit CQL3 CfDefs from those we return to describe_keyspace[s] calls?  Not quite as good as returning a CfDef that explicitly says ""I am here but you can't touch this,"" but compare to displaying incomplete information (that the cli doesn't know is incomplete) it's much more obvious that the cli and other old thrift-base schema manipulators can't cope with such.
# Sylvain mentioned adding a thrift or JMX method to enable validation-free updates but I'd rather make ThriftValidation cql3-aware, which would let this work without any special flags.  I don't see any downside to this except added complexity for ThriftValidation.","17/Aug/12 16:11;jbellis;I do think we should push this to 1.2 however, since I'm leery of changing the behavior or describe_keyspace[s] when we are fairly deep into 1.1's stable lifetime.","22/Aug/12 09:50;xedin;CQL3 defined CFs won't be exposed to Thrift API anymore (with warning), I also checked CassandraServer and ThriftValidation and figured that column name validation already in place via ThriftValidation.validateColumnNames(...).","31/Aug/12 15:25;nickmbailey;Just glanced at the patch, but it looks like this just makes cql3 cfs not show up in describe_keyspaces calls right? Reading/writing to the cf would still go through and error?

If I'm looking at that right then it seems like we would want the opposite behavior. At least from my perspective, I would like to have an indication that the cf exists in my client even if i can't write to it.","31/Aug/12 23:43;jbellis;The idea was to do two things:

- update ThriftValidation to be cql3-aware, so if a Thrift user manually composes a valid column, we accept it
- but, we don't expose cql3 CFs via describe_keyspaces since clients do not have enough information to generate valid requests automatically",03/Sep/12 09:21;slebresne;I agree that the two points above (make ThriftValidation cql3-aware but don't expose cql3 CFs defs) make sense as a strategy.,"06/Sep/12 11:16;slebresne;On Pavel's patch:
* I'm not a fan of logging a warning (or to log anything really) if someone has CQL3 CFs. We're trying to push CQL3 as a good thing, let's not log anything that could be interpreted as if something was wrong/abnormal.
* The check is excluding composite CF without any ColumnDefinition while it shouldn't.

Attaching a v2 that:
* Fix the two remarks above.
* Rename the check as isThriftIncompatible. I think it's more about detecting CF definitions that cannot be exploided fully by thrift rather than discriminate between what is a thrift CF and CQL3 CF. Especially since the intersection between those two notions is not empty.
* Ship the changes to ThriftValidation from my first patch, though modified a bit to be more generic and handle correctly collections. I'll note that this part makes validation potentially iterate over all ColumnDefinition for composite CF, but that's not really an issue since composite CF created on the thrift side are almost guaranteed to have no ColumnDefinition.
","06/Sep/12 11:42;xedin;bq. I'm not a fan of logging a warning (or to log anything really) if someone has CQL3 CFs. We're trying to push CQL3 as a good thing, let's not log anything that could be interpreted as if something was wrong/abnormal.

Wouldn't that create confusion when some CFs are visible through Thrift and some are not or do we rely on that users should know what is so special about CQL3 CFs?","06/Sep/12 11:52;slebresne;bq. Wouldn't that create confusion when some CFs are visible through Thrift and some are not

I'm not saying this shouldn't be documented at all, but merely that it's a documentation issue and as such logging it at each startup is not the right place. ","06/Sep/12 11:57;xedin;If everyone else is ok with that, lgtm.","11/Sep/12 02:36;jbellis;So, this is good to commit?","11/Sep/12 09:03;slebresne;Alright, committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,
Setting TTL to Integer.MAX causes columns to not be persisted.,CASSANDRA-4771,12610619,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,dbrosius@apache.org,tnine,tnine,05/Oct/12 19:34,12/Mar/19 14:15,13/Mar/19 22:27,24/Jan/18 16:49,1.1.6,,,,,0,,,,,,,"When inserting columns via batch mutation, we have an edge case where columns will be set to Integer.MAX.  When setting the column expiration time to Integer.MAX, the columns do not appear to be persisted.

Fails:

Integer.MAX_VALUE 
Integer.MAX_VALUE/2

Works:
Integer.MAX_VALUE/3

",,,,,,,,,,,,,,,,,,,07/Oct/12 13:15;dbrosius@apache.org;4771.txt;https://issues.apache.org/jira/secure/attachment/12548161/4771.txt,09/Oct/12 03:15;dbrosius@apache.org;4771_b.txt;https://issues.apache.org/jira/secure/attachment/12548355/4771_b.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-07 13:00:45.804,,,no_permission,,,,,,,,,,,,244052,,,Wed Jan 24 16:49:18 UTC 2018,,,,,,0|i05fkn:,29620,slebresne,slebresne,,,,,,,,,,"07/Oct/12 13:00;dbrosius@apache.org;The value written is delta-ed from the current time as

(System.currentTimeMillis() / 1000) + timeToLive

which causes the written ttl to go negative

the error back to the client could be better, and perhaps pre-flighted.",07/Oct/12 13:15;dbrosius@apache.org;fix preflighting to catch ttl too large problems. 4771.txt,"08/Oct/12 14:02;slebresne;I think we'll want to fix that for CQL too (in ModificationStatement.validate(); and yes, that would be nice to not have duplication of the validation code but ...). Also, the patch is against trunk, we should push that to 1.1. Last thing: just returning ""ttl too large"" doesn't really help to know why it is too large, and it can be hard to check in the application that your ttl won't be too large. Maybe it would be simpler to pick a fixed max TTL value, like say 20 years (which would give us something like 16 years to lift that limitation)?","09/Oct/12 03:15;dbrosius@apache.org;makes sense

1) patch against 1.1
2) limit to 20 years
3) exception contains requested vs max limit
4) added to cql","09/Oct/12 06:58;slebresne;nit: I would have put MAX_TTL in ExpiringColumn rather than IColumn.

But +1 in any case.",10/Oct/12 02:00;dbrosius@apache.org;committed to 1.1 as 46fc843bbd39bf6b007fb6d1c8e823f8b3ba2425,13/Mar/14 17:21;rbfblk;Isn't this going to break again in less than 4 years when (now + 20 years) > 2038-01-19?,"24/Jan/18 12:54;christianmovi;[~rbfblk]: You are a smart guy. It indeed broke 4 years later :)

 

Edit: I will buy you a beer, should we ever meet ;) ","24/Jan/18 13:30;christianmovi;Can't we get another 68 years if we cast to long instead of int?

 

Class SerializationHeader:
{code:java}
public int readLocalDeletionTime(DataInputPlus in) throws IOException
{
return (int)in.readUnsignedVInt() + stats.minLocalDeletionTime;
}{code}
 ","24/Jan/18 16:00;rbfblk;[~christianmovi]: I forgot about this! I just tried on a 2.1 cluster and have also confirmed it is broken. I get an AssertionError back when doing an insert with ttl 630720000 (20 years). Are you on Cassandra 3+? I was hoping this was fixed as part of CASSANDRA-8099, but from that snippet I guess not.

It looks like expiration time is encoded per cell as an offset in seconds from the minimum deletion time in the SSTable. If I'm reading it right the offset is serialized as a variable length integer, so reading it into a long would give a very large range of offsets that would be more than sufficient. However it looks to me like the minimum deletion time at the SSTable metadata level is saved as a fixed-size 4-byte signed integer. Without changing the serialization format we could change the interpretation of that metadata field to assume it is an unsigned integer (and de-serialize it also into a long variable) which should indeed give us another 68 years. A better fix would probably be to also change the serialization format of the minLocalDeletionTime to an 8 byte integer or a variable size integer so it could hold values in the far far future. Changing the SSTable format might be a 4.0+ thing though. This probably needs a new ticket.

I would gladly accept that beer! But I probably deserve negative points for not following up on my comment several years ago...","24/Jan/18 16:11;christianmovi;[~rbfblk]: Nop, it seems it was not fixed with the refactoring.

For now changing the typecast to signed int would be a big improvement already.

 Edit: It happened in C* 3.0.15","24/Jan/18 16:49;pauloricardomg;Normally we don't reopen issues which were already released, the idea is to open a new issue if there is a regression of a previous defect. Since there is CASSANDRA-14092 for the reappearance of this issue I will close this and add a link to this on that ticket. Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CFs should have globally and temporally unique CF IDs to prevent ""reusing"" data from earlier incarnation of same CF name",CASSANDRA-5202,12629944,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,marat,marat,30/Jan/13 17:17,12/Mar/19 14:14,13/Mar/19 22:27,15/Jan/14 03:13,2.1 beta1,,,,,9,qa-resolved,test,,,,,"Attached is a driver that sequentially:

1. Drops keyspace
2. Creates keyspace
4. Creates 2 column families
5. Seeds 1M rows with 100 columns
6. Queries these 2 column families

The above steps are repeated 1000 times.

The following exception is observed at random (race - SEDA?):

ERROR [ReadStage:55] 2013-01-29 19:24:52,676 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[ReadStage:55,5,main]
java.lang.AssertionError: DecoratedKey(-1, ) != DecoratedKey(62819832764241410631599989027761269388, 313a31) in C:\var\lib\cassandra\data\user_role_reverse_index\business_entity_role\user_role_reverse_index-business_entity_role-hf-1-Data.db
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:60)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:67)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:79)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:256)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:64)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1367)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1229)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1164)
	at org.apache.cassandra.db.Table.getRow(Table.java:378)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:69)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:822)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1271)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


This exception appears in the server at the time of client submitting a query request (row slice) and not at the time data is seeded. The client times out and this data can no longer be queried as the same exception would always occur from there on.

Also on iteration 201, it appears that dropping column families failed and as a result their recreation failed with unique column family name violation (see exception below). Note that the data files are actually gone, so it appears that the server runtime responsible for creating column family was out of sync with the piece that dropped them:

Starting dropping column families
Dropped column families
Starting dropping keyspace
Dropped keyspace
Starting creating column families
Created column families
Starting seeding data
Total rows inserted: 1000000 in 5105 ms
Iteration: 200; Total running time for 1000 queries is 232; Average running time of 1000 queries is 0 ms
Starting dropping column families
Dropped column families
Starting dropping keyspace
Dropped keyspace
Starting creating column families
Created column families
Starting seeding data
Total rows inserted: 1000000 in 5361 ms
Iteration: 201; Total running time for 1000 queries is 222; Average running time of 1000 queries is 0 ms
Starting dropping column families
Starting creating column families
Exception in thread ""main"" com.netflix.astyanax.connectionpool.exceptions.BadRequestException: BadRequestException: [host=127.0.0.1(127.0.0.1):9160, latency=2468(2469), attempts=1]InvalidRequestException(why:Keyspace names must be case-insensitively unique (""user_role_reverse_index"" conflicts with ""user_role_reverse_index""))
	at com.netflix.astyanax.thrift.ThriftConverter.ToConnectionPoolException(ThriftConverter.java:159)
	at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:60)
	at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:27)
	at com.netflix.astyanax.thrift.ThriftSyncConnectionFactoryImpl$1.execute(ThriftSyncConnectionFactoryImpl.java:140)
	at com.netflix.astyanax.connectionpool.impl.AbstractExecuteWithFailoverImpl.tryOperation(AbstractExecuteWithFailoverImpl.java:69)
	at com.netflix.astyanax.connectionpool.impl.AbstractHostPartitionConnectionPool.executeWithFailover(AbstractHostPartitionConnectionPool.java:255)
	at com.netflix.astyanax.thrift.ThriftKeyspaceImpl.createKeyspace(ThriftKeyspaceImpl.java:569)
	at com.nuance.mca.astyanax.App.recreateKeyspaceSchema(App.java:139)
	at com.nuance.mca.astyanax.App.main(App.java:88)
Caused by: InvalidRequestException(why:Keyspace names must be case-insensitively unique (""user_role_reverse_index"" conflicts with ""user_role_reverse_index""))
	at org.apache.cassandra.thrift.Cassandra$system_add_keyspace_result.read(Cassandra.java:30010)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_system_add_keyspace(Cassandra.java:1285)
	at org.apache.cassandra.thrift.Cassandra$Client.system_add_keyspace(Cassandra.java:1272)
	at com.netflix.astyanax.thrift.ThriftKeyspaceImpl$14.internalExecute(ThriftKeyspaceImpl.java:584)
	at com.netflix.astyanax.thrift.ThriftKeyspaceImpl$14.internalExecute(ThriftKeyspaceImpl.java:572)
	at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:55)
	... 7 more



","OS: Windows 7, 
Server: Cassandra 1.1.9 release drop
Client: astyanax 1.56.21, 
JVM: Sun/Oracle JVM 64 bit (jdk1.6.0_27)",,,,,,,,,,,,,CASSANDRA-4687,CASSANDRA-6060,,,,10/Jan/14 00:23;yukim;0001-make-2i-CFMetaData-have-parent-s-CF-ID.patch;https://issues.apache.org/jira/secure/attachment/12622301/0001-make-2i-CFMetaData-have-parent-s-CF-ID.patch,10/Jan/14 00:23;yukim;0002-Don-t-scrub-2i-CF-if-index-type-is-CUSTOM.patch;https://issues.apache.org/jira/secure/attachment/12622302/0002-Don-t-scrub-2i-CF-if-index-type-is-CUSTOM.patch,10/Jan/14 00:23;yukim;0003-Fix-user-defined-compaction.patch;https://issues.apache.org/jira/secure/attachment/12622303/0003-Fix-user-defined-compaction.patch,10/Jan/14 00:23;yukim;0004-Fix-serialization-test.patch;https://issues.apache.org/jira/secure/attachment/12622304/0004-Fix-serialization-test.patch,10/Jan/14 17:45;yukim;0005-Create-system_auth-tables-with-fixed-CFID.patch;https://issues.apache.org/jira/secure/attachment/12622407/0005-Create-system_auth-tables-with-fixed-CFID.patch,10/Jan/14 23:10;iamaleksey;0005-auth-v2.txt;https://issues.apache.org/jira/secure/attachment/12622458/0005-auth-v2.txt,03/Jan/14 02:44;yukim;5202.txt;https://issues.apache.org/jira/secure/attachment/12621224/5202.txt,30/Jan/13 17:19;marat;astyanax-stress-driver.zip;https://issues.apache.org/jira/secure/attachment/12567169/astyanax-stress-driver.zip,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2013-01-30 18:44:25.35,,,no_permission,,,,,,,,,,,,310440,,,Thu Mar 20 15:22:09 UTC 2014,,,,,,0|i1hkqn:,310785,xedin,xedin,,,,,,,,,shawn.kumar,"30/Jan/13 17:19;marat;Unpack, run: mvn install, mvn eclipse:eclipse",30/Jan/13 17:35;marat;The sever is a single node Cassandra 1.1.9 release drop and there is one and only client running at the same time.,"30/Jan/13 17:47;marat;Also, I can no longer create this column family - even though when I try to drop it - it errors with an exception that this column family does not exists, but then when I try to create it I get an exception that ""why:Keyspace names must be case-insensitively unique (""user_role_reverse_index"" conflicts with ""user_role_reverse_index""""","30/Jan/13 18:44;yukim;Thanks Marat, your testcase greatly helped me find the cause.
I tested your driver against current 1.1 branch.

The key cache is kept in memory beyond drop/create keyspace.
So you create first ks/cf and do insert/read, which causes key cache to fill up.
Then, you drop ks/cf and do the same insert/read again. This time in read, it finds the reading position from key cache from previous read, but that position is for dropped ks/cf not the current one.

We definitely need to clear caches when we drop columnfamilies.

I think this is also the cause of CASSANDRA-4687.","30/Jan/13 18:53;jbellis;The cache key is a {{Pair<Descriptor, DecoratedKey>}} -- I would expect that to be robust across drop/recreate unless there's a bug in Descriptor HashCode/Equals.","30/Jan/13 19:46;marat;Thanks Yuki. When do you expect the fix to be available for testing and released? 
Also, what about failure to recreate column family because system thinks that it still exists even though the data files are gone?","30/Jan/13 19:56;yukim;When cf is created again, it starts with file generation of 1 because SSTables are once removed when cf is dropped. That makes Descriptor collision of newly created SSTables and Descriptors in key cache.

Patch attached to clear key cache when cf is dropped.",30/Jan/13 20:12;jbellis;Is this race-proof vs cache additions from concurrent reads?,"30/Jan/13 20:20;jbellis;One way to race-proof it would be to

# make Descriptor use CFMetadata.cfId uuid instead of ksname/cfname (bonus: would make lookups faster).  May have to make a separate class instead of actual Descriptor since that's really more about FS-level data.
# make CFM.getId generate random uuids instead of deterministic ones","30/Jan/13 23:09;marat;Clarification. I think what happened at iteration 201 was that dropping of keyspace failed, so the subsequent attempts of running the driver would fail as dropping column families and dropping the keyspace were part of the same try-catch block, and attempt to drop non existing column family would throw and so the code would attempt to create an already existing keyspace, which was not being dropped because an exception thrown by column family drop never gave column family drop a chance to execute.

So the issue was not in dropping the column family but rather the keyspace.","11/Mar/13 18:41;yukim;First try for random CF Id for trunk: https://github.com/yukim/cassandra/commits/5202-3

I added 'cf_id' column to schema_columnfamilies to track current CF Id so I made this patch to trunk. I think this should work on 1.2 branch though.
I still need to work on loading saved key cache, because when opening files like saved key caches, those would have the CF ID at the time of opening because right now only keyspace and column family names are available. Same thing can be said to SSTables.
I think it is better to store CF ID along with the files so that we can determine which version those files are written. But I'dont want to embed UUID cfId to file name.

I'm considering to modify file format, so that saved key cache has CF ID in its header. But for SSTables, I don't have better way to embed CF ID.","13/Mar/13 14:46;copumpkin;I was getting the same issue and found this bug. As a workaround, I tried changing my testsuite to avoid dropping the keyspace each time, and simply truncate every CF in the keyspace instead, but that seems to lead to the same symptoms. Just thought I'd point that out in case the truncate code path was being overlooked.","13/Mar/13 15:04;jbellis;bq. I don't want to embed UUID cfId to file name

It's ugly, but that's the logical solution, isn't it?  We're saying that sstables for table foo on Tuesday, and table foo on Wednesday after drop and re-create, are really different things, so distinguishing them as such on the FS is if nothing else a good extra layer of protection against mixing the two.

And your patch already stores cfid in the system table, so users could still find out which sstables belong to which table definitions, although granted it's not as convenient.","13/Mar/13 15:07;jbellis;bq. truncate ... seems to lead to the same symptoms

I think we can fix that by recording the sstable generation somewhere, instead of just assuming that ""if no files of generation >= X exist, we should start creating new sstables with X.""

(Which I think would actually be a simpler fix for the issue in general...  but giving different ""versions"" of a table distinct IDs is the Right Solution, so I'm still +1 on that unless Yuki thinks the cost is too high for the practical benefits.)","19/Mar/13 02:12;jjordan;bq. I don't want to embed UUID cfId to file name
I like this idea a lot.  I have seen some users with issues of files not be deleted after truncate/drop cf, but the cf is dropped from the schema.  On re-create those old files are going to get re-imported.  Now, this does make it a pain for when you want to restore a cf to another cluster, or to the same cluster after a drop, as you have to rename all the files, or bulk load them and make bulk loader ignore the UUID or something...
","19/Mar/13 13:27;jbellis;-For truncate, we added some code in CASSANDRA-4940 that attempts to fix it but it is race-prone (a new entry can be added to the cache, even after it's cleared).  So we should remove that code as part of our fix.- (Edit: this was row cache, not key cache.)","19/Mar/13 17:52;copumpkin;As an additional data point around our truncate issues, it seems like although ALTER cf WITH caching = 'none' helps significantly with the drop/create-related issues, it doesn't seem to help with the truncate issues. On the other hand, disabling key cache altogether in cassandra.yaml seems to help permanently, and using nodetool invalidatekeycache helps temporarily until we access keys after a truncate again. It seems to suggest that caching = 'none' doesn't disable key cache use altogether (perhaps if one already existed before the ALTER, it stays in use?)

Does that jibe with everyone's current understanding of the causes of the issue?","20/Mar/13 19:57;yukim;I took the route to add CFID to file name for both SSTable and serialized cache. https://github.com/yukim/cassandra/commits/5202-4

File's CF ID is checked on CF initialization. I did't change the behavior of sstableloader so it can still stream to the CF of different version.
As Jeremiah stated above, downside is that you cannot just place the file from backup and restore CF without changing the filename.

For truncate issue, I want to see more detail about your operation and setup. SSTable generation number is not reset when you truncate, so usually there is no collision between the generation in keycache and in sstable after truncate.","26/Mar/13 15:41;jbellis;Since we're talking 2.0 anyway for CFID-in-the-filename...

What if we switched to using CAS for CFID creation?  Then everyone would agree on the same CFID, and we could use that in our MessagingService instead of table + cf strings.  (We could also use an int instead of a UUID.)

Would that make any difference here?  Or should we just proceed with this change now and worry about ""global CFIDs"" later?","26/Mar/13 17:04;yukim;If we want to switch (back again) to int for CFID, then CAS would be good.
But we already use UUID for that, I don't feel we need to switch.
","27/Mar/13 16:54;copumpkin;Yuki: I'll try to reduce the truncate issue we're seeing when I get some time. It definitely appears connected to this, and might suggest that there's a broader issue here. 

For what it's worth, we're now randomly also getting the following error message, as described in the closely related CASSANDRA-4687 bug:

Caused by: java.io.EOFException: unable to seek to position X in Y (Z bytes) in read-only mode

also cured by clearing the key caches.
","12/Aug/13 17:21;yukim;Attaching patch which is rebased version of my previous 5202-4 branch against cassandra-2.0.0.

It adds UUID CF ID to SSTable descriptor(== file name).
I tested upgrading from latest 1.2, bulk loader, and truncate and they all worked fine.

I think 2.0 is good number for this change, but it is now on RC1 stage, I'm not sure we should wait till 2.1.","12/Aug/13 21:32;jbellis;So right now our file layout looks like

ks/cf/ks-cf-version-generation-component.db

This would change it to

ks/cf/ks-cf-uuid-version-generation-component.db

Could we clean up the redundancy a little by moving the ID into the directory name?

ks/cf-uuid/version-generation-component.db

","13/Aug/13 12:49;iamaleksey;I've got a feeling that this patch breaks (concurrent) auth setup and certain DSE tables setup as well (all issuing CREATE TABLE statements for each of the required tables on startup, unless a table already exists). With non-deterministic cf uuids they'll potentially end up with different ids. Then, when schema versions converge to a single id per table, we might lose data (on the nodes that 'lost').","13/Aug/13 13:01;iamaleksey;We could work around by parsing the create statements instead, setting the CFMetaData id to a predetermined one and manually calling announceMigration(), or adding something like 'WITH id = <uuid>' syntax to CREATE TABLE for these special cases.","13/Aug/13 13:39;jbellis;What if we were willing to give up automagic auth setup?  ""Run this script to enable auth on your cluster"" seems reasonable to me.",13/Aug/13 13:43;iamaleksey;Auth is just one example. DSE would be affected as well. Probably others who rely on 1.2 schema code with deterministic ids.,13/Aug/13 13:45;iamaleksey;Auth is the simplest case - there *is* a workaround *and* we control it. I'm more worried about surprising other users unpleasantly.,"13/Aug/13 13:56;jbellis;# I'm 100% convinced that (a) this is a problem worth fixing, and (b) we need to switch to nondeterministic IDs to fix it.  Deterministic IDs was, in retrospect, a mistake.  (More pedantically: it's not nondeterminism that fixes it, but unique IDs per unique CREATE; as I've said, an int id incremented via CAS would also work.)  So, you have an uphill battle if you are arguing that it's fine the way it is!
# Adding a ""WITH ID"" workaround doesn't break people any less, it's just a different (hackier, IMO) workaround than moving things to an external script.
# Startup is messy enough without auth/dse special cases, moving them out of core into scripts that can run ""normally"" sounds like a fantastic idea to me even without uuid breakage as a motivation.
# I would be shocked if there is anyone else besides DSE abusing the internals this way, but anyone who does surely understands that if you wire yourself that closely to the internals, you have absolutely no grounds to complain if we break compatibility.",13/Aug/13 14:03;iamaleksey;I'm not saying that it's worth fixing. Just mentioning what gets broken once the patch is committed.,13/Aug/13 14:50;jbellis;I do think you are right in that we should do this in 2.1 rather than 2.0.1.,"13/Aug/13 15:05;yukim;Targeting this to be fixed in 2.1.

[~jbellis] I think we can change path and file name. I also want to try and see how switching to CAS would fix this.","13/Aug/13 23:49;xedin;I actually looked at CASSANDRA-4687 yesterday myself and I came to the same conclusions. I actually like [~jbellis] proposition to include uuid into directory name, instead of including it into file name, which would make it a lot simpler to migrate sstables over to different cluster or backup.

I also think that it could be a good bonus to also clean the cache because of LRU nature of it, if previous entries where hot enough it would just waste time/work to push them out where we would still suffer cache miss every time we read from new cf (in multi ks/cf setup that is still fine as index read is not that bad).",20/Sep/13 16:14;jbellis;CASSANDRA-6060 is related since it also contemplates changing CFID assignment (back to unique ints via CAS).,07/Oct/13 19:14;jbellis;Is there anything that we want to do as part of this ticket instead of 6060?,"07/Oct/13 19:21;yukim;Add CF ID to directory name if we still want to distinguish one KS/CF directory to another.
Updating key cache key to use CF ID is another one, but I think that will be done through 6060.","07/Oct/13 19:27;jbellis;bq. Add CF ID to directory name if we still want to distinguish one KS/CF directory to another.

All right, I'm down to narrow the scope here to that.","03/Jan/14 02:44;yukim;(also: https://github.com/yukim/cassandra/commits/5202)

Patch attached for review.

* CF ID is generated randomly upon new CFMetaData creation.
    CF under system keyspaces and ones from older version have deterministic CF ID based on their name.
* SSTable directories are created as ks/cf-cfid. cfid here is hex encoding of UUID bytes. When upgrading, older format ks/cf is still used.
* Saved key cache file name also has cfid appended at the end, and key cache look up is CF ID aware.
",03/Jan/14 23:25;xedin;@jbellis I can review in case Markus has more important stuff on his plate.,"04/Jan/14 00:57;jbellis;Thanks, Pavel!",07/Jan/14 06:32;xedin;+1,"07/Jan/14 16:35;yukim;Thank you, Pavel.
Committed with update on NEWS.txt.","08/Jan/14 21:29;yukim;Some unit test failures revealed I have to work on this a little bit more.
With committed version, secondary indexes can get wrong directory.
Will post fix here.","08/Jan/14 21:33;iamaleksey;[~yukim] any way you could alter auth setup code (creation of auth tables) to use the old deterministic cfIds explicitly, while at it?",08/Jan/14 21:56;yukim;[~iamaleksey] sure. Is it just enough to special case tables under 'system_auth' keyspace?,"08/Jan/14 21:57;xedin;[~yukim] Sorry, Yuki, I completely forgot about secondary indexes, tested standard scenarios with different setups with stress tho. Will be happy to review v2.",08/Jan/14 22:01;iamaleksey;[~yukim] I was thinking about announcing the migration for all the auth tables with cf_id preset to what 2.0 would've generated. No special cases anywhere else.,"10/Jan/14 00:23;yukim;0001: Fix for 2i CF

I decided to force 2i CF to have the same CF ID from parents'. I think this is safe because 2i CFs are never managed by Schema.

0002: Fix scrub directory for 2i CF

had to exclude scrubbing CUSTOM index type.

0003: Fix user defined compaction

User defined compaction via JMX was broken. It assumes SSTable files are under the directory with older name format.

0004: Fix SerializationsTest
Serialization test on Row object was failing. This is due to the serialized CF ID on test file not matching non deterministic CF ID generated for every test run.
I decided to just generate test file every time.

> [~iamaleksey]

Sorry, still figuring out how to handle auth.
I think current code is sufficient to handle update from older version or setting up authnz. When upgrade, it uses existing directory with deterministic CF ID. Is there any concern that I'm not aware of?

Anyway, I have a feeling we need some way of forcing CF ID, especially when CASSANDRA-6060 comes in. ""WITH ID=xxx"" seems good to me. Maybe separate ticket?","10/Jan/14 00:27;iamaleksey;[~yukim] Upgrading's not a concern. Concurrent auth CFs creation on fresh 2.1 is, though - they will end up w/ different cf ids. So yes, we need to force a deterministic one.","10/Jan/14 00:44;iamaleksey;[~yukim] Specifically, instead of simply executing those ""CREATE TABLE""s in PA, CA, and Auth, prepare them, and extract the generated CFMetaData object. Then set cf_id to the deterministic ones (pre-2.1 ones), explicitly. And then call MigrationManager.announceNewColumnFamily() manually, with that final CFMetaData, with the cf_id preset.

This way you only have to change the setup code in PasswordAuthenticator/CassandraAuthorizer/Auth, and not special-case anything beyond that.

Am I making any sense? ","10/Jan/14 17:45;yukim;[~iamaleksey] 0005 attached for system_auth tables with fixed CF ID as we have currently.
Can you give it a review?","10/Jan/14 23:10;iamaleksey;[~yukim] LGTM. Attaching a v2 though, b/c I don't want to involve overriding the parameters set by newSystemMetadata(), uncluding memtableFlushPeriod, that probably wasn't being overriden at all - just the cf id.","13/Jan/14 14:18;yukim;[~iamaleksey] thanks, I like your approach.

[~xedin] can you review rest of the patches(0001-0004)?","14/Jan/14 21:23;xedin;[~yukim] Looked at 1-4 patches, everything looks good but I have one question to clarify regarding #3 - we can do sstable.descriptor.equals(descriptor) now as ""descriptor"" would also have an absolute path because of the new ""find"" method, is that correct?","14/Jan/14 22:20;yukim;Yes, that is my intention. Since descriptor begins to get directory part from Directories, there is no need to compare file names in string.","15/Jan/14 00:07;xedin;sounds good to me, +1.","15/Jan/14 03:13;yukim;Committed, thanks!","20/Feb/14 19:59;jbellis;Did we give up on this?

bq. Could we clean up the redundancy a little by moving the ID into the directory name?  e.g., ks/cf-uuid/version-generation-component.db

I'm worried about path length, which is limited on Windows.

Edit: to give a specific example, for KS foo Table bar we now have

/var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/foo-bar-ka-1-Data.db

I'm proposing

/var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/ka-1-Data.db
",20/Mar/14 15:22;brandon.williams;Perhaps we should open a new ticket for that?
AssertionError: originally calculated column size of 629444349 but now it is 588008950,CASSANDRA-4206,12553475,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,patrik.modesto,patrik.modesto,01/May/12 11:28,12/Mar/19 14:14,13/Mar/19 22:27,18/Mar/14 14:35,,,,,,11,,,,,,,"I've 4 node cluster of Cassandra 1.0.9. There is a rfTest3 keyspace with RF=3 and one CF with two secondary indexes. I'm importing data into this CF using Hadoop Mapreduce job, each row has less than 10 colkumns. From JMX:
MaxRowSize:  1597
MeanRowSize: 369

And there are some tens of millions of rows.

It's write-heavy usage and there is a big pressure on each node, there are quite some dropped mutations on each node. After ~12 hours of inserting I see these assertion exceptiona on 3 out of four nodes:

{noformat}
ERROR 06:25:40,124 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of 629444349 but now it is 588008950
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:388)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:256)
       at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:84)
       at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:437)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of
629444349 but now it is 588008950
       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
       at java.util.concurrent.FutureTask.get(FutureTask.java:83)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:384)
       ... 7 more
Caused by: java.lang.AssertionError: originally calculated column size
of 629444349 but now it is 588008950
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:161)
       at org.apache.cassandra.db.compaction.CompactionManager$7.call(CompactionManager.java:380)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       ... 3 more
{noformat}

Few lines regarding Hints from the output.log:

{noformat}
 INFO 06:21:26,202 Compacting large row system/HintsColumnFamily:70000000000000000000000000000000 (1712834057 bytes) incrementally
 INFO 06:22:52,610 Compacting large row system/HintsColumnFamily:10000000000000000000000000000000 (2616073981 bytes) incrementally
 INFO 06:22:59,111 flushing high-traffic column family CFS(Keyspace='system', ColumnFamily='HintsColumnFamily') (estimated 305147360 bytes)
 INFO 06:22:59,813 Enqueuing flush of Memtable-HintsColumnFamily@833933926(3814342/305147360 serialized/live bytes, 7452 ops)
 INFO 06:22:59,814 Writing Memtable-HintsColumnFamily@833933926(3814342/305147360 serialized/live bytes, 7452 ops)
{noformat}

I think the problem may be somehow connected to an IntegerType secondary index. I had a different problem with CF with two secondary indexes, the first UTF8Type, the second IntegerType. After a few hours of inserting data in the afternoon and midnight repair+compact, the next day I couldn't find any row using the IntegerType secondary index. The output was like this:

{noformat}
[default@rfTest3] get IndexTest where col1 = '3230727:http://zaskolak.cz/download.php';
-------------------
RowKey: 3230727:8383582:http://zaskolak.cz/download.php
=> (column=col1, value=3230727:http://zaskolak.cz/download.php, timestamp=1335348630332000)
=> (column=col2, value=8383582, timestamp=1335348630332000)
-------------------
RowKey: 3230727:8383583:http://zaskolak.cz/download.php
=> (column=col1, value=3230727:http://zaskolak.cz/download.php, timestamp=1335348449078000)
=> (column=col2, value=8383583, timestamp=1335348449078000)
-------------------
RowKey: 3230727:8383579:http://zaskolak.cz/download.php
=> (column=col1, value=3230727:http://zaskolak.cz/download.php, timestamp=1335348778577000)
=> (column=col2, value=8383579, timestamp=1335348778577000)

3 Rows Returned.
Elapsed time: 292 msec(s).

[default@rfTest3] get IndexTest where col2 = 8383583;

0 Row Returned.
Elapsed time: 7 msec(s
{noformat}

You can see there really is an 8383583 in col2 in on of the listed rows, but the search by secondary index returns nothing.

The Assert Exception also happend only on CF with the secondary index of IntegerType. There were also secondary indexes of UTF8Type and
LongType types. It's the first time I've tried secondary indexes of other type than UTF8Type.

Regards,
Patrik","Debian Squeeze Linux, kernel 2.6.32, sun-java6-bin 6.26-0squeeze1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-02 20:53:57.51,,,no_permission,,,,,,,,,,,,237639,,,Fri Aug 22 16:15:15 UTC 2014,,,,,,0|i0dcnr:,75980,,,,,,,,,,,,02/May/12 20:53;jbellis;Do you have {{multithreaded_compaction}} enabled?,"02/May/12 20:55;jbellis;bq. It's the first time I've tried secondary indexes of other type than UTF8Type

I think that's a red herring; the error occurs when compacting the internal hints columnfamily, which has no indexes on it.","02/May/12 20:56;jbellis;bq. After a few hours of inserting data in the afternoon and midnight repair+compact, the next day I couldn't find any row using the IntegerType secondary index

That part is definitely a [separate] bug, assuming the index completed building.","31/Aug/12 02:02;thobbs;I'm not sure how useful it is at this point, but just for the record I'm seeing what appears to be the same issue with 1.0.7 (which means it shouldn't be CASSANDRA-3579):

{noformat}
 INFO [GossipTasks:1] 2012-08-27 14:22:50,242 Gossiper.java (line 818) InetAddress /xx.xx.178.59 is now dead.
 INFO [GossipStage:1] 2012-08-27 14:22:59,090 Gossiper.java (line 804) InetAddress /xx.xx.178.59 is now UP
 INFO [HintedHandoff:1] 2012-08-27 14:23:41,548 HintedHandOffManager.java (line 296) Started hinted handoff for token: 132332031580364958013534569556798748899 with IP: /xx.xx.178.59
 INFO [HintedHandoff:1] 2012-08-27 14:23:41,870 ColumnFamilyStore.java (line 704) Enqueuing flush of Memtable-HintsColumnFamily@2081164539(597050/47764000 serialized/live bytes, 857 ops)
 INFO [FlushWriter:181] 2012-08-27 14:23:41,870 Memtable.java (line 246) Writing Memtable-HintsColumnFamily@2081164539(597050/47764000 serialized/live bytes, 857 ops)
 INFO [FlushWriter:181] 2012-08-27 14:23:41,959 Memtable.java (line 283) Completed flushing /xx/xx/xx/cassandra/datafile/system/HintsColumnFamily-hc-6730-Data.db (624946 bytes)
 INFO [CompactionExecutor:884] 2012-08-27 14:23:41,961 CompactionTask.java (line 113) Compacting [SSTableReader(path='/xx/xx/xx/cassandra/datafile/system/HintsColumnFamily-hc-6729-Data.db'), SSTableReader(path='/ngs/app/xcardp/cassandra/datafile/system/HintsColumnFamily-hc-6730-Data.db')]
 INFO [CompactionExecutor:884] 2012-08-27 14:23:41,987 CompactionController.java (line 133) Compacting large row system/HintsColumnFamily:31372e33342e3137382e3534 (274816343 bytes) incrementally
ERROR [CompactionExecutor:884] 2012-08-27 14:23:56,322 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:884,1,main]
java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
    at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
    at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
    at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
ERROR [HintedHandoff:1] 2012-08-27 14:23:56,323 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:369)
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:248)
    at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:84)
    at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:416)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:365)
    ... 7 more
Caused by: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
    at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
    at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
    at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    ... 3 more
ERROR [HintedHandoff:1] 2012-08-27 14:23:56,345 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:369)
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:248)
    at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:84)
    at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:416)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:365)
    ... 7 more
Caused by: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
    at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
    at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
    at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    ... 3 more
 INFO [COMMIT-LOG-WRITER] 2012-08-27 14:48:20,812 CommitLogSegment.java (line 60) Creating new commitlog segment /xxx/xxx/xxx/commitlog/CommitLog-1346078900812.log
{noformat}

I should note that this node was upgraded from 0.7.4 about three days prior.",30/Oct/12 23:38;rbranson;Just had this pop up on a 1.1.6 node after doing a 'nodetool removetoken' for a node that had collected a lot of hints.,"01/Mar/13 14:32;joeyi;I am seeing what seems like the same issue on 1.2.2 on nodes that had collected a lot of hints after a node had been down for a couple hours.

{noformat}
ERROR [CompactionExecutor:104] 2013-03-01 00:29:54,211 CassandraDaemon.java (line 132) Exception in thread Thread[CompactionExecutor:104,1,main]
java.lang.AssertionError: originally calculated column size of 350273328 but now it is 350297055
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:163)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:59)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:422)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
ERROR [HintedHandoff:1] 2013-03-01 00:29:54,211 CassandraDaemon.java (line 132) Exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 350273328 but now it is 350297055
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:406)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:252)
        at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:89)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:459)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 350273328 but now it is 350297055
        at java.util.concurrent.FutureTask$Sync.innerGet(Unknown Source)
        at java.util.concurrent.FutureTask.get(Unknown Source)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:402)
        ... 7 more
Caused by: java.lang.AssertionError: originally calculated column size of 350273328 but now it is 350297055
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:163)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:59)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:422)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        ... 3 more
{noformat}","01/Mar/13 15:39;joeyi;Version: 1.2.2 - 6 nodes, RF3
OS: Centos 6.3

Looking closer at the errors, I'm only seeing this for the system/hints:

{noformat}
 INFO [CompactionExecutor:555] 2013-03-01 15:37:10,737 CompactionController.java (line 158) Compacting large row system/hints:384ac791
-d0d7-4f5e-b66b-57af885341d5 (350297131 bytes) incrementally
ERROR [CompactionExecutor:555] 2013-03-01 15:37:28,377 CassandraDaemon.java (line 132) Exception in thread Thread[CompactionExecutor:5
55,1,main]
{noformat}","02/May/13 20:44;arya;I have 1600 compactions pending and no compaction is running, so they are all stuck. I also got this in the logs. And Jonathan, I have multi_threaded_compaction disabled. 

ERROR [CompactionExecutor:111] 2013-05-02 13:54:05,913 CassandraDaemon.java (line 174) Exception in thread Thread[CompactionExecutor:111,1,main]java.lang.AssertionError: originally calculated column size of 1337269150 but now it is 1337269195        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:188)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)

","05/Jun/13 08:43;arodrime;Similar thing over here.

C*1.2.2

I have a lot of hints (3 & 4 GB) on 2 nodes out of 12 due to an unknown issue while growing to multiple datacenter (all the nodes of the new datacenter saw themselves as UNREACHABLE from the cassandra-cli - nodetool ring is ok...).

On these 2 nodes I see now :

{code}
ERROR 08:37:16,878 Exception in thread Thread[CompactionExecutor:1540,1,main]
java.lang.AssertionError: originally calculated column size of 5469343266 but now it is 5469343506
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:163)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:59)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:422)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR 08:37:16,878 Exception in thread Thread[HintedHandoff:176,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 5469343266 but now it is 5469343506
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:406)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:252)
        at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:89)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:459)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 5469343266 but now it is 5469343506
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:402)
        ... 7 more
Caused by: java.lang.AssertionError: originally calculated column size of 5469343266 but now it is 5469343506
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:163)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
{code}","17/Jun/13 15:58;efalcao;Seeing this on my 1.2.5 cluster since upgrading:

ubuntu@c2-1d:~$ nodetool -h localhost compact TRProd Timelines                                                                                                                               
Error occurred during compaction
java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 91281671 but now it is 91281729
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:334)
        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1657)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:2146)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:235)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:250)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:791)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1447)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:89)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1292)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1380)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:812)
        at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at sun.rmi.transport.Transport$1.run(Transport.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.AssertionError: originally calculated column size of 91281671 but now it is 91281729
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:355)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",17/Jun/13 17:19;efalcao;I should add that I was upgrading from a 1.1 cluster.,26/Jun/13 17:33;arya;Any clues? We are not able to get one of our critical CFs in good shape as we cannot run compactions on it and its performance is deteriorating. ,"26/Jun/13 17:44;joeyi;I think these issues are the same, just different versions of C*.","16/Jul/13 23:15;arya;Here is our exact scenario:

Some bad code, resulted in a 600Mb row in one of our CFs. It is spanned across 2 SSTables. We've deleted it but it does not go away and nodetool cfstats still reports the max row size being the size of this row. It is causing problems because anytime this SSTable is part of a compaction or we do range queries on this CF, we end up with slow 16 sec GCs. I have tried the following to try and clean it up, but it does not go away:

1. nodetool compact with sizeTiered;
2. LCS with large SSTables and wait till it decides to promote the 2 SSTables and merge them;
3. sizeTiered with forceUserDefined compaction trying to compact the 2 SSTable with the deleted key.

Every time we end up getting the exception complaining about calculated columns size like above. scrub, repair, rebuild, upgradesstable, no luck!

Any love? ","21/Aug/13 21:11;arya;I don't remember which bug but there was one bug where Johnathan and Sylvain were arguing about discrepancy of column calculation logic in various places in the code which depended on gc_grace, deleted columns, etc. I took that as a hint and applied it to my scenario. I increased the gc_grace for the CF in question to a large number like 1 year. Then ran user defined compactions and they happily ran. Afther that I change the gc_grace setting back to 10 days and ran compactions again and my tombsones got cleaned up without seeing that exception. So, although, it didn't make sense to me what happened exactly, but at least this could be worth a try for those who are stuck.","27/Aug/13 16:46;zznate;FTR - seeing this currently in 1.2.8 on batchlog compaction attempt (unfortunately I can't easily modify the gc_grace for this). Stack trace:
{code}
ERROR [CompactionExecutor:105] 2013-08-27 17:54:39,942 CassandraDaemon.java (line 192) Exception in thread Thread[CompactionExecutor:105,1,main]
java.lang.AssertionError: originally calculated column size of 17391408 but now it is 17391426
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:445)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
ERROR [OptionalTasks:1] 2013-08-27 17:54:39,942 CassandraDaemon.java (line 192) Exception in thread Thread[OptionalTasks:1,5,main]
{code}","27/Aug/13 17:17;jbellis;TBH I'm not sure we're going to fix this in 1.2.x.  If you have a snapshot set of sstables that can reproduce it, then we can dig in, but eyeballing the code hasn't fixed it yet (despite multiple efforts) and probably won't.

The good news is that 2.0 fixed it by always doing single-pass compaction.","26/Sep/13 14:45;dankogan;We are also seeing the same error during compaction.  We can provide the sstables if that helps resolving the issues.

INFO [CompactionExecutor:74953] 2013-09-26 14:23:53,978 CompactionController.java (line 166) Compacting large row iqtell/mail_folder_data_subject_withdate_asc:97995 (131986528 bytes) incrementally
ERROR [CompactionExecutor:74953] 2013-09-26 14:24:01,126 CassandraDaemon.java (line 174) Exception in thread Thread[CompactionExecutor:74953,1,main]
java.lang.AssertionError: originally calculated column size of 131986437 but now it is 131986500
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:188)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)
","24/Nov/13 20:20;patricioe;Seen the same error in 1.2.10 with multithreaded_compaction disabled. It causes ""nodetool rebuild"" to fail making it really hard to rebuild a DC fully.","06/Dec/13 19:31;christianmovi;Seen with 1.2.11:

{code}
java.lang.AssertionError: originally calculated column size of 44470356 but now it is 44470410
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}","06/Dec/13 19:49;christianmovi;For starters, could we add the SSTable filenames to the Exception? That way users could submit broken SStable files to this ticket in the future.","21/Dec/13 20:43;dkador;We're seeing the AssertionError in LazilyCompactedRow a lot too on 1.2.8.  Not a problem with hinted handoffs for us.  We just have a large CF (close to 1TB on disk) that has had most of its row keys deleted at this point.  Lots of pending compactions on many nodes and these errors seem to stymie progress.  Hoping to get those compactions to finish to reclaim disk...

Has anybody figured out a decent workaround?  Should we try disabling multithreaded_compaction?  Looks like folks are still seeing the errors with that off (it's on for us).

How stupid would running Cassandra with assertions off be?

Alternatively, has anybody who's had this problem attempted to upgrade to 2.0 and had the problem fixed?",21/Dec/13 20:52;dkador;I'll add that this CF is using leveled compaction in case that's useful.,"21/Dec/13 21:12;jbellis;Disabling assertions to ""fix"" this is a great way to corrupt your sstables and get errors at read time later on.

This is definitely fixed in 2.0.","21/Dec/13 21:19;dkador;That's sort of what I figured.  That's why I asked how stupid it would be.  The answer is ""very"", clearly.

I understand that it's fixed in 2.0 but many of us are still on 1.2.x.  Speaking for myself, upgrading to 2.0 is on the roadmap but I'd prefer to do that as part of a staged rollout and not as an attempt to fix what seems like a bug.

Do you think disabling multithreaded_compaction would help?",22/Dec/13 00:21;jbellis;Doubt it.  Suspect only workaround is to increase in_memory_compaction limit.,22/Dec/13 11:06;christianmovi;I can confirm that this error also occurs with multithreaded_compaction=false.,"22/Dec/13 18:05;dkador;We got desperate and tried multithreaded_compaction=false yesterday afternoon and it seems to have worked for us.  Pending compactions across the cluster have dropped from over 20,000 to under 600 (we're not quite done yet).  The numbers were so high because we added six new nodes and they were unable to compact quickly until we made this settings change.

No idea why it worked for us but thought I'd share our experience.  If there's any other useful information I can share, let me know.","01/Jan/14 16:52;elubow;We are seeing this as well with 1.2.11.  As was mentioned above, knowing which CF would very useful here.  It seems to be happening to hints.  The only other major action we are seeing that is out of the ordinary is thousands of hint SSTables being transferred at a time.  Here is the Java error:

{quote}
ERROR [HintedHandoff:6] 2014-01-01 16:45:19,914 CassandraDaemon.java (line 191) Exception in thread Thread[HintedHandoff:6,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 1028119265 but now it is 1028119453
	at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:436)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:282)
	at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:90)
	at org.apache.cassandra.db.HintedHandOffManager$4.run(HintedHandOffManager.java:502)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 1028119265 but now it is 1028119453
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:432)
	... 6 more
Caused by: java.lang.AssertionError: originally calculated column size of 1028119265 but now it is 1028119453
	at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:442)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	... 3 more
{quote}

Multithreaded compactions are set to false in our cluster on all nodes.  We also don't have any pending compactions in the cluster.  Just seeing this error a lot in the logs.  The error seems to happen more frequently during bootstraps or repairs that have a lot of work to do.","15/Jan/14 17:49;dhendry;I am also seeing this happening during compaction with Cassandra 1.2.13 on a non-hints column family. I have multithreaded compaction disabled. 

Here is one new piece of information: based on the thread name the exception occurred on, I searched back through the Cassandra log and found a 'Compacting large row' message. I recognized the row key from *that* message because it has come up before within the context of a bug in our client code which resulted in runaway writes to Cassandra.  Its entirely possible that that particular row had more than 2^31 columns written to it. 

This smells like an integer overflow bug to me although I have not had a chance to dig into the Cassandra code yet.","18/Mar/14 14:12;edevil;We're seeing this with 1.2.15, and seems related with large rows being compacted incrementally and being changed at the same time.

We upgraded from 1.1.5 and did not have this assertion error before.",18/Mar/14 14:35;jbellis;This was present in 1.1 and earlier releases as well.  The fix is to upgrade to 2.0.x.,18/Mar/14 14:38;efalcao;+1 just recently upgraded to 2.0.6 and all the errors went away.,"18/Mar/14 15:09;rcoli;{quote}
This was present in 1.1 and earlier releases as well. The fix is to upgrade to 2.0.x.
{quote}
Which change in the 2.0 series fixes this, and how? Do you have a JIRA number we could refer to?","18/Mar/14 15:13;edevil;I think the fix that is mentioned is the fact that 2.0 does only single-pass compactions, so it will never be in the situation of having calculated a previous value that has changed.

It seems an architectural change, and not something easily backported.","24/Jul/14 14:56;mat.gomes;Same issues on v 1.2.12 and 1.2.18
Error occurred during compaction
java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 116397997 but now it is 116398382
...",22/Aug/14 16:15;rlow;The root cause of this in 1.2 is CASSANDRA-7808.,,,,,,,,,,,,,,,,,,,,,,
BulkLoader throws NPE at start up,CASSANDRA-4846,12612999,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,yukim,yukim,22/Oct/12 17:10,12/Mar/19 14:14,13/Mar/19 22:27,23/Oct/12 20:27,1.2.0 beta 2,,,,,0,bulkloader,,,,,,"BulkLoader in trunk throws below exception at start up and exit abnormally.

{code}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:180)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:148)
	at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:96)
	at java.io.File.list(File.java:1010)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:67)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:117)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:63)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.service.CacheService.initRowCache(CacheService.java:154)
	at org.apache.cassandra.service.CacheService.<init>(CacheService.java:102)
	at org.apache.cassandra.service.CacheService.<clinit>(CacheService.java:83)
	... 8 more
{code}

This comes from CASSANDRA-4732, which moved keyCache in SSTableReader initialization at instance creation. This causes access to CacheService that did not happen for v1.1 and ends up NPE because BulkLoader does not load cassandra.yaml.",,,,,,,,,,,,,,,,,,,23/Oct/12 13:43;jbellis;4846.txt;https://issues.apache.org/jira/secure/attachment/12550454/4846.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-23 13:43:26.041,,,no_permission,,,,,,,,,,,,250372,,,Tue Oct 23 20:27:00 UTC 2012,,,,,,0|i0axyf:,61774,yukim,yukim,,,,,,,,,,23/Oct/12 13:43;jbellis;fix attached,"23/Oct/12 14:55;yukim;yeah, let's (almost) revert CASSANDRA-4732 with this patch until we find more cleaner way to use SSTR without caches.",23/Oct/12 20:27;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to start Cassandra with simple authentication enabled,CASSANDRA-4648,12607116,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jsanda,jsanda,11/Sep/12 19:39,12/Mar/19 14:14,13/Mar/19 22:27,27/Sep/12 09:03,1.2.0 beta 2,,,,,0,security,,,,,,"I followed the steps for enabling simple authentication as described here, http://www.datastax.com/docs/1.1/configuration/authentication. I tried starting Cassandra with, 

cassandra -f -Dpasswd.properties=conf/passwd.properties -Daccess.properties=conf/access.properties

Start up failed with this exception in my log:

ERROR [main] 2012-09-11 15:03:04,642 CassandraDaemon.java (line 403) Exception encountered during startup
java.lang.AssertionError: org.apache.cassandra.exceptions.InvalidRequestException: You have not logged in
        at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:136)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:298)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:203)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:386)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:429)
Caused by: org.apache.cassandra.exceptions.InvalidRequestException: You have not logged in
        at org.apache.cassandra.service.ClientState.validateLogin(ClientState.java:254)
        at org.apache.cassandra.service.ClientState.hasColumnFamilyAccess(ClientState.java:235)
        at org.apache.cassandra.cql3.statements.SelectStatement.checkAccess(SelectStatement.java:105)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:106)
        at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:124)
        ... 4 more",Mac OS X,,,,,,,,,,,,,,,,,,12/Sep/12 12:32;slebresne;4648.txt;https://issues.apache.org/jira/secure/attachment/12544811/4648.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-11 20:02:34.891,,,no_permission,,,,,,,,,,,,256315,,,Sun Nov 18 12:39:17 UTC 2012,,,,,,0|i0gy2v:,96951,jbellis,jbellis,,,,,,,,,,"11/Sep/12 20:02;jbellis;Related: CASSANDRA-4617

I think we need to either not use QP internally, or add a compile-and-local-query only mode that skips StorageProxy and auth.","12/Sep/12 12:32;slebresne;Attaching patch that is so that processInternal skips StorageProxy and authorization (so it also solve CASSANDRA-4617 in particular).

I'm keen on keeping QP here because we use a mix of cf with and without compact storage internally, and not using QP would get annoying and error prone, while QP already deal with that. Also, collections are another thing that might be painful without QP (I don't think we have any in the system tables yet but I'm betting we'll have some soon enough). ","26/Sep/12 17:56;jbellis;Is permissionalteringstatement related or just a refactor that happened to be included in this patch?

Rest LGTM.

NB I removed IOException from IM.apply in b781ee7d52c9f30136ae2ce851c4af6def8df38e so you can drop that catch block.","27/Sep/12 07:32;slebresne;bq. Is permissionalteringstatement related or just a refactor that happened to be included in this patch?

It was meant to make it easier to add executeInternal, i.e. adding it to PermissionAlteringStatement only instead of all its subclasses. And since it factor a few other methods too ...","27/Sep/12 09:03;slebresne;Alright, committed after rebase (and removal of IOException catch). Thanks","17/Nov/12 21:37;iamaleksey;We still need something like old processInternal (not restricting queries to the local node). Had to reimplement pre-change processInternal in NativeAuthority (#4874), will have to do the same in NativeAuthenticator unless processInternal gets changed (or another method like it is added).

It is my understanding that this patch does more than just skipping authorization (which is achieved easier by simply moving isInternall check in has*Access higher, before validateLogin happens).","18/Nov/12 12:39;slebresne;Yes, authentication was only one small problem, the big one this ticket solvers is that it's not a good idea to use the normal path (basically everythign related to replication) to write to the System tables (that are not replicated anyway).

If there is a need to do normal queries but without authentication checks, I would look into subclassing ClientState with no-op authentication methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sometimes Cassandra starts compacting system-shema_columns cf repeatedly until the node is killed,CASSANDRA-4781,12611005,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,iamaleksey,iamaleksey,09/Oct/12 15:39,12/Mar/19 14:14,13/Mar/19 22:27,30/Oct/12 16:59,1.2.0 beta 2,,,,,0,,,,,,,"Cassandra starts flushing system-schema_columns cf in a seemingly infinite loop:

 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,804 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32107-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.202762MB/s.  Time: 18ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,804 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32107-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,824 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32108-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.182486MB/s.  Time: 20ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,825 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32108-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,864 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32109-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.096045MB/s.  Time: 38ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,864 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32109-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,894 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32110-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.121657MB/s.  Time: 30ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,894 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32110-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,914 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32111-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.202762MB/s.  Time: 18ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,914 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32111-Data.db')]
.........

Don't know what's causing it. Don't know a way to predictably trigger this behaviour. It just happens sometimes.","Ubuntu 12.04, single-node Cassandra cluster",,,,,,,,,,,,,,,,,,26/Oct/12 22:00;yukim;4781-v2.txt;https://issues.apache.org/jira/secure/attachment/12551032/4781-v2.txt,29/Oct/12 16:18;yukim;4781-v3.txt;https://issues.apache.org/jira/secure/attachment/12551205/4781-v3.txt,25/Oct/12 19:58;yukim;4781.txt;https://issues.apache.org/jira/secure/attachment/12550850/4781.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-10-09 15:43:32.819,,,no_permission,,,,,,,,,,,,246173,,,Tue Oct 30 20:18:38 UTC 2012,,,,,,0|i07hxb:,41671,slebresne,slebresne,,,,,,,,,,"09/Oct/12 15:43;brandon.williams;Here is something possibly related that is easy to reproduce.  Given nodes X, Y and Z, start X.  Now start Y, and see X flush schema_columns and schema_keyspaces in addition to peers/local, even though there's no schema and nothing changed.  Now start Z and see both X and Y exhibit this behavior, and sometimes even flush the schema twice in a row.","23/Oct/12 17:58;brandon.williams;Interesting to note that if you hit this and then add nodes to the cluster, they will inherit the problem too.  Issuing a schema changes seems to stop the infinite compactions.","25/Oct/12 19:57;yukim;I found the cause of this recursive compaction.

We calculate how many columns in sstable are expected to be dropped, and if that exceeds threshold we do single sstable compaction. Cassandra does not drop tombstones when overlapping keys exist in other sstables, so when we have overlap, we calculate what percentage of columns that do not belong to overlapped keys can be dropped.
Here we use sstable.estimatedKeys to calculate, but since the value comes from index interval and index summary, we have chance to calculate wrong value if the number of keys in sstable is small, like in this case, schema_columns. (schema_columns also has lot of overlapped keys among sstable since its key is Keyspace name.)

So I propose to skip single sstable compaction if sstable contains small number of keys.
Patch is attached for this.","26/Oct/12 08:16;slebresne;I believe you are right that this is a problem. But I think there is another problem in that computation (that do not only impact small number of keys), namely in the estimation of remaining columns:
{noformat}
long columns = sstable.getEstimatedColumnCount().percentile(remainingKeysRatio) * remainingKeys;
{noformat}
I think the use of percentile here is not correct. For instance, say the remaingKeysRatio is very big (say 99%), and say that your rows are such that you have many small rows and a handful (5%) of very big ones. In that case, percentile will give you the number of columns the very big row have (it will give you a number such that 99% of the rows have less than this number of columns), and you'll end up with an estimate of columns that is way off (that is, you could end up with a number of remaining column that is order of magnitude bigger than the total number of columns). I believe we should simply use:
{noformat}
long columns = sstable.getEstimatedColumnCount().mean() * remainingKeys;
{noformat}

For the estimated key number, I'm good with going with your solution, but an alternative one would be to use a more conservative estimated key number that would be:
{noformat}
public long conservativeKeyEstimate()
{
    return indexSummary.getKeys().size() < 2
         ? 1
         : (indexSummary.getKeys().size() - 1) * DatabaseDescriptor.getIndexInterval();
}
{noformat}
That advantage being that this would always under-estimate the number of keys, while estimatedKeys() always over-estimate it, which seems a better option here because we don't have a choose a rather random value of minimum samples after which we consider that the over-estimation is ""acceptable"" in proportion.

But all this being said, and while we should definitively fix the things above, they will only make the estimation better, but it still an estimation. So at least in theory, we could always end up in a case where the estimate thinks there is enough droppable tombstones, but in practice all the droppable tombstones are in overlapping ranges. Meaning that I'd suggest skipping the worthDroppingTombstones check for sstables that have been compacted (using the creation time of the file is probably good enough) since less than some time threshold (say maybe gcGrace/4). After all, if I've just been compacted and still have a high ratio of droppable, it's probably that those are in fact not droppable due to overlapping sstables.
","26/Oct/12 21:58;yukim;Sylvain, thanks for the feedback. v2 attached.

I changed use of percentile to mean. For estimated keys, I just kept mine since estimatedKeysForRanges might return greater value than under estimated keys count.

bq. But all this being said, and while we should definitively fix the things above, they will only make the estimation better, but it still an estimation. So at least in theory, we could always end up in a case where the estimate thinks there is enough droppable tombstones, but in practice all the droppable tombstones are in overlapping ranges.

You are right, we need safe stopper. I followed your advice of using sstable creation time. But I made time threshold configurable via compaction strategy option with default value of 5 minutes, since gcGraceSecond can be set to 0, and makes testing easy.

Speaking of schema_* columnfamilies, can we lower minimum compaction threshold to 2? schema_* have almost always overlapped key and tombstones, and less likely to updated.","29/Oct/12 10:33;slebresne;Looks overall ok, but a few small remaining remarks:
* I'm good with making the ""tombstone compaction interval"" configurable but I would have gone with a longer default one (say a day, or at least a few hours). The tombstone compaction can be fairly improductive if we do them too often: compacting a sstable for tombstones, even if you do collect tombstone, is kinda bad if you're going to compact the sstable a short time later. In other words, I don't think this interval is only useful for avoiding an infinite loop. Besides, if it's configurable, the few people that have very heavy delete/expiring workload can set that lower if that helps them.
* It's probably not worth exposing an interval in milliseconds, seconds would be more than good enough. I also don't dislike putting the unit the option use in the name too, so ""tombstone_compaction_interval_seconds"", though maybe it's too long a name.
* Could be nice to validate the user input for the new option (should be > 0). 
",29/Oct/12 16:18;yukim;v3 attached. Default interval is now 1 day in second unit with validation.,"29/Oct/12 22:59;minaguib;FWIW I've just hit this bug, on the very first startup of a node after it was upgraded from 1.1.2 to 1.1.6",30/Oct/12 02:25;minaguib;Restarting the node produced the same endless loop.  Downgrading back to 1.1.2 fixed it.,"30/Oct/12 07:55;slebresne;Alright, v3 lgtm, +1 (I had in mind of throwing an exception if compaction_compaction_interval < 0 but I realize that it's slightly more complex than I though and not only related to that option so let's leave it like that here and I'll open a separate ticket for improving validation).

@Mina you cannot have hit this bug in 1.1.6 because it concerns code that is 1.2.0 only. So if you had a problem upgrading, that would be something else.","30/Oct/12 12:58;minaguib;@[~slebresne] Here's the relevant portion of cassandra 1.1.6 starting up (after loading the SStables and replaying the commitlog): http://mina.naguib.ca/misc/cassandra_116_startup_loop.txt

It loops and never ends until I kill cassandra.

As I've said, restarting cassandra repeated the behavior.  Downgrading from 1.1.6 to 1.1.2 resolved it.

If you still feel it's unrelated to this ticket, let me know and I'll file a new one.
","30/Oct/12 14:04;slebresne;bq. If you still feel it's unrelated to this ticket

I feel nothing, I know it's unrelated. It *cannot* be related to this ticket since that ticket is due to code that does not exist in 1.1. Besides, your log shows System.schema_keyspaces memtables being flush over and over again (and they are not even empty) so that is what triggers compaction. Why you get continuous activity on the schema_keyspaces CF is a good question though, but definitively not related to this ticket.","30/Oct/12 16:59;yukim;Committed v3 to trunk.

[~minaguib] I'm closing this one because it was caused by functionality only available in trunk (as Sylvain stated) and I pushed the fix to trunk. Feel free to open another ticket for investigation in 1.1 version.",30/Oct/12 19:26;brandon.williams;[~minaguib] your issue is slightly different; there is constant flushing of the schema too.,30/Oct/12 20:18;minaguib;I've filed CASSANDRA-4880,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible problem with widerow in Pig URI,CASSANDRA-4749,12609917,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,oberman,oberman,02/Oct/12 14:30,12/Mar/19 14:14,13/Mar/19 22:27,12/Oct/12 03:03,1.1.6,,,,,0,,,,,,,"I don't have a good way to test this directly, but I'm concerned the Uri parsing for widerows isn't going to work.  setLocation 
1.) calls setLocationFromUri (which sets widerows to the Uri value)
2.) sets widerows to a static value (which is defined as false)
3.) sets widerows to the system setting if it exists.  
That doesn't seem right...

But setLocationFromUri also gets called from setStoreLocation, and I don't really know the difference between setLocation and setStoreLocation in terms of what is going on in terms of the integration between cassandra/pig/hadoop.",AWS running Centos 5.6 using Sun build 1.6.0_24-b07,,,,,,,,,,,,,,,,,,11/Oct/12 18:45;brandon.williams;4749.txt;https://issues.apache.org/jira/secure/attachment/12548790/4749.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-02 19:29:33.716,,,no_permission,,,,,,,,,,,,247767,,,Fri Oct 12 03:03:51 UTC 2012,,,,,,0|i08ppj:,48764,jeromatron,jeromatron,,,,,,,,,,"02/Oct/12 19:29;brandon.williams;You're right.  Patch attached to instead init the vars as default, then parse from the uri, and finally allow overriding with env vars.","12/Oct/12 02:51;jeromatron;+1 to the changes, though don't we want to expose a way for them to set those variables with standard hadoop config, possibly namespaced with pig?  e.g. cassandra.pig.wide.row?","12/Oct/12 02:58;jeromatron;I was looking at 1.1.5 that didn't yet have the URL location for Cassandra accepting the widerows or use_secondary flags.  That's in there in the 1.1 branch.

in other words +1 :)",12/Oct/12 03:03;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
range queries return incorrect results,CASSANDRA-4797,12611454,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,brandon.williams,brandon.williams,11/Oct/12 22:55,12/Mar/19 14:14,13/Mar/19 22:27,26/Oct/12 12:48,1.2.0,,,,,1,,,,,,,"I've only seen this fail once, but it's quite obviously returning incorrect results since the query is ""SELECT * FROM clicks WHERE userid >= 2 LIMIT 1"" and it's getting userid 0 in return.

{noformat}
======================================================================
FAIL: cql_tests.TestCQL.limit_ranges_test
Validate LIMIT option for 'range queries' in SELECT statements
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 183, in runTest
    self.test(*self.arg)
  File ""/var/lib/buildbot/cassandra-dtest/tools.py"", line 187, in wrapped
    f(obj)
  File ""/var/lib/buildbot/cassandra-dtest/cql_tests.py"", line 302, in limit_ranges_test
    assert res == [[ 2, 'http://foo.com', 42 ]], res
AssertionError: [[0, u'http://foo.com', 42]]
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-10-16 12:18:12.633,,,no_permission,,,,,,,,,,,,247820,,,Tue Oct 30 09:55:59 UTC 2012,,,,,,0|i08q3r:,48828,,,,,,,,,,,,"11/Oct/12 23:06;brandon.williams;I take it back, this reproduces reliably.",16/Oct/12 12:18;slebresne;Can you reproduce on current trunk? I can't and I suspect this may have been due to CASSANDRA-4796.,"26/Oct/12 12:46;julien_campan;Hi, I have the same problem when using cassandra 1.2 beta 1 with vnode :
I 'm using cql 3.
My table has 66 000 rows : 

{noformat}
cqlsh:pns_fr> select count(*) from syndic  limit 100 ;
count
-------
   100

cqlsh:pns_fr> select count(*) from syndic  limit 1000 ;
count
-------
   999

cqlsh:pns_fr> select count(*) from syndic  limit 10000 ;
count
   9983

cqlsh:pns_fr> select count(*) from syndic limit 100000 ;
count
-------
65883
{noformat}",26/Oct/12 12:48;brandon.williams;Passes on trunk now.,"30/Oct/12 09:41;julien_campan;Hi, i took the trunk version this morning  and i still have the problem.
I'm using : [cqlsh 2.3.0 | Cassandra 1.2.0-beta1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.35.0]

My use case is : 
I create a table 
CREATE TABLE premier (
  id int PRIMARY KEY,
  value int
) WITH
  comment='' AND
  caching='KEYS_ONLY' AND
  read_repair_chance=0.100000 AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  replicate_on_write='true' AND
  compression={'sstable_compression': 'SnappyCompressor'};

1) I insert 10 000 000 rows (they are like  id = 1 and value =1)
2) I delete 2 000 000 rows (i use random method to choose the key value)
3) I do select * from premier ; and my result is 7944 instead of 10 000.

So after a lot of delete, the range operator is not working.","30/Oct/12 09:47;slebresne;Hum ok, but that's actually a different problem from what the initial test case was catching (the test was returning a wrong value, the problem you're describing is about select returning fewer results than expected). Would you mind opening a separate ticket with that ""new"" problem?","30/Oct/12 09:55;julien_campan;Ok, i did it 
CASSANDRA-4877 - Range queries return fewer result after a lot of delete ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query with WHERE statement delivers wrong results,CASSANDRA-4390,12596249,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,bite,bite,28/Jun/12 12:00,12/Mar/19 14:14,13/Mar/19 22:27,28/Jun/12 14:03,,,,,,0,,,,,,,"Data is written to cassandra via phpcassa ( https://github.com/thobbs/phpcassa v1.0).

Inspection the records via cqlsh,the query with a WHERE statement doesn't deliver all records.

https://issues.apache.org/jira/secure/attachment/12533818/Cassandra_Query_Issue.png

I'am working with a 2 node cluster and the effect is the same on both nodes. Furthermore a nodetool repair, nodetool rebuild has no positive effect. The data type for this column is varint. 


******

DESCRIBE COLUMNFAMILY rm_advertisements

CREATE TABLE rm_advertisements (
  KEY text PRIMARY KEY,
  css text,
  form text,
  custom_field2 text,
  vacancy_type text,
  custom_field3 text,
  active boolean,
  vacancy_responsibles text,
  job_site text,
  vacancy_email_notification text,
  custom_field4 text,
  vacancy_name text,
  startdate text,
  vacancy_id varint,
  custom_field5 text,
  html text,
  company_id varint,
  ctime text,
  title text,
  expires text,
  atime text,
  custom_field1 text,
  vacancy_language text,
  tags text,
  mtime text,
  mailapply boolean,
  vacancy_description text,
  vacancy_function text,
  shorthash text
) WITH
  comment='Advertisements' AND
  comparator=text AND
  read_repair_chance=1.000000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

CREATE INDEX rm_advertisements_active ON rm_advertisements (active);

CREATE INDEX rm_advertisements_job_site ON rm_advertisements (job_site);

CREATE INDEX rm_advertisements_vacancy_id ON rm_advertisements (vacancy_id);

CREATE INDEX rm_advertisements_company_id ON rm_advertisements (company_id);

CREATE INDEX rm_advertisements_title ON rm_advertisements (title);

CREATE INDEX rm_advertisements_expires ON rm_advertisements (expires);

CREATE INDEX rm_advertisements_vacancy_language ON rm_advertisements (vacancy_language);

CREATE INDEX rm_advertisements_mailapply ON rm_advertisements (mailapply);

CREATE INDEX rm_advertisements_shorthash ON rm_advertisements (shorthash);
",Linux version 2.6.32-5-amd64 (Debian 2.6.32-45) |cqlsh 2.2.0 | Cassandra 1.1.1 | CQL spec 2.0.0 | Thrift protocol 19.32.0],,,,,,,,,,,,,,,,,,28/Jun/12 12:06;bite;Cassandra_Query_Issue.png;https://issues.apache.org/jira/secure/attachment/12533818/Cassandra_Query_Issue.png,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-28 13:12:11.181,,,no_permission,,,,,,,,,,,,256119,,,Thu Jun 28 14:07:10 UTC 2012,,,,,,0|i0gvcn:,96509,,,,,,,,,,,,"28/Jun/12 13:12;slebresne;Do you also get wrong result when using cassandra-cli or when using phpcassa to fetch the results, or is that a CQL thing only?",28/Jun/12 13:14;bite;I get the wrong results even with cassandra-cli also as with phpcassa.,28/Jun/12 13:20;slebresne;Have you tried a 'nodetool rebuild_index'?,"28/Jun/12 14:01;bite;That's it! It works. Thanks a lot for your support.

(Now I also found this thread: http://comments.gmane.org/gmane.comp.db.cassandra.user/26569 )

",28/Jun/12 14:07;slebresne;I guess that doesn't explain why the 2ndary index was not up to date in the first place but glad that fixed it for you.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair -pr hangs,CASSANDRA-5146,12627129,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,mkjellman,mkjellman,11/Jan/13 05:35,12/Mar/19 14:14,13/Mar/19 22:27,22/Feb/13 23:32,1.2.2,,,,,0,,,,,,,"while running a repair -pr the repair seems to hang after getting a merkle tree

{code}
 INFO [AntiEntropySessions:9] 2013-01-10 18:23:01,652 AntiEntropyService.java (line 652) [repair #d29fd100-5b95-11e2-b9c7-dd50a26832ff] new session: will sync /10.8.25.101, /10.8.30.14 on range (28356863910078205288614550619314017620,42535295865117307932921825928971026436] for evidence.[fingerprints, messages]
 INFO [AntiEntropySessions:9] 2013-01-10 18:23:01,653 AntiEntropyService.java (line 857) [repair #d29fd100-5b95-11e2-b9c7-dd50a26832ff] requesting merkle trees for fingerprints (to [/10.8.30.14, /10.8.25.101])
 INFO [ValidationExecutor:7] 2013-01-10 18:23:01,654 ColumnFamilyStore.java (line 647) Enqueuing flush of Memtable-fingerprints@500862962(12960712/12960712 serialized/live bytes, 469 ops)
 INFO [FlushWriter:25] 2013-01-10 18:23:01,655 Memtable.java (line 424) Writing Memtable-fingerprints@500862962(12960712/12960712 serialized/live bytes, 469 ops)
 INFO [FlushWriter:25] 2013-01-10 18:23:02,058 Memtable.java (line 458) Completed flushing /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-192-Data.db (11413718 bytes) for commitlog position ReplayPosition(segmentId=1357767160463, position=8921654)
 INFO [AntiEntropyStage:1] 2013-01-10 18:25:52,735 AntiEntropyService.java (line 214) [repair #d29fd100-5b95-11e2-b9c7-dd50a26832ff] Received merkle tree for fingerprints from /10.8.25.101
{code}",Ubuntu 12.04,,,,,,,,,,CASSANDRA-5105,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-01-16 17:17:34.081,,,no_permission,,,,,,,,,,,,303898,,,Fri Feb 22 23:32:11 UTC 2013,,,,,,0|i17gr3:,251683,mkjellman,mkjellman,,,,,,,,,,"11/Jan/13 07:28;mkjellman;node 10.8.25.101 is logging ""Received merkle tree for fingerprints from /10.8.25.101""

why would a node be receiving a merkle tree from itself?","11/Jan/13 08:09;mkjellman;a working repair -pr seems to log ""requesting merkle trees for messages xx""

we should have an alarm of some type (when things are working this never takes that long in reality) in here if we don't get to this code block log that the repair failed...","16/Jan/13 17:17;slebresne;bq. why would a node be receiving a merkle tree from itself?

Because we didn't bothered ""specializing"" the local case. So the repair coordinator sends itself a merkle tree request as any other node and later sends itself the tree oblivious to the fact it is itself the coordinator of said repair. It's definitively not very efficient, as we serialize/deserialize the tree uselessly, but on the other side it's not in any performance critical path. If it bugs your OCD (no judgment, we're an OCD-friendly project), feel free to open a ticket for the improvement (and maybe giving a shot at a patch?).

For the hanging problem, all the log above is saying is that 10.8.30.14 hasn't responded with his merkle tree. Maybe check the log of said node around the time the tree request was sent and check if something jumps out. Is that something you can reproduce? If it is, maybe reproducing with DEBUG logging on might shed some light.",14/Feb/13 05:49;mkjellman;fixed with the patch in 5105,"15/Feb/13 18:23;mkjellman;[~slebresne] [~yukim] don't kill me, but i can still reproduce this with 5105.. :(","20/Feb/13 05:07;slebresne;You will have to help us here, because as I said earlier the line of logs above doesn't really says much: Do you have steps to reproduce? Are you sure it really hangs and it's not just slow? Would you have the logs from all the nodes when a hanging happen maybe?","20/Feb/13 05:14;mkjellman;positive it hangs, and yes i agree we need more to debug here.

i took a stack trace on the nodes that started the repair and there were no deadlocks.

In one case however, the repair triggered another node to stream for the repair session so maybe that's a place to look.

Nothing interesting/abnormal logged with TRACE logging on .streaming or cassandra.db and a few other classes I tried","20/Feb/13 05:19;mkjellman;steps to repro is literally to run a 'nodetool repair -pr' on a node with > 300GB of data.

Other users on the users mailing list reporting the same behavior:
http://www.mail-archive.com/user@cassandra.apache.org/msg27891.html","20/Feb/13 22:35;yukim;From the code, I see two points where repair can hang.

- Merkle tree calculation failure
- Data streaming failure

Those have been addressed in CASSANDRA-3112, so let's revisit that issue.
For point one, we need to change message to report back failure, so we have to do that in next major release.
But point two more likely to happen than point one, fix that alone should be in minor release.","20/Feb/13 22:42;mkjellman;I agree that the vast majority of repair issues are actually caused by a streaming failures which is why i thought 5105 would fix it.

i'm thinking that #5151 might have created another exception which once again appeared to break repairs. After reverting 5151 and keeping the other fixes repair seems much more stable.","22/Feb/13 23:32;mkjellman;so, after testing on 15 nodes, i think #5105 did actually fix the issue here but #5151 reintroduced the ""hangs"". with #5151 reverted we are good to go.

since we already have a feature request for 2.0 to improve reporting of repair tasks i think we can close this as fixed in 1.2.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update sstableloader for 1.1.x,CASSANDRA-4818,12612102,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,j.casares,j.casares,16/Oct/12 20:51,12/Mar/19 14:14,13/Mar/19 22:27,20/Sep/13 22:40,,,,,,0,datastax_qa,,,,,,"This was done on Cassandra 1.1.5:
{CODE}
$ ls -1 Keyspace1/Keyspace1/Standard1/
Standard1-51-Data.db
Standard1-51-Index.db
Standard1-hc-51-Data.db
Standard1-hc-51-Index.db
Standard1-tmp-hc-46-Data.db
Standard1-tmp-hc-46-Index.db
$ ~/repos/cassandra/bin/sstableloader -d localhost Keyspace1/Keyspace1/Standard1/
 WARN 15:41:56,023 Invalid file 'Standard1-51-Data.db' in data directory Keyspace1/Keyspace1/Standard1.
 WARN 15:41:56,024 Invalid file 'Standard1-51-Index.db' in data directory Keyspace1/Keyspace1/Standard1.
Skipping file Standard1-hc-51-Data.db: column family Keyspace1.hc doesn't exist
Skipping file Standard1-tmp-hc-46-Data.db: column family Keyspace1.tmp doesn't exist
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]
{CODE}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-10-16 20:58:06.616,,,no_permission,,,,,,,,,,,,249126,,,Tue Oct 16 20:58:06 UTC 2012,,,,,,0|i0a4xr:,57073,,,,,,,,,,,,"16/Oct/12 20:58;nickmbailey;The sstable file names there don't appear to be valid. Shouldn't they be 

{noformat}
<keyspace>-<cf>-....db
{noformat}

rather than just

{noformat}
<cf>-....db
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect slice count even if column expire mid-request,CASSANDRA-5149,12627182,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,slebresne,slebresne,11/Jan/13 15:25,12/Mar/19 14:14,13/Mar/19 22:27,18/Jun/13 16:16,2.0 beta 1,,,,,0,,,,,,,"This is a follow-up of CASSANDRA-5099.

If a column expire just while a slice query is performed, it is possible for replicas to count said column as live but to have the coordinator seeing it as dead when building the final result. The effect that the query might return strictly less columns that the requested slice count even though there is some live columns matching the slice predicate but not returned in the result.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-02-12 18:58:38.176,,,no_permission,,,,,,,,,,,,303951,,,Tue Jun 18 16:16:48 UTC 2013,,,,,,0|i17h2v:,251736,slebresne,slebresne,,,,,,,,,,"11/Jan/13 15:30;slebresne;As said on CASSANDRA-5099, the only good way to fix this that I can see right now would be to have the coordinator determine an expireBefore value (the current time at the beginning of the request) and use that exclusively during the query to decide whether a query is expired or not (similar to what we do for LazilyCompactedRow but at the scale of the query).

Unfortunately, this means shipping that expireBefore value to replicas with the query and that implies a inter-node protocol change, which make this only viable for 2.0 now. Hence the 'fix version'. Of course if we can find a solution that don't require protocol change, then great.",12/Feb/13 18:58;jbellis;What if we forced ExpiringColumn to either Column or DeletedColumn on the reply?,"13/Feb/13 09:55;slebresne;That's a good idea, I think that would work (at least I don't see why it wouldn't right away).","25/Feb/13 15:44;slebresne;Actually, this is not as free of a lunch as it sound. We cannot really force non-expired ExpiringColumn to Column, because we need to return the expiring time to the client. So in practice, we would need to either add a dontExpire flag for ExpiringColumn or a special Column+TTL column type for just that purpose. Any of those will have impact on the inter-node protocol (and at the column serialization, where we don't yet handle versions well (because we never really had to)).

And if that's not significantly simpler, I think I prefer the expireBefore solution because that feels less hacky to me in that it fixes the issue of having column expire at uncontrolled times more generally (this is also how we deal with it in LazilyCompactedRow).","25/Feb/13 19:05;jbellis;If it's fix-for 2.0, can we just omit returning expiration time?","25/Feb/13 19:26;slebresne;I don't understand. Why would not returning the expiration time be more ok for 2.0 than 1.2? That is, even in CQL3 it's possible to query the expiration time of a column.",25/Feb/13 20:04;jbellis;I have trouble coming up with a use case for it.,"26/Feb/13 10:48;slebresne;bq. I have trouble coming up with a use case for it.

You mean, for querying the expiration time of a column? My own experience with TTL leads me to believe that anyone using expiring columns will want to query the expiration time at one point or another. I personally had a case where if some TTLed columns were read, we were ""extending"" the TTL and how long it was extended depended on the current expiration time. Or, on the other end of the spectrum, wanting to update a column value without extending the TTL. I had also a number of case where not all columns had the same TTL, if any, and just knowing if the column was an expiring one was necessary (including but not limited to checking during development that the code was doing what it was supposed to do in terms of setting TTLs).

Also, if we were to transform expiring column to standard column on read, even ignoring the fact that we wouldn't be able to return the expiration time to clients, you'd have to be careful about read-repair ""cancelling"" your TTLs.  

Besides, as said above, especially if it's fix-for 2.0 (and so things like ""this may require a protocol change"" become largely irrelevant), I think normalizing all code on having expiration be based on a time fixed at well know places (like we do for gcBefore really) is the right long term solution.","06/May/13 16:34;jjordan;Does CASSANDRA-4415 make this a non-issue (as long as it deals with this)?  To me, this might as well just be ""won't fix"", and you have to make the extra query and see if you only get one column back.  I guess there is an edge case where you could just get one column, and there really are more, but its been this way since 0.7, so meh.","06/May/13 16:46;slebresne;bq. Does CASSANDRA-4415 make this a non-issue

I don't know. I think ""we'll only ever going to return less than asked columns if there is less than asked columns"" seems a reasonably semantic to have, and we fail that here with TTL. And even if CASSANDRA-4415 ends up hiding the problem, it's still a performance issue, because if you can't rely on the semantic above, you'll almost always have to do one more query than would be enough ""just to be sure"" (which is what getCount() does for thrift today).

So, without saying this is a big or pressing issue, I'm -1 on ""wont fixing it"" in the long run.","28/May/13 10:14;slebresne;Actually, now that I think about it, I think CASSANDRA-4415 is why I'd really rather have this in 2.0.

Currently, because of this, when you page a slice query, you cannot trust a given page to return strictly results than you've asked only if paging is done, because you could have result expiring mid-request and thus you pretty much can never know if the paging is really done or if some columns expired on you. CASSANDRA-5099 ""solves"" this by waiting until basically a query return an empty page. However:
# this is really correct. In theory, you could have *all* of the columns fetch by the current patch that have expired mid-request, while there's still some live columns that match what your are trying to page. Granted, with a large enough page size it's very unlikely but still.
# this means you'll *always* do one more query (and that's StorageProxy level queries, it's not cheap than would be needed if this ticket was fixed. And while having paged get_count being slow don't really make me shed tears, it bugs me quite a bit more in the context of CASSANDRA-4415.
# this complicate reasoning about the logic for CASSANDRA-4415 imo. It's much easier not to have to care about ""oh, what if a column expires mid-request, is that ok?"".

Besides, I don't think fixing this is very complicated in practice. All we need is ship a 'queryServerTimestamp' with the read commands, and carry that down to the Column.isMarkedForDelete() method so it uses that instead of System.currentTimeMillis(). This might end up being a few lines of code to pass this timestamp down as parameter, but it's pretty trivial changes.","09/Jun/13 23:32;iamaleksey;There is another method that is affected - ExpiringColumn.create() that returns either a DeletedColumn or an ExpiringColumn instance, called by ColumnSerializer.deserializeColumnBody(). Now, we already do pass expireBefore to it as a parameter (in a limited way) and it's not (was not) difficult to make it be derived from read requests' timestamp. Except in one case - row cache deserialization. The required modifications go beyond the need for Cache API change and I haven't found a good way to deal with it.
Is there any change we actually will get rid of row cache in 2.0?","10/Jun/13 00:08;slebresne;Can't we create a version of ExpiringColumn.create() that never transform a DeletedColumn to an ExpiringColumn and use that for cache deserialization (or say do that when expireBefore is negative and pass -1 for expireBefore in the row cache code).

Because that behavior of ExpiringColumn.create() is really just an optimization. It would be ok to never transform expired columns to deleted ones from a correction point of view.","10/Jun/13 02:06;iamaleksey;bq. Can't we create a version of ExpiringColumn.create() that never transform a DeletedColumn to an ExpiringColumn and use that for cache deserialization (or say do that when expireBefore is negative and pass -1 for expireBefore in the row cache code).

That is easy, actually, since you only have to special-case starting at SerializingCacheProvider.deserialize() method. But I'm afraid it's not enough. That is, special-casing it just for the row cache is not enough, must do the same for sstable deserialization (or remove the optimization entirely), or else a row serialized into the cache as a result of request (a) with timestamp Y might not be the right row for request (b) with timestamp X (< Y) coming out of order if a column expires between X and Y.

Making ExpiringColumn.create() to never return DeletedColumn instances would be the easiest way to deal with it, but what would impact compaction (and repair), so I suggest making NO expring->deleted optimization the default behavior, and only enabling it for compaction (incl. validation compaction) (and SSTableExport), or, in other words, special-case it for SSTII only.","10/Jun/13 15:37;slebresne;bq. But I'm afraid it's not enough

I'm not sure I understand the case you are talking about. Whether or not ExpiringColumn.create() decides to return a deleted column or not, it still returns a column, so this should have no impact whatsoever on anything timestamp related. I.e. the code will treat an ExpiringColumn that is expired exactly as a deleted column, so it should always be safe to not transform an expired column to a DeletedColumn, even if it's only in parts of the code.

But it could be I just don't understand your example.","10/Jun/13 16:18;iamaleksey;I wasn't clear enough. I'm talking about read request timestamps, not column timestamps. And I'm saying that to transfrom is not always correct, in the context of 5149. But *not to* transfrom is perfectly all right.","10/Jun/13 17:14;slebresne;I still don't see where the problem is. Can you illustrate with a concrete example what is the problem we can run into if we just have Expiring.create() that doesn't transform in the case of the row cache but do transform based on the ""read request timestamp"" in all other cases?","12/Jun/13 03:10;iamaleksey;https://github.com/iamaleksey/cassandra/commits/5149

The first patch adds timestamp field to all IReadCommands and makes sure it's always set appropriately.

The second patch updates the read path to respect the request timestamp. It also normalizes the argument order for CFS.getRangeSlice() and CFS.search(). This was not necessary, but the order they were in bothered me immensely. The third patch fixes the tests broken by this rearrangement.

I'll add some unit tests after the code review.","14/Jun/13 13:18;slebresne;The overall approach looks good to me, though:
* I'd feel a bit better if we were just removing the isMarkedForDelete() (without arguments) call in Column, so that every caller knows he has to deal with it. It's otherwise hard to make sure we never call the no-argument version by accident. There's probably a few other places too (Row for instance) where I'd prefer avoiding having a shortcut calling System.currentTimeMillis().
* In RowIteratorFactory, I think it would make sense to use the filter timestamp rather than relying on System.currentTimeMillis() for gcBefore.
* There is possibly a few places where a function takes 'gcBefore' and 'now', either directly (collectReducedColumns), or because they have a filter and gcBefore (CFS.filterColumnFamily()). Maybe we could drop the gcBefore and just rely on the timestamp.
","17/Jun/13 14:33;iamaleksey;Pushed the fourth commit to the same branch.

- Removed Column.isMarkedForDelete/0 and most similar methods (including QueryFilter getXFilter helper methods)
- RowIteratorFactory/System.currentTimeMillis() - done.
- CFS.filterColumnFamily() was the only place where I could get rid of passing gcBefore explicitly - thanks for catching that. In IDiskAtomFilter.collectReducedColumns() timestamp and gcbefore are sometimes unrelated (with Integer.MIN_VALUE passed as gcBefore, intentionally). Same goes for CFS.collateOnDiskAtom(). Usually it's either because of either compaction or the row cache.","18/Jun/13 13:54;slebresne;bq. In IDiskAtomFilter.collectReducedColumns() timestamp and gcbefore are sometimes unrelated

Right, forgot about that.


Last version lgtm, +1.

A few optional minor nits for the commit:
* In CounterColumn.reconcile (and CounterMutation and ... in fact), we don't support expiring columns in counter tables so it's ok to just use say Long.MIN_VALUE (which a comment why).
* the comment inside DeletedColumn.isMarkedForDelete is obsolete (it's more confusing that helpful now :)).
","18/Jun/13 13:58;iamaleksey;Thanks!

bq. In CounterColumn.reconcile (and CounterMutation and ... in fact), we don't support expiring columns in counter tables so it's ok to just use say Long.MIN_VALUE (which a comment why)

I know. Same with Column.getString() - it's overloaded by ExpiringColumn anyway. I was debating with myself what to use - Long.MIN_VALUE, 0, or just System.currentTimeMillis() where it doesn't matter, and went with System.currentTimeMillis(). Will change to Long.MIN_VALUE with a comment in both places.",18/Jun/13 16:16;iamaleksey;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexHelper.IndexFor call throws AOB exception when passing multiple slices,CASSANDRA-5030,12618912,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,05/Dec/12 17:26,12/Mar/19 14:14,13/Mar/19 22:27,07/Dec/12 19:49,1.2.0 rc1,,,,,0,,,,,,,"While testing multiple slices I'm seeing some exceptions when a slice hits the end of an index.

{code}
ERROR [ReadStage:138179] 2012-12-04 18:04:28,796 CassandraDaemon.java (line 132) Exception in thread Thread[ReadStage:138179,5,main]
java.lang.IndexOutOfBoundsException: toIndex = 6
        at java.util.SubList.<init>(AbstractList.java:602)
        at java.util.RandomAccessSubList.<init>(AbstractList.java:758)
        at java.util.AbstractList.subList(AbstractList.java:468)
        at org.apache.cassandra.io.sstable.IndexHelper.indexFor(IndexHelper.java:182)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.setNextSlice(IndexedSliceReader.java:253)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.<init>(IndexedSliceReader.java:246)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.<init>(IndexedSliceReader.java:91)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:68)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:44)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:101)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:267)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:61)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1387)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1247)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1159)
        at org.apache.cassandra.db.Table.getRow(Table.java:348)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:48)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}


I can reproduce this in a test, attached",,,,,,,,,,,,,,,,,,,07/Dec/12 18:38;tjake;5030-1.txt;https://issues.apache.org/jira/secure/attachment/12559923/5030-1.txt,05/Dec/12 18:26;tjake;5030.txt;https://issues.apache.org/jira/secure/attachment/12556129/5030.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-07 06:03:19.2,,,no_permission,,,,,,,,,,,,296178,,,Fri Dec 07 19:49:41 UTC 2012,,,,,,0|i146jb:,232522,jbellis,jbellis,,,,,,,,,,"07/Dec/12 06:03;jbellis;SSTableNamesIterator doesn't check for index < 0, which could be problematic.

Nit: prefer static import for assertEquals.",07/Dec/12 18:38;tjake;New version with recommended changes,07/Dec/12 19:10;jbellis;+1,07/Dec/12 19:49;tjake;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
symlinks to data directories are broken in 1.2.0,CASSANDRA-5185,12629151,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,jjordan,jjordan,24/Jan/13 21:09,12/Mar/19 14:14,13/Mar/19 22:27,28/Jan/13 16:13,1.2.2,,,,,0,,,,,,,"symlinks to data directories is broken in 1.2.0
{noformat}
cd ~
tar xzf apache-cassandra-1.2.0-bin.tar.gz
cd apache-cassandra-1.2.0/conf
vim cassandra.yaml
#set data/commitlog/savecache dirs to
#~/apache-cassandra-1.2.0/var/...
cd ../bin
# start once to make folders
./cassandra -f
#cntrl-c
cd ..
mkdir var/lib/cassandra2
mv var/lib/cassandra/data/system var/lib/cassandra2/system
cd var/lib/cassandra/data
ln -s ../../cassandra2/system .
cd ~/apache-cassandra-1.2.0/bin/
cassandra -f
#get lots of assertion errors see attached log file
{noformat}

{noformat}
 INFO 21:59:44,883 Enqueuing flush of Memtable-local@1578022692(52/52 serialized/live bytes, 2 ops)
 INFO 21:59:44,890 Writing Memtable-local@1578022692(52/52 serialized/live bytes, 2 ops)
ERROR 21:59:44,892 Exception in thread Thread[FlushWriter:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.io.sstable.Descriptor.<init>(Descriptor.java:190)
	at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:593)
	at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:588)
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:428)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:417)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

I traced it back some, I think it is coming from:
{noformat}
org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:428)
which calls: cfs.directories.getLocationForDisk(dataDirectory)

    public File getLocationForDisk(File dataDirectory)
    {
        for (File dir : sstableDirectories)
        {
            if (FileUtils.getCanonicalPath(dir).startsWith(FileUtils.getCanonicalPath(dataDirectory)))
                return dir;
        }
        return null;
    }
{noformat}

My guess is that the FileUtils.getCanonicalPath calls aren't matching because of the symlinks.  So null is being returned there.",,,,,,,,,,,,,,,,,,,24/Jan/13 21:11;jjordan;5185-errors.log;https://issues.apache.org/jira/secure/attachment/12566374/5185-errors.log,24/Jan/13 22:51;yukim;5185.txt;https://issues.apache.org/jira/secure/attachment/12566394/5185.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-24 22:51:11.207,,,no_permission,,,,,,,,,,,,308990,,,Mon Jan 28 16:13:16 UTC 2013,,,,,,0|i1dxnj:,289549,jbellis,jbellis,,,,,,,,,,24/Jan/13 22:51;yukim;File#getCannonicalPath returns path after resolving symlinks. Patch instead uses getAbsolutePath so symlinks under data dir works as expected.,"24/Jan/13 22:59;jbellis;If we cannonicalize both sizes of what we're comparing, shouldn't it work either way?","24/Jan/13 23:08;yukim;For example, if you have data dir of ""/var/lib/cassandra/data"" and you symlinked system keyspace under that data dir to ""/var/lib/cassandra2/system"" as above, cannonicalized paths are 

{noformat}
/var/lib/cassandra/data -> /var/lib/cassandra/data
/var/lib/cassandra/data/system -> /var/lib/cassandra2/system
{noformat}

thus current comparison doesn't work.",25/Jan/13 15:25;jbellis;+1,28/Jan/13 16:13;yukim;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: handle when full cassandra type class names are given,CASSANDRA-4546,12603535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thepaul,thepaul,thepaul,14/Aug/12 23:28,12/Mar/19 14:14,13/Mar/19 22:27,16/Aug/12 22:27,1.2.0 beta 1,,Legacy/Tools,,,0,cqlsh,,,,,,"When a builtin Cassandra type was being used for data in previous versions of Cassandra, only the short name was sent: ""UTF8Type"", ""TimeUUIDType"", etc. Starting with 1.2, as of CASSANDRA-4453, the full class names are sent.

Cqlsh doesn't know how to handle this, and is currently treating all data as if it were an unknown type. This goes as far as to cause an exception when the type is actually a number, because the driver deserializes it right, and then cqlsh tries to use it as a string.

Here for googlage:

{noformat}
AttributeError: 'int' object has no attribute 'replace'
{noformat}

Fixeries are in order.",,,,,,,,,,,,,,,,,,,14/Aug/12 23:39;thepaul;4546.patch.txt;https://issues.apache.org/jira/secure/attachment/12540977/4546.patch.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-16 22:10:07.709,,,no_permission,,,,,,,,,,,,256234,,,Thu Aug 16 22:27:35 UTC 2012,,,,,,0|i0gwyn:,96770,brandon.williams,brandon.williams,,,,,,,,,,"14/Aug/12 23:39;thepaul;Fix attached. Also available on the 4546 branch on my github- this version tagged pending/4546:

https://github.com/thepaul/cassandra/tree/4546","16/Aug/12 22:10;kirktrue;+1 on the first part of the patch.

The third change in the patch _appears_ unrelated to me. Please clarify for my own edification.

Thanks.",16/Aug/12 22:17;thepaul;It's not directly related. Just a minor problem with error reporting that came up while I was testing this.,16/Aug/12 22:27;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RuntimeException when bootstrapping a node without an explicitely set token,CASSANDRA-4850,12613134,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,slebresne,slebresne,23/Oct/12 13:21,12/Mar/19 14:13,13/Mar/19 22:27,31/Oct/12 04:55,1.2.0 beta 2,,,,,0,,,,,,,"Trying to boostrap a node for which no initial token has been set result in:
{noformat}
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:603)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:490)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:386)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:305)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:393)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:436)
{noformat}

This has been broken by CASSANDRA-4416. More specifically, now that we storage the system metadata in the schema on startup, the check
{noformat}
                // if we see schema, we can proceed to the next check directly
                if (!Schema.instance.getVersion().equals(Schema.emptyVersion))
                {
                    logger.debug(""got schema: {}"", Schema.instance.getVersion());
                    break;
                }
{noformat}
in StorageService.joinTokenRing is broken. This result in the node trying to check the Load map to pick a token before any gossip state has been received.

Note sure what is the best fix (an easy would be to always wait RING_DELAY before attempting to pick the token, at least in the case where an initial token isn't set, but that's a big hammer).",,,,,,,,,,,,,,,,,,,30/Oct/12 19:18;xedin;CASSANDRA-4850.patch;https://issues.apache.org/jira/secure/attachment/12551411/CASSANDRA-4850.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-30 04:25:05.968,,,no_permission,,,,,,,,,,,,250564,,,Wed Oct 31 04:55:40 UTC 2012,,,,,,0|i0azbj:,62000,jbellis,jbellis,,,,,,,,,,"30/Oct/12 04:25;jbellis;IMO we should not include system tables in serializedSchema at all, for version or for sending to other nodes, since they are hardcoded.  (Sending a diff to another node will not have the desired effects.)",30/Oct/12 19:18;xedin;schema is now set to ignore system rows when version is calculated and when migrations are to be send to the remote node.,30/Oct/12 19:29;jbellis;+1,31/Oct/12 04:55;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool ownership is incorrect with vnodes,CASSANDRA-5065,12623814,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,13/Dec/12 20:16,12/Mar/19 14:13,13/Mar/19 22:27,18/Dec/12 20:32,1.2.0 rc2,,,,,0,,,,,,,"Example:

{noformat}
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address           Load       Tokens  Owns   Host ID                               Rack
UN  10.179.65.102     197.96 MB  256     0.1%   6ac56251-08ff-46be-be06-5b8dd929b937  rack1
UN  10.179.111.137    209.3 MB   256     0.0%   aade8ef6-c907-427c-87be-a5fe05a27fa4  rack1
UN  10.179.64.227     205.86 MB  256     0.1%   4634cc80-0832-4ea1-b4a6-39ae54985206  rack1
{noformat}
",,,,,,,,,,,,,,,,,,,17/Dec/12 21:14;jbellis;5065.txt;https://issues.apache.org/jira/secure/attachment/12561353/5065.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-17 21:14:05.934,,,no_permission,,,,,,,,,,,,297534,,,Tue Dec 18 20:32:54 UTC 2012,,,,,,0|i14r87:,235876,yukim,yukim,,,,,,,,,,17/Dec/12 21:14;jbellis;patch attached to fix getOwnership's assumption that each ip address only has one token associated with it.,"18/Dec/12 19:38;yukim;Patch lgtm, +1.",18/Dec/12 20:32;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 doesn't allow static CF definition with compact storage in C* 1.1,CASSANDRA-4910,12614758,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,05/Nov/12 08:31,12/Mar/19 14:13,13/Mar/19 22:27,06/Nov/12 14:21,1.1.7,,,,,0,,,,,,,"In Cassandra 1.1, the following CQL3 definition:
{noformat}
CREATE TABLE user_profiles (
    user_id text PRIMARY KEY,
    first_name text,
    last_name text,
    year_of_birth int
) WITH COMPACT STORAGE;
{noformat}
yields:
{noformat}
Bad Request: COMPACT STORAGE requires at least one column part of the clustering key, none found
{noformat}

This works fine in 1.2 however.",,,,,,,,,,,,,,,,,,,05/Nov/12 09:16;slebresne;4910.txt;https://issues.apache.org/jira/secure/attachment/12552078/4910.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-05 16:15:57.529,,,no_permission,,,,,,,,,,,,255184,,,Tue Nov 06 14:21:56 UTC 2012,,,,,,0|i0epx3:,83961,jbellis,jbellis,,,,,,,,,,05/Nov/12 09:16;slebresne;Simple fix attached (not sure why we've refused that in the first place).,05/Nov/12 16:15;jbellis;+1,"06/Nov/12 14:21;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nodes can't rejoin after stopping, when using GossipingPropertyFileSnitch",CASSANDRA-5133,12626666,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,exabytes18,exabytes18,08/Jan/13 23:10,12/Mar/19 14:13,13/Mar/19 22:27,16/Jan/13 18:29,1.2.1,,,,,0,,,,,,,"I can establish a 1.2 ring with GossipingPropertyFileSnitch, but after killing a node and restarting it, the node cannot rejoin.

[Node 1] ./bin/cassandra -f
[Node 2] ./bin/cassandra -f
[Node 3] ./bin/cassandra -f

[Node 1] ./bin/nodetool ring
 ... ok ...

[Node 1] ^C
 ... node shutdown ...

[Node 1] ./bin/cassandra -f
 ... Exception! ...



ERROR 05:45:39,305 Exception encountered during startup
java.lang.RuntimeException: Could not retrieve DC for /10.114.18.51 from gossip and PFS compatibility is disabled
  at org.apache.cassandra.locator.GossipingPropertyFileSnitch.getDatacenter(GossipingPropertyFileSnitch.java:109)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:127)
	at org.apache.cassandra.locator.TokenMetadata$Topology.addEndpoint(TokenMetadata.java:1040)
	at org.apache.cassandra.locator.TokenMetadata.updateNormalTokens(TokenMetadata.java:185)
	at org.apache.cassandra.locator.TokenMetadata.updateNormalTokens(TokenMetadata.java:157)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:441)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:397)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:309)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:397)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:440)
java.lang.RuntimeException: Could not retrieve DC for /10.114.18.51 from gossip and PFS compatibility is disabled
	at org.apache.cassandra.locator.GossipingPropertyFileSnitch.getDatacenter(GossipingPropertyFileSnitch.java:109)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:127)
	at org.apache.cassandra.locator.TokenMetadata$Topology.addEndpoint(TokenMetadata.java:1040)
	at org.apache.cassandra.locator.TokenMetadata.updateNormalTokens(TokenMetadata.java:185)
	at org.apache.cassandra.locator.TokenMetadata.updateNormalTokens(TokenMetadata.java:157)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:441)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:397)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:309)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:397)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:440)



Full environment + exceptions + stacktraces: https://gist.github.com/1e74ff02c2d4f622ce8f ",3 ec2 instances (CentOS 6.3; java 1.7.0_05; Cassandra 1.2),,,,,,,,,,,,,,,,,,16/Jan/13 17:26;brandon.williams;5133-2.txt;https://issues.apache.org/jira/secure/attachment/12565147/5133-2.txt,15/Jan/13 23:25;brandon.williams;5133.txt;https://issues.apache.org/jira/secure/attachment/12565037/5133.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-09 03:23:47.851,,,no_permission,,,,,,,,,,,,303273,,,Wed Jan 16 18:29:33 UTC 2013,,,,,,0|i1792f:,250437,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"09/Jan/13 03:23;tscanausa;While trying to help Matt I can verify, that GossipPropertySnitch does not seem to work as expected.

Below is a detailed explanation of what happened to me while working to help Matt,

3 nodes A, B, C.

setup:
Node A -> seeds=""node A ip address""
Node B -> seeds=""node A ip address""
Node C -> seeds=""node A ip address"" 

1) start node A and let it fully start up. ( first node needs to be a seed of itself to start )
2) start node B and let it fully join the ring.
3) start node C and let it fully join the ring.
4) nodetool ring shows all nodes up
5) stop node C and update the cassandra-rackdc.protperites file to rack=2
6) start node C ( fails to start error about cant find DC for node A, maybe GossipPropertySnitch wont let a seed node talk about itself? )
7) stop node A and update seeds=""node B ip address"", to try and solve question above
8) start node A ( fails to start cant find DC of node C )
9) stuck not being able to start node A and C","09/Jan/13 03:43;brandon.williams;Caused by CASSANDRA-3881, since before then we didn't need to know the dc/rack for saved endpoints at startup.  A workaround is to disable loading the persisted ring via -Dcassandra.load_ring_state=false",11/Jan/13 21:06;exabytes18;Is there any risk to specifying cassandra.load_ring_state=false? In what version will this be resolved?,"11/Jan/13 21:07;brandon.williams;The main risk is if it is used as a coordinator when it starts up before it discovers the rest of the ring, it will think it owns all writes, even though it doesn't.",15/Jan/13 20:29;jbellis;I don't think I understand the problem -- why can't we use rack + dc from system.peers?,"15/Jan/13 22:07;brandon.williams;I totally missed that we were already saving that info.  Patch to have GPFS load it, but only use it outside of compat mode, since with compat mode you've made a mistake if PFS can't load it either.",16/Jan/13 00:35;vijay2win@yahoo.com;+1,16/Jan/13 00:43;brandon.williams;Committed.,"16/Jan/13 03:01;brandon.williams;Still some kind of race on initial startup:

{noformat}
ERROR 02:58:45,376 Exception in thread Thread[WRITE-cassandra-1/10.179.65.102,5,main]
java.lang.RuntimeException: Could not retrieve DC for cassandra-1/10.179.65.102 from gossip and PFS compatibility is disabled
        at org.apache.cassandra.locator.GossipingPropertyFileSnitch.getDatacenter(GossipingPropertyFileSnitch.java:80)
        at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:127)
        at org.apache.cassandra.net.OutboundTcpConnection.isLocalDC(OutboundTcpConnection.java:73)
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:266)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:138)
{noformat}","16/Jan/13 03:06;brandon.williams;The problem in this case is cassandra-1 is the seed and when this node starts up it knows about it via being listed in the yaml as seed, but doesn't have dc info that OTC wants when connecting.","16/Jan/13 03:11;vijay2win@yahoo.com;How about, instead of throwing an exception use something like EC2Snitch.DEFAULT_DC? 
I think we can live with this instead of doing the system table lookup too.","16/Jan/13 17:26;brandon.williams;Patch on top of what's already committed to go the default rack/dc route.  I think it's better to keep the system table lookup since that's going to be correct 99% of the time, where the default only needs to be returned in the corner case of contacting a seed the very first time.",16/Jan/13 18:25;vijay2win@yahoo.com;+1,16/Jan/13 18:29;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageServiceClientTest/RecoveryManager2Test fail on 1.2.0 and above branch,CASSANDRA-4980,12617135,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,yukim,yukim,21/Nov/12 16:50,12/Mar/19 14:13,13/Mar/19 22:27,21/Nov/12 19:18,,,,,,0,,,,,,,"Looks like change in c4cca2d8bba20a7651b956e1893727391bf5f10a (store schema_version to system.local) broke both StorageServiceClientTest and RecoveryManager2Test.
StorageServiceClientTest assert data directories are not created in client mode but this change actually creates data directories. RecoveryManager2Test fails with ""junit.framework.AssertionFailedError: Expecting only 1 replayed mutation, got 10"" error and I think extra commit log also comes from this insert to system.local.",,,,,,,,,,,,,,,,,,,21/Nov/12 17:26;slebresne;0001-fix-StorageServiceClientTest.txt;https://issues.apache.org/jira/secure/attachment/12554535/0001-fix-StorageServiceClientTest.txt,21/Nov/12 17:26;slebresne;0002-Clean-up-the-commit-log-before-RecoveryManager2Test.txt;https://issues.apache.org/jira/secure/attachment/12554536/0002-Clean-up-the-commit-log-before-RecoveryManager2Test.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-11-21 16:54:30.36,,,no_permission,,,,,,,,,,,,259336,,,Wed Nov 21 19:18:31 UTC 2012,,,,,,0|i0lofr:,124609,yukim,yukim,,,,,,,,,,21/Nov/12 16:54;jbellis;My vote would be to revert c4cca2 for 1.2.0 and try again for 1.2.1.  [~slebresne]?,"21/Nov/12 17:26;slebresne;I'd rather avoid that if possible.

The problem in StorageServiceClientTest is imo a bug that this commit just happens to reveal. Namely, that for some reason {{SS.initClient()}} calls updateVersionAndAnnounce. But since fat clients are supposed to not have local tables, they will in particular have no schema and this is useless. If for some weird reason we really need fat client to send a 'I have no schema' on gossip, let's call {{MigrationManager.passiveAnnounce(Schema.emptyVersion)}} directly (but I don't see why we would need that).

As for RecoveryManager2Test, it's just that this test was somehow assuming the commit log was empty when the test started. Which was the case because we almost always flush when we write the system table, but it happens that for writing the schemaVersion in the local table I figured flushing was overkill (since it's only for client sake and we don't need to flush to have it visible).

Attaching simple fixes for both problems.",21/Nov/12 18:45;yukim;patch lgtm. +1,"21/Nov/12 19:18;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
max client timestamp,CASSANDRA-5153,12627370,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,wy96f,wy96f,14/Jan/13 03:16,12/Mar/19 14:13,13/Mar/19 22:27,24/Jan/13 14:58,1.1.10,1.2.1,,,,0,,,,,,,"1. In public LazilyCompactedRow(CompactionController controller, List<? extends ICountableColumnIterator> rows)
   columnStats = new ColumnStats(reducer == null ? 0 : reducer.columns, reducer == null ? Long.MIN_VALUE : reducer.maxTimestampSeen,
                                      reducer == null ? new StreamingHistogram(SSTable.TOMBSTONE_HISTOGRAM_BIN_SIZE) : reducer.tombstones

  Tthe maxTimestampSeen should be max(emptyColumnFamily.deletionInfo().maxTimestamp(), reducer.maxTimestampSeen)?

2. In private ColumnFamily collectTimeOrderedData()
                // if we've already seen a row tombstone with a timestamp greater
                // than the most recent update to this sstable, we're done, since the rest of the sstables
                // will also be older
                if (sstable.getMaxTimestamp() < mostRecentRowTombstone)
                    break; 
   In the case that sstable.getMaxTimestamp == Long.MIN_VALUE, is it logical?

",,,,,,,,,,,,,,,,,,,16/Jan/13 08:27;jbellis;5153.txt;https://issues.apache.org/jira/secure/attachment/12565093/5153.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-14 03:27:57.184,,,no_permission,,,,,,,,,,,,304154,,,Thu Jan 24 14:58:34 UTC 2013,,,,,,0|i17js7:,252174,slebresne,slebresne,,,,,,,,,,14/Jan/13 03:27;jbellis;What about max client timestamp?,"16/Jan/13 01:40;wy96f;Any ideas?
Thanks",16/Jan/13 08:27;jbellis;I think you're right.  Patch attached to fix and to treat old sstables with MAX_VALUE instead of MIN.,"21/Jan/13 14:40;slebresne;I think the correct fix in LCR would be:
{noformat}
markedAt = emptyColumnFamily.getMarkedForDeleteAt();
maxTimestamp = reducer == null ? markedAt : Math.max(markedAt, reducer.maxTimestampSeen);
{noformat}
because {{reducer == null}} implies there is no columns for that row in any of the source SSTable, and thus guarantees we have a row tombstone (otherwise we wouldn't have written the row in the first place). And in fact, even if we were to write non-tombstoned empty rows, then MIN_VALUE would be the correct value for maxTimestamp.

Also, while the default of MIN_VALUE for old non-timestamp-tracking sstables is clearly bogus and should be fixed, I would almost suggest not bumping the sstable version (I'm still hesitant but leaning towards not doing it). The rational is that there are only 2 cases currently where a sstable can have a MIN_VALUE max timestamp:
# the sstable is a pre-1.0.10 one that don't track timestamp. Fixing SSTableMetada fixes that part.
# the sstable is entirely and uniquely composed of row tombstones.

The version bump is ""fixing"" only the latter but at the price of temporarly breaking the collectTimeOrderedData optimization for everyone that will upgrade to the version containing this. Granted collectTimeOrderedData is ""just an optimization"", but having a sstable entirely composed of row tombstones is a pretty remote risk. And besides, for it to trigger a problem you actually need at least 2 sstables entirely composed of row tombstones (and even then there is no guaranteed you'll get the bug). Feels even more than remote.",24/Jan/13 14:58;jbellis;Committed w/ suggested changes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL queries using LIMIT sometimes missing results,CASSANDRA-4579,12605133,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,thepaul,thepaul,27/Aug/12 20:51,12/Mar/19 14:13,13/Mar/19 22:27,18/Sep/12 06:35,1.2.0 beta 1,,,,,0,cql,cql3,,,,,"In certain conditions, CQL queries using LIMIT clauses are not being given all of the expected results (whether unset column values or missing rows).

Here are the condition sets I've been able to identify:

First mode: all rows are returned, but in the last row of results, all columns which are not part of the primary key receive no values, except for the first non-primary-key column. Conditions:

 * Table has a multi-component primary key
 * Table has more than one column which is not a component of the primary key
 * The number of results which would be returned by a query is equal to or more than the specified LIMIT

Second mode: result has fewer rows than it should, lower than both the LIMIT and the actual number of matching rows. Conditions:

 * Table has a single-column primary key
 * Table has more than one column which is not a component of the primary key
 * The number of results which would be returned by a query is equal to or more than the specified LIMIT

It would make sense to me that this would have started with CASSANDRA-4329, but bisecting indicates that this behavior started with commit 91bdf7fb4220b27e9566c6673bf5dbd14153017c, implementing CASSANDRA-3647.

Test case for the first failure mode:

{noformat}
DROP KEYSPACE test;

CREATE KEYSPACE test
    WITH strategy_class = 'SimpleStrategy'
    AND strategy_options:replication_factor = 1;

USE test;

CREATE TABLE testcf (
    a int,
    b int,
    c int,
    d int,
    e int,
    PRIMARY KEY (a, b)
);

INSERT INTO testcf (a, b, c, d, e) VALUES (1, 11, 111, 1111, 11111);
INSERT INTO testcf (a, b, c, d, e) VALUES (2, 22, 222, 2222, 22222);
INSERT INTO testcf (a, b, c, d, e) VALUES (3, 33, 333, 3333, 33333);
INSERT INTO testcf (a, b, c, d, e) VALUES (4, 44, 444, 4444, 44444);

SELECT * FROM testcf;

SELECT * FROM testcf LIMIT 1; -- columns d and e in result row are null
SELECT * FROM testcf LIMIT 2; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 3; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 4; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 5; -- results are correct (4 rows returned)
{noformat}

Test case for the second failure mode:

{noformat}
CREATE KEYSPACE test
    WITH strategy_class = 'SimpleStrategy'
    AND strategy_options:replication_factor = 1;

USE test;

CREATE TABLE testcf (
    a int primary key,
    b int,
    c int,
);

INSERT INTO testcf (a, b, c) VALUES (1, 11, 111);
INSERT INTO testcf (a, b, c) VALUES (2, 22, 222);
INSERT INTO testcf (a, b, c) VALUES (3, 33, 333);
INSERT INTO testcf (a, b, c) VALUES (4, 44, 444);

SELECT * FROM testcf;

SELECT * FROM testcf LIMIT 1; -- gives 1 row
SELECT * FROM testcf LIMIT 2; -- gives 1 row
SELECT * FROM testcf LIMIT 3; -- gives 2 rows
SELECT * FROM testcf LIMIT 4; -- gives 2 rows
SELECT * FROM testcf LIMIT 5; -- gives 3 rows
{noformat}",,,,,,,,,,,,,,,,,,,17/Sep/12 07:35;slebresne;0001-Add-all-columns-from-a-prefix-group-before-stopping.txt;https://issues.apache.org/jira/secure/attachment/12545373/0001-Add-all-columns-from-a-prefix-group-before-stopping.txt,17/Sep/12 07:35;slebresne;0002-Fix-LIMIT-for-NamesQueryFilter.txt;https://issues.apache.org/jira/secure/attachment/12545374/0002-Fix-LIMIT-for-NamesQueryFilter.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-04 13:40:45.174,,,no_permission,,,,,,,,,,,,256263,,,Tue Sep 18 06:35:47 UTC 2012,,,,,,0|i0gxc7:,96831,xedin,xedin,,,,,,,,,,27/Aug/12 20:51;thepaul;Thanks to Christoph Hack for identifying the problem.,"04/Sep/12 13:40;slebresne;There is indeed 2 bugs when counting columns with composites (introduced by the change made for collections, so 1.1 is not affected in particular).

The first one is that to count the number of CQL row to return, SliceQueryFilter groups columns having the same composite prefix (i.e. all the columns belonging to the same CQL row) and count that as 1. However the code was stopping collecting columns as sound as the requested count was reached, without waiting having seen all the columns of the last ""group"".

The second one is that for NamesQueryFilter, each internal Cassandra row will yield exactly one CQL row, so we must use the ""count keys"" rather than ""count columns"" argument for getRangeSlice in that case.

Attached fix for both (I've pushed a dtest with the two examples from that ticket).
",17/Sep/12 07:35;slebresne;Rebased patches attached.,17/Sep/12 21:23;xedin;+1,"18/Sep/12 06:35;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Force provided columns in clustering key order in 'CLUSTERING ORDER BY',CASSANDRA-4881,12614094,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,pmcfadin,pmcfadin,30/Oct/12 22:18,12/Mar/19 14:13,13/Mar/19 22:27,31/Oct/12 14:33,1.2.0 beta 2,,,,,0,,,,,,,"Using this table:
CREATE TABLE video_event (
  videoid_username varchar,
  event varchar,
  event_timestamp timestamp,
  video_timestamp timestamp,
  PRIMARY KEY (videoid_username, event, event_timestamp)
)WITH CLUSTERING ORDER BY (event_timestamp DESC);

Inserting these records:

INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','start','2012-09-02 18:05:00','2012-09-02 18:05:00');
INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','stop','2012-09-02 18:05:30','2012-09-02 18:05:30');
INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','start','2012-09-02 18:35:00','2012-09-02 18:35:00');
INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','stop','2012-09-02 18:37:30','2012-09-02 18:37:30');

Running this select:

select * from video_event where videoid_username = '99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd' limit 1;

I get this:

 videoid_username                           | event | event_timestamp          | video_timestamp
--------------------------------------------+-------+--------------------------+--------------------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start | 2012-09-02 18:05:00+0000 | 2012-09-02 18:05:00+0000

I would expect to see this:

 videoid_username                           | event | event_timestamp          | video_timestamp
--------------------------------------------+-------+--------------------------+--------------------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd |  stop | 2012-09-02 18:37:30+0000 | 2012-09-02 18:37:30+0000

where the first record pulled was the sorted record by event_timestamp in reverse order.
",,,,,,,,,,,,,,,,,,,31/Oct/12 08:39;slebresne;4881.txt;https://issues.apache.org/jira/secure/attachment/12551503/4881.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-31 08:39:06.019,,,no_permission,,,,,,,,,,,,253222,,,Wed Oct 31 14:33:09 UTC 2012,,,,,,0|i0dcjb:,75960,jbellis,jbellis,,,,,,,,,,"31/Oct/12 08:39;slebresne;I think there is indeed one thing that should be improved, but I'm not sure this is what you think :)

Currently, the {{CLUSTERING ORDER BY (event_timestamp DESC)}} is a shorthand for {{CLUSTERING ORDER BY (event ASC, event_timestamp DESC)}}. This isn't very clear however and so I think we should refuse the former and require the latter. Attaching a patch to do that.

Now with the caveat above, the rest work as designed. If you don't specify any ordering for your request (like in the select above), we are free to return what's most convenient and in practice we return row in disk order. But since the disk order will be pretty much the one of the inserts (because 'start' sorts before 'stop', which trumps any sorting for event_timestamp in that example), the result is correct.

To have things sorted by event_timestamp independently of the event, you will have to put the {{event_timestamp}} before {{event}} in the primary key definition.
",31/Oct/12 08:41;slebresne;I've updated the title to reflect what the patch attached actually fixes.,"31/Oct/12 13:48;jbellis;LGTM.

Nit: ""Too many columns"" check could be moved out of the for loop for greater clarity.","31/Oct/12 14:33;slebresne;Committed (with nit fixed), thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When upgrading from 1.1.6 to 1.20 change in partitioner causes nodes not to start,CASSANDRA-4843,12612879,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,appodictic,appodictic,21/Oct/12 14:15,12/Mar/19 14:13,13/Mar/19 22:27,27/Nov/12 15:36,1.2.0 beta 3,,,,,0,,,,,,,"ERROR 10:17:20,341 Cannot open /home/edward/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-hf-1 because partitioner does not match org.apache.cassandra.dht.RandomPartitioner != org.apache.cassandra.dht.Murmur3Partitioner

This is because 1.2 has a new default partitioner, why are we changing the default? Is this wise? The current partitioner has been rock solid for years. 

Should the previously known partition be stored in the schema like the previously know seed nodes, and schema?",,,,,,,,,,,,,,,,,,,27/Nov/12 15:31;jbellis;4843.txt;https://issues.apache.org/jira/secure/attachment/12555016/4843.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-22 07:38:02.291,,,no_permission,,,,,,,,,,,,250189,,,Tue Nov 27 15:36:18 UTC 2012,,,,,,0|i0audr:,61195,slebresne,slebresne,,,,,,,,,,"21/Oct/12 14:18;appodictic;The error message is strange as well.

Changing paritioner on a existing cluster can cause data loose, Please verify your partitioner in cassandra.yaml

""Can cause data *loss"" is impossible because Cassandra will not even start. If it did start it would definitely cause data loss.","21/Oct/12 14:20;appodictic;Also the comments say, 

{noformat}
# - RandomPartitioner distributes rows across the cluster evenly by md5.
#   When in doubt, this is the best option.
{noformat}

If this is the best option why is another option chosen as the default?","22/Oct/12 07:38;slebresne;bq. why are we changing the default?

The reason is that md5 is a bit cpu intensive and in some 2ndary index requests (that does a token computation for each key on disk it scans for internal reason that can't be changed easily) this was a bottleneck. The new default is the Murmur3Partitioner that is much cheaper to compute. Besides, for every query we do compute a bunch of token and vnodes will probably not reduce that, so it's a generic improvement.

That being said, I fully agree that the current upgrade experience is pretty harsh (even the NEWS file don't clearly explain the action to take to avoid this error). And since we save the partitonner and don't start if the user change it in the yaml, maybe it's time to change the behavior so that if a partitioner is saved in the system table, we use that (and log a warning if it differs from the yaml configured one).

","22/Oct/12 21:15;jbellis;bq. maybe it's time to change the behavior so that if a partitioner is saved in the system table, we use that

Partitioner is saved per-sstable so we can do this panic doublecheck, but you have to know (or think you know) the partitioner before you can actually open up a system table.  I could be wrong, but I don't think it's a quick fix.

In the meantime, I've updated the comments and error message in 8f3d9b8371fa7c5dea83d45d83ec7fe4911a96c0.","23/Oct/12 06:19;slebresne;bq. but you have to know (or think you know) the partitioner before you can actually open up a system table

I don't think you do since the system table keyspace uses LocalStrategy (and this is hardcoded). That being said, even if we start saving the partitioner in the system table, it's possibly a little late for the upgrade to 1.2. I still think we should do it for 1.1.7 though (but to be clear, I'm all for keeping the panic doublecheck when we open a sstable because that protects again a different problem anyway).","27/Nov/12 15:31;jbellis;We missed the 1.1.7 window so I think it's pretty safe to guess that most 1.2 upgraders won't have any extra safety information we add to 1.1.8.

Patch attached to clarify the exception message. ",27/Nov/12 15:33;slebresne;+1,27/Nov/12 15:36;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow preparing queries without parameters,CASSANDRA-4577,12604982,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tux21b,tux21b,tux21b,25/Aug/12 21:51,12/Mar/19 14:13,13/Mar/19 22:27,05/Sep/12 14:10,1.2.0 beta 1,,Legacy/CQL,,,0,cql3,,,,,,"Currently it's not possible to prepare any queries that do not take any parameters using Cassandra's new native protocol because of an assertion error.

This makes client development rather difficult (you need to parse CQL queries to detect the number of parameters and skip the preparation of those) and there is probably no reason to handle queries with no parameters separately.",,,,,,,,,,,,,,,,,,,25/Aug/12 21:55;tux21b;trunk-4577.txt;https://issues.apache.org/jira/secure/attachment/12542426/trunk-4577.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-05 14:10:54.724,,,no_permission,,,,,,,,,,,,256262,,,Wed Sep 05 14:10:54 UTC 2012,,,,,,0|i0gxbj:,96828,slebresne,slebresne,,,,,,,,,,"25/Aug/12 21:55;tux21b;This simple patch seems to work fine for me, but please review carefully.","05/Sep/12 14:10;slebresne;lgtm, +1. Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 predicate logic is reversed when used on a reversed column,CASSANDRA-4716,12609060,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,tjake,tjake,25/Sep/12 15:00,12/Mar/19 14:13,13/Mar/19 22:27,02/Oct/12 16:52,1.1.6,,,,,1,,,,,,,"Example:

{code}
cqlsh:test>
cqlsh:test> CREATE TABLE testrev (
        ... key text,
        ... rdate timestamp,
        ... num double,
        ... PRIMARY KEY(key,rdate)
        ... ) WITH COMPACT STORAGE
        ...   AND CLUSTERING ORDER BY(rdate DESC);
cqlsh:test> INSERT INTO testrev(key,rdate,num) VALUES ('foo','2012-01-01',10.5);
cqlsh:test> select key from testrev where rdate > '2012-01-02' ;
 key 
-----
 foo 

cqlsh:test> select key from testrev where rdate < '2012-01-02' ;
cqlsh:test>
{code}",,,,,,,,,,,,,,,,,,,02/Oct/12 14:35;slebresne;4716.txt;https://issues.apache.org/jira/secure/attachment/12547388/4716.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-02 14:35:18.922,,,no_permission,,,,,,,,,,,,253218,,,Tue Oct 02 16:52:20 UTC 2012,,,,,,0|i0dcif:,75956,tjake,tjake,,,,,,,,,,"01/Oct/12 19:46;tjake;How deeply do we want to fix this? It goes pretty deep into how we deal with isReversed flag vs ReversedType.

We can, however easily fix it in the CQLParser by just flipping the predicates signs around.","02/Oct/12 14:35;slebresne;This can't be fixed in the parser as this all depends on what's the clustering order and the requested order (if any).

But attaching a patch to fix this (I've pushed dtests to make sure we're good with different variation of clustering order/order by). ",02/Oct/12 16:35;tjake;+1,"02/Oct/12 16:52;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop keyspace causes schema disagreement,CASSANDRA-4752,12610062,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,beobal,beobal,03/Oct/12 10:38,12/Mar/19 14:13,13/Mar/19 22:27,04/Oct/12 15:29,1.1.6,,,,,0,,,,,,,"The fix for CASSANDRA-4698 introduced a bug whereby when drop keyspace is issued a schema disagreement immediately occurs. This seems to be because the 

{code}ColumnFamily cf = ColumnFamily.create(modification.getValue().metadata());{code}

in {{RowMutation.deserializeFixingTimestamps}} does not preserve deletion info for the cf in the modification. In most cases, this doesn't cause a problem, but for a drop keyspace modification, there are no columns in the cf, so the deletion info is effectively lost leading to an incorrect digest being created and ultimately a schema disagreement.

Replacing the {{create}} with {{cloneMeShallow}} does preserve the deletion info and avoids the schema disagreement issue.
",,,,,,,,,,,,,,,,,,,03/Oct/12 10:44;beobal;0001-CASSANDRA-4752.txt;https://issues.apache.org/jira/secure/attachment/12547518/0001-CASSANDRA-4752.txt,03/Oct/12 15:30;jbellis;4752-v2.txt;https://issues.apache.org/jira/secure/attachment/12547542/4752-v2.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-03 15:30:40.421,,,no_permission,,,,,,,,,,,,239448,,,Thu Oct 04 15:29:52 UTC 2012,,,,,,0|i00hef:,752,jbellis,jbellis,,,,,,,,,,"03/Oct/12 15:30;jbellis;Thanks, Sam!  I think you nailed it with your diagnosis of the problem.  But the fix will not actually correct the timestamp (neither did the original code).  Attaching v2 that makes the code do what I think was originally intended -- copy the old timestamp into the clone, or cap it at current time if necessary.",04/Oct/12 13:57;tjake;Verified Jonathans fix +1,04/Oct/12 15:29;jbellis;committed!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
corrupted saved caches,CASSANDRA-4622,12606257,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,brandon.williams,brandon.williams,05/Sep/12 15:31,12/Mar/19 14:13,13/Mar/19 22:27,11/Sep/12 17:44,1.2.0,,,,,0,,,,,,,"I'm seeing this fairly frequently on trunk:

{noformat}

 INFO 05:15:23,805 reading saved cache /var/lib/cassandra/saved_caches/system-schema_columnfamilies-KeyCache-b.db
 WARN 05:15:23,808 error reading saved cache /var/lib/cassandra/saved_caches/system-schema_columnfamilies-KeyCache-b.db
java.lang.NullPointerException
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:151)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:247)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:362)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:334)
        at org.apache.cassandra.db.Table.initCf(Table.java:333)
        at org.apache.cassandra.db.Table.<init>(Table.java:271)
        at org.apache.cassandra.db.Table.open(Table.java:101)
        at org.apache.cassandra.db.Table.open(Table.java:79)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:285)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:201)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:349)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:392)
 INFO 05:15:23,858 Opening /var/lib/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-ia-5 (171 bytes)
 INFO 05:15:23,870 Opening /var/lib/cassandra/data/system/peers/system-peers-ia-1 (7983 bytes)
 INFO 05:15:23,870 Opening /var/lib/cassandra/data/system/peers/system-peers-ia-2 (7876 bytes)
 INFO 05:15:23,884 Opening /var/lib/cassandra/data/system/local/system-local-ia-35 (4910 bytes)
 INFO 05:15:23,885 Opening /var/lib/cassandra/data/system/local/system-local-ia-33 (75 bytes)
 INFO 05:15:23,885 Opening /var/lib/cassandra/data/system/local/system-local-ia-34 (4676 bytes)
 INFO 05:15:23,912 reading saved cache /var/lib/cassandra/saved_caches/system-local-KeyCache-b.db
 WARN 05:15:23,912 error reading saved cache /var/lib/cassandra/saved_caches/system-local-KeyCache-b.db
java.lang.NullPointerException
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:151)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:247)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:362)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:334)
        at org.apache.cassandra.db.Table.initCf(Table.java:333)
        at org.apache.cassandra.db.Table.<init>(Table.java:271)
        at org.apache.cassandra.db.Table.open(Table.java:101)
        at org.apache.cassandra.db.Table.open(Table.java:79)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:285)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:201)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:349)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:392)
{noformat}",,,,,,,,,,,,,,,,,,,11/Sep/12 17:13;vijay2win@yahoo.com;0001-CASSANDRA-4622.patch;https://issues.apache.org/jira/secure/attachment/12544672/0001-CASSANDRA-4622.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-11 17:13:03.587,,,no_permission,,,,,,,,,,,,256293,,,Tue Sep 11 17:44:47 UTC 2012,,,,,,0|i0gxrz:,96902,jbellis,jbellis,,,,,,,,,,11/Sep/12 17:13;vijay2win@yahoo.com;Moved the null check to be checked before adding to the future list.,11/Sep/12 17:36;jbellis;+1,"11/Sep/12 17:44;vijay2win@yahoo.com;Committed, Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offline scrub does not migrate the directory structure on migration from 1.0.x to 1.1.x and causes the keyspace to disappear,CASSANDRA-5195,12629730,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,omid,omid,omid,29/Jan/13 17:08,12/Mar/19 14:13,13/Mar/19 22:27,22/Mar/13 14:26,1.1.11,,,,,0,,,,,,,"Due to CASSANDRA-4411, upon migration from 1.0.x to 1.1.x containing LCS-compacted sstables, an offline scrub should be run before Cassandra 1.1.x is started. But Cassandra 1.1.x uses a new directory structure (CASSANDRA-2749) that offline scrubber doesn't detect or try to migrate.

How to reproduce:

1- Run cassandra 1.0.12.
2- Run stress tool, let Cassandra flush Keyspace1 or flush manually.
3- Stop cassandra 1.0.12
4- Run ./bin/sstablescrub Keyspace1 Standard1
  which returns ""Unknown keyspace/columnFamily Keyspace1.Standard1"" and notice the data directory isn't migrated.
5- Run cassandra 1.1.9. Keyspace1 doesn't get loaded and Cassandra doesn't try to migrate the directory structure. Also commitlog entries get skipped: ""Skipped XXXXX mutations from unknown (probably removed) CF with id 1000""

Without the unsuccessful step 4, Cassandra 1.1.9 loads and migrates the Keyspace correctly.

  ",,,,,,,,,,,,,,,,,,,16/Mar/13 15:01;omid;0001-Flush-newly-migrated-system-CFs.patch;https://issues.apache.org/jira/secure/attachment/12574012/0001-Flush-newly-migrated-system-CFs.patch,29/Jan/13 17:26;omid;5195.patch;https://issues.apache.org/jira/secure/attachment/12567003/5195.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-29 17:20:14.964,,,no_permission,,,,,,,,,,,,310226,,,Fri Mar 22 14:26:48 UTC 2013,,,,,,0|i1hjf3:,310571,krummas,krummas,,,,,,,,,,"29/Jan/13 17:20;jbellis;Ryan, can you reproduce?",29/Jan/13 17:26;omid;I tried to fix the issue in offline-scrub but the patch doesn't fully fix the issue. Cassandra 1.1.9 with this patch only loads the migrated keyspaces after 2nd restart after offine-scrub has applies the migration.,29/Jan/13 20:27;jbellis;2nd try of assign-to-ryan,"30/Jan/13 03:50;enigmacurry;I have reproduced this issue. Omid's patch works as he described: it does not fully fix the issue, but does allow the keyspace to be loaded on the 2nd restart of cassandra. Below is my verification workflow:

* Checkout/build 1.0.12
** cd $CASSANDRA_DIR
** git checkout -b 5195-1.0.12
** git reset --hard cassandra-1.0.12
** git clean -f -d
** ant build
* Run 1.0.12 test:
** sudo rm -rf /var/lib/cassandra
** sudo cassandra
** cd tool/stress
** ant build
** ./bin/stress
** sudo pkill -f CassandraDaemon
* Verify the keyspace/cf was created by stress:
** 10:20 PM:~/git/datastax/cassandra/tools/stress[5195-1.0.12*]$ cqlsh
   Connected to Test Cluster at localhost:9160.
   [cqlsh 2.0.0 | Cassandra unknown | CQL spec unknown | Thrift protocol 19.20.0]
   Use HELP for help.
   cqlsh> use Keyspace1 ;
   cqlsh:Keyspace1> select count(*) from Standard1;
   count
   -------
   10000
* Checkout/build 1.1.9
** cd $CASSANDRA_DIR
** git checkout -b 5195-1.1.9
** git reset --hard cassandra-1.1.9
** git clean -f -d
** ant build
* Run 1.1.9 test:
** sudo ./bin/sstablescrub Keyspace1 Standard1
*** stdout: Unknown keyspace/columnFamily Keyspace1.Standard1
** sudo cassandra
*** log:  INFO [main] 2013-01-29 22:28:44,800 CommitLogReplayer.java (line 103) Skipped 585748 mutations from unknown (probably removed) CF with id 1000
* Verify that Keyspace1 does or does not exist:
** 10:30 PM:~/git/datastax/cassandra[5195-1.1.9*]$ cqlsh
   Connected to Test Cluster at localhost:9160.
   [cqlsh 2.2.0 | Cassandra 1.1.9-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.33.0]
   Use HELP for help.
   cqlsh> use Keyspace1 ;
   Bad Request: Keyspace 'Keyspace1' does not exist
* Run 1.1.9 test again without the sstablescrub (restoring /var/lib/cassandra from before):
** sudo pkill -f CassandraDaemon    
** sudo cassandra
*** log:  INFO 22:33:01,240 Replaying /var/lib/cassandra/commitlog/CommitLog-1359515707503.log, /var/lib/cassandra/commitlog/CommitLog-1359515946450.log
    INFO 22:33:01,244 Replaying /var/lib/cassandra/commitlog/CommitLog-1359515707503.log
    INFO 22:33:02,318 CFS(Keyspace='Keyspace1', ColumnFamily='Standard1') liveRatio is 4.55084790673026 (just-counted was 4.55084790673026).  calculation took 866ms for 4590 columns
    INFO 22:33:02,930 CFS(Keyspace='Keyspace1', ColumnFamily='Standard1') liveRatio is 5.226616220760892 (just-counted was 5.226616220760892).  calculation took 357ms for 11635 columns
    INFO 22:33:04,186 CFS(Keyspace='Keyspace1', ColumnFamily='Standard1') liveRatio is 5.094053078093754 (just-counted was 4.9614899354266155).  calculation took 859ms for 26720 columns
* Verify that Keyspace1 does or does not exist:
** 10:36 PM:~/git/datastax/cassandra[5195-1.1.9*]$ cqlsh
   Connected to Test Cluster at localhost:9160.
   [cqlsh 2.2.0 | Cassandra 1.1.9-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.33.0]
   Use HELP for help.
   cqlsh> use Keyspace1;
   cqlsh:Keyspace1> select count(*) from Standard1;
   count
   -------
   10000
* Apply patch and retest:
** cd $CASSANDRA_DIR
** git apply ~/Downloads/5195.patch
** ant clean build 
** sudo rm -rf /var/lib/cassandra
** (restore /var/lib/cassandra from 1.0.12)
** sudo pkill -f CassandraDaemon
** sudo ./bin/sstablescrub Keyspace1 Standard1
*** stdout:
    Pre-scrub sstables snapshotted into snapshot pre-scrub-1359517364042
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-17-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-17-Data.db') complete: 63608 rows in new sstable and 0 empty (tombstoned) rows dropped
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-10-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-10-Data.db') complete: 258153 rows in new sstable and 0 empty (tombstoned) rows dropped
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-18-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-18-Data.db') complete: 65207 rows in new sstable and 0 empty (tombstoned) rows dropped
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-15-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-15-Data.db') complete: 254487 rows in new sstable and 0 empty (tombstoned) rows dropped
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-5-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-5-Data.db') complete: 243561 rows in new sstable and 0 empty (tombstoned) rows dropped
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-16-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-16-Data.db') complete: 64230 rows in new sstable and 0 empty (tombstoned) rows dropped
* Verify that Keyspace1 does or does not exist:
** sudo cassandra
** 10:44 PM:~/git/datastax/cassandra[5195-1.1.9*]$ cqlsh
   Connected to Test Cluster at localhost:9160.
   [cqlsh 2.2.0 | Cassandra 1.1.9-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.33.0]
   Use HELP for help.
   cqlsh> use Keyspace1;
   Bad Request: Keyspace 'Keyspace1' does not exist
* Restart cassandra as suggested:
** sudo pkill -f CassandraDaemon
** sudo cassandra
* Verify that Keyspace1 does or does not exist:
** 10:47 PM:~/git/datastax/cassandra[5195-1.1.9*]$ cqlsh
   Connected to Test Cluster at localhost:9160.
   [cqlsh 2.2.0 | Cassandra 1.1.9-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.33.0]
   Use HELP for help.
   cqlsh> use Keyspace1;
   cqlsh:Keyspace1> select count(*) from Standard1;
   count
   -------
   10000
   ","30/Jan/13 04:07;enigmacurry;One other interesting aspect: Removing the patch, resetting /var/lib/cassandra to the 1.0.12 state, re-running sstablescrub, restarting cassandra (1,1.9) TWICE allows the keyspace to be read, but the table is empty! :


* 11:03 PM:~/git/datastax/cassandra[5195-1.1.9*]$ cqlsh
  Connected to Test Cluster at localhost:9160.
  [cqlsh 2.2.0 | Cassandra 1.1.9-SNAPSHOT | CQL spec 2.0.0 | Thrift   protocol 19.33.0]
  Use HELP for help.
  cqlsh> use Keyspace1;
  cqlsh:Keyspace1> select count(*) from Standard1;
   count
   -------
        0
",16/Mar/13 15:01;omid;New patch to fix the condition that CFs show up after second restart.,"16/Mar/13 15:03;omid;I think upon loading the schema via offline scrubber, DefsTable.loadFromStorage migrates the old system tables to the new format. Therefore it drops the old ones but doesn't flush the commitlogs of new ones. Offline scrubber exits and on the next start, schema_keyspaces, schema_columnfamilies and schema_columns CFs have no persisted sstables, and the commitlog only gets replayed after Cassandra tries to load CF schemas, therefore finds none. This explains why after second restart, column families appear again (something force-flushes system CFs?).

A new patch (0001-Flush-newly-migrated-system-CFs.patch) is attached to fix the problem (although I'm not sure if there is a more proper fix for this or if there is a better place to put the foced flush.)","21/Mar/13 15:08;jbellis;[~krummas], can you review?","22/Mar/13 08:43;krummas;looks good to me and verified it works as expected

bug very similar to CASSANDRA-5061","22/Mar/13 14:26;jbellis;committed, thanks all!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HSHA doesn't handle large messages gracefully,CASSANDRA-4573,12604662,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,thobbs,thobbs,23/Aug/12 18:44,12/Mar/19 14:13,13/Mar/19 22:27,06/Aug/13 22:22,2.0.0,,,,,0,,,,,,,"HSHA doesn't seem to enforce any kind of max message length, and when messages are too large, it doesn't fail gracefully.

With debug logs enabled, you'll see this:

{{DEBUG 13:13:31,805 Unexpected state 16}}

Which seems to mean that there's a SelectionKey that's valid, but isn't ready for reading, writing, or accepting.

Client-side, you'll get this thrift error (while trying to read a frame as part of {{recv_batch_mutate}}):

{{TTransportException: TSocket read 0 bytes}}",,,,,,,,,,,,,,,,,,,02/Aug/13 04:14;xedin;CASSANDRA-4573.patch;https://issues.apache.org/jira/secure/attachment/12595548/CASSANDRA-4573.patch,02/Aug/13 04:14;xedin;disruptor-thrift-server-0.2.2-SNAPSHOT.jar;https://issues.apache.org/jira/secure/attachment/12595549/disruptor-thrift-server-0.2.2-SNAPSHOT.jar,23/Aug/12 18:46;thobbs;repro.py;https://issues.apache.org/jira/secure/attachment/12542170/repro.py,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-08-24 00:56:08.764,,,no_permission,,,,,,,,,,,,256258,,,Thu Aug 08 18:31:45 UTC 2013,,,,,,0|i0gx9r:,96820,thobbs,thobbs,,,,,,,,,thobbs,"23/Aug/12 18:46;thobbs;The attached repro.py reproduces the issue using pycassa, but I can also reproduce with phpcassa.  I'm testing against 1.1.4, with the only change to cassandra.yaml being a switch to hsha.","24/Aug/12 00:56;vijay2win@yahoo.com;{quote}
With debug logs enabled, you'll see this:
DEBUG 13:13:31,805 Unexpected state 16
{quote}
Unexpected state 16 means the intrest operation is accept. 
If you look at the code it is a while loop (the intrestOps can change between the if conditions) and will be handled in the next iteration.

{quote}
HSHA doesn't seem to enforce any kind of max message length
{quote}
neither does sync :) i can reproduce this error both in Sync and hsha, still trying to see where the timeout comes from though.","24/Aug/12 03:37;jbellis;bq. neither does sync 

sync does enforce frame size","24/Aug/12 04:32;vijay2win@yahoo.com;but Frame size is set for both HSHA and Sync

{code}
TNonblockingServer.Args serverArgs = new TNonblockingServer.Args(serverTransport).inputTransportFactory(inTransportFactory)
                                                                                       .outputTransportFactory(outTransportFactory)
{code}","27/Aug/12 02:24;vijay2win@yahoo.com;Hi Tyler, I think the issue is related to GC pause can you conform you see the same?

Run: tail -f /var/log/cassandra/system.log |grep GCInspector
Enable GC logging.
Run: python repro.py

You would see timeout when you see ""threads were stopped:"" to be > 1.5 Seconds or so.","27/Aug/12 17:18;thobbs;Vijay, I'm actually not seeing very long garbage collections, if I'm reading the logs correctly.  These are the relevant logs, running with a heap of 2GB and young gen size of 400MB:

{noformat}
{Heap before GC invocations=0 (full 0):
 par new generation   total 368640K, used 327680K [0x2f200000, 0x48200000, 0x48200000)
  eden space 327680K, 100% used [0x2f200000, 0x43200000, 0x43200000)
  from space 40960K,   0% used [0x43200000, 0x43200000, 0x45a00000)
  to   space 40960K,   0% used [0x45a00000, 0x45a00000, 0x48200000)
 concurrent mark-sweep generation total 1687552K, used 0K [0x48200000, 0xaf200000, 0xaf200000)
 concurrent-mark-sweep perm gen total 16384K, used 14333K [0xaf200000, 0xb0200000, 0xb3200000)
2012-08-27T12:03:56.096-0500: [GC Before GC:
Statistics for BinaryTreeDictionary:
------------------------------------
Total Free Space: 432013312
Max   Chunk Size: 432013312
Number of Blocks: 1
Av.  Block  Size: 432013312
Tree      Height: 1
Before GC:
Statistics for BinaryTreeDictionary:
------------------------------------
Total Free Space: 0
Max   Chunk Size: 0
Number of Blocks: 0
Tree      Height: 0
[ParNew
Desired survivor size 20971520 bytes, new threshold 1 (max 1)
- age   1:    2692712 bytes,    2692712 total
: 327680K->2642K(368640K), 0.0564410 secs] 327680K->2642K(2056192K)After GC:
Statistics for BinaryTreeDictionary:
------------------------------------
Total Free Space: 431996928
Max   Chunk Size: 431996928
Number of Blocks: 1
Av.  Block  Size: 431996928
Tree      Height: 1
After GC:
Statistics for BinaryTreeDictionary:
------------------------------------
Total Free Space: 0
Max   Chunk Size: 0
Number of Blocks: 0
Tree      Height: 0
, 0.0567720 secs] [Times: user=0.03 sys=0.00, real=0.06 secs] 
Heap after GC invocations=1 (full 0):
 par new generation   total 368640K, used 2642K [0x2f200000, 0x48200000, 0x48200000)
  eden space 327680K,   0% used [0x2f200000, 0x2f200000, 0x43200000)
  from space 40960K,   6% used [0x45a00000, 0x45c94998, 0x48200000)
  to   space 40960K,   0% used [0x43200000, 0x43200000, 0x45a00000)
 concurrent mark-sweep generation total 1687552K, used 0K [0x48200000, 0xaf200000, 0xaf200000)
 concurrent-mark-sweep perm gen total 16384K, used 14333K [0xaf200000, 0xb0200000, 0xb3200000)
}
Total time for which application threads were stopped: 0.0576140 seconds
Total time for which application threads were stopped: 0.0080490 seconds
Total time for which application threads were stopped: 0.0000810 seconds
Total time for which application threads were stopped: 0.0000410 seconds
Total time for which application threads were stopped: 0.0000360 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000360 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000320 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000370 seconds
Total time for which application threads were stopped: 0.0000360 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000360 seconds
Total time for which application threads were stopped: 0.0000320 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000320 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000760 seconds
Total time for which application threads were stopped: 0.0000490 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000370 seconds
Total time for which application threads were stopped: 0.0000460 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0004150 seconds
Total time for which application threads were stopped: 0.0001230 seconds
Total time for which application threads were stopped: 0.0035150 seconds
{noformat}

The client-side socket timeout is set to 3 seconds, so it's not hitting that timeout due to garbage collections.  I should also note that the client-side error is different when there is a client socket timeout (something like {{TTransportException: timed out reading 4 bytes}}).","13/Sep/12 03:10;vijay2win@yahoo.com;Hi Tyler, I am not able to re-produce it so far. I am running 2GB/400MB on AWS M4XL....

[ec2-user@ip-10-82-21-221 ~]$ grep -i ThriftServer.java /mnt/log/cassandra/system.log 
 INFO [main] 2012-09-11 21:52:43,702 ThriftServer.java (line 112) Binding thrift service to localhost/127.0.0.1:9160
 INFO [main] 2012-09-11 21:52:43,704 ThriftServer.java (line 121) Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO [main] 2012-09-11 21:52:43,710 ThriftServer.java (line 191) Using custom half-sync/half-async thrift server on localhost/127.0.0.1 : 9160
 INFO [Thread-2] 2012-09-11 21:52:43,720 ThriftServer.java (line 200) Listening for thrift clients...
[ec2-user@ip-10-82-21-221 ~]$ 


The Timeout happens both in Sync and HSHA servers (randomly and i am not able to reproduce both cases reliably) and the only thing which i can notice is that the client (pycassa) runs 100% CPU most of the time... other than that everything else looks normal.","20/Jul/13 06:42;peter-librato;We may be seeing this behavior in 1.2.6. I haven't enabled debug but we are definitely seeing a correlation between groups of 'Read an invalid frame size of 0' messages (dozens at a time) during the same second that we're seeing ""large"" (10 seconds or more) 'GC for ConcurrentMarkSweep' events.

On a 9 node cluster we see this anywhere from 1 to 9 times a day.

","20/Jul/13 21:26;vijay2win@yahoo.com;Peter, Looks like your issue is because of the client timeout when you didn't receive a response for 10 sec. Time to tune the heap or add more nodes.

Tyler, is this ticket still valid?","23/Jul/13 19:00;thobbs;bq. Tyler, is this ticket still valid?

The changes for CASSANDRA-5582 probably make this invalid for 2.0, which I'm fine with.  I'll assign to myself, test against 2.0, close this ticket and open a new one for the 2.0 implementation if needed.","01/Aug/13 22:56;thobbs;After a quick check, it looks like the CASSANDRA-5582 implementation also doesn't enforce the max frame size.","01/Aug/13 23:15;jbellis;[~xedin], can you have a look?","02/Aug/13 00:14;xedin;It looks like strict read option was removed from TBinaryProtocol which has actually responsible for graceful failure, I also notice that Config.thrift_max_message_length_in_mb is marked as Deprecated, starting from 1.1 I can bring that back but curious is there any good reason behind that? ","02/Aug/13 00:17;jbellis;Those are not the same thing as frame length, see THRIFT-820 and CASSANDRA-5529.","02/Aug/13 00:40;xedin;I actually this I know what is the problem, I will work on solution for distruptor server asap.",02/Aug/13 04:14;xedin;[~thobbs] Please try trunk with attached patch and replace thrift-server-0.2.1.jar in lib/ with the one attached. This would detect frame size violation right when it's read from the socket and report an error as well as close connection.,"02/Aug/13 16:38;thobbs;[~xedin] that looks good to me.  There's one problem with the patch: you changed the cassandra.yaml default to {{hsha}}; other than that, +1.",02/Aug/13 16:55;xedin;Thanks! Yaml change wasn't intentional :) I will release 0.2.2 version of thrift-server and commit changes to cassandra once it's in maven central. ,06/Aug/13 22:22;xedin;Committed to 2.0.0 branch (with updated disruptor hsha server to 0.3.0).,"08/Aug/13 15:43;jbellis;[~xedin], can you add the license file for thrift-server to lib/licences?","08/Aug/13 18:31;xedin;[~jbellis] Done, licences committed to cassandra-2.0.0 for thrift-server and LMAX disruptor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replaying old batches can 'undo' deletes,CASSANDRA-5314,12635489,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,06/Mar/13 04:01,12/Mar/19 14:13,13/Mar/19 22:27,21/May/13 21:42,1.2.6,,,,,0,,,,,,,Batchlog manager does not subtract the time spent in the batchlog from hints' ttls and this may cause undoing deletes. The attached patch fixes it.,,,,,,,,,,,,,,CASSANDRA-9917,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-11 21:56:38.778,,,no_permission,,,,,,,,,,,,315982,,,Tue May 21 21:42:36 UTC 2013,,,,,,0|i1iix3:,316325,jbellis,jbellis,,,,,,,,,,06/Mar/13 04:16;iamaleksey;https://github.com/iamaleksey/cassandra/compare/5314,"11/Mar/13 21:56;jbellis;Hmm, this is tricky: as you say, we don't want to undo more-recent deletes, but we also don't want to discard other rows in the batch, or even the row with low gcgs, that may be necessary to preserve our Atomic Batch guarantees.  I.e.: replaying the batch may be incorrect, but not replaying it may also be incorrect.

I think the only solution is to require that gcgs be greater than the batch timeout we use.","12/Mar/13 18:10;slebresne;Isn't that a problem we already have with normal hints? After all, if an insert takes a long time to get delivered and you have a short gc_grace, some delete that override an hint could get gced before the hint gets delivered.

In any case, I agree with Jonathan: saying that you shouldn't have a gc_grace too short if you do deletes seems fair to me, in the sense that it's what gc_grace is about: providing some time frame after which you consider everything has been delivered.","12/Mar/13 18:31;jbellis;We TTL hints with gcgs to prevent this, iirc.","13/Mar/13 08:03;slebresne;bq. We TTL hints with gcgs to prevent this

Interesting. There is 2 (somewhat conflicting) things that seems weird to me with that:
# the intent seems broken if gc_grace==0, since a TTL of 0 means no TTL at all.
# in the (not uncommon) case of using TTL on all columns without doing manual deletion, it makes sense to use a very low gc_grace. In this case, this seems to make hints useless, which I'm not sure is what people expect (I'll admit I was unaware of that).",03/May/13 15:21;jbellis;I don't think there's a quick fix here; pushing to 2.0.,"07/May/13 14:57;iamaleksey;So, for now:
1. Apply the original patch (subtract the time spent in the batchlog from hints' ttls)
2. If the target nodes are up (of at least FD says so), try writing the mutations directly instead of hinting them
3. Reduce the batch replay interval to, say, 1 minute
4. Fix hints to not write the hint at all if it's computed TTL == 0 (when gc_grace is 0, for example).

Am I missing anything?","07/May/13 15:00;jbellis;5. Update hint ttl to be sum(gcgs, cf default ttl)

(Updating fix-for back to 1.2)","17/May/13 19:00;iamaleksey;https://github.com/iamaleksey/cassandra/compare/5314  ('push -f'd)

Haven't updated ttl here to be sum (gcgs, def ttl) since 1.2 doesn't have default ttl.","21/May/13 18:27;jbellis;Why do we add this?

{code}
        cf.addColumn(new Column(columnName(""""), ByteBufferUtil.EMPTY_BYTE_BUFFER, timestamp));
{code}

Nit: CopyOnWriteArraySet is a bit of an odd choice, since we expect to mutate it once for each entry.","21/May/13 18:48;iamaleksey;bq. Why do we add this?

It's the CQL3 row marker. Technically we don't have to do this, but it's just the right way to do it and I would sleep better with it in place. Could prevent something like CASSANDRA-5572 from happening again in the future (although it's true that we only remove the whole rows in the batchlog so the probability of something like this is low).

bq. Nit: CopyOnWriteArraySet is a bit of an odd choice, since we expect to mutate it once for each entry.

It's mostly irrelevant here - just used the first thread-safe set that came to mind.",21/May/13 18:56;jbellis;+1,"21/May/13 21:42;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER TABLE ADD - data loss,CASSANDRA-5232,12631375,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,vasily.v.d,vasily.v.d,08/Feb/13 02:21,12/Mar/19 14:13,13/Mar/19 22:27,08/Feb/13 17:54,1.2.2,,,,,0,,,,,,,"cqlsh:Test> CREATE TABLE t1 (id int PRIMARY KEY, t text);
cqlsh:Test> UPDATE t1 SET t = '111' WHERE id = 1;
cqlsh:Test> ALTER TABLE t1 ADD l list<text>;
cqlsh:Test> SELECT * FROM t1;

 id | l    | t
----+------+-----
  1 | null | 111

cqlsh:Test> ALTER TABLE t1 ADD m map<int, text>;
cqlsh:Test> SELECT * FROM t1;
cqlsh:Test>

Last query doesn't return any data.
",,,,,,,,,,,,,,,,,,,08/Feb/13 13:02;slebresne;5232.txt;https://issues.apache.org/jira/secure/attachment/12568562/5232.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-08 13:02:09.591,,,no_permission,,,,,,,,,,,,311871,,,Fri Feb 08 17:54:49 UTC 2013,,,,,,0|i1htkf:,312217,jbellis,jbellis,,,,,,,,,,"08/Feb/13 13:02;slebresne;That's a bug in DataTracker.replaceFlushed, that doesn't handle correctly the case where there is no sstable created by the flush (and in that case one such flush is triggered by the ALTER). Namely, it ""removes"" (from the DataTracker instance, not from disk) all existing sstables. This actually comes from the CASSANDRA-4667 (the second patch committed). So in practice that means that ALTER TABLE is just one of the thing this broke, but this also break the atomic batch guarantees (as the batchlog sstables might be discarded wrongfully).

Trivial patch attached.
","08/Feb/13 13:09;slebresne;Btw, in the test case above the data is not really lost. Restarting the node should have him pick up the ignored sstables.",08/Feb/13 15:25;jbellis;+1,"08/Feb/13 17:54;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deletable rows are sometimes not removed during compaction,CASSANDRA-5182,12628835,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,binhnv,binhnv,23/Jan/13 01:22,12/Mar/19 14:13,13/Mar/19 22:27,04/Mar/13 19:37,1.2.3,,,,,0,,,,,,,"Our use case is write heavy and read seldom.  To optimize the space used, we've set the bloom_filter_fp_ratio=1.0  That along with the fact that each row is only written to one time and that there are more than 20 SSTables keeps the rows from ever being compacted. Here is the code:
https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/db/compaction/CompactionController.java#L162
We hit this conner case and because of this C* keeps consuming more and more space on disk while it should not.",,,,,,,,,,,,,,,,,,,23/Jan/13 23:26;yukim;5182-1.1.txt;https://issues.apache.org/jira/secure/attachment/12566217/5182-1.1.txt,06/Feb/13 15:15;yukim;5182-1.2.txt;https://issues.apache.org/jira/secure/attachment/12568232/5182-1.2.txt,23/Jan/13 01:43;binhnv;test_ttl.tar.gz;https://issues.apache.org/jira/secure/attachment/12566071/test_ttl.tar.gz,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-01-23 20:42:45.491,,,no_permission,,,,,,,,,,,,308314,,,Mon Mar 04 19:37:12 UTC 2013,,,,,,0|i1b46f:,272975,slebresne,slebresne,,,,,,,,,,"23/Jan/13 01:43;binhnv;Here is simple code to reproduce the issue. The simple code uses 10 threads to continuously write to C*. All the column has 300 in its TTL. While running the code, check the number of sstables of column family, you will see that it keeps growing and never stop
Here are steps to run:
- Start C* on your localhost
- Create a keyspace named test
- Use the following command to create column family cf
CREATE COLUMN FAMILY cf WITH comparator = UTF8Type AND key_validation_class = UTF8Type AND default_validation_class = UTF8Type AND gc_grace = 0 AND caching = none AND bloom_filter_fp_chance = 1.0 AND compaction_strategy='LeveledCompactionStrategy' AND compaction_strategy_options = { sstable_size_in_mb : 1 } AND compression_options = { chunk_length_kb : 64, sstable_compression : 'SnappyCompressor' };

- Extract the code switch to extracted directory
- Run mvn package
- Run java -jar target/test_ttl-1.0-SNAPSHOT-jar-with-dependencies.jar 10
",23/Jan/13 20:42;batalbot;A mailing list thread with more details about the use case and symptoms can be found at http://www.mail-archive.com/user@cassandra.apache.org/msg27049.html,"23/Jan/13 23:26;yukim;Binh,

Thanks for investigation. You are right that purging row depends on bloom filter check, so if you have bloomfilter_fp_chance of 1.0, it is very likely that the row is not going to be purged.

In 1.2 and above, we use AlwaysPresentFilter which always returns true for row presence when fp_change is 1.0, so the row is never going to be purged when you set fp_change=1.0.

Simple fix is attached. Instead of only hitting bloom filter, we check through key cache and index file for actual row presence.","23/Jan/13 23:32;yukim;Maybe it is better to check if fp_chance is high before going through index file, since it has performance penalty.","23/Jan/13 23:52;binhnv;I agreed that we should find a better way since getPosition will check bloom filter, key cache and in the worst case (which is our case) it will scan whole index table. This will cause the performance issue.","24/Jan/13 00:01;jbellis;Do you want a performance issue, or do you only want to remove tombstones during major compaction? :)","24/Jan/13 00:02;jbellis;Personally I am +1 on the fix; if you run a lot of deletes and can't cache your index files in ram, then don't disable bloom filters.","24/Jan/13 00:25;batalbot;Using the test program attached, I've reproduce the problem using 1.1.9 and then upgraded that cluster (1 node on laptop) to 1.2.0.  The problem remains with the load and sstable count increasing.

However, when I run the test program on a fresh 1.2.0 cluster the problem does not come up.  My process to reproduce on upgrade is:

install fresh 1.1.9
run test to get 500 MB of data (20-30 mins)
drain and shutdown 1.1.9
start 1.2.0
run nodetool upgradesstables
run test and watch load grow to 2.5 GB while away at lunch


When running the test program on a fresh 1.2.0 installation, the load tops out at about 200 MB and 90 or so SSTables which is what is desired.
","24/Jan/13 00:28;batalbot;About the check for a high fp_chance before checking indexes.  Did you mean to only check index files if fp_chance is high (say over 0.5 or something)?  That way the additional check is only incurred with bloom filters are effectively disabled and the common case using an effective (low fp) bloom filter is not impacted.
","24/Jan/13 01:24;jbellis;getPosition does the right thing here: it checks the index file only on bloom filter positives, so a high bloom filter setting will benefit automatically.

The only improvement I think makes sense would be adding support for compaction strategy tombstone threshold.","24/Jan/13 08:42;slebresne;bq. Maybe it is better to check if fp_chance is high before going through index file

Actually, I agree with Yuki on that and I'm kind of -1 on the patch in his current form. The current patch means that whatever your fp_chance is, each time the row is indeed present in a non compacted sstable (which does prevent gcing the row for this compaction but is not something that will necessarily be rare) might hit the disk (unless the key cache save you). So I'd be in favor of using getPosition only if fp_chance == 1, at least on 1.1 as we have no idea of the impact this can have on people that haven't disabled bloom filter and have no problem whatsoever with gcing tombstone.

As a side note, I've opened CASSANDRA-5183 that is related to this purge tombstone problem.","24/Jan/13 14:28;jbellis;I don't think we should touch 1.1 at all.  Updated fixver, and updated affectsver to when we added configurable bf_fp_chance.",06/Feb/13 15:15;yukim;Patch attached for 1.2 and above. It checks index file using getPosition if sstable has AlwaysPresentFilter as a bloom filter.,"12/Feb/13 23:37;jbellis;I'm still not comfortable with this.

If our goal is to throw out the maximum possible amount of obsolete data, we should perform getPosition across the board.

But if our goal is to be minimally impactful with compaction then we shouldn't do it at all, and rely instead on the timestamp check.  If that's not enough, then you shouldn't disable bloom filters on workloads that perform deletes.  I'm okay with that message.","12/Feb/13 23:53;batalbot;Our use case doesn't require maximum effort to delete rows.  What we ran into was an unexpected interaction between two features: bloom filter tuned for low read rate, and deleting tombstoned rows.  With that configuration NO rows were being removed.  

As long as there is some reasonable effort to remove rows with bloom filter disabled OR it's clearly known that a reasonable FP setting is required to remove tombstones, I think we could have avoided a lot of headaches.

How does the new tombstone histogram feature in 1.2 affect this issue?  If that feature solves the problem already, maybe this fix is irrelevant.
","13/Feb/13 09:48;slebresne;bq. If our goal is to throw out the maximum possible amount of obsolete data

I kind agree with Bryan, this doesn't have to be black and white. What we want is doing the best we can to remove obsolete rows without impacting compaction too much. Now if you do have active bloom filters, then I think just checking the bloom filters as we do now is the right trade-off: it maximize  with a very high probability the amount of removed data at the relatively cheap cost. Using getPosition in that case would be a bad idea, because the reward (a tiny fraction of additional data potentially removed) is not worth the cost (hitting disk each time a row we compact is also in a non-compacted sstable) imo, hence my opposition to the idea.

But if you deactivate bloom filters, you also fully destroy our bloom filter trade-off. So using getPosition does now provide a substantial benefit as it allows to go from 'no deletion' to 'maximize deletion'. The reward is, in that case, likely worth the cost, especially since people shouldn't desactivate bloom filters unless their index files fits in memory, in which case getPosition costs won't be that big.

So overall I do like the last patch attached by Yuki. Of course, the solution of just saying ""you shouldn't disable bloom filters on workloads that perform deletes"" works too, and I wouldn't oppose it, but it doesn't have my preference because I'm always a bit afraid of solving an issue by saying ""don't do this"", as it usually end up in people getting bitten first and hearing they shouldn't have done it second. ","01/Mar/13 15:02;jbellis;bq. So overall I do like the last patch attached by Yuki. Of course, the solution of just saying ""you shouldn't disable bloom filters on workloads that perform deletes"" works too, and I wouldn't oppose it, but it doesn't have my preference because I'm always a bit afraid of solving an issue by saying ""don't do this"", as it usually end up in people getting bitten first and hearing they shouldn't have done it second. 

The problem is it's not as simple as ""people get bitten if we don't getPosition, and don't if we do"" -- they get bitten either way, and IMO the bite from getPosition is worse, since it will destroy compaction performance for any workload where index doesn't fit entirely in ram, which makes BF disabling almost useless.  But if we say ""only disable BF where you're not doing deletes,"" it has a legitimate if narrow use case.","01/Mar/13 16:35;slebresne;bq.  if we say ""only disable BF where you're not doing deletes,"" it has a legitimate if narrow use case

I guess I agree on the principle that we should say ""only disable BF where you're not doing deletes"". That being said, if we do use getPosition, we extend the possible use cases, since it become ""only disable BF where you're not doing deletes or your index fit entirely in RAM"" (because getPosition will not destroy performance for the ""not doing delete case"", since we don't even call shouldPurge() unless we know there is tombstones).

bq. and IMO the bite from getPosition is worse, since it will destroy compaction performance

I'm not totally sure I agree on the worse. As said above, if people have not tombstone, it won't destroy compaction performance. So I guess the question is: for people that 1) do not follow recommendation (cause we should definitively say when disabling BF is ok or not) and that 2) do have deletes, is it better for them to be bitten by a) bad compaction performance or b) their tombstones not being purged ever.

I don't doubt that which of a) or b) is worse is a matter of perspective. That being said, my own personal preference goes to avoiding because:
* to me b) is a break of correctness which somewhat trumps performance consideration. It purely subjective though.
* accumulating tombstones forever is a pretty nasty time-bomb. Having compaction being slow because it hit disk more than it should on the other seems easier to me to detect (and thus fix by following the recommendation of not disabling BF when you shouldn't).

So, I still have a preference for using Yuki's last patch (and making it clear that you shall ""only disable BF where you're not doing deletes or your index fit entirely in RAM""). If only because that's a bit better than ""only disable BF where you're not doing deletes"". But if you still prefer keeping the status quo, I won't oppose, do feel free to close that issue (we still should write the recommendation on when to disable BF somewhere in any case).","01/Mar/13 21:07;jbellis;bq. if we do use getPosition, we extend the possible use cases, since it become ""only disable BF where you're not doing deletes or your index fit entirely in RAM""

That makes sense.  Let's ship Yuki's patch.",04/Mar/13 19:37;yukim;Committed to 1.2 and above. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in DataTracker.markCompacting,CASSANDRA-5214,12630359,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,01/Feb/13 11:53,12/Mar/19 14:13,13/Mar/19 22:27,04/Feb/13 16:51,1.2.2,,,,,0,,,,,,,"On 1.2 branch:

{noformat}
 INFO [main] 2013-02-01 05:50:07,709 CassandraDaemon.java (line 103) Logging initialized
 INFO [main] 2013-02-01 05:50:07,730 CassandraDaemon.java (line 125) JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26
 INFO [main] 2013-02-01 05:50:07,731 CassandraDaemon.java (line 126) Heap size: 1046937600/1046937600
 INFO [main] 2013-02-01 05:50:07,731 CassandraDaemon.java (line 127) Classpath: /tmp/dtest-4ju3_j/test/node1/conf:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/build/classes/main:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/build/classes/thrift:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/antlr-3.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/avro-1.4.0-fixes.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/avro-1.4.0-sources-fixes.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/commons-cli-1.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/commons-codec-1.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/commons-lang-2.6.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/compress-lzf-0.8.4.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/concurrentlinkedhashmap-lru-1.3.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/guava-13.0.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/high-scale-lib-1.1.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jackson-core-asl-1.9.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jackson-mapper-asl-1.9.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jamm-0.2.5.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jline-1.0.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/json-simple-1.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/libthrift-0.7.0.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/log4j-1.2.16.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/metrics-core-2.0.3.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/netty-3.5.9.Final.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/servlet-api-2.5-20081211.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/slf4j-api-1.7.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/slf4j-log4j12-1.7.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/snakeyaml-1.6.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/snappy-java-1.0.4.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/snaptree-0.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jamm-0.2.5.jar
 INFO [main] 2013-02-01 05:50:07,733 CLibrary.java (line 61) JNA not found. Native methods will be disabled.
 INFO [main] 2013-02-01 05:50:07,748 DatabaseDescriptor.java (line 131) Loading settings from file:/tmp/dtest-4ju3_j/test/node1/conf/cassandra.yaml
 INFO [main] 2013-02-01 05:50:08,168 DatabaseDescriptor.java (line 190) DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO [main] 2013-02-01 05:50:08,168 DatabaseDescriptor.java (line 204) disk_failure_policy is stop
 INFO [main] 2013-02-01 05:50:08,174 DatabaseDescriptor.java (line 265) Global memtable threshold is enabled at 332MB
 INFO [main] 2013-02-01 05:50:08,911 CacheService.java (line 111) Initializing key cache with capacity of 49 MBs.
 INFO [main] 2013-02-01 05:50:08,923 CacheService.java (line 140) Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO [main] 2013-02-01 05:50:08,924 CacheService.java (line 154) Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
 INFO [main] 2013-02-01 05:50:08,931 CacheService.java (line 166) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [main] 2013-02-01 05:50:09,438 DatabaseDescriptor.java (line 542) Couldn't detect any schema definitions in local storage.
 INFO [main] 2013-02-01 05:50:09,440 DatabaseDescriptor.java (line 545) Found table data in data directories. Consider using the CLI to define your schema.
 INFO [CompactionExecutor:1] 2013-02-01 05:50:09,579 ColumnFamilyStore.java (line 678) Enqueuing flush of Memtable-local@524805736(133/133 serialized/live bytes, 6 ops)
 INFO [FlushWriter:1] 2013-02-01 05:50:09,592 Memtable.java (line 447) Writing Memtable-local@524805736(133/133 serialized/live bytes, 6 ops)
 INFO [FlushWriter:1] 2013-02-01 05:50:09,670 Memtable.java (line 481) Completed flushing /tmp/dtest-4ju3_j/test/node1/data/system/local/system-local-ib-1-Data.db (176 bytes) for commitlog position ReplayPosition(segmentId=1359719409398, position=425)
ERROR [CompactionExecutor:2] 2013-02-01 05:50:09,681 CassandraDaemon.java (line 135) Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.AssertionError
    at org.apache.cassandra.db.DataTracker.markCompacting(DataTracker.java:183)
    at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:128)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:185)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-02-04 16:51:17.11,,,no_permission,,,,,,,,,,,,310854,,,Mon Feb 04 16:51:17 UTC 2013,,,,,,0|i1hnaf:,311199,,,,,,,,,,,,01/Feb/13 12:26;brandon.williams;CASSANDRA-5151 looks like my guess here.,04/Feb/13 16:51;jbellis;should be fixed in 82de0ec75689d84ee6a4fa2d9a960f3326e5387a.  (CliTest no longer throws.),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cql3 token queries broken,CASSANDRA-5050,12623218,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,10/Dec/12 21:34,12/Mar/19 14:13,13/Mar/19 22:27,11/Dec/12 07:55,1.2.0 rc1,,,,,0,,,,,,,"Currently any select statement that uses a token() predicate breaks with ""Bad Input""

After tracing the logic this error is caused in getTokenBounds because it assumes the token term is an actual token string what will pass the tokenizer

",,,,,,,,,,,,,,,,,,,10/Dec/12 21:35;tjake;5050.txt;https://issues.apache.org/jira/secure/attachment/12560285/5050.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-11 07:55:31.409,,,no_permission,,,,,,,,,,,,296831,,,Tue Dec 11 07:55:31 UTC 2012,,,,,,0|i14hkv:,234311,slebresne,slebresne,,,,,,,,,,"11/Dec/12 07:55;slebresne;+1, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix cqlsh after move of CL to the protocol level,CASSANDRA-4823,12612167,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,slebresne,slebresne,17/Oct/12 07:59,12/Mar/19 14:13,13/Mar/19 22:27,19/Oct/12 19:48,1.2.0 beta 2,,Legacy/Tools,,,0,,,,,,,"CASSANDRA-4734 moved the consistency level at the protocol level (and in doing so, separated the cql3 thrift methods from the cql2 ones). We should adapt cqlsh to that change.",,,,,,,,,,,,,,,,,,,18/Oct/12 23:58;iamaleksey;CASSANDRA-4823.txt;https://issues.apache.org/jira/secure/attachment/12549785/CASSANDRA-4823.txt,19/Oct/12 16:48;iamaleksey;cql-internal-only-1.4.0.zip;https://issues.apache.org/jira/secure/attachment/12550003/cql-internal-only-1.4.0.zip,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-19 19:48:29.01,,,no_permission,,,,,,,,,,,,249230,,,Fri Oct 19 19:48:29 UTC 2012,,,,,,0|i0a68n:,57284,brandon.williams,brandon.williams,,,,,,,,,,19/Oct/12 19:48;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh can't describe system tables,CASSANDRA-4863,12613503,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,jbellis,jbellis,25/Oct/12 15:12,12/Mar/19 14:13,13/Mar/19 22:27,26/Oct/12 11:42,1.2.0 beta 2,,Legacy/Tools,,,0,cqlsh,,,,,,"{noformat}
cqlsh> describe table system_traces.sessions;

Unconfigured column family 'sessions'
{noformat}",,,,,,,,,,,,,,,,,,,25/Oct/12 18:00;iamaleksey;4863.txt;https://issues.apache.org/jira/secure/attachment/12550818/4863.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-25 17:59:55.124,,,no_permission,,,,,,,,,,,,251043,,,Fri Oct 26 11:42:24 UTC 2012,,,,,,0|i0b3h3:,62675,brandon.williams,brandon.williams,,,,,,,,,,"25/Oct/12 17:59;iamaleksey;This has nothing to do with cql3. Well, not exclusively with cql3. It's about system cql3 tables. The fix is trivial now that we have info on system tables in system.schema_*.
",26/Oct/12 11:42;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL2 CREATE COLUMNFAMILY checks wrong permission,CASSANDRA-4864,12613595,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,26/Oct/12 00:39,12/Mar/19 14:13,13/Mar/19 22:27,26/Oct/12 01:12,1.1.7,1.2.0 beta 2,,,,0,,,,,,,"Permission is asked for resource [cassandra, keyspaces, <column family name>] instead of [cassandra, keyspaces, <keyspace name>]",,,,,,,,,,,,,,,,,,,26/Oct/12 00:40;iamaleksey;4864.txt;https://issues.apache.org/jira/secure/attachment/12550901/4864.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-26 01:12:36.024,,,no_permission,,,,,,,,,,,,251153,,,Fri Oct 26 01:12:36 UTC 2012,,,,,,0|i0b53r:,62939,jbellis,jbellis,,,,,,,,,,26/Oct/12 01:12;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken get_paged_slice,CASSANDRA-4816,12612041,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,pkolaczk,pkolaczk,pkolaczk,16/Oct/12 14:21,12/Mar/19 14:13,13/Mar/19 22:27,24/Oct/12 19:48,1.1.7,1.2.0 beta 2,,,,0,,,,,,,"get_paged_slice doesn't reset the start column filter for the second returned row sometimes. So instead of getting a slice:

row 0: <start_column>...<last_column_in_row>
row 1: <first column in a row>...<last_column_in_row>
row 2: <first column in a row>...

you sometimes get:

row 0: <start_column>...<last_column_in_row>
row 1: <start_column>...<last_column_in_row>
row 2: <first column in a row>...
",,,,,,,,,,,,,,,,,,,16/Oct/12 17:23;slebresne;4816-2.txt;https://issues.apache.org/jira/secure/attachment/12549345/4816-2.txt,17/Oct/12 07:52;pkolaczk;4816-3.txt;https://issues.apache.org/jira/secure/attachment/12549461/4816-3.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-16 15:33:18.127,,,no_permission,,,,,,,,,,,,249001,,,Wed Oct 24 19:48:43 UTC 2012,,,,,,0|i0a3ev:,56826,slebresne,slebresne,,,,,,,,,,"16/Oct/12 15:33;slebresne;My bet would be that this is due to the memtable iterator. The way we implement get_paged_slice, we reset the start of the QueryFilter used after the first row have been read. Which means that for each row, we need to make sure the filter is not used until we've processed the preceding row. This is the case for the sstable iterator, but not for the memtable one. Attaching patch to change that (I haven't tested the patch though).","16/Oct/12 16:14;slebresne;Scratch that patch, brain fart. The AbstractIterator.computeNext() is called no sooner than on hasNext() so we should be good on that front. Still don't know what is the problem here.","16/Oct/12 17:23;slebresne;Ok, second attempt. Looking more closing it does seem that this is a problem with the interaction of the mergeIterator and the SSTableScanner. Basically the mergeIterator always needs to know what his ""the next row"" (during the reducing phase). If that next row was the one that the reducer ended up returning, we were fine (so with 1 sstable or if all sstables had the same rows, it was ok), but otherwise it might end up using the QueryFilter before it should for our get_paged_slice ""hack"".

Anyway, all that the mergeIterator needs during its reduction phase is to know the next key. So attaching a patch that delay the use of the filter until the row data is actually queried.","17/Oct/12 07:52;pkolaczk;Attaching 3rd version of the patch. LazyColumnIterator is used both for SSTableScanners and Memtable.
This version works for me.",24/Oct/12 06:36;cdaw;+1 this version fixes my tests as well.,"24/Oct/12 06:57;slebresne;To be clear, I'm also good on version 3 but I'll let Jonathan review since I've wrote version 2 on which version 3 is based.","24/Oct/12 19:48;jbellis;LGTM, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming can put files in the wrong location,CASSANDRA-4788,12611230,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,10/Oct/12 19:35,12/Mar/19 14:13,13/Mar/19 22:27,22/Oct/12 21:48,1.2.0 beta 2,,,,,2,,,,,,,"Some, but not all streaming incorrectly puts files in the top level data directory.  Easiest way to repro that I've seen is bootstrap where it happens 100% of the time, but other operations like move and repair seem to do the right thing.",,,,,,,,,,,,,,,,,,,19/Oct/12 23:07;yukim;4788.txt;https://issues.apache.org/jira/secure/attachment/12550102/4788.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-19 23:07:45.946,,,no_permission,,,,,,,,,,,,247101,,,Mon Nov 12 15:19:14 UTC 2012,,,,,,0|i07z47:,44456,brandon.williams,brandon.williams,,,,,,,,,,"19/Oct/12 23:07;yukim;You are right. Streaming writes file directly under the data directory.
Patch attached.",19/Oct/12 23:12;brandon.williams;Can you explain why this only seemed to affect bootstrap?,"22/Oct/12 15:53;yukim;Actually, it does regardless of streaming type. I observed it with sstableloader.
SSTables are put in right place when they get compacted, so maybe that's why you only see when bootstrapping.",22/Oct/12 21:22;brandon.williams;Thanks. +1,22/Oct/12 21:48;yukim;Committed in 8c99a376980b90faf7b1ca1aa33d9fde0a356cda.,12/Nov/12 15:11;jbellis;What would have caused this that is 1.2-specific?  Superficially this sounds like it is related to CASSANDRA-2749.,"12/Nov/12 15:19;yukim;This came from CASSANDRA-4292, so it was only affected on 1.2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bulk loader won't work with CQL3,CASSANDRA-4755,12610134,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,nickmbailey,nickmbailey,03/Oct/12 20:46,12/Mar/19 14:12,13/Mar/19 22:27,22/Oct/12 16:48,1.2.0 beta 2,,Legacy/Tools,,,0,,,,,,,"Currently the bulk loader uses thrift to get the schema and validate it before bulk loading a cf. Since we stopped returning cql3 cfs through describe_keyspaces, the bulk loader will be unable to load those cfs.

If we figure out/add a way to validate the schema over jmx, we could use that for getting token ranges as well and drop thrift completely from the bulk loader.

Another option might be querying system tables manually to validate things.",,,,,,,,,,,,,,,,,,,20/Oct/12 22:17;iamaleksey;CASSANDRA-4755.txt;https://issues.apache.org/jira/secure/attachment/12550154/CASSANDRA-4755.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-03 21:46:25.619,,,no_permission,,,,,,,,,,,,240680,,,Mon Oct 22 16:48:36 UTC 2012,,,,,,0|i014jb:,4504,jbellis,jbellis,,,,,,,,,,"03/Oct/12 21:46;jbellis;I think I'd prefer just converting that code to use a CQL query or two, all it cares about is ""is this ks/cf pair valid,"" which isn't a big deal to fix.

Side note: looks like some code duplication in BulkLoader.ExternalClient and BulkRecordWriter.External client, be nice to clean that up.","20/Oct/12 22:28;iamaleksey;Haven't dealt with BulkRecordWriter.External/BulkLoader.ExternalClient duplication. The attached patch fixes the BulkLoader+CQL3 issue and does nothing beyond that.

Please create a new issue for removing the duplication (or tell me to create it) if you still think it's worth the effort. But that would have a different priority compared to this issue, so I've dealt with the bug first.","22/Oct/12 16:48;jbellis;LGTM, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tombstone estimation needs to avoid using global partitioner against index sstables,CASSANDRA-4404,12596837,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,jbellis,jbellis,02/Jul/12 22:26,12/Mar/19 14:12,13/Mar/19 22:27,03/Jul/12 19:37,1.2.0 beta 1,,,,,0,compaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-07-03 19:37:06.009,,,no_permission,,,,,,,,,,,,256131,,,Wed Jul 04 23:16:47 UTC 2012,,,,,,0|i0gvif:,96535,jbellis,jbellis,,,,,,,,,,"03/Jul/12 19:37;yukim;Patch is available at https://github.com/yukim/cassandra/commit/1644a5b701b054b646e049ef5cf725b2b2670709.diff
Reviewed and committed while JIRA is down.","04/Jul/12 23:16;hudson;Integrated in Cassandra #1646 (See [https://builds.apache.org/job/Cassandra/1646/])
    use proper partitioner for Range; patch by yukim, reviewed by jbellis for CASSANDRA-4404 (Revision d2b60f28935466f6e37fc9d64a44c5c81bc14fb4)

     Result = ABORTED
yukim : 
Files : 
* src/java/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategy.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableNamesIterator misses some tombstones,CASSANDRA-4395,12596377,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,29/Jun/12 11:07,12/Mar/19 14:12,13/Mar/19 22:27,29/Jun/12 16:34,1.2.0 beta 1,,,,,0,,,,,,,The title says it all.,,,,,,,,,,,,,,,,,,,29/Jun/12 11:09;slebresne;4395.txt;https://issues.apache.org/jira/secure/attachment/12533972/4395.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-29 16:14:50.764,,,no_permission,,,,,,,,,,,,256123,,,Fri Jun 29 16:34:04 UTC 2012,,,,,,0|i0gven:,96518,yukim,yukim,,,,,,,,,,29/Jun/12 11:09;slebresne;Patch attached to fix (the tombstone were only skipped in the case where the row was not indexed). The patch also adds some unit tests for range tombstones.,29/Jun/12 16:14;yukim;+1,"29/Jun/12 16:34;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stackoverflow building interval tree & possible sstable corruptions,CASSANDRA-4321,12559783,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,awinter,awinter,08/Jun/12 02:00,12/Mar/19 14:12,13/Mar/19 22:27,29/Jun/12 09:20,1.1.2,,,,,5,lcs,,,,,,"After upgrading to 1.1.1 (from 1.1.0) I have started experiencing StackOverflowError's resulting in compaction backlog and failure to restart. 

The ring currently consists of 6 DC's and 22 nodes using LCS & compression.  This issue was first noted on 2 nodes in one DC and then appears to have spread to various other nodes in the other DC's.  

When the first occurrence of this was found I restarted the instance but it failed to start so I cleared its data and treated it as a replacement node for the token it was previously responsible for.  This node successfully streamed all the relevant data back but failed again a number of hours later with the same StackOverflowError and again was unable to restart. 

The initial stack overflow error on a running instance looks like this:

ERROR [CompactionExecutor:314] 2012-06-07 09:59:43,017 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:314,1,main]
java.lang.StackOverflowError
        at java.util.Arrays.mergeSort(Arrays.java:1157)
        at java.util.Arrays.sort(Arrays.java:1092)
        at java.util.Collections.sort(Collections.java:134)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.findMinMedianMax(IntervalNode.java:114)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:49)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow.  Compactions stop from this point onwards]


I restarted this failing instance with DEBUG logging enabled and it throws the following exception part way through startup:

ERROR 11:37:51,046 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.db.Table$2.apply(Table.java:574)
        at org.apache.cassandra.db.Table$2.apply(Table.java:571)
        at com.google.common.collect.Iterators$8.next(Iterators.java:751)
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1625)
        at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:118)
        at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:45)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
DEBUG 11:37:51,052 Initializing ksU.cfS


And then finally fails with the following:

DEBUG 11:49:03,752 Creating IntervalNode from [Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b))]
java.lang.reflect.InvocationTargetException
DEBUG 11:49:03,753 Configured datacenter replicas are dc1:2, dc2:2, dc3:2, dc4:2, dc5:0, dc6:2, dc7:0, dc8:0, dc9:2
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3

Running with assertions enabled allows me to start the instance but when doing so I get errors such as:

ERROR 01:22:22,753 Exception in thread Thread[SSTableBatchOpen:2,5,main]java.lang.AssertionError: SSTable first key DecoratedKey(100294972947100949193477090306072672386, 4fcf051ef5067d7f17d9fc35) > last key DecoratedKey(90250429663386465697464050082134975058, 4fce996e3c1eed8c4b17dd66)
at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:412)
at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:187)
at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:225)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:636)

and:

ERROR 01:27:58,946 Exception in thread Thread[CompactionExecutor:9,1,main]
java.lang.AssertionError: Last written key DecoratedKey(81958437188197992567937826278457419048, 4fa1aebad23f81e4321d344d) >= current key DecoratedKey(64546479828744423263742604083767363606, 4fcafc0f19f6a8092d4d4f94) writing into /var/lib/XX/data/cassandra/ks1/cf1/ks1-cf1-tmp-hd-657317-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

Just like the initial errors compactions appear to stop occurring after this point.  

Given the above this looks like sstables are getting corrupted.  By restarting nodes I am able to identify several hundred sstables exhibiting the same problem and this appears to be growing.

I have tried scrubbing those affected nodes but the problem continues to occur.  If this is due to sstable corruptions is there another way of validating sstables for correctness?  Given that it has spread to various servers in other DC's it looks like this is directly related to the 1.1.1 upgrade recently performed on the ring.",,,,,,,,,,,,,,,,,,,28/Jun/12 17:54;slebresne;0001-Fix-overlapping-computation-v7.txt;https://issues.apache.org/jira/secure/attachment/12533857/0001-Fix-overlapping-computation-v7.txt,28/Jun/12 17:54;slebresne;0002-Scrub-detects-and-repair-outOfOrder-rows-v7.txt;https://issues.apache.org/jira/secure/attachment/12533858/0002-Scrub-detects-and-repair-outOfOrder-rows-v7.txt,28/Jun/12 17:54;slebresne;0003-Create-standalone-scrub-v7.txt;https://issues.apache.org/jira/secure/attachment/12533859/0003-Create-standalone-scrub-v7.txt,28/Jun/12 17:54;slebresne;0004-Add-manifest-integrity-check-v7.txt;https://issues.apache.org/jira/secure/attachment/12533860/0004-Add-manifest-integrity-check-v7.txt,28/Jun/12 18:44;jbellis;cleanup.txt;https://issues.apache.org/jira/secure/attachment/12533865/cleanup.txt,14/Jun/12 20:18;al@ooyala.com;ooyala-hastur-stacktrace.txt;https://issues.apache.org/jira/secure/attachment/12532132/ooyala-hastur-stacktrace.txt,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2012-06-08 02:31:17.309,,,no_permission,,,,,,,,,,,,249133,,,Thu Jul 05 04:53:11 UTC 2012,,,,,,0|i0a4zr:,57082,jbellis,jbellis,,,,,,,,,,"08/Jun/12 02:31;jbellis;bq. SSTable first key DecoratedKey(100294972947100949193477090306072672386, 4fcf051ef5067d7f17d9fc35) > last key DecoratedKey(90250429663386465697464050082134975058, 4fce996e3c1eed8c4b17dd66)

Cassandra checks key ordering for correctness with every row that is added at write time, so unless you changed your partitioner (which is emphatically Not Supported), I'm not sure how this can happen.  Whatever it is, it's unlikely to be related to the 1.1.1 upgrade.

Scrub checks that the sstable content is well-formed and readable.  It doesn't check for out-of-order rows.  You can use a tool like sstablekeys to do that.",08/Jun/12 02:48;awinter;The partitioner (RP) was not changed.,"08/Jun/12 16:15;jsotelo;We are also seeing this. We also upgraded from 1.1.0 to 1.1.1. This problem only started after the upgrade. Our cluster is smaller, two DCs of 3 nodes each.","08/Jun/12 16:27;jsotelo;Our stack overflow is a little different though:


ERROR [CompactionExecutor:35] 2012-06-08 15:52:42,086 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:35,1,main]
java.lang.StackOverflowError
        at java.util.AbstractList$Itr.hasNext(Unknown Source)
        at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
        at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)

[repeats]

        at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
        at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
        at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at com.google.common.collect.Iterators.size(Iterators.java:129)
        at com.google.common.collect.Sets$3.size(Sets.java:670)
        at com.google.common.collect.Iterables.size(Iterables.java:80)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:557)
        at org.apache.cassandra.db.compaction.CompactionController.<init>(CompactionController.java:79)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:105)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)

","08/Jun/12 20:17;omid;We're seeing the same issue after upgrading from 1.0.9 to 1.1.1 on only a single node in a 16 node cluster. Wiping the data off and bootstrapping again didn't help. Compaction looks to be not progressing (according to compactionstats) and I can reproduce this on every ""nodetool flush"".","11/Jun/12 12:26;jbellis;Omid, just to verify -- are you also using LeveledCompactionStrategy?","11/Jun/12 12:34;omid;Jonathan, yes I use LeveledCompactionStrategy with non-default sstable_size_in_mb = 10","11/Jun/12 15:44;slebresne;I believe I may have an idea on what's going on here. When computing overlapping sstables, LeveledManifest was using Range, which excludes its left bound, but sstable intervals are fully inclusive. This means the computation is incorrect when multiple sstables have the same start token. But that in turn is very much possible across levels. This also seem to fit the stack traces above as those mention interval trees where all the sstable actually have the same first and last token.

This is not really a new bug, but I believe that prior to CASSANDRA-4142, this had less consequences.

Attaching a patch that fixes this by using Bounds instead of Range. But since this but ended up creating sstables with out of order keys, I'm also attaching a second patch that add the ability to scrub to detect this and ""repair"" the situation. This is far from perfect in that the way to ""repair"" consists in buffering the out of order rows and write them all in order in a new sstable. So this can easily OOM if the sstable has lots of out of order rows. But I suppose this is better than nothing. There is a unit test included to check that new feature of scrub.

Note that I believe this patch (at least the first one) must be committed to 1.0 too.","11/Jun/12 15:53;jbellis;bq. this can easily OOM if the sstable has lots of out of order rows

Is that likely, given that Range vs Bounds is basically an off-by-one?",11/Jun/12 16:03;jbellis;Backing up: how does incorrect overlapping sets result in out-of-order key writes (that pass that last-written-key check)?,"11/Jun/12 16:04;slebresne;bq. Is that likely, given that Range vs Bounds is basically an off-by-one

The thing is, even an off-by-one is enough to have essentially 2 identical sstable on the same level. If so, our new by-level-iterator will happily write a new sstable that is a concatenation of both of those and we'll end up with half of the resulting sstable being out of order.

That being said, now that you mention it, leveled limits the size of sstables so we should be good on that front.",11/Jun/12 16:04;jbellis;Comment on the patch itself: intersects is missing the case of {{that}} entirely containing {{this}}.,"11/Jun/12 18:09;slebresne;bq. how does incorrect overlapping sets result in out-of-order key writes (that pass that last-written-key check)?

The last-written-key check is an assertion, so if assertion are disabled this may happen. And the following excerpt of the description above make me believe assertions weren't enabled when the problem occured first:
{noformat}
Running with assertions enabled allows me to start the instance but when doing so I get errors such as:
ERROR 01:22:22,753 Exception in thread Thread[SSTableBatchOpen:2,5,main]java.lang.AssertionError: SSTable first key DecoratedKey(100294972947100949193477090306072672386, 4fcf051ef5067d7f17d9fc35) > last key DecoratedKey(90250429663386465697464050082134975058, 4fce996e3c1eed8c4b17dd66)
{noformat}

bq. intersects is missing the case of that entirely containing this

Oups, you're right. Attaching a v2 of the first patch that fixes that and also turn the last-written-key check into a RuntimeException, since  this is an important check.
","11/Jun/12 19:23;jbellis;v2 LGTM (nit: would like to rename variables to xBounds instead of xRange)

re 0002, does this actually work w/ LCR?  ","12/Jun/12 07:35;awinter;If I use the v2 patch startup stops with the following:
{code}
 INFO [main] 2012-06-12 14:23:33,899 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-LocationInfo@1141455324(41/51 serialized/live bytes, 1 ops)
 INFO [FlushWriter:2] 2012-06-12 14:23:33,903 Memtable.java (line 266) Writing Memtable-LocationInfo@1141455324(41/51 serialized/live bytes, 1 ops)
ERROR [FlushWriter:2] 2012-06-12 14:23:33,953 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[FlushWriter:2,5,main]java.lang.RuntimeException: Last written key null >= current key DecoratedKey(61078635599166706937511052402724559481, 4c) writing into /var/lib/XXXX/cassandra/system/LocationInfo/system-LocationInfo-tmp-hd-65597-Data.db
{code}

Given the above I (probably incorrectly) scrubbed the system keyspace which removed all sstables, leaving only the snapshots eg:

{code}
 WARN [CompactionExecutor:5] 2012-06-12 14:29:41,672 CompactionManager.java (line 651) Row at 100 is unreadable; skipping to next
 WARN [CompactionExecutor:5] 2012-06-12 14:29:41,672 CompactionManager.java (line 602) Non-fatal error reading row (stacktrace follows)
java.lang.RuntimeException: Last written key null >= current key DecoratedKey(135285944860343992175601105924967452217, 63716c) writing into /var/lib/XXXX/data/cassandra/system/Versions/system-Versions-tmp-hd-37-Data.db
{code}
..eventually resulting in
{code}
WARN [CompactionExecutor:5] 2012-06-12 14:29:41,674 CompactionManager.java (line 692) No valid rows found while scrubbing SSTableReader(path='/var/lib/XXXX/data/cassandra/system/Versions/system-Versions-hd-35-Data.db'); it is marked for deletion now. If you want to attempt manual recovery, you can find a copy in the pre-scrub snapshot
{code}

A clean bootstrap also stops with similar errors:
{code}
java.lang.RuntimeException: Last written key null >= current key DecoratedKey(61078635599166706937511052402724559481, 4c) writing into /var/lib/XXXX/data/cassandra/system/LocationInfo/system-LocationInfo-tmp-hd-1-Data.db
{code}
and 
{code}
java.lang.RuntimeException: Last written key null >= current key DecoratedKey(93220794208128599841715671226150005828, 746872696674) writing into /var/lib/XXXX/data/cassandra/system/Versions/system-Versions-tmp-hd-1-Data.db
{code}
","12/Jun/12 08:45;slebresne;My bad, I screwed up the assertion -> RuntimeException transition. Attaching v3 that fixes this and renames variables from xRange to xBounds.

bq. re 0002, does this actually work w/ LCR?

Good question :). I honestly haven't tested it, but it should since LCR resets the underlying SSTableIdentityIterator before each iteration (it shouldn't make any difference that you reset while at the end of the row or at the end of the file).
","12/Jun/12 16:52;omid;Tried the patch but the server still doesn't start. The StackOverFlow that gets thrown causes an already loaded column family to be loaded again:

Load CF1:

{code}
reading saved cache /var/lib/cassandra/abcd/saved_caches/SOMEKSP-CF1-KeyCache
2012-06-12_16:18:04.12387  INFO 16:18:04,123 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF1/SOMEKSP-CF1-hd-2248
...
{code}

Load CF3 which has the corrupted sstables
{code}
2012-06-12_15:31:20.56185  INFO 15:31:20,561 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-7924 (2372910 bytes)
2012-06-12_15:31:20.81897 ERROR 15:31:20,811 Exception in thread Thread[OptionalTasks:1,5,main]
2012-06-12_15:31:20.81901 java.lang.StackOverflowError
2012-06-12_15:31:20.81901 	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:90)
2012-06-12_15:31:20.81906 	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)
2012-06-12_15:31:20.81918 	at java.util.Arrays.mergeSort(Unknown Source)
2012-06-12_15:31:20.81927 	at java.util.Arrays.mergeSort(Unknown Source)
2012-06-12_15:31:20.81934 	at java.util.Arrays.mergeSort(Unknown Source)
2012-06-12_15:31:20.81940 	at java.util.Arrays.mergeSort(Unknown Source)
2012-06-12_15:31:20.81946 	at java.util.Arrays.mergeSort(Unknown Source)
2012-06-12_15:31:20.81954 	at java.util.Arrays.sort(Unknown Source)
2012-06-12_15:31:20.81960 	at java.util.Collections.sort(Unknown Source)
2012-06-12_15:31:20.81980 	at org.apache.cassandra.utils.IntervalTree.IntervalNode.findMinMedianMax(IntervalNode.java:114)
2012-06-12_15:31:20.81981 	at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:49)
2012-06-12_15:31:20.81990 	at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

// stacktrace goes on

2012-06-12_15:31:20.88633 	at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
2012-06-12_15:31:20.88643 	at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
2012-06-12_15:31:20.88654 	at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
2012-06-12_15:31:20.88664 	at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
2012-06-12_15:31:20.88673 	at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
2012-06-12_15:31:20.88683 	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
2012-06-12_15:31:20.88692 	at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
2012-06-12_15:31:20.88702 	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
2012-06-12_15:31:20.88712 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
2012-06-12_15:31:20.88723 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
2012-06-12_15:31:20.88734 	at org.apache.cassandra.db.Table.initCf(Table.java:367)
2012-06-12_15:31:20.88742 	at org.apache.cassandra.db.Table.<init>(Table.java:299)
2012-06-12_15:31:20.88750 	at org.apache.cassandra.db.Table.open(Table.java:114)
2012-06-12_15:31:20.88758 	at org.apache.cassandra.db.Table.open(Table.java:97)
2012-06-12_15:31:20.88766 	at org.apache.cassandra.db.Table$2.apply(Table.java:574)
2012-06-12_15:31:20.88773 	at org.apache.cassandra.db.Table$2.apply(Table.java:571)
2012-06-12_15:31:20.88782 	at com.google.common.collect.Iterators$8.next(Iterators.java:751)
2012-06-12_15:31:20.88790 	at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1625)
2012-06-12_15:31:20.88800 	at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:118)
2012-06-12_15:31:20.88810 	at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:45)
2012-06-12_15:31:20.88818 	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
2012-06-12_15:31:20.88833 	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-06-12_15:31:20.88842 	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(Unknown Source)
2012-06-12_15:31:20.88851 	at java.util.concurrent.FutureTask.runAndReset(Unknown Source)
2012-06-12_15:31:20.88860 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(Unknown Source)
2012-06-12_15:31:20.88870 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(Unknown Source)
2012-06-12_15:31:20.88882 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)
2012-06-12_15:31:20.88892 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-06-12_15:31:20.88901 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-06-12_15:31:20.88910 	at java.lang.Thread.run(Unknown Source)
{code}

Then it tries to load CF1 again:

{code}

2012-06-12_15:33:52.92593  INFO 15:33:52,925 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF1/SOMEKSP-CF1-hd-3578 (2528503 bytes)
2012-06-12_15:33:52.92792  INFO 15:33:52,927 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF1/SOMEKSP-CF1-hd-2613 (3374796 bytes)
{code}

Therefore the server fails with the following exception:

{code}
2012-06-12_15:33:53.17913 ERROR 15:33:53,178 Exception encountered during startup
2012-06-12_15:33:53.17919 java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=SOMEKSP,columnfamily=CF1
2012-06-12_15:33:53.17934 	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:257)
2012-06-12_15:33:53.17940 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
2012-06-12_15:33:53.17948 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
2012-06-12_15:33:53.17956 	at org.apache.cassandra.db.Table.initCf(Table.java:367)
2012-06-12_15:33:53.17962 	at org.apache.cassandra.db.Table.<init>(Table.java:299)
2012-06-12_15:33:53.17967 	at org.apache.cassandra.db.Table.open(Table.java:114)
2012-06-12_15:33:53.17972 	at org.apache.cassandra.db.Table.open(Table.java:97)
2012-06-12_15:33:53.17979 	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
2012-06-12_15:33:53.17987 	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
2012-06-12_15:33:53.17996 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
2012-06-12_15:33:53.18002 Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=SOMEKSP,columnfamily=CF1
2012-06-12_15:33:53.18013 	at com.sun.jmx.mbeanserver.Repository.addMBean(Unknown Source)
2012-06-12_15:33:53.18019 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(Unknown Source)
2012-06-12_15:33:53.18028 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(Unknown Source)
2012-06-12_15:33:53.18038 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(Unknown Source)
2012-06-12_15:33:53.18045 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(Unknown Source)
2012-06-12_15:33:53.18053 	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(Unknown Source)
2012-06-12_15:33:53.18060 	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:253)
2012-06-12_15:33:53.18067 	... 9 more
2012-06-12_15:33:53.18069 java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=SOMEKSP,columnfamily=CF1
2012-06-12_15:33:53.18083 	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:257)
2012-06-12_15:33:53.18092 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
2012-06-12_15:33:53.18100 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
2012-06-12_15:33:53.18109 	at org.apache.cassandra.db.Table.initCf(Table.java:367)
2012-06-12_15:33:53.18114 	at org.apache.cassandra.db.Table.<init>(Table.java:299)
2012-06-12_15:33:53.18119 	at org.apache.cassandra.db.Table.open(Table.java:114)
2012-06-12_15:33:53.18124 	at org.apache.cassandra.db.Table.open(Table.java:97)
2012-06-12_15:33:53.18129 	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
2012-06-12_15:33:53.18139 	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
2012-06-12_15:33:53.18148 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
2012-06-12_15:33:53.18155 Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=SOMEKSP,columnfamily=CF1
2012-06-12_15:33:53.18167 	at com.sun.jmx.mbeanserver.Repository.addMBean(Unknown Source)
2012-06-12_15:33:53.18173 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(Unknown Source)
2012-06-12_15:33:53.18181 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(Unknown Source)
2012-06-12_15:33:53.18191 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(Unknown Source)
2012-06-12_15:33:53.18198 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(Unknown Source)
2012-06-12_15:33:53.18206 	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(Unknown Source)
2012-06-12_15:33:53.18212 	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:253)
2012-06-12_15:33:53.18219 	... 9 more
2012-06-12_15:33:53.18221 Exception encountered during startup: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=SOMEKSP,columnfamily=CF1
{code}

Enabling assertions though causes the corrupted sstables to be ignored:

{code}
2012-06-12_16:25:43.32075  INFO 16:25:43,320 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hc-6965 (2105724 bytes)
2012-06-12_16:25:43.32562 ERROR 16:25:43,325 Exception in thread Thread[SSTableBatchOpen:1,5,main]
2012-06-12_16:25:43.32577 java.lang.AssertionError: SSTable first key DecoratedKey(41255474878128469814942789647212295629, 31303132393937357c3337313730333536) > last key DecoratedKey(41219536226656199861610796307350537953, 31303234323538397c3331383436373338)
2012-06-12_16:25:43.32614   at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:412)
2012-06-12_16:25:43.32626   at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:187)
2012-06-12_16:25:43.32638   at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:225)
2012-06-12_16:25:43.32651   at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-06-12_16:25:43.32662   at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
2012-06-12_16:25:43.32672   at java.util.concurrent.FutureTask.run(Unknown Source)
2012-06-12_16:25:43.32681   at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-06-12_16:25:43.32692   at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-06-12_16:25:43.32703   at java.lang.Thread.run(Unknown Source)
{code}

which leads to cassandra booting up. I wonder if scrub will pick up the ignored sstables.

Shouldn't the assertion above (SSTable first key > last key) turn into an exception and get handled properly?

",14/Jun/12 17:14;omid;Scrubbed the column family on a node which had booted up with assertions `on` and there were still corrupt sstables.,"14/Jun/12 20:20;al@ooyala.com;I think I just hit the same thing. We're using reverse comparator with bytescomparator on the CF's that seem to be having trouble if that's relevant at all.

Cluster is 1.1.1 on Ubuntu 12.04 and only has about 7GB per node at the moment.

Stacktrace attached.","14/Jun/12 20:22;al@ooyala.com;BTW if somebody points me to a build, tag, or commit ID to test, I'll push it out right away. It's only a 3-node cluster and I can easily take a filesystem snapshot before running.","15/Jun/12 14:49;slebresne;bq. Tried the patch but the server still doesn't start.

Right. So the problem is, as you noticed, that there is really no way to start the server and having it load a broken sstable, which means there is no way to run scrub on it. Even without assertions, we rely on interval trees which breaks if the sstable first key is not before the last one.

After having look a bit more closely on that problem, I think the cleaner way to solve this is to provide a way to run scrub ""offline"", which allows to skip the interval trees. So attaching a 3rd patch that provide that. It adds a new binary 'sstablescrub' that takes as argument a keyspace name and column family name and scrub the relevant sstables, and does this without breaking if the sstable have some out of order keys. I kind of think that having an offline scrub is not a bad idea anyway.

With that, you should be able to stop the node, run 'sstablescrub ksname cfname' and then restart the node and you should be good to go.
","16/Jun/12 00:26;al@ooyala.com;What SHA / tag should these patches apply against? I've tried trunk, 1.1.1 and 1.1.0 and can't get a clean apply. I'll try a manual merge tomorrow.",16/Jun/12 00:31;jbellis;cassandra-1.1 branch,"18/Jun/12 21:04;omid;Thanks for the patch. Offline scrub is indeed very useful.

Tried the v3 patches and the scrub didn't complete, possibly because of a different issue, with the following failed assertion:

{code}
Exception in thread ""main"" java.lang.AssertionError: Unexpected empty index file: RandomAccessReader(filePath='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-tmp-hd-33827-Index.db', skipIOCache=true)
        at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:221)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:376)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:203)
        at org.apache.cassandra.io.sstable.SSTableReader.openNoValidation(SSTableReader.java:143)
        at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:79)
{code}

which consequently, encountered corrupt SSTables during start-up:

{code}
2012-06-18_20:36:19.89543  INFO 20:36:19,895 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-24984 (1941993 bytes)
2012-06-18_20:36:19.90217 ERROR 20:36:19,900 Exception in thread Thread[SSTableBatchOpen:9,5,main]
2012-06-18_20:36:19.90222 java.lang.IllegalStateException: SSTable first key DecoratedKey(41255474878128469814942789647212295629, 31303132393937357c3337313730333536) > last key DecoratedKey(41219536226656199861610796307350537953, 31303234323538397c3331383436373338)
2012-06-18_20:36:19.90261 	at org.apache.cassandra.io.sstable.SSTableReader.validate(SSTableReader.java:441)
2012-06-18_20:36:19.90275 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:208)
2012-06-18_20:36:19.90291 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:153)
2012-06-18_20:36:19.90309 	at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:245)
2012-06-18_20:36:19.90324 	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-06-18_20:36:19.90389 	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
2012-06-18_20:36:19.90391 	at java.util.concurrent.FutureTask.run(Unknown Source)
2012-06-18_20:36:19.90391 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-06-18_20:36:19.90392 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-06-18_20:36:19.90392 	at java.lang.Thread.run(Unknown Source)
{code}

although didn't prevent Cassandra from starting up, but compaction failed subsequently:

{code}
2012-06-18_20:51:41.79122 ERROR 20:51:41,790 Exception in thread Thread[CompactionExecutor:81,1,main]
2012-06-18_20:51:41.79131 java.lang.RuntimeException: Last written key DecoratedKey(12341204629749023303706929560940823070, 33363037353338) >= current key DecoratedKey(12167298275958419273792070792442127650, 31363431343537) writing into /var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-tmp-hd-40992-Data.db
2012-06-18_20:51:41.79161 	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
2012-06-18_20:51:41.79169 	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
2012-06-18_20:51:41.79180 	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
2012-06-18_20:51:41.79189 	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
2012-06-18_20:51:41.79199 	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
2012-06-18_20:51:41.79210 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2012-06-18_20:51:41.79218 	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-06-18_20:51:41.79227 	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
2012-06-18_20:51:41.79235 	at java.util.concurrent.FutureTask.run(Unknown Source)
2012-06-18_20:51:41.79242 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-06-18_20:51:41.79250 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-06-18_20:51:41.79259 	at java.lang.Thread.run(Unknown Source)
{code}",18/Jun/12 22:34;al@ooyala.com;Offline scrub ran fine for me.  I downgraded to 1.1.0 and ran a compaction and it looks fine.  (edit) finished offline scrub on both affected nodes and they're back to normal.,"19/Jun/12 01:06;awinter;I can confirm I also experienced the ""Unexpected empty index file"" errors on some of the nodes that I have run sstablescrub on.

Other nodes had this error when running sstablescrub:
{code}
Scrub of SSTableReader(path='/var/lib/XXXX/data/cassandra/KS/CF/KS-CF-hd-259648-Data.db') complete: 1592 rows in new sstable and 0 empty (tombstoned) rows dropped
EOF after 6 bytes out of 8
{code}

Compactions stop with the ""java.lang.RuntimeException: Last written key DecoratedKey"" error on the nodes affected by either of the above 2 errors .

Nodes that seem to have been repaired by the sstablescrub still continue to have ""java.lang.RuntimeException: Last written key DecoratedKey"" errors scattered through the logs but are still compacting.

Is there any further information we can supply to help debug?","19/Jun/12 06:37;slebresne;My bad. Forgot to exclude temporary and compacted files from the scrubbed files.

Attaching a v4 of last patch to fix. Hopefully this should fix the offline scrub.","19/Jun/12 14:28;omid;Tried v4 patch and offline scrub went through completely. Cassandra started without any error but compaction halted again:

{code}
2012-06-19_14:01:03.47432  INFO 14:01:03,474 Compacting [SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-67792-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-65607-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-63279-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-65491-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-68332-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-64720-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-65322-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-66557-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-64504-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-68179-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-65005-Data.db')]
2012-06-19_14:01:08.73528 ERROR 14:01:08,733 Exception in thread Thread[CompactionExecutor:11,1,main]
2012-06-19_14:01:08.73538 java.lang.RuntimeException: Last written key DecoratedKey(42351003983459534782466386414991462257, 313632303432347c3130303632313432) >= current key DecoratedKey(38276735926421753773204634663641518108, 31343638373735327c3439343837333932) writing into /var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-tmp-hd-68399-Data.db
2012-06-19_14:01:08.73572 	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
2012-06-19_14:01:08.73581 	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
2012-06-19_14:01:08.73590 	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
2012-06-19_14:01:08.73600 	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
2012-06-19_14:01:08.73611 	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
2012-06-19_14:01:08.73622 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2012-06-19_14:01:08.73633 	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-06-19_14:01:08.73642 	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
2012-06-19_14:01:08.73650 	at java.util.concurrent.FutureTask.run(Unknown Source)
2012-06-19_14:01:08.73657 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-06-19_14:01:08.73665 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-06-19_14:01:08.73672 	at java.lang.Thread.run(Unknown Source)
{code}

All SSTables that participated in compaction were new ones written by the offline scrub (according their timestamp and also id range.) although the first one didn't exist any more (already promoted before the exception?)

{quote}This is not really a new bug, but I believe that prior to CASSANDRA-4142, *this had less consequences*.{quote}

Sylvain, could you please elaborate on this? I'd like to know how pre-1.1.1 data is affected by the Range-vs-Bounds bug. Only overlapping/duplicate sstables on the same level leading to slower reads caused by unneeded sstable lookups?
","19/Jun/12 15:40;slebresne;Just to make sure: you did apply 0001-Change-Range-Bounds-in-LeveledManifest.overlapping-v3.txt before restarting the server after having run the offline scrub, right?

If so, that would mean we have yet another bug that generates out of order keys during compaction.","19/Jun/12 16:15;omid;Exactly.

- Applied v3
- Ran offline scrub and it failed because of tmp files.
- Started Cassandra and saw failures.
- Applied v4 to cassandra-1.1 branch.
- Ran offline scrub successfully.
- Started Cassandra successfully.
- Compaction failed because of above error.

All done on the same instance.",19/Jun/12 16:25;omid;Let me know if I can provide more data.,"19/Jun/12 16:33;slebresne;Did the new exception happened quickly after having started the node with the scrubbed files? Are you able to reproduce easily (i.e. if you restart the node and compact, do you still get the same error). If you are able to reproduce, would you be at liberty to provide a set of sstables that produce the error during compaction (in private for instance). It would be much more easy to tack that down with an easy way to reproduce.",20/Jun/12 11:44;omid;The exceptions happens not quickly afterwards but after some rounds of compaction on the CF. I had re-bootstrapped so there are tons of ~(1500) pending compaction tasks. If I restart the node and compact the problem happens again and I can reproduce it. I'll send you an email about the data.,"20/Jun/12 15:40;slebresne;Alright, I think the problem may just be that my offline scrub was kind of broken in that it wasn't dealing with the leveled manifest correctly. Attaching a v5 that update the manifest correctly but also check that there is no overlapping sstables in the manifest (and send back sstables to L0 if that happens).

So hopefully running this new version of the offline scrub should fix it (the 2 first patch are untouched, I only rebased them).","20/Jun/12 17:05;omid;Will try it again. LeveledCompactionStrategyTest:testValidationMultipleSSTablePerLevel fails because of junit timeout when I run it together with all other suits, but passes when I only run LeveledCompactionStrategyTest suit. Is it related?
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest:testValidationMultipleSSTablePerLevel:	Caused an ERROR
    [junit] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest FAILED (timeout)
{code}","20/Jun/12 19:37;omid;Got ""java.lang.OutOfMemoryError: Java heap space"" with -Xmx256M.

Tried with -Xmx512M and the scrub failed with:

{code}
Checking leveled manifest
d != org.apache.cassandra.io.sstable.SSTableReader
java.util.IllegalFormatConversionException: d != org.apache.cassandra.io.sstable.SSTableReader
        at java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:3999)
        at java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2709)
        at java.util.Formatter$FormatSpecifier.print(Formatter.java:2661)
        at java.util.Formatter.format(Formatter.java:2433)
        at java.util.Formatter.format(Formatter.java:2367)
        at java.lang.String.format(String.java:2769)
        at org.apache.cassandra.tools.StandaloneScrubber.checkManifest(StandaloneScrubber.java:179)
        at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:148)
{code}",20/Jun/12 19:43;jbellis;Are you using Java6 or Java7?,"20/Jun/12 19:51;omid;The above error is due to the %d in StandaloneScrubber.java:179's interpolation. Will fix and try again.

Jonathan: Sun Java 6, 1.6.0_26","21/Jun/12 05:34;awinter;After working around the issue with the 0003 v5 patch that Omid refers I've had an sstablescrub complete on one of my servers.  sstablescrub did detected several overlapping sstables, resetting them to L0, but no out of order keys.

The Last written key DecoratedKey >= current key exception however resurfaces again after the first set of compactions, 5 minutes after startup, in the exact same manner as before.  The same exception occurs for various CF's until compactions stop completely.  compactionstats still shows a large number of pending compaction tasks after this event.","21/Jun/12 10:19;slebresne;bq. The above error is due to the %d in StandaloneScrubber.java:179's interpolation.

Yes, sorry for that typo. I've updated the v5 patch to fix it.

bq. Got ""java.lang.OutOfMemoryError: Java heap space"" with -Xmx256M

The last version of the offline scrub ""loads"" all sstable readers, which means in particular that it loads the summary of the key index and the sstable bloom filter. In other words, it does use a bit more memory, so it's not necessarily surprising that -Xmx256M is not enough.

bq. LeveledCompactionStrategyTest:testValidationMultipleSSTablePerLevel fails because of junit timeout when I run it together with all other suits, but passes when I only run LeveledCompactionStrategyTest suit. Is it related?

I doubt it. I've already seen test timeout when run with the full suit but not alone. I wouldn't worry too much about that. At least that test is working fine on my machine.","21/Jun/12 16:40;omid;I experienced the same as Anton's. One observation is that the out-of-order key being wrongly iterated by CompactionIterable's MergeIterator which causes the exception, happen to be the start of an interval:

DEBUG 18:10:41,693 Creating IntervalNode from [... Interval(DecoratedKey(33736808147257072541807562080745136438, ... ), 

which leads me to suspect if it's due to the ""Range"" (vs. Bounds) used in LeveledCompactionStrategy::getScanners. Any ideas?","22/Jun/12 11:38;slebresne;bq. which leads me to suspect if it's due to the ""Range"" (vs. Bounds) used in LeveledCompactionStrategy::getScanners. Any ideas?

No, that Range is correct because this correspond to repair ranges and is correctly interpreted. And in fact, when doing compaction that range is actually null so that cannot be the problem.


So Anton sent me some sstables that triggered an out-of-order exception when compacting them. What I did with that is:
1) apply the last version of the v5 patch on this issue on top of the current git cassandra-1.1 branch (using the release of CASSANDRA 1.1.1 would probably work too because I don't think there is any other related fix since 1.1.1 that went into the git branch but anyway, that's what I used).
2) I ran the offline scrub *while the node was stopped*. I insist on that last part because that having the node run during the offline scrub would mess things up. I'd actually like to make the offline scrub check if the node is running and refuse to work in that case but I'm not sure of what is the best way to do that.
3) I restarted the node once the scrub was done

I was then able to compact the node fully (i.e, I ran compaction until there was nothing more to compact) and this without hitting any more error.

Was I lucky? Are you guys able to reproduce those steps and still get more errors?","22/Jun/12 13:03;omid;So I had offline-scrubbed the live Cassandra node and I had copied the sstables that participated in one of the failed compaction. Assuming the sstables had been offline-scrubbed, I had skipped step 2 above locally, so unfortunately I can't yet reproduce it locally with a limited set of data.","23/Jun/12 00:44;awinter;bq. Was I lucky? Are you guys able to reproduce those steps and still get more errors?

As discussed, but repeated here just for the ticket's reference; I was patching and scrubbing in the same way as described above.  Once the scrubbed nodes were restarted in the cluster they were then under normal read/write load and experienced the exceptions again.  Given that the sstablescrub and subsequent compactions run fine in Sylvain's test, using my out of order sstables, means that the sstablescrub command appears to do its job fine.  The root cause, originally expected to be resolved with the 0001 patch, still appears to be occurring so Sylvain was going to investigate the code further.","26/Jun/12 16:09;slebresne;So, still not sure what would cause out of order keys outside of the bug fixed by the first patch on this issue. But I've rebased the patch as v6 and added a small last patch to check for overlapping sstables in levels each time we modify the manifest.

Could you try doing an offline scrub (while the node is shutdown) and then restart the node with all those patches. If the problem comes back, hopefully the last patch should give us a bit more info. So if the error reproduce and as soon as it does, it would be useful to get the error log as well as the manifest files (the 2 json files along the sstables).","26/Jun/12 16:28;jbellis;Could the CASSANDRA-4341 regression have caused what Anton saw, if he was running from the 1.1 branch?","26/Jun/12 16:31;slebresne;Hum, yes that's possible. I figured this wouldn't be the problem since they were using 1.1.1 but you are right that if they tried against the 1.1 branch that could have been it. Worth checking on current 1.1 branch I guess.","27/Jun/12 20:02;jbellis;CompactionsTest.testBlacklistingWithLeveledCompactionStrategy is currently failing in trunk because of a similar integrity check that I added to promote() for CASSANDRA-1991:

{code}
.       DecoratedKey last = null;
        Collections.sort(generations[newLevel], SSTable.sstableComparator);
        for (SSTableReader sstable : generations[newLevel])
        {
            assert last == null || sstable.first.compareTo(last) > 0;
            last = sstable.last;
        }
{code}

Patch 0001 does not fix that test failure.","28/Jun/12 10:39;slebresne;bq. CompactionsTest.testBlacklistingWithLeveledCompactionStrategy is currently failing in trunk because of a similar integrity check that I added to promote()

That's a bug in the integrity check in fact, that should skip the check if newLevel=0 (since we can have newLevel=0 following CASSANDRA-4341). With that fixed there is no more failure of that unit test (I've committed the fix as 4725bf71e18550ac60f9).","28/Jun/12 17:54;slebresne;Alright, so I'm pretty sure I've found the root cause of all this. On top of the Range->Bound problem we were not correctly computing the set of overlapping sstable in L1 when compacting multiple L0 files. Citing the comment of the attached fix (where sstables = 'sstables in L0 to compact' and candidates = 'sstable in L1'):
{noformat}
/*
 * Note that picking each sstable from candidates that overlap one of the sstable of sstables is not enough
 * because you could have the following situation:
 *   sstables = [ s1(a, c), s2(m, z) ]
 *   candidates = [ s3(e, g) ]
 * In that case, s2 overlaps none of s1 or s2, but if we compact s1 with s2, the resulting sstable will be
 * overlapping s3, so we must return s3.
 */
{noformat}

So I'm attaching a v7 of the patches with the fix for that in the first patch. The last patch (the integrity tests) was also breaking the offline scrub so that's fix in v7 too. Though I note that the integrity tests of the last patch is probably overkill and I didn't really intended to commit it (i.e. the integrity check that is in trunk is probably enough).","28/Jun/12 18:44;jbellis;Nice work, I think you nailed it!

+1 from me, and agreed that I'd rather backport the limited sanity check from trunk than use 0004 here.

Also attached a cleanup patch (applies after 0001) to add javadoc, improve parameter names, and simplify ""overlapping"" by making union explicit when desired.",29/Jun/12 07:59;awinter;I've applied the v7 patches and have successfully offline scrubbed & reinserted a number of nodes in my ring without further occurrence of the previous issues.  Thanks :),29/Jun/12 09:20;slebresne;Great. So I committed the first 3 patches along with the cleanup patch and I backported the simpler integrity check from trunk.,"30/Jun/12 00:09;awinter;Maybe I spoke too soon.  Overnight I've seen the exceptions happen again on nodes that were v7 patched & scrubbed.  

{code}
ERROR [CompactionExecutor:1301] 2012-06-29 21:54:12,078 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:1301,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(116816802911061669023614481109871014436, 4faa631ca88ef85b8e26ddeb) >= current key DecoratedKey(115179899219377463875853982254751557438, 4fa892bf42d3f24479f627b6) writing into /var/lib/XXXX/data/cassandra/KS/CF/KS-CF-tmp-hd-837655-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}","30/Jun/12 00:19;jbellis;To clarify, is this 1.1.1 release + patches, or 1.1 dev branch + patches?",30/Jun/12 00:20;awinter;1.1 dev branch + patches,"04/Jul/12 03:28;awinter;I have repeatedly run sstablescrub across all my nodes and the exceptions do not occur as frequently now, however, the integrity check still throws exceptions and compactionstats shows a large number of pending tasks but no progression afterwards.

Should this ticket be reopened or a new one raised?

{code}
ERROR [CompactionExecutor:912] 2012-07-04 01:07:16,470 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:912,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}
","04/Jul/12 09:37;slebresne;Damn. Ok, since this has been released with 1.1.2 already, would you mind opening a new one?",05/Jul/12 04:53;awinter;New issue raised as requested: CASSANDRA-4411
overlapping sstables in leveled compaction strategy,CASSANDRA-4233,12554549,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,09/May/12 19:53,12/Mar/19 14:12,13/Mar/19 22:27,18/May/12 16:35,1.1.1,,,,,0,lcs,,,,,,"CASSANDRA-4142 introduces test failures, that are caused by overlapping tables within a level, which Shouldn't Happen.",,,,,,,,,,,,,,,,,,,17/May/12 16:01;slebresne;0001-Make-sure-Leveled-compaction-always-use-the-right-min-.txt;https://issues.apache.org/jira/secure/attachment/12527850/0001-Make-sure-Leveled-compaction-always-use-the-right-min-.txt,09/May/12 20:00;jbellis;4233-assert.txt;https://issues.apache.org/jira/secure/attachment/12526201/4233-assert.txt,17/May/12 16:38;jbellis;4233-v2.txt;https://issues.apache.org/jira/secure/attachment/12527852/4233-v2.txt,17/May/12 16:48;jbellis;4233-v3.txt;https://issues.apache.org/jira/secure/attachment/12527855/4233-v3.txt,17/May/12 20:11;jbellis;4233-v4.txt;https://issues.apache.org/jira/secure/attachment/12527904/4233-v4.txt,11/May/12 15:06;brandon.williams;system.log.bz2;https://issues.apache.org/jira/secure/attachment/12526534/system.log.bz2,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2012-05-09 20:02:02.492,,,no_permission,,,,,,,,,,,,238799,,,Tue Jun 12 09:48:55 UTC 2012,,,,,,0|i0gtjj:,96216,jbellis,jbellis,,,,,,,,,,"09/May/12 20:00;jbellis;I see CompactionsTest.testStandardColumnCompactions fail 100% of the time -- but only when run as part of the entire CompactionsTest suite; testStandardColumnCompactions run alone passes.

About 80% of the time the assertion in LCS fails, but sometimes the one in LM fails.  (How the former can fail, without the latter, is a mystery to me.)

Is there an off-by-one bug in IntervalTree?",09/May/12 20:00;jbellis;(Assertions in question are attached.),"09/May/12 20:02;brandon.williams;An easy way to recreate this in a live situation that I just accidentally discovered is to set memtable_total_space_in_mb to something small like 3, and then run stress with LCS.",09/May/12 21:26;jbellis;Reproduced assertion errors applying patch (just the LM part) to version 1686a36 (the version before CASSANDRA-4142 was committed).,11/May/12 15:06;brandon.williams;Here's a log with compaction at debug illustrating the repro with small memtable space.,"11/May/12 15:31;slebresne;So it's not (only) a problem of overlapping sstables in a given level, since extracted from the log above we get:
{noformat}
DEBUG [CompactionExecutor:4] 2012-05-11 15:04:39,125 LeveledManifest.java (line 273) Compaction candidates for L0 are Standard1-4(L0), Standard1-3(L0), Standard1-2(L1), 
 INFO [CompactionExecutor:4] 2012-05-11 15:04:39,126 CompactionTask.java (line 109) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-4-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-2-Data.db')]
ERROR [CompactionExecutor:4] 2012-05-11 15:04:41,829 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:4,1,main]
java.lang.AssertionError: Last written key DecoratedKey(170030165514395172434635544532010976042, 30303033313634) >= current key DecoratedKey(983360854569225448206418564333137310, 30303036353031) writing into /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-tmp-hd-6-Data.db
{noformat}
That is, we get an error even though we compact 2 L0 with 1 L1.",11/May/12 15:49;jbellis;Brandon's error was from a node that didn't have 67ed39fa9bf71be4cfc13fccbdd7b76dcb46c062 applied.  So I guess we're dealing with a potential buggy test.,"11/May/12 15:49;brandon.williams;My mistake, that log is pre-67ed39fa9b which fixes it, so I think we just have a bad test now.","17/May/12 16:01;slebresne;This was kind of a test problem, though this is kind of a bug.

The main problem is that LeveledCompaction should never get a maxCompactionThreshold < Integer.MAX_VALUE, otherwise it ends up not compaction what it should and this screw up the levels. However, the way we try to ensure that is by overriding the maxCompactionThreshold in LeveledCompactionStrategy constructor. This is however dangerous because it means that if anyone change the threshold afterward, it will break. And sure enough, that is what happens with CompactionsTest.

So attaching a patch that change the code to ensure this doesn't happen. To a large extend this patch is a hack and in the long run, we should refactor all this to move the min/max compaction threshold inside SizeTieredCompaction, where they belong. This is however a bigger refactor than I want to do on the 1.1 branch as currently the thresholds are used to deactivate automatic compaction and whatnot.","17/May/12 16:38;jbellis;simpler v2 attached, to just ignore CFS min/max in the actual LeveledCompactionTask.

also respects isCompactionDisabled in LCS.","17/May/12 16:48;jbellis;tweaked v3 to not mess w/ min/max settings at all (which would result in re-activating compaction if it's ""disabled"")","17/May/12 16:56;slebresne;I'd submit that my version was giving a slightly better feedback through JMX but I guess that's not a big deal.

But other than that, the definition of isCompactionDisabled so far has been to disallow automatic compaction, but forced compaction (""maximal"" ones) was still allowed. v2 change that for LCS. I'll note that disabling compaction was respected for background tasks by LCS by the fact that DataTracker.maxCompacting was skipping compaction when maxCompactionThreshold is 0, but allowed for maximal tasks since the maxCompactionThreshold is overriden in compactionManager in that case. I don't know if it's a big deal though (I suspect some tests would fail if run with LCS because they rely on that) but wanted to mention it.","17/May/12 20:11;jbellis;I think the status quo (and v1, which perpetuates it) are too fragile...  Exhibit A being this bug, and exhibit B being my reading of the code leading me to believe that there was no way to disable LCS. Rather than try to prevent setting max inappropriately, much more clear to have LCS ignore min/max entirely, except via isCompactionDisabled.  This is closer to how it ""should"" be w/ min + max being STCS-specific options.

v4 rearranges background/maximal methods to continue allowing maximal to kick off a compaction when autocompaction is disabled.",18/May/12 14:08;slebresne;+1,18/May/12 16:35;jbellis;committed,"07/Jun/12 09:02;iscariot;I already have this bug in 1.1.1 version

java.lang.AssertionError: Last written key DecoratedKey(144093708553026671072703572185472027831, 00000000003eef36) >= current key DecoratedKey(50602742485951681279829043048319220646, 00000000004506bf) writing into /cassandra/data/XXXXXX/YYYYYY/AAAAAAA-BBBBBBB-tmp-hd-5217-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
...

any help?","07/Jun/12 09:09;slebresne;I may or may not be related to this issue. It would help to get the full stack trace and a bit more information on when that happened, if that was a fresh 1.1.1 cluster or an upgraded one from 1.1.0 and the compaction strategy used. And it's probably worth creating a new ticket.","07/Jun/12 09:36;iscariot;Stack trace of this error

ERROR 04:09:21,216 Exception in thread Thread[CompactionExecutor:410,1,main]
java.lang.AssertionError: Last written key DecoratedKey(144093708553026671072703572185472027831, 00000000003eef36) >= current key DecoratedKey(50602742485951681279829043048319220646, 00000000004506bf) writing into /cassandra/data/Server/Messages/Server-Messages-tmp-hd-5380-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

Linux version 2.6.32-5-amd64 (Debian 2.6.32-45)
32GB ram, 24 threads (12 cores)

We are upgraded Cassandra server from repository(config file was overwritten, but we change it later). 
And we REcreated keyspace with all CF (we in not a production stage yet).

all us CFs
UPDATE COLUMN FAMILY Messages WITH compression_options={sstable_compression:SnappyCompressor, chunk_length_kb:1};
UPDATE COLUMN FAMILY Messages WITH compaction_strategy=LeveledCompactionStrategy AND compaction_strategy_options={sstable_size_in_mb: 10};
UPDATE COLUMN FAMILY Messages WITH gc_grace = 0;

This CF has many inserts in one time. 
Over 1000-3000 keys with 9 columns each in one sec.
And this CF has 4 secondary indexes (numbers ~ from 1 to 20)

More info about Cassandra server
partitioner: org.apache.cassandra.dht.RandomPartitioner
key_cache_size_in_mb - default
row_cache_size_in_mb - default
row_cache_provider: SerializingCacheProvider

MAX_HEAP_SIZE=""8G""
HEAP_NEWSIZE=""2G""


Thanks for any help.
",12/Jun/12 09:48;slebresne;I believe this is the same than CASSANDRA-4321 and being tracked there.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IncomingTcpConnection can not be closed when the peer is brutaly terminated or switch is failed,CASSANDRA-4053,12546556,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,hanzhu,hanzhu,15/Mar/12 10:09,12/Mar/19 14:12,13/Mar/19 22:27,24/Sep/13 07:59,2.0.2,,,,,0,,,,,,,"IncomingTcpConnection has no way to detect the peer is down when the peer meets power loss or the network infrastructure is failed, and the thread is leaked...

For safety, as least SO_KEEPALIVE should be set on those IncomingTcpConnections. The better way is to close the incoming connections when failure detector notifies the peer failure, but it requires some extra bookmarking.

Besides it, it would be better if IncomingTcpConnection and OutgoingTcpConnection is marked as daemon thread...

",,,,,,,,,,,,,,,,,,,23/Sep/13 07:32;krummas;0001-enable-keepalive-on-incoming-connections.patch;https://issues.apache.org/jira/secure/attachment/12604533/0001-enable-keepalive-on-incoming-connections.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-09-20 21:38:09.406,,,no_permission,,,,,,,,,,,,231714,,,Tue Sep 24 07:59:01 UTC 2013,,,,,,0|i0gren:,95870,,,,,,,,,,,,20/Sep/13 21:38;jbellis;Is this still relevant [~krummas]?,"23/Sep/13 07:32;krummas;hmm it might actually be, attaching patch to enable tcp keepalive on incoming connections",23/Sep/13 13:21;jbellis;+1,24/Sep/13 07:59;krummas;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cleanup optimization can delete data but not corresponding index entries,CASSANDRA-4379,12595941,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,26/Jun/12 17:18,12/Mar/19 14:12,13/Mar/19 22:27,27/Jun/12 16:40,1.1.2,,,,,0,compaction,,,,,,introduced by CASSANDRA-4079,,,,,,,,,,,,,,,,,,,26/Jun/12 17:21;jbellis;4379.txt;https://issues.apache.org/jira/secure/attachment/12533503/4379.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-27 08:25:18.696,,,no_permission,,,,,,,,,,,,256109,,,Wed Jun 27 16:40:34 UTC 2012,,,,,,0|i0gv7z:,96488,slebresne,slebresne,,,,,,,,,,26/Jun/12 17:21;jbellis;fix attached,27/Jun/12 08:25;slebresne;+1,27/Jun/12 16:40;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgradesstables strips active data from sstables,CASSANDRA-4462,12600069,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,mheffner,mheffner,24/Jul/12 21:23,12/Mar/19 14:12,13/Mar/19 22:27,27/Jul/12 08:46,1.0.11,1.1.3,,,,0,,,,,,,"From the discussion here: http://mail-archives.apache.org/mod_mbox/cassandra-user/201207.mbox/%3CCAOac0GCtyDqS6ocuHOuQqre4re5wKj3o-ZpUZGkGsjCHzDVbTA%40mail.gmail.com%3E

We are trying to migrate a 0.8.8 cluster to 1.1.2 by migrating the sstables from the 0.8.8 ring to a parallel 1.1.2 ring. However, every time we run the `nodetool upgradesstables` step we find it removes active data from our CFs -- leading to lost data in our application.

The steps we took were:


1. Bring up a 1.1.2 ring in the same AZ/data center configuration with
tokens matching the corresponding nodes in the 0.8.8 ring.
2. Create the same keyspace on 1.1.2.
3. Create each CF in the keyspace on 1.1.2.
4. Flush each node of the 0.8.8 ring.
5. Rsync each non-compacted sstable from 0.8.8 to the corresponding node in
1.1.2.
6. Move each 0.8.8 sstable into the 1.1.2 directory structure by renaming the file to the  /cassandra/data/<keyspace>/<cf>/<keyspace>-<cf>... format. For example, for the keyspace ""Metrics"" and CF ""epochs_60"" we get:
""cassandra/data/Metrics/epochs_60/Metrics-epochs_60-g-941-Data.db"".
7. On each 1.1.2 node run `nodetool -h localhost refresh Metrics <CF>` for each CF in the keyspace. We notice that storage load jumps accordingly.
8. On each 1.1.2 node run `nodetool -h localhost upgradesstables`.

Afterwards we would test the validity of the data by comparing it with data from the original 0.8.8 ring. After an upgradesstables command the data was always incorrect.

With further testing we found that we could successfully use scrub to convert our sstables without data loss. However, any invocation of upgradesstables causes active data to be culled from the sstables:

 INFO [CompactionExecutor:4] 2012-07-24 04:27:36,837 CompactionTask.java (line 109) Compacting [SSTableReader(path='/raid0/cassandra/data/Metrics/metrics_900/Metrics-metrics_900-hd-51-Data.db')]
 INFO [CompactionExecutor:4] 2012-07-24 04:27:51,090 CompactionTask.java (line 221) Compacted to [/raid0/cassandra/data/Metrics/metrics_900/Metrics-metrics_900-hd-58-Data.db,].  60,449,155 to 2,578,102 (~4% of original) bytes for 4,002 keys at 0.172562MB/s.  Time: 14,248ms.

These are the steps we've tried:

WORKS		refresh -> scrub
WORKS		refresh -> scrub -> major compaction
WORKS		refresh -> scrub -> cleanup
WORKS		refresh -> scrub -> repair

FAILS		refresh -> upgradesstables
FAILS		refresh -> scrub -> upgradesstables
FAILS		refresh -> scrub -> repair -> upgradesstables
FAILS		refresh -> scrub -> major compaction -> upgradesstables

We have fewer than 143 million row keys in the CFs we're testing and none
of the *-Filter.db files are > 10MB, so I don't believe this is our
problem: https://issues.apache.org/jira/browse/CASSANDRA-3820

The keyspace is defined as:

Keyspace: Metrics:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [us-east:3]

And the column family that we tested with is defined as:

    ColumnFamily: metrics_900
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.1
      DC Local Read repair chance: 0.0
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
      Compression Options:
        sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor

All rows have a TTL of 30 days and a gc_grace=0 so it's possible that a small number of older columns would be removed during a compaction/scrub/upgradesstables step. However, the majority should still be kept as their TTL's have not expired yet.",Ubuntu 11.04 64-bit,,,,,,,,,,,,,,,,,,25/Jul/12 07:49;slebresne;4462.txt;https://issues.apache.org/jira/secure/attachment/12537812/4462.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-25 07:49:06.488,,,no_permission,,,,,,,,,,,,256178,,,Fri Jul 27 08:46:01 UTC 2012,,,,,,0|i0gw5b:,96638,jbellis,jbellis,,,,,,,,,,"25/Jul/12 07:49;slebresne;There is indeed an unfortunate typo in the code of upgradesstables that makes it purge every tombstone instead of purging none. Since we upgrade one sstable at a time, purging tombstone is a bug that will resurrect data.

Attached patch to fix (which also fix a 2nd occurrence of the same problem but that 2nd one was introduce by CASSANDRA-4456 so wasn't released yet).",25/Jul/12 12:11;mheffner;Would that typo lead to the behavior we saw where non-tombstoned data would be removed from the sstable during an upgradesstables run?,"26/Jul/12 16:59;slebresne;Best way to make sure would be to test with the patch :)

But since you have ttl, I would say that yes, there is a good chance that's related.","26/Jul/12 17:18;mheffner;Fair enough, though we are not quite equipped to test patch sets yet so it might take awhile.","26/Jul/12 17:19;jbellis;+1 on the basic fix, but I'm not really convinced that GC_ALL and NO_GC are improvements since we don't really get any extra typesafety from it.  (Maybe switching to an enum for ALL, NONE, and CURRENT_TIME enum would be okay though?  But that's out of scope here.)","27/Jul/12 08:46;slebresne;bq. I'm not really convinced that GC_ALL and NO_GC are improvements since we don't really get any extra typesafety from it

The goal is not really to gain type safety, but to avoid having to take the few seconds to think about which of MIN_VALUE or MAX_VALUE means to not GC anything or conversely to GC everything. At least I have a tendency of screwing that up as showed by this issue, and I think having more explicit and readable constant names might avoid that kind of mistake in the future (again, at least for me). Anyway, I've committed as is, but if you really don't like, feel free to ninja edit it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes with Ec2Snitch and Ec2MultiRegionSnitch cannot start due to cassandra-rackdc.properties not getting installed by packages!,CASSANDRA-5281,12633735,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,arya,arya,23/Feb/13 05:44,12/Mar/19 14:12,13/Mar/19 22:27,23/Feb/13 06:05,,,,,,0,,,,,,,The patch in CASSANDRA-5155 breaks the node startup if cassandra-rackdc.properties is not present for nodes using Ec2 and Ec2MultiRegion snitches as a RuntimeException is thrown in SnitchProperties.java. I think we should include cassandra-rackdc.properties in the Debian package or have the default empty property being returned on FileNotFoundException. What do you think?,"Cassandra 1.2.1
Ubuntu 10.04",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,314230,,,Sat Feb 23 06:05:24 UTC 2013,,,,,,0|i1i84f:,314575,,,,,,,,,,,,23/Feb/13 05:47;arya;Also it would be nice to include a note in change log or upgrade notes for this.,"23/Feb/13 05:50;arya;I noticed that the rackdc properties file is in the source, but it doesn't get built into the debian package. Not sure about the CentOS package.",23/Feb/13 06:05;arya;This appears to be fixed in trunk. Thanks Vijay.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error while deleting a columnfamily that is being compacted.,CASSANDRA-4221,12553932,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,04/May/12 13:23,12/Mar/19 14:12,13/Mar/19 22:27,28/May/12 18:26,1.1.1,,,,,0,,,,,,,"The following dtest command produces an error:
{code}export CASSANDRA_VERSION=git:cassandra-1.1; nosetests --nocapture --nologcapture concurrent_schema_changes_test.py:TestConcurrentSchemaChanges.load_test{code}

Here is the error:
{code}
Error occured during compaction
java.util.concurrent.ExecutionException: java.io.IOError: java.io.FileNotFoundException: /tmp/dtest-6ECMgy/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-47-Data.db (No such file or directory)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:239)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1580)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1770)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.IOError: java.io.FileNotFoundException: /tmp/dtest-6ECMgy/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-47-Data.db (No such file or directory)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:61)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:839)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:851)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:142)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:148)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:121)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:264)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	... 3 more
Caused by: java.io.FileNotFoundException: /tmp/dtest-6ECMgy/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-47-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:102)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:985)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
	... 13 more
{code}

For reference, here is the dtest function that causes the failure. The error happens on the line near the bottom that drops the columnfamily:
{code}
    def load_test(self):                                                        
        """"""                                                                     
        apply schema changes while the cluster is under load.                   
        """"""                                                                     
        debug(""load_test()"")                                                    
                                                                                
        cluster = self.cluster                                                  
        cluster.populate(1).start()                                             
        node1 = cluster.nodelist()[0]                                           
        wait(2)                                                                 
        cursor = self.cql_connection(node1).cursor()                            
                                                                                
        def stress(args=[]):                                                    
            debug(""Stressing"")                                                  
            node1.stress(args)                                                  
            debug(""Done Stressing"")                                             
                                                                                
        def compact():                                                          
            debug(""Compacting..."")                                              
            node1.nodetool('compact')                                           
            debug(""Done Compacting."")                                           
                                                                                
        # put some data into the cluster                                        
        stress(['--num-keys=1000000'])                                          
                                                                                
        # now start compacting...                   
        tcompact = Thread(target=compact)                                       
        tcompact.start()                                                        
        wait(1)                                                                 
                                                                                
        # now the cluster is under a lot of load. Make some schema changes.     
        cursor.execute(""USE Keyspace1"")                                         
        wait(1)                                                                 
        cursor.execute(""DROP COLUMNFAMILY Standard1"")                           
                                                                                
        wait(3)                                                                 
                                                                                
        cursor.execute(""CREATE COLUMNFAMILY Standard1 (KEY text PRIMARY KEY)"")  
                                                                                
        tcompact.join()                                                         
 
{code}
Again, the error happens on cassandra-1.1, but not on cassandra-1.0.","ccm, dtest, cassandra-1.1. The error does not happen in cassandra-1.0.",,,,,,,,,,,,,,,,,,28/May/12 15:33;jbellis;4221-v3.txt;https://issues.apache.org/jira/secure/attachment/12529964/4221-v3.txt,17/May/12 10:27;xedin;CASSANDRA-4221-logging.patch;https://issues.apache.org/jira/secure/attachment/12527820/CASSANDRA-4221-logging.patch,25/May/12 15:02;xedin;CASSANDRA-4221-v2.patch;https://issues.apache.org/jira/secure/attachment/12529726/CASSANDRA-4221-v2.patch,09/May/12 19:35;xedin;CASSANDRA-4221.patch;https://issues.apache.org/jira/secure/attachment/12526198/CASSANDRA-4221.patch,18/May/12 16:49;tpatterson;system.log;https://issues.apache.org/jira/secure/attachment/12528099/system.log,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-05-09 18:13:35.394,,,no_permission,,,,,,,,,,,,238149,,,Mon May 28 18:26:32 UTC 2012,,,,,,0|i0gtef:,96193,jbellis,jbellis,,,,,,,,,,09/May/12 18:13;xedin;This one seems to be caused by the same problem as CASSANDRA-4230.,"09/May/12 18:16;jbellis;Maybe, but I'm skeptical -- 4230 is complaining about a file existing when it shouldn't, while this one says a file doesn't exist that should :)",09/May/12 19:00;xedin;Patch adds a try to stop all running compactions on given Keyspace or ColumnFamily before running a drop command. I have tried the test you have in the description and it ran without failures.,"09/May/12 19:12;jbellis;That takes us back to the Bad Old Days pre-CASSANDRA-3116, though.  We should be able to fix w/o resorting to A Big Lock.","09/May/12 19:34;xedin;For the KS or CF drop this seems necessary to try to wait until all running compactions finish otherwise it would end up in errors like one in the description, also other operations - create, update - are not affected by this.","09/May/12 22:54;jbellis;The idea from 3116 was:

- Drop will only delete sstables not actively being compacted
- post-compaction, we check if the CF was dropped, and if so we delete the sstables then","09/May/12 23:06;xedin;I don't know which one is better tho because if compaction fails for some reason which that scenario, wouldn't that mean that all SSTables that were left behind are staying until somebody manually deletes them (or restart would drop them)? We would have to add complexity to the schema merge just to handle that case as well as on the local side...","09/May/12 23:10;xedin;The other way would be to 'mark a CF for delete' and return to the user right way (making CF invisible to users), sending the drop request to the others where they would apply the same thing (try to stop all compactions running, wait until they are done) and drop.","10/May/12 02:32;jbellis;bq. wouldn't that mean that all SSTables that were left behind are staying until somebody manually deletes them (or restart would drop them)? 

We already clean up partially-written sstables after compaction failure, I don't see why we couldn't use similar logic here.",10/May/12 16:14;xedin;The problem I see that that is we need to do a snapshot before start dropping or deleting any CF files so it's probably better to make that drop option 'deferred' until running compactions are stopped so we have a persistent view of the files we would have to operate upon.,"10/May/12 16:19;jbellis;DataTracker already makes sstable changes atomic, though.  At any time you can snapshot with that and get a consistent view.","16/May/12 16:40;jbellis;Tyler, can you still reproduce after the recent schema fixes on the 1.1 branch?","17/May/12 04:15;tpatterson;Yes, the error just happened again for me. I did a fresh pull on branch branch cassandra-1.1.","17/May/12 10:27;xedin;Interesting, I can't reproduce it myself. Can you please run it with logging patch attached (and enabled DEBUG logging) and attach debug log from your C* node to this task, so I can check that is happening inside of DataTracker in your case?... ","17/May/12 17:59;tpatterson;This was after applying both patches to the cassandra-1.1 branch, and setting logging to DEBUG.",17/May/12 18:13;tpatterson;Somehow that server.log did not have the debug info. Looking into it now.,17/May/12 20:12;tpatterson;Debug is enabled now; It looks like CCM overwrites the log level. Only the logging patch was applied in this run. ,"17/May/12 20:23;xedin;I see debug information I added right now, but there is no IOError in that log described in this task...","18/May/12 16:49;tpatterson;So after some experimentation, the problem is only happening when the log level is set to INFO, but it doesn't happen at DEBUG. Gotta love these ones! I modified the logging patch to do logging.info() rather then logging.debug(), and the problem still happens, so at least you can see those debug messages. I hope this is enough to go on.","18/May/12 19:58;xedin;Hah, now I know what is causing it - it's not a drop problem, the situation is triggered when you re-create ColumnFamily right after drop +(before all SSTables were actually deleted by background task)+ so it reads up all SSTables in the directory back to system and tries to compact them simultaneously with them being deleted in the background. That is why we warn people to *avoid* making any modifications to the active CFs otherwise it could lead to the strange situations like this one.","19/May/12 16:31;jbellis;Would this be fixed by CASSANDRA-3794 then, since old and new CF will have different IDs?",19/May/12 21:45;xedin;Not really because it generates UUID from ksName + cfName to be able make it the same across all machines independent of their state.,"19/May/12 22:19;jbellis;Should we just add a call to abort in-progress compactions at drop time (which will help cleanup happen faster) and call that ""as close as we're going to get?""",19/May/12 22:28;xedin;This is what I did in my patch :),"23/May/12 21:53;jbellis;- stopCompactionFor should take CFS parameters instead of String
- I don't see any reason to not wait indefinitely here; in fact, if we make sure to wait until compaction finishes, the odds are much better that when we tell the client ""all done"" he won't be able to send a ""create"" quickly enough to hit the bug
- Need to call stopCompactionFor on every replica, not just CompactionServer -- move this to DefsTable.dropColumnFamily?
","23/May/12 22:21;xedin;bq. stopCompactionFor should take CFS parameters instead of String

I don't really follow here, if you want it to have list of CFMetaData instead of String? String is better suited because CompactionInfo.getColumnFamily() returns a String (CF name).

bq. I don't see any reason to not wait indefinitely here; in fact, if we make sure to wait until compaction finishes, the odds are much better that when we tell the client ""all done"" he won't be able to send a ""create"" quickly enough to hit the bug

We don't really try to wait indefinitely here, just for 30 seconds (worst case), if compactions don't finish until then we just move on with delete. do you want it to wait until all compactions  to finish?

bq. Need to call stopCompactionFor on every replica, not just CompactionServer – move this to DefsTable.dropColumnFamily?

I agree, I'm going to move that into dropColumnFamily call so it gets called on the replicas too.","23/May/12 22:24;jbellis;bq. String is better suited because CompactionInfo.getColumnFamily() returns a String 

Feel free to fix that. :)

bq. if compactions don't finish until then we just move on with delete

It throws IOException.

Remember that we check for ability to abort compaction every row; if we're compacting a wide row, it could easily take over 30s w/ throttling.","23/May/12 22:47;xedin;bq. It throws IOException. Remember that we check for ability to abort compaction every row; if we're compacting a wide row, it could easily take over 30s w/ throttling.

Oh yes, sorry. I think we can just remove that exception and move on with drop, or do you want it to until all compactions finish?

","28/May/12 15:33;jbellis;v3 attached.  removes CompactionInfo fields that are redundant w/ the introduction of CFM, and removes the wait from the stop method (it doesn't help clean up the sstables involved any faster, so there is no point in slowing down the drop for it).",28/May/12 18:26;xedin;Committed with nit in CompactionInfo.getColumnFamily() to return cfName instead of ksName as in v3.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stream sessions can only fail via the FailureDetector,CASSANDRA-4051,12546493,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,14/Mar/12 22:37,12/Mar/19 14:12,13/Mar/19 22:27,11/Apr/12 20:08,1.1.0,,,,,0,streaming,,,,,,"If for some reason, FileStreamTask itself fails more than the number of retry attempts but gossip continues to work, the stream session will never be closed.  This is unlikely to happen in practice since it requires blocking the storage port from new connections but keeping the existing ones, however for the bulk loader this is especially problematic since it doesn't have access to a failure detector and thus no way of knowing if a session failed.",,,,,,,,,,CASSANDRA-4045,,,,,,,,,27/Mar/12 18:57;yukim;4051-v2.txt;https://issues.apache.org/jira/secure/attachment/12520175/4051-v2.txt,30/Mar/12 17:54;yukim;4051-v3.txt;https://issues.apache.org/jira/secure/attachment/12520639/4051-v3.txt,16/Mar/12 18:06;brandon.williams;4051.txt;https://issues.apache.org/jira/secure/attachment/12518713/4051.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-03-27 16:42:09.093,,,no_permission,,,,,,,,,,,,231651,,,Wed Apr 11 20:08:22 UTC 2012,,,,,,0|i0grdr:,95866,brandon.williams,brandon.williams,,,,,,,,,,15/Mar/12 03:27;brandon.williams;It looks like we could extract/rebase the streaming changes from CASSANDRA-3112's first patch to solve this well enough for the bulk loader and BOF.,"16/Mar/12 18:06;brandon.williams;Updated patch extracted as mentioned, doesn't change any streaming behavior but does provide a way to detect errors that CASSANDRA-3112 and CASSANDRA-4045 can build on.","27/Mar/12 16:42;yukim;Since CASSANDRA-3216 added IEndpointStateChangeSubscriber and IFailureDetectionEventListner to StreamOutSession, we need to keep that functionality. I proposed modified version of CASSANDRA-3112 except limiting retry part on CASSANDRA-3817, I would like to rebase that patch and add retry, so that I can post it here. (I will post it soon.)","27/Mar/12 18:57;yukim;Patch attached based on CASSANDRA-3817 with retry limit.
(I think it is nice to have retry limit per stream session, so that we can configure, say, no retry for bulk loading, which I think is enough. But that's beyond this issue.)

> Brandon

Can you test and see if BOF is OK with this patch?","27/Mar/12 20:07;brandon.williams;BOF looks good, +1, committed.","28/Mar/12 21:06;brandon.williams;Reopening because this only fixes the problem in one way, FileStreamTask can still fail all 8 times and never close the session.  In general, outbound streaming's ""fire and forget"" methodology is problematic for bulk loading.","30/Mar/12 17:54;yukim;v3 attached for 1.1 branch.

It basically catches IOException on both sides and lets sessions closed.
I also implemented IStreamCallback#onFailure to make sure latches count down to avoid process hang.",11/Apr/12 20:08;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data insertion fails because of commitlog rename failure,CASSANDRA-4337,12560470,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,patrycjusz,patrycjusz,13/Jun/12 09:34,12/Mar/19 14:12,13/Mar/19 22:27,31/Jul/12 05:14,1.1.4,,,,,0,commitlog,,,,,,"h3. Configuration
Cassandra server configuration:
{noformat}heap size: 4 GB
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: ""xxx.xxx.xxx.10,xxx.xxx.xxx.11""
listen_address: xxx.xxx.xxx.10
rpc_address: 0.0.0.0
rpc_port: 9160
rpc_timeout_in_ms: 20000
endpoint_snitch: PropertyFileSnitch{noformat}

cassandra-topology.properties
{noformat}xxx.xxx.xxx.10=datacenter1:rack1
xxx.xxx.xxx.11=datacenter1:rack1
default=datacenter1:rack1{noformat}

Ring configuration:
{noformat}Address         DC          Rack        Status State   Load            Effective-Ownership Token
                                                                                           85070591730234615865843651857942052864
xxx.xxx.xxx.10  datacenter1 rack1       Up     Normal  23,11 kB        100,00%             0
xxx.xxx.xxx.11  datacenter1 rack1       Up     Normal  23,25 kB        100,00%             85070591730234615865843651857942052864{noformat}

h3.Problem
I have ctreated keyspace and column family using CLI commands:
{noformat}create keyspace testks with placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and strategy_options = {datacenter1:2};
use testks;
create column family testcf;{noformat}

Then I started my Java application, which inserts 50 000 000 rows to created column family using Hector client. Client is connected to node 1.
After about 30 seconds (160 000 rows were inserted) Cassandra server on node 1 throws an exception:
{noformat}ERROR [COMMIT-LOG-ALLOCATOR] 2012-06-13 10:26:38,393 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7345742389552.log to 7475933520374 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:204)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:166)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7345742389552.log to 7475933520374 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
	... 5 more{noformat}
	
Then, few seconds later Cassandra server on node 2 throws the same exception:
{noformat}ERROR [COMMIT-LOG-ALLOCATOR] 2012-06-14 10:26:44,005 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7320337904033.log to 7437675489307 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:204)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:166)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7320337904033.log to 7437675489307 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
	... 5 more{noformat}

After that, my application cannot insert any more data. Hector gets TimedOutException from Thrift:
{noformat}Thread-4 HConnectionManager.java 306 2012-06-14 10:26:56,034 HConnectionManager  operateWithFailover 	 WARN  	 %Could not fullfill request on this host CassandraClient<xxx.xxx.xxx.10:9160-10> 
Thread-4 HConnectionManager.java 307 2012-06-14 10:26:56,034 HConnectionManager operateWithFailover 	 WARN  	 %Exception:  
me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
	at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:35)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:264)
	at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecuteOperation(ExecutingKeyspace.java:97)
	at me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:243)
	at patrycjusz.nosqltest.db.cassandra.CassandraHectorDbAdapter.commitTransaction(CassandraDbAdapter.java:63)
	at patrycjusz.nosqltest.DbTest.insertData(DbTest.java:459)
	at patrycjusz.nosqltest.gui.InsertPanel.executeTask(NePanel.java:154)
	at patrycjusz.nosqltest.gui.InsertPanel$1.run(NePanel.java:141)
	at java.lang.Thread.run(Unknown Source)
Caused by: TimedOutException()
	at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:20269)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:922)
	at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:908)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:246)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:243)
	at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:103)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:258)
	... 8 more{noformat}","- Node 1:
   Hardware: Intel Xeon 2.83 GHz (4 cores), 24GB RAM, Dell VIRTUAL DISK SCSI 500GB
   System: Windows Server 2008 R2 x64
   Java version: 7 update 4 x64
- Node 2:
    Hardware: Intel Xeon 2.83 GHz (4 cores), 8GB RAM, Dell VIRTUAL DISK SCSI 500GB
    System: Windows Server 2008 R2 x64
	Java version: 7 update 4 x64",,,,,,,,,,,,,,,,,,01/Jul/12 01:12;jbellis;4337-poc.txt;https://issues.apache.org/jira/secure/attachment/12534138/4337-poc.txt,25/Sep/12 18:35;hbarot;Cassandra-Error.txt;https://issues.apache.org/jira/secure/attachment/12546563/Cassandra-Error.txt,15/Jun/12 05:58;patrycjusz;system-node1-stress-test.log;https://issues.apache.org/jira/secure/attachment/12532164/system-node1-stress-test.log,13/Jun/12 09:36;patrycjusz;system-node1.log;https://issues.apache.org/jira/secure/attachment/12531940/system-node1.log,15/Jun/12 05:58;patrycjusz;system-node2-stress-test.log;https://issues.apache.org/jira/secure/attachment/12532165/system-node2-stress-test.log,13/Jun/12 09:37;patrycjusz;system-node2.log;https://issues.apache.org/jira/secure/attachment/12531941/system-node2.log,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2012-06-13 15:14:56.168,,,no_permission,,,,,,,,,,,,256073,,,Tue Sep 25 18:40:54 UTC 2012,,,,,,0|i0guqf:,96409,patrycjusz,patrycjusz,,,,,,,,,,13/Jun/12 09:36;patrycjusz;Attached Cassandra server logs,13/Jun/12 15:14;jbellis;Why would commitlog rename fail?  Do you have some kind of permissions problem?,"14/Jun/12 06:26;patrycjusz;I don't think that it has anything to do with permissions. I'm running Cassandra as administrator.
Last night I found a probable cause of the problem. I switched to Java 6 update 33 x64 on both machines and was able to perform all planed tests without any errors.
It seems that this bug is related to Java version I used. Problem doesn't occur when using Java 6.",14/Jun/12 21:02;jbellis;Can you reproduce w/ java7 and the Stress tool that ships with Cassandra?,"15/Jun/12 05:58;patrycjusz;I've attached logs from test with Stress tool on Java 7 update 5.
I had to run Stress tool twice to reproduce this bug. At first run no exception was thrown.",01/Jul/12 00:53;jbellis;It may be the CommitLogSegment mmap'd buffer preventing rename.  Can you test the attached patch?,03/Jul/12 13:48;patrycjusz;I've tested with attached patch and couldn't reproduce this bug.,"31/Jul/12 05:14;jbellis;I was going to try to make this super robust for JVMs that don't provide an munmap cleaner, but I decided that since (a) it's extremely unlikely that anyone runs such JVMs on windows, and (b) such JVMs work fine anyway on *nix since you can rename w/o unmapping first, and (c) this doesn't make things worse for such JVMs; it just fixes it for OpenJDK / Oracle JDK, I'll just commit this as is.

(In any case, the ""right"" solution for such a posited JVM is to wrap its munmap funcationality the way we did for Oracle's...  such functionality has to exist for the GC to clean up direct buffers.)","25/Sep/12 18:40;hbarot;I ran into this same problem and I have tried all the solutions mentioned above and I still cant fix the problem.
I am running Cassandra on my windows 7.
    Cassandra : 1.1.5 with Java 6, 
    64 bit version on Windows 7. 

ERROR 11:03:01,454 Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
 java.io.IOError: java.io.IOException: Rename from C:\DataStax Community\data\commitlog\CommitLog-83930807354964.log to 84059497979959 failed
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$3.run(CommitLogAllocator.java:203)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Rename from C:\DataStax Community\data\commitlog\CommitLog-83930807354964.log to 84059497979959 failed
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
        ... 4 more

Any help is greatly appreciated.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem with creating keyspace after drop,CASSANDRA-4219,12553879,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jeffwilliams,jeffwilliams,04/May/12 07:29,12/Mar/19 14:12,13/Mar/19 22:27,15/May/12 14:20,1.1.1,,,,,0,,,,,,,"Hi,

I'm doing testing and wanted to drop a keyspace (with a column family) to re-add it with a different strategy. So I ran in cqlsh:

DROP KEYSPACE PlayLog;

CREATE KEYSPACE PlayLog WITH strategy_class = 'SimpleStrategy'
 AND strategy_options:replication_factor = 2;

And everything seemed to be fine. I ran some inserts, which also seemed to go fine, but then selecting them gave me:

cqlsh:PlayLog> select count(*) from playlog;
TSocket read 0 bytes

I wasn't sure what was wrong, so I tried dropping and creating again, and now when I try to create I get:

cqlsh> CREATE KEYSPACE PlayLog WITH strategy_class = 'SimpleStrategy'
  ...   AND strategy_options:replication_factor = 2;
TSocket read 0 bytes

And the keyspace doesn't get created. In the log it shows:

ERROR [Thrift:4] 2012-05-03 18:23:05,124 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
       at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:372)
       at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:191)
       at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:129)
       at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:701)
       at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:875)
       at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1235)
       at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3458)
       at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3446)
       at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
       at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
       at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
       at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
       at java.util.concurrent.FutureTask$Sync.innerGet(Unknown Source)
       at java.util.concurrent.FutureTask.get(Unknown Source)
       at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:368)
       ... 13 more
Caused by: java.lang.AssertionError
       at org.apache.cassandra.db.DefsTable.updateKeyspace(DefsTable.java:441)
       at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:339)
       at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:269)
       at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
       at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
       at java.util.concurrent.FutureTask.run(Unknown Source)
       ... 3 more
ERROR [MigrationStage:1] 2012-05-03 18:23:05,124 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError
       at org.apache.cassandra.db.DefsTable.updateKeyspace(DefsTable.java:441)
       at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:339)
       at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:269)
       at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
       at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
       at java.util.concurrent.FutureTask.run(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
       at java.lang.Thread.run(Unknown Source)

Any ideas how I can recover from this?

I am running version 1.1.0 and have tried nodetool repair, cleanup, compact. I can create other keyspaces, but still can't create a keyspace called PlayLog even though it is not listed anywhere.

Jeff",Debian 6.0.4 x64,,,,,,,,,,,,,,,,,,04/May/12 09:27;slebresne;0001-Add-debug-logs.txt;https://issues.apache.org/jira/secure/attachment/12525582/0001-Add-debug-logs.txt,11/May/12 18:33;xedin;CASSANDRA-4219.patch;https://issues.apache.org/jira/secure/attachment/12526555/CASSANDRA-4219.patch,25/Dec/17 14:34;mhanna;k2fabric.log.100;https://issues.apache.org/jira/secure/attachment/12903648/k2fabric.log.100,25/Dec/17 14:34;mhanna;k2fabric.log.101;https://issues.apache.org/jira/secure/attachment/12903647/k2fabric.log.101,25/Dec/17 14:34;mhanna;k2fabric.log.102;https://issues.apache.org/jira/secure/attachment/12903646/k2fabric.log.102,25/Dec/17 14:34;mhanna;k2fabric.log.103;https://issues.apache.org/jira/secure/attachment/12903645/k2fabric.log.103,25/Dec/17 14:34;mhanna;k2fabric.log.98;https://issues.apache.org/jira/secure/attachment/12903650/k2fabric.log.98,25/Dec/17 14:34;mhanna;k2fabric.log.99;https://issues.apache.org/jira/secure/attachment/12903649/k2fabric.log.99,25/Dec/17 14:35;mhanna;no-keyspace-ondisk.png;https://issues.apache.org/jira/secure/attachment/12903643/no-keyspace-ondisk.png,25/Dec/17 14:34;mhanna;no_keyspace.png;https://issues.apache.org/jira/secure/attachment/12903644/no_keyspace.png,25/Dec/17 14:35;mhanna;schema_after_ks_creation.png;https://issues.apache.org/jira/secure/attachment/12903642/schema_after_ks_creation.png,25/Dec/17 14:35;mhanna;schema_before_ks_creation.png;https://issues.apache.org/jira/secure/attachment/12903641/schema_before_ks_creation.png,08/May/12 13:44;jeffwilliams;system-91.223.192.26.log.gz;https://issues.apache.org/jira/secure/attachment/12525993/system-91.223.192.26.log.gz,04/May/12 10:25;jeffwilliams;system-debug.log.gz;https://issues.apache.org/jira/secure/attachment/12525590/system-debug.log.gz,05/May/12 12:30;jeffwilliams;system-startup-debug.log.gz;https://issues.apache.org/jira/secure/attachment/12525727/system-startup-debug.log.gz,04/May/12 09:44;jeffwilliams;system.log.gz;https://issues.apache.org/jira/secure/attachment/12525585/system.log.gz,16.0,,,,,,,,,,,,,,,,,,,2012-05-04 09:27:46.1,,,no_permission,,,,,,,,,,,,238092,,,Mon Dec 25 14:36:27 UTC 2017,,,,,,0|i0gtdj:,96189,jbellis,jbellis,,,,,,,,,,"04/May/12 09:27;slebresne;Would that be possible for you to try applying the attached patch (0001-Add-debug-logs.txt) on one of the machine, switch the log to DEBUG, try recreating said keyspace and send us the resulting log. This should give us more info on what's going on.

Also, do you still have the log of when you first got the 'TSocket read 0 bytes' during a select? Is there any corresponding errors?","04/May/12 09:52;jeffwilliams;I have attached the entire system.log from yesterday for the server I was running cqlsh on (10.20.20.25). The cluster is:

oot@meta01:~# nodetool -h meta01 ring PlayLog3
Address         DC          Rack        Status State   Load            Effective-Owership  Token                                       
                                                                                           113427455640312821154458202477256070485     
10.20.20.25     CPH         R1          Up     Normal  27.88 MB        66.67%              0                                           
10.20.20.26     CPH         R1          Up     Normal  17.5 MB         66.67%              56713727820156410577229101238628035242      
10.20.20.24     CPH         R1          Up     Normal  72.44 MB        66.67%              113427455640312821154458202477256070485     

However, I have switched from SimpleSnitch to PropertyFileSnitch this morning.

I'm not sure about the times I run the commands and where that corresponds to in the log, but I'm guessing you may know.

I was testing fail-over and was shutting down the server 10.20.20.26 during writes as a testing. However, it looks like I took it down at 2012-05-03 16:07:13 and that it was down when I did the drop and create, which could be the cause. I see the first error soon after it came back up. Also, I remember that these 'Couldn't find cfId=1013' errors occurred in the system.log on the other servers at the same time (I can send these if you want).

I'm currently running from the Debian packages. I'll try to apply the patch there and reinstall the packages.",04/May/12 10:25;jeffwilliams;Log for adding Keyspace with debug patch applied and log mode DEBUG.,"04/May/12 12:56;slebresne;The weird part here is that this DEBUG log seems to say that the keyspace should exist before you even do the create keyspace (but if it was correctly loaded, you should get a different error, so something is wrong). Could you try restarting the node with DEBUG log and attach that (just wait for the node to be up and running). I want to try to see what's going on with the loading of that keyspace.",05/May/12 12:30;jeffwilliams;Debug log for node startup,"08/May/12 13:43;jeffwilliams;Anything useful in that log?

I seems to have replicated the issue somehow. Firstly, I moved the servers onto public IP's, though the last octet is the same:

nodetool -h meta01 ring
Address         DC          Rack        Status State   Load            Owns                Token                                       
                                                                                           113427455640312821154458202477256070485     
91.223.192.25   CPH         R1          Up     Normal  11.2 MB         33.33%              0                                           
91.223.192.26   CPH         R1          Up     Normal  15.16 MB        33.33%              56713727820156410577229101238628035242      
91.223.192.24   CPH         R1          Up     Normal  20.11 MB        33.33%              113427455640312821154458202477256070485   

I created a new keyspace PlayLog2 (PlayLog still does not work), and a column family playlog.

This was available on all nodes. I then ran a few test inserts which worked fine. Then, to test fail-over, I shutdown the node 91.223.192.26 during inserts. The inserts completed fine and a while later I restarted the node 91.223.192.26. Then, when I went to re-run my tests, I see (Hector client):

5710 [Thread-1] DEBUG me.prettyprint.cassandra.connection.client.HThriftClient  - Creating a new thrift connection to meta02.cph.aspiro.com(91.223.192.26):9160
5711 [Thread-0] DEBUG me.prettyprint.cassandra.connection.client.HThriftClient  - keyspace reseting from null to PlayLog2
Exception in thread ""Thread-1"" me.prettyprint.hector.api.exceptions.HInvalidRequestException: InvalidRequestException(why:Keyspace PlayLog2 does not exist)

Sure enough, from command line client on 91.223.192.26, I see no PlayLog2 keyspace, yet it exists on 91.223.192.24 and 91.223.192.25. I have attached the system.log from 91.223.192.26 in the hope that it is useful.","08/May/12 14:58;jeffwilliams;Ok, I can now reproduce this on my cluster.

If I start with all three servers running. And on one of the servers I create a keyspace, create a column family and test, it all works fine. If I then drop the keyspace and re-create it, everything continues to work. However, as soon as one of the nodes is restarted, the keyspace disappears on that node. If I restart every node in the cluster, then the keyspace cannot be seen anyhwere, however, I can still no longer create a keyspace with that name.","11/May/12 08:17;jeffwilliams;Hi Pavel,

Have you been able to reproduce this? I am wanting to use these servers for production traffic and am wondering if this issue is a general bug, or due to a corruption in my cluster.

Regards,
Jeff","11/May/12 10:45;xedin;Hi Jeff, unfortunately I wasn't able to reproduce the situation you are seeing using the following steps: 

   1. run ccm cluster with 3 nodes
   2. create keyspace/cf and added some data
   3. stopped node 2
   4. dropped keyspace (using CLI from node 1)
   5. re-created keyspace and column family (using CLI from node 3 and on the other try from node 1)
   6. added some data to the keyspace
   7. started node 2

I have seen some of ('Couldn't find cfId=X) but this is unavoidable since we use sequential numbering of CFs and you have re-created one with the same name, we have an issue to switch to UUID ids too (CASSANDRA-3794).

Can you try to run 'resetlocalschema' nodetool command on the failing node? It would truncate all of the schema system tables and try to request it again and re-apply, this was designed specially to resolve such weird situations.

","11/May/12 10:51;jeffwilliams;Pavel,

I am able to recreate this with a freshly installed cluster using the debian 1.1 packages. The steps are:

1. Setup the cluster (I only had 2 nodes in my test)
2. create keyspace on node1 (confirm created on node2)
3. drop keyspace on node1 (confirm dropped on node2)
4. re-created keyspace on node1 (confirm re-created on node2)
5. restart node2
6. keyspace no longer exists on node2

Regards,
Jeff



","11/May/12 11:06;xedin;Yeah, I can reproduce this one - that happens because when you re-create keyspace it wouldn't change the UUID version from the original so when migration is send to the node2 originally it wouldn't correctly merge it into the system table. I will try to fix this one asap.","11/May/12 18:33;xedin;The problem was that when KS/CF is deleted row in the system table is marked for delete and all of it's columns are moved, so when it's re-created columns are added but row stays marked for delete, we need to check if given KS/CF doesn't have attributes in it's system table and if it's marked for delete all together, when we do schema version re-generate or load.","11/May/12 18:34;xedin;Jeff, I have done both scenarios you mentioned to check if everything now working as expected, can you please also confirm (just to double-check) if it works on your side too?...",11/May/12 19:38;jeffwilliams;Just did a quick test and it is looking good. The keyspace doesn't disappear! I'll do some more checks to make sure that all of the data inserts works before and after.,"11/May/12 22:27;jbellis;do we need to test for row.cf.isEmpty when there was no deletion involved?

do we need a removeDeleted call in there so the row tombstone can supress obsolete columns pre-compaction?","11/May/12 22:35;xedin;No, we only check for row.cf.isEmpty when row was marked for delete which could be that KS/CF was actually deleted (empty but row is still there) or re-created. Don't think that we should worry about removeDeleted because isMarkedForDelete + isEmpty gives us sufficient information.","11/May/12 22:39;jbellis;bq. Don't think that we should worry about removeDeleted because isMarkedForDelete + isEmpty gives us sufficient information

To clarify: what I'm concerned about is, if there is a row-level tombstone against a previously existing CF definition, then row.isEmpty will be false.  so (markedForDelete && isEmpty) will also be false...","11/May/12 22:51;xedin;Ok, I understand your worries, we use CFS.getRangeSlice to fetch data which calls removeDeleted in the process of request processing.",11/May/12 22:57;jbellis;+1,15/May/12 06:44;jeffwilliams;I've done some thorough testing now and this looks fixed. Thanks guys.,15/May/12 14:20;xedin;Committed.,"22/May/12 18:55;dmuth;
Good Afternoon,

It turns out that I am having this exact issue, and I found this bug via Google.  As previously stated, I dropped a keyspace during testing, but was unable to re-create it.  Looking in system.schema_keyspaces still shows an entry for that keyspace, but I cannot drop that keyspcae either.

First, is there anything I can do as a workaround, short of deleting all of my data and starting over?

Second, this isn't in a cluster, it's a single machine, and a development machine at that.  So there's no confidential data involved.  If it would be of any help whatsoever, I'd be happy to send copies of the entire /var/lib/cassandra/ directory.  Please let me know!

Thanks for your time,

-- Doug


","22/May/12 19:04;xedin;Hi Doug, I think that the best option would be to simply apply patch from this issue and re-compile Cassandra.","24/Jan/13 02:46;mharris;I am seeing this issue on version 1.1.6 as well upon dropping and recreating a keyspace.  Any idea whether there might have been a regression between the fix of this issue and the release of 1.1.6?

java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:194)
	at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:127)
	at org.apache.cassandra.thrift.CassandraServer.system_add_keyspace(CassandraServer.java:992)
	... (redacted)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
	... 64 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.DefsTable.updateKeyspace(DefsTable.java:518)
	at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:415)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:345)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:217)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more","24/Jan/13 03:04;brandon.williams;Before 1.1.7 all bets are off for schema problems.  Jira is not the best support forum, try the mailing list or irc.","24/Jan/13 03:32;mharris;Your bug-tracking system isn't the right place to report that a bug that was supposed to be fixed might have a regression in a later version...?  Ok, I'll mail the list then...","25/Dec/17 14:36;mhanna;I think i have the same issue in C* 3.11.1

i have a multi-dc cluster
6 nodes
2 DCs

DC1
98
99 - seed
100

DC2
101
102
103 - seed

via cqlsh from 102
1. using a fresh cluster
2. create keyspace k2view_functionslu
3. create table
Works!

from node 98
via cqlsh drop keyspace - succeded

now from node 102
via cqlsh from 102
1. create keyspace k2view_functionslu
2. create table => Keyspace k2view_functionslu doesn't exist


- attacged logs from all nodes
- attached schema version from all nodes
- attached ""desc keyspaces"" ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix dependency versions in generated pos,CASSANDRA-4183,12552532,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stephenc,stephenc,stephenc,25/Apr/12 08:12,12/Mar/19 14:11,13/Mar/19 22:27,04/May/12 13:06,1.1.1,,Packaging,,,0,,,,,,,Some of the versions of dependencies have fallen out of sync,,,,,,,,,,,,,,,,,,,08/Jan/13 21:43;amorton;5126-2.txt;https://issues.apache.org/jira/secure/attachment/12563831/5126-2.txt,25/Apr/12 08:12;stephenc;CASSANDRA-4183.diff;https://issues.apache.org/jira/secure/attachment/12524162/CASSANDRA-4183.diff,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-04 13:06:46.166,,,no_permission,,,,,,,,,,,,236792,,,Fri May 04 13:06:46 UTC 2012,,,,,,0|i0gsy7:,96120,slebresne,slebresne,,,,,,,,,,25/Apr/12 08:13;stephenc;Ready to be applied to the cassandra-1.1 branch,25/Apr/12 08:17;stephenc;Patch should also be applied to trunk,"04/May/12 13:06;slebresne;+1, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool ring throws java.lang.AssertionError in TokenMetadata.getTopology,CASSANDRA-4429,12598029,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,soverton,soverton,soverton,09/Jul/12 13:04,12/Mar/19 14:11,13/Mar/19 22:27,09/Jul/12 15:36,,,Tool/nodetool,,,0,,,,,,,"
{noformat}
$ bin/nodetool -h localhost ring
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.getTopology(TokenMetadata.java:851)
        at org.apache.cassandra.service.StorageService.effectiveOwnership(StorageService.java:2781)
        at org.apache.cassandra.service.StorageService.effectiveOwnership(StorageService.java:70)
{noformat}

TokenMetadata.getTopology() can only be called on a clone of TokenMetadata, not the StorageService instance.
",,,,,,,,,,,,,,,,,,,09/Jul/12 13:20;soverton;4429.patch;https://issues.apache.org/jira/secure/attachment/12535665/4429.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-09 15:36:55.303,,,no_permission,,,,,,,,,,,,256150,,,Mon Jul 09 15:36:55 UTC 2012,,,,,,0|i0gvrz:,96578,jbellis,jbellis,,,,,,,,,,"09/Jul/12 13:10;soverton;Another minor issue with CASSANDRA-3047 is that the IPs are printed using InetAddress.toString() which results in output like this:

{noformat}
$ bin/nodetool -h localhost ring
Note: Ownership information does not include topology; for complete information, specify a keyspace

Datacenter: dc1
==========
Address         Rack        Status State   Load            Owns                Token                                       
                                                                               127605887595351923798765477786913079296     
miles/10.2.129.41rack1       Up     Normal  22.57 KB        25.00%              0                                           
/10.2.129.51    rack2       Up     Normal  4.63 KB         25.00%              127605887595351923798765477786913079296     

Datacenter: dc2
==========
Address         Rack        Status State   Load            Owns                Token                                       
                                                                               85070591730234615865843651857942052864      
/10.2.129.15    rack1       Up     Normal  13.54 KB        25.00%              42535295865117307932921825928971026432      
/10.2.129.16    rack2       Up     Normal  9.06 KB         25.00%              85070591730234615865843651857942052864      
{noformat}

It should be using InetAddress.getHostAddress() instead.",09/Jul/12 13:20;soverton;Attached patch against trunk,"09/Jul/12 15:36;jbellis;+1, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix bug on decommission and removeNode,CASSANDRA-5216,12630576,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,wy96f,wy96f,wy96f,03/Feb/13 15:42,12/Mar/19 14:10,13/Mar/19 22:27,04/Feb/13 19:10,1.2.2,,,,,0,gossip,,,,,,"1. If one node decommissioned, the epState.isAlive is always true since it is DEAD_STATES in convict(). 0001 patch fixes that.
2. If we removeNode B on A,  we should put expireTime of B in expireTimeEndpointMap of A, otherwise the epState of B will never be removed from gossip entirely. 0002 patch fixes that.
3. After removeNode B, C reboots and receives epState of B. Since B is not in the tokenMetadata, C just wipes away B without recording B's expireTime. In this case, B will be always in gossip. 0003 patch fixes that.",,,,,,,,,,,,,,,,,,,03/Feb/13 15:45;wy96f;0001-fix-isAlive-bug-on-decommission.patch;https://issues.apache.org/jira/secure/attachment/12567771/0001-fix-isAlive-bug-on-decommission.patch,03/Feb/13 15:45;wy96f;0002-fix-expireTime-bug-on-removeNode.patch;https://issues.apache.org/jira/secure/attachment/12567772/0002-fix-expireTime-bug-on-removeNode.patch,03/Feb/13 15:45;wy96f;0003-fix-expireTime-bug-on-handleStateRemoving-after-rebo.patch;https://issues.apache.org/jira/secure/attachment/12567773/0003-fix-expireTime-bug-on-handleStateRemoving-after-rebo.patch,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-02-04 19:10:08.733,,,no_permission,,,,,,,,,,,,311072,,,Mon Feb 04 19:10:08 UTC 2013,,,,,,0|i1honb:,311420,brandon.williams,brandon.williams,,,,,,,,,,"04/Feb/13 19:10;brandon.williams;Good work, thanks.  Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple values for CurrentLocal Node ID,CASSANDRA-4626,12606503,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,amorton,amorton,06/Sep/12 23:25,12/Mar/19 14:10,13/Mar/19 22:27,07/Sep/12 16:46,1.0.12,1.1.5,,,,0,,,,,,,"From this email thread http://www.mail-archive.com/user@cassandra.apache.org/msg24677.html

There are multiple columns for the CurrentLocal row in NodeIdInfo:

{noformat}

[default@system] list NodeIdInfo ;
Using default limit of 100
...
-------------------
RowKey: 43757272656e744c6f63616c
=> (column=01efa5d0-e133-11e1-0000-51be601cd0ff, value=0a1020d2, timestamp=1344414498989)
=> (column=92109b80-ea0a-11e1-0000-51be601cd0af, value=0a1020d2, timestamp=1345386691897)
{noformat}

SystemTable.getCurrentLocalNodeId() throws an assertion that occurs when the static constructor for o.a.c.utils.NodeId is in the stack.

The impact is a java.lang.NoClassDefFoundError when accessing a particular CF (I assume on with counters) on a particular node.

Cannot see an obvious cause in the code. ",,,,,,,,,,,,,,,,,,,07/Sep/12 15:01;slebresne;4626.txt;https://issues.apache.org/jira/secure/attachment/12544224/4626.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-07 15:01:16.955,,,no_permission,,,,,,,,,,,,256296,,,Fri Sep 07 16:46:11 UTC 2012,,,,,,0|i0gxtr:,96910,jbellis,jbellis,,,,,,,,,,"07/Sep/12 15:01;slebresne;I think this can happen because of the commit log. Basically, it's possible that when you restart a node that it doesn't pick the correct current NodeId if he attempts to read the current NodeId before the commit log if fully replayed (and the more recent NodeId is in the log, not yet replayed). This would then lead to having 2 columns in the CurrentLocal row.

However, the main problem is that the way we maintain the CurrentLocal row is fragile and honestly dumb (I wrote it so I'm blaming myself). We store all the generated NodeId sorted by creation time in a separated row, so reading the last column of that row is a much simpler and resilient way to do it. Attaching a patch that does just that.  

The patch also adds a forceFlush in SystemTable.writeCurrentNodeId to avoid the problem of not reading the last NodeId because of log replay.
","07/Sep/12 15:04;slebresne;The patch is against 1.0. However if we're incomfortable messing with the SystemTable with 1.0, there is always the workaround of deleting the NodeIdInfo sstables.",07/Sep/12 15:16;jbellis;Maybe we should name the accessor methods CounterId instead of NodeId?  NodeId is confusingly similar to HostId.,07/Sep/12 16:11;jbellis;+1 on the fix,"07/Sep/12 16:26;slebresne;bq. Maybe we should name the accessor methods CounterId instead of NodeId?

That's a good idea, I'll do that.","07/Sep/12 16:46;slebresne;Committed, thanks (I haven't done the rename yet, I'll do it later).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keyspace disappears when upgrading node from cassandra-1.1.1 to cassandra-1.1.5,CASSANDRA-4698,12608662,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,21/Sep/12 16:21,12/Mar/19 14:10,13/Mar/19 22:27,27/Sep/12 14:18,1.1.6,,,,,0,,,,,,,"Here is how I got the problem to happen:

1. Get this zipped data directory (about 33Mb):
  scp cass@50.57.69.32:/home/cass/cassandra.zip ./ (password cass)
2. Unzip it in /var/lib/
3. clone the cassandra git repo
4. git checkout cassandra-1.1.1; ant jar;
5. bin/cassandra 
6. Run cqlsh -3, then DESC COLUMNFAMILIES; Note the presence of Keyspace performance_tests
7. pkill -f cassandra; git checkout cassandra-1.1.5; ant realclean; ant jar;
8. bin/cassandra
9. Run cqlsh -3, then DESC COLUMNFAMILIES; Note that there is no performance_tests keyspace",ubuntu. JNA not installed.,,,,,,,,,,,,,,,,,,25/Sep/12 00:00;xedin;CASSANDRA-4698.patch;https://issues.apache.org/jira/secure/attachment/12546412/CASSANDRA-4698.patch,21/Sep/12 16:24;tpatterson;start_1.1.1_system.log;https://issues.apache.org/jira/secure/attachment/12546056/start_1.1.1_system.log,21/Sep/12 16:24;tpatterson;start_1.1.5_system.log;https://issues.apache.org/jira/secure/attachment/12546057/start_1.1.5_system.log,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-09-24 22:03:45.846,,,no_permission,,,,,,,,,,,,241134,,,Thu Oct 11 19:06:53 UTC 2012,,,,,,0|i019iv:,5312,jbellis,jbellis,,,,,,,,,,21/Sep/12 16:24;tpatterson;These are the log files after starting 1.1.1 and 1.1.5 respectively,"24/Sep/12 22:03;cdaw;[~tpatterson]
Can you try this test out and let me know if the keyspace reappears?

* In 1.1.1, show keyspaces to get schema definitions
* Upgrade to 1.1.5
* Recreate table definitions:
** rm data/system/system_schema*
** restart
** CREATE KEYSPACE + CREATE TABLE as necessary",24/Sep/12 22:05;jbellis;It sounds like this reproduces on a single node?  No schema disagreement should be necessary/possible.,"24/Sep/12 22:13;tpatterson;{quote}It sounds like this reproduces on a single node? No schema disagreement should be necessary/possible.{quote}
yes, this happened on a single node.

{quote} Can you try this test out and let me know if the keyspace reappears? ...{quote}
Yes. I did those steps on one of the columnfamilies in the keyspace and the data did indeed come back.","25/Sep/12 00:00;xedin;Attaching the patch to fix the problem which was related to fixing timestmaps in ColumnSerializer, instead of doing that patch does fixing only schema RowMutation timestamps from remote nodes.

Tyler, can you please test the scenario when local correct schema data are getting overriden by incorrect remote (as seen at CASSANDRA-4561)?","25/Sep/12 04:03;jbellis;ISTM that only fixing remote timestamps is the wrong solution in general -- it fixes this particular scenario, but re-introduces ""unmodifiable"" keyspaces, since the [local] timestamp is now permanently left unnaturally high.

I think that we need to fix it at a higher level than serialization: once the rows [from different sstables] have all been deserialized and merged, THEN fix the timestamp.  Then we won't have the problem of an adjusted tombstone clobbering a value that it didn't, pre-adjustment.","25/Sep/12 09:43;xedin;bq. ISTM that only fixing remote timestamps is the wrong solution in general – it fixes this particular scenario, but re-introduces ""unmodifiable"" keyspaces, since the [local] timestamp is now permanently left unnaturally high.

The local timestamp fixing didn't go anywhere and still fixed _before_ we accept remote schema mutations that it why it's useful to fix only schema mutation without touching anything else...","26/Sep/12 03:00;tpatterson;{quote}Tyler, can you please test the scenario when local correct schema data are getting overriden by incorrect remote (as seen at CASSANDRA-4561)?{quote}
[~xedin] Could you explain what you mean by local and remote schemas, and how I can go about testing this? Thanks","26/Sep/12 10:00;xedin;[~tpatterson] By local I mean - definitions from system.schema_*, by remote - mutations that could be sent (pushed) to the node from others or could be requested if local schema version is different from remote ones.","26/Sep/12 18:06;tpatterson;[~xedin] I ran this test:

I set up a 1-node 1.1.1 cluster with that broken keyspace mentioned at the beginning of this test, then I bootstrapped another 1.1.1 node and verified that the keyspace was visible to the second node. The timestamps were 16 digits long on both nodes. 

Then I took down node 2, upgraded it to 1.1.5, and started it back up. The keyspace was not visible. In cassandra-cli I did was not able to see the timestamps on node 2, here is what the output looked like:
{code}
[default@system] list schema_columns;
Using default limit of 100
Using default column limit of 100
-------------------
RowKey: performance_tests

1 Row Returned.
Elapsed time: 2 msec(s).
{code}
I got the same results for list schema_keyspaces and list schema_columnfamilies.

On node1 (which was still on 1.1.1) the timestamps were still 16-digits long. I don't know how to get the now-incorrect remote schema (on node2) to overwrite the local correct schema (on node1).",26/Sep/12 18:14;xedin;Did you apply patch attached to this issue on top of 1.1.5?,26/Sep/12 18:23;jbellis;Patch LGTM.,"26/Sep/12 19:50;tpatterson;I hadn't applied the patch, but I just tried it again with the patch. With the patch the keyspace disappearing bug didn't happen. The timestamps all were 16-digits long on both nodes after upgrading one node.",26/Sep/12 20:20;xedin;Great! Tyler will be running the last test to confirm that new node correctly handles timestamps from old nodes and we are good to go.,"26/Sep/12 22:31;tpatterson;I re-ran the test like the one described earlier today, but after node2 was upgraded to 1.1.5 (with the patch), I followed Pavel's instructions:
start first and second, stop second, run resetschema on first, stop first, start second, start first

Both nodes then showed 16-digit timestamps with 3 zeros at the end. ","27/Sep/12 00:02;tpatterson;Per Pavel's request I ran the test again. Everything checked out, here are the steps followed:

1: get that data onto both nodes in a 1.1.1 cluster
2: upgrade and start node2 on 1.1.5+patch
3: stop both nodes
4: start node1 (1.1.1) and verify that timestamps are fixed (zero last 3 digits) 
5: stop node1 
6: start node2 (1.1.5+patch), verify that timestamps are fixed (3 zeros)
7: nodetool resetlocalschema on node2
8: Verify that the troublesome keyspace is *not* present on node2
9: stop node2
10: start node1
11: start node2 
12: verify that the troublesome keyspace appears on node2 and timestamps are fixed (3 zeros)",27/Sep/12 14:18;xedin;Committed.,"27/Sep/12 15:06;hudson;Integrated in Cassandra #2190 (See [https://builds.apache.org/job/Cassandra/2190/])
    adopt RM code of CASSANDRA-4698 to trunk (Revision a0d7d9713d776506ce6c2eea577eea3b7c5099bd)

     Result = ABORTED
xedin : 
Files : 
* src/java/org/apache/cassandra/db/RowMutation.java
",11/Oct/12 19:06;tpatterson;Verified that the issue is fixed for 1.1.6-tentative,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
alter table alter causes TSocket read 0 bytes,CASSANDRA-5012,12618499,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,khahn,khahn,02/Dec/12 18:12,12/Mar/19 14:10,13/Mar/19 22:27,07/Dec/12 09:14,1.2.0 rc1,,,,,0,,,,,,,"Altering the type of a clustering key column causes TSocket error. 

To reproduce the problem:
1. CREATE SCHEMA ""Excalibur"" WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };

2. CREATE TABLE ""Excalibur"".Test ( id int, species text, color text, PRIMARY KEY ((id, species), color)) WITH compaction = { 'class' : 'SizeTieredCompactionStrategy', 'min_compaction_threshold' : 6 };

3. Alter table ""Excalibur"".test ALTER color type int;

Expected result: Error message saying something like, ""Changing the type of a clustering key is not allowed.""

Actual result: TSocket read 0 bytes","On Mac OSX ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 2.3.0 | Cassandra 1.2.0-rc1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.35.0]",,,,,,,,,,,,,,,,,,03/Dec/12 15:32;slebresne;5012.patch;https://issues.apache.org/jira/secure/attachment/12555770/5012.patch,03/Dec/12 20:04;khahn;log.txt;https://issues.apache.org/jira/secure/attachment/12555817/log.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-02 19:27:06.561,,,no_permission,,,,,,,,,,,,293317,,,Fri Dec 07 09:14:39 UTC 2012,,,,,,0|i0swon:,166772,jbellis,jbellis,,,,,,,,,,"02/Dec/12 19:27;dbrosius;Some exception occurred on the server, that caused the TSocket error. Finding and posting that error in the server logs, will help what went wrong.",03/Dec/12 15:32;slebresne;Patch attached to move the validation at the time of the application of the alter statement.,03/Dec/12 20:04;khahn;Sorry for not including the log. Here it is if you're still interested in it.,07/Dec/12 03:20;jbellis;+1,"07/Dec/12 09:14;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad disk causes death of node despite disk_failure_policy,CASSANDRA-4847,12613006,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,kirktrue,kirktrue,kirktrue,22/Oct/12 18:11,12/Mar/19 14:10,13/Mar/19 22:27,13/Dec/12 16:20,1.2.1,,,,,0,,,,,,,"Steps:

# Create a bad disk via device mapper
# Specify good disk and bad disk is data directory
# Set {{disk_failure_policy}} to {{best_effort}} in cassandra.yaml
# Start node

Expected:

Attempts to create system directories to fail (as expected) on bad disk, and have it added to blacklisted directories.

Actual:

Node start up aborts due to uncaught error:

{noformat}
FSWriteError in /mnt/bad_disk/system_traces/sessions
        at org.apache.cassandra.io.util.FileUtils.createDirectory(FileUtils.java:258)
        at org.apache.cassandra.db.Directories.<init>(Directories.java:104)
        at org.apache.cassandra.db.Directories.create(Directories.java:90)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:404)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:227)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:393)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:436)
Caused by: java.io.IOException: Failed to mkdirs /mnt/bad_disk/system_traces/sessions
        ... 7 more
{noformat}
",,,,,,,,,,,,,,,,,,,13/Dec/12 02:04;kirktrue;trunk-4847.txt;https://issues.apache.org/jira/secure/attachment/12560705/trunk-4847.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-13 15:30:56.131,,,no_permission,,,,,,,,,,,,250379,,,Thu Dec 13 16:20:29 UTC 2012,,,,,,0|i0ay0f:,61783,jbellis,jbellis,,,,,,,,,,"13/Nov/12 04:58;kirktrue;I'd like to know if dying on startup with a bad disk regardless of disk_failure_policy setting is desired behavior. I'd lean toward it being 'no' from a pragmatic standpoint, but would like to know if there's a precedent against _simply_ black-listing the bad disk and moving on. I'd like to fix it from a robustness POV.",13/Nov/12 15:30;jbellis;I do think we should respect the policy wrt {{stop}}.  But I would agree that {{best_effort}} and {{ignore}} should both blacklist and move on here.,"13/Dec/12 16:20;jbellis;LGTM, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Subcolumn slice ends not respected,CASSANDRA-4826,12612301,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,thobbs,thobbs,17/Oct/12 20:17,12/Mar/19 14:10,13/Mar/19 22:27,22/Oct/12 07:22,1.2.0 beta 2,,,,,0,,,,,,,"When performing {{get_slice()}} on a super column family with the {{supercolumn}} argument set as well as a slice range (meaning you're trying to fetch a slice of subcolumn from a particular supercolumn), the slice ends don't seem to be respected.",,,,,,,,,,,,,,,,,,,19/Oct/12 19:44;vijay2win@yahoo.com;0001-CASSANDRA-4826.patch;https://issues.apache.org/jira/secure/attachment/12550064/0001-CASSANDRA-4826.patch,17/Oct/12 20:19;thobbs;4826-repro.py;https://issues.apache.org/jira/secure/attachment/12549568/4826-repro.py,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-18 20:07:46.981,,,no_permission,,,,,,,,,,,,249388,,,Mon Oct 22 07:22:46 UTC 2012,,,,,,0|i0a7b3:,57457,slebresne,slebresne,,,,,,,,,,"17/Oct/12 20:19;thobbs;The attached script reproduces using pycassa.  The assertions fail when running against 1.2.0-beta1 or trunk, but pass when running against 1.0 or 1.1.

The pycassa test suite covers these areas a bit more thoroughly, which may be useful to check a patch against.","18/Oct/12 20:07;jbellis;Could you have a look at this, Vijay?",18/Oct/12 20:12;vijay2win@yahoo.com;Will do thanks!,19/Oct/12 19:44;vijay2win@yahoo.com;Attached patch fixes the bug.,19/Oct/12 23:47;thobbs;The patch passes all of the pycassa tests. I'll leave the code review to Sylvain.,"22/Oct/12 07:22;slebresne;+1 (committed).

For the record, the reason this affects trunk but not 1.1 is that on trunk we've removed a end-of-slice check in SliceQueryFilter.colllectReducedColumns because this was redundant with the job of the sstable and memtable iterators. We forgot the super column case however, which this patch fixes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
isReadyForBootstrap doesn't compare schema UUID by timestamp as it should,CASSANDRA-4159,12551182,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,17/Apr/12 08:02,12/Mar/19 14:10,13/Mar/19 22:27,17/Apr/12 16:11,1.0.10,,,,,0,,,,,,,"CASSANDRA-3629 introduced a wait to be sure the node is up to date on the schema before starting bootstrap. However, the isReadyForBootsrap() method compares schema version using UUID.compareTo(), which doesn't compare UUID by timestamp, while the rest of the code does compare using timestamp (MigrationManager.updateHighestKnown).

During a test where lots of node were boostrapped simultaneously (and some schema change were done), we ended up having some node stuck in the isReadyForBoostrap loop. Restarting the node fixed it, so while I can't confirm it, I suspect this was the source of that problem.",,,,,,,,,,,,,,,,,,,17/Apr/12 08:05;slebresne;4159.txt;https://issues.apache.org/jira/secure/attachment/12522924/4159.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-17 15:37:46.704,,,no_permission,,,,,,,,,,,,236046,,,Tue Apr 17 16:11:11 UTC 2012,,,,,,0|i0gso7:,96075,brandon.williams,brandon.williams,,,,,,,,,,17/Apr/12 15:37;brandon.williams;+1,"17/Apr/12 16:11;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFRR wide row iterators improvements,CASSANDRA-4803,12611668,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,pkolaczk,pkolaczk,pkolaczk,13/Oct/12 08:16,12/Mar/19 14:10,13/Mar/19 22:27,01/Dec/12 07:05,1.1.8,,,,,0,,,,,,,"{code}
 public float getProgress()
    {
        // TODO this is totally broken for wide rows
        // the progress is likely to be reported slightly off the actual but close enough
        float progress = ((float) iter.rowsRead() / totalRowCount);
        return progress > 1.0F ? 1.0F : progress;
    }
{code}

The problem is iter.rowsRead() does not return the number of rows read from the wide row iterator, but returns number of *columns* (every row is counted multiple times). ",,,,,,,,,,,,,,,,,,,25/Nov/12 21:08;pkolaczk;0007-Fallback-to-describe_splits-v3.patch;https://issues.apache.org/jira/secure/attachment/12554809/0007-Fallback-to-describe_splits-v3.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-19 23:31:36.267,,,no_permission,,,,,,,,,,,,248417,,,Sat Dec 01 07:05:21 UTC 2012,,,,,,0|i09uxz:,55446,jbellis,jbellis,,,,,,,,,,"18/Oct/12 13:48;pkolaczk;I attach a list of patches affecting operation of CFRR:

# Fix for obvious counting bug in wide row iterator that was counting columns instead of rows.
# Several fixes in describe_splits:
   fixed non-uniform splitting - caused by integer math roundoff errors
   fixed insane behaviour when number of splits was higher than number of key samples
   added estimated size of the split to the result, and make use of it in CFIF
# This is a patch for broken get_paged_slice; addressed in a separate ticket, but I had to include it in order to test my code
# Fix for creating excessively small splits (and wrong progress reporting) due to range wrap around.
# get_range_slices allows for (start_key, end_token) exactly the same as get_paged_slice 
# I tried to de-spaghettize CFRR code a little. This also fixes some bug that accidentally slipped in with previous patches.","19/Oct/12 23:31;jbellis;reviewed + committed patches 01 and 02, rest still pending.","24/Oct/12 19:57;jbellis;03 committed in CASSANDRA-4816.

Not sure about 04 -- I'm a fan of the simplifications we get from letting CFRR only need to deal with non-wrapping splits.",05/Nov/12 09:36;pkolaczk;#04 - what about virtual nodes in 1.2? Do we insist that split may not span more than one contiguous token range? It will be harder to avoid too small splits. And too small split = bigger task book-keeping overhead.,05/Nov/12 10:13;pkolaczk;Hold on with applying patch 2 for a while. We just discovered it breaks running hive queries while doing rolling upgrade. There is a need for falling back to old describe_splits method if describe_splits_ex is not found.,06/Nov/12 16:11;pkolaczk;Attaching a patch allowing to generate splits also when talking to an older version of thrift server.,"12/Nov/12 09:40;pkolaczk;Rebased patches for recent 1.1. Some patches have been already applied, so removed them from the list.","17/Nov/12 13:02;jbellis;bq. what about virtual nodes in 1.2? Do we insist that split may not span more than one contiguous token range?

That's kind of orthogonal to wrapping ranges per se -- you'll still only have a single [virtual] node whose range wraps.  So vnodes won't make that worse.  Moreover, you're still going to need two scans at the disk level since a wrapping range won't be contiguous there.  (Currently wrapping ranges are split by StorageProxy.getRestrictedRanges but this may change for CASSANDRA-4858.)  Doing an extra Thrift or CQL query is negligible overhead compared to the actual scan.

Finally, getRestrictedRanges *will* split it up into scan-per-vnode which I agree is something we should fix but I don't think this patch does it.  As an optimization I don't think it's something we should block 1.2.0 for.  Should we split this into a separate ticket?","17/Nov/12 13:02;jbellis;0006: Can you split out the bug fix and rebase?  Refactoring is fine but let's keep it separate from bug fixes.

0007: I'm unclear what this is useful for, anyone running a 1.1 recent enough to have this patch, would also have describe_splits_ex, no?","20/Nov/12 15:48;pkolaczk;0007 - this is for rolling upgrade. When you upgrade one node and the other nodes don't have describe_splits_ex yet, starting a hadoop job on a newly upgraded node fails.

As for 0004 / 0006 fixes - I agree. Let's move them to a separate ticket. ","20/Nov/12 15:49;pkolaczk;BTW: I don't get email notifications from ASF Jira. It is because I registered long, long time ago and my email is obsolete. How to change my email to a newer one? I can't see an option in the user profile for doing that. ",21/Nov/12 11:48;jbellis;Is there a more specific exception we can catch besides TException?,"21/Nov/12 11:49;jbellis;Without trying it, I think it would be TApplicationException.WRONG_METHOD_NAME.","21/Nov/12 20:33;pkolaczk;[~jbellis] right, I change it.",22/Nov/12 08:58;pkolaczk;0007-Fallback-to-describe-splits-v2.patch: Catching TApplicationException and checking its type. ,"25/Nov/12 21:08;pkolaczk;Attached third version of the patch - this time it works. The right error code is UNKNOWN_METHOD (1), not WRONG_METHOD_NAME.",01/Dec/12 07:05;jbellis;(committed),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage doesn't decode name in widerow mode,CASSANDRA-5098,12625705,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,justen_walker,justen_walker,02/Jan/13 17:08,12/Mar/19 14:10,13/Mar/19 22:27,04/Jan/13 17:13,1.1.9,1.2.1,,,,0,cassandra,hadoop,pig,,,,"CassandraStorage doesn't decode name in widerow mode. This causes functions such as FILTER to fail with a ClassCastException, since the key is a bytearray instead of a chararray.

{code:title=test.pig}
DEFINE CassandraStorage org.apache.cassandra.hadoop.pig.CassandraStorage;

A  = LOAD 'cassandra://Metrics/EventEntries?widerows=true' USING CassandraStorage();
-- describe A --> A: {key: chararray,columns: {(name: (),value: chararray)}}

B = FILTER A BY key matches '^user.hit';
-- Throws CCE: org.apache.pig.data.DataByteArray cannot be cast to java.lang.String
{code}","Ubuntu 12.04.1 x64, Cassandra 1.1.8",,,,,,,,,,,,,,,,,,04/Jan/13 13:53;brandon.williams;5098.txt;https://issues.apache.org/jira/secure/attachment/12563288/5098.txt,02/Jan/13 17:08;justen_walker;pig.log;https://issues.apache.org/jira/secure/attachment/12562923/pig.log,02/Jan/13 17:26;justen_walker;test_schema.cli;https://issues.apache.org/jira/secure/attachment/12562927/test_schema.cli,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-01-04 13:53:08.713,,,no_permission,,,,,,,,,,,,302253,,,Fri Jan 04 17:13:14 UTC 2013,,,,,,0|i16zpj:,248920,iamaleksey,iamaleksey,,,,,,,,,,02/Jan/13 17:08;justen_walker;Add Stack Trace,02/Jan/13 17:26;justen_walker;Attach test_schema,"04/Jan/13 13:53;brandon.williams;In widerow mode, we weren't decoding the key since we have to track it to recompose the row into a bag.  Simple patch to decode the key when we add it to the tuple.",04/Jan/13 16:34;iamaleksey;+1,04/Jan/13 17:13;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compact storage metadata is broken,CASSANDRA-5189,12629420,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,27/Jan/13 04:35,12/Mar/19 14:09,13/Mar/19 22:27,29/Jan/13 18:10,1.2.2,,Legacy/CQL,,,0,,,,,,,"{noformat}
cqlsh:foo> CREATE TABLE bar (
       ...     id int primary key,
       ...     i int
       ... ) WItH COMPACT STORAGE;

cqlsh:foo> INSERT INTO bar (id, i) VALUES (1, 2);
Bad Request: Missing PRIMARY KEY part column1
Perhaps you meant to use CQL 2? Try using the -2 option when starting cqlsh.

cqlsh:foo> INSERT INTO bar (id, column1) VALUES (1, 2);
Bad Request: Missing mandatory column i
Perhaps you meant to use CQL 2? Try using the -2 option when starting cqlsh.
{noformat}",,,,,,,,,,,,,,,,,,,28/Jan/13 10:28;slebresne;5189.txt;https://issues.apache.org/jira/secure/attachment/12566741/5189.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-29 14:46:51.148,,,no_permission,,,,,,,,,,,,309916,,,Tue Jan 29 18:10:39 UTC 2013,,,,,,0|i1hhhz:,310260,jasobrown,jasobrown,,,,,,,,,,29/Jan/13 14:46;jasobrown;Tested locally and it worked properly. LGTM. +1,"29/Jan/13 16:10;jbellis;I'm not actually sure how this fixes the problem, although it apparently does -- looks like only validation changes were made to CREATE.","29/Jan/13 17:45;slebresne;It's not only validation change in fact, the change to the first {{if}} is what fixes it. ","29/Jan/13 17:50;jbellis;+1
","29/Jan/13 18:10;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non Unique Streaming session ID's,CASSANDRA-4223,12554104,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,amorton,amorton,amorton,06/May/12 23:10,12/Mar/19 14:09,13/Mar/19 22:27,18/May/12 18:40,1.0.11,1.1.1,,,,0,datastax_qa,,,,,,"I have observed repair processes failing due to duplicate Streaming session ID's. In this installation it is preventing rebalance from completing. I believe it has also prevented repair from completing in the past. 

The attached streaming-logs.txt file contains log messages and an explanation of what was happening during a repair operation. it has the evidence for duplicate session ID's.

The duplicate session id's were generated on the repairing node and sent to the streaming node. The streaming source replaced the first session with the second which resulted in both sessions failing when the first FILE_COMPLETE message was received. 

The errors were:

{code:java}
DEBUG [MiscStage:1] 2012-05-03 21:40:33,997 StreamReplyVerbHandler.java (line 47) Received StreamReply StreamReply(sessionId=26132848816442266, file='/var/lib/cassandra/data/FMM_Studio/PartsData-hc-1-Data.db', action=FILE_FINISHED)
ERROR [MiscStage:1] 2012-05-03 21:40:34,027 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MiscStage:1,5,main]
java.lang.IllegalStateException: target reports current file is /var/lib/cassandra/data/FMM_Studio/PartsData-hc-1-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:195)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{code}

and

{code:java}
DEBUG [MiscStage:2] 2012-05-03 21:40:36,497 StreamReplyVerbHandler.java (line 47) Received StreamReply StreamReply(sessionId=26132848816442266, file='/var/lib/cassandra/data/OpsCenter/rollups7200-hc-3-Data.db', action=FILE_FINISHED)
ERROR [MiscStage:2] 2012-05-03 21:40:36,497 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MiscStage:2,5,main]
java.lang.IllegalStateException: target reports current file is /var/lib/cassandra/data/OpsCenter/rollups7200-hc-3-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:195)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{code}


I think this is because System.nanoTime() is used for the session ID when creating the StreamInSession objects (driven from StorageService.requestRanges()) . 

From the documentation (http://docs.oracle.com/javase/6/docs/api/java/lang/System.html#nanoTime()) 

{quote}
This method provides nanosecond precision, but not necessarily nanosecond accuracy. No guarantees are made about how frequently values change. 
{quote}

Also some info here on clocks and timers https://blogs.oracle.com/dholmes/entry/inside_the_hotspot_vm_clocks

The hypervisor may be at fault here. But it seems like we cannot rely on successive calls to nanoTime() to return different values. 

To avoid message/interface changes on the StreamHeader it would be good to keep the session ID a long. The simplest approach may be to make successive calls to nanoTime until the result changes. We could fail if a certain number of milliseconds have passed. 

Hashing the file names and ranges is also a possibility, but more involved. 

(We may also want to drop latency times that are 0 nano seconds.)
","Ubuntu 10.04.2 LTS

java version ""1.6.0_24""
Java(TM) SE Runtime Environment (build 1.6.0_24-b07)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02, mixed mode)

""Bare metal"" servers from https://www.stormondemand.com/servers/baremetal.html 
The servers run on a custom hypervisor.
 ",,,,,,,,,,,,,,,,,,10/May/12 22:25;amorton;4223_counter_session_id-V2.diff;https://issues.apache.org/jira/secure/attachment/12526438/4223_counter_session_id-V2.diff,09/May/12 11:38;amorton;4223_counter_session_id.diff;https://issues.apache.org/jira/secure/attachment/12526146/4223_counter_session_id.diff,07/May/12 21:50;amorton;NanoTest.java;https://issues.apache.org/jira/secure/attachment/12525924/NanoTest.java,06/May/12 23:11;amorton;fmm streaming bug.txt;https://issues.apache.org/jira/secure/attachment/12525790/fmm+streaming+bug.txt,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-05-07 22:51:22.33,,,no_permission,,,,,,,,,,,,238328,,,Fri May 18 18:39:56 UTC 2012,,,,,,0|i0gtfb:,96197,yukim,yukim,,,,,,,,,,"07/May/12 21:45;amorton;Did some more testing. On the stormondemand.com machines it looks like nanoTime() is updated every 10ms. On EC2 nodes nanoTime() was always unique. The test script is attached, examples are below...

{code}
root@db6:~/aaron# java -classpath ./ NanoTest 100
nanoTime 460519702418991 occurred 5176 times
nanoTime 460519712419053 occurred 12090 times
nanoTime 460519722419115 occurred 22602 times
nanoTime 460519732419177 occurred 36154 times
nanoTime 460519742419239 occurred 36089 times
nanoTime 460519752419301 occurred 36866 times
nanoTime 460519762419363 occurred 36997 times
nanoTime 460519772419425 occurred 36763 times
nanoTime 460519782419487 occurred 36910 times
nanoTime 460519792419549 occurred 35481 times
Ran for 100 milliseconds, got 295128 duplicates and 11 uniques.
{code}

If it takes 10ms to get a unique value calling multiple times is out of the question. Will put thinking hat back on.",07/May/12 21:50;amorton;Test for unique nanoTime() results.,"07/May/12 22:51;jbellis;bq. The simplest approach may be to make successive calls to nanoTime until the result changes. We could fail if a certain number of milliseconds have passed. 

Hector has (had?) some code to make per-client unique timestamps by taking counter = max(currentTimeMillis, counter + 1).  I suppose you could do something similar here so you don't need to busywait.","07/May/12 23:11;amorton;yeah was thinking an incrementing counter.
Any concern about how unique the id's have to be ? 

I was thinking of creating a long out the (int) gossip generation for the local machine and an int counter that wraps around.   ","07/May/12 23:23;jbellis;That sounds reasonable.  They just need to be unique-per-host, not per-cluster.",08/May/12 15:33;jbellis;... actually just an AtomicLong counter should be fine.  If we have more than 2^63 streaming sessions I'll live w/ the overflow. :),"08/May/12 21:02;amorton;What about if a node received streaming requests from two other nodes? There would be an (small) chance of the nodes generating the same session ID.

Adding the gossip generation adds a little entropy to the id's. Happy to go with the simpler counter idea if you think this is a non-problem. ","08/May/12 21:19;jbellis;It doesn't need to be unique-per-cluster because StreamInSession tracks it as a Pair<InetAddress, Long>.","08/May/12 21:23;yukim;I see possible collision here, since StreamOutSession is identified by destination host + timestamp(for now) and StreamInSession in destination node uses it when received it from remote.
Adding one more key (source IP?) would make ID unique?","08/May/12 22:12;jbellis;Ugh, so the problem is that sometimes session IDs are generated by the target, and sometimes by the source?  That's broken...

I think there's several possible solutions:
# always generate session IDs on the source, so our Pair really is unique [the way I thought it worked :)]
# try to make the 64bit session IDs ""unique enough"" across the cluster [aaron's timestamp + counter]
# guarantee the session IDs are unique-per-host, and make the session context (source, id-generated-by-ip, id) instead of just (source, id) [yuki's suggestion]
# just switch to a UUID

#4 is probably simplest, but only #2 and #3 will be backwards-compatible.  Of those two I feel more confident about #3...  The only unique-id-in-64-bit schemas I know of require some coordination up front among participating nodes (e.g., http://engineering.twitter.com/2010/06/announcing-snowflake.html)",09/May/12 02:05;amorton;I'll take another look at how unique the session id need to be.,"09/May/12 11:38;amorton;Use an AtomicLong in StreamInSession and one in StreamOutSession for the session id. 

Sessions are always accessed using <inet_address, session_id>, and in and out session are in their own collections. ","09/May/12 16:37;yukim;After looking closer to the code, I can say that AtomicLong is enough here, as Aaron's patch suggests.
There is no need to generate cluster wide unique id, it just need to be unique between two nodes(source, dest).
I thought a pair of host and long value is shared among nodes but that's not true. My apologies.

So +1 to the patch attached.","09/May/12 16:47;jbellis;StreamIn.requestRanges will create a session ID using the *target* node's id generator (whatever it is) but in a Pair with the *source* node's IP.

Conversely, when a Stream is originated on the source node, it creates a session id using the *source* node's id generator, and sends that over in the stream header, where IncomingStreamReader picks it up and sticks it in the Session map.

So it looks to me like Yuki was right the first time ... ?","09/May/12 17:07;yukim;Name *source* is probably misleading here. I found that it's actually *source* of data you're requesting.
When node A initiates streaming to node B with StreamIn.requestRanges, it creates StreamInSession with pair of id, say, <B, 1> and sends request with session id 1.
Node B receives request and creates StreamOutSession from sender's ip and received session id 1, ends up having StreamOutSession of id <A, 1>.

||A                    ||                              ||B                    ||
|StreamInSession<B, 1> | \-\-(session ID of 1 from A)\-\-> |StreamOutSession<A, 1> |
|                      | <\-(session ID of 1 from B)\-\-- |                       |","09/May/12 17:33;jbellis;Right, so the problem is when A creates a {{<B, 1>}} Session in StreamIn, while B simultaneously creates a {{<B, 1>>}} for an unrelated StreamOut.transferRanges (move or unbootstrap).  ","09/May/12 22:27;amorton;Cassandra 4226

Agree. We could end up with the StreamOutSessions with the same <ip, id> context, one generated externally one internally. 

bq. 3. guarantee the session IDs are unique-per-host, and make the session context (source, id-generated-by-ip, id) instead of just (source, id) [yuki's suggestion]

how does ""id-generated-by-ip"" work ? We can only generate a streaming context from the StreamHeader or Stream Reply and sender ip address. Unfortunately these messages are not extensible. 

Another idea: make the session id [source_flag + local_int_counter]. Source flag is:

* 0 for a StreamInSession generated to request data.
*  1 for a StreamOutSession generated to transfer/push data.
* A StreamOutSession created in response to a stream request (still) uses the sessionID generated remotely.

One node may now have multiple stream out sessions to the same ip with the same session id:

IP | source_flag | session id
1.1.1.1 | 0 (started remotely) | 6
1.1.1.1 | 1 (started locally) | 6   ",10/May/12 16:49;yukim;Isn't it enough to just increment AtomicLong counter until there is no collision when creating locally?,"10/May/12 20:15;amorton;There are two path ways for a StreamOutSession to be added to the SOS.streams map:

1) Push / transfer. When node A wants to push data to node B using StreamOut.transferRanges(). In this case the SOS context using the target ip address - <node_b_ip, node_a_sos_counter>

2) Pull / request. When node B wants node A to send it data, using StreamIn.requestRanges(). Node B creates a StreamInSession that uses the target ip address - <node_a_ip, node_b_sis_counter>. When node A gets the stream request from node b it creates a StreamOutSession with context <node_b_ip, node_b_sis_counter>

So we can end up with these two sos contexts on node A:
* <node_b_ip, node_a_sos_counter>
* <node_b_ip, node_b_sis_counter>

We need to avoid collisions between node_a_sos_counter and node_b_sis_counter. It's an unlikely event but we need to be safe.

Adding the source_flag means we end up with this on node A:

* <node_b_ip, 1 ""push"", node_a_sos_counter>
* <node_b_ip, 0 ""pull"", node_b_sis_counter>


bq. Isn't it enough to just increment AtomicLong counter until there is no collision when creating locally?

Are you saying ""lock around creating the StreamOutSessions from both code paths and create session ID's that do not collide with known session ids ?""
","10/May/12 22:25;amorton;4223_counter_session_id-V2.diff 

Uses stream source flag as discussed. Added the flags to StreamHeader so they were together. 
","14/May/12 17:12;yukim;I'm fine with Aaron's approach. Session Id counter will overflow if > Integer.MAX_VALUE but I think thats enough for streaming. Regarding the patch, doing 0 << 32 for StreamOutSession feels redundant to me. I think it's better to just increment the counter inside StreamOutSession constructor.","14/May/12 22:34;amorton;bq.  Regarding the patch, doing 0 << 32 for StreamOutSession feels redundant to me. I think it's better to just increment the counter inside StreamOutSession constructor.

Always doing the shift makes it explicit that the session ID is a composite value constructed from two smaller values. And that StreamInSession and StreamOutSession have the same sessionID construction. Remember the StreamInSession ID becomes a StreamOutSession ID. 

For the same reasons I pulled the logic into nextSessionID(), it's easier to document and make explicit.

Can change if you think it's poor style.","15/May/12 15:51;yukim;OK, +1.",17/May/12 10:59;amorton;Yuki were you going to commit this or do you want me to?,"17/May/12 15:19;yukim;Aaron,

Go ahead. :)","18/May/12 18:39;amorton;committed to cassandra-1.0, cassandra-1.1 and trunk.

thanks. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow larger cache capacities than 2GB,CASSANDRA-4150,12550874,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,13/Apr/12 20:08,12/Mar/19 14:08,13/Mar/19 22:27,07/Jun/12 04:26,1.1.1,,,,,0,,,,,,,"The problem is that capacity is a Integer which can maximum hold 2 GB,
I will post a fix to CLHM in the mean time we might want to remove the maximumWeightedCapacity code path (atleast for Serializing cache) and implement it in our code.",,,,,,,,,,,,,,,,,,,16/May/12 01:59;vijay2win@yahoo.com;0001-CASSANDRA-4139-v2.patch;https://issues.apache.org/jira/secure/attachment/12527553/0001-CASSANDRA-4139-v2.patch,09/May/12 00:40;vijay2win@yahoo.com;0001-CASSANDRA-4150.patch;https://issues.apache.org/jira/secure/attachment/12526080/0001-CASSANDRA-4150.patch,09/May/12 00:40;vijay2win@yahoo.com;0002-Use-EntryWeigher-for-HeapCache.patch;https://issues.apache.org/jira/secure/attachment/12526081/0002-Use-EntryWeigher-for-HeapCache.patch,16/May/12 01:59;vijay2win@yahoo.com;0002-add-bytes-written-metric-v2.patch;https://issues.apache.org/jira/secure/attachment/12527554/0002-add-bytes-written-metric-v2.patch,09/May/12 00:40;vijay2win@yahoo.com;concurrentlinkedhashmap-lru-1.3.jar;https://issues.apache.org/jira/secure/attachment/12526079/concurrentlinkedhashmap-lru-1.3.jar,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-05-15 03:58:05.4,,,no_permission,,,,,,,,,,,,235738,,,Thu Dec 13 22:44:37 UTC 2012,,,,,,0|i0gskf:,96058,jbellis,jbellis,,,,,,,,,,13/Apr/12 20:30;vijay2win@yahoo.com;issue: http://code.google.com/p/concurrentlinkedhashmap/issues/detail?id=33 created.,"14/Apr/12 20:17;vijay2win@yahoo.com;For the records the comments from issue in CLHM library:
{quote}
I think a long capacity is fine, but I'm not actively working on a next release to roll this into soon. If this is critical than it could be a patch release. You are of course welcome to fork if neither of those options are okay.

I helped my former colleagues at Google with Guava's CacheBuilder (formerly MapMaker), which could be considered the successor to this project. There the maximum weight is a long.
{quote}

IRC: Guava doesnt support descendingKeySetWithLimit
Possibly fork the CLHM code into Cassandra code base or drop the hotkey's method and use guava (Thats the only limitation which i see for now).","09/May/12 00:40;vijay2win@yahoo.com;CLHM (Ben) fixed the issue:

{quote}
Fixed in v1.3. I plan on releasing this tonight.

Also introduced EntryWeigher<K, V> to allow key/value weighing. We fixed this oversight in Guava's CacheBuilder from the get-go. 

I believe Cassandra wanted entry weighers too, but it wasn't high priority (no bug filed). Please consider adopting it when you upgrade the library.
{quote}","15/May/12 03:58;jbellis;Using MemoryMeter on a hot path like cache updates scares me a bit -- it's pretty slow.  Might want to switch to using the dataSize * liveRatio measurement that memtables use.

Aside #1: So basically current 1.1 CLHM is totally broken since it has a capacity of X MB but weighs each item as one byte?

Aside #2: I don't see any use of IRCP with useMemoryWeigher=false, looks like we can remove that parameter","15/May/12 04:00;jbellis;Aside #3: would prefer to assert value.size() < Integer.MAX_VALUE in SC.Weigher, instead of having a Math.min call in there.  Strongly doubt the rest of the code would support serialized values larger than that anyway.","15/May/12 04:06;jbellis;bq. So basically current 1.1 CLHM is totally broken since it has a capacity of X MB but weighs each item as one byte?

(If that's the case, we should target this for 1.1.1.)","15/May/12 04:41;vijay2win@yahoo.com;>>> Using MemoryMeter on a hot path like cache updates scares me a bit – it's pretty slow. Might want to switch to using the dataSize * liveRatio measurement that memtables use.
Will do.

>>> So basically current 1.1 CLHM is totally broken since it has a capacity of X MB but weighs each item as one byte?
No we assume that the KeyCache to be 48 byte long average size (CacheService.AVERAGE_KEY_CACHE_ROW_SIZE)... 
basically, cache capacity = keyCacheInMemoryCapacity / AVERAGE_KEY_CACHE_ROW_SIZE
hence each entry is considered to be 1 byte/one entry. (Kind of hackie may be we didn't have EntryWeigher i guess, now that we have it we can get rid of that logic).

>>> I don't see any use of IRCP with useMemoryWeigher=false, looks like we can remove that parameter
We still need it if we dont have Jamm Enabled.

>>> would prefer to assert value.size() < Integer.MAX_VALUE in SC.Weigher
Will do, Plz let me know if everything else is fine...

>>>If that's the case, we should target this for 1.1.1
Either ways we have to target for 1.1.1 because the serialized cache is broken :( 
","16/May/12 01:59;vijay2win@yahoo.com;The problem with the liveRatio is that it is variable and we might have a leak if we use the ratio... 
- Lets say when we add the entry we have a ratio of 1.2 and when the entry is removed we will have the ratio of 1.1 then we will leak some space for cache utilization. So it will be better if this number doesn't change.

0001 removes memory meter and does changes to fix the issue.
0002 adds entry weight and small refactor to change the Provider interface as suggested.

Thanks!","16/May/12 16:55;jbellis;{code}
.       EntryWeigher<KeyCacheKey, RowIndexEntry> weigher = new EntryWeigher<KeyCacheKey, RowIndexEntry>()
        {
            public int weightOf(KeyCacheKey key, RowIndexEntry value)
            {
                return key.serializedSize() + value.serializedSize();
            }
        };
        ICache<KeyCacheKey, RowIndexEntry> kc = ConcurrentLinkedHashCache.create(keyCacheInMemoryCapacity, weigher);
{code}

Using the serialized size for a non-serialized cache looks fishy to me.","21/May/12 21:02;vijay2win@yahoo.com;Partially committed 0001 d to 1.1.1 and trunk, which will remove the limitation of 2 GB which this ticket was originally ment to do.
for 0002 i am working on calculating the object overhead + size without using reflection. Thanks!","23/May/12 17:40;vijay2win@yahoo.com;Before i go ahead with the complicated implementation (touching most of the cache values), i did a smoke test and thought of sharing the results.

lgmac-vparthsarathy:cass vparthasarathy$ java -javaagent:/Users/vparthasarathy/Documents/workspace/cassandraT11/lib/jamm-0.2.5.jar -jar ~/Desktop/TestJamm.jar 100000000
Using reflection took: 25954
Using NativeCalculation took: 178
Using MemoryMeter took: 992
lgmac-vparthsarathy:cass vparthasarathy$ 

I used  https://github.com/twitter/commons/blob/master/src/java/com/twitter/common/objectsize/ObjectSizeCalculator.java for reflection test.","06/Jun/12 22:43;jbellis;bq. Lets say when we add the entry we have a ratio of 1.2 and when the entry is removed we will have the ratio of 1.1 then we will leak some space for cache utilization. So it will be better if this number doesn't change

I did some code diving into CLHM -- it looks like it annotates the cache entry with the weight-at-put-time, and uses that in remove or replace.  If so, I think MemoryMeter might be fine after all.

OTOH if the cache isn't churning maybe MemoryMeter is just fine the way it is.

I'm inclined to close this and just open a new ticket for updating to CLHM 1.3 in 1.2 and including keys in our weights.  Thoughts?","07/Jun/12 04:26;vijay2win@yahoo.com;{quote}
just open a new ticket for updating to CLHM 1.3 in 1.2 and including keys in our weights. Thoughts?
{quote}
Done! opened CASSANDRA-4315... Thanks!","07/Jun/12 12:36;cburroughs;I'm having trouble following what happened from this ticket and 4315 (""just open a new ticket for updating to CLHM 1.3 in 1.2... "").  Isn't 02672936f635c93e84ed6625bb994e1628da5a9b in the 1.1 branch and we already upgraded to CLHM 1.3?

 https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=02672936f635c93e84ed6625bb994e1628da5a9b","07/Jun/12 16:14;vijay2win@yahoo.com;Yes we did upgrade it to CLHM 1.3 in both Cassandra 1.1 and Cassandra 1.2 to fix the original issue mentioned in the ticket.

Ben (author of CLHM) also added EntryWeight<K, V> to CLHM 1.3 on our request. Which is 0002 part of the patch originally submitted. 
To use EntryWeights we needed to calculate the memory size's (We where trying out multiple options using MemoryMeter or using other mechanisms) of the Key and Value, we where trying that in this ticket.

Hence CASSANDRA-4315 was opened to continue the work of updating CLHM Weigher<V> to CLHM EntryWeight<K, V>.","12/Jun/12 16:18;ssmith;It looks like the Maven dependencies in build.xml weren't updated to reference CLHM 1.3:

{noformat}
...
<dependency groupId=""com.googlecode.concurrentlinkedhashmap"" artifactId=""concurrentlinkedhashmap-lru"" version=""1.2""/>
...
{noformat}

As a result, the [Cassandra Maven Plugin|http://mojo.codehaus.org/cassandra-maven-plugin/] blows up if you try to make it use Cassandra 1.1.1.

{noformat}
...
[ERROR] 10:59:07,698 Exception encountered during startup
[INFO] java.lang.NoSuchMethodError: com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Builder.maximumWeightedCapacity(J)Lcom/googlecode/concurrentlinkedhashmap/ConcurrentLinkedHashMap$Builder;
[INFO]     at org.apache.cassandra.cache.ConcurrentLinkedHashCache.create(ConcurrentLinkedHashCache.java:70)
[INFO]     at org.apache.cassandra.cache.ConcurrentLinkedHashCache.create(ConcurrentLinkedHashCache.java:70)
...
{noformat}",12/Jun/12 18:41;vijay2win@yahoo.com;Thanks! committed to 1.1 and trunk.,13/Dec/12 21:16;tooda01;The Maven plugin issue still exists with Cassandra 1.1.5,13/Dec/12 22:44;jbellis;Feel free to attach a patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage allow_deletes doesn't work in Hadoop cluster,CASSANDRA-4499,12602099,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mrtidy,mrtidy,07/Aug/12 23:21,12/Mar/19 14:08,13/Mar/19 22:27,21/Sep/12 20:15,1.1.6,,,,,0,hadoop,pig,,,,,"When using CassandraStorage in a Pig script that runs in a Hadoop cluster, the environment variable configuration option for allow_deletes doesn't work.  I'd like to see allow_deletes added as a URL parameter.

For example, I'd like to do
STORE storable_events INTO 'cassandra://drlcrs/event?allow_deletes=true' USING org.apache.cassandra.hadoop.pig.CassandraStorage();",,,,,,,,,,,,,,,,,,,19/Sep/12 12:15;brandon.williams;4499-v2.txt;https://issues.apache.org/jira/secure/attachment/12545717/4499-v2.txt,07/Aug/12 23:38;mrtidy;trunk-4499.txt;https://issues.apache.org/jira/secure/attachment/12539741/trunk-4499.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-07 23:54:46.735,,,no_permission,,,,,,,,,,,,256202,,,Fri Sep 21 20:15:47 UTC 2012,,,,,,0|i0gwhr:,96694,xedin,xedin,,,,,,,,,,07/Aug/12 23:38;mrtidy;No unit tests available yet - I'm not sure how to test the URL or failure cases by looking at the existing tests.  I'll have to dig a little more.,07/Aug/12 23:46;mrtidy;I've submitted trunk-4499.txt which is missing unit tests but is what I'm looking for so that allow_deletes can be set without using environment variables.,"07/Aug/12 23:54;brandon.williams;bq. No unit tests available yet - I'm not sure how to test the URL or failure cases by looking at the existing tests. I'll have to dig a little more.

We should be able to just insert empty tuples in the tests in examples/pig.

While we're at it, maybe we should move the wide row flag to the url too.",19/Sep/12 12:15;brandon.williams;v2 also allows setting widerow and secondary index mode with url params.,21/Sep/12 10:58;xedin;+1,21/Sep/12 20:15;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support indexes on composite column components (clustered columns),CASSANDRA-5125,12626383,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,07/Jan/13 18:33,12/Mar/19 14:08,13/Mar/19 22:27,04/Apr/13 16:33,2.0 beta 1,,,,,2,,,,,,,"Given

{code}
CREATE TABLE foo (
  a int,
  b int,
  c int,
  PRIMARY KEY (a, b)
);
{code}

We should support {{CREATE INDEX ON foo(b)}}.",,,,,,,,,,,,,,CASSANDRA-5534,CASSANDRA-4511,,,,02/Apr/13 17:42;slebresne;0001-Refactor-aliases-into-column_metadata.txt;https://issues.apache.org/jira/secure/attachment/12576611/0001-Refactor-aliases-into-column_metadata.txt,02/Apr/13 17:42;slebresne;0002-Generalize-CompositeIndex-for-all-column-type.txt;https://issues.apache.org/jira/secure/attachment/12576612/0002-Generalize-CompositeIndex-for-all-column-type.txt,02/Apr/13 17:42;slebresne;0003-Handle-new-type-of-IndexExpression.txt;https://issues.apache.org/jira/secure/attachment/12576613/0003-Handle-new-type-of-IndexExpression.txt,02/Apr/13 17:42;slebresne;0004-Handle-partition-key-indexing.txt;https://issues.apache.org/jira/secure/attachment/12576614/0004-Handle-partition-key-indexing.txt,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-02-18 05:25:01.462,,,no_permission,,,,,,,,,,,,302975,,,Tue Oct 21 11:07:08 UTC 2014,,,,,,0|i176wn:,250087,iamaleksey,iamaleksey,,,,,,,,,,"18/Feb/13 05:25;slebresne;Attaching patches for that (also pushed to https://github.com/pcmanus/cassandra/commits/5125).

The first part of this ticket is about how we store the information that a clustering key column is indexed. Turns out that for ""regular"" columns we use ColumnDefinition and the indexing code also assumes that, so the probably best and simplest approach is to reuse ColumnDefinition for that too.  But then it's easier to always store all primary key columns as ColumnDefinition, pretty much obsoleting the old key_aliases and column_aliases. There is a few related details worth noticing:
# while this obsolete the aliases, those are not removed of the schema by the patch for compatibility sake. Truth is, I'm not sure there is a way to remove a field from the schema without breaking rolling upgrades at this point.
# after this patch, CFDefinition becomes much less useful as CFMetadata + ColumnDefinition holds pretty much the same information in pretty much the same form. So we could slightly simplify things by removing CFDefinition.  However, this is left to later (this won't be a 3 lines patch).

After that, the patch adds a new type of composite indexes to handle indexing clustering keys (which share most code with the existing regular composite index) and update CQL3 to allow adding and querying the new indexes (in particular, it is slighty tricky in SelectStatment to recognize when a clustering key is restricted if 2ndary indexes should be used or not).

The last patch adds support for indexing components of the partition key (we don't allow indexing the first component of the partition key as it makes no sense (it's already primary indexed), but if the partition key is composite, secondary indexing the 2+ parts can be useful).

Lastly, I'll note that the patches only add theses news indexes for non compact tables. We should generalize to compact tables too, but that would require a bit of generalization that I'd rather add in a second phase.
",02/Apr/13 17:42;slebresne;Rebased patches attached.,"03/Apr/13 08:31;slebresne;Things move fast on trunk lately, so I've pushed a rebased version at https://github.com/pcmanus/cassandra/commits/5125-2 to avoid rebasing every day.","04/Apr/13 04:16;carlyeks;A few errors when running the test suite:

Testcase: testCli(org.apache.cassandra.cli.CliTest):	Caused an ERROR
java.lang.RuntimeException: org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 4
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1533)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 4
	at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:69)
	at org.apache.cassandra.db.index.AbstractSimplePerColumnSecondaryIndex.insert(AbstractSimplePerColumnSecondaryIndex.java:121)
	at org.apache.cassandra.db.index.SecondaryIndexManager$PerColumnIndexUpdater.update(SecondaryIndexManager.java:623)
	at org.apache.cassandra.db.AtomicSortedColumns$Holder.addColumn(AtomicSortedColumns.java:313)
	at org.apache.cassandra.db.AtomicSortedColumns.addAllWithSizeDelta(AtomicSortedColumns.java:168)
	at org.apache.cassandra.db.Memtable.resolve(Memtable.java:253)
	at org.apache.cassandra.db.Memtable.put(Memtable.java:169)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:852)
	at org.apache.cassandra.db.Table.apply(Table.java:379)
	at org.apache.cassandra.db.Table.apply(Table.java:342)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:189)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:667)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1529)
	... 3 more
	
Testcase: testIndexDeletions(org.apache.cassandra.db.ColumnFamilyStoreTest):	Caused an ERROR
A long is exactly 8 bytes: 4
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 4
	at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:69)
	at org.apache.cassandra.db.index.AbstractSimplePerColumnSecondaryIndex.insert(AbstractSimplePerColumnSecondaryIndex.java:121)
	at org.apache.cassandra.db.index.SecondaryIndexManager$PerColumnIndexUpdater.update(SecondaryIndexManager.java:623)
	at org.apache.cassandra.db.AtomicSortedColumns$Holder.addColumn(AtomicSortedColumns.java:313)
	at org.apache.cassandra.db.AtomicSortedColumns.addAllWithSizeDelta(AtomicSortedColumns.java:168)
	at org.apache.cassandra.db.Memtable.resolve(Memtable.java:253)
	at org.apache.cassandra.db.Memtable.put(Memtable.java:169)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:852)
	at org.apache.cassandra.db.Table.apply(Table.java:379)
	at org.apache.cassandra.db.Table.apply(Table.java:342)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:189)
	at org.apache.cassandra.db.ColumnFamilyStoreTest.testIndexDeletions(ColumnFamilyStoreTest.java:301)


Testcase: testIndexUpdate(org.apache.cassandra.db.ColumnFamilyStoreTest):	Caused an ERROR
Index: 0, Size: 0
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.db.ColumnFamilyStoreTest.testIndexUpdate(ColumnFamilyStoreTest.java:398)",04/Apr/13 07:10;slebresne;My bad. That was due to a rebase typo that I had fixed in my CASSANDRA-5417 branch but not on that one. I've push the fix to the same github branch than above (https://github.com/pcmanus/cassandra/commits/5125-2).,04/Apr/13 09:35;iamaleksey;+1,"04/Apr/13 16:33;slebresne;Committed, thanks","20/Oct/14 20:45;denis.angilella;??Lastly, I'll note that the patches only add theses news indexes for non compact tables. We should generalize to compact tables too, but that would require a bit of generalization that I'd rather add in a second phase.??

With 2.1 and *compact* tables it is possible to CREATE INDEX on composite primary key columns, but queries returns no results for the tests below.
Adding this comment for now, can open a new ticket if you prefer.

{code:SQL}
CREATE TABLE users2 (
   userID uuid,
   fname text,
   zip int,
   state text,
  PRIMARY KEY ((userID, fname))
) WITH COMPACT STORAGE;

CREATE INDEX ON users2 (userID);
CREATE INDEX ON users2 (fname);

INSERT INTO users2 (userID, fname, zip, state) VALUES (b3e3bc33-b237-4b55-9337-3d41de9a5649, 'John', 10007, 'NY');

// the following queries returns 0 rows, instead of 1 expected
SELECT * FROM users2 WHERE fname='John'; 
SELECT * FROM users2 WHERE userid=b3e3bc33-b237-4b55-9337-3d41de9a5649;
SELECT * FROM users2 WHERE userid=b3e3bc33-b237-4b55-9337-3d41de9a5649 AND fname='John';

// dropping 2ndary indexes restore normal behavior
{code}","21/Oct/14 10:18;slebresne;[~denis.angilella] Correct, the validation during index creation is broken. Do you mind creating a ticket indeed so we track the fix?",21/Oct/14 11:07;denis.angilella;[~slebresne]: I created CASSANDRA-8156 to track the fix.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
isLocalDc() in OutboundTcpConnection class retrieves local IP in wrong way,CASSANDRA-5299,12634619,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,michalm,michalm,28/Feb/13 14:49,12/Mar/19 14:08,13/Mar/19 22:27,01/Mar/13 01:30,1.2.3,,,,,0,,,,,,,"My question from dev mailing list:

bq. Can someone explain me why isLocalDc() in OutboundTcpConnection class uses DatabaseDescriptor.getRpcAddress() for retrieving ""local"" IP, instead of using DD.getListenAddress() or - even better - FBUtilities.getLocalAddress()? I mean - I don't get why RPC address is checked before initializing internode (so not RPC-based)  communication, which will use IP address returned by (in OTCPool) FBUtilities.getLocalAddress()?

And response by Marcus Eriksson:

bq. That should probably be FBUtilities.getBroadCastAddress even, could you file a ticket?
",,,,,,,,,,,,,,,,,,,28/Feb/13 16:11;krummas;0001-CASSANDRA-5299-use-broadCastAddress-when-figuring-ou.patch;https://issues.apache.org/jira/secure/attachment/12571424/0001-CASSANDRA-5299-use-broadCastAddress-when-figuring-ou.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-28 16:11:22.62,,,no_permission,,,,,,,,,,,,315112,,,Fri Mar 01 01:30:46 UTC 2013,,,,,,0|i1idk7:,315456,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,28/Feb/13 16:11;krummas;use broadCastAddress when deciding local datacenter,28/Feb/13 19:21;vijay2win@yahoo.com;Oops +1,"01/Mar/13 01:30;vijay2win@yahoo.com;Committed to 1.2 and trunk, Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"hsha default thread limits make no sense, and yaml comments look confused",CASSANDRA-4277,12557047,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,23/May/12 18:38,12/Mar/19 14:07,13/Mar/19 22:27,29/May/12 16:27,1.2.0 beta 1,,,,,0,,,,,,,"The cassandra.yaml states with respect to {{rpc_max_threads}}:

{code}
# For the Hsha server, the min and max both default to quadruple the number of
# CPU cores.
{code}

The code seems to indeed do this. But this makes, as far as I can tell, no sense what-so-ever since the number of concurrent RPC threads you need is a function of the throughput and the average latency of requests (that includes synchronously waiting on network traffic).

Defaulting to anything having to do with CPU cores seems inherently wrong. If a default is non-static, a closer guess might be to look at thread stack size and heap size and infer what ""might"" be reasonable.

*NOTE*: The effect of having this too low, is ""strange"" (if you don't know what's going on) latencies observed form the client on all thrift requests (*any* thrift request, including e.g. {{describe_ring()}}), that isn't visible in any latency metric exposed by Cassandra. This is why I consider this ""major"", since unwitting users may be seeing detrimental performance for no good reason.

In addition, I read this about async:

{code}
# async -> Nonblocking server implementation with one thread to serve 
#          rpc connections.  This is not recommended for high throughput use
#          cases. Async has been tested to be about 50% slower than sync
#          or hsha and is deprecated: it will be removed in the next major release.
{code}

This makes even less sense. Running with *one* rpc thread limits you to a single concurrent request. How was that 50% number even attained? By single-node testing being completely CPU bound locally on a node? The actual effect should be ""stupidly slow"" in any real situation with lots of requests on a cluster of many nodes and network traffic (though I didn't test that) - especially in the event of any kind of hiccup like a node doing GC. I agree that if the above is true, async should *definitely* be deprecated, but the reasons seem *much* stronger than implied.

I may be missing something here, in which case I apologize,, but I specifically double-checked after I fixed this setting on on our our clusters after seeing exactly the expected side-effect of having it be too low. I always was under the impression that rpc_max_threads affects the number of RPC requests running concurrently, and code inspection (it being used for the worker thread limit) + the effects of client-observed latency is consistent with my understanding.

I suspect the setting was set strangely by someone because the phrasing of the comments in {{cassandra.yaml}} strongly suggest that this should be tied to CPU cores, hiding the fact that this really has to do with the number of requests that can be serviced concurrently regardless of implementation details of thrift/networking being sync/async/etc.

",,,,,,,,,,,,,,,,,,,25/May/12 19:39;scode;CASSANDRA-4277-trunk-v2.txt;https://issues.apache.org/jira/secure/attachment/12529776/CASSANDRA-4277-trunk-v2.txt,25/May/12 07:07;scode;CASSANDRA-4277-trunk.txt;https://issues.apache.org/jira/secure/attachment/12529684/CASSANDRA-4277-trunk.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-23 19:55:02.592,,,no_permission,,,,,,,,,,,,256019,,,Mon Jul 02 10:45:24 UTC 2012,,,,,,0|i0gu27:,96300,slebresne,slebresne,,,,,,,,,,"23/May/12 18:40;scode;To be clear: I suggest changing the default to some pretty high number like 512, or possibly be a function of stack size/heap size.

Additionally, *if* my analysis is correct, the 'async' stuff should be completely deprecated as no one should ever be using it for any real cluster unless possibly if they use Cassandra as single-node local storage.

I also suggest changing comments to reflect the real meaning of this and what it affects.

I will provide a patch for the comments and changing the HSHA defaults to a fixed number, if anyone +1:s the idea.","23/May/12 19:55;vijay2win@yahoo.com;Peter,

when implemented the solution was intrim (till thrift fix was in a released. THRIFT-1167), I dont remember or have the test classes which i used to come up with the data mentioned above... 
It is really hard to get all the real world traffic pattern to simulate... 

One thing which we can do have it configurable and by default (assuming that the clients behave well and keep the CPU busy) choose based on the CPU, i dont think there is a right number of threads which will make sense for the selector to be default.","23/May/12 19:59;vijay2win@yahoo.com;Also, IMO
{quote}
To be clear: I suggest changing the default to some pretty high number like 512, or possibly be a function of stack size/heap size.
{quote}
512 is really really high and the context switch will be really high and you will not be seeing a lot of advantage with this.","23/May/12 20:38;brandon.williams;bq. 512 is really really high and the context switch will be really high and you will not be seeing a lot of advantage with this.

I agree.  Logically the optimal amount is the number of cores, but benchmarking revealed better results with oversubscription, hence it defaulting to cores*4 now.","23/May/12 21:49;scode;Please re-read. What you're saying is re-iterating the same confusion that seems to be the reason for this setting to begin with.

It has *nothing* to do with CPU cores. This is about concurrent requests, that are not CPU bound.

Logically the optimal amount *IS NOT* the number of cores. See my original submission.","23/May/12 21:53;scode;The point is, zoom out of all implementation details. Forget about context switching. Just look at requests. If you have 10 000 requests with an average latency of 15 milliseconds for example, it would take 10000*15/1000=150 seconds to execute them with a single rpc thread. With 10 threads, 15 seconds.

So if you're trying to push 10 000 requests per second through a node, with an average latency of 15 milliseconds, you cannot possible do so if you limit the number of concurrent requests to less than 15*10=150.","23/May/12 21:54;scode;This is a result of the architecture of Cassandra, which fundamentally requires a thread for an active thrift request. Fixing that would mean making the entire thrift front-end asynchronous. But without that happening, it is not correct to consider the number of CPU cores when selecting what is a reasonable limit to the number of concurrent thrift requests.","23/May/12 21:59;jbellis;bq. This is a result of the architecture of Cassandra, which fundamentally requires a thread for an active thrift request

Isn't the whole point of HSHA to fix this?","23/May/12 22:05;scode;Not unless there's an entire re-implementation of the StorageProxy interface to be asynchronous that exists in parallel with the synchronous one that I have somehow missed, or we've done some bytecode weaving tricks to create continuations on the JVM... Just look at {{fetchRows}} for example. Unless I've gone completely senile, it's still standard synchronous code that blocks on futures and finally returns.","23/May/12 22:06;scode;The point of the HSHA as I have been treating it, is to be asynch on the thrift protocol side. In other words, instead of thread-per-*connection* we have thread-per-*active-request*, which is much much better.

I have taken it to mean that half (the front-end/connection i/o stuff) being asynch, and the other half (the implementation of the RPC calls) being synchronous.","24/May/12 04:11;vijay2win@yahoo.com;Peter, i dont understand how will latency come into picture? Selectors are woken when the data is available, right? if for some reason your connection is taking 15 ms or what even in the middle of a read you are better off disconnecting and reconnecting... I still dont understand how 500 threads will help it will all hang right?

Basically these threads (4*CPU core's) are used for selection read/write (only during that time) and the TP executes it and the selector is woken up again when the data has to be written. Are we having the same conversation as in CASSANDRA-3590 (but this is within the DC's where latencies are really low)?","24/May/12 05:44;scode;The RPC calls, I'm talking about the actual Java methods that correspond to names of methods in the thrift spec, run in threads. Both reads and writes run as such calls. The thread will be ""hogged"" for that request until the call returns. This is just standard Java, nothing having to do with thrift or selector threads. Whatever thrift is doing, at some point it's handing off the servicing of an RPC call to a Java method call.

In order to avoid having a thread per request, you would have to actually re-write the relevant code to be asynchronous (keep necessary state explicitly instead of implicitly on the stack, make it up-side-down at each block point, no longer have working exception handling across block points, etc) - that is, barring generalized things like JVM coroutines/bytecode weaving.

The ""necessary code"" here being things like fetchRows() for example, and everything else involving servicing thrift requests.
","24/May/12 05:46;scode;And latency directly translates into the amount of time a thread is ""hogged"" for the purpose of a request. Thus, it directly affects concurrency.","24/May/12 05:49;scode;If you want an analogy, just compare with any standard LAMP server where you have some PHP script blocking on remote network calls. In the same way as the apache admin would have to tweak max clients to get throughput and saturate resources, we have to have enough RPC worker threads in Cassandra.","24/May/12 06:06;vijay2win@yahoo.com;haaa you are talking about 

{code}
            if (conf.rpc_min_threads == null)
                conf.rpc_min_threads = conf.rpc_server_type.toLowerCase().equals(""hsha"")
                                     ? Runtime.getRuntime().availableProcessors() * 4
                                     : 16;
            if (conf.rpc_max_threads == null)
                conf.rpc_max_threads = conf.rpc_server_type.toLowerCase().equals(""hsha"")
                                     ? Runtime.getRuntime().availableProcessors() * 4
                                     : Integer.MAX_VALUE;
{code}

I was talking about 

CassandraDaemon

{code}
serverEngine = new CustomTHsHaServer(serverArgs, executorService, Runtime.getRuntime().availableProcessors());
{code}

In our environment we run @ rpc_min_threads: 16 rpc_max_threads:1024 and has worked well for us :)

My bad for misreading the ticket.

{quote}
This is a result of the architecture of Cassandra, which fundamentally requires a thread for an active thrift request.
{quote}
This actually made me think you are really talking about CustomTHSHA server... because the whole point of THSHA server is to eliminate that as jonathan mentioned. anyways...","24/May/12 06:23;scode;Yes, I'm talking about {{rpc_max_threads}}.

{quote}
This actually made me think you are really talking about CustomTHSHA server... because the whole point of THSHA server is to eliminate that as jonathan mentioned. anyways...
{quote}

I don't understand why this is still a point of contention though. There's still no code in Cassandra that in any way avoids having a thread per active (client) request. By ""active"" here I mean ""getting processed by Cassandra"" (so I'm not talking about requests that are being read from the client TCP connection, or are queueing to get executed on a worker thread).

We no longer requires a thread *per client connection*, which is good. But we still have a thread *per client request*.
","24/May/12 16:19;slebresne;I agree with Peter, StorageProxy being synchronous, we do need on thread per active request (this is ""annoying"" for CASSANDRA-2478 too). It would be neat to make StorageProxy asynchronous but that's likely very much non-trivial. So on the thread numbers, I also agree that some big number would be much better. Those threads will mostly spend time waiting, so I don't think the context switching will kill us anyway.
","25/May/12 07:07;scode;Attaching suggested patch against trunk.

Since the pre-existing comments claim async will be removed in the next major release, I removed it entirely from comments (but not the code).

I re-phrased some of the stuff and added an attempted explanation for the user as to how to figure out what limit to set. As usual I think I may be too verbose; maybe it's better to just refer to a separate wiki page than to try to explain inline?

As an aside, I'd favor making hsha the default despite it being slower on Windows, though that's a concern not within the scope of this ticket. I didn't make that change in the patch.","25/May/12 09:54;slebresne;I do think it's maybe a bit too much info for a config file :). I'd remove the two last paragraphs and maybe rewrite the second one with something along the lines of: ""The default is unlimited and thus provide no protection against clients overwhelming the server. You are encouraged to set a maximum that makes sense for you in production, but do keep in mind that rpc_max_threads represents the maximum number of client requests this server may execute concurrently."".

Also, there's a typo: ""Regardless of your *cohice*"". And for ""# rpc_max_threads: (unlimited)"", it suggests that '(unlimited)' is a valid value which is not the case. I'd prefer leaving the 2048 value, it does not have to represent the actual default (it doesn't always for other configs).
",25/May/12 19:39;scode;Attached v2 that incorporates those changes.,"29/May/12 16:27;slebresne;+1, committed, thanks.",01/Jul/12 05:57;jeromatron;Would a StorageProxy rewrite to be completely asynchronous be a separate ticket to be tackled later?,01/Jul/12 05:59;jeromatron;Sounds from Sylvain that a SP asynch rewrite would be good for implementing CASSANDRA-2478 as well.,"02/Jul/12 07:39;slebresne;bq. Would a StorageProxy rewrite to be completely asynchronous be a separate ticket to be tackled later?

Probably a separate ticket to be tackled never :) I mean, it would be neat intellectually speaking to get rid of the one-thread-per-active-request thing, but it's unclear it is a real problem in practice today and that would require a very substantial effort. Not sure at all it is worth the trouble. ",02/Jul/12 10:45;jeromatron;Thanks for the clarification Sylvain.  I was just going through some past tickets and didn't want something that sounded like a good opportunity for the future fall through the cracks.  That's all.  Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deleting a CF always produces an error and that CF remains in an unknown state,CASSANDRA-4230,12554460,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,edevil,edevil,09/May/12 10:28,12/Mar/19 14:07,13/Mar/19 22:27,15/May/12 17:10,1.1.1,,,,,0,,,,,,,"From the CLI perspective:

[default@Disco] drop column family client; 
null
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_system_drop_column_family(Cassandra.java:1222)
	at org.apache.cassandra.thrift.Cassandra$Client.system_drop_column_family(Cassandra.java:1209)
	at org.apache.cassandra.cli.CliClient.executeDelColumnFamily(CliClient.java:1301)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:234)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)

Log:

 INFO [MigrationStage:1] 2012-05-09 11:25:35,686 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,687 Memtable.java (line 266) Writing Memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,748 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-34-Data.db (1041 bytes)
 INFO [MigrationStage:1] 2012-05-09 11:25:35,749 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,750 Memtable.java (line 266) Writing Memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,812 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-Data.db (649 bytes)
 INFO [CompactionExecutor:20] 2012-05-09 11:25:35,814 CompactionTask.java (line 114) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-27-Data.db'), SSTableReader
(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-25-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-26-Data.db'), SSTableReader(path
='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-Data.db')]
 INFO [MigrationStage:1] 2012-05-09 11:25:35,918 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-Client@864320066(372/465 serialized/live bytes, 6 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,919 Memtable.java (line 266) Writing Memtable-Client@864320066(372/465 serialized/live bytes, 6 ops)
 INFO [CompactionExecutor:20] 2012-05-09 11:25:35,945 CompactionTask.java (line 225) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-29-Data.db,].  22,486 to 20,621 (~91% of orig
inal) bytes for 2 keys at 0.150120MB/s.  Time: 131ms.
 INFO [FlushWriter:3] 2012-05-09 11:25:36,013 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/Disco/Client/Disco-Client-hc-5-Data.db (407 bytes)
ERROR [MigrationStage:1] 2012-05-09 11:25:36,043 CLibrary.java (line 158) Unable to create hard link
com.sun.jna.LastErrorException: errno was 17
        at org.apache.cassandra.utils.CLibrary.link(Native Method)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:150)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

ERROR [Thrift:3] 2012-05-09 11:25:36,048 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:372)
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:191)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyDrop(MigrationManager.java:182)
        at org.apache.cassandra.thrift.CassandraServer.system_drop_column_family(CassandraServer.java:948)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3348)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3336)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:368)
        ... 11 more
Caused by: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        ... 10 more
ERROR [MigrationStage:1] 2012-05-09 11:25:36,051 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)

        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        ... 10 more
 INFO [CompactionExecutor:22] 2012-05-09 11:25:36,052 CompactionTask.java (line 114) Compacting [SSTableReader(path='/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-5-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-4-Data.db')]
 INFO [CompactionExecutor:22] 2012-05-09 11:25:36,187 CompactionTask.java (line 225) Compacted to [/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-6-Data.db,].  728 to 458 (~62% of original) bytes for 8 keys at 0.003235MB/s.  Time: 135ms.

Schema:

CREATE COLUMN FAMILY Client WITH
       key_validation_class = UUIDType AND
       comparator = UTF8Type AND
       column_metadata = [ { column_name: key, validation_class: BytesType }
                           { column_name: name, validation_class: UTF8Type }
                           { column_name: userid, validation_class: UUIDType, index_type: KEYS }
                         ] AND
       compression_options = { sstable_compression:SnappyCompressor, chunk_length_kb:64 } AND
       compaction_strategy = LeveledCompactionStrategy AND
       compaction_strategy_options = { sstable_size_in_mb: 10 } AND
       gc_grace = 432000;

State of data dir after deletion attempt:

# ls -lah /var/lib/cassandra/data/Disco/Client/ 
total 76K
drwxr-xr-x  3 cassandra cassandra 4.0K May  9 11:25 .
drwxr-xr-x 17 cassandra cassandra 4.0K May  3 12:34 ..
-rw-r--r--  2 cassandra cassandra  420 May  9 11:25 Client-old.json
-rw-r--r--  1 cassandra cassandra  418 May  7 18:04 Client.Client_userid_idx-old.json
-rw-r--r--  1 cassandra cassandra  418 May  7 18:04 Client.Client_userid_idx.json
-rw-r--r--  1 cassandra cassandra  418 May  9 11:25 Client.json
-rw-r--r--  1 cassandra cassandra   46 May  9 11:25 Disco-Client-hc-6-CompressionInfo.db
-rw-r--r--  1 cassandra cassandra  458 May  9 11:25 Disco-Client-hc-6-Data.db
-rw-r--r--  1 cassandra cassandra  976 May  9 11:25 Disco-Client-hc-6-Filter.db
-rw-r--r--  1 cassandra cassandra  208 May  9 11:25 Disco-Client-hc-6-Index.db
-rw-r--r--  1 cassandra cassandra 4.3K May  9 11:25 Disco-Client-hc-6-Statistics.db
-rw-r--r--  4 cassandra cassandra   46 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-CompressionInfo.db
-rw-r--r--  4 cassandra cassandra   92 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Data.db
-rw-r--r--  4 cassandra cassandra  496 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Filter.db
-rw-r--r--  4 cassandra cassandra   26 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Index.db
-rw-r--r--  4 cassandra cassandra 4.3K May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Statistics.db
drwxr-xr-x  6 cassandra cassandra 4.0K May  9 11:25 snapshots
",Debian Linux Squeeze with the cassandra debian package from Apache.,,,,,,,,,,,,,,,,,,14/May/12 16:45;jbellis;4230-v2.txt;https://issues.apache.org/jira/secure/attachment/12526778/4230-v2.txt,13/May/12 14:47;xedin;CASSANDRA-4230.patch;https://issues.apache.org/jira/secure/attachment/12526679/CASSANDRA-4230.patch,12/Mar/15 15:18;cvertiz;cassandraDEBUG.log;https://issues.apache.org/jira/secure/attachment/12704165/cassandraDEBUG.log,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-05-09 16:10:11.928,,,no_permission,,,,,,,,,,,,238702,,,Thu Mar 12 17:21:05 UTC 2015,,,,,,0|i0gti7:,96210,jbellis,jbellis,,,,,,,,,,09/May/12 16:10;jbellis;probably related to CASSANDRA-4219,"09/May/12 16:17;xedin;From the exception message I see that it's related to the JNA hardlinks while it does a snapshot:

{noformat}
ERROR [MigrationStage:1] 2012-05-09 11:25:36,043 CLibrary.java (line 158) Unable to create hard link
com.sun.jna.LastErrorException: errno was 17
...
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
{noformat}

and errno of 17 means ""File exists"" so this is something with snapshot or the system rather then migrations.","09/May/12 16:29;brandon.williams;Since snapshot dirs include a timestamp, perhaps we should just move along if it already exists.","09/May/12 16:32;xedin;Yeah, we probably should do just that.",09/May/12 16:43;jbellis;But why would it attempt the same snapshot twice?  Sounds like there's a real bug here.,09/May/12 16:57;brandon.williams;snapshot on compaction is one way it can happen.,"09/May/12 17:02;xedin;Yeah, it's only called in snapshotWithoutFlush so if you try to drop CF in the middle of compaction snapshoting files it would be the only way to lead to such behavior. I think what we need to do here is to stop compaction and run drop after that...","09/May/12 18:12;jbellis;bq. snapshot on compaction is one way it can happen.

I suppose there's a really small chance of that happening if you happen to time it just right, but that wouldn't explain it being easily reproducible (since the compaction and drop snapshots generate their snapshot name independently).

Also, snapshot-on-compaction is off by default, I doubt André has it enabled.  André, can you confirm?","09/May/12 18:41;edevil;I suppose you mean this:
{noformat}
$ cat /etc/cassandra/cassandra.yaml |grep snapshot |grep compac
# Whether or not to take a snapshot before each compaction.  Be
snapshot_before_compaction: false
{noformat}

I did not enable it.","13/May/12 14:14;xedin;I have figured out that this problem is caused only when LeveledCompaction is used for Secondary Index, it seems like when index's leveled manifest is snapshoted it uses the wrong name.",13/May/12 22:53;jbellis;Do we need this in 1.0 too?,"13/May/12 23:00;xedin;No, this bug came together with directory-per-CF which was added into 1.1.",14/May/12 16:45;jbellis;simpler v2 attached,"14/May/12 17:05;xedin;Heh, I overlooked that possibility, +1.","14/May/12 17:07;xedin;Andre, can you please test and confirm that everything works as expected before we commit it?","15/May/12 17:08;edevil;It works, thanks!",15/May/12 17:10;jbellis;committed,"12/Mar/15 15:17;cvertiz;Note: Issue reproduced on cassandra 2.0.12 (datastax for windows)
Log: refer to cassandraDEBUG.log attached file.","12/Mar/15 17:21;JoshuaMcKenzie;[~cvertiz]: doesn't necessarily look like the same issue to me. Your attached logs don't show any errors symlinking for snapshots during compaction though they do contain the same thrift error message:
{noformat}
DEBUG [Thrift:13] 2015-03-12 09:52:49,291 CustomTThreadPoolServer.java (line 214) Thrift transport error occurred during processing of message.
 org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
{noformat}

What behavior produced the errors you're seeing?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nodetool getendpoints keys do not work with spaces, key_validation=ascii value of key => ""a test""  no delimiter",CASSANDRA-4551,12603783,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdeangel,mvaldez,mvaldez,16/Aug/12 20:36,12/Mar/19 14:21,13/Mar/19 22:27,23/Aug/13 01:16,1.2.9,,Tool/nodetool,,,0,datastax_qa,lhf,,,,,"Nodetool getendpoints keys do not work with embedded spaces, key_validation=ascii value of key => ""a test""  no delimiter tried to escape key => ""a test"" with double and single quotes. It doesn't work. It just reiterates the format of the tool's command: getendpoints requires ks, cf and key args",,,,,,,,,,,,,,,,,,,21/Aug/13 05:50;gdeangel;CASSANDRA-4551.txt;https://issues.apache.org/jira/secure/attachment/12599119/CASSANDRA-4551.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-21 03:27:16.973,,,no_permission,,,,,,,,,,,,256238,,,Fri Aug 23 01:16:22 UTC 2013,,,,,,0|i0gx0n:,96779,dbrosius,dbrosius,,,,,,,,,,"21/Aug/13 03:27;gdeangel;The documentation for nodetool getendpoints says that the key must be in hex format. Is the key being entered as the string ""a test"" or the hex representation?","21/Aug/13 04:35;jbellis;That's outdated -- we do call keyValidator.fromString, so human-readable keys are okay.

The goal here is to get {{getendpoints myks mytable ""a test""}} to parse the key as {{a test}} rather than the key {{a}} followed by an unparsed argument {{test}}.",22/Aug/13 01:11;dbrosius;Is this a problem in nodetool.bat as well?,22/Aug/13 05:17;gdeangel;I'm not sure about that but I can try to double check tomorrow if I can get access to a Windows machine.,22/Aug/13 17:54;gdeangel;I did not see the same issue with nodetool.bat. Seems to work fine.,"23/Aug/13 01:16;dbrosius;thanks for checking, +1

committed to cassandra-1.2 as commit ddb501df408af59e213380263f3c519d11b89977",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid ConcurrentModificationExceptions on relocateTokens,CASSANDRA-4727,12609344,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius@apache.org,dbrosius@apache.org,dbrosius@apache.org,27/Sep/12 04:47,12/Mar/19 14:21,13/Mar/19 22:27,27/Sep/12 12:11,1.2.0 beta 2,,,,,0,,,,,,,"code loops over a HashMap and deletes items from the hashmap without using the iterator.

will result in ConcurrentModificationExceptions... remove thru the iterator instead.",,,,,,,,,,,,,,,,,,,27/Sep/12 04:47;dbrosius@apache.org;cme_patch.txt;https://issues.apache.org/jira/secure/attachment/12546804/cme_patch.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-27 05:01:45.067,,,no_permission,,,,,,,,,,,,255202,,,Thu Sep 27 12:10:20 UTC 2012,,,,,,0|i0eq13:,83979,jbellis,jbellis,,,,,,,,,,27/Sep/12 05:01;jbellis;+1,27/Sep/12 12:10;dbrosius@apache.org;committed to trunk as 1078e6f514bcfbcaadb2517099baab5f3d21d510,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is an inconsistency between default configuration in cassandra.yaml and java code,CASSANDRA-4754,12610091,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,azotcsit,azotcsit,azotcsit,03/Oct/12 15:57,12/Mar/19 14:21,13/Mar/19 22:27,03/Oct/12 18:48,1.1.6,,,,,0,configurations,docs,,,,,Options max_hint_window_in_ms and in_memory_compaction_limit_in_mb have different values in cassandra.yaml and in java code. I suggest to lead java code values to cassandra.yaml values.,,,,,,,,,,,,,,,,,,,03/Oct/12 15:59;azotcsit;cassandra-1.1-4754_default_configs.txt;https://issues.apache.org/jira/secure/attachment/12547547/cassandra-1.1-4754_default_configs.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-03 18:48:25.274,,,no_permission,,,,,,,,,,,,240585,,,Thu Oct 04 06:04:15 UTC 2012,,,,,,0|i013q7:,4373,brandon.williams,brandon.williams,,,,,,,,,,"03/Oct/12 18:48;brandon.williams;Committed, without yaml comments since they seem a bit redundant and out of place.","04/Oct/12 06:04;azotcsit;Ok, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix consistency ALL parsing in CQL3,CASSANDRA-4659,12607311,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,12/Sep/12 17:35,12/Mar/19 14:21,13/Mar/19 22:27,12/Sep/12 19:21,1.2.0 beta 1,,,,,0,,,,,,,"CASSANDRA-4490 made some change to the parsing of ALL for consistency levels (introducing a specific token K_ALL). It's unclear why since that new token is not used (that is, except for the consistency level), probably some left over of a previous version of the patch.

In any case, this doesn't work. K_ALL and K_LEVEL being both tokens, the string 'ALL' will always generate K_ALL and never K_LEVEL and thus 'USING CONSISTENCY ALL' doesn't parse anymore.",,,,,,,,,,,,,,,,,,,12/Sep/12 17:41;slebresne;4659.txt;https://issues.apache.org/jira/secure/attachment/12544850/4659.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-12 19:17:38.478,,,no_permission,,,,,,,,,,,,256323,,,Wed Sep 12 19:21:27 UTC 2012,,,,,,0|i0gy73:,96970,xedin,xedin,,,,,,,,,,12/Sep/12 17:41;slebresne;Trivial patch that revert the change from CASSANDRA-4490 since it's not used anyway.,"12/Sep/12 19:17;xedin;+1, it was a left behind, grant/revoke commands actually using {full, no}_access.","12/Sep/12 19:21;slebresne;Committed, thanks
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh --color option doesn't allow you to disable colors,CASSANDRA-4634,12606735,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,08/Sep/12 21:53,12/Mar/19 14:21,13/Mar/19 22:27,10/Sep/12 20:07,1.1.6,,,,,0,,,,,,,"There's no way to disable colors with cqlsh, despite it having a {{--color}} option, because that option can only enable color if present, not disable it, and the default is that color is enabled.

(Incidentally, if the {{--file}} option is used, it will disable color.)",,,,,,,,,,,,,,,,,,,10/Sep/12 18:37;thobbs;4634-color-option-v2.txt;https://issues.apache.org/jira/secure/attachment/12544500/4634-color-option-v2.txt,08/Sep/12 22:49;thobbs;4634-color-option.txt;https://issues.apache.org/jira/secure/attachment/12544371/4634-color-option.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-09 00:13:29.995,,,no_permission,,,,,,,,,,,,256304,,,Mon Sep 10 20:07:42 UTC 2012,,,,,,0|i0gxxb:,96926,brandon.williams,brandon.williams,,,,,,,,,,"08/Sep/12 22:49;thobbs;Attached patch changes the color option to accept a choice of {{always}}, {{never}}, or {{auto}}, defaulting to {{auto}}, which uses colors if not reading from a file and stdout is connected to a terminal (at least as well as we can detect that).  I based the options off of {{ls}} and {{grep}}, which use the same format.","09/Sep/12 00:13;thepaul;Since I'm not a particular fan of ls/grep's command-line color control (too much typing for cases which are fairly common) and since this isn't backwards compatible (using {{\--color}} alone will give an error), I'm inclined to vote -1 on it. Can we just add a {{\--no-color}} option to force color off for the people who don't want to do {{TERM=dumb}} or {{cqlsh | cat}} or something?","10/Sep/12 08:36;slebresne;I don't have any problem with ls/grep's command-line color control but I would tend to agree that it's probably not worth breaking any backward compatibility and a --no-color would be good enough. I don't care much about the matter though, just my 2 cents.","10/Sep/12 18:37;thobbs;v2 patch adds a {{\-\-no-color}} option instead.  The behavior now is that with {{\-\-color}}, color will always be enabled, with {{\-\-no-color}}, it will always be disabled, and without either one, the behavior is similar to {{auto}}, where color is only enabled if not reading a file and stdout is connected to a terminal.",10/Sep/12 19:12;thepaul;+1,10/Sep/12 20:07;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Event tracing always records thread name as 'TracingStage',CASSANDRA-4599,12605804,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,31/Aug/12 22:19,12/Mar/19 14:21,13/Mar/19 22:27,31/Aug/12 23:42,1.2.0 beta 1,,,,,0,,,,,,,"Since LoggingEvent#getThreadName gets current thread name when accessed, name of tracing thread ('TracingStage') is always logged to events CF.",,,,,,,,,,,,,,,,,,,31/Aug/12 22:20;yukim;4599.txt;https://issues.apache.org/jira/secure/attachment/12543353/4599.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-31 22:31:26.377,,,no_permission,,,,,,,,,,,,256279,,,Sat Sep 01 00:45:09 UTC 2012,,,,,,0|i0gxjj:,96864,dr-alves,dr-alves,,,,,,,,,,31/Aug/12 22:20;yukim;Trivial fix attached.,"31/Aug/12 22:31;dr-alves;lgtm, +1",31/Aug/12 23:42;yukim;Committed.,"01/Sep/12 00:45;hudson;Integrated in Cassandra #1988 (See [https://builds.apache.org/job/Cassandra/1988/])
    fix for logging events' correct thread name; patch by yukim reviewed by David Alves for CASSANDRA-4599 (Revision e49d140bb72264a503a634d5603c25766c78e50d)

     Result = ABORTED
yukim : 
Files : 
* src/java/org/apache/cassandra/tracing/TracingAppender.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multithreaded cache saving can skip caches,CASSANDRA-4533,12603184,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,hanzhu,hanzhu,13/Aug/12 06:40,12/Mar/19 14:21,13/Mar/19 22:27,22/Aug/12 14:01,1.1.5,,,,,0,,,,,,,"Cassandra flushes the key and row cache to disk periodically. It also uses a atomic flag in flushInProgress to enforce single cache writer at any time.

However, the cache saving task could be submitted to CompactionManager concurrently, as long as the number of worker thread in CompactionManager is larger than 1. 

Due to the effect of above atomic flag, only one cache will be written out to disk. Other writer are cancelled when the flag is true.

I observe the situation in Cassandra 1.0. If nothing is changed, the problem should remain in Cassandra 1.1, either.",,,,,,,,,,,,,,,,,,,20/Aug/12 20:33;yukim;4533-1.1.txt;https://issues.apache.org/jira/secure/attachment/12541641/4533-1.1.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-14 20:15:45.662,,,no_permission,,,,,,,,,,,,256223,,,Wed Aug 22 14:01:47 UTC 2012,,,,,,0|i0gwtj:,96747,jbellis,jbellis,,,,,,,,,,14/Aug/12 20:15;jbellis;Looks like we should switch to a ConcurrentSet like we did in Memtable.meteringInProgress.,15/Aug/12 06:15;hanzhu;Is it possible to fix it also in 1.0?,"15/Aug/12 13:50;jbellis;No, it's not worth risking regressions over in 1.0.x. 

The good news is that 1.1.4+ look pretty stable.","15/Aug/12 15:19;hanzhu;Looks like we have to maintain our own fork, as we can not keep up with the upgrade cycle... ","20/Aug/12 20:33;yukim;Attaching patch against 1.1 branch.
Caches are grobal since 1.1, so I used CacheType as key for flushInProgerss concurrent set.","21/Aug/12 20:08;jbellis;Hmm, I don't think this quite works because it still means we can skip saving cache for CF X when CF Y is being flushed.

I think the problem this code is trying to solve, over a basic executor + queue, is multiple tasks for X getting queued up while (say) compaction is sucking a lot of i/o, then firing off those cache-save tasks for X faster than the defined saving period when it speeds up.

I guess we could make it a Pair<CF, CacheType>?

TBH this is probably premature optimization, if your cache period is so frequent that multiple queued tasks is a problem, then you should just fix that.  I'd be okay with just ripping this out.  Alternatively, we could have the task check to see if the last-saved cache is older than M minutes before overwriting it, similar to how normal background compaction submissions are a no-op if it turns out there's nothing to do by the time we execute the task.","22/Aug/12 01:05;yukim;bq. Hmm, I don't think this quite works because it still means we can skip saving cache for CF X when CF Y is being flushed.

In my understanding, since 1.1, C* stores key and row caches globally, those are saved at once for every CF for each cache type.
AutoSavingCache$Writer writes all CF for certain CacheType in one execution.","22/Aug/12 02:24;jbellis;You're right, my mistake.  +1","22/Aug/12 14:01;yukim;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Regular startup log has confusing ""Bootstrap/Replace/Move completed!"" without boostrap, replace, or move",CASSANDRA-4802,12611644,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,kmueller,kmueller,12/Oct/12 22:21,12/Mar/19 14:21,13/Mar/19 22:27,17/Oct/12 19:09,1.2.0,,,,,0,,,,,,,"A regular startup completes successfully, but it has a confusing message the end of the startup:

""  INFO 15:19:29,137 Bootstrap/Replace/Move completed! Now serving reads.""

This happens despite no bootstrap, replace, or move.

While purely cosmetic, this makes you wonder what the node just did - did it just bootstrap?!  It should simply read something like ""Startup completed! Now serving reads"" unless it actually has done one of the actions in the error message.



Complete log at the end:


INFO 15:13:30,522 Log replay complete, 6274 replayed mutations
 INFO 15:13:30,527 Cassandra version: 1.0.12
 INFO 15:13:30,527 Thrift API version: 19.20.0
 INFO 15:13:30,527 Loading persisted ring state
 INFO 15:13:30,541 Starting up server gossip
 INFO 15:13:30,542 Enqueuing flush of Memtable-LocationInfo@1828864224(29/36 serialized/live bytes, 1 ops)
 INFO 15:13:30,543 Writing Memtable-LocationInfo@1828864224(29/36 serialized/live bytes, 1 ops)
 INFO 15:13:30,550 Completed flushing /data2/data-cassandra/system/LocationInfo-hd-274-Data.db (80 bytes)
 INFO 15:13:30,563 Starting Messaging Service on port 7000
 INFO 15:13:30,571 Using saved token 31901471898837980949691369446728269823
 INFO 15:13:30,572 Enqueuing flush of Memtable-LocationInfo@294410307(53/66 serialized/live bytes, 2 ops)
 INFO 15:13:30,573 Writing Memtable-LocationInfo@294410307(53/66 serialized/live bytes, 2 ops)
 INFO 15:13:30,579 Completed flushing /data2/data-cassandra/system/LocationInfo-hd-275-Data.db (163 bytes)
 INFO 15:13:30,581 Node kaos-cass02.xxxxxxx/1.2.3.4 state jump to normal
 INFO 15:13:30,598 Bootstrap/Replace/Move completed! Now serving reads.
 INFO 15:13:30,600 Will not load MX4J, mx4j-tools.jar is not in the classpath
","RHEL6, JDK1.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-10-15 19:48:22.881,,,no_permission,,,,,,,,,,,,248134,,,Wed Oct 17 19:09:37 UTC 2012,,,,,,0|i09lqf:,53954,brandon.williams,brandon.williams,,,,,,,,,,"15/Oct/12 19:48;vijay2win@yahoo.com;How about just saying:
Bootstrap completed! Now serving reads.

? Do we need any additional information?","15/Oct/12 19:51;brandon.williams;I think the point is that we should not print it if we didn't actually bootstrap, and we should be able to distinguish between bootstrap/replace/move.","15/Oct/12 19:59;vijay2win@yahoo.com;Move doesnt use the same code anymore, replace uses this but there are other log info explaining that....

If Bootstrap is a wrong word then how about: Startup completed?
(I am still looking for an abstract word :))","15/Oct/12 22:10;kmueller;Bootstrap means something specifically with cassandra in that you think some data has streamed in.

I think ""Startup completed"" would be great.

If there IS a bootstrap/replace/move then I think the message ought to specify which has happened and that it's ready now (if it's easy to do) :)","16/Oct/12 04:28;vijay2win@yahoo.com;Committed https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/service/StorageService.java;h=8de0bd24632c89ea1b41c952ee6ec2db58808894;hp=7d92fbe0ff15c8c686a93425f4fccca49b921c0b;hb=d525cf969c042b21a9375446f5449ee82d7d1484;hpb=7e937b3d1308c0774e4b0366b6e66b14af1dd5f6

Let me know if you need more info, i will reopen this ticket.","16/Oct/12 20:48;brandon.williams;This isn't quite what I had in mind.  It's not a semantic issue, it's a logical issue.  We should clearly indicate the operation that was actually performed, which after a quick glance at the code means we need to store this state somewhere to do so.","17/Oct/12 19:09;vijay2win@yahoo.com;Had a discussion with Brandon offline, 
There is enough information in the logs to show the operation was Bootstrap vs Repair vs Startup, so closing the ticket for now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcDate.compose is not null safe,CASSANDRA-4830,12612363,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,skuppa,skuppa,skuppa,18/Oct/12 05:03,12/Mar/19 14:21,13/Mar/19 22:27,06/Dec/12 19:23,1.1.7,,,,,0,,,,,,,"I am using the cassandra-jdbc for CQL.  I have a table with timestamp column.  When timestamp column is null it throws, IndexOutOfBoundsException exception since JdbcDate.compose calls the new Date(ByteBufferUtil.toLong(value)).  The ByteBufferUtil.toLong(bytes) throws exception the exception since position and limit pointers are same (similar to null).  This has to be handled gracefully in the JdbcDate.compose method instead of throwing exception.  I would like to see implementation something like,

    public Date compose(ByteBuffer bytes)
    {
        if(bytes.limit() - bytes.position() > 0) 
        {
            return new Date(ByteBufferUtil.toLong(bytes));
        } 
      
        return  null;
    }

BTW, this matches exactly reverse with decompose method.  Logically it supposed to be implemented in the first place ;)

",Any,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-10-18 22:44:18.353,,,no_permission,,,,,,,,,,,,249473,,,Thu Oct 18 22:44:18 UTC 2012,,,,,,0|i0aepr:,58657,jbellis,jbellis,,,,,,,,,,"18/Oct/12 22:44;jbellis;done in 9d7ba39cbb6f93759f654c7df1771b52354dec36, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_range_slices does not validate end_token,CASSANDRA-5089,12625154,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,apesternikov,apesternikov,23/Dec/12 21:04,12/Mar/19 14:21,13/Mar/19 22:27,09/Jan/13 22:29,1.1.9,1.2.0,,,,0,,,,,,,"get_range_slices times out, java log has the following exception:
ERROR [Thrift:1] 2012-12-22 08:14:30,120 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[Thrift:1,5,main]
java.lang.AssertionError: [DecoratedKey(28555413689034504124051437792156504, 6434313866653035643631663962323635323937343337653666636265616162),max(0)]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:45)
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:38)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:698)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3083)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3071)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)

We see it every time on the SECOND get_range_slices call when we clear start_token and set start_key in the key range.
We noticed this in 1.1.7 first, 1.1.8 still affected. 1.1.6 is fine.
Please contact me if you need more information.
 ",,,,,,,,,,,,,,,,CASSANDRA-5106,,,27/Dec/12 16:26;jbellis;5089-v2.txt;https://issues.apache.org/jira/secure/attachment/12562485/5089-v2.txt,24/Dec/12 02:17;jbellis;5089.txt;https://issues.apache.org/jira/secure/attachment/12562302/5089.txt,07/Jan/13 16:52;apesternikov;5089unittest.diff;https://issues.apache.org/jira/secure/attachment/12563593/5089unittest.diff,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-12-24 02:03:52.222,,,no_permission,,,,,,,,,,,,301694,,,Tue Jan 08 21:40:49 UTC 2013,,,,,,0|i16vpr:,248270,dbrosius,dbrosius,,,,,,,,,,"24/Dec/12 02:03;jbellis;It looks like you are specifying start_key and end_token, with an invalid end_token.  Don't do that.  (If you want to specify ""the rest of the token range"" then use end_key="""" and leave end_token null.)
","24/Dec/12 02:17;jbellis;Prior to CASSANDRA-4804 we always ignored end_token which is why this is biting you now.

Patch attached to add extra validation so we don't rely on AssertionError to stop this.","24/Dec/12 15:44;apesternikov;what is the proper format for token? I assumed it is decimal string. I think it is worked in 1.1.6
re using end_key="""": start_key + end_token range has its very valid use case, when the whole data set should be processed (think of map/reduce) on multiple distributed workers. We split the ring into N parts, every worker start with (start_token - end_token) range, when first batch is received it switches to (star_key - end_token). ColumnFamilyRecordReader is cheating here by using internals for calculating token:
startToken = partitioner.getTokenFactory().toString(partitioner.getToken(Iterables.getLast(rows).key));
unfortunately it is not an option for us, we are using C++
So, what is the proper format for token and how to do iteration over several ranges?
Thank you.",27/Dec/12 16:26;jbellis;simplified v2 attached ,"27/Dec/12 16:37;dbrosius@apache.org;v2 lgtm, assuming we don't allow a crazy combination of range.start_key and range.end_token or such.

also note, pushing to trunk will not compile cleanly as on trunk throws of IRE needs to be fully qualified... (or an import needs to be added).","27/Dec/12 17:10;apesternikov;Dave,
either your statement about craziness of start_key - end_token or first paragraph of https://issues.apache.org/jira/browse/CASSANDRA-4804 description is incorrect.
I would like to reiterate that start_key - end_token combination let us split ranges and iterate over segments using API only without reaching internals.
Ok, if you guys decide that you don't want to support this combination, can you give us another tool for iteration? IMHO something like 
3:  optional string token,
in KeySlice would be sufficient for proper iteration over token ranges. As a free benefit we would get a better token iterator semantics.","27/Dec/12 17:13;jbellis;bq. assuming we don't allow a crazy combination of range.start_key and range.end_token or such

Heh, that's what I was trying to validate in v1.  I retract v2. :)","27/Dec/12 17:25;dbrosius@apache.org;Sorry Aleksey, ignore my craziness comment :)

V1, needs semi at RowPosition stop = p.getTokenFactory().fromString(range.end_token).maxKeyBound(p)

also, it seems there is ambiquity if end_key and end_token are specified. (or start_key and start_token for that matter).","27/Dec/12 19:17;dbrosius@apache.org;ah, there's a check at the top for this ambiquity, so i was mistaken...

patch lgtm, except for semi.","27/Dec/12 20:45;jbellis;committed, thanks","31/Dec/12 05:54;apesternikov;I applied the 5089-v2.txt to 1.1.8. Unfortunately, I have to report that it does not fix the problem.
It is not really surprising, because the patch does not change anything for our case of (start_key, end_token) range.
I have a huge log with debug logging level, please contact me if interested.","31/Dec/12 06:18;dbrosius;i would have thought this

+            RowPosition stop = p.getTokenFactory().fromString(range.end_token).maxKeyBound(p)
+            if (range.start_key != null && RowPosition.forKey(range.start_key, p).compareTo(stop) > 0)
+                throw new InvalidRequestException(""Start key's token sorts after end token"");


would have addressed the issue. Can you produce a simple test case that shows the problem?","07/Jan/13 16:52;apesternikov;test attached.
Please note that you need -ea JVM command line flag to reveal the problem, without -ea it works fine.
I think the problem is in Bounds.java:45 assert statement:

        assert left.compareTo(right) <= 0 || right.isMinimum(partitioner) : ""["" + left + "","" + right + ""]"";

 Why left is even compared to right? Why ""bounds may not wrap"" as it stated in the comment? Logically, ""less"" and ""more"" are irrelevant to iterator position as we are talking about RING.",08/Jan/13 21:40;jbellis;Let's leave this closed since 1.2.0 is already released with the patch.  CASSANDRA-5106 is open to fix a regression it caused.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFS.allUserDefined() doesn't exclude system_auth and system_traces keysapces,CASSANDRA-5160,12627654,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,15/Jan/13 14:51,12/Mar/19 14:21,13/Mar/19 22:27,15/Jan/13 15:10,1.2.1,,,,14/Jan/13 00:00,0,,,,,,,Make sure CFS.allUserDefined() excludes all system-related keyspaces.,,,,,,,,,,,,,,,,,,,15/Jan/13 14:52;jasobrown;0001-Make-sure-CFS.allUserDefined-excludes-all-system-rel.patch;https://issues.apache.org/jira/secure/attachment/12564933/0001-Make-sure-CFS.allUserDefined-excludes-all-system-rel.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-15 15:10:47.7,,,no_permission,,,,,,,,,,,,304440,,,Tue Jan 15 15:10:47 UTC 2013,,,,,,0|i17mjb:,252620,brandon.williams,brandon.williams,,,,,,,,,,15/Jan/13 14:52;jasobrown;Very trivial one-line change,"15/Jan/13 15:10;brandon.williams;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Validate login for Thrift describe_keyspace, describe_keyspaces and set_keyspace methods",CASSANDRA-5144,12627043,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,10/Jan/13 21:47,12/Mar/19 14:21,13/Mar/19 22:27,11/Jan/13 00:06,1.2.1,,,,,0,,,,,,,"Not validating login leaks info about keyspaces and columnfamilies if the configured authenticator requires validation.

This change does not affect AllowAllAuthenticator, but if an implementation forbids anonymous access, we should deny this information to unauthenticated users.",,,,,,,,,,,,,,,,,,,10/Jan/13 21:48;iamaleksey;5144.txt;https://issues.apache.org/jira/secure/attachment/12564274/5144.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-10 23:43:34.042,,,no_permission,,,,,,,,,,,,303736,,,Fri Jan 11 00:06:52 UTC 2013,,,,,,0|i17e4v:,251259,jbellis,jbellis,,,,,,,,,,10/Jan/13 23:43;jbellis;Thrift has AuthenticationException and AuthorizationException instead of IRE.  Otherwise +1,"11/Jan/13 00:06;iamaleksey;Can't use Thrift {noformat}{Authentication,Authorization}Exception{noformat} without breaking thrift interface.

Committed with changes: removed the conversion method from ThriftConversion.

Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong description of 'setstreamthroughput' option,CASSANDRA-5036,12619033,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,azotcsit,azotcsit,azotcsit,06/Dec/12 11:48,12/Mar/19 14:21,13/Mar/19 22:27,24/Mar/13 06:43,1.2.4,,Legacy/Documentation and Website,Local/Config,,0,,,,,,,"There is a typo in description of 'setstreamthroughput' option. It is measured in megabits per second. Page with wrong description:
http://www.datastax.com/docs/1.1/references/nodetool#nodetool-setstreamthroughput
Page with right description:
http://www.datastax.com/docs/1.1/configuration/node_configuration#stream-throughput-outbound-megabits-per-sec



Also I want to discuss possibility to reduce default value for this option. I think that 400 Mbs is too high for common cases. 

Preface:
This option is used only in case streams. There are two cases when streams are actual. They are rebuilding of a node and repair process. Let's skip first case and will talk only about the second. Let's imagine that we have replication factor 3.

Cross-datacenter connectivity case: 
When we start repair process it will borrow all network channel. Let's do some calculations. You start repair on an one node, e.g. 5 node (3 remote and 2 local) should send us some data. Note that 3 of them are from remote datacenter. So 400 * 3 = 1,2 Gbs we should receive through WAN. I'm sure that it's too high.

I suggest to make it 2 times less.
",Cassandra 1.1.6 (DataStax distribution),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-24 05:13:33.664,,,no_permission,,,,,,,,,,,,296311,,,Sun Mar 24 06:43:03 UTC 2013,,,,,,0|i148ef:,232824,krummas,krummas,,,,,,,,,,"24/Mar/13 05:13;jbellis;What do you think about the suggested default, [~krummas]?","24/Mar/13 06:28;krummas;I agree it might be a bit too high

defaulting to 200 sounds good",24/Mar/13 06:43;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migration during shutdown can cause AE,CASSANDRA-5236,12631536,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,08/Feb/13 23:08,12/Mar/19 14:21,13/Mar/19 22:27,27/Feb/13 16:38,1.1.11,,,,,0,,,,,,,"Mostly just a problem for the dtests on occasion:

{noformat}
 INFO [FlushWriter:1] 2013-02-08 17:03:49,727 Memtable.java (line 453) Writing Memtable-schema_columns@1757448463(793/793 serialized/live bytes, 15 ops)
 INFO [FlushWriter:1] 2013-02-08 17:03:49,752 Memtable.java (line 487) Completed flushing /tmp/dtest-gEW6x2/test/node3/data/system/schema_columns/system-schema_columns-ib-4-Data.db (339 bytes) for commitlog position ReplayPosition(segmentId=1360364624598, position=66887)
 INFO [CompactionExecutor:2] 2013-02-08 17:03:49,754 CompactionTask.java (line 112) Compacting [SSTableReader(path='/tmp/dtest-gEW6x2/test/node3/data/system/schema_columns/system-schema_columns-ib-4-Data.db'), SSTableReader(path='/tmp/dtest-gEW6x2/test/node3/data/system/schema_columns/system-schema_columns-ib-3-Data.db'), SSTableReader(path='/tmp/dtest-gEW6x2/test/node3/data/system/schema_columns/system-schema_columns-ib-2-Data.db'), SSTableReader(path='/tmp/dtest-gEW6x2/test/node3/data/system/schema_columns/system-schema_columns-ib-1-Data.db')]
 INFO [CompactionExecutor:3] 2013-02-08 17:03:49,759 CompactionTask.java (line 272) Compacted 4 sstables to [/tmp/dtest-gEW6x2/test/node3/data/system/schema_columnfamilies/system-schema_columnfamilies-ib-5,].  7,473 bytes to 5,595 (~74% of original) in 32ms = 0.166744MB/s.  6 total rows, 4 unique.  Row merge counts were {1:3, 2:0, 3:1, 4:0, }
ERROR [InternalResponseStage:1] 2013-02-08 17:03:49,787 CassandraDaemon.java (line 135) Exception in thread Thread[InternalResponseStage:1,5,main]
java.lang.AssertionError
    at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:320)
    at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:458)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:347)
    at org.apache.cassandra.service.MigrationTask$1.response(MigrationTask.java:66)
    at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:47)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,22/Feb/13 21:05;brandon.williams;5236.txt;https://issues.apache.org/jira/secure/attachment/12570537/5236.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-27 14:28:19.002,,,no_permission,,,,,,,,,,,,312032,,,Wed Feb 27 16:38:40 UTC 2013,,,,,,0|i1hujz:,312378,iamaleksey,iamaleksey,,,,,,,,,,"22/Feb/13 21:05;brandon.williams;The simplest thing to do is remove this assertion, since the gossiper actually doesn't need to be running to add a state, and a migration may come in right after disabling gossip (or someone could disable gossip then issue a migration against the node in thrift.)",27/Feb/13 14:28;iamaleksey;Agreed. +1,27/Feb/13 16:38;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli shouldn't set default username and password,CASSANDRA-5208,12630190,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,31/Jan/13 19:00,12/Mar/19 14:21,13/Mar/19 22:27,31/Jan/13 19:17,1.2.2,,Legacy/Tools,,,0,,,,,,,"Currently cli sets default username and password if none are set (in CliOptions.processArgs). Because of this cli will always authenticate, whether or not this was the intent of the user and CliMain.connect() ""if ((sessionState.username != null) && (sessionState.password != null))"" condition will always be true.

This breaks authentication in at least two scenarios:
1. Authenticator allows anonymous access and a user wants to login anonymously - instead he will get AuthenticationException because user ""default"" will most likely not exist.
2. Authenticator doesn't user username/password pairs for login but something like Kerberos instead. Thrift's login with u:default, p:"""" will still be called and AuthenticationException will be thrown, again.",,,,,,,,,,,,,,,,,,,31/Jan/13 19:00;iamaleksey;5208.txt;https://issues.apache.org/jira/secure/attachment/12567410/5208.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-31 19:09:26.086,,,no_permission,,,,,,,,,,,,310686,,,Thu Jan 31 19:17:52 UTC 2013,,,,,,0|i1hm93:,311031,jbellis,jbellis,,,,,,,,,,31/Jan/13 19:09;jbellis;+1,"31/Jan/13 19:17;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair command should report error when replica node is dead,CASSANDRA-5203,12630040,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,30/Jan/13 23:58,12/Mar/19 14:21,13/Mar/19 22:27,01/Feb/13 21:35,1.1.10,1.2.2,,,,0,,,,,,,"CASSANDRA-4767 makes nodetool repair command to print progress, but when replica node is dead and repair cannot be proceeded, nodetool repair just report session finished. Instead, nodetool should report session is failed.

Also, it is nice to exit command with status code of 1 when repair failed.
",,,,,,,,,,,,,,,,,,,31/Jan/13 23:01;yukim;5203-1.1.txt;https://issues.apache.org/jira/secure/attachment/12567453/5203-1.1.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-01 00:44:31.61,,,no_permission,,,,,,,,,,,,310536,,,Fri Feb 01 21:35:48 UTC 2013,,,,,,0|i1hlbz:,310881,brandon.williams,brandon.williams,,,,,,,,,,31/Jan/13 23:01;yukim;Patch to fail on node dead and exit nodetool with error status code.,01/Feb/13 00:44;brandon.williams;+1,"01/Feb/13 21:35;yukim;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default read_repair_chance value is wrong,CASSANDRA-4114,12549455,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,mkmainali,mkmainali,04/Apr/12 07:20,12/Mar/19 14:21,13/Mar/19 22:27,04/Apr/12 17:06,1.1.0,,Legacy/Tools,,,0,,,,,,,"The documents says that the default read_repair_chance value is 0.1, and it is also declared so in CFMetaDeta. However, when creating a column family with ""create column family foo"" via cli and checking with ""show keyspaces"" shows that the read_repair_chance=1.0. This also happens when creating the column family through Hector.

Going through the code, I find that in CfDef class, the constructor without any parameters sets the read_repair_chance to 1. Changing this value to 0.1 seems to create a column family with the 0.1 read_repair_chance. The best might be to remove it from the CfDef as the read_repair_chance is set to the default value in CFMetaDeta.",,,,,,,,,,,,,,,,,,,04/Apr/12 16:19;jbellis;4114.txt;https://issues.apache.org/jira/secure/attachment/12521329/4114.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-04 16:19:17.718,,,no_permission,,,,,,,,,,,,234446,,,Wed Apr 04 17:06:56 UTC 2012,,,,,,0|i0gs5j:,95991,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"04/Apr/12 16:19;jbellis;I think that since the cli has behaved this way since 1.0.0, changing it now might surprise people who didn't read NEWS (and thus don't know that it was supposed to change to default of 0.1).  So I propose fixing this in 1.1.0 instead. 

(For completeness, I note that cql {{CREATE COLUMNFAMILY}} does default to 0.1 correctly since it does not build its CFMetadata objects from Thrift.)
",04/Apr/12 16:38;vijay2win@yahoo.com;+1 and tested by creating a cf via cli. (  and read_repair_chance = 0.1 ),04/Apr/12 17:06;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh can't handle python being a python3,CASSANDRA-4090,12548273,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ash211,ash211,ash211,27/Mar/12 07:34,12/Mar/19 14:21,13/Mar/19 22:27,03/Apr/12 16:12,1.0.10,1.1.0,Legacy/Tools,,,0,cqlsh,,,,,,"cqlsh fails to run when {{python}} is a Python 3, with this error message:

{code}
andrew@spite:~/src/cassandra-trunk/bin $ ./cqlsh 
  File ""./cqlsh"", line 97
    except ImportError, e:
                      ^
SyntaxError: invalid syntax
andrew@spite:~/src/cassandra-trunk/bin $ 
{code}

The error occurs because the cqlsh script checks for a default installation of python that is older than a certain version, but not one newer that is incompatible (e.g. Python3).  To fix this, I update the logic to only run {{python}} if it's a version at least 2.5 but before 3.0  If this version of python is in that range then role with it, otherwise try python2.6, python2.7, then python2.5 (no change from before).

This is working on my installation, where {{python}} executes python 3.2.2 and doesn't break backwards compatibility to distributions that haven't made the jump to Python3 as default yet.","On Archlinux, where Python3 installations are default (most distros currently use Python2 as default now)

{code}
$ ls -l `which python` 
lrwxrwxrwx 1 root root 7 Nov 21 09:05 /usr/bin/python -> python3
{code}",,,,,,,,,,,,,,,,,,02/Apr/12 21:36;thepaul;4090.patch.txt;https://issues.apache.org/jira/secure/attachment/12521052/4090.patch.txt,27/Mar/12 07:35;ash211;python3-fix.patch;https://issues.apache.org/jira/secure/attachment/12520076/python3-fix.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-02 21:36:55.253,,,no_permission,,,,,,,,,,,,233370,,,Tue Apr 03 16:12:34 UTC 2012,,,,,,0|i0grun:,95942,thepaul,thepaul,,,,,,,,,,27/Mar/12 07:37;ash211;Patch attached.,"02/Apr/12 21:36;thepaul;I would argue that Archlinux is the broken one, in this respect: nearly everything executable in the Python ecosystem still expects an unqualified ""python"" to be python2.

But oh well. This is an easy improvement to make. The logic in the original patch is backward; you want sys.exit to exit with False when the version is good (since False becomes 0, which shell treats as success). Adjusted a little for brevity and inverted logic.","03/Apr/12 02:14;ash211@gmail.com;Paul's patch looks good to me, and I agree that python being a python3
is nonstandard. An easy fix to support though, and flipping the error
code is right too.

Anything else needed before applying the patch to trunk?

Thanks!

On Apr 2, 2012, at 16:37, ""paul cannon (Updated) (JIRA)""

",03/Apr/12 15:36;thepaul;+1 from me.,03/Apr/12 16:12;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFS.setMaxCompactionThreshold doesn't allow 0 unless min is also 0,CASSANDRA-4070,12547350,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Mar/12 08:41,12/Mar/19 14:21,13/Mar/19 22:27,22/Mar/12 12:45,1.0.9,,,,,0,,,,,,,"Thrift allows to set the max compaction threshold to 0 to disable compaction. However, CFS.setMaxCompactionThreshold throws an exception min > max even if max is 0.

Note that even if someone sets 0 for both the min and max thresholds, we'll can have a problem because SizeTieredCompaction calls CFS.setMaxCompactionThreshold before calling CFS.setMinCompactionThreshold and thus will trigger the RuntimeException when it shouldn't.",,,,,,,,,,,,,,,,,,,21/Mar/12 08:47;slebresne;4070.patch;https://issues.apache.org/jira/secure/attachment/12519201/4070.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-21 11:35:46.596,,,no_permission,,,,,,,,,,,,232508,,,Thu Mar 22 12:45:19 UTC 2012,,,,,,0|i0grm7:,95904,jbellis,jbellis,,,,,,,,,,21/Mar/12 08:47;slebresne;Trivial patch attached (against 1.0),"21/Mar/12 11:35;jbellis;+1, but for the record using min/max as ""disable compaction"" signal is kind of broken. ","22/Mar/12 12:45;slebresne;Committed, thanks

I agree that we probably should have a better way to disable compaction. Actually given that leveled compaction pretty much ignore the max and min threshold, I think we should think about moving those to the compaction options.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeveledManifest.maxBytesForLevel calculates wrong for sstable_size_in_mb larger than 512m,CASSANDRA-4263,12556574,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,20/May/12 05:01,12/Mar/19 14:21,13/Mar/19 22:27,21/May/12 23:21,1.1.1,,,,,0,,,,,,,"need to use long math

        if (level == 0)
            return 4 * maxSSTableSizeInMB * 1024 * 1024;",,,,,,,,,,,,,,,,,,,20/May/12 05:01;dbrosius@apache.org;use_long_math.diff;https://issues.apache.org/jira/secure/attachment/12528299/use_long_math.diff,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-21 21:04:33.734,,,no_permission,,,,,,,,,,,,256005,,,Mon May 21 23:20:52 UTC 2012,,,,,,0|i0gtw7:,96273,jbellis,jbellis,,,,,,,,,,21/May/12 21:04;jbellis;+1,21/May/12 21:05;jbellis;Let's go ahead to commit to 1.1 as well.,21/May/12 23:20;dbrosius@apache.org;committed to 1.1 as e515e4b25b4839d8c15fbf0b747185d2fa93ca66,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting column metadata for non-string comparator CFs breaks,CASSANDRA-4269,12556760,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,thobbs,thobbs,21/May/12 21:39,12/Mar/19 14:21,13/Mar/19 22:27,22/May/12 15:13,1.1.1,,,,,0,,,,,,,"For example, use a comparator of LongType and try to create an index on a column named 2 (0x0000000000000002).  You'll get a stracktrace in the logs similar to this:

{noformat}
java.lang.RuntimeException: java.nio.charset.MalformedInputException: Input length = 2
	at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:50)
	at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:115)
	at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1278)
	at org.apache.cassandra.config.CFMetaData.columnMetadata(CFMetaData.java:225)
	at org.apache.cassandra.config.CFMetaData.fromThrift(CFMetaData.java:636)
	at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1061)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3436)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3424)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.nio.charset.MalformedInputException: Input length = 2
	at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
	at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:163)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:120)
	at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:46)
	... 13 more
{noformat}

This works in Cassandra 0.8 and 1.0.",,,,,,,,,,,,,,,,,,,22/May/12 10:35;slebresne;4269.txt;https://issues.apache.org/jira/secure/attachment/12528576/4269.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-21 22:10:26.597,,,no_permission,,,,,,,,,,,,256011,,,Tue May 22 15:13:57 UTC 2012,,,,,,0|i0gtyn:,96284,jbellis,jbellis,,,,,,,,,,21/May/12 22:10;jbellis;related to CASSANDRA-4093?,22/May/12 10:35;slebresne;Patch attached. The column names == utf8 assumption was a bit too hardwired.,22/May/12 14:31;jbellis;I assumed Tyler was referring to Thrift/CLI usage here -- I do think that requiring CQL3 column names to be strings is a Good Thing.,"22/May/12 14:45;slebresne;Agreed. The problem here is really much of an implementation detail. We happen to generate the CFDefinition object used by CQL3 in CFMetatData, and this even if CQL3 is not used per se. So internally it's easier if the code support non-utf8 columns, even though CQL3 won't allow to create them. Basically the patch make sure we correctly convert column names to string when going in the CQL3 side.",22/May/12 14:59;jbellis;+1,"22/May/12 15:13;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kick off background compaction when min/max changed,CASSANDRA-4279,12557067,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,23/May/12 20:51,12/Mar/19 14:21,13/Mar/19 22:27,25/May/12 20:53,1.0.11,1.1.1,,,,0,compaction,,,,,,"When the threshold changes, we may be eligible for a compaction immediately (without waiting for a flush to trigger the eligibility check).",,,,,,,,,,,,,,,,,,,23/May/12 20:54;jbellis;4279.txt;https://issues.apache.org/jira/secure/attachment/12528778/4279.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-24 14:50:23.683,,,no_permission,,,,,,,,,,,,256021,,,Fri May 25 20:53:52 UTC 2012,,,,,,0|i0gu33:,96304,slebresne,slebresne,,,,,,,,,,23/May/12 20:54;jbellis;patch attached.,24/May/12 14:50;slebresne;+1,25/May/12 20:53;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicEndpointSnitch calculates score incorrectly,CASSANDRA-4213,12553700,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,03/May/12 02:59,12/Mar/19 14:21,13/Mar/19 22:27,04/May/12 18:07,1.2.0 beta 1,,,,,0,,,,,,,"updateScore does double = long/long math which calculates the score wrong 1/3 becomes 0.0 not 0.3333


need 1 to be cast to double",,,,,,,,,,,,,,,,,,,04/May/12 01:39;dbrosius@apache.org;4213_use_double_math.diff;https://issues.apache.org/jira/secure/attachment/12525547/4213_use_double_math.diff,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-04 15:26:56.293,,,no_permission,,,,,,,,,,,,237894,,,Fri May 04 18:07:18 UTC 2012,,,,,,0|i0gtav:,96177,brandon.williams,brandon.williams,,,,,,,,,,04/May/12 01:39;dbrosius@apache.org;against trunk,04/May/12 15:26;jbellis;Any reason not to put this in 1.0 and 1.1 branches?,"04/May/12 15:34;dbrosius@apache.org;I believe this code is new

http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java;h=b0249169601add3e9e222348cfbbc9722676cf39;hp=3b80e67ccc52f6de9e1172a974a8faf490bce3c3;hb=0cc97d91c0cf92cd8476b5a5d0bdf7d3d66a45fc;hpb=5b4a7f29980621a162fdc202a17bd3300c20e298","04/May/12 15:36;jbellis;ah, got it.  marking affects: 1.2","04/May/12 18:07;brandon.williams;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Atomicity violation bugs because of misusing concurrent collections,CASSANDRA-4402,12596739,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jacklondongood,jacklondongood,jacklondongood,02/Jul/12 20:40,12/Mar/19 14:21,13/Mar/19 22:27,09/Nov/12 22:09,1.2.0,,,,,0,gossip,,,,,,"My name is Yu Lin. I'm a Ph.D. student in the CS department at
UIUC. I'm currently doing research on mining Java concurrent library
misusages. I found some misusages of ConcurrentHashMap in Cassandra
1.1.1, which may result in potential atomicity violation bugs or harm
the performance.

The code below is a snapshot of the code in file
src/java/org/apache/cassandra/db/Table.java from line 348 to 369

L348        if (columnFamilyStores.containsKey(cfId))
L349        {
L350            // this is the case when you reset local schema
L351            // just reload metadata
L352            ColumnFamilyStore cfs = columnFamilyStores.get(cfId);
L353            assert cfs.getColumnFamilyName().equals(cfName);
            ...
L364        }
L365        else
L366        {
L367            columnFamilyStores.put(cfId, ColumnFamilyStore.createColumnFamilyStore(this, cfName));
L368        }

In the code above, an atomicity violation may occur between line 348
and 352. Suppose thread T1 executes line 348 and finds that the
concurrent hashmap ""columnFamilyStores"" contains the key
""cfId"". Before thread T1 executes line 352, another thread T2 removes
the ""cfId"" key from ""columnFamilyStores"". Now thread T1 resumes
execution at line 352 and will get a null value for ""cfs"". Then the
next line will throw a NullPointerException when invoking the method
on ""cfs"".

Second, the snapshot above has another atomicity violation. Let's look
at lines 348 and 367. Suppose a thread T1 executes line 348 and finds
out the concurrent hashmap does not contain the key ""cfId"". Before it
gets to execute line 367, another thread T2 puts a pair <cfid, v> in
the concurrent hashmap ""columnFamilyStores"". Now thread T1 resumes
execution and it will overwrite the value written by thread T2. Thus,
the code no longer preserves the ""put-if-absent"" semantics.

I found some similar misusages in other files:

In src/java/org/apache/cassandra/gms/Gossiper.java, similar atomicity
violation may occur if thread T2 puts a value to map
""endpointStateMap"" between lines <1094 and 1099>, <1173 and 1178>. Another
atomicity violation may occur if thread T2 removes the value on key
""endpoint"" between lines <681 and 683>.
",,1814400,1814400,,0%,1814400,1814400,,,,,,,,,,,,02/Jul/12 23:03;jbellis;4402-v2.txt;https://issues.apache.org/jira/secure/attachment/12534477/4402-v2.txt,02/Jul/12 22:21;jacklondongood;cassandra-1.1.1-4402.txt;https://issues.apache.org/jira/secure/attachment/12534467/cassandra-1.1.1-4402.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-07-02 23:03:15.946,,,no_permission,,,,,,,,,,,,255954,,,Fri Nov 09 22:11:25 UTC 2012,,,,,,0|i0fw9z:,90827,jbellis,jbellis,,,,,,,,,,02/Jul/12 22:15;jacklondongood;This is the patch that may fix the atomicity violation problem.,"02/Jul/12 23:03;jbellis;Thanks for the patch, Yu.  The getExpireTimeForEndpoint is the most serious since we can fairly easily have concurrent contains with remove calls.  maybeInitializeLocalState shouldn't be called concurrently but it doesn't hurt to clean that up too.

I'm not sure what to do about Table.initCf, though.  It seems like CASSANDRA-2963 has made that inherently unsafe.  A ""mechanical"" fix like the one here won't help since if two CFS objects are created for the same CF, they will conflict on the mbean definition as well.  I'll follow up on that over on the 2963 ticket.

In the meantime, v2 is attached with some cleanup.","08/Nov/12 14:30;slebresne;That v2 patch lgtm on principle from reading the patch, but it'll need to be rebased.",09/Nov/12 22:09;jbellis;committed,09/Nov/12 22:11;jbellis;(with comments explaining the CASSANDRA-2963 situation),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only consider whole row tombstone in collation controller,CASSANDRA-4409,12597426,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,04/Jul/12 09:55,12/Mar/19 14:21,13/Mar/19 22:27,04/Jul/12 17:36,1.2.0 beta 1,,,,,0,,,,,,,"CollationController has that optimization that if it has already seen a row tombstone more recent that a sstable max timestamp, it skips the sstable.  However, this was not updated correctly while introducing range tombstone and currently the code might skip a sstable based on the timestamp of a tombstone that does not cover the full row.",,,,,,,,,,,,,,,,,,,04/Jul/12 09:58;slebresne;0001-Use-only-top-level-row-deletion-to-avoid-sstable-durin.txt;https://issues.apache.org/jira/secure/attachment/12535065/0001-Use-only-top-level-row-deletion-to-avoid-sstable-durin.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-04 15:28:22.426,,,no_permission,,,,,,,,,,,,256136,,,Wed Jul 04 17:36:04 UTC 2012,,,,,,0|i0gvkf:,96544,jbellis,jbellis,,,,,,,,,,"04/Jul/12 09:58;slebresne;Patch attached to fix. I will note that in practice this was not really a bug because the deletionInfo used were those of the columnIterator, and at that point those deletionInfo should only contain whole row tombstone, not range tombstone. Yet, the code was misleading and could have trigger a bug if the code change underneath.",04/Jul/12 15:28;jbellis;+1,"04/Jul/12 17:36;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SystemTable.setBootstrapState always sets bootstrap state to true,CASSANDRA-4460,12599929,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,dbrosius@apache.org,dbrosius@apache.org,24/Jul/12 00:45,12/Mar/19 14:21,13/Mar/19 22:27,26/Jul/12 01:38,,,,,,0,,,,,,,"    public static void setBootstrapState(BootstrapState state)
    {
        String req = ""INSERT INTO system.%s (key, bootstrapped) VALUES ('%s', '%b')"";
        processInternal(String.format(req, LOCAL_CF, LOCAL_KEY, getBootstrapState()));
        forceBlockingFlush(LOCAL_CF);
    }

Third parameter %b is set from getBootstrapState() which returns an enum, thus %b collapses to null/non null checks. This would seem then to always set it to true.",,,,,,,,,,,,,,,,,,,25/Jul/12 21:52;brandon.williams;4460.txt;https://issues.apache.org/jira/secure/attachment/12537905/4460.txt,25/Jul/12 00:38;dbrosius@apache.org;use_bootstrap_enum_strings.txt;https://issues.apache.org/jira/secure/attachment/12537782/use_bootstrap_enum_strings.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-07-24 20:43:12.046,,,no_permission,,,,,,,,,,,,256176,,,Thu Jul 26 01:38:15 UTC 2012,,,,,,0|i0gw4f:,96634,dbrosius@apache.org,dbrosius@apache.org,,,,,,,,,,"24/Jul/12 20:43;brandon.williams;Actually, it breaks, and buildbot knows it:

{noformat}

ERROR [main] 2012-07-24 14:31:19,156 CassandraDaemon.java (line 335) Exception encountered during startup
java.lang.IndexOutOfBoundsException
        at java.nio.Buffer.checkIndex(Buffer.java:520)
        at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:340)
        at org.apache.cassandra.utils.ByteBufferUtil.toInt(ByteBufferUtil.java:414)
        at org.apache.cassandra.cql.jdbc.JdbcInt32.compose(JdbcInt32.java:94)
        at org.apache.cassandra.db.marshal.Int32Type.compose(Int32Type.java:33)
        at org.apache.cassandra.cql3.UntypedResultSet$Row.getInt(UntypedResultSet.java:104)
        at org.apache.cassandra.db.SystemTable.getBootstrapState(SystemTable.java:375)
        at org.apache.cassandra.db.SystemTable.setBootstrapState(SystemTable.java:391)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:691)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:476)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:367)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:228)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:318)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:361)
{noformat}

Obviously we should be using %i instead.","25/Jul/12 00:26;dbrosius@apache.org;switch the systemtable.bootstrap field to be text and hold BootstrapState.name() so that the schema is more readable and easier to mutate in the future, if needed.

I'm not sure how upgrades from old schema generally is handled, that part still needs to be added to the patch.","25/Jul/12 17:51;brandon.williams;While doing this would certainly prevent anyone from wanting to bikeshed the names in the future, it doesn't seem any less fragile, nor worth the complexity of migrating the system table schema.","25/Jul/12 20:44;dbrosius@apache.org;it has to be migrated anyway. The table is defined to be boolean currently. So either you migrate to integer or string. I chose string as 0, 1, 2 mean nothing to me.","25/Jul/12 21:52;brandon.williams;bq. it has to be migrated anyway. The table is defined to be boolean currently

Actually, it's only boolean in trunk and we don't need to keep trunk compatible with itself.  It turns out upgradeSystemData() is handling the 1.1 to trunk transition for us already.

bq.  I chose string as 0, 1, 2 mean nothing to me.

Fair enough.  Attaching a new version which takes all of this into account, and fixes a bug in setBootstrapState using getBootstrapState instead of the state passed to it.",26/Jul/12 01:25;dbrosius@apache.org;LGTM,26/Jul/12 01:38;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix online help in cqlsh for COPY commands,CASSANDRA-4469,12600569,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,28/Jul/12 18:55,12/Mar/19 14:21,13/Mar/19 22:27,28/Jul/12 21:03,1.1.3,,Legacy/Tools,,,0,cqlsh,,,,,,"the HELP COPY output from cqlsh says:

{noformat}
COPY [cqlsh only]

  Imports CSV data into a Cassandra table.

COPY <table_name> [ ( column [, ...] ) ]
     FROM ( '<filename>' | STDIN )
     [ WITH <option>='value' [AND ...] ];
COPY <table_name> [ ( column [, ...] ) ]
     TO ( '<filename>' | STDOUT )
     [ WITH <option>='value' [AND ...] ];
{noformat}

It's confusing cause COPY is now for both export and import, since CASSANDRA-4434.",,,,,,,,,,,,,,,,,,,28/Jul/12 18:56;thepaul;4469.txt;https://issues.apache.org/jira/secure/attachment/12538251/4469.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-28 21:03:51.352,,,no_permission,,,,,,,,,,,,256181,,,Sat Jul 28 21:03:51 UTC 2012,,,,,,0|i0gw73:,96646,brandon.williams,brandon.williams,,,,,,,,,,28/Jul/12 18:56;thepaul;fix the docstring/help text for COPY,28/Jul/12 21:03;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using 'key' as primary key throws exception when using CQL2,CASSANDRA-4475,12600870,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,yukim,yukim,31/Jul/12 15:01,12/Mar/19 14:21,13/Mar/19 22:27,31/Jul/12 15:37,1.2.0 beta 1,,,,,0,cql,,,,,,"When I run following CQL on trunk, throws exception (only in CQL2). This statement used to work and I think something is broken after CASSANDRA-4179.

{code}
CREATE TABLE Standard1 (key ascii PRIMARY KEY, c0 ascii);
{code}

Exception is:

{code}
ERROR [Thrift:1] 2012-07-31 09:54:02,585 CustomTThreadPoolServer.java (line 202) Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:166)
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:123)
        at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
        at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:49)
        at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:45)
        at org.apache.cassandra.cql3.CFDefinition.getKeyId(CFDefinition.java:167)
        at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:81)
        at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1382)
        at org.apache.cassandra.config.CFMetaData.keyAliases(CFMetaData.java:235)
        at org.apache.cassandra.cql.CreateColumnFamilyStatement.getCFMetaData(CreateColumnFamilyStatement.java:170)
        at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:692)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:846)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}",,,,,,,,,,,,,,,,,,,31/Jul/12 15:09;slebresne;4445.txt;https://issues.apache.org/jira/secure/attachment/12538560/4445.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-31 15:09:46.874,,,no_permission,,,,,,,,,,,,256187,,,Tue Jul 31 15:37:06 UTC 2012,,,,,,0|i0gw9r:,96658,yukim,yukim,,,,,,,,,,"31/Jul/12 15:09;slebresne;Right, forgot that CQL2 can have a null keyAlias. Trivial fix attached.",31/Jul/12 15:31;yukim;+1,"31/Jul/12 15:37;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMX attribute setters not consistent with cassandra.yaml,CASSANDRA-4479,12601104,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cmerrill,edong,edong,01/Aug/12 18:51,12/Mar/19 14:21,13/Mar/19 22:27,23/Aug/12 16:01,1.2.0 beta 1,,,,,0,,,,,,,"If a setting is configurable both via cassandra.yaml and JMX, the two should be consistent. If that doesn't hold, then the JMX setter can't be trusted. 

Here I present the example of phi_convict_threshold.

I'm trying to set phi_convict_threshold via JMX, which sets FailureDetector.phiConvictThreshold_, but this doesn't update Config.phi_convict_threshold, which gets its value from cassandra.yaml when starting up.

Some places, such as FailureDetector.interpret(InetAddress), use FailureDetector.phiConvictThreshold_; others, such as AntiEntropyService.line 813 in cassandra-1.1.2, use Config.phi_convict_threshold:
{code}
            // We want a higher confidence in the failure detection than usual because failing a repair wrongly has a high cost.
            if (phi < 2 * DatabaseDescriptor.getPhiConvictThreshold())
                return;
{code}

where DatabaseDescriptor.getPhiConvictThreshold() returns Conf.phi_convict_threshold.


So, it looks like there are cases where a value is stored in multiple places, and setting the value via JMX doesn't set all of them. I'd say there should only be a single place where a configuration parameter is stored, and that single field:
* should read in the value from cassandra.yaml, optionally falling back to a sane default
* should be the field that the JMX attribute reads and sets, and
* any place that needs the current global setting should get it from that field. However, there could be cases where you read in a global value at the start of a task and keep that value locally until the end of the task.

Also, anything settable via JMX should be volatile or set via a synchronized setter, or else according to the Java memory model other threads may be stuck with the old setting.


So, I'm requesting the following:
* Setting up guidelines for how to expose a configuration parameter both via cassandra.yaml and JMX, based on what I've mentioned above
* Going through the list of configuration parameters and fixing any that don't match those guidelines


I'd also recommend logging any changes to configuration parameters.",,,,,,,,,,,,,,,,,,,20/Aug/12 17:59;cmerrill;trunk-4479.txt;https://issues.apache.org/jira/secure/attachment/12541616/trunk-4479.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-03 21:09:32.438,,,no_permission,,,,,,,,,,,,256189,,,Thu Aug 23 16:01:41 UTC 2012,,,,,,0|i0gwav:,96663,brandon.williams,brandon.williams,,,,,,,,,,"01/Aug/12 19:02;edong;For comparison, rpc_timeout_in_ms is settable through JMX via StorageProxy[MBean], but StorageProxy doesn't have its own rpc_timeout_in_ms field, it calls DatabaseDescriptor.setRpcTimeout(Long), which sets Conf.rpc_timeout_in_ms. However, Conf.rpc_timeout_in_ms is neither volatile nor set via a synchronized method, which is still bad.","01/Aug/12 23:48;edong;A quick-and-dirty laundry list of MBean setters:

{noformat}
$ find . -name '*MBean.java' -exec grep 'void set' {} +
./src/java/org/apache/cassandra/concurrent/JMXConfigurableThreadPoolExecutorMBean.java:    void setCorePoolSize(int n);
./src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java:    public void setMinimumCompactionThreshold(int threshold);
./src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java:    public void setMaximumCompactionThreshold(int threshold);
./src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java:    public void setCompactionStrategyClass(String className) throws ConfigurationException;
./src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java:    public void setCompressionParameters(Map<String,String> opts) throws ConfigurationException;
./src/java/org/apache/cassandra/gms/FailureDetectorMBean.java:    public void setPhiConvictThreshold(int phi);
./src/java/org/apache/cassandra/service/CacheServiceMBean.java:    public void setRowCacheSavePeriodInSeconds(int rcspis);
./src/java/org/apache/cassandra/service/CacheServiceMBean.java:    public void setKeyCacheSavePeriodInSeconds(int kcspis);
./src/java/org/apache/cassandra/service/CacheServiceMBean.java:    public void setRowCacheCapacityInMB(long capacity);
./src/java/org/apache/cassandra/service/CacheServiceMBean.java:    public void setKeyCacheCapacityInMB(long capacity);
./src/java/org/apache/cassandra/service/StorageProxyMBean.java:    public void setHintedHandoffEnabled(boolean b);
./src/java/org/apache/cassandra/service/StorageProxyMBean.java:    public void setMaxHintWindow(int ms);
./src/java/org/apache/cassandra/service/StorageProxyMBean.java:    public void setMaxHintsInProgress(int qs);
./src/java/org/apache/cassandra/service/StorageProxyMBean.java:    public void setRpcTimeout(Long timeoutInMillis);
./src/java/org/apache/cassandra/service/StorageServiceMBean.java:    public void setLog4jLevel(String classQualifier, String level);
./src/java/org/apache/cassandra/service/StorageServiceMBean.java:    public void setStreamThroughputMbPerSec(int value);
./src/java/org/apache/cassandra/service/StorageServiceMBean.java:    public void setCompactionThroughputMbPerSec(int value);
./src/java/org/apache/cassandra/service/StorageServiceMBean.java:    public void setIncrementalBackupsEnabled(boolean value);
{noformat}

DatabaseDescriptor setters; according to [ArchitectureInternals|http://wiki.apache.org/cassandra/ArchitectureInternals], all node configuration parameters should be in here:
{noformat}
$ grep 'void set' src/java/org/apache/cassandra/config/DatabaseDescriptor.java 
    public static void setPartitioner(IPartitioner newPartitioner)
    public static void setEndpointSnitch(IEndpointSnitch eps)
    public static void setRpcTimeout(Long timeOutInMillis)
    public static void setInMemoryCompactionLimit(int sizeInMB)
    public static void setCompactionThroughputMbPerSec(int value)
    public static void setStreamThroughputOutboundMegabitsPerSec(int value)
    public static void setBroadcastAddress(InetAddress broadcastAdd)
    public static void setDynamicUpdateInterval(Integer dynamicUpdateInterval)
    public static void setDynamicResetInterval(Integer dynamicResetInterval)
    public static void setDynamicBadnessThreshold(Double dynamicBadnessThreshold)
    public static void setIncrementalBackupsEnabled(boolean value)
{noformat}",03/Aug/12 21:09;cmerrill;I'm looking into this and will submit a patch for consideration.,"23/Aug/12 16:01;brandon.williams;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After changing the compaction strategy, compression_strategy  always returning back to the ""SnappyCompressor"" through CQL 2.2.0",CASSANDRA-4996,12617744,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,shamim_ru,shamim_ru,27/Nov/12 08:06,12/Mar/19 14:21,13/Mar/19 22:27,13/Dec/12 19:50,1.1.8,1.2.0,,,,0,,,,,,,"faced very strange behaviour when changing compression_parameters of exisiting CF. After changing the compaction strategy, compression_strategy returning back to the ""SnappyCompressor"".

Using cassandra version 1.1.5.
[cqlsh 2.2.0 | Cassandra 1.1.5 | CQL spec 2.0.0 | Thrift protocol 19.32.0]
I have one column family with following paramters:

cqlsh > describe columnfamily auditlog_01;
CREATE TABLE auditlog_01 (
lid text PRIMARY KEY,
dscn text,
asid text,
soapa text
) WITH
comment='' AND
comparator=text AND
read_repair_chance=0.100000 AND
gc_grace_seconds=864000 AND
default_validation=text AND
min_compaction_threshold=4 AND
max_compaction_threshold=32 AND
replicate_on_write='true' AND
compaction_strategy_class='SizeTieredCompactionStrategy' AND
compaction_strategy_options:sstable_size_in_mb='5' AND
compression_parameters:sstable_compression='SnappyCompressor';

Changing compression strategy to 'DeflateCompressor

cqlsh> ALTER TABLE auditlog_01 WITH compression_parameters:sstabl
e_compression = 'DeflateCompressor' AND compression_parameters:chunk_length_kb =
 64;
cqlsh> describe columnfamily auditlog_01;

CREATE TABLE auditlog_01 (
lid text PRIMARY KEY,
dscn text,
asid text,
soapa text
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='5' AND
  compression_parameters:chunk_length_kb='64' AND
  compression_parameters:sstable_compression='DeflateCompressor';

it's sucessfuly changed the compression strategy to 'DeflateCompressor, after that when i am trying to change the compaction strategy, compression strategy returing back to ""SnappyCompressor"".
cqlsh> alter table auditlog_01 with compaction_strategy_class='Le
veledCompactionStrategy' AND compaction_strategy_options:sstable_size_in_mb=5;
cqlsh> describe columnfamily auditlog_01;

CREATE TABLE auditlog_01 (
  lid text PRIMARY KEY,
dscn text,
asid text,
soapa text
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='5' AND
  compression_parameters:sstable_compression='SnappyCompressor';",,,,,,,,,,,,,,,,,,,10/Dec/12 18:30;iamaleksey;4996-1.1.txt;https://issues.apache.org/jira/secure/attachment/12560232/4996-1.1.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-28 05:45:23.833,,,no_permission,,,,,,,,,,,,292260,,,Fri Dec 14 09:03:46 UTC 2012,,,,,,0|i0rsvz:,160324,jbellis,jbellis,,,,,,,,,,"28/Nov/12 05:45;derek.bromenshenkel;I've run into this in the past, also.

It was introduced in 1.1 when compression was turned on by default.  The problem is that CFPropDefs class is shared by both CREATE and ALTER statements' code path and to provide the ""default on"" functionality, the sstable_compression value is statically set inside this class.  Then, because the ALTER statement provided does not define the value (and thus override the default), it ends up switching it back to the default.  This is also present in CQL 3 as far as I know.  I think the fix will be to remove the defaulting from CFPropDefs and add it over in the CREATE code path, since that is where the ""default on"" behavior is needed.

Also, notice that on your ALTER command, it did not actually change the compaction_strategy_class as you requested, but that is probably another issue.",10/Dec/12 18:33;iamaleksey;Patch for 1.2 is similar - not sure whether it should go into 1.2.0 or rc1.,11/Dec/12 22:32;jbellis;Can you summarize the fix?,"11/Dec/12 22:37;iamaleksey;Basically what [~derek.bromenshenkel] suggested:
1) Remove the default strategy from CFPropDefs
2) Set default strategy in CreateColumnFamilyStatement
3) Leave AlterTableStatement as is (mostly, cql3 version of it needed a slight fix)",13/Dec/12 16:55;jbellis;+1,"13/Dec/12 19:50;slebresne;If I trust the changelog this has been committed, so closing.","13/Dec/12 20:05;iamaleksey;Thanks, committed.",14/Dec/12 09:03;shamim_ru;thanks a lot - will make a try,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"in cqlsh, alter table with compaction_strategy_class does nothing",CASSANDRA-4965,12616167,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jeromatron,jeromatron,15/Nov/12 00:18,12/Mar/19 14:21,13/Mar/19 22:27,19/Nov/12 10:12,1.2.0 beta 3,,,,,0,,,,,,,"The following cqlsh code appears to do nothing.
{code}
alter table blah with compaction_strategy_class = ‘LeveledCompactionStrategy’;
{code}

It completes as though it worked but when you describe the table, it's still STCS.",,,,,,,,,,,,,,,,,,,19/Nov/12 01:42;iamaleksey;4965.txt;https://issues.apache.org/jira/secure/attachment/12554105/4965.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-19 01:45:20.673,,,no_permission,,,,,,,,,,,,257914,,,Mon Nov 19 08:48:53 UTC 2012,,,,,,0|i0kktr:,118191,jbellis,jbellis,,,,,,,,,,19/Nov/12 01:45;iamaleksey;It's not a cqlsh issue. It's a CQL2 AlterTableStatement bug. Affects CQL2 1.1 and 1.2. Doesn't affect CQL3 in either C* version.,19/Nov/12 08:48;jbellis;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use CF comparator to sort indexed columns in SecondaryIndexManager,CASSANDRA-4365,12595431,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Jun/12 13:57,12/Mar/19 14:21,13/Mar/19 22:27,29/Jun/12 09:27,1.1.2,,,,,0,,,,,,,"SecondaryIndexManager is supposed to have it's internal map sorted according to the base CF comparator, but instead it sorts using the byte buffer natural ordering.

This order is carried along by the sorted set returned by getIndexedColumns(), which in turns end up in a NamesQueryFilter when reading indexed columns, so the order should really be the CF one.

I'll note that I don't think this is a bug because SSTableNamesIterator don't in fact rely on the actual ordering of the names. But it's worth fixing to avoid future problems.",,,,,,,,,,,,,,,,,,,21/Jun/12 13:57;slebresne;4365.txt;https://issues.apache.org/jira/secure/attachment/12532871/4365.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-21 16:57:31.066,,,no_permission,,,,,,,,,,,,256097,,,Fri Jun 29 09:27:35 UTC 2012,,,,,,0|i0gv27:,96462,jbellis,jbellis,,,,,,,,,,21/Jun/12 16:57;jbellis;+1,29/Jun/12 09:27;slebresne;Forgot to close that one but the patch has been committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression params validation assumes SnappyCompressor,CASSANDRA-5066,12623832,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,iamaleksey,iamaleksey,13/Dec/12 21:24,12/Mar/19 14:21,13/Mar/19 22:27,14/Dec/12 15:54,1.1.8,1.2.0,,,,0,,,,,,,"This hasn't caused any issues yet since DeflateCompressor and SnappyCompressor have the same empty set for supportedOptions, but is a potential issue.

Combined with CASSANDRA-4996 this also brings back CASSANDRA-4266.",,,,,,,,,,,,,,,,,,,14/Dec/12 01:33;iamaleksey;5066-1.1.txt;https://issues.apache.org/jira/secure/attachment/12560898/5066-1.1.txt,14/Dec/12 10:53;slebresne;5066-v2.txt;https://issues.apache.org/jira/secure/attachment/12560949/5066-v2.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-14 10:53:39.215,,,no_permission,,,,,,,,,,,,297552,,,Fri Dec 14 15:54:59 UTC 2012,,,,,,0|i14rcf:,235895,iamaleksey,iamaleksey,,,,,,,,,,"14/Dec/12 10:53;slebresne;I prefer the simpler/less hackish solution attached as v2. I.e. if no compressor class has been provided, you should have no options, period.",14/Dec/12 15:33;iamaleksey;+1,"14/Dec/12 15:54;slebresne;Alright, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix OOM with ReadMessageTest.testNoCommitLog,CASSANDRA-4312,12559485,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius@apache.org,dbrosius@apache.org,dbrosius@apache.org,06/Jun/12 04:06,12/Mar/19 14:21,13/Mar/19 22:27,07/Jun/12 02:41,1.2.0 beta 1,,Legacy/Testing,,,0,,,,,,,"this test can throw OOMs, because it uses a FileReader and readLine to read the commit log. However, some commit logs are fully allocated, but not initialized, (all 0s) so finding an EOL means reading 134M of data. Even for commit logs that have data they really aren't filereader-type streams.

changed to do simple byte finding in the streams instead.",,,,,,,,,,,,,,,,,,,06/Jun/12 04:07;dbrosius@apache.org;fix_oom_in_readmessagetest.txt;https://issues.apache.org/jira/secure/attachment/12531061/fix_oom_in_readmessagetest.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-06 16:08:32.665,,,no_permission,,,,,,,,,,,,256051,,,Thu Jun 07 02:41:32 UTC 2012,,,,,,0|i0gugf:,96364,yukim,yukim,,,,,,,,,,06/Jun/12 16:08;yukim;+1 with nit: you need to remove whitespaces on empty row.,07/Jun/12 02:41;dbrosius@apache.org;committed to trunk as commit de6dba73352b8406452bd2b0ab792e9af817c901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
be consistent in visible messages about DOWN versus dead state,CASSANDRA-5187,12629321,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,rcoli,rcoli,25/Jan/13 21:14,12/Mar/19 14:21,13/Mar/19 22:27,23/Mar/13 23:08,1.2.4,,,,,0,,,,,,,"Cassandra has two states that seem to be used interchangeably in some messages.

1) ""DOWN"" state, which simply means that the node has not responded in the last Gossip round.
2) ""dead"" which means a state listed in DEAD_STATES in Gossiper.java.

{noformat}
static final List<String> DEAD_STATES = Arrays.asList(VersionedValue.REMOVING_TOKEN, VersionedValue.REMOVED_TOKEN,VersionedValue.STATUS_LEFT, VersionedValue.HIBERNATE);
{noformat}

However, it seems to use the terms incorrectly in the following places :

a) the log message :
""
logger.info(""InetAddress {} is now dead."", addr);
""

this is in Gossiper::markDead (which ideally would also be renamed, because it does not seem to put anything in one of the DEAD_STATES..)

but in Gossiper::markAlive the paired log message is :
""
        logger.info(""InetAddress {} is now UP"", addr);
""

If being marked alive means you are UP, then being marked dead should mean you are DOWN.

b) NodeToolHelp.yaml has :
""
Disable gossip (effectively marking the node dead)
""

Attached is a patch against trunk which changes both ""dead""s to ""DOWN"".",,,,,,,,,,,,,,,,,,,25/Jan/13 21:15;rcoli;DOWN.vs.dead.patch;https://issues.apache.org/jira/secure/attachment/12566558/DOWN.vs.dead.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-23 23:08:14.567,,,no_permission,,,,,,,,,,,,309469,,,Sat Mar 23 23:08:14 UTC 2013,,,,,,0|i1fawf:,297527,jbellis,jbellis,,,,,,,,,,23/Mar/13 23:08;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy throws NPEs for when there's no hostids for a target,CASSANDRA-4227,12554426,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,dbrosius@apache.org,dbrosius@apache.org,09/May/12 04:04,12/Mar/19 14:21,13/Mar/19 22:27,23/May/12 20:58,1.2.0 beta 1,,,,,0,,,,,,,"On trunk...

if there is no host id due to an old node, an info log is generated, but the code continues to use the null host id causing NPEs in decompose... Should this bypass this code, or perhaps can the plain ip address be used in this case? don't know.

as follows...



                    UUID hostId = StorageService.instance.getTokenMetadata().getHostId(target);
                    if ((hostId == null) && (Gossiper.instance.getVersion(target) < MessagingService.VERSION_12))
                        logger.info(""Unable to store hint for host with missing ID, {} (old node?)"", target.toString());
                    RowMutation hintedMutation = RowMutation.hintFor(mutation, ByteBuffer.wrap(UUIDGen.decompose(hostId)));
                    hintedMutation.apply();
",,,,,,,,,,,,,,,,,,,22/May/12 21:47;urandom;4227_drop_hints.txt;https://issues.apache.org/jira/secure/attachment/12528656/4227_drop_hints.txt,22/May/12 00:30;dbrosius@apache.org;4227_guard_against_npes_for_old_gossip_versions.diff;https://issues.apache.org/jira/secure/attachment/12528538/4227_guard_against_npes_for_old_gossip_versions.diff,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-09 14:28:39.554,,,no_permission,,,,,,,,,,,,238668,,,Wed May 23 20:58:48 UTC 2012,,,,,,0|i0gth3:,96205,jbellis,jbellis,,,,,,,,,,"09/May/12 14:28;brandon.williams;We should just drop the hint in this case, see CASSANDRA-4120",22/May/12 00:30;dbrosius@apache.org;against trunk,"22/May/12 15:24;jbellis;So the situation is, I've upgraded to 1.2 so I'm supposed to have node IDs, but one of the nodes starts slower than the others (for instance) so it hasn't broadcast its nodeid to the rest of the cluster yet?","22/May/12 15:47;brandon.williams;I believe the more common situation is you're in the process of upgrading to 1.2 and have a mixed cluster, but need to generate a hint for an older version that hasn't generated a nodeid yet.","22/May/12 21:46;urandom;bq. We should just drop the hint in this case, see CASSANDRA-4120

Yeah, I think this is a straightforward brainfart and that it was supposed to have returned after logging ""cannot store hint"", instead of you know, trying to store it. :)

See attached.","22/May/12 22:16;jbellis;Hints are supposed to be reliable now, which I expect will lead more people to turn off read repair.  Which is a long way of saying I think we should
- log the message at WARN
- add a note to NEWS that you should upgrade when all nodes are up to minimize this

+1 otherwise.",22/May/12 23:52;dbrosius@apache.org;committed as bd32d4f0b9f0f88fed97e8ddf2ee41b5b048d31d,"23/May/12 00:45;urandom;I reverted this.

I'm not sure which patch Jonathan was +1'ing, but the what was committed here wasn't from either.

I don't think the test for version < VERSION_12 should be removed.  A null hostid is ""normal"" when the node is pre-1.2 (as would be the case during a rolling upgrade).  I can't foresee a reason that it would be null otherwise, it would represent a much more serious error and shouldn't be swept away with a WARN message.

Also, Jonathan wanted to see something added to NEWS.  The second bullet point under ""Upgrading"" for 1.2 already mentions this, but it would probably be better to strengthen that wording with a recommendation to only upgrade when all nodes are up.","23/May/12 01:49;dbrosius@apache.org;ok, but what ever the reason for a null host id, it's going to NPE.","23/May/12 03:23;jbellis;Sorry for the confusion, I was +1ing the 2nd patch.

I'm okay with NPEing if we have a null host on version 12, since that's Not Supposed To Happen.  (Alternatively, we could add an assert to make that explicit.)",23/May/12 20:58;urandom;committed with assertion and additional NEWS.txt recommendation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Have Cassandra return the right error for keyspaces with dots,CASSANDRA-4721,12609127,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,j.casares,j.casares,25/Sep/12 21:02,12/Mar/19 14:21,13/Mar/19 22:27,25/Sep/12 21:51,1.1.6,,,,,0,,,,,,,"cqlsh> CREATE KEYSPACE 'solr.test' WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
Bad Request: Invalid keyspace name: shouldn't be empty nor more than 48 characters long (got ""solr.test"")",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-09-25 21:46:29.097,,,no_permission,,,,,,,,,,,,255207,,,Tue Sep 25 21:51:56 UTC 2012,,,,,,0|i0eq27:,83984,,,,,,,,,,,,25/Sep/12 21:46;jbellis;Not a cqlsh issue.,25/Sep/12 21:51;jbellis;committed fix in 05a5ede91ac558998f93024439185fdd1e04345e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing String.format() in AntiEntropyService.java logs,CASSANDRA-4574,12604687,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius@apache.org,julienlambert,julienlambert,23/Aug/12 21:55,12/Mar/19 14:21,13/Mar/19 22:27,24/Aug/12 03:56,1.0.12,1.1.5,,,,0,,,,,,,"A String.format() is missing in AntiEntropyService.java (line 625 in 1.2). 
This is what is written to the logs: AntiEntropyService.java (line 625) \[repair #%s] No neighbors to repair with on range %s: session completed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-08-24 03:55:54.213,,,no_permission,,,,,,,,,,,,256259,,,Fri Aug 24 03:55:54 UTC 2012,,,,,,0|i0gxa7:,96822,,,,,,,,,,,,"24/Aug/12 03:55;dbrosius@apache.org;thanks
committed to cassandra-1.0 as ec76baf0dd2a976542106cd5e58652c3d36ffd23",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No entry in TypesMap for InetAddressType,CASSANDRA-4878,12614024,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ardot,ardot,ardot,30/Oct/12 14:03,12/Mar/19 14:21,13/Mar/19 22:27,30/Oct/12 14:43,1.2.0 beta 2,,Legacy/CQL,,,0,,,,,,,{{InetAddressType}} was added to {{o.a.c.db.marshal}} and {{JdbcInetAddress}} was added to {{o.a.c.cql.jdbc}} but no bridging entry was made for it in the {{o.a.c.cql.jdbc.TypesMap}} Class.,,,,,,,,,,,,,,,,,,,30/Oct/12 14:06;ardot;4878-v1.text;https://issues.apache.org/jira/secure/attachment/12551355/4878-v1.text,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-30 14:43:09.969,,,no_permission,,,,,,,,,,,,253146,,,Tue Oct 30 14:43:09 UTC 2012,,,,,,0|i0dbz3:,75869,jbellis,jbellis,,,,,,,,,,"30/Oct/12 14:43;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typo in tuning cassandra doc,CASSANDRA-4849,12613020,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,mkjellman,mkjellman,22/Oct/12 18:56,12/Mar/19 14:20,13/Mar/19 22:27,22/Oct/12 19:04,,,Legacy/Documentation and Website,,,0,,,,,,,"http://www.datastax.com/docs/1.1/operations/tuning#tuning-bloomfilters has a typo

ALTER TABLE addamsFamily WITH bloomfilter_fp_chance = 0.01; should be ALTER TABLE addamsFamily WITH bloom_filter_fp_chance = 0.01;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-10-22 19:04:22.159,,,no_permission,,,,,,,,,,,,250394,,,Mon Oct 22 19:04:22 UTC 2012,,,,,,0|i0ay3r:,61798,,,,,,,,,,,,"22/Oct/12 19:04;jbellis;thanks, emailed docs@datastax.com",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] guard against npe due to null sstable,CASSANDRA-4056,12546699,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,16/Mar/12 00:48,12/Mar/19 14:20,13/Mar/19 22:27,16/Mar/12 10:28,1.1.1,,,,,0,,,,,,,SSTableIdentityIterator ctor can be called from sibling ctor with a null sstable. So catch block's markSuspect should be npe guarded.,,,,,,,,,,,,,,,,,,,16/Mar/12 00:49;dbrosius@apache.org;npe_guard.diff;https://issues.apache.org/jira/secure/attachment/12518587/npe_guard.diff,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-16 10:28:20.044,,,no_permission,,,,,,,,,,,,231857,,,Fri Mar 16 10:28:20 UTC 2012,,,,,,0|i0grfr:,95875,slebresne,slebresne,,,,,,,,,,16/Mar/12 00:49;dbrosius@apache.org;against trunk,"16/Mar/12 10:28;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
default cache provider does not match default yaml,CASSANDRA-4828,12612327,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,17/Oct/12 22:18,12/Mar/19 14:20,13/Mar/19 22:27,18/Oct/12 01:49,1.1.7,,,,,0,,,,,,,"The yaml indicates SerializingCacheProvider is the default, however the default when not present in the yaml is actually ConcurrentLinkedHashCacheProvider.",,,,,,,,,,,,,,,,,,,17/Oct/12 22:22;brandon.williams;4828.txt;https://issues.apache.org/jira/secure/attachment/12549592/4828.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-18 01:42:08.274,,,no_permission,,,,,,,,,,,,249423,,,Thu Oct 18 01:49:55 UTC 2012,,,,,,0|i0a85z:,57596,jbellis,jbellis,,,,,,,,,,18/Oct/12 01:42;jbellis;+1,18/Oct/12 01:49;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exclude system_traces from repair,CASSANDRA-4956,12615929,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,brandon.williams,brandon.williams,13/Nov/12 18:04,12/Mar/19 14:20,13/Mar/19 22:27,15/Nov/12 17:01,1.2.0 beta 3,,,,,0,,,,,,,"When a repair is issued, the system ks is skipped but not system_traces.",,,,,,,,,,,,,,,,,,,14/Nov/12 19:35;yukim;4956-1.2.0.txt;https://issues.apache.org/jira/secure/attachment/12553533/4956-1.2.0.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-13 18:57:15.704,,,no_permission,,,,,,,,,,,,257509,,,Thu Nov 15 17:01:36 UTC 2012,,,,,,0|i0k2x3:,115290,slebresne,slebresne,,,,,,,,,,"13/Nov/12 18:57;jbellis;Can we generalize this to ""don't bother doing validation compactions for RF=1?""","14/Nov/12 19:35;yukim;Patch attached for 1.2.0 branch.
Cassandra already skips repair when there is no endpoint to repair with. This patch goes further and makes it skip queueing repair session.
I also did refactoring around various repair methods.",15/Nov/12 10:29;slebresne;+1,15/Nov/12 17:01;yukim;Committed. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
doc/native_protocol.txt isn't up to date,CASSANDRA-4737,12609728,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,tux21b,tux21b,01/Oct/12 09:22,12/Mar/19 14:20,13/Mar/19 22:27,01/Oct/12 14:34,1.2.0 beta 2,,,,,0,binary_protocol,,,,,,CASSANDRA-4449 seems to have changed the datatype of the query id returned by a RESULT-PREPARED message from an {{int}} to a {{short}} n followed by n bytes (representing a md5sum). The specification at doc/native_protocol.txt doesn't cover this change yet.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-10-01 10:21:29.111,,,no_permission,,,,,,,,,,,,255196,,,Mon Oct 01 14:34:23 UTC 2012,,,,,,0|i0epzr:,83973,,,,,,,,,,,,01/Oct/12 10:21;slebresne;Fixed in commit 76107078438211816153722b86f8bf2c49b34ddc. Thanks.,"01/Oct/12 12:26;tux21b;Thanks for looking into this. Unfortunately, the specification is still wrong in my opinion:

[short]        A 2 bytes unsigned integer
[short bytes]  A [short] n, followed by n bytes if n >= 0. If n < 0, no byte should follow and the value represented is `null`.

I'm not entirely sure how I should represent those n<0 using an unsigned integer.","01/Oct/12 14:34;slebresne;Oups, fixed in commit 859473db505bd4e5953f69f06066af815da9a0ce.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress scripts should be executable,CASSANDRA-4302,12558854,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,thobbs,thobbs,31/May/12 20:15,12/Mar/19 14:20,13/Mar/19 22:27,31/May/12 20:38,1.1.1,1.2.0 beta 1,Legacy/Tools,,,0,,,,,,,Just need to {{chmod u+x tools/bin/cassandra-stress tools/bin/cassandra-stressd}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-31 20:38:29.666,,,no_permission,,,,,,,,,,,,256042,,,Thu May 31 20:38:29 UTC 2012,,,,,,0|i0gucn:,96347,,,,,,,,,,,,"31/May/12 20:19;thobbs;I suppose tools/bin/sstablemetadata needs to be executable, as well.","31/May/12 20:38;yukim;Committed in a5e0994a130dac325dfde8b8d258b14677788c60, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: Tab completion should not suggest consistency level ANY for select statements,CASSANDRA-4074,12547710,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,tpatterson,tpatterson,22/Mar/12 23:30,12/Mar/19 14:20,13/Mar/19 22:27,14/Jul/12 21:22,1.1.3,,,,,0,cql,cqlsh,,,,,consistency level ANY should not be suggested in tab-completion for SELECT statements,,,,,,,,,,,,,,,,,,,13/Jul/12 23:42;thepaul;0001-cqlsh-don-t-suggest-CL.ANY-for-SELECT.patch;https://issues.apache.org/jira/secure/attachment/12536476/0001-cqlsh-don-t-suggest-CL.ANY-for-SELECT.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-13 23:42:22.558,,,no_permission,,,,,,,,,,,,232807,,,Sat Jul 14 21:22:27 UTC 2012,,,,,,0|i0grnr:,95911,brandon.williams,brandon.williams,,,,,,,,,,"13/Jul/12 23:42;thepaul;Patch attached; changes also available in the 4074 branch of my github. Current version tagged pending/4074.

https://github.com/thepaul/cassandra/tree/4074",14/Jul/12 21:22;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFS always try to load key cache,CASSANDRA-4313,12559572,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,06/Jun/12 17:42,12/Mar/19 14:20,13/Mar/19 22:27,06/Jun/12 19:00,1.2.0 beta 1,,,,,0,,,,,,,"Inside constructor, below condition is always evaluated to true:

{code}
if (caching != Caching.NONE || caching != Caching.ROWS_ONLY)
    CacheService.instance.keyCache.loadSaved(this);
{code}

should be

{code}
 if (caching == Caching.ALL || caching == Caching.KEYS_ONLY)
{code}",,,,,,,,,,,,,,,,,,,06/Jun/12 17:42;yukim;4313.txt;https://issues.apache.org/jira/secure/attachment/12531127/4313.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-06 17:47:32.096,,,no_permission,,,,,,,,,,,,256052,,,Sat Jun 16 14:42:50 UTC 2012,,,,,,0|i0gugn:,96365,slebresne,slebresne,,,,,,,,,,"06/Jun/12 17:47;slebresne;+1

Does that affect 1.1? If so, we should commit there too.","06/Jun/12 19:00;yukim;Committed to trunk only, 1.1 is not affected.
Thanks for the review!","16/Jun/12 14:42;hudson;Integrated in Cassandra #1502 (See [https://builds.apache.org/job/Cassandra/1502/])
    only load key cache when caching is ALL/KEYS_ONLY, fix by yukim, reviewed by slebresne for CASSANDRA-4313 (Revision 383a608e7ff73befaaf34ad7cc0aab4cc26d1316)

     Result = FAILURE
yukim : 
Files : 
* src/java/org/apache/cassandra/db/ColumnFamilyStore.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set max frame size in CLI to avoid OOM when SSL is enabled,CASSANDRA-4969,12616491,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,16/Nov/12 22:12,12/Mar/19 14:20,13/Mar/19 22:27,16/Nov/12 22:37,1.2.0 beta 3,,Legacy/Tools,,,0,,,,,,,"If SSL is enabled on Cassandra but not on the cli, the cli will OOM when connecting to Cassandra because it thinks it's getting a message with a frame size of ~350mb.",,,,,,,,,,,,,,,,,,,16/Nov/12 22:13;thobbs;4969-cli-max-frame-size.txt;https://issues.apache.org/jira/secure/attachment/12553848/4969-cli-max-frame-size.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-16 22:37:46.526,,,no_permission,,,,,,,,,,,,258357,,,Fri Nov 16 22:37:46 UTC 2012,,,,,,0|i0krlz:,119290,,,,,,,,,,,,"16/Nov/12 22:13;thobbs;Attached patch hardcodes the max frame size to 15 MiB, matching Cassandra's default.",16/Nov/12 22:37;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOGGING: Info log is not displaying number of rows read from saved cache at startup,CASSANDRA-4249,12555985,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kohlisankalp,kohlisankalp,kohlisankalp,16/May/12 08:33,12/Mar/19 14:20,13/Mar/19 22:27,16/May/12 16:08,1.1.1,,,,,0,caching,,,,,,"As part of commit with revision c9270f4e info logging for number of rows read from saved cache is not working. 
This is happening because we are not incrementing the counter cachedRowsRead in ColumnFamilyStore.initRowCache().",,360,360,,0%,360,360,,,,,,,,,,,,16/May/12 08:45;kohlisankalp;trunk-4249.txt;https://issues.apache.org/jira/secure/attachment/12527593/trunk-4249.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-16 16:08:17.135,,,no_permission,,,,,,,,,,,,255991,,,Wed May 16 16:08:17 UTC 2012,,,,,,0|i0gtpz:,96245,jbellis,jbellis,,,,,,,,,,16/May/12 08:45;kohlisankalp;This is a very small change.,"16/May/12 16:08;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""The system cannot find the path specified"" when creating hard link on Windows",CASSANDRA-4590,12605562,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,allenservedio,allenservedio,30/Aug/12 18:44,12/Mar/19 14:20,13/Mar/19 22:27,07/Sep/12 16:58,1.1.5,,,,,0,,,,,,,"When upgrading from Cassandra 1.0.5 to 1.1.3, we have a test case (uses embedded Cassandra) that started failing as shown below. Other than the upgrade, no changes were made to the code or config. I believe this MAY be related to the change made in CASSANDRA-3101.

We verified that the file it is trying to create the hard link to does exist - so it is purely the creation of the link that is failing.

Here is the basic failure:

# [11:31:00.307] [ERROR] [o.a.c.u.CLibrary] [createHardLinkWithExec] [Unable to create hard link]
java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.


Here is a more complete log output:


# [11:30:59.975] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.976] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Digest.sha1]
# [11:30:59.977] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Index.db]
# [11:30:59.978] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Filter.db]
# [11:30:59.978] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.979] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.979] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Statistics.db]
# [11:30:59.979] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.980] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.980] [DEBUG] [o.a.c.i.s.SSTable] [delete] [Deleted target\test\cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-he-4]
# [11:30:59.981] [INFO ] [o.a.c.d.ColumnFamilyStore] [maybeSwitchMemtable] [Enqueuing flush of Memtable-PropertyProductDefaultInventoryCounts@2002512083(74/92 serialized/live bytes, 1 ops)]
# [11:30:59.981] [INFO ] [o.a.c.d.Memtable] [writeSortedContents] [Writing Memtable-PropertyProductDefaultInventoryCounts@2002512083(74/92 serialized/live bytes, 1 ops)]
# [11:30:59.992] [DEBUG] [o.a.c.d.Directories] [getLocationWithMaximumAvailableSpace] [expected data files size is 134; largest free partition (target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts) has 82645161984 bytes free]
# [11:31:00.012] [INFO ] [o.a.c.d.Memtable] [writeSortedContents] [Completed flushing target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-Data.db (123 bytes) for commitlog position ReplayPosition(segmentId=592725621297887, position=6701)]
# [11:31:00.013] [DEBUG] [o.a.c.u.I.IntervalNode] [<init>] [Creating IntervalNode from [Interval(DecoratedKey(70791399548943621833439300945136455431, 50726f706572747950726f6475637431323334), DecoratedKey(70791399548943621833439300945136455431, 50726f706572747950726f6475637431323334))]]
# [11:31:00.013] [DEBUG] [o.a.c.d.DataTracker] [addNewSSTablesSize] [adding target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1 to list of files tracked for RevKeyspace.PropertyProductDefaultInventoryCounts]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CompactionManager] [submitBackground] [Scheduling a background task check for RevKeyspace.PropertyProductDefaultInventoryCounts with SizeTieredCompactionStrategy]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CompactionManager] [runMayThrow] [Checking RevKeyspace.PropertyProductDefaultInventoryCounts]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CommitLog] [call] [discard completed log segments for ReplayPosition(segmentId=592725621297887, position=6701), column family 1001]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.SizeTieredCompactionStrategy] [getNextBackgroundTask] [Compaction buckets are [[SSTableReader(path='target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-Data.db')]]]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CommitLog] [call] [Not safe to delete commit log CommitLogSegment(target\test\cassandra\commitlog\CommitLog-592725621297887.log); dirty is Versions (7), ; hasNext: false]
# [11:31:00.015] [DEBUG] [o.a.c.d.c.CompactionManager] [runMayThrow] [No tasks available]
# [11:31:00.307] [ERROR] [o.a.c.u.CLibrary] [createHardLinkWithExec] [Unable to create hard link]
java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) [cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.308] [ERROR] [o.a.c.s.AbstractCassandraDaemon] [uncaughtException] [Exception in thread Thread[MigrationStage:1,5,main]]
java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1526) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) ~[na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
Caused by: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 10 common frames omitted
# [11:31:00.309] [ERROR] [o.a.c.t.CustomTThreadPoolServer] [run] [Error occurred during processing of message.]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:170) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.CassandraServer.system_drop_keyspace(CassandraServer.java:1008) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace.getResult(Cassandra.java:3476) ~[cassandra-thrift-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace.getResult(Cassandra.java:3464) ~[cassandra-thrift-1.1.3.jar:1.1.3]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.get(FutureTask.java:83) ~[na:1.6.0_33]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 11 common frames omitted
Caused by: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1526) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) ~[na:1.6.0_33]
	... 3 common frames omitted
Caused by: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 10 common frames omitted
# [11:31:00.309] [DEBUG] [o.a.c.s.ClientState] [logout] [logged out: #<User allow_all groups=[]>]
# [11:31:00.310] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-5>]
# [11:31:00.310] [ERROR] [m.p.c.c.HConnectionManager] [markHostAsDown] [MARK HOST AS DOWN TRIGGERED for host 127.0.0.1(127.0.0.1):9162]
# [11:31:00.310] [ERROR] [m.p.c.c.HConnectionManager] [markHostAsDown] [Pool state on shutdown: <ConcurrentCassandraClientPoolByHost>:{127.0.0.1(127.0.0.1):9162}; IsActive?: true; Active: 1; Blocked: 0; Idle: 15; NumBeforeExhausted: 49]
# [11:31:00.311] [INFO ] [m.p.c.c.ConcurrentHClientPool] [shutdown] [Shutdown triggered on <ConcurrentCassandraClientPoolByHost>:{127.0.0.1(127.0.0.1):9162}]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-6>]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-15>]
# [11:31:00.311] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-14>]
# [11:31:00.312] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.312] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.312] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-13>]
",Windows 7 - 64 bit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-08-31 12:31:24.317,,,no_permission,,,,,,,,,,,,256272,,,Fri Sep 07 16:58:53 UTC 2012,,,,,,0|i0gxfz:,96848,,,,,,,,,,,,31/Aug/12 12:31;jbellis;Does the directory exist that it's trying to create the link in?,"31/Aug/12 13:57;allenservedio;We just went back and double checked - yes the directory does exist. Also, before even submitting this defect we verified that: a) the developers have plenty of space on their drive and b) we were able to verify the same behavior on another developer's box (in case it was environmental).

I forgot to mention... We have other tests that also use Embedded Cassandra and work fine. So, either they are not triggering this functionality OR there is something about the path that is messing things up (my guess).",31/Aug/12 14:41;jbellis;What happens if you run the mklink command manually?,31/Aug/12 14:46;jbellis;Also: it's worth trying a shorter path to put your data in.  According to http://msdn.microsoft.com/en-us/library/aa365247.aspx the maximum path length should be 256 and you're only at ~200 but I don't know how far people have successfully pushed it.,"31/Aug/12 14:59;allenservedio;Yep, it looks like a path length problem. 

Trying with existing path:

c:\ABC\source_code\s7-t1>cmd /c mklink /H C:\ABC\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-Compressio
nInfo.db C:\ABC\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db
The system cannot find the path specified.

Shortening path (IT WORKS):

C:\>cmd /c mklink /H C:\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db 

C:\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCoun
ts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db
Hardlink created for C:\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db <<===>> C:\test\cassandra\data\RevKeyspace\PropertyProductDefaultInven
toryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db

Probably worth just updating the failure message to let folks know of the likely problem. My apologies for opening this before doing better due diligence on tracking down the problem (esp. since I had suspected what it was...). Anyway, it is documented now :-)
","07/Sep/12 16:58;jbellis;Updated error message as follows:

{code}
            String st = osname.startsWith(""Windows"")
                      ? ""Unable to create hard link.  This probably means your data directory path is too long.  Exception follows:""
                      : ""Unable to create hard link with exec.  Suggest installing JNA to avoid the need to exec entirely.  Exception follows: "";
            logger.error(st, ex);
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unfriendly error message during create table map collection,CASSANDRA-5132,12626651,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,khahn,khahn,08/Jan/13 21:34,12/Mar/19 14:20,13/Mar/19 22:27,09/Jan/13 11:06,1.2.1,,,,,0,,,,,,,"cqlsh:music> create table test (id uuid PRIMARY KEY, testmap map<timestamp, nonsense>);
Bad Request: Failed parsing statement: [create table test (id uuid PRIMARY KEY, testmap map<timestamp, nonsense>);] reason: NullPointerException null","[cqlsh 2.3.0 | Cassandra 1.2.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.35.0]
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-01-09 11:06:57.28,,,no_permission,,,,,,,,,,,,303258,,,Wed Jan 09 11:06:57 UTC 2013,,,,,,0|i178z3:,250422,,,,,,,,,,,,09/Jan/13 11:06;slebresne;Fix committed as commit cc0c9f3. Thanks for the report.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql3 ALTER TABLE foo WITH default_validation=int has no effect,CASSANDRA-4171,12551531,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,18/Apr/12 21:01,12/Mar/19 14:20,13/Mar/19 22:27,20/Apr/12 15:36,1.1.0,,Legacy/CQL,,,0,cql3,,,,,,"running the following with cql3:

{noformat}
CREATE TABLE test (foo text PRIMARY KEY) WITH default_validation=timestamp;
ALTER TABLE test WITH default_validation=int;
{noformat}

does not actually change the default validation type of the CF. It does under cql2.

No error is thrown. Some properties *can* be successfully changed using ALTER WITH, such as comment and gc_grace_seconds, but I haven't tested all of them. It seems probable that default_validation is the only problematic one, since it's the only (changeable) property which accepts CQL typenames.",,,,,,,,,,,,,,,,,,,20/Apr/12 10:25;slebresne;4171.txt;https://issues.apache.org/jira/secure/attachment/12523476/4171.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-18 21:15:41.053,,,no_permission,,,,,,,,,,,,236339,,,Fri Apr 20 15:36:00 UTC 2012,,,,,,0|i0gstr:,96100,jbellis,jbellis,,,,,,,,,,"18/Apr/12 21:15;jbellis;We should raise an error for trying to use default_validation under cql3.  The right way to model this would be something like:

{noformat}
CREATE TABLE test (
    foo text,
    i   int,
    PRIMARY KEY (foo, i)
) WITH COMPACT STORAGE;
{noformat}
","18/Apr/12 21:17;jbellis;(Re-opening, didn't mean to resolve.)",20/Apr/12 10:25;slebresne;I believe the correct fix is to reset the set of obsolete keywords for CQL3. We've used that in CLQ2 for keywords to ignore (by opposition to reject) that became obsolete in order to ensure backward compatibility. But CQL3 is not backward compatible anyway so we should start afresh.,20/Apr/12 15:30;jbellis;+1,"20/Apr/12 15:36;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NodeCmd misspells ""positives""",CASSANDRA-5110,12625991,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jalkanen,jalkanen,jalkanen,04/Jan/13 08:58,12/Mar/19 14:20,13/Mar/19 22:27,04/Jan/13 12:41,1.1.9,,,,,0,,,,,,,"Running nodetool cfstats speaks of ""Bloom Filter False Postives"". It annoys my OCD to see the misspelling all the time. :-)",Any,,,,,,,,,,,,,,,,,,04/Jan/13 09:02;jalkanen;CASSANDRA-5110.patch;https://issues.apache.org/jira/secure/attachment/12563264/CASSANDRA-5110.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-04 12:41:03.004,,,no_permission,,,,,,,,,,,,302573,,,Fri Jan 04 12:41:03 UTC 2013,,,,,,0|i1741j:,249622,brandon.williams,brandon.williams,,,,,,,,,,04/Jan/13 09:02;jalkanen;Patch against cassandra-1.1,"04/Jan/13 12:41;brandon.williams;This bothered me too and I seem to recall fixing it, but I guess I missed 1.1.  Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli should escape keyspace name,CASSANDRA-5052,12623347,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,shalupov,shalupov,11/Dec/12 08:39,12/Mar/19 14:20,13/Mar/19 22:27,24/Mar/13 15:07,1.1.11,,Legacy/Tools,,,0,,,,,,,"show schema yields ""use __someKeyspace"", expecting ""use '__someKeyspace'"".
",,,,,,,,,,,,,,,,,,,24/Mar/13 13:52;iamaleksey;5052.txt;https://issues.apache.org/jira/secure/attachment/12575228/5052.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-24 14:21:17.297,,,no_permission,,,,,,,,,,,,297043,,,Sun Mar 24 16:44:42 UTC 2013,,,,,,0|i14kzz:,234865,jbellis,jbellis,,,,,,,,,,24/Mar/13 14:21;jbellis;Why does underscore cause trouble?,"24/Mar/13 14:33;iamaleksey;bq. Why does underscore cause trouble?
Because names starting with _ cannot be identifiers (identifiers can contain _, but not in the beginning) and must be quoted.",24/Mar/13 14:50;jbellis;+1,"24/Mar/13 15:07;iamaleksey;Committed, thanks.",24/Mar/13 16:44;iamaleksey;Followed up with 0e03790059e390fb58f45b8dc5e44e76f5e913d3 to give names starting with digits the same treatment.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typo in ConfigHelper.java,CASSANDRA-4930,12615269,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,mkjellman,mkjellman,08/Nov/12 02:00,12/Mar/19 14:20,13/Mar/19 22:27,08/Nov/12 08:43,1.2.0 beta 3,,,,,0,,,,,,,"line 136 describing setOutputColumnFamily() is currently ""Set the column family for the input of this job.""

It should read ""Set the column family for the output of this job.""

similar typo on line 148.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-11-08 08:43:26.575,,,no_permission,,,,,,,,,,,,255874,,,Thu Nov 08 08:43:26 UTC 2012,,,,,,0|i0fvi7:,90702,,,,,,,,,,,,"08/Nov/12 08:43;slebresne;Thanks, I've committed the typo fix as commit c016b31.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MoveTest madness,CASSANDRA-4564,12604290,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,swordheart,urandom,urandom,21/Aug/12 22:17,12/Mar/19 14:05,13/Mar/19 22:27,04/Jan/13 13:39,1.1.9,1.2.1,Legacy/Testing,,,0,lhf,,,,,,"I encountered what looks like bugs in {{o.a.c.service.MoveTest.newTestWriteEndpointsDuringMove()}} while doing something else; Here is a (poorly researched )ticket before I forget :)

* There are two loops over non-system tables, and the first is a NOOP
* In the second loop, a set exactly {{replicationFactor}} in size is compared against {{tmd.getWriteEndpoints()}}, which should produce greater than {{replicationFactor}} endpoints during a move (shouldn't it?); How does this pass?",,,,,,,,,,,,,,,,,,,20/Dec/12 06:26;swordheart;cassandra-1.1-4564.txt;https://issues.apache.org/jira/secure/attachment/12561849/cassandra-1.1-4564.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-20 06:25:11.777,,,no_permission,,,,,,,,,,,,256250,,,Fri Jan 04 13:39:56 UTC 2013,,,,,,0|i0gx67:,96804,brandon.williams,brandon.williams,,,,,,,,,,"20/Dec/12 06:25;swordheart;For the second issue, the root cause is: the key token is endpointToken+5, and the endpoint token is moved to endpointToken+2, so there is no key need to be moved. I have changed the move token to enpointToken+7 and updated the test cases.","22/Dec/12 15:21;jbellis;Thanks, Liu.  Review may proceed a bit slowly due to the US holidays, but we'll get to it eventually!",04/Jan/13 13:39;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when trying to select a slice from a composite table,CASSANDRA-4532,12603138,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,basu76,basu76,12/Aug/12 19:41,12/Mar/19 14:05,13/Mar/19 22:27,06/Sep/12 14:57,1.1.5,,Legacy/CQL,,,1,cql,cql3,Slice,,,,"I posted this question on StackOverflow, because i need a solution. 

Created a table with :

{noformat}
create table compositetest(m_id ascii,i_id int,l_id ascii,body ascii, PRIMARY KEY(m_id,i_id,l_id));
{noformat}

wanted to slice the results returned, so did something like below, not sure if its the right way. The first one returns data perfectly as expected, second one to get the next 3 columns closes the transport of my cqlsh

{noformat}
cqlsh:testkeyspace1> select * from compositetest where i_id<=3 limit 3;
 m_id | i_id | l_id | body
------+------+------+------
   m1 |    1 |   l1 |   b1
   m1 |    2 |   l2 |   b2
   m2 |    1 |   l1 |   b1

cqlsh:testkeyspace1> Was trying to write something for slice range.

TSocket read 0 bytes
{noformat}

Is there a way to achieve what I am doing here, it would be good if some meaning ful error is sent back, instead of cqlsh closing the transport.

On the server side I see the following error.

{noformat}
ERROR [Thrift:3] 2012-08-12 15:15:24,414 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.NullPointerException
	at org.apache.cassandra.cql3.statements.SelectStatement$Restriction.setBound(SelectStatement.java:1277)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.updateRestriction(SelectStatement.java:1151)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:1001)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:215)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}

With ThriftClient I get :

{noformat}
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_execute_cql_query(Cassandra.java:1402)
	at org.apache.cassandra.thrift.Cassandra$Client.execute_cql_query(Cassandra.java:1388)
{noformat}",Cassandra 1.1.3 (2 nodes) on a single host - mac osx,,,,,,,,,,,,,,,,,,06/Sep/12 13:41;slebresne;4532.txt;https://issues.apache.org/jira/secure/attachment/12544045/4532.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-25 19:15:03.007,,,no_permission,,,,,,,,,,,,256222,,,Thu Sep 06 16:48:06 UTC 2012,,,,,,0|i0gwt3:,96745,jbellis,jbellis,,,,,,,,,,"25/Aug/12 19:15;thepaul;Please give the exact queries you used which reproduce the problem. ""{{Was trying to write something for slice range.}}"" isn't a valid query.","29/Aug/12 13:44;basu76;I was trying to get a slice range, like you could do in thrift.

table defn :

create tables schedules(status ascii, timecreated bigint, key ascii, nil ascii, PRIMARY KEY(status,timecreated,key));

for the same time there can be a lot of entries.

Lets suppose there are 50 entries that match where timecreated is < Ln

1st query : select * from schedules where timecreated <= <Ln> limit 10;


2nd Query : select * from schedules where timecreated>=L10 AND key=K10 and timecreated<Ln.

In CQL terms this is a wrong query I know, basically not sure how to represent Between in CQL


In Hector I would do, get slice range limiting 10 first time, 

for the next query (until no more are returned) I would use the time returned by last query and key returned by last query as the start range. This is in production and works perfectly fine",29/Aug/12 13:54;jbellis;can you test against trunk?,"29/Aug/12 14:21;basu76;No luck. See the last query closed the socket. I took the latest from git and compiled

Here are the steps to reproduce :

cqlsh:testkeyspace1> create table compositetest(status ascii,ctime bigint,key ascii,nil ascii,PRIMARY KEY(status,ctime,key));

cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345678,'key1','');
cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345678,'key2','');
cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345679,'key3','');
cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345679,'key4','');
cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345679,'key5','');
cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345680,'key6','');
cqlsh:testkeyspace1> select * from compositetest;
 status | ctime    | key  | nil
--------+----------+------+-----
      C | 12345678 | key1 |    
      C | 12345678 | key2 |    
      C | 12345679 | key3 |    
      C | 12345679 | key4 |    
      C | 12345679 | key5 |    
      C | 12345680 | key6 |    

1st query of slice :

cqlsh:testkeyspace1> select * from compositetest where ctime<=12345680 limit 3;
 status | ctime    | key  | nil
--------+----------+------+------
      C | 12345678 | key1 |     
      C | 12345678 | key2 |     
      C | 12345679 | key3 | null

Second Query : I want to get values where first one left off (Yes you could do this with hector) [Try 1]

cqlsh:testkeyspace1> select * from compositetest where ctime>=12345679 and key='key3' and ctime<=12345680 limit 3;
Bad Request: PRIMARY KEY part key cannot be restricted (preceding part ctime is either not restricted or by a non-EQ relation) [Try 2]
cqlsh:testkeyspace1> select * from compositetest where ctime=12345679 and key='key3' and ctime<=12345680 limit 3;
TSocket read 0 bytes
cqlsh:testkeyspace1>","06/Sep/12 13:41;slebresne;Attaching (trivial) patch to fix the validation issue (the request is invalid because it mixes an equal and inequal on the same ctime column).

For the record, a valid request that does the equivalent of a thrift sliceRange would be:
{noformat}
SELECT * FROM compositetest WHERE ctime>=12345679 and ctime<=12345680 limit 3;
{noformat}
If you further want to restrict the result to those column where key='key3', you'd have to do it client side (but that would be the same with thrift).
",06/Sep/12 13:53;jbellis;+1,"06/Sep/12 14:57;slebresne;Committed, thanks","06/Sep/12 15:59;basu76;Sylvain,
Thanks for the fix, May be I should have opened the ticket differently. My main issue is not that the connection on cqlsh was getting closed (because of NPE) or exception being thrown in thrift client. To adopt CQL3 mainstream slice with paging is need, I saw some other ticket, where this proposal was there. 

Main reason I opened the ticket was for not being able to slice (rather no syntax support for that, to continue where the last query left off). This is a piece of code that was written using Hector and my assumption is the filtering was done on server side. We are using this in production. I modified attributes (to get rid of proprietary stuff, so may be broken, but should give an idea of what I was trying to do with CQL3)

            long cStartTime = 0L;
            long startTime = System.nanoTime();
            boolean fetchNextBatch = true;
            int totalKeysFetched = 0;
            String lastKeyFetched = null;
            long cEndTime = <Some Time in Millis>;
            while(fetchNextBatch) {
                fetchNextBatch = false;
                int pageSize = 3; //Just for demonstration
                SliceQuery<Object,DynamicComposite,String> sliceQuery =   HFactory.createSliceQuery(keyspace,<KEY SERIALIZER>,DynamicCompositeSerializer.get(),StringSerializer.get());
                sliceQuery.setKey(""C"");
                sliceQuery.setColumnFamily(""<SOME CF NAME>"");
                DynamicComposite startRange = new DynamicComposite();
  
                startRange.addComponent(cStartTime,LongSerializer.get()); //For the first fetch - this will be 0L
                startRange.addComponent(lastKeyFetched,StringSerializer.get()); // this will be null for first fetch

                DynamicComposite endRange = new DynamicComposite();
                endRange.addComponent(new Long(cEndTime), LongSerializer.get(), ""LongType"", AbstractComposite.ComponentEquality.LESS_THAN_EQUAL);
                //Add another config if we need columnPageSize
                sliceQuery.setRange(startRange,endRange,false,pageSize);

                long start = System.nanoTime();
                QueryResult<ColumnSlice<DynamicComposite, String>> result = sliceQuery.execute();
                float t =  (float)((System.nanoTime() - start)/1000000);
                System.out.println(""TIME FOR QUERY :"" + t  + "" MILLI SECONDS"");

                ColumnSlice<DynamicComposite, String> cs = result.get();
                List<HColumn<DynamicComposite,String>> compositeList = cs.getColumns();

                for(int i =0;i<compositeList.size();i++) {
                    HColumn<DynamicComposite, String> col = compositeList.get(i);
                    cStartTime = col.getName().get(0,LongSerializer.get()); //This will be the cTime for the start range of the next query
                    lastKeyFetched = col.getName().get(1,StringSerializer.get()); //In the start range for the next query, this key will be used.
                    keyTimeMap.put(lastKeyFetched,scheduleStartTime);
                    totalKeysFetched ++;
                }

                //Process Fetched Data 

                fetchNextBatch = compositeList.size() == pageSize; // If the number of records retrieved is equal to the page size, then there are probably more records left 
            }","06/Sep/12 16:48;slebresne;@basanth Slices are definitely supported in CQL3, the query I gave you earlier involves a slice for instance. In a fair amount of cases, paging within a Cassandra row is also pretty simple to do in CQL3 too. It is true that in some cases it's not completely as easy as in thrift. For those last cases, CASSANDRA-4415 should provide a solution.

If you questions on how to translate some piece thrift code to CQL3, you're definitively welcome to ask them but please use the user mailing list for such inquiries.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE while loading Saved KeyCache,CASSANDRA-4553,12603795,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,16/Aug/12 22:35,12/Mar/19 14:05,13/Mar/19 22:27,17/Aug/12 02:30,1.2.0 beta 1,,,,,0,,,,,,,"WARN [main] 2012-08-16 15:31:13,896 AutoSavingCache.java (line 146) error reading saved cache /var/lib/cassandra/saved_caches/system-local-KeyCache-b.db
java.lang.NullPointerException
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:140)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:251)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:354)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:326)
	at org.apache.cassandra.db.Table.initCf(Table.java:312)
	at org.apache.cassandra.db.Table.<init>(Table.java:252)
	at org.apache.cassandra.db.Table.open(Table.java:97)
	at org.apache.cassandra.db.Table.open(Table.java:75)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:285)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:168)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:318)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:361)
",,,,,,,,,,,,,,,,,,,16/Aug/12 22:36;vijay2win@yahoo.com;0001-CASSANDRA-4553.patch;https://issues.apache.org/jira/secure/attachment/12541289/0001-CASSANDRA-4553.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-16 22:47:16.119,,,no_permission,,,,,,,,,,,,256240,,,Fri Aug 17 02:30:00 UTC 2012,,,,,,0|i0gx1r:,96784,xedin,xedin,,,,,,,,,,16/Aug/12 22:36;vijay2win@yahoo.com;Simple fix to handle null in ASC,"16/Aug/12 22:47;jbellis;+1, although would be even better w/ comment as to why we expect keycache deserialize to return nulls sometimes (but not rowcache)",16/Aug/12 22:49;xedin;+1 with Jonathan,"17/Aug/12 02:30;vijay2win@yahoo.com;Committed with comments, Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL/JDBC: date vs. timestamp issues,CASSANDRA-4628,12606538,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mkrumpholz,mkrumpholz,mkrumpholz,07/Sep/12 07:25,12/Mar/19 14:05,13/Mar/19 22:27,07/Sep/12 15:23,1.2.0 beta 1,,,,,0,cql,jdbc,,,,,"Cassandra's datatypes only have one Date/Time type named timestamp containing both date and time. Calling the validator org.apache.cassandra.db.marshal.DateType might be OK in general but can be confusing in the jdbc context where there is a distinction between date, time and timestamp. In terms of jdbc there should be more datatypes for dates and times or the jdbc driver should take one of the following options:
- stick to timestamp
- check if the date has a time part and distinguish by the data between date and timestamp automatically
- use distinct datatypes according to the jdbc spec, the types would need to be in cassandra then too

Now back to my actual problem:
org.apache.cassandra.cql.jdbc.JdbcDate returns Types.DATE in getType(). Even if having inserted a complete date with time (making it a timestamp) the ResultSetMetaData.getColumnType() implementation still returns Types.DATE (source of this is in JdbcDate). If some other java code (where i don't have access to) uses the metadata to get the type and then getDate() to get the value the time is cut off the value and only the date is returned.

But the ResultSet.getObject() implementation returns a complete java.util.Date including the time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-09-07 10:26:33.638,,,no_permission,,,,,,,,,,,,256298,,,Fri Sep 07 15:27:16 UTC 2012,,,,,,0|i0gxun:,96914,jbellis,jbellis,,,,,,,,,,"07/Sep/12 10:26;slebresne;I'm a fairly strong believer that adding the Jdbc* classes was a mistake in the first place. See CASSANDRA-4495 for a bit more argumentation.

So I must admit that I'm not sure I understand what it is you suggest exactly concerning Cassandra itself in this ticket, so I don't want to close this right way, but I have the feeling that whatever needs to be done probably concerns the jdbc driver, not Cassandra.

As a side note, I wouldn't disagree that Cassandra/CQL handling of dates is a tad limited and it might probably be worth beefing it up at some point, but that's largely a different issue.
","07/Sep/12 11:22;mkrumpholz;The JdbcDate class is not part of the jdbc driver package which is a separate project at google (as you mentioned in my other issue). But the getType there returns the JDBC datatype (from java.sql.Types) that is returned in the metadata according to the jdbc spec. The problem is that Types.DATE is the wrong type. It should be Types.TIMESTAMP because the data can be more that just a date, it can be a timestamp. Using ResultSet.getDate() because of the returned type cuts off the time part of the timestamp. JDBC has the date time distinction, cassandra not. in short words, but i assume you are aware of this: date != timestamp, timestamp = date + time","07/Sep/12 14:19;jbellis;bq. the getType there returns the JDBC datatype 

that is an issue with the jdbc driver then, not Cassandra",07/Sep/12 15:15;slebresne;I guess it is true that JdbcDate could have been made to return java.sql.Types.TIMESTAMP rather than DATE for its getJdbcType method. ,"07/Sep/12 15:18;jbellis;Oops, I guess that is our code.","07/Sep/12 15:23;mkrumpholz;{quote}
that is an issue with the jdbc driver then, not Cassandra
{quote}
That would be the case only if that class (JdbcDate) would NOT be part of cassandra (apache-cassandra-clientutil-x.x.x.jar) but located in the jdbc lib/jar in the project at google. 

{quote}
Oops, I guess that is our code.
{quote}
Right! For now it's within cassandra code and included as dependency in cassandra-jdbc. If this type class and the meaning of if should change with CASSANDRA-4495 then the code should be moved to the jdbc project or a update there should be done. I don't know how much the casandra dev team is involved with the cassandra-jdbc project...",07/Sep/12 15:23;jbellis;fixed in e1201dff2c9c89ce2ff9a197ac5d99d9288c06e2,"07/Sep/12 15:25;jbellis;(I think the confusion arose because it represents a Java Date object, which does include time.  So we had a DateType around, and when we added CQL just named the corresponding class JdbcDate following the convention for the other classes.)","07/Sep/12 15:27;mkrumpholz;yeah, as i wrote in the issue description, in the schema_columns table it is a DateType class, data type in the create statement is timestamp as it is and should be in the jdbc context.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
countPendingHints JMX operation is returning garbage for the key,CASSANDRA-4568,12604476,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,jblangston@datastax.com,jblangston@datastax.com,22/Aug/12 19:29,12/Mar/19 14:05,13/Mar/19 22:27,28/Aug/12 14:32,1.0.12,1.1.5,,,,0,datastax_qa,,,,,,"countPendingHints JMX operation should return a map from key: endpoint IP address to value: number of pending hints. It is returning garbage for the key (looks like binary data concerning the hint itself). The value looks correct.

Steps to reproduce:

1) Set up a two-node cluster. 

2) Disable gossip on the second node.  

`nodetool ring` output from node 1:

Address         DC          Rack        Status State   Load            Effective-Ownership Token                                       
                                                                                           85070591730234615865843651857942052864      
192.168.1.162   datacenter1 rack1       Up     Normal  21.46 KB        100.00%             0                                           
192.168.1.130   datacenter1 rack1       Down   Normal  6.67 KB         100.00%             85070591730234615865843651857942052864      


3) While the second node is still down, create a keyspace with RF=2 and a CF within this keyspace. Then insert two records into the CF:

Connected to Test Cluster at 192.168.1.162:9160.
[cqlsh 2.2.0 | Cassandra 1.1.2 | CQL spec 2.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> create KEYSPACE demo WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 2;
cqlsh> use demo;
cqlsh:demo> create table users (username varchar primary key, password varchar);
cqlsh:demo> insert into users (username, password) values (scott, tiger);
cqlsh:demo> insert into users (username, password) values (root, password);

4) Use a JMX client to execute the countPendingHints operation:

jblangston:~ jblangston$ java -jar cmdline-jmxclient-0.10.3.jar - localhost:7199 org.apache.cassandra.db:type=HintedHandoffManager countPendingHints

08/22/2012 14:21:37 -0500 org.archive.jmx.Client countPendingHints: {@B^h ??	?[b??scottdemoscott?????password?ߞHtigerdemoF
?P??	?[b??rootdemoroot?????password?ߞ?Wpassworddemo=2}

5) Notice the output.  The value (2) is correct but the key is garbage instead of an endpoint IP address.",,,,,,,,,,,,,,,,,,,24/Aug/12 18:32;brandon.williams;4568.txt;https://issues.apache.org/jira/secure/attachment/12542310/4568.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-24 18:32:27.659,,,no_permission,,,,,,,,,,,,256254,,,Tue Aug 28 14:32:42 UTC 2012,,,,,,0|i0gx7z:,96812,jbellis,jbellis,,,,,,,,,,24/Aug/12 18:32;brandon.williams;Patch to correctly display the token.,"24/Aug/12 20:56;jbellis;why not just use r.key.token?

+1 otherwise.","28/Aug/12 14:32;jbellis;Went ahead and committed w/ that change because open issues for 1.0.12 damage my calm.

Note on the issue description: Since 1.0.0 (CASSANDRA-2045) the row key for hints is the token of the [v]node owning the hints, not the ip address.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COPY TO and COPY FROM don't default to consistent ordering of columns,CASSANDRA-4594,12605617,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,tpatterson,tpatterson,30/Aug/12 23:13,12/Mar/19 14:05,13/Mar/19 22:27,18/Sep/12 19:03,1.1.6,,,,,0,cqlsh,,,,,,"Here is the input:
{code}                                                         
CREATE KEYSPACE test WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
USE test;                                                                       
                                                                                
CREATE TABLE airplanes (                                                        
                name text PRIMARY KEY,                                          
                manufacturer ascii,                                             
                year int,                                                       
                mach float                                                      
            );                                                                  
                                                                                
INSERT INTO airplanes (name, manufacturer, year, mach) VALUES ('P38-Lightning', 'Lockheed', 1937, '.7');
                                                                                
COPY airplanes TO 'temp.cfg' WITH HEADER=true;                                  
                                                                                
TRUNCATE airplanes;                                                                
                                                                                   
COPY airplanes FROM 'temp.cfg' WITH HEADER=true;                                
                                                                                   
SELECT * FROM airplanes;
{code}

Here is what happens when executed. Note how it tried to import the float into the int column:
{code}
cqlsh:test> DROP KEYSPACE test;                                                                
cqlsh:test> CREATE KEYSPACE test WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
cqlsh:test> USE test;                                                                       
cqlsh:test>                                                                                    
cqlsh:test> CREATE TABLE airplanes (                                            
        ...                 name text PRIMARY KEY,                              
        ...                 manufacturer ascii,                                 
        ...                 year int,                                           
        ...                 mach float                                          
        ...             );                                                      
cqlsh:test>                                                                     
cqlsh:test> INSERT INTO airplanes (name, manufacturer, year, mach) VALUES ('P38-Lightning', 'Lockheed', 1937, '.7');
cqlsh:test>                                                                     
cqlsh:test> COPY airplanes TO 'temp.cfg' WITH HEADER=true;                      
1 rows exported in 0.003 seconds.                                               
cqlsh:test> TRUNCATE airplanes;                                                 
cqlsh:test>                                                                     
cqlsh:test> COPY airplanes FROM 'temp.cfg' WITH HEADER=true;                    
Bad Request: unable to make int from '0.7'                                      
Aborting import at record #0 (line 1). Previously-inserted values still present.
0 rows imported in 0.002 seconds.
{code}","Happens in CQLSH 2, may or may not happen in CQLSH 3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-08-31 02:33:41.603,,,no_permission,,,,,,,,,,,,256275,,,Tue Sep 18 18:58:19 UTC 2012,,,,,,0|i0gxhj:,96855,brandon.williams,brandon.williams,,,,,,,,,,"31/Aug/12 02:33;thepaul;This is because a {{select * from airplanes;}} does not give the columns in the order they were defined. I'm not sure why not; if that's a bug in C*, then we should fix that. If there isn't supposed to be any expectation of order, then cqlsh should be inspecting the columns and specifying them explicitly.","04/Sep/12 07:21;slebresne;bq. This is because a select * from airplanes; does not give the columns in the order they were defined.

No it doesn't. The vaguely technical reason is that we don't keep internally the order of definition of columns, so we would have to start keeping that information (which honestly wouldn't be that much of a burden now that we have the list support, but was a tad more annoying to do at the time I wrote the code for select *). And I think that it's one place where it's not worth committing to any specific order but rather left it unspecified, so I don't see the point in bothering to record said definition order.

For the record though, the order of * is not completely random. It returns first the columns composing the PK (in the order of the PK) and then the rest of the columns in lexicographic order. But really it's mostly because it's convenient for the implementation to do it that way.",07/Sep/12 04:01;thepaul;Seems worth it to fix this in the 1.1 branch too.,"12/Sep/12 19:58;thepaul;Fixed in my github clone, in the 4594-1.1 branch:

http://github.com/thepaul/cassandra/tree/4594-1.1

Current version is tagged pending/4594-1.1.

And since the merge forward of this change will conflict with CASSANDRA-4491, and the proper resolution isn't obvious, I've also made a branch with this change for 1.2 on top of 4491:

http://github.com/thepaul/cassandra/tree/4594-1.2","13/Sep/12 18:58;brandon.williams;Committed your 1.1 fix, and committed 4491 to 1.2 but I can't get your 1.2 branch to apply to cql3handling.py.","18/Sep/12 04:41;thepaul;Ah, sorry, I didn't make it clear on 4491 that the github branch had multiple commits from off of trunk. Looks like you only cherry-picked the last. I'll make a note there.","18/Sep/12 16:20;iamaleksey;trunk, cql3, python 2.7:

{quote}
cqlsh:test> COPY airplanes TO 'temp.cfg' WITH HEADER=true;                                  
CQL query must be bytes, not unicode
{quote}
and
{quote}
cqlsh:test> COPY airplanes FROM 'temp.cfg' WITH HEADER=true; 
Traceback (most recent call last):
  File ""bin/cqlsh"", line 914, in perform_statement
    self.cursor.execute(statement, decoder=decoder)
  File ""bin/../lib/cql-internal-only-1.2.0.zip/cql-1.2.0/cql/cursor.py"", line 75, in execute
    raise ValueError(""CQL query must be bytes, not unicode"")
ValueError: CQL query must be bytes, not unicode
{quote}

I've attached a patch that fixes the issue but probably isn't the best possible solution.",18/Sep/12 17:56;thepaul;Aleksey: you must be missing the patch for this ticket. It includes a change to the cql_protect_name function to encode unicode names in utf8 when necessary. Using {{str}} won't work when there are non-ascii characters and the default encoding is not utf8.,"18/Sep/12 18:18;iamaleksey;Paul: if it's in the trunk then I'm not missing it. Just double-checked it, still got exact same exceptions.","18/Sep/12 18:23;thepaul;It's not yet in trunk. See above; Brandon couldn't get the commit to apply, because it was missing two of the commits from 4491.",18/Sep/12 18:26;iamaleksey;Ah. Then I misunderstood you. Sorry.,"18/Sep/12 18:28;brandon.williams;Sylvain said he committed those and I think I see them there, maybe you just need to pull.","18/Sep/12 18:31;iamaleksey;I pulled, of course. So unless git is lying to me, I should have the latest trunk.","18/Sep/12 18:40;thepaul;Yes, the commits from 4491 are there. The patch for _this commit_ is not.","18/Sep/12 18:48;brandon.williams;Ok, I got it to merge cleanly and committed it to trunk.",18/Sep/12 18:58;iamaleksey;The problem is now gone.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflowError in LeveledCompactionStrategy$LeveledScanner.computeNext,CASSANDRA-4587,12605495,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,schnidrig,schnidrig,30/Aug/12 09:20,12/Mar/19 14:05,13/Mar/19 22:27,30/Aug/12 18:46,1.1.5,,,,,0,compaction,,,,,,"while running nodetool repair, the following was logged in system.log:


ERROR [ValidationExecutor:2] 2012-08-30 10:58:19,490 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.StackOverflowError
        at sun.nio.cs.UTF_8.updatePositions(UTF_8.java:76)
        at sun.nio.cs.UTF_8$Encoder.encodeArrayLoop(UTF_8.java:411)
        at sun.nio.cs.UTF_8$Encoder.encodeLoop(UTF_8.java:466)
        at java.nio.charset.CharsetEncoder.encode(CharsetEncoder.java:561)
        at java.lang.StringCoding$StringEncoder.encode(StringCoding.java:258)
        at java.lang.StringCoding.encode(StringCoding.java:290)
        at java.lang.String.getBytes(String.java:954)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:64)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:46)
        at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1007)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
        at org.apache.cassandra.io.sstable.SSTableBoundedScanner.<init>(SSTableBoundedScanner.java:41)
        at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:869)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:247)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
.

(about 900 lines deleted)
.


        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:202)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:147)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:90)
        at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:47)
        at org.apache.cassandra.db.compaction.CompactionIterable.iterator(CompactionIterable.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:703)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:69)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:442)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
","debian
OpenJDK 64-Bit Server VM/1.6.0_18
Heap size: 8341422080/8342470656",,,,,,,,,,,,,,,,,,30/Aug/12 16:09;jbellis;4587.txt;https://issues.apache.org/jira/secure/attachment/12543115/4587.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-30 16:09:51.751,,,no_permission,,,,,,,,,,,,256269,,,Thu Aug 30 18:46:59 UTC 2012,,,,,,0|i0gxfb:,96845,yukim,yukim,,,,,,,,,,30/Aug/12 16:09;jbellis;patch to simplify LeveledScanner.computeNext and avoid recursion,30/Aug/12 18:34;yukim;+1,30/Aug/12 18:46;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dead lock in mutation stage when many concurrent writes to few columns,CASSANDRA-4578,12605040,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,suguru,suguru,27/Aug/12 04:46,12/Mar/19 14:05,13/Mar/19 22:27,01/Nov/12 19:34,1.1.5,,,,,0,,,,,,,"When I send many request to increment counters to few counter columns, sometimes mutation stage cause dead lock. When it happened, all of mutation threads are locked and do not accept updates any more.

{noformat}
""MutationStage:432"" - Thread t@1389
   java.lang.Thread.State: TIMED_WAITING
	at java.lang.Object.wait(Native Method)
	- waiting on <b90b45b> (a org.apache.cassandra.utils.SimpleCondition)
	at java.lang.Object.wait(Object.java:443)
	at java.util.concurrent.TimeUnit.timedWait(TimeUnit.java:292)
	at org.apache.cassandra.utils.SimpleCondition.await(SimpleCondition.java:54)
	at org.apache.cassandra.service.AbstractWriteResponseHandler.get(AbstractWriteResponseHandler.java:55)
	at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:51)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

   Locked ownable synchronizers:
	- locked <4b1b0a6f> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
{noformat}","15 cassandra instances
CentOS5
8 Core 64GB Memory

java version ""1.6.0_33""
Java(TM) SE Runtime Environment (build 1.6.0_33-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.8-b03, mixed mode)
",,,,,,,,,,,,,,,,,,31/Oct/12 16:56;zznate;4578-1.0-backport.txt;https://issues.apache.org/jira/secure/attachment/12551577/4578-1.0-backport.txt,06/Sep/12 13:02;slebresne;4578.txt;https://issues.apache.org/jira/secure/attachment/12544039/4578.txt,27/Aug/12 04:46;suguru;threaddump-1344957574788.tdump;https://issues.apache.org/jira/secure/attachment/12542575/threaddump-1344957574788.tdump,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-08-27 14:34:20.761,,,no_permission,,,,,,,,,,,,253438,,,Thu Nov 01 19:34:07 UTC 2012,,,,,,0|i0dnin:,77739,jbellis,jbellis,,,,,,,,,,27/Aug/12 04:46;suguru;Attached thread dump,"27/Aug/12 14:34;jbellis;You're right, since CMVH grabs a writer thread until it gets replies from the other replicas, you can have two replicas deadlock with A waiting for a reply from B, and B waiting for a reply from A.

One fix would be to move the local write into CMVH and the remote part into a separate stage (or maybe just a custom callback).

As a workaround, use CL.ONE with counters.",06/Sep/12 13:02;slebresne;Attaching patch to use a callback (as it avoids creating lots of thread that just spend time waiting on a condition) to send back the response from CMVH.,"06/Sep/12 19:59;jbellis;Would prefer to have the callback final in constructor to make it more clear that it doesn't get changed during processing, otherwise +1","07/Sep/12 08:52;slebresne;bq. Would prefer to have the callback final in constructor

I initially feared pushing the callback to the constructor would artifically require too much code changes but looking closer it doesn't really, so committed with that changed.
",31/Oct/12 16:54;zznate;Re-open for backport to 1.0.x,"31/Oct/12 16:56;zznate;Against cassandra-1.0 latest. Only differs in line numbers, otherwise no issues. All tests pass.","01/Nov/12 19:34;jbellis;Committed the backport to the 1.0 branch.

Not tagging with a 1.0 version since there are no plans for an official 1.0.13 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make CQL3 the default,CASSANDRA-4640,12606864,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,10/Sep/12 13:13,12/Mar/19 14:05,13/Mar/19 22:27,10/Sep/12 16:51,1.2.0 beta 1,,Legacy/CQL,,,0,,,,,,,"In 1.2, CQL3 will be final and thus I believe we should make it the default for CQL (and thus cqlsh).

Of course CQL2 will still be available, one will just have to call set_cql_version (for thrift) or 'cqlsh -2'.",,,,,,,,,,,,,,,,,,,10/Sep/12 13:14;slebresne;4640.txt;https://issues.apache.org/jira/secure/attachment/12544464/4640.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-10 13:37:31.827,,,no_permission,,,,,,,,,,,,256308,,,Mon Sep 10 16:51:23 UTC 2012,,,,,,0|i0gxzj:,96936,jbellis,jbellis,,,,,,,,,,10/Sep/12 13:37;jbellis;Maybe add to NEWS that set_cql_version is available and supported in 1.1?  Otherwise +1.,10/Sep/12 16:51;slebresne;Committed (with the mention that set_cql_version was already supported in 1.1). Thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql version race condition with rpc_server_type: sync,CASSANDRA-4657,12607257,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,ecourreges,ecourreges,12/Sep/12 15:10,12/Mar/19 14:05,13/Mar/19 22:27,08/Oct/12 16:05,1.1.6,,,,,0,features,,,,,,"If clients connect to a cassandra cluster configured with rpc_server_type: sync with heterogeneous cql versions (2 and 3), the cql version used for execution on the server changes seemingly randomly.
It's due to the fact that CustomTThreadPoolServer.java does not set the remoteSocket anytime, or does not clear the cql version in the ThreadLocal clientState object.
When CassandraServer.java calls state() it gets the ThreadLocal object clientState, which has its cqlversion already changed by a previous socket that was using the same thread.


The easiest fix is probably to do a SocketSessionManagementService.instance.set when accepting a new client and SocketSessionManagementService.instance.remove when the client is closed, but if you really want to use the ThreadLocal clientState and not alloc/destroy a ClientState everytime, then you should clear this clientState on accept of a new client.

The problem can be reproduced with cqlsh -3 on one side and a client that does not set the cql version, expecting to get version 2 by default, but actually gettingv v2/v3 depending on which thread it connects to.

The problem does not happen with other rpc_server_types, nor with clients that set their cql version at connection.",Ubuntu 12.04,,,,,,,,,,,,,,,,,,12/Sep/12 15:54;ecourreges;4657.patch;https://issues.apache.org/jira/secure/attachment/12544831/4657.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-06 03:56:20.169,,,no_permission,,,,,,,,,,,,244165,,,Mon Oct 08 16:05:40 UTC 2012,,,,,,0|i05iu7:,30149,dbrosius,dbrosius,,,,,,,,,,12/Sep/12 15:54;ecourreges;Patch with set and remove,"06/Oct/12 03:56;jbellis;Thanks Emmanuel!

Your patch will work, but I'd like to do a deeper cleanup.

I've pushed this to https://github.com/jbellis/cassandra/branches/4657.

In order these commits

# fix cql version reset under the existing regime of CassandraServer.clientState threadlocal
# centralizes all session management into ThriftSessionManager
# removes the threadlocal approach in favor of standardizing on the socket-based approach everywhere",06/Oct/12 05:17;dbrosius@apache.org;+1 patch lgtm.,"08/Oct/12 16:05;jbellis;committed reset fix to 1.1.6, remainder to trunk (after much rebasing over CASSANDRA-4608, grr)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
minimum stack size for u34 and later is 180k,CASSANDRA-4631,12606609,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,07/Sep/12 15:26,12/Mar/19 14:05,13/Mar/19 22:27,07/Sep/12 16:06,1.0.12,1.1.5,,,,0,,,,,,,"We currently only set the stack to 180k for java 7, but it looks like java 6 u34 and later now need this too.  Let's just set them all to 180k.",,,,,,,,,,,,,,,,,,,07/Sep/12 15:26;brandon.williams;4631.txt;https://issues.apache.org/jira/secure/attachment/12544228/4631.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-07 15:54:29.843,,,no_permission,,,,,,,,,,,,256301,,,Fri Sep 07 15:54:29 UTC 2012,,,,,,0|i0gxvz:,96920,jbellis,jbellis,,,,,,,,,,07/Sep/12 15:26;brandon.williams;Trivial patch.,07/Sep/12 15:54;jbellis;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress:  --enable-cql does not work with COUNTER_ADD,CASSANDRA-4633,12606649,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,cdaw,cdaw,07/Sep/12 19:55,12/Mar/19 14:05,13/Mar/19 22:27,11/Sep/12 19:53,1.2.0,,Legacy/Tools,,,0,,,,,,,"When I remove --enable-cql the following runs successfully.
Note:  INSERT/READ are fine.

{code}
./cassandra-stress --operation=COUNTER_ADD --enable-cql --replication-factor=3 --consistency-level=ONE --num-keys=10000  --columns=20 

total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [1] retried 10 times - error incrementing key 0001 ((InvalidRequestException): cannot parse 'C58' as hex bytes)

Operation [0] retried 10 times - error incrementing key 0000 ((InvalidRequestException): cannot parse 'C58' as hex bytes)

0,0,0,NaN,0
FAILURE
{code}",,,,,,,,,,,,,,,,,,,11/Sep/12 19:46;iamaleksey;CASSANDRA-4633.patch;https://issues.apache.org/jira/secure/attachment/12544694/CASSANDRA-4633.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-11 19:53:51.706,,,no_permission,,,,,,,,,,,,256303,,,Tue Sep 11 19:53:51 UTC 2012,,,,,,0|i0gxwv:,96924,xedin,xedin,,,,,,,,,,"11/Sep/12 19:53;xedin;+1, Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
(CQL3) Re-allow order by on non-selected columns,CASSANDRA-4645,12607078,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,11/Sep/12 15:07,12/Mar/19 14:05,13/Mar/19 22:27,05/Nov/12 16:06,1.2.0 beta 2,,,,,0,,,,,,,"CASSANDRA-4612 added a limitation to ORDER BY query in that it requires the columns part of the ORDER BY to be in the select clause, while this wasn't the case previously.

The reason for that is that for ORDER BY with IN queries, the sorting is done post-query, and by the time we do the ordering, we've already cut down the result set to the select clause, so if the column are not in the select clause we cannot sort on them.

We should remove that that limitation however as this is a regression from what we had before. As far as 1.2.0 is concerned, at the very least we should lift the limitation for EQ queries since we don't do any post-query sorting in that case and that was working correctly pre-CASSANDRA-4612. But we should also remove that limitation for IN query, even if it's in a second time.",,,,,,,,,,,,,,,,,,,05/Nov/12 11:19;slebresne;4645.txt;https://issues.apache.org/jira/secure/attachment/12552087/4645.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-05 15:28:16.018,,,no_permission,,,,,,,,,,,,239465,,,Mon Nov 05 16:06:53 UTC 2012,,,,,,0|i00hjj:,775,jbellis,jbellis,,,,,,,,,,"05/Nov/12 11:19;slebresne;Attaching trivial patch to fix the limitation introduced by CASSANDRA-4612. I.e. it doesn't force the order by columns to be select, unless this is an IN query. I've opened CASSANDRA-4911 to remove the limitation for IN queries, but that part has never worked anyway so it's more of an improvement.",05/Nov/12 15:28;jbellis;+1,"05/Nov/12 16:06;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty .cqlsh_history file causes cqlsh to crash on startup.,CASSANDRA-4669,12607713,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,boneill,boneill,boneill,14/Sep/12 20:55,12/Mar/19 14:05,13/Mar/19 22:27,18/Sep/12 01:44,1.1.6,,Legacy/Tools,,,0,,,,,,,"Not sure how I got it, but I ended up with an empty .cqlsh_history file.  In that state, when starting cqlsh, you end up with:

bone@zen:~/dev/boneill42/cassandra-> bin/cqlsh 
Traceback (most recent call last):
  File ""bin/cqlsh"", line 2588, in <module>
    main(*read_options(sys.argv[1:], os.environ))
  File ""bin/cqlsh"", line 2543, in main
    readline.read_history_file(HISTORY)
IOError: [Errno 22] Invalid argument

Its a simple fix to check for a non-empty history file.  I'll attach the patch.",Python 2.7.1 on Mac OSX ,,,,,,,,,,,,,,,,,,18/Sep/12 02:31;iamaleksey;fix-cqlsh-history.patch;https://issues.apache.org/jira/secure/attachment/12545516/fix-cqlsh-history.patch,14/Sep/12 20:58;boneill;trunk-4669.txt;https://issues.apache.org/jira/secure/attachment/12545209/trunk-4669.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-14 21:47:39.003,,,no_permission,,,,,,,,,,,,256330,,,Tue Sep 18 02:42:50 UTC 2012,,,,,,0|i0gyav:,96987,brandon.williams,brandon.williams,,,,,,,,,,14/Sep/12 20:58;boneill;Here is the patch.,14/Sep/12 20:58;boneill;Patch attached.,14/Sep/12 21:47;brandon.williams;Committed.,"18/Sep/12 01:42;iamaleksey;This patch breaks cqlsh history loading. Also, the original issue isn't an issue at all - readline handles empty history files perfectly well.
Reverting the commit should fix the new issue.",18/Sep/12 01:44;brandon.williams;Reverted.,"18/Sep/12 01:59;boneill;I trust you guys, but you may just want to double check that this isn't an issue.  I can easily reproduce the problem.  Here is a log.  You can see it working initially.  I truncate the file, and no joy.

bone@zen:~/dev/boneill42/cassandra-> bin/cqlsh 
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.5 | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> quit
bone@zen:~/dev/boneill42/cassandra-> rm -fr ~/.cqlsh_history 
bone@zen:~/dev/boneill42/cassandra-> touch ~/.cqlsh_history
bone@zen:~/dev/boneill42/cassandra-> bin/cqlsh 
Traceback (most recent call last):
  File ""bin/cqlsh"", line 2588, in <module>
    main(*read_options(sys.argv[1:], os.environ))
  File ""bin/cqlsh"", line 2543, in main
    readline.read_history_file(HISTORY)
IOError: [Errno 22] Invalid argument
bone@zen:~/dev/boneill42/cassandra-> ","18/Sep/12 02:03;brandon.williams;Maybe we should just catch any error from loading the history, issue a warning, and move along.","18/Sep/12 02:13;iamaleksey;Brian, what are your python/readline versions and OS?

{quote}
 ➤ bin/cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra unknown | CQL spec 3.0.0 | Thrift protocol 19.34.0]
Use HELP for help.
cqlsh> quit
 ➤ rm -fr ~/.cqlsh_history 
 ➤ touch ~/.cqlsh_history
 ➤ bin/cqlsh 
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra unknown | CQL spec 3.0.0 | Thrift protocol 19.34.0]
Use HELP for help.
cqlsh>
{quote}

Also, shell will write empty .cqlsh_history when you open it for the first time and close it without entering any commands. And will open just fine next time.

Brandon, or do that, yes. Then we can get rid of os.path.exists(HISTORY) check.","18/Sep/12 02:28;boneill;
Not sure how to tell exact version of readline. (I'm a java head =)
This is on OSX.  Looks like the version is the one provided by apple in the python install.

Here is what i've got:
bone@zen:~/dev/boneill42/cassandra-> python
Python 2.7.1 (r271:86832, Jul 31 2011, 19:30:53) 
[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import readline
>>> readline
<module 'readline' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/readline.so'>

FWW, this can be a really annoying error unless you can read Python.  It doesn't get far enough to recrete the history file.  So, the problem persists. Your average user would be stuck.  I first reinstalled Cassandra because I thought something was corrupt.  Fortunately, python isn't compiled. =)
","18/Sep/12 02:36;iamaleksey;Ah, OS X. Maybe it really does fail on OS X. Works on Ubuntu though. Attached a patch that catches IOError when reading/writing from/to history file.",18/Sep/12 02:36;boneill;Beautiful.  Thanks guys. ,18/Sep/12 02:42;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix binary protocol NEW_NODE event,CASSANDRA-4679,12608065,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,18/Sep/12 14:01,12/Mar/19 14:05,13/Mar/19 22:27,05/Nov/12 16:36,1.2.0 beta 2,,,,,0,,,,,,,"As discussed on CASSANDRA-4480, the NEW_NODE/REMOVED_NODE of the binary protocol are not correctly fired (NEW_NODE is fired on node UP basically). This ticket is to fix that.",,,,,,,,,,,,,,,,,,,30/Oct/12 07:24;slebresne;0001-4679.txt;https://issues.apache.org/jira/secure/attachment/12551313/0001-4679.txt,30/Oct/12 07:24;slebresne;0002-Start-RPC-binary-protocol-before-gossip.txt;https://issues.apache.org/jira/secure/attachment/12551314/0002-Start-RPC-binary-protocol-before-gossip.txt,29/Oct/12 17:24;slebresne;0003-Remove-hardcoded-initServer-from-AntiEntropyServiceTes.txt;https://issues.apache.org/jira/secure/attachment/12551212/0003-Remove-hardcoded-initServer-from-AntiEntropyServiceTes.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-10-01 13:54:44.098,,,no_permission,,,,,,,,,,,,239456,,,Mon Nov 05 16:36:48 UTC 2012,,,,,,0|i00hgn:,762,yukim,yukim,,,,,,,,,,"18/Sep/12 14:19;slebresne;Attaching patch that adds a new listener interface for ""higher level"" events than the gossip ones.",01/Oct/12 10:15;slebresne;I've rebased the patch and added a short paragraph to the protocol doc to explain that UP events are usually sent a few milliseconds before the binary protocol server is fully up (since gossip is started before the protocol server).,01/Oct/12 13:54;jbellis;We can always change that ordering.  Maybe we should.,"01/Oct/12 14:15;slebresne;I suppose we can, yes. Though I suspect there may be races so that some queries (I'm thinking schema altering ones though honestly I haven't checked) might trigger assertion errors or related if gossip isn't started. Probably worth checking though. Maybe in a separate ticket however since it's not directly related (clients will have a to handle the case where after a NEW_NODE the node cannot be joined anyway).",23/Oct/12 14:03;slebresne;I've rebased the patch and attaching a second patch to start the binary protocol server before gossip. This seems to be working as expected but the fact that bootstrap is broken on trunk (due to CASSANDRA-4764) makes it a bit harder to test correctly.,26/Oct/12 11:24;slebresne;I've also updated the first patch to send a TOPOLOGY_CHANGE event when a node has moved since that was missing and trivial to add.,"29/Oct/12 16:43;yukim;Patch looks good, though AntiEntropyServiceStandardTest/AntiEntropyServiceCounterTest is failing after applying this patch.","29/Oct/12 17:24;slebresne;Interesting. This is due to the hardcoded call to StorageServer.initServer() in AntiEntropyServiceTestAbstract. But I have absolutely no clue what we have that call. In fact, removing that call (patch 3 attached) fixes the test. I'm not totally sure why the test was working previously, maybe the 2 patch of this ticket just changed the timing of the server initialization triggering that issue?","29/Oct/12 21:04;yukim;I found the difference between patched and trunk.
Your initServer tries to ""join ring"" even after server is initialized, whilst in trunk it is guarded by ""initialized"" check.
I think it is better to check if initialized before calling matbeJoinRing in your patch.","30/Oct/12 07:26;slebresne;Ok, you're right and I've updated the first patch to add back the same 'is initialized' check. I leave the third patch though since that hardcoded call is still useless (but the test pass even if we don't remove it).",30/Oct/12 14:28;yukim;+1 for all patches.,"30/Oct/12 15:13;slebresne;Committed, thanks","05/Nov/12 16:03;brandon.williams;Reopening because (at least sometimes) this allows thrift to start before MS and gossip, which is wrong, but also other things seem to be breaking like bootstrap.","05/Nov/12 16:14;slebresne;As much as I'm happy to revert the part about starting the thrift/binary protocol server first, I'd prefer some precision about ""breaks all kind of things"". That is, does that break things only if people query the thrift interface before MS and gossip are up (which would make sense, but in real life doesn't seem like very likely to happen (which doesn't mean we shouldn't fix it btw)), or does it break things in other cases?","05/Nov/12 16:16;brandon.williams;Sorry, edited my comment to clarify while you were posting yours :)","05/Nov/12 16:36;slebresne;Alright, I've reverted the 'start thrift before gossip part of this patch'. That has never been the most important part anyway. This should fix any regression at least as far as this ticket is concerned.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in NTS when using LQ against a node (DC) that doesn't have replica,CASSANDRA-4675,12607993,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,18/Sep/12 01:23,12/Mar/19 14:05,13/Mar/19 22:27,27/Sep/12 14:31,1.1.6,,,,,0,,,,,,,"in a NetworkTopologyStrategy where there are 2 DC:

{panel}
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
127.0.0.1       dc1         r1          Up     Normal  115.78 KB       50.00%  0                                           
127.0.0.2       dc2         r1          Up     Normal  129.3 KB        50.00%  85070591730234615865843651857942052864  
{panel}
I have a KS that has replica is 1 of the dc (dc1):

{panel}
[default@unknown] describe Keyspace3;                                                                                                                     
Keyspace: Keyspace3:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [dc1:1]
  Column Families:
    ColumnFamily: testcf
{panel}

But if I connect to a node in dc2, using LOCAL_QUORUM, I get NPE in the Cassandra node's log:

{panel}
[default@unknown] consistencylevel as LOCAL_QUORUM;                       
Consistency level is set to 'LOCAL_QUORUM'.
[default@unknown] use Keyspace3;                                          
Authenticated to keyspace: Keyspace3
[default@Keyspace3] get testcf[utf8('k1')][utf8('c1')];                     
Internal error processing get
org.apache.thrift.TApplicationException: Internal error processing get
        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:511)
        at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:492)
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:648)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:348)
{panel}

node2's log:
{panel}
ERROR [Thrift:3] 2012-09-17 18:15:16,868 Cassandra.java (line 2999) Internal error processing get
java.lang.NullPointerException
        at org.apache.cassandra.locator.NetworkTopologyStrategy.getReplicationFactor(NetworkTopologyStrategy.java:142)
        at org.apache.cassandra.service.DatacenterReadCallback.determineBlockFor(DatacenterReadCallback.java:90)
        at org.apache.cassandra.service.ReadCallback.<init>(ReadCallback.java:67)
        at org.apache.cassandra.service.DatacenterReadCallback.<init>(DatacenterReadCallback.java:63)
        at org.apache.cassandra.service.StorageProxy.getReadCallback(StorageProxy.java:775)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:609)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:564)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.internal_get(CassandraServer.java:383)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:401)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:2989)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
{panel}

I could workaround it by adding dc2:0 to the option:

{panel}
[default@Keyspace3] describe Keyspace3;                                           
Keyspace: Keyspace3:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [dc2:0, dc1:1]
  Column Families:
    ColumnFamily: testcf
{panel}

Now you get UA:

{panel}
[default@Keyspace3] get testcf[utf8('k1')][utf8('c1')];                           
null
UnavailableException()
        at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6506)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:519)
        at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:492)
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:648)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:348)
{panel}


On a side note, is there a thought on having a CL.LOCAL_ONE? Ie if local node (wrt the dc) does not have replica, on a LOCAL_ONE, it won't try to go across DC to try to get it. It would be similar to LOCAL_QUORUM.",,,,,,,,,,,,,,,,,,,26/Sep/12 22:47;jbellis;4675.txt;https://issues.apache.org/jira/secure/attachment/12546768/4675.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-18 15:14:49.614,,,no_permission,,,,,,,,,,,,256332,,,Mon Oct 01 22:12:57 UTC 2012,,,,,,0|i0gycn:,96995,brandon.williams,brandon.williams,,,,,,,,,,18/Sep/12 15:14;jbellis;Not sure what version this stacktrace is supposed to be against.  Doesn't appear to be against a recent 1.0 or 1.1.,18/Sep/12 22:21;cywjackson;1.0.10,"26/Sep/12 22:47;jbellis;Following ""explicit is better than implicit"" convention, I was inclined to make it raise an InvalidRequestException instead of failing with NPE.  But the request isn't invalid; it's a configuration problem.

After discussion on IRC, decided the best thing to do is just default undefined DC to zero replicas.  Patch attached to do that.",26/Sep/12 23:37;brandon.williams;+1,27/Sep/12 14:31;jbellis;committed,01/Oct/12 22:12;cywjackson;+1 thx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
_TRACE verb is not droppable which causes an AssertionError,CASSANDRA-4672,12607934,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dr-alves,dr-alves,dr-alves,17/Sep/12 18:49,12/Mar/19 14:05,13/Mar/19 22:27,22/Oct/12 21:32,1.2.0 beta 2,,,,16/Sep/12 00:00,0,,,,,,,When a big enough statement is traced (like select *) an assertion error is fired because the _TRACE verb is not droppable.,,,,,,,,,,,,,,,,,,,17/Sep/12 18:52;dr-alves;4672.patch;https://issues.apache.org/jira/secure/attachment/12545454/4672.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-26 22:16:06.375,,,no_permission,,,,,,,,,,,,250448,,,Mon Oct 22 21:32:30 UTC 2012,,,,,,0|i0ayl3:,61876,jbellis,jbellis,,,,,,,,,,17/Sep/12 18:52;dr-alves;attaching trivial fix ,"17/Sep/12 18:52;dr-alves;trivial fix attached
","26/Sep/12 22:16;jbellis;I get making _TRACE droppable, but what is the rest of the patch doing?",22/Oct/12 21:32;jbellis;committed the droppableVerb part,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't require quotes for true and false,CASSANDRA-4776,12610780,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,jsanda,jsanda,08/Oct/12 01:20,12/Mar/19 14:05,13/Mar/19 22:27,25/Oct/12 16:00,1.2.0 beta 2,,,,,0,cql3,,,,,,"The docs at http://cassandra.apache.org/doc/cql3/CQL.html#identifiers describe using double quotes for an identifier that is a reserved word. The following works as expected,

cqlsh:test> select ""columnfamily"" from system.schema_columnfamilies;

I have a table with a boolean column. In order to insert a boolean value, I have to enclose it in single quotes. The table looks like,

CREATE TABLE bool_test (
  id int PRIMARY KEY,
  val boolean
);

Here is what happens when I try using double quotes,

cqlsh:rhq> insert into bool_test (id, val) values (4, ""false"");
Bad Request: line 1:43 no viable alternative at input 'false'


The use of single quotes here seems inconsistent with what is described in the docs, and makes things a bit confusing. It would be nice if single or double quotes could be used for identifiers that are reserved words. I also think it is a bit counter-intuitive to require quotes for true and false which are literal values.","Mac OS X, Fedora 16",,,,,,,,,,,,,,,,,,25/Oct/12 13:51;iamaleksey;4776-cqlsh.txt;https://issues.apache.org/jira/secure/attachment/12550774/4776-cqlsh.txt,25/Oct/12 10:47;slebresne;4776.txt;https://issues.apache.org/jira/secure/attachment/12550765/4776.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-08 15:59:58.506,,,no_permission,,,,,,,,,,,,245004,,,Thu Oct 25 16:00:01 UTC 2012,,,,,,0|i05xiv:,32530,jbellis,jbellis,,,,,,,,,,"08/Oct/12 15:59;jbellis;# it sounds like you're complaining that the server uses single quotes to denote the input it couldn't parse.  that sounds completely unreleated to how keywords must be quoted, and a non-problem.
# open to making true/false not require quoting but if it's a pita i'm also fine with leaving this alone","08/Oct/12 16:07;slebresne;In fact there is a documentation bug as the doc pretends that <identifier> are valid <term> but they are not (a <term> is a value, an <identifier> is not a value, not in CQL at least). So that should be fixed. So, with the documentation fixed, it should be clear that you should use single quotes because that's a string you should pass.

That being said, I'm also open at making true/false being values themselves (without quoting).","25/Oct/12 02:05;iamaleksey;I agree with making true/false valid identifiers, but don't understand what's otherwise expected from this issue. It's a documentation bug, and that doc has quite some outdated stuff (assuming it's intended to reflect latest CQL3 and not 3.0.0-beta1).",25/Oct/12 03:24;jbellis;Updated title.,25/Oct/12 03:38;jsanda;Where/how can I access the latest CQL3 docs? I just ran ant generate-cql-html from trunk where my HEAD is a commit from 10/17. I am not sure how different that is from the doc I cited in the description. ,25/Oct/12 10:47;slebresne;Attaching the fairly trivial patch to make boolean literals.,"25/Oct/12 10:50;slebresne;bq. Where/how can I access the latest CQL3 docs?

The good question is when. There has been many change to CQL3 on trunk but the doc hasn't been updated yet. We'll update it for the release though.",25/Oct/12 13:38;jbellis;+1,25/Oct/12 13:40;iamaleksey;Please don't commit it yet - I'll attach a patch for cqlsh as well. Soon.,"25/Oct/12 16:00;slebresne;Alright, committed both patches, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
(CQL3) data type not in lowercase are not handled correctly.,CASSANDRA-4770,12610588,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,05/Oct/12 15:56,12/Mar/19 14:05,13/Mar/19 22:27,05/Oct/12 16:56,1.1.6,,,,,0,cql3,,,,,,"Seems that we accept {{int}} but we don't accept {{INT}} (that is, the parser accepts it, but we fail later to recognize it).",,,,,,,,,,,,,,,,,,,05/Oct/12 16:00;slebresne;4770.txt;https://issues.apache.org/jira/secure/attachment/12547998/4770.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-05 16:44:18.797,,,no_permission,,,,,,,,,,,,243598,,,Fri Oct 05 16:56:12 UTC 2012,,,,,,0|i04bun:,23185,jbellis,jbellis,,,,,,,,,,05/Oct/12 16:00;slebresne;Trivial fix attached.,05/Oct/12 16:44;jbellis;+1,"05/Oct/12 16:56;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException in org.apache.cassandra.gms.Gossiper.sendGossip,CASSANDRA-4774,12610704,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cnlwsu,bcoverston,bcoverston,06/Oct/12 13:14,12/Mar/19 14:05,13/Mar/19 22:27,01/Aug/13 14:36,1.2.9,,,,,0,,,,,,,"ERROR [GossipTasks:1] 2012-10-06 10:47:48,390 Gossiper.java (line 169) Gossip error
java.lang.IndexOutOfBoundsException: Index: 13, Size: 5
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.gms.Gossiper.sendGossip(Gossiper.java:541)
	at org.apache.cassandra.gms.Gossiper.doGossipToUnreachableMember(Gossiper.java:575)
	at org.apache.cassandra.gms.Gossiper.access$300(Gossiper.java:59)
	at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:141)
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)",Saw this when looking through some logs in version 1.0.0 system was under a lot of load.,,,,,,,,,,,,,,,,,,31/Jul/13 18:45;cnlwsu;patch.txt;https://issues.apache.org/jira/secure/attachment/12595236/patch.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-13 18:47:15.499,,,no_permission,,,,,,,,,,,,244709,,,Thu Aug 01 14:36:40 UTC 2013,,,,,,0|i05q87:,31346,brandon.williams,brandon.williams,,,,,,,,,,"13/Nov/12 18:47;brandon.williams;Line 541 in 1.0.0 is in doGossipToSeed, are you certain about the version?  I'm not sure what to look at.","31/Jul/13 18:28;cnlwsu;Saw this once on a 1.1.5 node under high load with significant heap pressure, GCInspector reported 95% full right before.

{code}
 WARN [ScheduledTasks:1] 2013-07-07 19:37:03,834 GCInspector.java (line 145) Heap is 0.9552299542288667 full.  You may need to reduce memtable and/or cache sizes.  Cassandra will now flush up to the two largest memtables to free up memory.  Adjust flush_largest_memtables_at threshold in cassandra.yaml if you don't want Cassandra to do this automatically
 WARN [ScheduledTasks:1] 2013-07-07 19:37:03,834 StorageService.java (line 2855) Flushing CFS(Keyspace='x', ColumnFamily='x') to relieve memory pressure
 INFO [ScheduledTasks:1] 2013-07-07 19:37:03,834 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-x@766608353(261434/1801824 serialized/live bytes, 5150 ops)
 INFO [GossipStage:1] 2013-07-07 19:37:05,125 Gossiper.java (line 816) InetAddress /10.x.x.x is now UP
 INFO [GossipStage:1] 2013-07-07 19:37:05,146 Gossiper.java (line 816) InetAddress /10.x.x.x is now UP
ERROR [GossipTasks:1] 2013-07-07 19:37:05,155 Gossiper.java (line 171) Gossip error
java.lang.IndexOutOfBoundsException: Index: 10, Size: 10
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.gms.Gossiper.sendGossip(Gossiper.java:560)
	at org.apache.cassandra.gms.Gossiper.doGossipToUnreachableMember(Gossiper.java:594)
	at org.apache.cassandra.gms.Gossiper.access$300(Gossiper.java:61)
	at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:143)
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}
","31/Jul/13 18:46;cnlwsu;doGossipToUnreachableMember calls
{code}
sendGossip(prod, unreachableEndpoints.keySet());
{code}
the keyset returned is backed by the set so changes are reflected in the set.  Since sendGossip gets the size, then picks the random number, then does a get on a list created it created a race condition where the list is smaller then the keyset.  attached a possible change to do this.

note: patch off of 1.1","01/Aug/13 14:36;brandon.williams;Your analysis sounds spot on and the timing proximity in the log seems to back it up.  Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NPE with some load of writes, but possible snitch setting issue for a cluster",CASSANDRA-4728,12609349,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,alexliu68,alexliu68,27/Sep/12 05:11,12/Mar/19 14:05,13/Mar/19 22:27,28/Sep/12 14:41,1.2.0 beta 2,,,,,0,snitch,,,,,,"The following errors are showing under height load

ERROR [MutationStage:8294] 2012-09-25 22:01:47,628 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:8294,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.locator.PropertyFileSnitch.getDatacenter(PropertyFileSnitch.java:104)
	at com.datastax.bdp.snitch.DseDelegateSnitch.getDatacenter(DseDelegateSnitch.java:69)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:122)
	at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:93)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:100)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1984)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1972)
	at org.apache.cassandra.service.StorageProxy.getWriteEndpoints(StorageProxy.java:262)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:248)
	at org.apache.cassandra.service.StorageProxy.applyCounterMutationOnLeader(StorageProxy.java:505)
	at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:56)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)


ERROR [MutationStage:13164] 2012-09-25 22:19:06,486 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13164,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [Thrift:12] 2012-09-25 22:19:07,433 Cassandra.java (line 3462) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [Thrift:16] 2012-09-25 22:19:07,437 Cassandra.java (line 2999) Internal error processing get


java.lang.NullPointerException
 INFO [GossipStage:280] 2012-09-26 00:15:15,371 Gossiper.java (line 818) InetAddress /172.16.233.208 is now dead.
ERROR [GossipStage:280] 2012-09-26 00:15:15,372 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:280,5,main]
j

ERROR [MutationStage:40529] 2012-09-26 00:15:21,527 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:40529,5,main]
java.lang.NullPointerException
 INFO [GossipStage:281] 2012-09-26 00:15:23,013 Gossiper.java (line 818) InetAddress /172.16.232.159 is now dead.
ERROR [GossipStage:281] 2012-09-26 00:15:23,014 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:281,5,main]
",,,,,,,,,,,,,,,,,,,27/Sep/12 20:56;jbellis;4728.txt;https://issues.apache.org/jira/secure/attachment/12546903/4728.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-27 17:10:34.435,,,no_permission,,,,,,,,,,,,255201,,,Thu Sep 27 21:36:10 UTC 2012,,,,,,0|i0eq0v:,83978,brandon.williams,brandon.williams,,,,,,,,,,27/Sep/12 17:10;brandon.williams;Duplicate of CASSANDRA-4675,"27/Sep/12 17:30;jbellis;Actually not a duplicate, this is a PFS NPE for an unknown host.",27/Sep/12 18:07;jbellis;Would it be sufficient for each (server-mode) node to check that it's present in the PFS file when starting up?,27/Sep/12 20:05;brandon.williams;That sounds like a good solution.,"27/Sep/12 20:56;jbellis;attached, with extra assert for good measure.",27/Sep/12 21:36;brandon.williams;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some operations of HintedHandOffManager bean have wrong output,CASSANDRA-4724,12609250,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,azotcsit,azotcsit,azotcsit,26/Sep/12 14:25,12/Mar/19 14:05,13/Mar/19 22:27,26/Sep/12 15:10,1.1.6,,Legacy/Tools,,,0,hintedhandoff,jmx,,,,,"I suggest to change output of listEndpointsPendingHints and countPendingHints operations in HintedHandOffManager bean.

Current output:
  - listEndpointsPendingHints:
{code}
c�.@ÁM��JprV���c�.@ÁM��JprV���
{code}
  - countPendingHints:
{code}
116570217535704627=1170
{code}

Suggested output:
  - listEndpointsPendingHints:
{code}
localhost/127.0.0.1
{code}
  - countPendingHints:
{code}
localhost/127.0.0.1=1170
{code}

",Cassandra 1.1.2,,,,,,,,,,,,,,,,,,26/Sep/12 14:28;azotcsit;cassandra-1.2-4724-handoff_bean.txt;https://issues.apache.org/jira/secure/attachment/12546702/cassandra-1.2-4724-handoff_bean.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-26 15:10:01.538,,,no_permission,,,,,,,,,,,,255205,,,Wed Sep 26 18:39:55 UTC 2012,,,,,,0|i0eq1r:,83982,jbellis,jbellis,,,,,,,,,,"26/Sep/12 15:10;jbellis;Every Row will have the same key for all the hints it contains.  Could use that to avoid needing the extra Map<ByteBuffer, InetAddress>.

That said, I don't think using the inetaddress instead of the token is really an improvement... it's keyed off of token on purpose so that if the actual machine owning that token changes, we can still deliver the hints.

(For {{listEndpointsPendingHints}}, using new String(key.array) is a bug, I've changed it to use the same tokenFactory.toString that {{count}} was using in commit e752de96f7e3ae676b5dba0564e1321d6661a0cc on the 1.1 branch.)","26/Sep/12 18:39;azotcsit;Ok, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql 2 counter validations need to use default consistencylevel if none is explicitly given,CASSANDRA-4700,12608695,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,niteesh.kumar,niteesh.kumar,niteesh.kumar,21/Sep/12 21:05,12/Mar/19 14:05,13/Mar/19 22:27,24/Sep/12 17:47,1.1.6,,,,,0,,,,,,,"i was trying to run cql 2 query

cqlsh:stats> UPDATE Minutewise_Product_Stats SET '2LX:OQ:XYZ.com:664230591:1:totalView'='2LX:SOQ:XYZ.com:664230591:1:totalView'+1, '2LX:OQ:XYZ.com:664230591:1:keywordClick'='2LX:SOQ:xyz.com:664230591:1:keywordClick'+1 WHERE KEY='2017:4' ;

WHEN I GOT  this error

ERROR 20:38:46,220 Error occurred during processing of message.
java.lang.NullPointerException
    at org.apache.cassandra.cql.UpdateStatement.prepareRowMutations(UpdateStatement.java:151)
    at org.apache.cassandra.cql.UpdateStatement.prepareRowMutations(UpdateStatement.java:128)
    at org.apache.cassandra.cql.QueryProcessor.batchUpdate(QueryProcessor.java:245)
    at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:563)
    at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:817)
    at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1675)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:1)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)


attached is the patch file  to fix this bug

CQL2fix.patch

diff --git a/src/java/org/apache/cassandra/cql/UpdateStatement.java b/src/java/org/apache/cassandra/cql/UpdateStatement.java
index 3a47712..0caa61b 100644
--- a/src/java/org/apache/cassandra/cql/UpdateStatement.java
+++ b/src/java/org/apache/cassandra/cql/UpdateStatement.java
@@ -146,8 +146,11 @@ public class UpdateStatement extends AbstractModification
         }
 
         CFMetaData metadata = validateColumnFamily(keyspace, columnFamily, hasCommutativeOperation);
-        if (hasCommutativeOperation)
-            cLevel.validateCounterForWrite(metadata);
+        
+        if (hasCommutativeOperation){
+        	ConsistencyLevel currentCLevel = getConsistencyLevel();
+        	currentCLevel.validateCounterForWrite(metadata);
+        }
 
         QueryProcessor.validateKeyAlias(metadata, keyName);
 
",java,,,,,,,,,,,,,,,,,,21/Sep/12 21:08;niteesh.kumar;CQL2fix.patch;https://issues.apache.org/jira/secure/attachment/12546090/CQL2fix.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-24 17:47:07.377,,,no_permission,,,,,,,,,,,,256343,,,Mon Sep 24 17:47:07 UTC 2012,,,,,,0|i0gyl3:,97033,jbellis,jbellis,,,,,,,,,,"21/Sep/12 21:07;niteesh.kumar;
diff --git a/src/java/org/apache/cassandra/cql/UpdateStatement.java b/src/java/org/apache/cassandra/cql/UpdateStatement.java
index 3a47712..0caa61b 100644
--- a/src/java/org/apache/cassandra/cql/UpdateStatement.java
+++ b/src/java/org/apache/cassandra/cql/UpdateStatement.java
@@ -146,8 +146,11 @@ public class UpdateStatement extends AbstractModification
         }
 
         CFMetaData metadata = validateColumnFamily(keyspace, columnFamily, hasCommutativeOperation);
-        if (hasCommutativeOperation)
-            cLevel.validateCounterForWrite(metadata);
+        
+        if (hasCommutativeOperation){
+        	ConsistencyLevel currentCLevel = getConsistencyLevel();
+        	currentCLevel.validateCounterForWrite(metadata);
+        }
 
         QueryProcessor.validateKeyAlias(metadata, keyName);
 
","24/Sep/12 17:47;jbellis;committed to trunk and backported to 1.1 (where the error has existed since 0.8, only more subtly since it doesn't NPE)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
(CQL3) Missing validation for IN queries on column not part of the PK,CASSANDRA-4709,12608856,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,24/Sep/12 12:12,12/Mar/19 14:05,13/Mar/19 22:27,09/Oct/12 14:16,1.1.6,,,,,0,,,,,,,"Copy-pasting from the original mail (http://mail-archives.apache.org/mod_mbox/cassandra-user/201209.mbox/%3C20120922185826.GO6205@pslp2%3E):
{noformat}
[cqlsh 2.2.0 | Cassandra 1.1.5 | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> 
cqlsh> create keyspace xpl1 WITH strategy_class ='SimpleStrategy' and strategy_options:replication_factor=1;
cqlsh> use xpl1;
cqlsh:xpl1> create table t1 (pk varchar primary key, col1 varchar, col2 varchar);
cqlsh:xpl1> create index t1_c1 on t1(col1);
cqlsh:xpl1> create index t1_c2 on t1(col2);
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1a','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1b','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1c','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk2','foo2','bar2');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk3','foo3','bar3');
cqlsh:xpl1> select * from t1 where col2='bar1';
 pk   | col1 | col2
------+------+------
 pk1b | foo1 | bar1
  pk1 | foo1 | bar1
 pk1a | foo1 | bar1
 pk1c | foo1 | bar1

cqlsh:xpl1> select * from t1 where col2 in ('bar1', 'bar2') ;
cqlsh:xpl1> 
{noformat}

We should either make that last query work or refuse the query but returning nothing is wrong.",,,,,,,,,,,,,,,,,,,25/Sep/12 13:31;slebresne;4709.txt;https://issues.apache.org/jira/secure/attachment/12546513/4709.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-25 16:52:09.624,,,no_permission,,,,,,,,,,,,239445,,,Tue Oct 09 14:16:27 UTC 2012,,,,,,0|i00han:,735,jbellis,jbellis,,,,,,,,,,25/Sep/12 13:31;slebresne;For now I think we should just refuse the query since this would require secondary indexes to do an OR which they can't do right now. Attaching patch that simply refuse such queries.,25/Sep/12 16:52;jbellis;+1,"09/Oct/12 14:16;slebresne;Forgot to mark that one resolved somehow, doing it now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh fails to format values of ReversedType-wrapped classes,CASSANDRA-4717,12609084,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,25/Sep/12 16:54,12/Mar/19 14:05,13/Mar/19 22:27,03/Oct/12 15:31,1.1.6,,Legacy/Tools,,,0,cqlsh,,,,,,"See the test case for CASSANDRA-4715, but run it on trunk. The ReversedType-wrapped column (rdate) will be displayed as a floating-point integer (it gets deserialized into a native type correctly, but cqlsh's format-according-to-type machinery doesn't know how to handle the cqltypes.ReversedType subclass.",,,,,,,,,,,,,,,,,,,03/Oct/12 07:49;slebresne;4717-1.1.txt;https://issues.apache.org/jira/secure/attachment/12547509/4717-1.1.txt,25/Sep/12 17:18;thepaul;4717-test.patch.txt;https://issues.apache.org/jira/secure/attachment/12546552/4717-test.patch.txt,25/Sep/12 17:18;thepaul;4717.patch.txt;https://issues.apache.org/jira/secure/attachment/12546551/4717.patch.txt,03/Oct/12 07:49;slebresne;4717.txt;https://issues.apache.org/jira/secure/attachment/12547508/4717.txt,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-09-25 17:03:45.105,,,no_permission,,,,,,,,,,,,255210,,,Wed Oct 03 15:31:45 UTC 2012,,,,,,0|i0eq2v:,83987,tjake,tjake,,,,,,,,,,"25/Sep/12 17:03;jbellis;Is cql3 not giving back the ""base"" type in resultset info?  If not I think that should be addressed server-side.

But if this is a a cql2-only problem then it's not on my priority list...",25/Sep/12 17:04;jbellis;Also should we close CASSANDRA-4715 as duplicate?,"25/Sep/12 17:18;thepaul;4717.patch.txt is a fix; 4717-test.patch.txt adds a test for it that fits in the framework from CASSANDRA-3920, if you guys decide to add that.","25/Sep/12 17:21;thepaul;I'm not sure whether it's better for the thrift interface to include ReversedType in the resultset or not, but the cql library can handle it either way. And this fix for cqlsh is pretty simple too.

If you do want to change the interface so that ReversedType doesn't get reported, then 4715 probably is a dupe. Otherwise, it's a different problem with a different solution.

This isn't specific to cql2.","26/Sep/12 12:37;slebresne;bq. Is cql3 not giving back the ""base"" type in resultset info? If not I think that should be addressed server-side.

It's not currently but that's definitively an oversight. I'll fix it (and thus CASSANDRA-4715 is a dupe).

That being said, I'm fine committing Paul's patch in the meantime to solve the issue for cql2, but there seems to be some kind of mess in the cqlsh sources on the cassandra-1.1 as both displaying.py and formatting.py are missing (as shown here: https://github.com/apache/cassandra/tree/cassandra-1.1/pylib/cqlshlib; trunk is fine).","26/Sep/12 16:36;thepaul;Sorry, you're right, this patch was only for trunk. The 1.1 branch doesn't have formatting.py or displaying.py. The patch will be easy to adapt- I can do it within a day or two, if no one else picks it up",03/Oct/12 07:49;slebresne;Attaching patch to skip returning a ReversedType to CQL3 clients. I'm attaching a separate patch for 1.1 and trunk because they are different enough.,03/Oct/12 12:57;tjake;+1 I like this idea,"03/Oct/12 15:31;slebresne;Alright, committed, thanks.

I'm closing that ticket because this solves the original issue. But I'm still fine committing paul's patch for the sake of CQL2 once the 1.1 version of the patch is attached.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DateType in Column MetaData causes server crash,CASSANDRA-4842,12612807,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,devdazed,devdazed,20/Oct/12 01:20,12/Mar/19 14:05,13/Mar/19 22:27,24/Oct/12 18:41,1.1.7,1.2.0,,,,0,,,,,,,"when creating a column family with column metadata containing a date, there is a server crash that will prevent startup.

To recreate from the cli:
{code}
create keyspace test;
use test;
create column family foo
  with column_type = 'Standard'
  and comparator = 'CompositeType(LongType,DateType)'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'UTF8Type'
  and column_metadata = [ 
    { column_name : '1234:1350695443433', validation_class : BooleanType} 
  ];
{code}

Produces this error in the logs:

{code}
ERROR 21:11:18,795 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:194)
	at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:141)
	at org.apache.cassandra.thrift.CassandraServer.system_add_column_family(CassandraServer.java:931)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family.getResult(Cassandra.java:3410)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family.getResult(Cassandra.java:3398)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
	... 11 more
Caused by: org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:117)
	at org.apache.cassandra.db.marshal.DateType.fromString(DateType.java:85)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.fromString(AbstractCompositeType.java:213)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:257)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1318)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1250)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:299)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:434)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:346)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:217)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
Caused by: java.text.ParseException: Unable to parse the date: 2012-10-19 21
	at org.apache.commons.lang.time.DateUtils.parseDate(DateUtils.java:285)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:113)
	... 14 more
ERROR 21:11:18,795 Exception in thread Thread[MigrationStage:1,5,main]
org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:117)
	at org.apache.cassandra.db.marshal.DateType.fromString(DateType.java:85)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.fromString(AbstractCompositeType.java:213)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:257)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1318)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1250)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:299)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:434)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:346)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:217)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.text.ParseException: Unable to parse the date: 2012-10-19 21
	at org.apache.commons.lang.time.DateUtils.parseDate(DateUtils.java:285)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:113)
	... 14 more
{code}

This error is repeated when attempting to restart the server, and results in the server failing to start.",All,,,,,,,,,,,,,,,,,,23/Oct/12 12:32;slebresne;4842.txt;https://issues.apache.org/jira/secure/attachment/12550448/4842.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-22 16:33:55.14,,,no_permission,,,,,,,,,,,,250070,,,Wed Oct 24 18:41:32 UTC 2012,,,,,,0|i0ao7b:,60194,jbellis,jbellis,,,,,,,,,,"22/Oct/12 16:33;jbellis;verified that this only affects 1.1, not 1.0, probably due to changes in schema handling.","22/Oct/12 18:22;xedin;This is the problem with CompositeType, because DateType would convert input to a human readable format with has *colons* inside and on AbstractCompositeType.fromString it tries to spit composite into parts based on colons as well.","23/Oct/12 12:32;slebresne;Oups, it's indeed time we fix that (it's in fact a problem for any string that have a ':', so not only dates). Attaching a patch that makes the string representation of composites safer. Namely, the patch escapes ':' character in the components to know where to split correctly on fromString. There is a slight complication if a '\' is at the end of one part, but the patch deals with that (and hopefully handle all cases).

Unfortunately, this won't fix the getString for existing data. However since the only time we were using fromString/getString of the comparator is in CFDefinition.to/fromSchema, and since things used to crash right away, this is probably not a big deal. People may have to trash/recreate their schema however if they ran into this problem.",24/Oct/12 18:30;jbellis;+1,"24/Oct/12 18:41;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remnants of removed nodes remain after removal,CASSANDRA-4840,12612800,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,jeromatron,jeromatron,19/Oct/12 22:28,12/Mar/19 14:05,13/Mar/19 22:27,02/Nov/12 13:14,1.1.7,,,,,0,,,,,,,"After nodes are removed from the ring and no longer appear in any of the nodes' nodetool ring output, some of the dead nodes show up in the o.a.c.net.FailureDetector SimpleStates metadata.  Also, some of the JMX stats are updating for the removed nodes (ie RecentTimeoutsPerHost and ResponsePendingTasks).",,,,,,,,,,,,,,,,,,,26/Oct/12 18:07;brandon.williams;4840.txt;https://issues.apache.org/jira/secure/attachment/12550992/4840.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-19 22:36:42.331,,,no_permission,,,,,,,,,,,,250060,,,Fri Nov 02 13:14:39 UTC 2012,,,,,,0|i0ao3b:,60176,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"19/Oct/12 22:36;brandon.williams;I believe this may be evidence of the issue I've heard reported where nodes are still trying to connect to dead IPs that were removed.  I suspect a message might be getting stuck in OTC and causing this.

As far as the FD goes, we definitely remove it there in Gossiper.removeEndpoint, so something must be adding it back.",22/Oct/12 15:58;jbellis;Does this happen in 1.1 as well?,"26/Oct/12 12:46;brandon.williams;Yes, it can happen in 1.1.  Even on restart some kind of message sits in MS for the host that I haven't tracked down yet.  Worth noting that at least the FD portion is a red herring, it's dumping the gossiper's endpoint state map which of course contains hosts with dead state.","26/Oct/12 18:07;brandon.williams;The gossiper has to notify subscribers of joins, even on dead state.  Specifically it needs to notify SS in case some action needs to be taken, however SS in turn needs to notify the gossiper to remove the endpoint if the endpoint is already a non-member.  When removing an endpoint, the gossiper should notify MS to destroy any conn pools and remove the timeout tracking.  Patch to do so.",26/Oct/12 22:15;vijay2win@yahoo.com;Wondering if there might be a race condition since we remove and disconnect should we take a membership lock on the node and do this? (Not sure if this is a real problem),"30/Oct/12 20:46;brandon.williams;I'm not sure what you mean, what is a membership lock on the node?","31/Oct/12 08:46;vijay2win@yahoo.com;Membership lock so no one else add's the node back into the map during the remove sequence, it would be rare... +1 other than that.","02/Nov/12 13:14;brandon.williams;The node would need a new generation for that to happen, and if so, it should be processed accordingly.  Since we're backed by NBHM, I don't see a reason to worry about this.  Committed as is.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong assumption for KeyRange about range.end_token in get_range_slices().,CASSANDRA-4804,12611757,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nmmm,nmmm,nmmm,14/Oct/12 15:45,12/Mar/19 14:05,13/Mar/19 22:27,17/Oct/12 04:01,1.1.7,1.2.0 beta 2,Legacy/CQL,,,0,,,,,,,"In get_range_slices() there is parameter KeyRange range.

There you can pass start_key - end_key, start_token - end_token, or start_key - end_token.

This is described in the documentation.

in thrift/ThriftValidation.java there is validation function validateKeyRange() (line:489) that validates correctly the KeyRange, including the case start_key - end_token.

However in thrift/CassandraServer.java in function get_range_slices() on line: 686 wrong assumption is made:

   if (range.start_key == null)

   {

      ... // populate tokens

   }

   else

   {

      bounds = new Bounds<RowPosition>(RowPosition.forKey(range.start_key, p), RowPosition.forKey(range.end_key, p));

   }

This means if there is start key, no end token is checked.
The opposite - null is ""inserted"" as end_key.

Solution:
same file - thrift/CassandraServer.java on next function - get_paged_slice(), on line:741 same code is written correctly

   if (range.start_key == null)

   {

      ... // populate tokens

   }

   else

   {

      RowPosition end = range.end_key == null ? p.getTokenFactory().fromString(range.end_token).maxKeyBound(p)

                           : RowPosition.forKey(range.end_key, p);

      bounds = new Bounds<RowPosition>(RowPosition.forKey(range.start_key, p), end);

   }

",,3600,3600,,0%,3600,3600,,,,,,,,,,,,15/Oct/12 19:53;nmmm;cassa.1.1.6.diff.txt;https://issues.apache.org/jira/secure/attachment/12549192/cassa.1.1.6.diff.txt,15/Oct/12 19:53;nmmm;cassa.1.2.x.diff.txt;https://issues.apache.org/jira/secure/attachment/12549193/cassa.1.2.x.diff.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-16 01:20:00.666,,,no_permission,,,,,,,,,,,,248569,,,Wed Oct 17 04:01:27 UTC 2012,,,,,,0|i09w3z:,55643,dbrosius,dbrosius,,,,,,,,,,"15/Oct/12 19:54;nmmm;patch tested in 1.1.5 and 1.1.6
did not check in 1.2, but is trivial.","16/Oct/12 01:20;jbellis;Can you review, Dave?","16/Oct/12 02:13;dbrosius;it seems to me you should check range.isSetEnd_key() and range.isSetEnd_token() to see what option you should use as i believe it's valid for the value to be null, meaning end of range.


bah... ignore this comment. new byte[0] is the way to specify end of range.
","16/Oct/12 02:30;dbrosius;1.2 patch doesn't apply cleanly. 

remove commented out code

otherwise patch works as expected.",17/Oct/12 04:01;dbrosius;committed to cassandra-1.1 as commit 4d637f1f1b62593e0c52e49966e3f286bf65c3e9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inet datatype does not work with cqlsh on windows,CASSANDRA-4801,12611640,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jbellis,jbellis,12/Oct/12 21:37,12/Mar/19 14:05,13/Mar/19 22:27,07/Mar/13 17:00,1.1.11,1.2.3,Legacy/Tools,,,0,cqlsh,windows,,,,,"{noformat}
create keyspace foo with replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};
use foo;
create table one (id int primary key, c int);
TRACING ON;
insert into one (id, c) values (1, 2);

value '\x7f\x00\x00\x01' (in col 'source') can't be deserialized as inet: 'module' object has no attribute 'inet_ntop'
{noformat}","Windows 7, Python 2.7.2",,,,,,,,,,,,,,,,,,07/Mar/13 03:26;iamaleksey;4801.txt;https://issues.apache.org/jira/secure/attachment/12572480/4801.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-12 21:43:30.621,,,no_permission,,,,,,,,,,,,248129,,,Thu Mar 07 17:00:37 UTC 2013,,,,,,0|i09lp3:,53948,brandon.williams,brandon.williams,,,,,,,,,,"12/Oct/12 21:43;brandon.williams;It looks like we can check socket.has_ipv6 to decide between inet_ntop and inet_ntoa, and if there are more than 4 bytes just display the escaped bytes.  In other words, ipv4 would work on windows but not ipv6.","07/Mar/13 02:44;iamaleksey;bq. It looks like we can check socket.has_ipv6 to decide between inet_ntop and inet_ntoa, and if there are more than 4 bytes just display the escaped bytes.

I'm thinking about just using inet_ntoa if there are 4 bytes, and inet_ntop if there are more - instead of using inet_ntop for both. It's slightly uglier, but will at least support ipv4 properly on Windows, which is still the most common case.. and let it continue raising an error on Windows when facing ipv6 addresses.

Don't want to just display the escaped bytes - this breaks cqlsh COPY FROM, for example. We could also reimplement inet_ntop and inet_pton in Python ourselves, since they won't be added until Python 3.4, but I'd rather not to - it's not *that* important. Working ipv4 and failing ipv6 on Windows is good enough for me.",07/Mar/13 02:49;brandon.williams;Agreed.,"07/Mar/13 03:26;iamaleksey;Attached the cassandra-dbapi2 patch (committed, but haven't pushed it yet).",07/Mar/13 16:38;brandon.williams;+1,07/Mar/13 17:00;iamaleksey;Thanks. Committed the fix to cassandra-dbapi2 and updated the bundled cql-internal-only-1.4.0.zip in 1.1/1.2/trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh help is obsolete for cql3,CASSANDRA-4800,12611627,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jbellis,jbellis,12/Oct/12 20:24,12/Mar/19 14:05,13/Mar/19 22:27,01/Nov/12 23:18,1.2.0 beta 2,,Legacy/Tools,,,0,cql3,,,,,,"For example, new syntax for CREATE KEYSPACE is

create keyspace foo with replication = {'class': 'SimpleStrategy', 'replication_factor': 1}
",,,,,,,,,,,,,,,,,,,30/Oct/12 20:53;iamaleksey;CASSANDRA-4800-v2.txt;https://issues.apache.org/jira/secure/attachment/12551418/CASSANDRA-4800-v2.txt,30/Oct/12 18:27;iamaleksey;CASSANDRA-4800.txt;https://issues.apache.org/jira/secure/attachment/12551399/CASSANDRA-4800.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-24 02:01:35.498,,,no_permission,,,,,,,,,,,,248112,,,Thu Nov 01 23:18:04 UTC 2012,,,,,,0|i09lj3:,53921,brandon.williams,brandon.williams,,,,,,,,,,"24/Oct/12 02:01;iamaleksey;I've noticed that a lot of help topics in cqlsh are outdated, not just ddl ones. outdated is the wrong word here.. they are for cql2 and don't have cql3 analogues. Anything that mentions USING CONSISTENCY for example.

Should they all be handled as part of this issue or should we deal with DDL in this one and create an issue for all others?","24/Oct/12 07:03;slebresne;I've updated the title to reflect my opinion :)

But basically, we need to fix all the cqlsh help for 1.2 release anyway, so I don't see a point in creating many different tickets.",30/Oct/12 18:53;jbellis;Why move everything out of docstrings?  Nice to have it inline IMO.,"30/Oct/12 18:57;iamaleksey;Only 8 commands or so had help in docstrings, everything else wasn't.
I can move (some of) them back - those that are purely cqlsh.
As for everything else, I'd rather have it all in one place.","30/Oct/12 19:03;jbellis;Ah, right.  I skimmed too quickly. :)",01/Nov/12 23:18;brandon.williams;Committed v2.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction progress counts more than 100%,CASSANDRA-4807,12611845,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,omid,omid,15/Oct/12 15:10,12/Mar/19 14:05,13/Mar/19 22:27,16/Oct/12 23:12,1.1.7,1.2.0 beta 2,,,,1,,,,,,,"'nodetool compactionstats' compaction progress counts more than 100%:

{code}
pending tasks: 74
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Validation        KSP        CF1           56192578305         84652768917    66.38%
               Compaction        KSP        CF2           162018591           119913592     135.11%
{code}

Hadn't experienced this before 1.1.3. Is it due to changes in 1.1.4-1.1.6 ?",,,,,,,,,,,,,,,,,,,16/Oct/12 19:52;yukim;4807-1.1.txt;https://issues.apache.org/jira/secure/attachment/12549369/4807-1.1.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-16 18:15:00.249,,,no_permission,,,,,,,,,,,,248720,,,Tue Oct 16 23:12:03 UTC 2012,,,,,,0|i09xzz:,55949,jbellis,jbellis,,,,,,,,,,16/Oct/12 18:15;yukim;[~omid] What compaction strategy are you using? Do you turn on multithreaded compaction?,"16/Oct/12 18:21;azotcsit;Yuki, I've seen that too. I've used the following configs:
* LCS with singlethreaded compaction (CompactionIterable).
* snappy compression with chunk_length_kb = 32. 
* Cassandra 1.1.4.","16/Oct/12 18:59;omid;[~yukim] LCS with snappy with chunk_length_kb = 64, sstable_size_in_mb = 10 and multithreaded compaction disabled on Cassandra 1.1.6.","16/Oct/12 19:52;yukim;Alexey, Omid,

Thank you both for providing information.
I think this regression comes from CASSANDRA-4587.
compactionstats' bytes compacted comes from LeveledScanner#getCurrentPosition(https://github.com/apache/cassandra/blob/cassandra-1.1.6/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java#L261) when using LCS.

We need to reset currentScanner to null after scanning through to the end, otherwise we would add twice for last scanned SSTable.

Patch attached as well as unit test for this.
(github: https://github.com/yukim/cassandra/tree/4807)","16/Oct/12 20:13;jbellis;LGTM, +1.

When committing, can you add a comment to the computeNext code explaining why we set to null?","16/Oct/12 20:21;omid;Thanks Yuki. It also probably affects compaction throttling, by throttling more than it should (https://github.com/apache/cassandra/blob/cassandra-1.1.6/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L612)","16/Oct/12 23:12;yukim;Committed with comment.

[~omid] Correct, though you pointed to cleanup source code which is not affected by this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError: keys must not be empty,CASSANDRA-4832,12612366,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mithrandi,mithrandi,mithrandi,18/Oct/12 05:15,12/Mar/19 14:05,13/Mar/19 22:27,22/Oct/12 16:50,1.1.7,,,,,0,indexing,,,,,,"I'm getting errors like this logged:

 INFO 07:08:32,104 Compacting [SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-114-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-113-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-110-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hd-108-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hd-106-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hd-107-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-112-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-109-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-111-Data.db')]
ERROR 07:08:32,108 Exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.AssertionError: Keys must not be empty
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:154)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:154)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

I'm not really sure when this started happening; they tend to be logged during a repair but I can't reproduce the error 100% reliably.",Debian 6.0.5,,,,,,,,,,,,,,,,,,20/Oct/12 22:35;cherro;FlushWriterKeyAssertionBlock.txt;https://issues.apache.org/jira/secure/attachment/12550155/FlushWriterKeyAssertionBlock.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-19 22:45:32.878,,,no_permission,,,,,,,,,,,,249477,,,Sun Nov 18 09:59:09 UTC 2012,,,,,,0|i0aeqn:,58661,jbellis,jbellis,,,,,,,,,,"18/Oct/12 05:31;mithrandi;Actually, it seems like this gets logged any time Cassandra tries to flush this column family now.","18/Oct/12 06:55;mithrandi;After some further investigation, it looks like this began after I upgraded from 1.1.2 to 1.1.6. The assertion seems to have been introduced as part of some debugging in CASSANDRA-4687. I'm not familiar with the code at all, but it seems to me that perhaps the assertion is bogus? If I have a secondary index on a particular column, and that column has an empty value in some row, would this not result in an empty key in the secondary index column family?","19/Oct/12 00:08;mithrandi;I modified my application to avoid inserting the empty column in question (which was a bug anyway; if that particular column was empty, the application should not have been doing an insert at all) and the problem has gone away, so it seems like my hypothesis is at least partially correct. I'm going to attempt to reproduce this in a more controlled environment now that my immediate issue in production has been resolved.",19/Oct/12 22:45;jbellis;you're right that the assert is bogus.  removed it in 72dcc298d335721c053444249c157e9a6431ebea.,"20/Oct/12 22:35;cherro;Came across this investigating an apparent deadlock in Schema Migrations.

If this assertion fails on the flushWriter executor, it blocks indefinitely. Anything upstream locking-wise gets stuck also. This was on 1.1.6.

Log output below, thread dump attached.

ERROR [FlushWriter:3] 2012-10-19 22:27:56,948 org.apache.cassandra.service.AbstractCassandraDaemon Exception in thread Thread[FlushWriter:3,5,main]
java.lang.AssertionError: Keys must not be empty
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:176)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:295)
        at org.apache.cassandra.db.Memtable.access$600(Memtable.java:48)
        at org.apache.cassandra.db.Memtable$5.runMayThrow(Memtable.java:316)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

","22/Oct/12 06:44;e1zorro;I've run into the same symptoms as Chris, but while attempting to migrate data between a 1.1.1 cluster and 1.1.6.

Whilst using sstableloader to import the data, nodes would hit certain column indexes and stop processing further requests once the assertion was thrown - restarting the node would almost immediately throw the assertion again, and the node would just fail to rejoin the ring.

tpstats would show active flushwriter tasks but no further node activity.

We had to work around the issue by:

1. Removing all indexes from our target cluster schema
2. Importing the data via sstableloader
3. Scanning through relevant column families and inserting data into any empty indexed columns
4. Re-applying the indexes to the target cluster schema

Only then was the migration successful.

I'll also note that attempting to apply an index to a column which has null data will also throw the cluster out of sync as the nodes which throw the assertion fail to migrate their schemas properly

 INFO [Creating index: Transactions.TransactionsCountryCode] 2012-10-21 07:45:25,978 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-Transactions.TransactionsCountryCode@1802367190(38862/169512 serialized/live bytes, 762 ops)
 INFO [Creating index: Transactions.TransactionsStatus] 2012-10-21 07:45:25,980 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-Transactions.TransactionsStatus@673679943(38862/125966 serialized/live bytes, 762 ops)
 INFO [FlushWriter:1] 2012-10-21 07:45:25,987 Memtable.java (line 264) Writing Memtable-Transactions.TransactionsCountryCode@1802367190(38862/169512 serialized/live bytes, 762 ops)
 INFO [FlushWriter:2] 2012-10-21 07:45:26,004 Memtable.java (line 264) Writing Memtable-Transactions.TransactionsStatus@673679943(38862/125966 serialized/live bytes, 762 ops)
ERROR [FlushWriter:1] 2012-10-21 07:45:26,004 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[FlushWriter:1,5,main]
java.lang.AssertionError: Keys must not be empty
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:176)
	at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:295)
	at org.apache.cassandra.db.Memtable.access$600(Memtable.java:48)
	at org.apache.cassandra.db.Memtable$5.runMayThrow(Memtable.java:316)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
 INFO [FlushWriter:2] 2012-10-21 07:45:26,049 Memtable.java (line 305) Completed flushing /var/lib/cassandra/data/xxxxxx/Transactions/xxxxxx-Transactions.TransactionsStatus-hf-2-Data.db (35259 bytes) for commitlog position ReplayPosition(segmentId=1350805525722, position=0)
 INFO [Creating index: Transactions.TransactionsLastUpdateDate] 2012-10-21 07:45:26,313 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-Transactions.TransactionsLastUpdateDate@1912098049(38862/240277 serialized/live bytes, 762 ops)
 INFO [FlushWriter:2] 2012-10-21 07:45:26,314 Memtable.java (line 264) Writing Memtable-Transactions.TransactionsLastUpdateDate@1912098049(38862/240277 serialized/live bytes, 762 ops)
 INFO [FlushWriter:2] 2012-10-21 07:45:26,743 Memtable.java (line 305) Completed flushing /var/lib/cassandra/data/xxxxxx/Transactions/xxxxxx-Transactions.TransactionsLastUpdateDate-hf-2-Data.db (37024 bytes) for commitlog position ReplayPosition(segmentId=1350805525722, position=0)
 INFO [main] 2012-10-21 07:45:27,052 CommitLogReplayer.java (line 272) Finished reading /var/lib/cassandra/commitlog/CommitLog-1350768744805.log
 INFO [main] 2012-10-21 07:45:27,054 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-Versions@1851630436(83/103 serialized/live bytes, 3 ops)
 INFO [FlushWriter:2] 2012-10-21 07:45:27,054 Memtable.java (line 264) Writing Memtable-Versions@1851630436(83/103 serialized/live bytes, 3 ops)
 INFO [FlushWriter:2] 2012-10-21 07:45:27,061 Memtable.java (line 305) Completed flushing /var/lib/cassandra/data/system/Versions/system-Versions-hf-1-Data.db (247 bytes) for commitlog position ReplayPosition(segmentId=1350805525722, position=0)","18/Nov/12 07:17;scottfines;Is there an actual patch for this, or a Workaround? We see this same AssertionError when attempting to start our nodes; restarting the node usually works correctly, however.",18/Nov/12 09:59;jbellis;The patch to fix this is git commit 72dcc298d335721c053444249c157e9a6431ebea.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commitlog files replayed but not in the order of their ids,CASSANDRA-4793,12611368,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,frousseau,frousseau,frousseau,11/Oct/12 15:10,12/Mar/19 14:05,13/Mar/19 22:27,22/Oct/12 21:23,1.2.0 beta 2,,,,,0,commitlog,,,,,,"I noticed that the commitlog files were not replayed in the order of their ids.
It seems that they are sorted by ""last modification date"" before being replayed, but this does not corresponds to their ids.

Moreover, ""last modification date"" is changed when a file is copied, so, this could also change the order of archived commitlogs.

Maybe it's safer to sort them using the id in the file name ?
",,,,,,,,,,,,,,,,,,,11/Oct/12 15:12;frousseau;4793-potential-patch-for-correctly-ordering-commit-log-fi.patch;https://issues.apache.org/jira/secure/attachment/12548751/4793-potential-patch-for-correctly-ordering-commit-log-fi.patch,12/Oct/12 09:36;frousseau;4793-trunk-potential-patch-for-correctly-ordering-commit-log-fi.patch;https://issues.apache.org/jira/secure/attachment/12548874/4793-trunk-potential-patch-for-correctly-ordering-commit-log-fi.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-11 20:30:07.096,,,no_permission,,,,,,,,,,,,247643,,,Mon Oct 22 21:23:54 UTC 2012,,,,,,0|i08mw7:,48308,jbellis,jbellis,,,,,,,,,,11/Oct/12 20:30;jbellis;Can you rebase to trunk?,"12/Oct/12 09:36;frousseau;Sure,

Attached the patch rebased to trunk","12/Oct/12 15:27;jbellis;committed, thanks!",22/Oct/12 21:23;jbellis;belatedly marking resolved,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Digest mismatch doesn't wait for writes as intended,CASSANDRA-4792,12611365,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/Oct/12 14:54,12/Mar/19 14:05,13/Mar/19 22:27,11/Oct/12 20:07,1.1.6,1.2.0 beta 2,,,,0,,,,,,,"As reported by Niklas Ekström on the dev list:

I’m looking in the file StorageProxy.java (Cassandra 1.1.5), and line 766 seems odd to me.

FBUtilities.waitOnFutures() is called with the repairResults from the RowRepairResolver resolver.

The problem though is that repairResults is only assigned when the object is created at line 737 in StorageProxy.java, and there it is assigned to Collections.emptyList(), and in the resolve() method in RowRepairResolver, which is indirectly called from line 771 in StorageProxy.java, that is, after the call to FBUtilities.waitOnFutures().

So the effect is that line 766 in StorageProxy.java is essentially a no-op.",,,,,,,,,,,,,,,,,,,11/Oct/12 14:55;jbellis;4792.txt;https://issues.apache.org/jira/secure/attachment/12548749/4792.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-11 17:40:57.963,,,no_permission,,,,,,,,,,,,247569,,,Thu Oct 18 20:10:38 UTC 2012,,,,,,0|i08fbj:,47081,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,11/Oct/12 14:55;jbellis;patch attached.,11/Oct/12 17:40;vijay2win@yahoo.com;+1,11/Oct/12 20:07;jbellis;committed,"17/Oct/12 23:59;rcoli;Is this bug's practical effect summarizable as :

""Because read repair didn't actually wait around for repair writes to ack, there was a non-zero chance of multiple concurrent read requests to a needing-repair key triggering multiple repair writes?""","18/Oct/12 20:10;jbellis;That, and ""it was possible for a quorum read to return an old value, even after a quorum read had returned a newer value [but the newer value had not yet been sent to the other replicas].""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactionsTest fails with timeout,CASSANDRA-4695,12608517,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,azotcsit,azotcsit,azotcsit,20/Sep/12 19:05,12/Mar/19 14:05,13/Mar/19 22:27,21/Sep/12 15:03,1.2.0 beta 2,,Legacy/Testing,,,0,test-fail,,,,,,"{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.CompactionsTest
    [junit] Testsuite: org.apache.cassandra.db.compaction.CompactionsTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.compaction.CompactionsTest:testStandardColumnCompactions:	Caused an ERROR
    [junit] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.CompactionsTest FAILED (timeout)
{code}","Ubuntu 12.04, Java 1.6, Ant 1.8.2, Maven 3.0.4
Ubuntu 11.10, Java 1.6, Ant 1.8.2, Maven 2.2.1
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-09-21 06:28:37.069,,,no_permission,,,,,,,,,,,,256340,,,Fri Sep 21 15:03:25 UTC 2012,,,,,,0|i0gyjb:,97025,jbellis,jbellis,,,,,,,,,,"21/Sep/12 06:28;slebresne;Downgrading the priority of this issue because I'm pretty sure this isn't really a problem. I also have CompactionsTest timeouting for ages on my desktop box with the default timeout (but is passes on some other box). Bumping the junit timeout fixes the issue however, so I think it's really just that CompactionsTest is a bit too long for the default timeout on system with a slow hard drive. ",21/Sep/12 08:07;azotcsit;May be CompactionTest.testStandardColumnCompactions() method should be moved to LongCompactionSpeedTest or somewhere in the long-test testsuite. What do you think about it?,21/Sep/12 15:03;jbellis;good idea.  done in trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debian packaging doesn't do auto-reloading of log4j properties file,CASSANDRA-4855,12613352,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,rbranson,rbranson,24/Oct/12 19:16,12/Mar/19 14:05,13/Mar/19 22:27,25/Oct/12 16:11,1.1.7,,Packaging,,,0,,,,,,,Cassandra isn't starting the log4j auto-reload thread because it requires -Dlog4j.defaultInitOverride=true on initialization. Is there a reason to not do this when installed from the Debian package?,,,,,,,,,,,,,,,,,,,24/Oct/12 21:06;brandon.williams;4855.txt;https://issues.apache.org/jira/secure/attachment/12550684/4855.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-24 21:06:07.776,,,no_permission,,,,,,,,,,,,250848,,,Thu Oct 25 16:11:48 UTC 2012,,,,,,0|i0b1z3:,62432,rbranson,rbranson,,,,,,,,,,24/Oct/12 21:06;brandon.williams;I don't see why not.  I guess when we added the reloading to bin/cassandra we neglected to also add it to the init.,"25/Oct/12 16:07;rbranson;Looks good to me, +1",25/Oct/12 16:11;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh --cql3 unable to describe CF created with cli,CASSANDRA-4827,12612325,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jbellis,jbellis,17/Oct/12 21:55,12/Mar/19 14:05,13/Mar/19 22:27,10/Dec/12 15:07,1.2.0 rc1,,Legacy/Tools,,,0,cql3,,,,,,"created CF with cli:

{noformat}
create column family playlists
with key_validation_class = UUIDType
 and comparator = 'CompositeType(UTF8Type, UTF8Type, UTF8Type)'
 and default_validation_class = UUIDType;
{noformat}

Then get this error with cqlsh:

{noformat}
cqlsh:music> describe table playlists;

/Users/jonathan/projects/cassandra/git-trunk/bin/../pylib/cqlshlib/cql3handling.py:771: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected composite key CF to have column aliases, but found none
/Users/jonathan/projects/cassandra/git-trunk/bin/../pylib/cqlshlib/cql3handling.py:794: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected [u'KEY'] length to be 3, but it's 1. comparator='org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)'
CREATE TABLE playlists (
  ""KEY"" uuid PRIMARY KEY
)
{noformat}",,,,,,,,,,,,,,,,,,,09/Dec/12 15:40;iamaleksey;4827-1.2.txt;https://issues.apache.org/jira/secure/attachment/12560091/4827-1.2.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-19 20:26:32.145,,,no_permission,,,,,,,,,,,,249421,,,Mon Dec 10 15:07:24 UTC 2012,,,,,,0|i0a85j:,57594,jbellis,jbellis,,,,,,,,,,"17/Oct/12 21:59;jbellis;Have only tested in 1.1 since CASSANDRA-4823 is not yet done for trunk.

Expected result would be

{code}
CREATE TABLE playlists (
  key uuid,
  column1 text,
  column2 text,
  column3 text,
  column4 uuid
  PRIMARY KEY (key, column1, column2, column3)
) WITH COMPACT STORAGE
{code}

(See column name ""forging"" in CFDefinition.)","19/Oct/12 20:26;iamaleksey;cql3 on trunk:

cqlsh:music> describe table playlists;

/Users/aleksey/Repos/ASF/cassandra/bin/../pylib/cqlshlib/cql3handling.py:1505: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. Dynamic storage CF does not have UTF8Type added to comparator","31/Oct/12 21:02;mkjellman;similar issue when trying to use COPY in cqlsh3. cf was created with cli

/opt/cassandra/apache-cassandra-1.1.6/bin/../pylib/cqlshlib/cql3handling.py:771: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected composite key CF to have column aliases, but found none
/opt/cassandra/apache-cassandra-1.1.6/bin/../pylib/cqlshlib/cql3handling.py:794: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected [u'KEY'] length to be 3, but it's 1. comparator='org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.IntegerType)'","09/Dec/12 15:41;iamaleksey;4827-1.2 eliminates most if not all DESCRIBE issues in 1.2, no matter what kind of table we are dealing with.","09/Dec/12 17:25;iamaleksey;Oh, and this only fixes DESCRIBE in CQL3 mode. Didn't bother with CQL2.",10/Dec/12 14:55;jbellis;+1,"10/Dec/12 15:07;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem using BulkOutputFormat while streaming several SSTables simultaneously from a given node.,CASSANDRA-4813,12611994,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,ralph.romanos,ralph.romanos,16/Oct/12 08:58,12/Mar/19 14:05,13/Mar/19 22:27,13/Nov/12 22:39,1.2.0 beta 3,,,,,0,Bulkoutputformat,Hadoop,SSTables,,,,"The issue occurs when streaming simultaneously SSTables from the same node to a cassandra cluster using SSTableloader. It seems to me that Cassandra cannot handle receiving simultaneously SSTables from the same node. However, when it receives simultaneously SSTables from two different nodes, everything works fine. As a consequence, when using BulkOutputFormat to generate SSTables and stream them to a cassandra cluster, I cannot use more than one reducer per node otherwise I get a java.io.EOFException in the tasktracker's logs and a java.io.IOException: Broken pipe in the Cassandra logs.","I am using SLES 10 SP3, Java 6, 4 Cassandra + Hadoop nodes, 3 Hadoop only nodes (datanodes/tasktrackers), 1 namenode/jobtracker. The machines used are Six-Core AMD Opteron(tm) Processor 8431, 24 cores and 33 GB of RAM. I get the issue on both cassandra 1.1.3, 1.1.5 and I am using Hadoop 0.20.2.",,,,,,,,,,,CASSANDRA-4922,,,,,,,08/Nov/12 21:55;yukim;4813.txt;https://issues.apache.org/jira/secure/attachment/12552717/4813.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-16 22:09:04.003,,,no_permission,,,,,,,,,,,,248935,,,Wed Nov 14 12:52:00 UTC 2012,,,,,,0|i0a2l3:,56692,mkjellman,mkjellman,,,,,,,,,,16/Oct/12 22:09;brandon.williams;Can you post a stacktrace?,"17/Oct/12 06:58;ralph.romanos;I get the following error in the tasktracker's logs when SSTables 
are streamed into the Cassandra cluster:

Exception in thread ""Streaming to /172.16.110.79:1"" java.lang.RuntimeException: java.io.EOFException
	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:628)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:194)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:94)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
Exception in thread ""Streaming to /172.16.110.92:1"" java.lang.RuntimeException: java.io.EOFException
	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:628)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:194)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:94)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more","17/Oct/12 19:47;mkjellman;Same issue with 1.1.6 and Hadoop 1.0.3

I have the following from the Cassandra logs as well

ERROR 12:46:06,256 Exception in thread Thread[Thread-1224,5,main]
java.lang.AssertionError: We shouldn't fail acquiring a reference on a sstable that has just been transferred
	at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:188)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:103)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:182)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)","17/Oct/12 20:42;mkjellman;Just confirmed that limiting the reducer to 1 does not change the behavior in my environment. Also noticed that BulkRecordWriter will always throw an IOException (as mentioned in the original bug) if mapreduce.output.bulkoutputformat.maxfailedhosts is ever > 0 (assuming defaults).

In my case future.getFailedHosts() always returns every node in my cluster when the condition occurs. I'm doing about 50 million insertions into 50 million rows and the EOFExceptions seem to crop up after a good number of the sstables have already been successfully sent.","17/Oct/12 20:50;ralph.romanos;Do you stream the SSTables in your reducer? And when you tried to limit the reducer to 1, did you restart the cassandra cluster?","17/Oct/12 20:57;mkjellman;Yes, in my reducer. Didn't restart the Cassandra cluster -- what would that accomplish? Limited the reducer in my job configuration.",17/Oct/12 21:06;ralph.romanos;I had the same error (IOException) when I did not restart Cassandra. It is caused by the EOFException and because the streaming failed in your previous job. Restarting Cassandra should allow you to make it work with 1 reducer.,18/Oct/12 00:45;mkjellman;you're quite right. no issues after a rolling restart and only one reducer.,19/Oct/12 20:21;mkjellman;just reproduced this again with one reducer.,"22/Oct/12 19:08;yukim;This is limitation of BulkOutputFormat right now. Currently, streaming session uses (IP, counter) for its ID. Since counter is per JVM, running two or more reducers on same node streaming to one cassandra node likely cause session conflict, and I think that is causing the issue here.
To resolve this, we need to change the way to distinguish each session(possibly by changing to use UUID for session ID).

[~mkjellman] Do you run your reducer on top of cassandra node? If that is the case, session conflict I described above may be the cause. If not, there is another issue in your one reducer case I think.",22/Oct/12 19:15;mkjellman;[~nakagawayk] Yes - the single reducer was running on top of a Cassandra node. Why would having the reducer+the cassandra node cause a session ID collision if there is only one reducer?,"23/Oct/12 07:35;ralph.romanos;I never get this exception when I run my reducer on a single node (either on top of a cassandra node or on a Hadoop only node). In addition, when I run multiple reducers, I create a java process for each reducer; therefore my reducers should be running on different JVMs right? If that is the case we shouldn't get counter conflicts when running multiple reducers.","23/Oct/12 15:07;yukim;Session counter is per JVM, so when you spawn two or more stream session on different JVM on same host, you will have chance to get same session ID, say (10.x.x.x, 1) on both JVM.
This is also true with Cassandra on one JVM and reducer on the other JVM on the same machine(but session collision in this set up is less likely than two reducers spawned simultaneously on the same node).",23/Oct/12 20:29;jbellis;If we need to change streaming protocol to fix this then we should target 1.2.,"30/Oct/12 20:14;yukim;Attaching patch to change streaming session ID from (host, counter) pair to Time UUID. This should hugely drop probability of session ID collision. I haven't tested with BOF, but I spawned 3 sstableloader simultaneously on the same node with C* and could finish streaming without getting errors.",31/Oct/12 15:38;mkjellman;Yuki- Thanks for your work on the patch. I'll test BOF with 1.1.6 today.,"31/Oct/12 15:41;yukim;Michael,

Patch is for trunk(version 1.2). The change breaks messaging compatibility in 1.1, so this won't be ported for next 1.1.7 release.
",31/Oct/12 15:45;mkjellman;Good to know - I'll get a 1.2 cluster setup on trunk then and test.,"31/Oct/12 16:12;ralph.romanos;Me too, I will let u know as soon as I test it. Thank you for this quick patch Yuki.","31/Oct/12 21:16;yukim;Attaching newer version. Since with this patch, we only distinguish streaming session by UUID, we don't need to carry around broadcast address(CASSANDRA-3503), so I removed it from StreamHeader.","01/Nov/12 15:42;mkjellman;[~yukim] not sure if this will be an issue for ""normal"" upgraders or if this was because i went between patch versions but just wanted you to be aware of the EOF i saw on startup. Node only has the system schema at this point.

WARN 08:40:29,821 error reading saved cache /ssd/saved_caches/system-local-KeyCache-b.db
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:278)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:393)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:365)
	at org.apache.cassandra.db.Table.initCf(Table.java:334)
	at org.apache.cassandra.db.Table.<init>(Table.java:272)
	at org.apache.cassandra.db.Table.open(Table.java:102)
	at org.apache.cassandra.db.Table.open(Table.java:80)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:320)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:203)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:395)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:438)

Edit: actually thinking this is a totally unrelated bug?","01/Nov/12 22:59;yukim;Michael,

I think it is not related to this issue.
Do you upgrade from which version? '-b' in system-local-KeyCache-b.db indicates it's from version 1.2.
Also, it possible that the saved cache file is really corrupted for some reason(shutting down in the middle of cache saving?), so C* was just WARNing you about it.","01/Nov/12 23:15;mkjellman;Brand new cluster running 1.2 with your original patch, nodetool drain, put new jar with new patch in place, started Cassandra and got error. had to delete the cache file as subsequent initializations threw the EOF as well. Happened on all three nodes in the 3 node test cluster.","05/Nov/12 16:48;yukim;Michael,

I couldn't reproduce your error, and I believe that is not related to this issue.
So if you see that error constantly, please open another issue.",05/Nov/12 16:53;mkjellman;Sounds good Yuki. I've been trying to get a secondary cluster setup since the 31st..keep getting pulled away. I promise i'll get multiple reducers tested today :),"05/Nov/12 23:19;mkjellman;Exception in thread ""Streaming to /10.25.9.5:1"" java.lang.RuntimeException: java.net.SocketException: Already bound
        at com.google.common.base.Throwables.propagate(Throwables.java:156)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.SocketException: Already bound
        at sun.nio.ch.Net.translateToSocketException(Net.java:109)
        at sun.nio.ch.Net.translateException(Net.java:141)
        at sun.nio.ch.Net.translateException(Net.java:147)
        at sun.nio.ch.SocketAdaptor.bind(SocketAdaptor.java:147)
        at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:128)
        at org.apache.cassandra.streaming.FileStreamTask.connectAttempt(FileStreamTask.java:236)
        at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:88)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        ... 3 more
Caused by: java.nio.channels.AlreadyBoundException
        at sun.nio.ch.SocketChannelImpl.bind(SocketChannelImpl.java:556)
        at sun.nio.ch.SocketAdaptor.bind(SocketAdaptor.java:145)
        ... 7 more

is it intended that we fail that reducer? this looks like just a more elegant collision, no? seeing the same failure on every node.

","06/Nov/12 18:26;mkjellman;Also, while we don't throw the EOF anymore further down the stack as far as I can tell from my testing, the net result is an IOException being thrown in the Reducer.","08/Nov/12 21:55;yukim;ok, it looks like we have CASSANDRA-3839 for BOF.
Updated patch to avoid socket re-binding.

[~mkjellman] How about this one?","08/Nov/12 23:13;mkjellman;new problem now with current patch.

2012-11-08 15:29:05,323 ERROR org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor: Error in ThreadPoolExecutor
java.lang.RuntimeException: java.io.IOException: Broken pipe
        at com.google.common.base.Throwables.propagate(Throwables.java:156)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.IOException: Broken pipe
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:89)
        at sun.nio.ch.IOUtil.write(IOUtil.java:60)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:450)
        at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)
        at java.nio.channels.Channels.writeFully(Channels.java:98)
        at java.nio.channels.Channels.access$000(Channels.java:61)
        at java.nio.channels.Channels$1.write(Channels.java:174)
        at com.ning.compress.lzf.LZFChunk.writeCompressedHeader(LZFChunk.java:77)
        at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:132)
        at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
        at com.ning.compress.lzf.LZFOutputStream.write(LZFOutputStream.java:97)
        at org.apache.cassandra.streaming.FileStreamTask.write(FileStreamTask.java:218)
        at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:164)
        at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        ... 3 more","09/Nov/12 21:16;yukim;[~mkjellman] Just checking, did you use patched version for both Cassandra and hadoop job?","09/Nov/12 21:19;mkjellman;yes, actually the first time I tested I did forget to ensure both the Hadoop jar and Cassandra nodes had the newest patched version. I've tested a few times now just to make sure i'm not missing anything. Seems to die as soon as it tries to stream the first sstable to the nodes. Never progresses past 0% on the streaming and then throws the exception.

applied patch to trunk when it was at commit f09a89f4cd13af2087fcc92f09f6cf1ee4785feb. i rebuilt the entire cluster, and ensured my maven dependencies were all set. Still reproduced the problem unfortunately (i actually thought it had been resolved but i just reproduced the java.io.IOException: Broken pipe again).

MD5 (build/apache-cassandra-1.2.0-beta2-SNAPSHOT.jar) = 92d8ffacb3963116dd153a2c8c83fbe9","12/Nov/12 18:04;yukim;[~mkjellman] Hmm, I tried standalone and psuedo-cluster hadoop on my machine and haven't seen that error. I will try in fully distributed mode.
By the way, what kind of error did you see on cassandra side? Can you post stacktrace?",12/Nov/12 19:06;mkjellman;[~yukim] I can produce it in local mode (standalone) and distributed mode with the current revision of the patch. I haven't ran it in psuedo-cluster mode. Also should mention I have reproduced it even when I limit to 1 reducer.,"13/Nov/12 18:45;yukim;[~mkjellman] OK, I did test on distributed hadoop cluster with multiple reducers and job succeeded.
At first I was getting the same error as yours (java.io.IOException: Broken pipe), but I figured out it was due to the mismatch in partitioner(My cluster was configured as RandomPartitioner and I used Murmur3Partitioner for BOF).
Can you check that also?
If that's not your case, can you upload Cassandra system.log?",13/Nov/12 20:16;mkjellman;[~yukim] Looks like you are right. Was Murmur Partitioner and my job was configured for Random. On a first run looks like this patch is good to go. Sorry for the wild goose chase there.,13/Nov/12 21:54;mkjellman;Just tested a few more times. Looks good. Ship it!,"13/Nov/12 22:39;yukim;Thanks for testing! Committed for 1.2 release.

For partitioner mismatch part, I opened CASSANDRA-4957 to follow up.","14/Nov/12 12:52;ralph.romanos;Hello, 
I just tested the patch on 1.2 with 2 reducers per node and 7 nodes. Everything works fine now.

Thank you Yuki!",,,,,,,,,,,,,,,,,,,,,
cqlsh renders bytes fields as strings,CASSANDRA-4970,12616535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jbellis,jbellis,17/Nov/12 17:17,12/Mar/19 14:05,13/Mar/19 22:27,17/Nov/12 18:13,1.2.0 beta 3,,,,,0,,,,,,,Jake reports (CASSANDRA-4968) that this is a regression against 1.1.,,,,,,,,,,,,,,,,,,,17/Nov/12 18:00;iamaleksey;4970.txt;https://issues.apache.org/jira/secure/attachment/12553907/4970.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-17 18:01:23.633,,,no_permission,,,,,,,,,,,,258407,,,Sat Nov 17 18:01:23 UTC 2012,,,,,,0|i0ksjb:,119440,brandon.williams,brandon.williams,,,,,,,,,,17/Nov/12 18:01;brandon.williams;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add debug logging to list filenames processed by o.a.c.db.Directories.migrateFile method,CASSANDRA-4939,12615525,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,jblangston@datastax.com,jblangston@datastax.com,09/Nov/12 17:52,12/Mar/19 14:05,13/Mar/19 22:27,13/Nov/12 01:05,1.2.1,,,,,0,,,,,,,"Customer is getting the following error when starting Cassandra:

ERROR 20:20:06,635 Exception encountered during startup
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(Unknown Source)
        at org.apache.cassandra.db.Directories.migrateFile(Directories.java:556)
        at org.apache.cassandra.db.Directories.migrateSSTables(Directories.java:493)

It looks like this is caused by an file with an unexpected name in one of his keyspace directories. However, because we don't log the name of the file as it is processed, it is hard to tell which file is causing it to choke.

It would be good to add a logger.debug statement at the beginning of the method to list the file currently being processed.",,,,,,,,,,,,,,,,,,,12/Nov/12 05:49;dbrosius;4939.txt;https://issues.apache.org/jira/secure/attachment/12553075/4939.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-12 04:57:49.323,,,no_permission,,,,,,,,,,,,256813,,,Tue Nov 13 01:05:24 UTC 2012,,,,,,0|i0isr3:,107752,slebresne,slebresne,,,,,,,,,,"12/Nov/12 04:57;dbrosius;fyi, filename doesn't end with .json and has no - in it","12/Nov/12 05:48;dbrosius;ignore files that don't fit the pattern and log with warning
throw exception with file information for files that should have been migrated.",12/Nov/12 11:17;slebresne;+1,13/Nov/12 01:05;dbrosius;committed as 6677d075e9cadc073633fd84f810b9fc5174db45 to cassandra-1.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure Detector should log or ignore sudden time change to the past,CASSANDRA-4925,12615031,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kohlisankalp,kohlisankalp,kohlisankalp,06/Nov/12 22:17,12/Mar/19 14:05,13/Mar/19 22:27,18/Oct/13 22:30,2.0.2,,,,,0,,,,,,,"If a machine goes back in time all of a sudden because of a problem, Gossip will insert a negative interArrivalTime. 
This will decrease the mean value and can cause this machine to mark other nodes as down and then mark them up as time passed. 
But we should log such occurrences.",,3600,3600,,0%,3600,3600,,,,,,,,,,,,17/Oct/13 21:43;kohlisankalp;trunk-4925-v1.patch;https://issues.apache.org/jira/secure/attachment/12609029/trunk-4925-v1.patch,17/Oct/13 23:26;kohlisankalp;trunk-4925-v2.patch;https://issues.apache.org/jira/secure/attachment/12609057/trunk-4925-v2.patch,08/Oct/13 17:21;kohlisankalp;trunk-4925.patch;https://issues.apache.org/jira/secure/attachment/12607391/trunk-4925.patch,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-03-24 05:46:30.12,,,no_permission,,,,,,,,,,,,255583,,,Fri Oct 18 22:30:50 UTC 2013,,,,,,0|i0fslz:,90233,jbellis,jbellis,,,,,,,,,,"24/Mar/13 05:46;jbellis;Can we replace this with nanoTime measurements?  (nanoTime is guaranteed to be monotonically increasing, given appropriate OS support.)",21/Sep/13 20:11;kohlisankalp;Yes changing to nanotime will be a good improvement. ,"21/Sep/13 22:08;kohlisankalp;@Jonathan
Also why is BoundedStatsDeque has all values in double. Why it can't be Long? ","08/Oct/13 02:59;kohlisankalp;Changed Double to Long in BoundedStatsDeque. Changed the unit test for it to take long instead of double.
Converted FD to use nano time. Also modified the test to use nano time. In this test I am comparing the values between passing millis and nano and checking whether the phi is the same for both. ","08/Oct/13 03:07;jbellis;Thanks!

What about this? Looks like we're mixing millis + nanos.

{code}
        if (tLast != 0L)
        {
            interArrivalTime = (value - tLast);
        }
        else
        {
            interArrivalTime = Gossiper.intervalInMillis / 2;
        }
{code}
","08/Oct/13 03:09;brandon.williams;Am I missing where we actually log that a negative value was received?  I'd be fine with a WARN since this should only happen rarely unless your clock is seriously crazy, but either way it's something you definitely want to know.","08/Oct/13 17:21;kohlisankalp;Fixed the Gossip interval and updated the patch
[~brandon.williams]  If I am not wrong, we are now using nano which is a counter and difference of a counter(higher - lower) will always be positive even with overflow.",09/Oct/13 13:28;jbellis;Is this the right patch?  I still see {{Gossiper.intervalInMillis}} being used.,09/Oct/13 17:18;kohlisankalp;Yes now I am doing (Gossiper.intervalInMillis * MILLI_TO_NANO) ,"09/Oct/13 17:44;jbellis;Ah, my jira client was confused by re-using the same patch name.  I see it now.

Why double up the ArrivalWindowTest?",10/Oct/13 04:27;kohlisankalp;I wanted to check whether passing  millis and nano gives out the same results. I can remove the millis part if you want. ,17/Oct/13 00:36;kohlisankalp;[~jbellis] Should I make the test single? ,17/Oct/13 08:59;jbellis;I think that would probably make more sense.,17/Oct/13 21:43;kohlisankalp;Changed the test,"17/Oct/13 22:11;jbellis;Hmm, I get

patch: **** malformed patch at line 164: diff --git a/test/unit/org/apache/cassandra/utils/BoundedStatsDequeTest.java b/test/unit/org/apache/cassandra/utils/BoundedStatsDequeTest.java
",17/Oct/13 23:26;kohlisankalp;Another patch,18/Oct/13 22:30;jbellis;committed; thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
occasional TableTest failure,CASSANDRA-4935,12615348,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,brandon.williams,brandon.williams,08/Nov/12 16:33,12/Mar/19 14:05,13/Mar/19 22:27,09/Nov/12 22:30,1.2.0 beta 3,,,,,0,,,,,,,"The TableTest unit test fails somewhat rarely:

{noformat}
    [junit] Testsuite: org.apache.cassandra.db.TableTest
    [junit] Tests run: 11, Failures: 1, Errors: 0, Time elapsed: 4.339 sec
    [junit] 
    [junit] Testcase: testGetSliceWithExpiration(org.apache.cassandra.db.TableTest):	FAILED
    [junit] Columns [636f6c31:false:4@1,636f6c32:true:4@1!1,636f6c33:false:4@1,])] is not expected [col1,col2]
    [junit] junit.framework.AssertionFailedError: Columns [636f6c31:false:4@1,636f6c32:true:4@1!1,636f6c33:false:4@1,])] is not expected [col1,col2]
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:574)
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:541)
    [junit] 	at org.apache.cassandra.db.TableTest$5.runMayThrow(TableTest.java:353)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    [junit] 	at org.apache.cassandra.db.TableTest.reTest(TableTest.java:56)
    [junit] 	at org.apache.cassandra.db.TableTest.testGetSliceWithExpiration(TableTest.java:362)
{noformat}",,,,,,,,,,,,,,,,,,,09/Nov/12 21:56;yukim;4935.txt;https://issues.apache.org/jira/secure/attachment/12552897/4935.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-08 19:49:07.855,,,no_permission,,,,,,,,,,,,255982,,,Fri Nov 09 22:30:40 UTC 2012,,,,,,0|i0gec7:,93753,jbellis,jbellis,,,,,,,,,,"08/Nov/12 19:49;yukim;Looks like the test TableTest#testGetSliceWithExpiration is from CASSANDRA-4761.
Since it is using TTL of 1 sec, test result will be different if it takes more than 1 sec before query.

Honestly, I don't understand the purpose of the test. It says ""// tests slicing against data from one row with expiring column in a memtable and then flushed to an sstable"", but it is forcing flush before slicing.",08/Nov/12 20:26;jbellis;It does look like the forceBlockingFlush call should be removed to make the test do what was intended.,"09/Nov/12 21:56;yukim;Alright, I removed forceBlockingFlush and set TTL long enough to not get tombstoned so we get consistent test result.",09/Nov/12 22:20;jbellis;+1,09/Nov/12 22:30;yukim;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactionSerializerTest fails to find jemalloc,CASSANDRA-4995,12617734,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,brandon.williams,brandon.williams,27/Nov/12 04:10,12/Mar/19 14:05,13/Mar/19 22:27,27/Nov/12 08:45,2.0 beta 1,,Legacy/CQL,,,0,,,,,,,"{noformat}
    [junit] Testcase: org.apache.cassandra.io.CompactSerializerTest:	Caused an ERROR
    [junit] Unable to load library 'jemalloc': libjemalloc.so: cannot open shared object file: No such file or directory
    [junit] java.lang.UnsatisfiedLinkError: Unable to load library 'jemalloc': libjemalloc.so: cannot open shared object file: No such file or directory
    [junit] 	at com.sun.jna.NativeLibrary.loadLibrary(NativeLibrary.java:163)
    [junit] 	at com.sun.jna.NativeLibrary.getInstance(NativeLibrary.java:236)
    [junit] 	at com.sun.jna.Library$Handler.<init>(Library.java:140)
    [junit] 	at com.sun.jna.Native.loadLibrary(Native.java:379)
    [junit] 	at com.sun.jna.Native.loadLibrary(Native.java:364)
    [junit] 	at org.apache.cassandra.io.util.JEMallocAllocator.<clinit>(JEMallocAllocator.java:32)
    [junit] 	at java.lang.Class.forName0(Native Method)
    [junit] 	at java.lang.Class.forName(Class.java:169)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:109)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest.scanClasspath(CompactSerializerTest.java:142)
{noformat}

If jemalloc is now the preferred allocator, we should add it to the debian packaging.  However, I did install the lib and it still didn't work. ",,,,,,,,,,,,,,,,,,,27/Nov/12 05:37;vijay2win@yahoo.com;0001-CASSANDRA-4995.patch;https://issues.apache.org/jira/secure/attachment/12554970/0001-CASSANDRA-4995.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-27 05:24:44.35,,,no_permission,,,,,,,,,,,,292249,,,Tue Nov 27 08:45:02 UTC 2012,,,,,,0|i0rstj:,160313,jbellis,jbellis,,,,,,,,,,"27/Nov/12 05:24;vijay2win@yahoo.com;JEMalloc is not default, looks like this is because the Directory scanner scans for all the files in the class path and initializes it which will cause the static variable to get intialized.","27/Nov/12 07:34;jbellis;Constructor initializing a static field makes my OCD go crazy. :)

Should we just make the field non-static?  Or use a inner class static initializer + static instance() method like in MessagingService.","27/Nov/12 07:43;vijay2win@yahoo.com;Hi Jonathan,

{quote}
Constructor initializing a static field 
{quote}
attached patch doesn't initialize the static field.... it was moved from static to non-static... 
{code}
-    private static final JEMLibrary instance = (JEMLibrary) Native.loadLibrary(""jemalloc"", JEMLibrary.class);
+    private final JEMLibrary instance;
{code}
not sure if we are talking about the same thing :) (May be the name caused the confusion?)",27/Nov/12 08:15;jbellis;I must be more jet lagged than I thought.  LGTM.,"27/Nov/12 08:45;vijay2win@yahoo.com;Committed, Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not allow use of collection with compact storage,CASSANDRA-4990,12617621,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,yukim,yukim,26/Nov/12 14:57,12/Mar/19 14:05,13/Mar/19 22:27,26/Nov/12 18:21,1.2.0 beta 3,,,,,0,,,,,,,"You can define ColumnFamily with collection type and compact storage as follows:

{code}
CREATE TABLE test (
  user ascii PRIMARY KEY,
  mails list<text>
) WITH COMPACT STORAGE;
{code}

This does not make sense and end up error when inserting data to collection.

{code}
INSERT INTO test (user, mails) VALUES ('foo', ['foo@foo.org']);
{code}

I think it is better not to allow defining such ColumnFamily.",,,,,,,,,,,,,,,,,,,26/Nov/12 16:12;slebresne;4990.txt;https://issues.apache.org/jira/secure/attachment/12554872/4990.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-26 16:13:09.203,,,no_permission,,,,,,,,,,,,292124,,,Mon Nov 26 18:21:15 UTC 2012,,,,,,0|i0rrxz:,160171,yukim,yukim,,,,,,,,,,26/Nov/12 16:13;slebresne;Trivial patch attached.,26/Nov/12 16:44;yukim;+1,"26/Nov/12 18:21;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORDER BY validation is not restrictive enough,CASSANDRA-4624,12606412,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,06/Sep/12 14:51,12/Mar/19 14:05,13/Mar/19 22:27,06/Sep/12 15:57,1.1.5,1.2.0 beta 1,,,,0,cql3,,,,,,"We're not able to do order by on anything that is a key range. However, we only refuse queries that have an empty where clause, but that doesn't exclude all key ranges at all.",,,,,,,,,,,,,,,,,,,06/Sep/12 14:53;slebresne;4624.txt;https://issues.apache.org/jira/secure/attachment/12544052/4624.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-06 15:12:51.716,,,no_permission,,,,,,,,,,,,256294,,,Thu Sep 06 15:57:20 UTC 2012,,,,,,0|i0gxsv:,96906,jbellis,jbellis,,,,,,,,,,06/Sep/12 15:12;jbellis;+1,"06/Sep/12 15:57;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
not possible to change crc_check_chance,CASSANDRA-5053,12623365,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,krummas,krummas,11/Dec/12 12:02,12/Mar/19 14:05,13/Mar/19 22:27,19/Dec/12 13:11,1.1.9,1.2.0 rc2,,,,0,,,,,,,"It is not possible to change crc_check_chance using a schema modification after CASSANDRA-4266

This patch fixes that and moves the setting out into a configuration parameter instead, you dont want to upgrade/scrub/.. all your sstables to change the crc_check_chance.

",,,,,,,,,,,,,,,,,,,18/Dec/12 12:55;krummas;0001-CASSANDRA-5053-make-it-possible-to-change-crc_check_.patch;https://issues.apache.org/jira/secure/attachment/12561480/0001-CASSANDRA-5053-make-it-possible-to-change-crc_check_.patch,11/Dec/12 12:06;krummas;0001-fix-CASSANDRA-5053-not-possible-to-change-crc_check_.patch;https://issues.apache.org/jira/secure/attachment/12560380/0001-fix-CASSANDRA-5053-not-possible-to-change-crc_check_.patch,19/Dec/12 05:08;iamaleksey;5053-v2.txt;https://issues.apache.org/jira/secure/attachment/12561642/5053-v2.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-12-11 16:29:59.817,,,no_permission,,,,,,,,,,,,297062,,,Wed Dec 19 13:10:50 UTC 2012,,,,,,0|i14l47:,234884,iamaleksey,iamaleksey,,,,,,,,,,"11/Dec/12 12:05;krummas;* Move crc_check_chance from schema definition to config file
* Support if someone has actually managed to set the crc_check_chance in a CompressionMetadata file - pre 1.1.1 maybe?","11/Dec/12 16:29;jbellis;Moving it from per-CF to global seems kind of backwards, don't you think?  The overwhelming tendency has been for people to push for more fine-grained control over things.","11/Dec/12 17:13;krummas;for this i actually prefer a config, then i can tweak it on one machine at a time

and since it has been broken since 1.1.1 i doubt anyone is actually using it","17/Dec/12 15:51;jbellis;Let's fix the schema code then, and we can add a JMX interface for testing it machine-at-a-time, which is our usual convention for these things.","17/Dec/12 16:11;krummas;ok, sure, that works

i'll get it fixed","18/Dec/12 12:55;krummas;let me know if the double brace initialization in CompressionParameters is not allowed, then i'll remove it

patch against 1.1","19/Dec/12 05:07;iamaleksey;LGTM and works.

Attaching v2 with minor tweaks:
- renamed CFS#crcCheckChance() to CFS#setCrcCheckChance()
- renamed CompressionParameters.globalOptions to CompressionParameters.GLOBAL_OPTIONS and made it an ImmutableSet instead of a HashSet
- CompressionParameters#parseCrcCheckChance() throws CE now - just logging the error and returning default is not enough (Cassandra will save invalid crc_check_chance and you'll get that NumberFormatException every time you start Cassandra)
- added crc_check_chance range validation to parseCrcCheckChance, otherwise we get the same issue as above with valid doubles, but out of range values
- very minor formatting tweaks for consistency

Will commit tomorrow.","19/Dec/12 08:20;krummas;uh my mbean/jmx skills fail me here, but seems when naming the method *set*CrcCheckChance it is not exposed via JMX, naming it crcCheckChance does, any clue why? naming convention rules i guess",19/Dec/12 08:24;iamaleksey;No idea. But all other setters in that mbean start with 'set'. Are they not exposed either?,"19/Dec/12 08:26;krummas;ah!

they end up under the attributes tab in jconsole",19/Dec/12 13:10;iamaleksey;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UUIGen should never use another host IP for its node part,CASSANDRA-5002,12618145,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,29/Nov/12 12:32,12/Mar/19 14:05,13/Mar/19 22:27,07/Dec/12 09:15,1.2.0 rc1,,,,,0,,,,,,,"UUIDGen allows to specify the inet address that we use to generate the node part of the created UUID. This is wrong however. More precisely, the node part is what make sure UUID generated on two different hosts are different, because we can't guarantee that the timestamp and clock parts will be different. In other words, generating on a host a UUID with the node part of another host is dangerous is clearly contrary to the spec.

And as it turns out, making sure we always use the local address means that the full lsb part of the UUID becomes constant (as it should) and we can remove the nodeCache from UUIDGen and simplify/speedup UUID generation, which is all the more reason to fix it.

I note that we were almost always using the local address to generate UUID anyway. The only place where we weren't is in Stream{In/Out}Session, and there is virtually no chance that this has ever broke anything (but we should still fix it).
",,,,,,,,,,,,,,,,,,,29/Nov/12 13:04;slebresne;5002.txt;https://issues.apache.org/jira/secure/attachment/12555348/5002.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-07 06:15:52.068,,,no_permission,,,,,,,,,,,,292767,,,Fri Dec 07 09:16:33 UTC 2012,,,,,,0|i0se2n:,163756,jbellis,jbellis,,,,,,,,,,"07/Dec/12 06:15;jbellis;I'm pretty sure the first part of getAllLocalAddresses (the getAllByName list) will be a subset of the ones seen by getNetworkInterfaces' addresses.  (We do this in CFRR too -- would make sense to extract it somewhere.)

Otherwise +1.","07/Dec/12 09:16;slebresne;You're right, the getAllByName part is probably useless. I've move the getNetworkInterfaces part in FBUtilities (and used it in CFRR and UUIDGen) and committed. Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in creating EnumSet in SimpleAuthorizer example,CASSANDRA-5072,12624114,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jsanda,jsanda,15/Dec/12 13:24,12/Mar/19 14:05,13/Mar/19 22:27,17/Dec/12 15:45,1.2.0 rc2,,,,,0,authentication,,,,,,"In SimpleAuthorizer around line 47 we have,

EnumSet<Permission> authorized = EnumSet.copyOf(Permission.NONE);

This results in an IllegalArgumentException since Permission.NONE is an empty set. I think it should be changed to,

EnumSet<Permission> authorized = EnumSet.noneOf(Permission.class);
",,,,,,,,,,,,,,,,,,,16/Dec/12 16:07;iamaleksey;5072.txt;https://issues.apache.org/jira/secure/attachment/12561191/5072.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-16 01:09:05.089,,,no_permission,,,,,,,,,,,,298474,,,Mon Dec 17 15:44:57 UTC 2012,,,,,,0|i15sen:,241902,jbellis,jbellis,,,,,,,,,,"16/Dec/12 01:09;dbrosius@apache.org;it used to just return Permission.NONE (1.1), which is an immutable set. doing what this patch says seems fine to me.","16/Dec/12 15:54;iamaleksey;It used to return Permission.NONE which used to be a (mutable) EnumSet. In 1.2 it's an ImmutableSet, as it should be. EnumsSet.copyOf is there to convert it back to EnumSet, since that's what the old interface requires.

I guess we should fix it, even though Simple* examples will be dropped in 1.2.1 or 1.2.2 entirely (were supposed to be dropped in 1.2.0 actually, which is why there wasn't much testing done with them).","16/Dec/12 15:57;iamaleksey;Still, there is a reason why they are in 'examples' directory. They are not intended to be used in production, and now they don't even serve as good IAuthenticator/IAuthorizer examples - they only showcase LegacyAuthenticator/LegacyAuthorizer classes).

Maybe we should drop'em for 1.2.0 actually.","16/Dec/12 16:11;iamaleksey;The attached patch updates SimpleAuthorizer. Not sure if the right solution is to fix it or to drop these examples entirely, but it should be resolved somehow.","17/Dec/12 15:26;jbellis;Let's go ahead and fix it (and drop the examples for 2.0).

Patch LGTM.","17/Dec/12 15:44;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCAL_QUORUM consistency causes Tracing to fail,CASSANDRA-5070,12623989,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,tjake,tjake,14/Dec/12 15:21,12/Mar/19 14:05,13/Mar/19 22:27,17/Dec/12 15:46,1.2.0,,,,,0,,,,,,,"{code}
cqlsh:prod4> CONSISTENCY LOCAL_QUORUM;
Consistency level set to LOCAL_QUORUM.

cqlsh:prod4> TRACING ON;
Now tracing requests.
cqlsh:prod4> select * from table1 limit 10 

Bad Request: consistency level LOCAL_QUORUM not compatible with replication strategy (org.apache.cassandra.locator.SimpleStrategy)
{code}


Looks to be the issue with LocalStrategy",,,,,,,,,,,,,,,,,,,14/Dec/12 17:20;iamaleksey;5070.txt;https://issues.apache.org/jira/secure/attachment/12560998/5070.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-14 16:26:03.242,,,no_permission,,,,,,,,,,,,297889,,,Mon Dec 17 15:45:50 UTC 2012,,,,,,0|i14vv3:,236627,jbellis,jbellis,,,,,,,,,,"14/Dec/12 16:26;jbellis;(a) this has nothing to do with tracing

(b) The IRE is deliberate; asking for LOCAL_QUORUM with a DC-oblivious replication strategy is nonsensical","14/Dec/12 16:40;tjake;My replication strategy *is* NTS, the SimpleStrategy is being used in the tracing system space.","14/Dec/12 16:57;jbellis;Ah, that makes sense.  Sounds like cqlsh needs to set CL back to ONE when selecting from the trace events.",14/Dec/12 17:22;iamaleksey;The attached patch also forces cf-name autocompletion and DESCRIBE to use CL.ONE.,17/Dec/12 15:27;jbellis;+1,"17/Dec/12 15:45;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CLONE - Once a host has been hinted to, log messages for it repeat every 10 mins even if no hints are delivered",CASSANDRA-5068,12623863,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,peter-librato,peter-librato,13/Dec/12 23:50,12/Mar/19 14:05,13/Mar/19 22:27,11/Feb/13 19:06,1.1.10,1.2.2,,,,0,qa-resolved,,,,,,"We have ""0 row"" hinted handoffs every 10 minutes like clockwork. This impacts our ability to monitor the cluster by adding persistent noise in the handoff metric.

Previous mentions of this issue are here:
http://www.mail-archive.com/user@cassandra.apache.org/msg25982.html

The hinted handoffs can be scrubbed away with
nodetool -h 127.0.0.1 scrub system HintsColumnFamily
but they return after anywhere from a few minutes to multiple hours later.

These started to appear after an upgrade to 1.1.6 and haven't gone away despite rolling cleanups, rolling restarts, multiple rounds of scrubbing, etc.

A few things we've noticed about the handoffs:
1. The phantom handoff endpoint changes after a non-zero handoff comes through

2. Sometimes a non-zero handoff will be immediately followed by an ""off schedule"" phantom handoff to the endpoint the phantom had been using before

3. The sstable2json output seems to include multiple sub-sections for each handoff with the same ""deletedAt"" information.



The phantom handoff endpoint changes after a non-zero handoff comes through:
 INFO [HintedHandoff:1] 2012-12-11 06:57:35,093 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.1
 INFO [HintedHandoff:1] 2012-12-11 07:07:35,092 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.1
 INFO [HintedHandoff:1] 2012-12-11 07:07:37,915 HintedHandOffManager.java (line 392) Finished hinted handoff of 1058 rows to endpoint /10.10.10.2
 INFO [HintedHandoff:1] 2012-12-11 07:17:35,093 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.2
 INFO [HintedHandoff:1] 2012-12-11 07:27:35,093 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.2



Sometimes a non-zero handoff will be immediately followed by an ""off schedule"" phantom handoff to the endpoint the phantom had been using before:
 INFO [HintedHandoff:1] 2012-12-12 21:47:39,335 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.3
 INFO [HintedHandoff:1] 2012-12-12 21:57:39,335 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.3
 INFO [HintedHandoff:1] 2012-12-12 22:07:43,319 HintedHandOffManager.java (line 392) Finished hinted handoff of 1416 rows to endpoint /10.10.10.4
 INFO [HintedHandoff:1] 2012-12-12 22:07:43,320 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.3
 INFO [HintedHandoff:1] 2012-12-12 22:17:39,357 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.4
 INFO [HintedHandoff:1] 2012-12-12 22:27:39,337 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.4



The first few entries from one of the json files:
{
    ""0aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"": {
        ""ccf5dc203a2211e20000e154da71a9bb"": {
            ""deletedAt"": -9223372036854775808, 
            ""subColumns"": []
        }, 
        ""ccf603303a2211e20000e154da71a9bb"": {
            ""deletedAt"": -9223372036854775808, 
            ""subColumns"": []
        }, 
","cassandra 1.1.6
java 1.6.0_30",,,,,,,,,,CASSANDRA-3733,,,,,,,,05/Feb/13 21:43;brandon.williams;5068.txt;https://issues.apache.org/jira/secure/attachment/12568090/5068.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-14 02:18:05.017,,,no_permission,,,,,,,,,,,,297583,,,Mon Feb 11 19:06:24 UTC 2013,,,,,,0|i14rjb:,235926,jbellis,jbellis,,,,,,,,,enigmacurry,13/Dec/12 23:53;peter-librato;Cloning CASSANDRA-3733 as it seems to be the same issue.,"14/Dec/12 00:11;peter-librato;When there are zero-row hinted handoffs the output of ""list HintsColumnFamily""
might show that 9 of 12 nodes in a ring have a row key like this:
9 of 12 nodes in a ring might have a row key like this:
RowKey: 75555555555555555555555555555554

1 of the 12 nodes will have a different row key than all the rest:
RowKey: 15555555555555555555555555555554

another 1-2 nodes might not have any RowKeys at all
","14/Dec/12 02:18;jbellis;Somehow you've got an empty hints row there that hasn't gotten compacted away.  Not sure how that's possible since we only ever do one handoff at a time per target, and each handoff does a full compaction after it's delivered hints successfully.",11/Jan/13 06:23;mkjellman;reproduced this in 1.2.0 after a rolling restart of the cluster,11/Jan/13 16:07;brandon.williams;[~mkjellman] can you post logs?,"11/Jan/13 16:35;brandon.williams;I'm not sure how we're getting into this situation with an empty hint row (machine restarted before compaction finished?) but one thing we can do to mitigate it is remove the check that we replayed > 0 rows before compacting.  It shouldn't really be necessary since the isEmpty check on hintStore should prevent it, unless something like this has happened.","11/Jan/13 16:46;mkjellman;a bit messy due to the repair log lines

{code}
tem-local-ib-671-Data.db'), SSTableReader(path='/data2/cassandra/system/local/system-local-ib-672-Data.db'), SSTableReader(path='/data2/cassandra/system/local/system-local-ib-670-Data.db')]
 INFO [CompactionExecutor:45] 2013-01-10 21:57:11,166 CompactionTask.java (line 267) Compacted 4 sstables to [/data/cassandra/system/local/system-local-ib-673,].  975 bytes to 590 (~60% of original) in 214ms = 0.002629MB/s.  4 tot
al rows, 1 unique.  Row merge counts were {1:0, 2:0, 3:0, 4:1, }
 INFO [GossipStage:1] 2013-01-10 21:57:16,342 Gossiper.java (line 772) InetAddress /10.8.30.102 is now dead.
 INFO [GossipStage:1] 2013-01-10 21:59:01,958 Gossiper.java (line 790) Node /10.8.30.102 has restarted, now UP
 INFO [GossipStage:1] 2013-01-10 21:59:01,959 Gossiper.java (line 758) InetAddress /10.8.30.102 is now UP
 INFO [HintedHandoff:2] 2013-01-10 21:59:01,960 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 21:59:02,000 ColumnFamilyStore.java (line 647) Enqueuing flush of Memtable-hints@479784922(38/69 serialized/live bytes, 46 ops)
 INFO [FlushWriter:9] 2013-01-10 21:59:02,001 Memtable.java (line 424) Writing Memtable-hints@479784922(38/69 serialized/live bytes, 46 ops)
 INFO [FlushWriter:9] 2013-01-10 21:59:02,195 Memtable.java (line 458) Completed flushing /data2/cassandra/system/hints/system-hints-ib-187-Data.db (85 bytes) for commitlog position ReplayPosition(segmentId=1357883369951, position
=806355)
 INFO [CompactionExecutor:60] 2013-01-10 21:59:02,200 CompactionTask.java (line 120) Compacting [SSTableReader(path='/data2/cassandra/system/hints/system-hints-ib-187-Data.db'), SSTableReader(path='/data2/cassandra/system/hints/sy
stem-hints-ib-186-Data.db')]
 INFO [CompactionExecutor:60] 2013-01-10 21:59:02,431 CompactionTask.java (line 267) Compacted 2 sstables to [/data2/cassandra/system/hints/system-hints-ib-188,].  32,814 bytes to 32,729 (~99% of original) in 230ms = 0.135708MB/s.
  8 total rows, 7 unique.  Row merge counts were {1:8, 2:0, }
 INFO [HintedHandoff:2] 2013-01-10 21:59:02,432 HintedHandOffManager.java (line 408) Finished hinted handoff of 47 rows to endpoint /10.8.30.102
 INFO [GossipStage:1] 2013-01-10 21:59:11,999 StorageService.java (line 1288) Node /10.8.30.102 state jump to normal
 INFO [GossipStage:1] 2013-01-10 21:59:12,003 ColumnFamilyStore.java (line 647) Enqueuing flush of Memtable-peers@1233529943(306/5247 serialized/live bytes, 21 ops)
 INFO [FlushWriter:10] 2013-01-10 21:59:12,004 Memtable.java (line 424) Writing Memtable-peers@1233529943(306/5247 serialized/live bytes, 21 ops)
 INFO [FlushWriter:10] 2013-01-10 21:59:12,265 Memtable.java (line 458) Completed flushing /data2/cassandra/system/peers/system-peers-ib-589-Data.db (351 bytes) for commitlog position ReplayPosition(segmentId=1357883369951, position=806482)
 INFO [GossipStage:1] 2013-01-10 21:59:12,272 ColumnFamilyStore.java (line 647) Enqueuing flush of Memtable-local@1657301357(69/69 serialized/live bytes, 2 ops)
 INFO [FlushWriter:9] 2013-01-10 21:59:12,273 Memtable.java (line 424) Writing Memtable-local@1657301357(69/69 serialized/live bytes, 2 ops)
 INFO [FlushWriter:9] 2013-01-10 21:59:12,455 Memtable.java (line 458) Completed flushing /data2/cassandra/system/local/system-local-ib-674-Data.db (129 bytes) for commitlog position ReplayPosition(segmentId=1357883369951, position=806675)
 WARN [MemoryMeter:1] 2013-01-10 21:59:30,213 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.09066707435830113
 INFO [MemoryMeter:1] 2013-01-10 21:59:30,214 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='messages') liveRatio is 1.0 (just-counted was 1.0).  calculation took 7ms for 55 columns
 INFO [HintedHandoff:1] 2013-01-10 22:00:20,287 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:00:20,288 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [Thread-50] 2013-01-10 22:02:39,618 StorageService.java (line 2304) Starting repair command #1, repairing 1 ranges for keyspace evidence
 INFO [AntiEntropySessions:1] 2013-01-10 22:02:39,637 AntiEntropyService.java (line 652) [repair #815023d0-5bb4-11e2-906d-dd50a26832ff] new session: will sync /10.8.25.101, /10.8.30.14 on range (28356863910078205288614550619314017620,42535295865117307932921825928971026436] for evidence.[fingerprints, messages]
 INFO [AntiEntropySessions:1] 2013-01-10 22:02:39,646 AntiEntropyService.java (line 857) [repair #815023d0-5bb4-11e2-906d-dd50a26832ff] requesting merkle trees for fingerprints (to [/10.8.30.14, /10.8.25.101])
 INFO [ValidationExecutor:1] 2013-01-10 22:02:39,665 ColumnFamilyStore.java (line 647) Enqueuing flush of Memtable-fingerprints@1756165009(409626/409626 serialized/live bytes, 53 ops)
 INFO [FlushWriter:11] 2013-01-10 22:02:39,666 Memtable.java (line 424) Writing Memtable-fingerprints@1756165009(409626/409626 serialized/live bytes, 53 ops)
 INFO [FlushWriter:11] 2013-01-10 22:02:39,871 Memtable.java (line 458) Completed flushing /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-195-Data.db (349405 bytes) for commitlog position ReplayPosition(segmentId=1357883369951, position=3000340)
 WARN [MemoryMeter:1] 2013-01-10 22:02:39,917 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.03491183672633014
 INFO [MemoryMeter:1] 2013-01-10 22:02:39,917 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='messages') liveRatio is 1.0 (just-counted was 1.0).  calculation took 19ms for 106 columns
 INFO [AntiEntropyStage:1] 2013-01-10 22:05:31,251 AntiEntropyService.java (line 214) [repair #815023d0-5bb4-11e2-906d-dd50a26832ff] Received merkle tree for fingerprints from /10.8.25.101
 WARN [MemoryMeter:1] 2013-01-10 22:05:53,141 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.0038005359709140643
 INFO [MemoryMeter:1] 2013-01-10 22:05:53,142 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='fingerprints') liveRatio is 1.0 (just-counted was 1.0).  calculation took 6ms for 10 columns
 INFO [MemoryMeter:1] 2013-01-10 22:08:53,699 Memtable.java (line 207) CFS(Keyspace='brts', ColumnFamily='evidence_index') liveRatio is 3.018170276918194 (just-counted was 3.018170276918194).  calculation took 24ms for 235 columns
 INFO [HintedHandoff:2] 2013-01-10 22:10:20,290 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:10:20,291 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 WARN [MemoryMeter:1] 2013-01-10 22:12:57,094 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.033105659834837216
 INFO [MemoryMeter:1] 2013-01-10 22:12:57,095 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='messages') liveRatio is 1.0 (just-counted was 1.0).  calculation took 18ms for 213 columns
 INFO [HintedHandoff:1] 2013-01-10 22:20:20,293 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:20:20,294 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:30:20,296 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:30:20,297 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:40:20,299 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:40:20,300 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 WARN [MemoryMeter:1] 2013-01-10 22:44:25,510 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.03129749755193589
 INFO [MemoryMeter:1] 2013-01-10 22:44:25,510 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='messages') liveRatio is 1.0 (just-counted was 1.0).  calculation took 20ms for 387 columns
 WARN [MemoryMeter:1] 2013-01-10 22:47:55,174 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.013355315914417125
 INFO [MemoryMeter:1] 2013-01-10 22:47:55,175 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='fingerprints') liveRatio is 1.0 (just-counted was 1.0).  calculation took 11ms for 63 columns
 INFO [HintedHandoff:2] 2013-01-10 22:50:20,302 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:50:20,307 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 23:00:20,304 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 23:00:20,305 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
{code}

digest version
{code}
INFO [HintedHandoff:1] 2013-01-10 22:00:20,288 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:10:20,291 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:20:20,294 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:30:20,297 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:40:20,300 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:50:20,307 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 23:00:20,305 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 23:10:20,308 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 23:20:20,311 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 23:30:20,314 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 23:40:20,317 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
{code}","11/Jan/13 17:16;brandon.williams;Hmm, so it did correctly compact just before logging:

{noformat}
 INFO [HintedHandoff:2] 2013-01-10 21:59:02,432 HintedHandOffManager.java (line 408) Finished hinted handoff of 47 rows to endpoint /10.8.30.102
{noformat}

I'm not sure why anything would be left after that.","18/Jan/13 20:34;kmueller;I'm also seeing this, running 1.1.8 :)","05/Feb/13 21:43;brandon.williams;I ran into this in 1.2, and the problem is definitely old sstables full of tombstones being left behind:

{noformat}
 activity                                                                                        | timestamp    | source        | source_elapsed
-------------------------------------------------------------------------------------------------+--------------+---------------+----------------
                                                                              execute_cql3_query | 21:10:25,847 | 10.179.64.227 |              0
                                                                               Parsing statement | 21:10:25,847 | 10.179.64.227 |             39
                                                                              Peparing statement | 21:10:25,847 | 10.179.64.227 |            208
                                                                   Determining replicas to query | 21:10:25,847 | 10.179.64.227 |            316
 Executing seq scan across 2 sstables for [min(-9223372036854775808), min(-9223372036854775808)] | 21:10:25,870 | 10.179.64.227 |          23223
                                                          Read 0 live cells and 13800 tombstoned | 21:10:25,928 | 10.179.64.227 |          81432
                                                          Read 0 live cells and 13068 tombstoned | 21:10:26,015 | 10.179.64.227 |         168213
                                                                    Scanned 2 rows and matched 2 | 21:10:26,016 | 10.179.64.227 |         169206
                                                                                Request complete | 21:10:26,016 | 10.179.64.227 |         169585
{noformat}


The problem appears to occur here:

{noformat}
 INFO [GossipStage:1] 2013-02-04 22:54:34,828 Gossiper.java (line 754) InetAddress /10.179.111.137 is now UP
 INFO [GossipStage:1] 2013-02-04 22:54:34,830 Gossiper.java (line 754) InetAddress /10.179.65.102 is now UP
 INFO [HintedHandoff:1] 2013-02-04 22:54:34,830 HintedHandOffManager.java (line 297) Started hinted handoff for host: 5b49c861-0cf6-48dc-872a-7fcb89429dae with IP: /10.179.111.137
 INFO [HintedHandoff:2] 2013-02-04 22:54:34,830 HintedHandOffManager.java (line 297) Started hinted handoff for host: 0fd1d3b1-0f73-40fd-ab36-9f4b9636a205 with IP: /10.179.65.102
 INFO [HintedHandoff:2] 2013-02-04 22:54:41,039 ColumnFamilyStore.java (line 678) Enqueuing flush of Memtable-hints@164677298(1020984/1020984 serialized/live bytes, 39151 ops)
 INFO [FlushWriter:2] 2013-02-04 22:54:41,041 Memtable.java (line 453) Writing Memtable-hints@164677298(1020984/1020984 serialized/live bytes, 39151 ops)
 INFO [CompactionExecutor:8] 2013-02-04 22:54:41,043 CompactionTask.java (line 112) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ib-1-Data.db')]
 INFO [FlushWriter:2] 2013-02-04 22:54:41,200 Memtable.java (line 487) Completed flushing /var/lib/cassandra/data/system/hints/system-hints-ib-2-Data.db (166760 bytes) for commitlog position ReplayPosition(segmentId=1360018215672, position=11404848)
 INFO [CompactionExecutor:9] 2013-02-04 22:54:41,215 CompactionManager.java (line 452) SSTables for user defined compaction are already being compacted.
 INFO [HintedHandoff:2] 2013-02-04 22:54:41,216 HintedHandOffManager.java (line 412) Finished hinted handoff of 13909 rows to endpoint /10.179.65.102
 INFO [CompactionExecutor:8] 2013-02-04 22:54:41,444 CompactionTask.java (line 272) Compacted 1 sstables to [/var/lib/cassandra/data/system/hints/system-hints-ib-3,].  625,567 bytes to 625,567 (~100% of original) in 401ms = 1.487749MB/s.  2 total rows, 2 unique.  Row merge counts were {1:2, }
 INFO [HintedHandoff:1] 2013-02-04 22:54:41,445 HintedHandOffManager.java (line 412) Finished hinted handoff of 13230 rows to endpoint /10.179.111.137
{noformat}

The problem is that with concurrent delivery we end up in a state where we have two sstables that contain tombstones, each containing some portion of them for the endpoints that were delivered to.  Since we never replay any rows again, we never compact again, and thus never evict the tombstones.

I think I'm back to what I proposed before of just removing the replayed > 0 check and letting isEmpty check handle the common case.
 ","05/Feb/13 21:51;jbellis;Shouldn't the ""I have > X % tombstones in this sstable"" code kick in?",05/Feb/13 21:56;peter-librato;We see this in 1.1.9 as well.,"05/Feb/13 22:04;brandon.williams;bq. Shouldn't the ""I have > X % tombstones in this sstable"" code kick in?

As in the tombstone-aggressive code in STS?  I suspect we're ending up under the min compaction threshold all the time.","11/Feb/13 18:59;slebresne;I believe this is a consequence of CASSANDRA-5241. Now while we should probably fix CASSANDRA-5241, relying on tombstone being always fully collected feels a bit fragile and not very useful since we have the isEmpty check at the beginning of the delivery. So +1 on the patch.",11/Feb/13 19:06;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL3 does handle List append or prepend with a ""Prepared"" list",CASSANDRA-4945,12615647,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,ardot,ardot,11/Nov/12 16:21,12/Mar/19 14:05,13/Mar/19 22:27,21/Nov/12 08:21,1.2.0 beta 3,,,,,0,,,,,,,"I can successfully update a List using the ""literal"" syntax:

{code}
UPDATE testcollection SET L = [98,99,100] + L WHERE k = 1;
{code}

And I can successfully ""upsert"" a List using the ""Prepared"" syntax:


{code}
UPDATE testcollection SET L = ? WHERE k = 1
{code}

by providing a decoded List<Integer> in the bind values.

But using the ""prepared"" syntax for an prepend like:
{code}
UPDATE testcollection SET L = ? + L WHERE k = 1
{code}
fails with the following message:
{code}
java.sql.SQLSyntaxErrorException: InvalidRequestException(why:line 1:33 mismatched input '+' expecting K_WHERE)
	at org.apache.cassandra.cql.jdbc.CassandraPreparedStatement.<init>(CassandraPreparedStatement.java:92)
...
...
{code}

and an append of a ""prepared"" syntax like:
{code}
UPDATE testcollection SET L = L + ? WHERE k = 1
{code}
fails as follows:
{code}
java.sql.SQLSyntaxErrorException: InvalidRequestException(why:invalid operation for non commutative columnfamily testcollection)
	at org.apache.cassandra.cql.jdbc.CassandraPreparedStatement.<init>(CassandraPreparedStatement.java:92)
...
...
{code}




",CQL3 Thrift methods (new),,,,,,,,,,,,,,,,,,12/Nov/12 11:15;slebresne;4945.txt;https://issues.apache.org/jira/secure/attachment/12553101/4945.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-12 10:47:03.712,,,no_permission,,,,,,,,,,,,256946,,,Wed Nov 21 08:21:55 UTC 2012,,,,,,0|i0j0x3:,109077,jbellis,jbellis,,,,,,,,,,"12/Nov/12 10:47;slebresne;Hum, apparently CASSANDRA-4739 hasn't been generic enough. Attaching patch to generalize the approach to support all of this.","14/Nov/12 12:30;jbellis;Can you review, [~xedin]?","20/Nov/12 21:17;jbellis;LGTM, +1","21/Nov/12 08:21;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool doesnt work well with negative tokens,CASSANDRA-4808,12611860,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,15/Oct/12 17:22,12/Mar/19 14:05,13/Mar/19 22:27,23/Oct/12 18:02,1.2.0 beta 2,,,,,0,,,,,,,"./apache-cassandra-1.2.0-beta1-SNAPSHOT/bin/nodetool move \-2253536297082652573
Unrecognized option: -2253536297082652573
usage: java org.apache.cassandra.tools.NodeCmd --host <arg> <command>
            
 -cf,--column-family <arg>   only take a snapshot of the specified column
                             family
",,,,,,,,,,,,,,,,,,,15/Oct/12 18:14;vijay2win@yahoo.com;0001-CASSANDRA-4808.patch;https://issues.apache.org/jira/secure/attachment/12549181/0001-CASSANDRA-4808.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-15 19:53:28.571,,,no_permission,,,,,,,,,,,,248767,,,Tue Oct 23 18:02:37 UTC 2012,,,,,,0|i09ztb:,56243,slebresne,slebresne,,,,,,,,,,"15/Oct/12 18:14;vijay2win@yahoo.com;There are 2 option, 

Make nt to accept '-' by making ""Options"" ignore any -xxxx values (dont verify if they are valid options), this can cause confusion on other commands.

Other option (Attached patch) is to support escape character for '-' 
{code}
Example: ./apache-cassandra-1.2.0-beta1-SNAPSHOT/bin/nodetool move \\-2253536297082652571
{code}",15/Oct/12 19:53;brandon.williams;I'm confused as to why we need negative tokens.,"15/Oct/12 20:31;vijay2win@yahoo.com;M3P supports -ve tokens the range is from Long.MIN_VALUE to Long.MAX_VALUE, 

explanation is in 
https://issues.apache.org/jira/browse/CASSANDRA-4621?focusedCommentId=13452829&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13452829

Example for generating tokens:

For RandomPartitioner

Node 0	: 0
Node 1	: 56713727820156410577229101238628035242
Node 2	: 113427455640312821154458202477256070484

For Murmur3Partitioner

Node 0	: 0
Node 1	: 6148914691236517204
Node 2	: -6148914691236517208","16/Oct/12 06:20;slebresne;bq. I'm confused as to why we need negative tokens.

To elaborate, M3P uses a long token internally and since long in java are signed, we end up with a tokens that can be negative. We could chose to interpret the long as unsigned, but even if we do that, we need the minimum token to not be a valid token, so tokens would have to be a value in [1, 2^64-1], not [0, 2^64-1] and so people would have to adapt whatever algorithm they use to compute tokens anyway and hence I'm not sure it's worth bothering changing from negative tokens.","23/Oct/12 08:39;slebresne;+1

nit: could be nice to add how to do the escaping in the help of nodetool move (I think that's the only command that take a token as argument).","23/Oct/12 18:02;vijay2win@yahoo.com;Added help comment and committed, Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair -pr throws EOFException,CASSANDRA-5105,12625910,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,mkjellman,mkjellman,03/Jan/13 21:01,12/Mar/19 14:05,13/Mar/19 22:27,14/Feb/13 20:00,1.2.2,,,,,1,,,,,,,"nodetool repair -pr threw an EOF exception

{code:title=node1}
ERROR 12:50:18,723 Exception in thread Thread[Streaming to /10.8.25.113:1,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:375)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:114)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
{code}

{code:title=node2}
 INFO 12:49:45,139 Finished streaming session to /10.8.30.13
ERROR 12:50:18,799 Exception in thread Thread[Thread-4031,5,main]
java.lang.RuntimeException: Last written key DecoratedKey(167625858728826091814875924785363245309, 6634333531356661643161636636373738353431363162353031376164386339) >= current ke
y DecoratedKey(33957321636818582219838207277782228619, 696c2e636f6d200a3c42523e0a3c42523e0a5472656e7420202020202020202020202020202020422e204d697261636c652020202020202020202020202
020202020202020202020202020202020202020202020202020202020200a2020266e62737020266e62737020746d697261636c654073696d6d6f6e736669726d2e636f6d2c206c776f6f74656e4073696d6d6f6e736669726
d2e636f6d203c42523e0a3c42523e0a56616e636520202020202020202020202020202020522e20416e64727573202020202020202020202020202020202020202020202020202020202020202020202020202020202020200
a2020266e62737020266e627370207672614061622d706c632e636f6d203c42523e0a3c42523e0a5665726e6f6e202020202020202020202020202020462e20476c656e6e20202020202020202020202020202020202020202
020202020202020202020202020202020202020202020200a2020266e62737020266e62737020676c656e6e6c6177406c6f77636f756e7472796c61777965722e636f6d203c42523e0a3c42523e0a56696e63656e742020202
0202020202020202020204a2e20446573616c766f2020202020202020202020202020202020202020202020202020202020202020202020202020202020200a2020266e62737020266e6273702076646573616c766f4064657
3616c766f6c61776669726d2e636f6d203c42523e0a3c42523e0a56696e63656e7420202020202020202020202020204a616d65732043617274657220202020202020202020202020202020202020202020202020202020202
0202020202020202020200a2020202020266e62737020266e627370207663617274657240676972617264696b656573652e636f6d2c207479616d6173616b6940676972617264696b656573652e636f6d203c42523e0a0a3c4
2523e0a572e202020202020202020202020202020202020204a616d65732053696e676c65746f6e202020202020202020202020202020202020202020202020202020202020202020202020200a2020202020266e627370202...trunkated...324132393239413134333439413834453531394133373431) writing into /data/cassandra/evidence/fingerprints/evidence-fingerprints-tmp-ia-161-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)
        at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:209)
        at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:226)
        at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:166)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:66)
{code}",Ubuntu 12.04 Java 7,,,,,,,,,,,,,,,,,,12/Feb/13 22:29;yukim;0001-add-CompressedInputStream-test.patch;https://issues.apache.org/jira/secure/attachment/12569081/0001-add-CompressedInputStream-test.patch,12/Feb/13 22:29;yukim;0002-fix-compressed-streaming-sends-extra-chunk.patch;https://issues.apache.org/jira/secure/attachment/12569082/0002-fix-compressed-streaming-sends-extra-chunk.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-03 22:07:14.953,,,no_permission,,,,,,,,,,,,302491,,,Wed Feb 27 08:10:27 UTC 2013,,,,,,0|i173iv:,249538,mkjellman,mkjellman,,,,,,,,,,"03/Jan/13 22:07;yukim;It looks like node1 is streaming wrong part of SSTable file.
With key cache turned on, this may relate to CASSANDRA-4687.",03/Jan/13 22:43;mkjellman;can not reproduce with keycache turned off on node1,04/Jan/13 06:08;mkjellman;i *think* i just reproduced this with key cache turned off..,15/Jan/13 17:08;mkjellman;confirmed that I have seen this with key_cache_size_in_mb: 0 for every node in the cluster,"23/Jan/13 13:54;alprema;We just migrated our cluster to 1.2 here and we're having a similar issue when running repairs. Since the call stack mentioned ""CompressedFileStreamTask"" we tried to disable the column family compaction and scrub it. The following repairs worked fine so we decided to re-enable the compression, which made the error come back.
I don't know if it was a coincidence or if there is an issue with the compression, can anyone else reproduce the behavior?","23/Jan/13 17:33;jbellis;Does that help narrow it down, Yuki?","24/Jan/13 14:40;alprema;Update: We moved back the failing CF to sstable_compression='' and it doesn't show errors in the log anymore.
The other CFs are working/repairing fine, but nodetool netstats during a repair shows wrong percentages (156098%...) could it be related?

I'd be happy to run other tests if it can help finding the issue.","24/Jan/13 14:48;yukim;[~alprema] wrong percentage was fixed in CASSANDRA-5130 and will be released in 1.2.1.

I'm trying to reproduce myself but still no luck. Some logs before error happend may help if you can upload.","25/Jan/13 09:45;alprema;[~yukim] Great for the percentages, I'm relieved my other CFs are not impacted by the issue.
For the main problem at hand here are a couple logs (We have a 3 nodes cluster and the problem occurred between 01 and 02, we can see that 02 throws an exception when receiving the stream, which causes a Broken Pipe exception on the 01 side).
{code:title=cassandra02}
 INFO 15:52:16,502 Compacted to [/var/lib/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-ia-902-Data.db,].  15,399,190 to 15,439,842 (~100% of original) bytes for 742 keys at 6.415939MB/s.  Time: 2,295ms.
 INFO 15:52:19,121 Compacted to [/var/lib/cassandra/data/MyMetrics/Metrics/MyMetrics-Metrics-ia-652-Data.db,].  34,516,769 to 34,493,377 (~99% of original) bytes for 735 keys at 5.948544MB/s.  Time: 5,530ms.
 INFO 15:52:19,566 Compacted to [/var/lib/cassandra/data/MyTimeouts/RequestTimeoutCommands/MyTimeouts-RequestTimeoutCommands-ia-746-Data.db,].  28,662,815 to 28,722,158 (~100% of original) bytes for 451,077 keys at 4.618375MB/s.  Time: 5,931ms.
 INFO 15:53:26,796 Enqueuing flush of Memtable-MyBusinessHistoryCF@335344726(17286127/177550150 serialized/live bytes, 360751 ops)
 INFO 15:53:26,803 Writing Memtable-MyBusinessHistoryCF@335344726(17286127/177550150 serialized/live bytes, 360751 ops)
 INFO 15:53:27,725 Completed flushing /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4879-Data.db (9746867 bytes) for commitlog position ReplayPosition(segmentId=1358949372386, position=24979117)
 INFO 15:53:36,532 [streaming task #aa70f8e0-656c-11e2-b226-d966287ae7ca] Received task from /10.80.90.51 to stream 7871 ranges to /10.80.90.53
 INFO 15:53:36,533 [streaming task #aa70f8e0-656c-11e2-b226-d966287ae7ca] Performing streaming repair of 7871 ranges with /10.80.90.53
 INFO 15:53:43,216 Stream context metadata [/var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4840-Data.db sections=5086 progress=0/350803667 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4787-Data.db sections=6079 progress=0/1160848303 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4878-Data.db sections=3469 progress=0/45248343 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4783-Data.db sections=6203 progress=0/1189290990 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4782-Data.db sections=3452 progress=0/517261421 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4872-Data.db sections=3518 progress=0/44182720 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4825-Data.db sections=5526 progress=0/571701570 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4879-Data.db sections=3043 progress=0/3086625 - 0%], 11 sstables.
 INFO 15:53:43,217 Streaming to /10.80.90.53
 INFO 15:53:43,325 Beginning transfer to /10.80.90.51
 INFO 15:53:43,362 Flushing memtables for [CFS(Keyspace='MyBusinessKeyspace', ColumnFamily='MyBusinessHistoryCF')]...
 INFO 15:53:43,363 Enqueuing flush of Memtable-MyBusinessHistoryCF@554695962(2424273/24900316 serialized/live bytes, 51289 ops)
 INFO 15:53:43,366 Writing Memtable-MyBusinessHistoryCF@554695962(2424273/24900316 serialized/live bytes, 51289 ops)
 INFO 15:53:43,558 Completed flushing /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4881-Data.db (2083586 bytes) for commitlog position ReplayPosition(segmentId=1358949372387, position=2780498)
 INFO 15:53:51,056 Stream context metadata [/var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4787-Data.db sections=6645 progress=0/1303785158 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4840-Data.db sections=5775 progress=0/392625457 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4878-Data.db sections=4530 progress=0/53236689 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4881-Data.db sections=3917 progress=0/715697 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4783-Data.db sections=6782 progress=0/1316663783 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4782-Data.db sections=3826 progress=0/579082484 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4825-Data.db sections=6223 progress=0/597029084 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4872-Data.db sections=4573 progress=0/52484841 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4879-Data.db sections=4083 progress=0/3265613 - 0%], 12 sstables.
 INFO 15:53:51,057 Streaming to /10.80.90.51
 INFO 15:54:19,013 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4840-Data.db to /10.80.90.53
ERROR 15:54:46,686 Exception in thread Thread[Thread-3087,5,main]
java.lang.RuntimeException: Last written key DecoratedKey(153906576608468125601485890282698016632, 72736b3a67726f75703a73656375726974793a496e74657276656e74696f6e3a496e7472616461793a66306135386333352d353262312d343361642d396430332d6130636630306330306565633a3937383a313330313137) >= current key DecoratedKey(33745288399064288388334698406389712581, bec0000000220a0b08f494e9b2b582a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da348880200093a80510693670004d3cdc5d50022000000220a0b08dac28aa4b782a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da38e9a0200093a80510693790004d3cdc6e6b7a1000000220a0b08d2ee93cfb882a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da3cdb70200093a805106938a0004d3cdc7ddf7ec000000220a0b08e6dd9ee9b982a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da4165a0200093a805106939c0004d3cdc8f8bedc000000220a0b08b299c99abb82a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da46ad40200093a80510693b20004d3cdca433560000000220a0b08828be8e8bc82a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da4c8760200093a80510693ca0004d3cdcbb0f580000000220a0b08fe8fb4cdbe82a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da6b4f90200093a80510694480004d3cdd338a8d9000000220a0b08c8fdeaffc782a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da72f8a0200093a80510694670004d3cdd5139c53000000220a0b08f8a088abca82a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da7cb3f0200093a805106948f0004d3cdd77320df000000220a0b08a6f39aa7cd82a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da8314d0200093a80510694a90004d3cdd901c839000000220a0b08b699afa0cf82a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da889710200093a80510694c00004d3cdda5f14bc000000220a0b08fec5c6f7d082a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5da969630200093a80510694f90004d3cdddc4def5000000220a0b08e8a6a69ad582a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5daa12b50200093a80510695240004d3cde05b3a38000000220a0b0894c3d6b7d882a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5daa5e360200093a80510695380004d3cde18637bc000000220a0b08febd81f0d982a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5daaa0fc0200093a80510695490004d3cde2864a6d000000220a0b088ae88493db82a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5dab00540200093a80510695610004d3cde3fa7a93000000220a0b08bef7e6fbdc82a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5dab382d0200093a805106956f0004d3cde4d4e148000000220a0b08a4fa9384de82a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5dab7cc90200093a80510695810004d3cde5e0a2d5000000220a0b08ecc1d3abdf82a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5dabab5b0200093a805106958d0004d3cde69651b5000000220a0b08a29ead9de082a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5dac33690200093a80510695b10004d3cde8b0b143000000220a0b08ccf8c1e9e282a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5dac79e80200093a80510695c20004d3cde9bd66f7000000220a0b0892e7d095e482a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5dad87d10200093a80510696070004d3cdeddcb0ac000000220a0b089ec5cba8e982a33010051213088080f4f3b1dc84c3c601108be9b0cc08182e00080000013c5dae07e00200093a80510696280004d3cdefd5aeec000000220a0b0882c19de1eb82a33010051213088080c0e6efa987a6830110e490fbca08182e00080000013c5dae30d50200093a80510696320004d3cdf06fb566000000220a0b0880b29cc5ec82a33010051213088080c0e6efa987a6830110e490fbca08182e00080000013c5daea3280200093a805106964f0004d3cdf22ec372000000220a0b08e2d0aadcee82a33010051213088080c0e6efa987a6830110e490fbca08182e00080000013c5daed3410200093a805106965c0004d3cdf2ea2b3a000000220a0b088a9be1d1ef82a33010051213088080c0e6efa987a6830110e490fbca08182e00080000013c5daf1e540200093a805106966f0004d3cdf40f6fd5000000220a0b08a6d38689f182a33010051213088080c0e6efa987a6830110e490fbca08182e00080000013c5dafddbf0200093a80510696a10004d3cdf7002874000000220a0b08a6e4afdcf482a33010051213088080c0d3b0c7f2fdca011089bab2ca08182e00080000013c5db035360200093a80510696b60004d3cdf851c61b000000220a0b0898a8f5b1f682a33010051213088080c0d3b0c7f2fdca011089bab2ca08182e00080000013c5db193f00200093a80510697100004d3cdfdac06dd000000220a0b08fcbc968afd82a33010051213088080c0d3b0c7f2fdca011089bab2ca08182e00080000013c5db2c7b00200093a805106975f0004d3ce025db5d5000000220a0b08e0a6c3f98283a33010051213088080c0d3b0c7f2fdca011089bab2ca08182e00080000013c5db385f20200093a80510697910004d3ce0549e6bc000000220a0b08fa8282ca8683a33010051213088080c0d3b0c7f2fdca011089bab2ca08182e00080000013c5db49c180200093a80510697d70004d3ce09836a9b000000220a0b08d4e58bf18b83a33010051213088080c0d3b0c7f2fdca011089bab2ca08182e00080000013c5db517540200093a80510697f60004d3ce0b645408000000220a0b08daf1fa9d8e83a33010051213088080c0d3b0c7f2fdca011089bab2ca08182e00080000013c5db55aa70200093a80510698090004d3ce0c74da57000000220a0b08b8f1a9c28f83a33010051213088080c0d3b0c7f2fdca011089bab2ca08182e00080000013c5db616490200093a80510698380004d3ce0f48b960000000220a0b08bcb4b48c9383a33010051213088080c0d3b0c7f2fdca011089bab2ca08182e00080000013c5db659da0200093a80510698490004d3ce1050303f000000220a0b08a2daafb19483a33010051213088080c0d3b0c7f2fdca011089bab2ca08182e00080000013c5db697800200093a80510698590004d3ce1141b7a1000000210a0b0886e5f0c79583a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5db6eac20200093a805106986e0004d3ce12876963000000220a0b08ae9892939783a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5db72f7c0200093a80510698800004d3ce1397efb2000000220a0b08faf2f7ba9883a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5db75c780200093a805106988b0004d3ce14422cc0000000220a0b08fcd7e1a89983a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5db7c49a0200093a80510698a60004d3ce15d96977000000220a0b08d0c1fea69b83a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5db80bd50200093a80510698b80004d3ce16efe5b9000000220a0b08aea2f2d49c83a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5db85bda0200093a80510698cc0004d3ce1827c349000000220a0b08acdda0989e83a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5db8b5b30200093a80510698e40004d3ce1987af36000000220a0b08ac94cef39f83a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5db8ffcd0200093a80510698f70004d3ce1aa8e62c000000220a0b08a8b4c2a8a183a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5db934a90200093a80510699040004d3ce1b7723f2000000220a0b08ece4c8a9a283a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dba1ac60200093a805106993f0004d3ce1efa04cb000000220a0b08e086afdba683a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dbad9070200093a80510699710004d3ce21e635b2000000220a0b08fae2edabaa83a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dbb1fa60200093a80510699820004d3ce22f510bd000000220a0b08c4e4a2d8ab83a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dbb5e170200093a80510699920004d3ce23e8fa80000000220a0b0882ebdbf0ac83a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dbc1bcc0200093a80510699c20004d3ce26cd4d27000000220a0b08caf1eebfb083a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dbc8a660200093a80510699df0004d3ce287dcfe3000000220a0b08f2cbf1cdb283a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dbcd7000200093a80510699f20004d3ce29a9477a000000220a0b0880f2f388b483a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dbcfe100200093a80510699fc0004d3ce2a41dfba000000220a0b0880bca3e8b483a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dbe82530200093a8051069a600004d3ce3033c495000000220a0b08f2ac979cbc83a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dc050140200093a8051069ad60004d3ce373ab777000000220a0b08ccdec083c583a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dc0ec360200093a8051069afe0004d3ce399c2451000000220a0b08c8f3d880c883a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dc17ca10200093a8051069b230004d3ce3bd006ed000000220a0b08c4e5a2e1ca83a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dc1d2ef0200093a8051069b390004d3ce3d212a81000000220a0b08d0f4fdb3cc83a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc22bde0200093a8051069b500004d3ce3e84e709000000220a0b08f29c8d8dce83a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc2b4690200093a8051069b730004d3ce40922982000000220a0b08acc3badad083a33010051213088080e0cfc3ebcff1f10110e5dcb4e907182e00080000013c5dc3d50f0200093a8051069bbc0004d3ce44f9b1b2000000220a0b08de8a959bd683a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc400a30200093a8051069bc90004d3ce45adf257000000220a0b08f294c885d783a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc429890200093a8051069bd20004d3ce46437119000000220a0b08aefcb3e9d783a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc463270200093a8051069be10004d3ce472484de000000220a0b088e938af6d883a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc517370200093a8051069c0f0004d3ce49e3e2a5000000220a0b089abad7addc83a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc5c8a70200093a8051069c3c0004d3ce4c99b6e8000000220a0b0890c8f0dedf83a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc66b230200093a8051069c660004d3ce4f13759e000000220a0b08dcbac8ebe283a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc70a530200093a8051069c8f0004d3ce5181c283000000220a0b08bcab9af0e583a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc7aa2e0200093a8051069cb90004d3ce54013a23000000220a0b08f284bef6e883a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc84e110200093a8051069ce10004d3ce56716f55000000220a0b08acd2cc86ec83a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc8952d0200093a8051069cf40004d3ce57877183000000220a0b0886a09ab4ed83a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc8f4560200093a8051069d0c0004d3ce58fc1bbc000000220a0b08f492c39cef83a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dc97ed40200093a8051069d300004d3ce5b1da26e000000220a0b08eee9d2eef183a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dcb53b90200093a8051069da80004d3ce62453f81000000220a0b08f2f4b3e7fa83a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dcc7c000200093a8051069df40004d3ce66c60db5000000220a0b08dee1deba8084a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dcd28dc0200093a8051069e210004d3ce6970ea3a000000220a0b08fea5e1e08384a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dcefd630200093a8051069e980004d3ce7093c28b000000220a0b08f6f7cfd88c84a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dcf3eb20200093a8051069ea80004d3ce718e9668000000220a0b08d2bd89f88d84a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd014260200093a8051069edf0004d3ce74d022df000000220a0b08e4b998819284a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd089c40200093a8051069efd0004d3ce769bd3ec000000220a0b08b2daaca09484a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd0d2190200093a8051069f100004d3ce77bb9ca8000000220a0b08b4e6f7d09584a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd148540200093a8051069f2e0004d3ce79847141000000220a0b0896e6caf19784a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd17abf0200093a8051069f3b0004d3ce7a486e67000000220a0b088a9ad6ec9884a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd1ba0a0200093a8051069f4b0004d3ce7b3faeb2000000220a0b08e4a59a879a84a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd1f7140200093a8051069f5b0004d3ce7c2f8ad0000000220a0b08b4d19c9c9b84a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd235e30200093a8051069f6b0004d3ce7d23eea6000000220a0b08fe90c8b59c84a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd2a6af0200093a8051069f880004d3ce7ee11464000000220a0b08eec1f9c89e84a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd2ed100200093a8051069f9a0004d3ce7feebe3f000000220a0b08b09de2f49f84a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd3a46b0200093a8051069fc90004d3ce82bbb32f000000220a0b08a8c6b5b4a384a33010051213088080c0eb97dac0de8c0110f8ebd5e907182e00080000013c5dd3e27f0200093a8051069fd80004d3ce83ae2eb8000000220a0b08da93fccba484a33010051213088080e0d7d4d9f183b00110f7fbf6e907182e00080000013c5dd47b650200093a805106a0000004d3ce86083764000000220a0b08acb0a1c1a784a33010051213088080e0d7d4d9f183b00110f7fbf6e907182e00080000013c5dd4dd9b0200093a805106a0190004d3ce878351a3000000220a0b08fefe83b1a984a33010051213088080e0d7d4d9f183b00110f7fbf6e907182e00080000013c5dd5336c0200093a805106a02f0004d3ce88d1d5cc000000220a0b08fac1c682ab84a33010051213088080e0d7d4d9f183b00110f7fbf6e907182e00080000013c5dd55c610200093a805106a0390004d3ce89724c4d000000220a0b08f8b2c5e6ab84a33010051213088080e0d7d4d9f183b00110f7fbf6e907182e00080000013c5dd5843d0200093a805106a0430004d3ce8a0dc101000000220a0b08d2f8ecc7ac84a33010051213088080e0d7d4d9f183b00110f7fbf6e907182e00080000013c5dd5c7ee0200093a805106a0550004d3ce8b166910000000220a0b08bcb18eedad84a33010051213088080e0d7d4d9f183b00110f7fbf6e907182e00080000013c5dd60ac40200093a805106a0660004d3ce8c1c34ab000000220a0b088ae5a490af84a33010051213088080e0d7d4d9f183b00110f7fbf6e907182e00080000013c5dd650880200093a805106a0790004d3ce8d35ca6a000000210a0b08b8e1cebab084a3301005121208808080bc80eb80971510e5ecd5e907182e00080000013c5dd7135d0200093a805106a0aa0004d3ce902514cf000000210a0b08a887a496b484a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5dd73f020200093a805106a0b50004d3ce90cf14d3000000210a0b08fe9aea80b584a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5dd7a4260200093a805106a0cf0004d3ce925adfb9000000210a0b08b0b2e0f7b684a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5dd7fff30200093a805106a0e60004d3ce93c13bac000000210a0b08f099f0d7b884a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5dd83dc70200093a805106a0f60004d3ce94b28605000000210a0b089ac1eaeeb984a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5dd9bd670200093a805106a1580004d3ce9a8d4a33000000210a0b08f4deb497c184a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5dd9f9860200093a805106a1680004d3ce9b7c6f34000000210a0b08e6fb98aac284a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5dda395f0200093a805106a1780004d3ce9c711014000000210a0b0892dd88c6c384a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5dda64b50200093a805106a1830004d3ce9d19dee8000000210a0b089ec1efafc484a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5ddabbde0200093a805106a1990004d3ce9e707e5b000000210a0b08c6d5d584c684a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5ddbcec90200093a805106a1e00004d3cea2a725c5000000210a0b08f6bfeca3cb84a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5ddc13930200093a805106a1f10004d3cea3ad6b73000000210a0b0884a4e5cbcc84a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5ddd42ef0200093a805106a23f0004d3cea850cc25000000210a0b08d8e0b4b0d284a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5ddda3300200093a805106a2580004d3cea9caf23e000000210a0b08eafeb49bd484a3301005121208808080b4effcde845710d3cd93e907182e00080000013c5ddf26990200093a805106a2bb0004d3ceafb07120000000210a0b08c0ea9dcddb84a3301005121208808080bc80eb80971510e5ecd5e907182e00080000013c5ddf5d880200093a805106a2c90004d3ceb0887574000000220a0b08c8deacd3dc84a3301005121308808080c491d9a2a9d30110f68b98ea07182e00080000013c5de14ccb0200093a805106a3480004d3ceb81bd7a8000000210a0b08acf8bd8ce684a330100512120880808090abbed5c47010d1bafbea07182e00080000013c5de206d60200093a805106a3770004d3cebaec9d33000000210a0b08fcc3d8d2e984a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de245560200093a805106a3870004d3cebbe049ed000000210a0b08fcd3a4ebea84a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de2c8e00200093a805106a3a90004d3cebde294a8000000210a0b0892eeb5aced84a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de321710200093a805106a3c00004d3cebf409847000000210a0b08a8ddd284ef84a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de3c2270200093a805106a3e90004d3cec1b0165d000000210a0b08fabb818df284a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de40e930200093a805106a3fc0004d3cec2da1fba000000210a0b08c2c5cac7f384a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de4dd3f0200093a805106a4310004d3cec6022923000000210a0b08b6a194c0f784a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de518450200093a805106a4400004d3cec6e7c4a0000000210a0b088493a1d0f884a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de5748f0200093a805106a4580004d3cec85045eb000000210a0b08d4c6c9b1fa84a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de685c40200093a805106a49e0004d3cecc7c32a1000000210a0b08cca6caccff84a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de6cadc0200093a805106a4b00004d3cecd8e6434000000210a0b08a4baa2f58085a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de70a670200093a805106a4c00004d3cece8210ed000000210a0b0886ecb2908285a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de7a6990200093a805106a4e80004d3ced0e4aef7000000210a0b08c48ade8d8585a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de8458a0200093a805106a5100004d3ced350997b000000210a0b089cd5e3918885a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de87d350200093a805106a51f0004d3ced42a4913000000210a0b08bcbbd7998985a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5de9af5f0200093a805106a56d0004d3ced8d63f22000000210a0b08ecad94858f85a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5dea74480200093a805106a5a00004d3cedbdbf317000000210a0b08a097f2e59285a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5dec4a360200093a805106a6190004d3cee3079dcf000000210a0b0886c497e19b85a33010051212088080e0a3eebea49f4d10d2aadaea07182e00080000013c5dec72310200093a805106a6220004d3cee39f41e8000000210a0b08e49ce5c29c85a330100512120880808090abbed5c47010d1bafbea07182e00080000013c5dece1f30200093a805106a63f0004d3cee5544c5c000000210a0b08f2abd2d39e85a330100512120880808090abbed5c47010d1bafbea07182e00080000013c5dedb2170200093a805106a6740004d3cee880dd7e000000210a0b0896ece5cfa285a33010051212088080e0abffacc6b10b10e4c99ceb07182e00080000013c5dee1e110200093a805106a6900004d3ceea2c2164000000210a0b08a8adb4d7a485a33010051212088080e0abffacc6b10b10e4c99ceb07182e00080000013c5dee5c530200093a805106a6a00004d3ceeb1a8f48000000210a0b08a097b4efa585a33010051212088080e0abffacc6b10b10e4c99ceb07182e00080000013c5def77f80200093a805106a6e80004d3ceef6e8a5c000000210a0b08aed2f2a3ab85a33010051212088080e0abffacc6b10b10e4c99ceb07182e00080000013c5defd7300200093a805106a7000004d3cef0e18951000000210a0b08deceae8cad85a3301005121208808098fd87f2abd56c1082cf8aeb07182e00080000013c5df1b8880200093a805106a77c0004d3cef83a4433000000220a0b08faeec2a3b685a3301005121308808080b3afe4c7f7d10110caf5c3e907182e00080000013c5df39d7a0200093a805106a7f90004d3ceffa51df6000000220a0b08ccc0bcc3bf85a3301005121308808080aeecf9ba8ea90110b5deabeb07182e00080000013c5df5709a0200093a805106a8700004d3cf06c73f2a000000220a0b08d6b7f4b7c885a33010051213088080808890c9c9b99d0110affcedeb07182e00080000013c5df7455f0200093a805106a8e80004d3cf0debffc9000000220a0b08d6afafb0d185a33010051213088080808890c9c9b99d0110affcedeb07182e00080000013c5df919c70200093a805106a9600004d3cf1510094b000000220a0b08caeef7a7da85a33010051213088080808890c9c9b99d0110affcedeb07182e00080000013c5e18ebc60200093a805106b1850004d3cf9158fd92000000220a0b08ee9ecfd7f586a33010051213088080808890c9c9b99d0110affcedeb07182e00080000013c5e1930bf0200093a805106b1960004d3cf92666a63000000220a0b08c29f8180f786a33010051213088080808890c9c9b99d0110affcedeb07182e00080000013c5e98661b0200093a805106d2270004d3d183500e3f000000220a0b08b08aa591e48ba33010051213088080808890c9c9b99d0110affcedeb07182e002c72736b3a6172626974726167653a496e74657276656e74696f6e3a486f6c643a32343031393a3133303131370000000000000da47fffffff80000000000000000000004f00080000013c4691e4de0200093a805100aba20004d373a9f7e89c0000000d0a0b08ec8ee2af94a1a130100500080000013c471963ba0200093a805100ce530004d375bb4fe4dd0000000d0a0b08a4e8bcfca9a6a130100500080000013c472634350200093a805100d1980004d375ed416cb50000000d0a0b08c6a1bbc5e8a6a130100500080000013c475051060200093a805100dc600004d37691c554ac0000000d0a0b08e8dae895b6a8a130100500080000013c4757a3fc0200093a805100de400004d376ae5c64cd0000000d0a0b08e4a7aef7d9a8a130100500080000013c475979000200093a805100deb80004d376b587d27b0000000d0a0b08ecc5b5f0e2a8a130100500080000013c475d22aa0200093a805100dfa80004d376c3d3b61a0000000d0a0b08f0c8d1e1f4a8a130100500080000013c476f72600200093a805100e4580004d3770b5b51240000000d0a0b08f0f89e96cea9a130100500080000013c47789a0c0200093a805100e6b00004d3772f1ff44b0000000d0a0b08aab48cf0faa9a130100500080000013c477e18e90200093a805100e8190004d377449b533d0000000d0a0b08fcf1e8da95aaa130100500080000013c477fed410200093a805100e8910004d3774bc86c300000000d0a0b08aea79ed29eaaa130100500080000013c4781c1990200093a805100e9090004d37752e7edf90000000d0a0b08e0dcd3c9a7aaa130100500080000013c478397780200093a805100e9810004d3775a0f4e030000000d0a0b088480e6c4b0aaa130100500080000013c47856bef0200093a805100e9f90004d3776137a2330000000d0a0b08bac8c1bcb9aaa130100500080000013c478741ed0200093a805100ea710004d3776864f8300000000d0a0b08e2fef9b7c2aaa130100500080000013c478914ce0200093a805100eae90004d3776f87d0810000000d0a0b08e4cfe5abcbaaa130100500080000013c478aeaac0200093a805100eb610004d37776ae796e0000000d0a0b0888f3f7a6d4aaa130100500080000013c478cbeb60200093a805100ebd90004d3777dd8f2f50000000d0a0b08f0f8cd9dddaaa130100500080000013c478e92c00200093a805100ec510004d37784fcfc770000000d0a0b08d8fea394e6aaa130100500080000013c4790689f0200093a805100ecc90004d3778c25cabb0000000d0a0b08fca1b68fefaaa130100500080000013c47923e5e0200093a805100ed410004d377934f8d260000000d0a0b089cb2a28af8aaa130100500080000013c4794115e0200093a805100edb90004d3779a72286e0000000d0a0b08a296b4fe80aba130100500080000013c4795e6040200093a805100ee310004d377a19e8a430000000d0a0b089efbc8f689aba130100500080000013c4797bb850200093a805100eea90004d377a8bf3d3d0000000d0a0b08b6e5e8f092aba130100500080000013c4799904b0200093a805100ef210004d377afe43ae60000000d0a0b08b6dda3e99baba130100500080000013c479d3a810200093a805100f0130004d377be45d0f80000000d0a0b088cb6ebdbadaba130100500080000013c479f0e6c0200093a805100f0890004d377c560cb090000000d0a0b08f0a89bd2b6aba130100500080000013c47a0e2570200093a805100f1010004d377cc83a35a0000000d0a0b08d49bcbc8bfaba130100500080000013c47a2b7c80200093a805100f17a0004d377d3b72c530000000d0a0b08aafcd7c2c8aba130100500080000013c47a48e230200093a805100f1f20004d377dadabbc10000000d0a0b08deeb82bfd1aba130100500080000013c47a6624d0200093a805100f2690004d377e1f9866e0000000d0a0b08ca84ffb5daaba130100500080000013c47a836370200093a805100f2e00004d377e91d8ff00000000d0a0b08aef7aeace3aba130100500080000013c47aa09e30200093a805100f3590004d377f045e4200000000d0a0b088ac492a2ecaba130100500080000013c47abe08d0200093a805100f3d10004d377f770209e0000000d0a0b0888e39c9ff5aba130100500080000013c47adb4970200093a805100f4490004d377fe9837c50000000d0a0b08f0e8f295feaba130100500080000013c47af897c0200093a805100f4c10004d37805be29940000000d0a0b08f4f3d38e87aca130100500080000013c47b15edd0200093a805100f5390004d3780ce3a1510000000d0a0b0888cbcd8890aca130100500080000013c47b332990200093a805100f5b10004d378140861f00000000d0a0b08a6a1c4fe98aca130100500080000013c47b505d80200093a805100f6290004d3781b32db770000000d0a0b08b4aba2f3a1aca130100500080000013c47b6db970200093a805100f6a10004d3782257d9200000000d0a0b08d4bb8eeeaaaca130100500080000013c47b8b0d90200093a805100f7190004d378298252a80000000d0a0b08e4ffe1e7b3aca130100500080000013c47ba85ed0200093a805100f7910004d37830a8076e0000000d0a0b08aea7fce0bcaca130100500080000013c47bc5a640200093a805100f8090004d37837cdf93d0000000d0a0b08e4efd7d8c5aca130100500080000013c47be2fc60200093a805100f8820004d3783efd37870000000d0a0b08f8c6d1d2ceaca130100500080000013c47c005180200093a805100f8fa0004d378462cb2da0000000d0a0b08ca94b8ccd7aca130100500080000013c47c1d8470200093a805100f9710004d3784d439f470000000d0a0b08969583c1e0aca130100500080000013c47c3adc80200093a805100f9e90004d378546b79640000000d0a0b08aeffa2bbe9aca130100500080000013c47c582bc0200093a805100fa610004d3785b94fec50000000d0a0b08f49397b4f2aca130100500080000013c47c757430200093a805100fad90004d37862bba7b20000000d0a0b08ece585acfbaca130100500080000013c47c92a340200093a805100fb520004d37869e9f1d50000000d0a0b08b0c084a084ada130100500080000013c47cb00700200093a805100fbc90004d3787107113d0000000d0a0b08e09c899c8dada130100500080000013c47ccd40c0200093a805100fc410004d378782f28640000000d0a0b08fadfd99196ada130100500080000013c47cea9eb0200093a805100fcb90004d3787f54a0200000000d0a0b089e83ec8c9fada130100500080000013c47d07dc60200093a805100fd310004d378867d315a0000000d0a0b08c0ec8883a8ada130100500080000013c47d253760200093a805100fda90004d3788da4545a0000000d0a0b089ef3e1fdb0ada130100500080000013c47d4280c0200093a805100fe210004d37894c7e3c90000000d0a0b08d8cee3f5b9ada130100500080000013c47d5fba90200093a805100fe990004d3789bef43d20000000d0a0b08f291b4ebc2ada130100500080000013c47d7d06e0200093a805100ff120004d378a320e47d0000000d0a0b08f289efe3cbada130100500080000013c47d9a68b0200093a805100ff890004d378aa4197770000000d0a0b089ed3cddfd4ada130100500080000013c47db79da0200093a80510100020004d378b16d05260000000d0a0b08eee6bed4ddada130100500080000013c47dd50830200093a805101007a0004d378b89966fb0000000d0a0b08ec85c9d1e6ada130100500080000013c47df24300200093a80510100f10004d378bfb36ce60000000d0a0b08c8d2acc7efada130100500080000013c47e0f8c60200093a805101016a0004d378c6dde66e0000000d0a0b0882aeaebff8ada130100500080000013c47e2ce570200093a80510101e10004d378ce0321200000000d0a0b08dca1e1b981aea130100500080000013c47e4a7040200093a805101025a0004d378d53af4c70000000d0a0b08dcfae0bb8aaea130100500080000013c47e677640200093a80510102d10004d378dc51a42a0000000d0a0b08ccc5bea993aea130100500080000013c47e84c390200093a80510103490004d378e376dedc0000000d0a0b088ec78ca29caea130100500080000013c47ea20ff0200093a80510103c10004d378ea9d87c90000000d0a0b088ebfc79aa5aea130100500080000013c47ebf5b40200093a80510104390004d378f1c6560d0000000d0a0b08ccadef92aeaea130100500080000013c47edcb160200093a80510104b10004d378f8f2f4ec0000000d0a0b08e084e98cb7aea130100500080000013c47ef9eb30200093a805101052a0004d379001de8870000000d0a0b08fac7b982c0aea130100500080000013c47f174530200093a80510105a10004d379073bfc160000000d0a0b0896c5fffcc8aea130100500080000013c47f349280200093a80510106190004d3790e62a5030000000d0a0b08d8c6cdf5d1aea130100500080000013c47f51d700200093a80510106910004d379158910e60000000d0a0b08c8f2efecdaaea130100500080000013c47f6f2740200093a80510107090004d3791cafb9d20000000d0a0b08d090f7e5e3aea130100500080000013c47f8c70a0200093a80510107820004d37923d8c5200000000d0a0b088aecf8ddecaea130100500080000013c48c5dbf00200093a8051013c030004d37c44fb76810000000d0a0b08d2b9fb8dd6b6a130100500080000013c49099d040200093a8051014d5d0004d37d4db8bde00000000d0a0b08a4e7abf8a0b9a130100500080000013c49cbb2c60200093a8051017f090004d38043b9bbb10000000d0a0b08e2acbbcfd4c0a1301005005672736b3a67726f75703a73656375726974793a496e74657276656e74696f6e3a486f6c643a38393039333337362d346632332d346661342d616261392d6131303030313263333063623a33313238363a313330313233000000000000260c7fffffff8000000000000000000000dd00080000013c65aceede0200093a805108a2290004d3ed2b8552c30000000d0a0b0884e596b3f6d0a330100500080000013c65ff590b0200093a805108b7440004d3ee6d8076e40000000d0a0b08b69bb2e888d4a330100500080000013c660128cf0200093a805108b7b90004d3ee74885dee0000000d0a0b089287d1d491d4a330100500080000013c6602fdc30200093a805108b8310004d3ee7bb0b21e0000000d0a0b08d89bc5cd9ad4a330100500080000013c6604d23a0200093a805108b8a90004d3ee82d6a3ee0000000d0a0b088ee4a0c5a3d4a330100500080000013c6606a7000200093a805108b9210004d3ee89fdc6ee0000000d0a0b088edcdbbdacd4a330100500080000013c66087bf40200093a805108b9990004d3ee9125a10b0000000d0a0b08d4f0cfb6b5d4a330100500080000013c660a509a0200093a805108ba110004d3ee984c0cee0000000d0a0b08d0d5e4aebed4a330100500080000013c660c264a0200093a805108ba8a0004d3ee9f7831b90000000d0a0b08aedcbda9c7d4a330100500080000013c660dfb0f0200093a805108bb020004d3eea69fcecd0000000d0a0b08aed4f8a1d0d4a330100500080000013c660fcf380200093a805108bb7a0004d3eeadc35e3b0000000d0a0b089aedf498d9d4a330100500080000013c6611a3900200093a805108bbf10004d3eeb4e898ee0000000d0a0b08cca2aa90e2d4a330100500080000013c661378560200093a805108bc690004d3eebc0f7ee40000000d0a0b08cc9ae588ebd4a330100500080000013c66154d2b0200093a805108bce10004d3eec3371bf70000000d0a0b088e9cb381f4d4a330100500080000013c661721d10200093a805108bd590004d3eeca5dc4e40000000d0a0b088a81c8f9fcd4a330100500080000013c6618f6d50200093a805108bdd20004d3eed18693280000000d0a0b08929fcff285d5a330100500080000013c661acc840200093a805108be4a0004d3eed8b200d60000000d0a0b08f0a5a8ed8ed5a330100500080000013c661ca06f0200093a805108bec10004d3eedfd45f140000000d0a0b08d498d8e397d5a330100500080000013c661e75540200093a805108bf3a0004d3eee6fc39320000000d0a0b08d8a3b9dca0d5a330100500080000013c662049ac0200093a805108bfb10004d3eeee222b010000000d0a0b088ad9eed3a9d5a330100500080000013c66221e900200093a805108c0290004d3eef548d3ee0000000d0a0b088ee4cfccb2d5a330100500080000013c6623f3e20200093a805108c0a20004d3eefc7296580000000d0a0b08e0b1b6c6bbd5a330100500080000013c6625c82b0200093a805108c11a0004d3ef03980e140000000d0a0b08d0ddd8bdc4d5a330100500080000013c66279d000200093a805108c1920004d3ef0abf6e1e0000000d0a0b0892dfa6b6cdd5a330100500080000013c662971d50200093a805108c20a0004d3ef11e654140000000d0a0b08d4e0f4aed6d5a330100500080000013c662b467b0200093a805108c2820004d3ef190d3a0b0000000d0a0b08d0c589a7dfd5a330100500080000013c662d1b020200093a805108c2f90004d3ef2033a5ee0000000d0a0b08c897f89ee8d5a330100500080000013c662eefe60200093a805108c3710004d3ef275ac8ee0000000d0a0b08cca2d997f1d5a330100500080000013c6630c4bb0200093a805108c3e90004d3ef2e81ebee0000000d0a0b088ea4a790fad5a330100500080000013c663299a00200093a805108c4620004d3ef35aa03140000000d0a0b0892af888983d6a330100500080000013c66346e560200093a805108c4da0004d3ef3cd0ac010000000d0a0b08d09db0818cd6a330100500080000013c663644250200093a805108c5520004d3ef43fd87ea0000000d0a0b08b2b7affc94d6a330100500080000013c6638182f0200093a805108c5ca0004d3ef4b20da4f0000000d0a0b089abd85f39dd6a330100500080000013c6639ecc50200093a805108c6420004d3ef524746320000000d0a0b08d49887eba6d6a330100500080000013c663bc16b0200093a805108c6ba0004d3ef596def1e0000000d0a0b08d0fd9be3afd6a330100500080000013c663d96cd0200093a805108c7320004d3ef6097b1890000000d0a0b08e4d495ddb8d6a330100500080000013c663f6b250200093a805108c7aa0004d3ef67bd664f0000000d0a0b08968acbd4c1d6a330100500080000013c66413f5e0200093a805108c8220004d3ef6ee1acda0000000d0a0b08c4acdacbcad6a330100500080000013c6643151d0200093a805108c89a0004d3ef760e88c30000000d0a0b08e4bcc6c6d3d6a330100500080000013c6644ea5f0200093a805108c9120004d3ef7d3757070000000d0a0b08f4809ac0dcd6a330100500080000013c6646be4a0200093a805108c98a0004d3ef8459b5450000000d0a0b08d8f3c9b6e5d6a330100500080000013c664893000200093a805108ca020004d3ef8b80d8450000000d0a0b0896e2f1aeeed6a330100500080000013c664a67960200093a805108ca7a0004d3ef92a744280000000d0a0b08d0bdf3a6f7d6a330100500080000013c664c3c6b0200093a805108caf20004d3ef99cded140000000d0a0b0892bfc19f80d7a330100500080000013c664e11210200093a805108cb6a0004d3efa0f510140000000d0a0b08d0ade99789d7a330100500080000013c664fe5a80200093a805108cbe20004d3efa81b7bf70000000d0a0b08c8ffd78f92d7a330100500080000013c6651bb580200093a805108cc5a0004d3efaf46ac9c0000000d0a0b08a686b18a9bd7a330100500080000013c66538fbf0200093a805108ccd20004d3efb66c61620000000d0a0b089ac5f981a4d7a330100500080000013c665564560200093a805108cd4a0004d3efbd9384620000000d0a0b08d4a0fbf9acd7a330100500080000013c6657393a0200093a805108cdc20004d3efc4ba2d4f0000000d0a0b08d8abdcf2b5d7a330100500080000013c66590de00200093a805108ce3a0004d3efcbe0d63b0000000d0a0b08d490f1eabed7a330100500080000013c665ae2c50200093a805108ceb20004d3efd3092a6c0000000d0a0b08d89bd2e3c7d7a330100500080000013c665cb75c0200093a805108cf2a0004d3efda2f1c3b0000000d0a0b0892f7d3dbd0d7a330100500080000013c665e8c8e0200093a805108cfa20004d3efe158a19c0000000d0a0b08e0b194d5d9d7a330100500080000013c666061060200093a805108d01a0004d3efe87f0d7f0000000d0a0b0896faefcce2d7a330100500080000013c6660b32e0200093a805108d02f0004d3efe9bca3f80000000d0a0b089a82ba95e4d7a330100500080000013c666235fa0200093a805108d0920004d3efefa761b00000000d0a0b08dc8ee4c5ebd7a330100500080000013c66640c750200093a805108d10b0004d3eff6d794200000000d0a0b089491b5c2f4d7a330100500080000013c6665e0210200093a805108d1820004d3effdf884240000000d0a0b08f0dd98b8fdd7a330100500080000013c6667b5830200093a805108d1fb0004d3f00526913d0000000d0a0b0884b592b286d8a330100500080000013c66698aa60200093a805108d2730004d3f00c49698f0000000d0a0b0890e6bfab8fd8a330100500080000013c666ace740200093a805108d2c50004d3f011389c4d0000000d0a0b08c29685c295d8a330100500080000013c666afdc20200093a805108d2d10004d3f011f164ab0000000d0a0b0890e5c3b596d8a330100500080000013c666b40e60200093a805108d2e20004d3f012f76d4f0000000d0a0b08a8c8b9d997d8a330100500080000013c666b99e40200093a805108d2f90004d3f0145388a10000000d0a0b088cfadbb299d8a330100500080000013c666bd5390200093a805108d3080004d3f01539241e0000000d0a0b08a49bc8c39ad8a330100500080000013c666c0e3b0200093a805108d3170004d3f016188c9f0000000d0a0b08f0d2dfce9bd8a330100500080000013c666c37110200093a805108d3210004d3f016b757db0000000d0a0b08eab0b8b29cd8a330100500080000013c666c6e6d0200093a805108d32f0004d3f0179050560000000d0a0b08c0e7ccb99dd8a330100500080000013c666c961a0200093a805108d33a0004d3f0182c3f1e0000000d0a0b08d490bb9a9ed8a330100500080000013c666cc8e20200093a805108d3470004d3f018f0f3610000000d0a0b08d4fdb8969fd8a330100500080000013c666d0aaf0200093a805108d3570004d3f019f32b6a0000000d0a0b08c08f8bb7a0d8a330100500080000013c666d34300200093a805108d3630004d3f01a9a11f00000000d0a0b0890d6b59ca1d8a330100500080000013c666d5f670200093a805108d36d0004d3f01b3e1c010000000d0a0b0898a7f685a2d8a330100500080000013c666d88d90200093a805108d3780004d3f01bdf499f0000000d0a0b08a6e48deba2d8a330100500080000013c666dd6db0200093a805108d38c0004d3f01d0f85f80000000d0a0b08a2e5c6a9a4d8a330100500080000013c666e11640200093a805108d39b0004d3f01df42d4e0000000d0a0b08e08abbb8a5d8a330100500080000013c666e5b100200093a805108d3ad0004d3f01f13f60a0000000d0a0b088ee8a9eca6d8a330100500080000013c666e90790200093a805108d3bb0004d3f01fe641750000000d0a0b08a4eedbeea7d8a330100500080000013c666ebc6b0200093a805108d3c70004d3f02091afb40000000d0a0b08c4b181daa8d8a330100500080000013c666ee3ba0200093a805108d3d00004d3f02129cde00000000d0a0b08cca1fdb9a9d8a330100500080000013c666f42d30200093a805108d3e90004d3f0229e78190000000d0a0b08f88a93a2abd8a330100500080000013c666f6b990200093a805108d3f30004d3f0233c8c380000000d0a0b08b0dfd885acd8a330100500080000013c666fb8620200093a805108d4070004d3f024687de30000000d0a0b0884a294c1add8a330100500080000013c666fe1c40200093a805108d4110004d3f0250a25940000000d0a0b08d0d598a6aed8a330100500080000013c6670298c0200093a805108d4240004d3f026228a230000000d0a0b08808cb8d5afd8a330100500080000013c667052420200093a805108d42e0004d3f026c33dad0000000d0a0b08f6d6eab8b0d8a330100500080000013c66707ed10200093a805108d43a0004d3f02770d1420000000d0a0b08aaf9cea5b1d8a330100500080000013c6670a5e20200093a805108d4440004d3f02808755b0000000d0a0b08aac3fe84b2d8a330100500080000013c6670cfff0200093a805108d44e0004d3f028ac7f6d0000000d0a0b088ee9e7ebb2d8a330100500080000013c66710d480200093a805108d45e0004d3f0299dc9c50000000d0a0b08e6bab681b4d8a330100500080000013c667142810200093a805108d46c0004d3f02a6db2d00000000d0a0b08b6a4af83b5d8a330100500080000013c667195c30200093a805108d4810004d3f02bb13f3b0000000d0a0b08ded7d0ceb6d8a330100500080000013c6671cabe0200093a805108d48f0004d3f02c80341e0000000d0a0b08a69bfdcfb7d8a330100500080000013c6671f2b90200093a805108d4990004d3f02d1d91200000000d0a0b0884f4cab1b8d8a330100500080000013c667245db0200093a805108d4ae0004d3f02e615a950000000d0a0b08a894c6fcb9d8a330100500080000013c66727b340200093a805108d4bc0004d3f02f32b1da0000000d0a0b08fc90e5febad8a330100500080000013c6672b2520200093a805108d4cb0004d3f0300dcfac0000000d0a0b08caa1ad85bcd8a330100500080000013c6672e5a70200093a805108d4d70004d3f030d152be0000000d0a0b089ce4d682bdd8a330100500080000013c66733e670200093a805108d4ee0004d3f0322de8230000000d0a0b08f8efacdbbed8a330100500080000013c667379ab0200093a805108d4fd0004d3f0331717320000000d0a0b08ce8786ecbfd8a330100500080000013c6673afb00200093a805108d50b0004d3f033e86e760000000d0a0b08f8ecf6efc0d8a330100500080000013c6673f5740200093a805108d51d0004d3f034f70c780000000d0a0b08a6e9a09ac2d8a330100500080000013c667429370200093a805108d52a0004d3f035c1b6ad0000000d0a0b08c6eecf98c3d8a330100500080000013c667451700200093a805108d5340004d3f0365e5c920000000d0a0b08acede9fac3d8a330100500080000013c667486d90200093a805108d5430004d3f0373478980000000d0a0b08c2f39bfdc4d8a330100500080000013c6674b58a0200093a805108d54e0004d3f037e693e60000000d0a0b08fce29befc5d8a330100500080000013c6674e8a10200093a805108d55b0004d3f038af92d70000000d0a0b08c6fff8ebc6d8a330100500080000013c6675133c0200093a805108d5660004d3f039535fdf0000000d0a0b08baf1fad3c7d8a330100500080000013c667563ed0200093a805108d57b0004d3f03a907c440000000d0a0b088e95fb98c9d8a330100500080000013c6675954e0200093a805108d5870004d3f03b50e5d80000000d0a0b08a0a7c291cad8a330100500080000013c6675c8e20200093a805108d5940004d3f03c18b3990000000d0a0b08fa8fb88fcbd8a330100500080000013c667604c30200093a805108d5a40004d3f03d02d6ce0000000d0a0b08e486d0a1ccd8a330100500080000013c66765bec0200093a805108d5bb0004d3f03e5cccc90000000d0a0b088c9bb6f6cdd8a330100500080000013c66768df90200093a805108d5c70004d3f03f1b11060000000d0a0b08f495cff0ced8a330100500080000013c6676c5070200093a805108d5d50004d3f03ff1e4290000000d0a0b08809d84f7cfd8a330100500080000013c667775200200093a805108d6020004d3f042a1ff830000000d0a0b08cad9f9a4d3d8a330100500080000013c6677a2e70200093a805108d60e0004d3f043554c020000000d0a0b08a6badb94d4d8a330100500080000013c6677e38b0200093a805108d61e0004d3f04452bf490000000d0a0b08ac97c3b2d5d8a330100500080000013c667810580200093a805108d62a0004d3f0450015d50000000d0a0b08e8dff39fd6d8a330100500080000013c6678437e0200093a805108d6370004d3f045ca45f60000000d0a0b08f485e49cd7d8a330100500080000013c6678783b0200093a805108d6440004d3f04695e4520000000d0a0b08b4a3c49dd8d8a330100500080000013c6678af1a0200093a805108d6530004d3f0476dab9d0000000d0a0b08fa8dc0a3d9d8a330100500080000013c6678d9c50200093a805108d65d0004d3f04812e6df0000000d0a0b08b089d58bdad8a330100500080000013c66791aa70200093a805108d66e0004d3f04912bc870000000d0a0b08be8c89aadbd8a330100500080000013c667967be0200093a805108d6820004d3f04a3f654f0000000d0a0b08dcfea3e6dcd8a330100500080000013c66798f8a0200093a805108d68c0004d3f04ada9cf90000000d0a0b08f4bab8c7ddd8a330100500080000013c6679e1e10200093a805108d6a10004d3f04c1c040d0000000d0a0b08bedfbb90dfd8a330100500080000013c667a2c0a0200093a805108d6b40004d3f04d4279d80000000d0a0b08fc88c3c5e0d8a330100500080000013c667a66e10200093a805108d6c30004d3f04e221f630000000d0a0b0884de96d5e1d8a330100500080000013c667a8f2a0200093a805108d6cd0004d3f04ebf7c650000000d0a0b08ace6c3b7e2d8a330100500080000013c667ac88b0200093a805108d6dc0004d3f04fa0cd340000000d0a0b0884d7cdc3e3d8a330100500080000013c667b1a550200093a805108d6f10004d3f050e140200000000d0a0b08fca5a58be5d8a330100500080000013c667b49c20200093a805108d6fd0004d3f0519951610000000d0a0b08ce878affe5d8a330100500080000013c667b78350200093a805108d7090004d3f0524f7a540000000d0a0b0880d1bdf0e6d8a330100500080000013c667bb4930200093a805108d7180004d3f053399d890000000d0a0b08fa93ee83e8d8a330100500080000013c667bfe010200093a805108d72b0004d3f0545a1d620000000d0a0b08a0cb90b7e9d8a330100500080000013c667c62880200093a805108d7450004d3f055e291c00000000d0a0b08be83c8acebd8a330100500080000013c667c99d50200093a805108d7530004d3f056bad31e0000000d0a0b08d2b0c9b3ecd8a330100500080000013c667cc57a0200093a805108d75e0004d3f05765102c0000000d0a0b08a8c48f9eedd8a330100500080000013c667d0cd40200093a805108d7700004d3f0587a5b3d0000000d0a0b088ab8a9cceed8a330100500080000013c667d40680200093a805108d77e0004d3f059444e550000000d0a0b08e4a09fcaefd8a330100500080000013c667d7eaa0200093a805108d78e0004d3f05a3780fb0000000d0a0b08dc8a9fe2f0d8a330100500080000013c667daed30200093a805108d79b0004d3f05af8deb60000000d0a0b08c6dee8d7f1d8a330100500080000013c667e04560200093a805108d7b00004d3f05c4261130000000d0a0b08f8f1cba8f3d8a330100500080000013c667e4a0b0200093a805108d7c20004d3f05d53246c0000000d0a0b08e4e4e2d2f4d8a330100500080000013c667e953e0200093a805108d7d50004d3f05e7869070000000d0a0b0884b0ae8af6d8a330100500080000013c667ee89e0200093a805108d7ea0004d3f05fbed1e60000000d0a0b08b0f6f5d5f7d8a330100500080000013c667f17310200093a805108d7f60004d3f060734f950000000d0a0b08e6d2cfc7f8d8a330100500080000013c667f67a30200093a805108d80b0004d3f061ad8f860000000d0a0b08b2d0838cfad8a330100500080000013c667faf7b0200093a805108d81d0004d3f062c66e280000000d0a0b08a490b6bbfbd8a330100500080000013c6680005b0200093a805108d8320004d3f0640165360000000d0a0b08bed0ef80fdd8a330100500080000013c668028750200093a805108d83c0004d3f0649e48240000000d0a0b08a0bce3e2fdd8a330100500080000013c66808dd70200093a805108d8560004d3f0662c38620000000d0a0b08daf9a5daffd8a330100500080000013c6680c8ae0200093a805108d8650004d3f06710a2ae0000000d0a0b08e2cef9e980d9a330100500080000013c6680f79f0200093a805108d8720004d3f067c9e51f0000000d0a0b08a4e4c5dc81d9a330100500080000013c668138fe0200093a805108d8820004d3f068c6a1490000000d0a0b08c2b392fc82d9a330100500080000013c668180770200093a805108d8940004d3f069de4ebb0000000d0a0b08a8bad2aa84d9a330100500080000013c6681c34e0200093a805108d8a50004d3f06ae3262f0000000d0a0b08f6ede8cd85d9a330100500080000013c668201130200093a805108d8b50004d3f06bd470880000000d0a0b08de8bd0e486d9a330100500080000013c668258aa0200093a805108d8cc0004d3f06d2b4d040000000d0a0b08d4e2bbba88d9a330100500080000013c6682a7480200093a805108d8e00004d3f06e5f59f90000000d0a0b08e4c2b3fa89d9a330100500080000013c6682d6a50200093a805108d8ec0004d3f06f1822560000000d0a0b08f49a85ee8ad9a330100500080000013c66831cc70200093a805108d8fe0004d3f0702a53e90000000d0a0b08aed0a1998cd9a330100500080000013c668457ac0200093a805108d94f0004d3f074f8255a0000000d0a0b08bc93869a92d9a330100500080000013c66847f490200093a805108d9590004d3f075935d040000000d0a0b088eb3e1fa92d9a330100500080000013c6684f8140200093a805108d9770004d3f077694eb30000000d0a0b08c4c2d5a195d9a330100500080000013c668531e20200093a805108d9870004d3f0784cc4d90000000d0a0b08eaf5e4ae96d9a330100500080000013c6686810b0200093a805108d9dc0004d3f07d6850a40000000d0a0b08909086e19cd9a330100500080000013c6686d65f0200093a805108d9f30004d3f07eba685e0000000d0a0b08fc86b0b19ed9a330100500080000013c668768dd0200093a805108da170004d3f080f1de8c0000000d0a0b08bcbc8297a1d9a330100500080000013c6687aac90200093a805108da280004d3f081f416950000000d0a0b08ace1fab7a2d9a330100500080000013c6687ed220200093a805108da390004d3f082f7f9e20000000d0a0b08eac8f8d9a3d9a330100500080000013c668826440200093a805108da480004d3f083d66e3c0000000d0a0b08ba93b6e5a4d9a330100500080000013c66887a210200093a805108da5d0004d3f0851e084c0000000d0a0b08f6a596b2a6d9a330100500080000013c6688ab440200093a805108da6b0004d3f085e744470000000d0a0b08809291aaa7d9a330100500080000013c6688f55e0200093a805108da7d0004d3f087009cfd0000000d0a0b08fcb185dfa8d9a330100500080000013c6689a17e0200093a805108daa90004d3f0899f8d9d0000000d0a0b088484a383acd9a330100500080000013c668a06150200093a805108dac30004d3f08b29ea490000000d0a0b08e4c5edf8add9a330100500080000013c668a80480200093a805108dae30004d3f08d0bc7dc0000000d0a0b0888b098a3b0d9a330100500080000013c668ab9f60200093a805108daf10004d3f08de816df0000000d0a0b08aad081b0b1d9a330100500080000013c668bd5aa0200093a805108db390004d3f0923ae0c30000000d0a0b08fa94d3e4b6d9a330100500080000013c668c54810200093a805108db5b0004d3f0942f94550000000d0a0b08b6d2a79ab9d9a330100500080000013c668e29f20200093a805108dbd30004d3f09b5993c90000000d0a0b088cb3b494c2d9a330100500080000013c668ffe4a0200093a805108dc4b0004d3f0a27d9d4b0000000d0a0b08bee8e98bcbd9a330100500080000013c6691d2c10200093a805108dcc30004d3f0a9a483410000000d0a0b08f4b0c583d4d9a330100500080000013c6693a8420200093a805108dd3b0004d3f0b0cefcc90000000d0a0b088c9be5fddcd9a330100500080000013c66957d360200093a805108ddb30004d3f0b7f6d6e60000000d0a0b08d2afd9f6e5d9a330100500080000013c6697518e0200093a805108de2b0004d3f0bf1b5a7b0000000d0a0b0884e58eeeeed9a330100500080000013c669925690200093a805108dea20004d3f0c63f26f40000000d0a0b08a6ceabe4f7d9a330100500080000013c669afacb0200093a805108df1b0004d3f0cd68e95e0000000d0a0b08baa5a5de80daa330100500080000013c669cd0b90200093a805108df930004d3f0d495883d0000000d0a0b08a0d2cad989daa330100500080000013c669e71fa0200093a805108dffd0004d3f0daedb65a0000000d0a0b0888eba2d491daa330100500080000013c669ea60b0200093a805108e00b0004d3f0dbbe938b0000000d0a0b08f29fb1d392daa330100500080000013c669eff380200093a805108e0210004d3f0dd1532fe0000000d0a0b089cee8cad94daa330100500080000013c669f5ff70200093a805108e03a0004d3f0de8f1c0c0000000d0a0b08bed8a59996daa330100500080000013c669f8bf90200093a805108e0450004d3f0df3b7e710000000d0a0b08a0a5de8497daa330100500080000013c66a078ec0200093a805108e0830004d3f0e2dd212e0000000d0a0b08f4f09cc79bdaa330100500080000013c66a24e1f0200093a805108e0fb0004d3f0ea0cd98b0000000d0a0b08c2abddc0a4daa330100500080000013c66a3e8b90200093a805108e1630004d3f0f047f1080000000d0a0b0890b796abacdaa330100500080000013c66a420440200093a805108e1720004d3f0f11fb8520000000d0a0b08ac8ae4b2addaa330100500080000013c66a447f10200093a805108e17c0004d3f0f1baeffd0000000d0a0b08c0b3d293aedaa330100500080000013c66a482c80200093a805108e18b0004d3f0f2a04e710000000d0a0b08c888a6a3afdaa330100500080000013c66a4aa750200093a805108e1950004d3f0f33a91f40000000d0a0b08dcb19484b0daa330100500080000013c66a4d5bc0200093a805108e1a00004d3f0f3e360c80000000d0a0b08a68ce8edb0daa330100500080000013c66a50b050200093a805108e1ae0004d3f0f4b532200000000d0a0b08b8fff3efb1daa330100500080000013c66a533ac0200093a805108e1b80004d3f0f553c0530000000d0a0b08ecc093d3b2daa330100500080000013c66a5f8360200093a805108e1eb0004d3f0f85880200000000d0a0b0894f1feb2b6daa330100500080000013c66a7cd2b0200093a805108e2630004d3f0ff7fe02a0000000d0a0b08da85f3abbfdaa330100500080000013c66a9a1e00200093a805108e2db0004d3f106a64c0d0000000d0a0b0898f49aa4c8daa330100500080000013c66ab76190200093a805108e3530004d3f10dcd32030000000d0a0b08c696aa9bd1daa330100500080000013c66ad4a420200093a805108e3cb0004d3f114f0475e0000000d0a0b08b2afa692dadaa330100500080000013c66af1fd30200093a805108e4430004d3f11c1bb50d0000000d0a0b088ca3d98ce3daa330100500080000013c66b0f3fc0200093a805108e4bb0004d3f1233fbe8f0000000d0a0b08f8bbd583ecdaa330100500080000013c66b2ca290200093a805108e5340004d3f12a719c430000000d0a0b08e68ec7fff4daa330100500080000013c66b49cdb0200093a805108e5ab0004d3f1318d4d720000000d0a0b08a2c3f9f2fddaa330100500080000013c66b673170200093a805108e6230004d3f138b8410d0000000d0a0b08d29ffeee86dba330100500080000013c66b846f20200093a805108e69b0004d3f13fdc4a8f0000000d0a0b08f4889be58fdba330100500080000013c66ba1b890200093a805108e7130004d3f1470185410000000d0a0b08aee49cdd98dba330100500080000013c66bbf06d0200093a805108e78b0004d3f14e2a53850000000d0a0b08b2effdd5a1dba330100500080000013c66bdc5b00200093a805108e8030004d3f15552a7b60000000d0a0b08c2b3d1cfaadba3301005005672736b3a67726f75703a73656375726974793a496e74657276656e74696f6e3a486f6c643a66306135386333352d353262312d343361642d396430332d6130636630306330306565633a31363530323a3133303130340000000000000ab77fffffff80000000000000000000006500080000013c034a2357010004d26cd99a0d390000000450ef725700080000013c034a6f26010004d26cdac26b520000000450ef726b00080000013c0494dc19010004d271e57bc50d0000000450efc70200080000013c04953c1d010004d271e6f30eb10000000450efc71a00080000013c04960d0b010004d271ea22f65a0000000450efc75000080000013c0496e52e010004d271ed6f3d860000000450efc78700080000013c04970d96010004d271ee0d149b0000000450efc79100080000013c0497c34c010004d271f0d2e2680000000450efc7c000080000013c0498bb0d010004d271f49aab340000000450efc7ff00080000013c0498f73c010004d271f585c2910000000450efc80f00080000013c04993b89010004d271f690cd010000000450efc82000080000013c04997835010004d271f77e83c80000000450efc83000080000013c0499b5ac010004d271f86da8c90000000450efc83f00080000013c049a1a62010004d271f9f7114e0000000450efc85900080000013c049b1f33010004d271fdf1e0360000000450efc89c00080000013c049b859f010004d271ff8232d40000000450efc8b600080000013c049bb105010004d272002b7bbb0000000450efc8c100080000013c049d1a9b010004d27205aff0320000000450efc91e00080000013c049d71d4010004d2720704a7570000000450efc93400080000013c049e5a04010004d2720a8fe0840000000450efc97000080000013c049ece4b010004d2720c55d8a80000000450efc98d00080000013c04a5edec010004d272282965b70000000450efcb6000080000013c04aae76b010004d2723b97fc440000000450efcca600080000013c04af53ea010004d2724ce0475d0000000450efcdc800080000013c04b02bde010004d272502b5d590000000450efce0000080000013c04b34424010004d2725c435e5b0000000450efceca00080000013c04b5154f010004d272635b3eee0000000450efcf4100080000013c04b5ad1c010004d27265ac382a0000000450efcf6800080000013c04be31d0010004d27286f2d1ec0000000450efd19700080000013c04c446a5010004d2729eb3f5520000000450efd32500080000013c04c7dc3a010004d272acb4be020000000450efd41000080000013c04c8d255010004d272b0759cb50000000450efd44f00080000013c04c8fe09010004d272b12090e00000000450efd45a00080000013c04cf9389010004d272cad850260000000450efd60a00080000013c04d05eda010004d272cdf2855c0000000450efd63e00080000013c04e0613e010004d2730c7bda870000000450efda5700080000013c04e1855e010004d27310f0f9df0000000450efdaa200080000013c04e1d7d5010004d273123318100000000450efdab700080000013c04e4b293010004d2731d5991b00000000450efdb7200080000013c04f31a9b010004d273559ff11e0000000450efdf2200080000013c04f3adc5010004d27357decb780000000450efdf4800080000013c04f4e4c2010004d2735c9d97860000000450efdf9700080000013c04f6b1d6010004d27363a6ecc90000000450efe00d00080000013c04f9f236010004d273705a25770000000450efe0e200080000013c04fe0819010004d273804fa1b80000000450efe1ee00080000013c0503a480010004d273963a96660000000450efe35e00080000013c050a96f6010004d273b15db36b0000000450efe52500080000013c051a627a010004d273ef10ec8f0000000450efe93000080000013c051c4980010004d273f67f1cda0000000450efe9ad00080000013c0521412a010004d27409e6c94d0000000450efeaf300080000013c0522cc04010004d2740fed2f6a0000000450efeb5800080000013c052c202f010004d274345df7ed0000000450efedbb00080000013c052e9d8b010004d2743e17688a0000000450efee5e00080000013c053088a7010004d27445960c720000000450efeedc00080000013c05312a19010004d274480c74a10000000450efef0500080000013c05320ac7010004d2744b7a1d1b0000000450efef3f00080000013c05323a92010004d2744c34cdc60000000450efef4b00080000013c05329eeb010004d2744dbcc8110000000450efef6500080000013c0535cc30010004d2745a259ceb0000000450eff03500080000013c05428b75010004d2748bf0c1aa0000000450eff37800080000013c0554bcf4010004d274d30219e90000000450eff82100080000013c05715c20010004d27542cfdd6a0000000450efff7400080000013c05854769010004d275909ef5e90000000450f0048e00080000013c058c989b010004d275ad341dbc0000000450f0066d00080000013c058d805d010004d275b0bd6e9b0000000450f006a900080000013c05999cd3010004d275e00c99550000000450f009c200080000013c0599e065010004d275e1148e2b0000000450f009d400080000013c059a5625010004d275e2e0c0150000000450f009f200080000013c059ae22d010004d275e50380070000000450f00a1600080000013c059ce3a6010004d275ecd943010000000450f00a9900080000013c059d33cb010004d275ee12534b0000000450f00aae00080000013c05a8b390010004d2761afd6bfb0000000450f00d9f00080000013c05a8fd2c010004d2761c1cf7ad0000000450f00db200080000013c05b6303b010004d2764faca6570000000450f0111300080000013c05b7abc4010004d276557770fb0000000450f0117400080000013c05bea733010004d27670bd207e0000000450f0133e00080000013c05bede8f010004d276719561dc0000000450f0134c00080000013c05bf05fe010004d276722f68570000000450f0135600080000013c05c34dce010004d27682e7ed970000000450f0146f00080000013c05db8c61010004d276e19c5e750000000450f01aa300080000013c05e6e7ec010004d2770df9f2610000000450f01d8c00080000013c05f142ad010004d277366cb0ee0000000450f0203200080000013c05f4e4d5010004d277449e1d590000000450f0212100080000013c05f68386010004d2774af1c3bd0000000450f0218b00080000013c05f87a6a010004d277529e2eeb0000000450f0220b00080000013c05f8fb44010004d277549581e80000000450f0222c00080000013c05f9ba7f010004d2775780819e0000000450f0225d00080000013c060a32c0010004d27797d63ef30000000450f0269500080000013c060b7248010004d2779cb66c4e0000000450f026e700080000013c060d70be010004d277a48068300000000450f0276900080000013c060dab18010004d277a56458690000000450f0277800080000013c060ff1f2010004d277ae491b3a0000000450f0280d00080000013c061526a6010004d277c29ef8880000000450f0296300080000013c061ad07b010004d277d8be9e960000000450f02ad600080000013c061b90df010004d277dbae26040000000450f02b0700080000013c061da83c010004d277e3d92c510000000450f02b9000080000013c061de95d010004d277e4d80dd20000000450f02ba100080000013c06227ae9010004d277f6b01ec50000000450f02ccc00080000013c06967813010004d279bbc50a750000000450f04a7e00080000013c069714c2010004d279be2916b90000000450f04aa600080000013c0697565f010004d279bf2966750000000450f04ab6005a72736b3a67726f75703a73656375726974793a496e74657276656e74696f6e3a496e7472616461793a66383530333165302d333736662d343031632d396536322d6130666130306664646161393a32313531383a31333031303700000000000016877fffffff8000000000000000000000d500080000013c139b9d3c010004d2ac97dfdfae0000000450f39fc700080000013c13b3710e010004d2acf501217a0000000450f3a5e200080000013c13b4790c010004d2acf8f98e010000000450f3a62400080000013c13b4d083010004d2acfa4efc430000000450f3a63a00080000013c13b543d0010004d2acfc1400410000000450f3a65800080000013c13b7171e010004d2ad033439280000000450f3a6d000080000013c13b8c9e4010004d2ad09d51fd30000000450f3a73f00080000013c13b93eb8010004d2ad0b9db7620000000450f3a75c00080000013c13bac05b010004d2ad11814df80000000450f3a7bf00080000013c13bb0c1b010004d2ad12a786b90000000450f3a7d300080000013c13bc9501010004d2ad18a7f6e40000000450f3a83700080000013c13be69c7010004d2ad1fcfd1010000000450f3a8b000080000013c13c03eca010004d2ad26f7310b0000000450f3a92700080000013c13c21342010004d2ad2e1d5fe40000000450f3a99f00080000013c13c3e836010004d2ad3544bfee0000000450f3aa1700080000013c13c5bcfb010004d2ad3c6be2ee0000000450f3aa8f00080000013c13c791a1010004d2ad439380010000000450f3ab0800080000013c13c7c3fd010004d2ad44560eec0000000450f3ab1400080000013c13c96686010004d2ad4aba28ee0000000450f3ab7f00080000013c13cb3b2c010004d2ad51e188f80000000450f3abf800080000013c13cca5ad010004d2ad5767e5bc0000000450f3ac5400080000013c13cd0fc3010004d2ad5907b7d10000000450f3ac6f00080000013c13cd92bf010004d2ad5b05b7dd0000000450f3ac9100080000013c13ce131c010004d2ad5cfae5820000000450f3acb200080000013c13cee4d6010004d2ad60300c010000000450f3ace800080000013c13d040b1010004d2ad657e23d40000000450f3ad4000080000013c13d0ba47010004d2ad675c30cd0000000450f3ad6000080000013c13d140fd010004d2ad69661cbe0000000450f3ad8200080000013c13d20fc9010004d2ad6c8e63310000000450f3adb700080000013c13d28ebf010004d2ad6e7fc03b0000000450f3add800080000013c13d46317010004d2ad75a5b20b0000000450f3ae5000080000013c13d637bd010004d2ad7ccbe0e40000000450f3aec700080000013c13d80c73010004d2ad83f2c6da0000000450f3af4000080000013c13d9e186010004d2ad8b1c4c3b0000000450f3afb800080000013c13dbb62c010004d2ad924186ee0000000450f3b03000080000013c13dc2c29010004d2ad940cfaf20000000450f3b04d00080000013c13dd8b30010004d2ad996a55320000000450f3b0a800080000013c13df6005010004d2ada090fe1e0000000450f3b12000080000013c13df9bc7010004d2ada178fbfc0000000450f3b12f00080000013c13e0c98c010004d2ada613c7510000000450f3b17c00080000013c13e1345d010004d2ada7b675da0000000450f3b19800080000013c13e30971010004d2adaedf07150000000450f3b21000080000013c13e3dbb7010004d2adb212bf590000000450f3b24500080000013c13e4622f010004d2adb41fc4c90000000450f3b26800080000013c13e4ddc9010004d2adb60441c70000000450f3b28800080000013c13e6b359010004d2adbd2f35620000000450f3b30000080000013c13e887e0010004d2adc455a1450000000450f3b37800080000013c13ea5c28010004d2adcb7a24da0000000450f3b3f000080000013c13ec31f7010004d2add2a700c30000000450f3b46800080000013c13ee05b3010004d2add9c921f80000000450f3b4e000080000013c13efdb24010004d2ade0f3216c0000000450f3b55800080000013c13f1afab010004d2ade8181f150000000450f3b5d000080000013c13f38451010004d2adef3ec8010000000450f3b64800080000013c13f558d8010004d2adf666a21e0000000450f3b6c000080000013c13f65cfd010004d2adfa5bb81e0000000450f3b70200080000013c13f718cd010004d2adfd3920ab0000000450f3b73200080000013c13f81531010004d2ae011308580000000450f3b77300080000013c13f902c1010004d2ae04b5dc450000000450f3b7b000080000013c13fad728010004d2ae0bdbce150000000450f3b82800080000013c13fcac2c010004d2ae13036b280000000450f3b8a000080000013c13fe8278010004d2ae1a363d030000000450f3b91900080000013c140056e0010004d2ae21576a110000000450f3b99000080000013c1400e362010004d2ae2378f0c20000000450f3b9b400080000013c14022b67010004d2ae287e12fd0000000450f3ba0800080000013c1404008a010004d2ae2fa84f7c0000000450f3ba8100080000013c14042bf0010004d2ae304c598d0000000450f3ba8b00080000013c14046deb010004d2ae314e548d0000000450f3ba9c00080000013c1405d501010004d2ae36ce04410000000450f3baf800080000013c140663d5010004d2ae38f6ef200000000450f3bb1c00080000013c1406f876010004d2ae3b3b08500000000450f3bb4200080000013c1407ab4d010004d2ae3dfa66170000000450f3bb7100080000013c1407f71c010004d2ae3f1dc2640000000450f3bb8400080000013c1408e44e010004d2ae42bc0e990000000450f3bbc000080000013c14097fa5010004d2ae451ee9ac0000000450f3bbe900080000013c1409d632010004d2ae466d30cb0000000450f3bbfe00080000013c140ab838010004d2ae49dfdb110000000450f3bc3800080000013c140b56cc010004d2ae4c4f96300000000450f3bc6100080000013c140badf5010004d2ae4da002a70000000450f3bc7700080000013c140d2884010004d2ae536cf2a20000000450f3bcd900080000013c140dbf19010004d2ae55b3312a0000000450f3bcfe00080000013c140efdc6010004d2ae5a9452ac0000000450f3bd5100080000013c140fb87d010004d2ae5d6962e60000000450f3bd8000080000013c141035ae010004d2ae5f52679c0000000450f3bda000080000013c1410d29b010004d2ae61cf7cdb0000000450f3bdca00080000013c1411675b010004d2ae63fc38540000000450f3bdee00080000013c1412a676010004d2ae68e0ed680000000450f3be4000080000013c14138b3b010004d2ae6c585c700000000450f3be7a00080000013c14141b58010004d2ae6e8d703d0000000450f3be9f00080000013c14147c45010004d2ae700a35bf0000000450f3beb900080000013c14154862010004d2ae732376cf0000000450f3beec00080000013c1415ce5c010004d2ae752ed0fb0000000450f3bf0f00080000013c14164fc3010004d2ae772d0e110000000450f3bf3100080000013c1416ccf3010004d2ae791110fc0000000450f3bf5000080000013c14182582010004d2ae7e5aa1170000000450f3bfa900080000013c141882b6010004d2ae7fbf14bc0000000450f3bfc000080000013c1419a531010004d2ae842dc40f0000000450f3c00a00080000013c1419f9ea010004d2ae857f24ac0000000450f3c02100080000013c141b7a45010004d2ae8b5655490000000450f3c08200080000013c141bcefd010004d2ae8ca553850000000450f3c09900080000013c141c4538010004d2ae8e6f96590000000450f3c0b600080000013c141da3d2010004d2ae93ccb38f0000000450f3c11100080000013c141f7869010004d2ae9af2e2680000000450f3c18900080000013c1420913e010004d2ae9f3815230000000450f3c1d000080000013c14214d8c010004d2aea21ca4d30000000450f3c20100080000013c1421a9b7010004d2aea38024520000000450f3c21800080000013c14232203010004d2aea941a27c0000000450f3c27900080000013c1425d813010004d2aeb3d579ce0000000450f3c32a00080000013c1426cad2010004d2aeb78ef4550000000450f3c36900080000013c1428679f010004d2aebdd57da30000000450f3c3d200080000013c1428a063010004d2aebeb6ce720000000450f3c3e100080000013c142a7576010004d2aec5df9cb60000000450f3c45900080000013c142b3771010004d2aec8d1c38f0000000450f3c48a00080000013c142c30e7010004d2aecc9fbf570000000450f3c4ca00080000013c142d7ef6010004d2aed1b962d50000000450f3c51f00080000013c142e1e74010004d2aed42b80550000000450f3c54900080000013c1431c7ff010004d2aee279c6550000000450f3c63900080000013c143570fd010004d2aef0c661110000000450f3c72900080000013c143eeca6010004d2af15cce55f0000000450f3c99500080000013c14406f15010004d2af1bbc2acf0000000450f3c9f900080000013c14421c3d010004d2af223f80c70000000450f3ca6600080000013c1442437c010004d2af22de0efa0000000450f3ca7100080000013c1442b89e010004d2af24a25bdb0000000450f3ca8e00080000013c144417d4010004d2af2a01db720000000450f3cae900080000013c1446383a010004d2af324be0ac0000000450f3cb7300080000013c14476d91010004d2af37056de40000000450f3cbc300080000013c1447c140010004d2af385246c90000000450f3cbd900080000013c1448ddfe010004d2af3ca3df7c0000000450f3cc2100080000013c144996b1010004d2af3f7a5df00000000450f3cc5100080000013c144a5010010004d2af424978370000000450f3cc8000080000013c144b6b57010004d2af46a1bdfa0000000450f3ccc900080000013c1450e92a010004d2af5c14c4990000000450f3ce3100080000013c1451f7fd010004d2af6032a0130000000450f3ce7600080000013c145266b6010004d2af61e1f19e0000000450f3ce9200080000013c1452be2e010004d2af6340ac5b0000000450f3cea900080000013c14535203010004d2af6578d9a60000000450f3cece00080000013c14541edb010004d2af689ab0130000000450f3cf0300080000013c14549209010004d2af6a60a8380000000450f3cf2100080000013c1455cc04010004d2af6f2621550000000450f3cf7000080000013c145667d8010004d2af718aa7ac0000000450f3cf9900080000013c1457018a010004d2af73def7700000000450f3cfc000080000013c14583bd2010004d2af78b0d6850000000450f3d01100080000013c146163fb010004d2af9c73174b0000000450f3d26900080000013c1464ac0c010004d2afa940c72d0000000450f3d33f00080000013c1464f53b010004d2afaa5e2d880000000450f3d35200080000013c1468b655010004d2afb90fa34b0000000450f3d44900080000013c1469bbf1010004d2afbd0758b50000000450f3d48b00080000013c146a8be6010004d2afc03ad3f00000000450f3d4c100080000013c1471dfd6010004d2afdcd854170000000450f3d6a200080000013c1479333a010004d2aff9755a2a0000000450f3d88100080000013c147a909b010004d2affec597550000000450f3d8da00080000013c147b06d6010004d2b00097f5720000000450f3d8f900080000013c147eafc5010004d2b00ee39c070000000450f3d9e900080000013c1486c68a010004d2b02e79f0170000000450f3dbfa00080000013c1487da4f010004d2b032b242c50000000450f3dc4200080000013c148d569c010004d2b0481fcd850000000450f3dda900080000013c14908e31010004d2b054ac29050000000450f3de7b00080000013c1490ff7a010004d2b0566f07ac0000000450f3de9a00080000013c14b3caec010004d2b0de56ddb60000000450f3e78100080000013c14b59e0b010004d2b0e57622760000000450f3e7f900080000013c14b6c98f010004d2b0ea044ac80000000450f3e84500080000013c14b7739c010004d2b0eca116110000000450f3e87100080000013c14bb04cc010004d2b0fa8c2c4e0000000450f3e95a00080000013c14bcf140010004d2b10213dfa60000000450f3e9d900080000013c14bdb87a010004d2b10519d0a40000000450f3ea0b00080000013c14bec579010004d2b10939d1760000000450f3ea5100080000013c14c06f07010004d2b10fb323d60000000450f3eabd00080000013c14c09acb010004d2b110616e890000000450f3eac900080000013c14c618dc010004d2b125d6206c0000000450f3ec3100080000013c14c788bc010004d2b12b6f16250000000450f3ec8e00080000013c14c7ecf6010004d2b12cffe2d70000000450f3eca900080000013c14d4be3c010004d2b15f0efb620000000450f3eff100080000013c14ea7998010004d2b1b3ebf0dd0000000450f3f58000080000013c14eab78c010004d2b1b4e4254f0000000450f3f59100080000013c14ec8c04010004d2b1bc086bda0000000450f3f60900080000013c14fd08e8010004d2b1fc77322a0000000450f3fa4200080000013c14fedc56010004d2b2039582c30000000450f3fab900080000013c1500554e010004d2b2094df17d0000000450f3fb1900080000013c1500b198010004d2b20abc2bb00000000450f3fb3200080000013c1509d84a010004d2b22e8148ea0000000450f3fd8900080000013c150f5717010004d2b243fa82850000000450f3fef200080000013c1527157d010004d2b2a0ad9cd00000000450f4050400080000013c152acef6010004d2b2af432d240000000450f405f900080000013c152c78c3010004d2b2b5b871e00000000450f4066500080000013c152ca4d5010004d2b2b66cef8f0000000450f4067200080000013c15304d85010004d2b2c4b98a4b0000000450f4076200080000013c1532246d010004d2b2cbe5720d0000000450f407da00080000013c15429c88010004d2b30c44627f0000000450f40c1200080000013c1549ef72010004d2b328df4f3b0000000450f40df200080000013c155e14a8010004d2b3778d19cb0000000450f4131a00080000013c156e94b9010004d2b3b7ffedc00000000450f4175300080000013c15723ae9010004d2b3c63d83190000000450f4184200080000013c157b0735010004d2b3e89466200000000450f41a8100080000013c157b62d3010004d2b3ea016f220000000450f41a9a00080000013c15874ddd010004d2b418891f1d0000000450f41da600080000013c15883429010004d2b41c18a2f80000000450f41de300080000013c158b09f6010004d2b4271fa3980000000450f41e9b00080000013c158bde50010004d2b42a6d96070000000450f41ed300080000013c158c3a4c010004d2b42bc4ec970000000450f41ee900080000013c158d8118010004d2b430c0ff620000000450f41f3c00080000013c158db26a010004d2b4318c9dbe0000000450f41f4b00080000013c1593d818010004d2b4498606150000000450f420dc00080000013c159502b0010004d2b44e210e740000000450f4212a00080000013c1596cdd1010004d2b4551699920000000450f4219e00080000013c15a1d5ac010004d2b4803158300000000450f4247200080000013c15a36913010004d2b486541dcf0000000450f424d800080000013c15a3b89b010004d2b48790c0210000000450f424ee00080000013c15a57f37010004d2b48e86c5530000000450f4256300080000013c15aea770010004d2b4b244f8740000000450f427ba00080000013c15aeec59010004d2b4b34c32490000000450f427cb00080000013c15b07bf7010004d2b4b96aea430000000450f4283200080000013c16ef10b8010004d2b995dafaa20000000450f479c000080000013c16f2ba71010004d2b9a42aaedc0000000450f47ab000080000013c172429db010004d2ba6544b2cc0000000450f48757005872736b3a67726f75703a73656375726974793a496e74657276656e74696f6e3a496e7472616461793a34666466346263312d393231332d343361642d613166612d6130616430306134356339363a3534343a313231323237000000000000c3647fffffff80000000000000000000073c00080000013bda16c2c2010004d1cbe8e940e40000000450e4e63700080000013bdaf31480010004d1cf459571e30000000450e51ea000080000013bdaffe104010004d1cf778a50440000000450e521e500080000013bdb01b656010004d1cf7eb48cc20000000450e5225d00080000013bdb038a60010004d1cf85d7df270000000450e522d500080000013bdb0e875e010004d1cfb0c499740000000450e525a500080000013bdb105bc6010004d1cfb7e95a130000000450e5261d00080000013bdb12303d010004d1cfbf0ed1d00000000450e5269400080000013bdb17aebc010004d1cfd485e6130000000450e527fd00080000013bdb198381010004d1cfdbac51f60000000450e5287500080000013bdb1b5808010004d1cfe2d280d00000000450e528ec00080000013bdb1d2ced010004d1cfe9fa5aed0000000450e5296500080000013bdb1f01b2010004d1cff12103d90000000450e529dd00080000013bdb22ab1e010004d1cfff72a0610000000450e52acd00080000013bdb247fd3010004d1d00695b5bc0000000450e52b4400080000013bdb29fec0010004d1d01c0fa6740000000450e52cad00080000013bdb2bd375010004d1d0233612570000000450e52d2500080000013bdb2da80c010004d1d02a5b8a130000000450e52d9d00080000013bdb2f7cf1010004d1d0318327270000000450e52e1500080000013bdb33267b010004d1d03fd29e570000000450e52f0500080000013bdb3c4df8010004d1d06394650a0000000450e5315d00080000013bdb41cc58010004d1d0790991000000000450e532c500080000013bdb43a16c010004d1d08031e5310000000450e5333d00080000013bdb457631010004d1d0875e84100000000450e533b500080000013bdb474b16010004d1d08e81996b0000000450e5342d00080000013bdb491f9d010004d1d095aa2aa50000000450e534a500080000013bdb4978d9010004d1d09700ca180000000450e534bb00080000013bdb4af462010004d1d09cceae3a0000000450e5351d00080000013bdb4cb939010004d1d0a3b402c50000000450e5359000080000013bdb4e9dae010004d1d0ab1de8610000000450e5360d00080000013bdb523ce8010004d1d0b93ea4240000000450e536fa00080000013bdb541c3d010004d1d0c0929a440000000450e5377500080000013bdb55f0e3010004d1d0c7b9803a0000000450e537ed00080000013bdb57c6d1010004d1d0cee7504a0000000450e5386600080000013bdb599b1a010004d1d0d60d05100000000450e538dd00080000013bdb5a1126010004d1d0d7d4e5820000000450e538fb00080000013bdb5a8adb010004d1d0d9afd8fc0000000450e5391a00080000013bdb5b0414010004d1d0db8a52630000000450e53939000800fec67800080000013c3f2c94b4010004d356c62b12de0000000450fec6f000080000013c3f2e6a15010004d356cd517ec10000000450fec76800080000013c3f303e9c010004d356d478deca0000000450fec7e000080000013c3f3213cf010004d356dba2de3f0000000450fec85800080000013c3f33e7d9010004d356e2c724ca0000000450fec8d000080000013c3f35bd79010004d356e9f1db5c0000000450fec94800080000013c3f37920f010004d356f11b60bd0000000450fec9c100080000013c3f396639010004d356f83c50c10000000450feca3800080000013c3f3b3b1d010004d356ff6373c10000000450fecab000080000013c3f3d0ff2010004d3570689dfa30000000450fecb2800080000013c3f3ee4d7010004d3570db3a20e0000000450fecba000080000013c3f40ba77010004d35714dbb9350000000450fecc1800080000013c3f428d77010004d3571bfdda690000000450fecc9000080000013c3f4462aa010004d3572325b4860000000450fecd0800080000013c3f46377f010004d3572a4dcbad0000000450fecd8000080000013c3f480bd7010004d3573174eead0000000450fecdf800080000013c3f49e158010004d357389bd4a30000000450fece7000080000013c3f4bb5ee010004d3573fc2f7a30000000450fecee800080000013c3f4d8a56010004d35746ea94b70000000450fecf6000080000013c3f4f5eec010004d3574e10497d0000000450fecfd800080000013c3f51347d010004d357553917c10000000450fed05000080000013c3f530896010004d3575c5e8f7d0000000450fed0c800080000013c3f54dcfe010004d3576384814c0000000450fed14000080000013c3f56b27f010004d3576aacd57d0000000450fed1b800080000013c3f588679010004d35771d28a420000000450fed23000080000013c3f5a5bac010004d35778f9ea4c0000000450fed2a800080000013c3f5c3004010004d35780210d4c0000000450fed32000080000013c3f5e0517010004d3578749db900000000450fed39800080000013c3f5fda3b010004d3578e6f534c0000000450fed41000080000013c3f61aea2010004d3579596764c0000000450fed48800080000013c3f638358010004d3579cbd5c420000000450fed50000080000013c3f6558aa010004d357a3e536600000000450fed57800080000013c3f672d12010004d357ab0c96690000000450fed5f000080000013c3f69016a010004d357b2324b2f0000000450fed66800080000013c3f6ad719010004d357b95b567d0000000450fed6e000080000013c3f6cac4c010004d357c0864a180000000450fed75900080000013c3f6e8094010004d357c7a922690000000450fed7d000080000013c3f70547f010004d357cecf8e4c0000000450fed84800080000013c3f7229c1010004d357d5f5fa2f0000000450fed8c000080000013c3f73fdbc010004d357dd20b0c10000000450fed93800080000013c3f75d2a0010004d357e4434c080000000450fed9b000080000013c3f77a7a4010004d357eb6b26250000000450feda2800080000013c3f797c5a010004d357f291cf120000000450fedaa000080000013c3f7b520a010004d357f9bbce860000000450fedb1800080000013c3f7d2652010004d35800e32e900000000450fedb9000080000013c3f7efa6c010004d3580807751c0000000450fedc0800080000013c3f80cf60010004d3580f31b19a0000000450fedc8000080000013c3f82a493010004d3581658d49a0000000450fedcf800080000013c3f84789c010004d3581d7ca1120000000450fedd7000080000013c3f864e0e010004d35824a84bca0000000450fedde800080000013c3f8821d9010004d3582bcd49730000000450fede6000080000013c3f89f6ae010004d35832f09bd80000000450feded800080000013c3f8bcb83010004d3583a1838eb0000000450fedf5000080000013c3f8da0a6010004d358414050120000000450fedfc800080000013c3f8f77ec010004d358487176a90000000450fee04100080000013c3f914b6a010004d3584f9229a30000000450fee0b800080000013c3f93232d010004d35856c6e3cc0000000450fee13200080000013c3f94f419010004d3585ddf7b7d0000000450fee1a900080000013c3f96c833010004d3586503ff120000000450fee22000080000013c3f989d18010004d3586c2d0a600000000450fee29800080000013c3f9a72a8010004d358735615ad0000000450fee31000080000013c3f9c46b2010004d3587a79e2250000000450fee38800080000013c3f9e1cee010004d35881a95d790000000450fee40100080000013c3f9ff0e9010004d35888c8a2390000000450fee47800080000013c3fa1c5ae010004d3588ff1ea900000000450fee4f100080000013c3fa39a06010004d358971725420000000450fee56800080000013c3fa56e4e010004d3589e3cda080000000450fee5e100080000013c3fa74381010004d358a562cbd80000000450fee65800080000013c3fa917ba010004d358ac8937bb0000000450fee6d000080000013c3faaeccd010004d358b3b14ee10000000450fee74800080000013c3facc1e1010004d358bad8aeeb0000000450fee7c000080000013c3fae9704010004d358c201031c0000000450fee83800080000013c3fb06afe010004d358c9263dce0000000450fee8b000080000013c3fb23fe3010004d358d04ecf080000000450fee92800080000013c3fb4140c010004d358d77409bb0000000450fee9a000080000013c3fb5e8e1010004d358de9b2cbb0000000450feea1800080000013c3fb7bdf5010004d358e5c6d7730000000450feea9000080000013c3fb992ba010004d358ece972bb0000000450feeb0800080000013c3fbb6712010004d358f4101ba70000000450feeb8000080000013c3fbd3cb2010004d358fb38ace10000000450feebf800080000013c3fbf10fa010004d359025e9eb10000000450feec7000080000013c3fc0e62d010004d3590986b5d80000000450feece800080000013c3fc2ba85010004d35910ace4b10000000450feed6000080000013c3fc48f6a010004d35917d62d080000000450feedd800080000013c3fc663d1010004d3591efab09e0000000450feee5000080000013c3fc838a6010004d3592621d39e0000000450feeec800080000013c3fca0cef010004d3592d4802770000000450feef4000080000013c3fcbe1c4010004d359346ee86d0000000450feefb800080000013c3fcdb6a8010004d3593b9ad02f0000000450fef03000080000013c3fcf8c29010004d35942bf53c40000000450fef0a800080000013c3fd160cf010004d35949e5fcb10000000450fef12000080000013c3fd33527010004d359510c2b8a0000000450fef19800080000013c3fd509ed010004d3595832d4770000000450fef21000080000013c3fd6de35010004d3595f597d630000000450fef28800080000013c3fd8b3b6010004d35966811a770000000450fef30000080000013c3fda87c0010004d3596da7865a0000000450fef37800080000013c3fdc5d12010004d35974d2f4080000000450fef3f000080000013c3fde333f010004d3597bfbc24c0000000450fef46800080000013c3fe006ac010004d359831f14b10000000450fef4e000080000013c3fe1db52010004d3598a4580940000000450fef55800080000013c3fe3b095010004d359916d1da70000000450fef5d000080000013c3fe58460010004d35998930f770000000450fef64800080000013c3fe758f7010004d3599fb8c43d0000000450fef6c000080000013c3fe92e97010004d359a6e2499e0000000450fef73800080000013c3feb02d0010004d359ae08f28a0000000450fef7b000080000013c3fecd776010004d359b52f9b770000000450fef82800080000013c3feead06010004d359bc56fb800000000450fef8a000080000013c3ff081ac010004d359c37d67630000000450fef91800080000013c3ff255f5010004d359caa504770000000450fef99000080000013c3ff42aaa010004d359d1cbea6d0000000450fefa0800080000013c3ff5ff12010004d359d8f2935a0000000450fefa8000080000013c3ff7d4e1010004d359e01bdbb10000000450fefaf800080000013c3ff9a8bc010004d359e74116630000000450fefb7000080000013c3ffb7d72010004d359ee67fc5a0000000450fefbe800080000013c3ffd5285010004d359f58ea5460000000450fefc6000080000013c3fff26ed010004d359fcb605500000000450fefcd800080000013c4000fc2f010004d35a03dddf6d0000000450fefd5000080000013c4002d1cf010004d35a0b07dee10000000450fefdc800080000013c4004a731010004d35a1231de560000000450fefe4100080000013c40067a12010004d35a194f3ac80000000450fefeb800080000013c40084e89010004d35a2074ef8e0000000450feff3000080000013c400a22f1010004d35a279b1e670000000450feffa800080000013c400bf787010004d35a2ec14d400000000450ff002000080000013c400dccba010004d35a35ea1b840000000450ff009800080000013c400fa112010004d35a3d11f5a10000000450ff011000080000013c40117700010004d35a443bb80c0000000450ff018800080000013c40134aad010004d35a4b61e6e50000000450ff020000080000013c401e478b010004d35a7649dc710000000450ff04d000080000013c4021f099010004d35a8496f1400000000450ff05c000080000013c403b93db010004d35ae8bd9e180000000450ff0c5300080000013c403d6777010004d35aefddd6fe0000000450ff0cce00080000013c403f3c4c010004d35af7062b2f0000000450ff0d4300080000013c404110c3010004d35afe2bdff50000000450ff0dc000080000013c4042e579010004d35b055288e10000000450ff0e3600080000013c4044ba9c010004d35b0c7aa0080000000450ff0eac00080000013c40468f14010004d35b13a148f50000000450ff0f2300080000013c404863c9010004d35b1ac86bf50000000450ff0f9800080000013c404a3812010004d35b21eb81500000000450ff100f00080000013c404c0cc7010004d35b2912e1590000000450ff108700080000013c406031c0010004d35b77c3c4ba0000000450ff15b4005a72736b3a67726f75703a73656375726974793a496e74657276656e74696f6e3a496e7472616461793a62326535306162322d323036612d343534632d616636322d6130353930306332636430363a33313238303a31323132323700000000000000467fffffff80000000000000000000000200080000013bdb507300010004d1d0b24454440000000450e5368500080000013bdb61dcd8010004d1d0f646ac140000000450e53afa005572736b3a67726f75703a73656375726974793a496e74657276656e74696f6e3a486f6c643a63623235383637332d333261632d343864642d396631352d6131303330303931613531353a363631393a3133303131310000000000002cc87fffffff8000000000000000000001a800080000013c27a9f6d4010004d2faeffc64220000000450f8c22d00080000013c28ffff83010004d30027fe1acc0000000450f919bb00080000013c297eaa04010004d30216d66e8a0000000450f93a2a00080000013c2982537f010004d302251c99400000000450f93b1900080000013c29a1745c010004d3029eb5722d0000000450f9431100080000013c29a1ee50010004d302a08c58020000000450f9432f00080000013c29a34875010004d302a5d9b8b80000000450f9438900080000013c29a95480010004d302bd730acb0000000450f9451400080000013c29aa9d02010004d302c27c77b50000000450f9456900080000013c29ac48d2010004d302c8ff16900000000450f945d600080000013c29ac70fc010004d302c9a1b2670000000450f945e200080000013c29ae4573010004d302d0c72a230000000450f9465900080000013c29af5ba9010004d302d501651f0000000450f946a000080000013c29b01a67010004d302d7eec7360000000450f946d100080000013c29b3c913010004d302e6546aed0000000450f947c300080000013c29b598f6010004d302ed65db7a0000000450f9483a00080000013c29b6cd34010004d302f21288a70000000450f9488700080000013c29b76cc1010004d302f489e4fc0000000450f948b200080000013c29b94233010004d302fbb1fc230000000450f9492900080000013c29bb1811010004d30302dde3e50000000450f949a200080000013c29bcec5a010004d3030a0398ab0000000450f94a1a00080000013c29bdce31010004d3030d73a3860000000450f94a5300080000013c29bdfe88010004d3030e2b77bc0000000450f94a5f00080000013c29bec092010004d303112d1e0c0000000450f94a9200080000013c29c09681010004d3031856a36d0000000450f94b0a00080000013c29c191eb010004d3031c2556520000000450f94b4a00080000013c29c2265d010004d3031e6932790000000450f94b6f00080000013c29c26b46010004d3031f81d4120000000450f94b8300080000013c29dde289010004d3038ac6710c0000000450f9528a00080000013c29ee6123010004d303cb388de30000000450f956c300080000013c29f03164010004d303d24b2fa10000000450f9573a00080000013c29f207ee010004d303d977ce800000000450f957b200080000013c29f3dc17010004d303e0a09cc40000000450f9582a00080000013c29f7862f010004d303eeed748a0000000450f9591a00080000013c29f95b32010004d303f61e5e180000000450f9599300080000013c29fa4d17010004d303f9c16f0e0000000450f959cf00080000013c29fb2dd5010004d303fd35c4970000000450f95a0a00080000013c29fc48be010004d304017ec7ed0000000450f95a5100080000013c29fd03e2010004d30404603e1f0000000450f95a8200080000013c29fd5fde010004d30405c198470000000450f95a9900080000013c29fed925010004d3040b8cdcfe0000000450f95afb00080000013c2a00aca2010004d30412b160940000000450f95b7200080000013c2a0281c5010004d30419d438e50000000450f95bea00080000013c2a045736010004d30420ff698a0000000450f95c6300080000013c2a04c10d010004d304229669380000000450f95c7c00080000013c2a05d494010004d30426c7d1cd0000000450f95cc300080000013c2a0610b3010004d30427b2e9290000000450f95cd200080000013c2a0651e3010004d30428b0997a0000000450f95ce300080000013c2a0689bd010004d304298b7a430000000450f95cf100080000013c2a075f7e010004d3042cceef080000000450f95d2800080000013c2a080258010004d3042f55cad40000000450f95d5300080000013c2a0862e7010004d30430c3c7fd0000000450f95d6a00080000013c2a096d65010004d30434d725cc0000000450f95dae00080000013c2a09d72d010004d304367b7f9a0000000450f95dcb00080000013c2a0a4682010004d3043825924f0000000450f95de600080000013c2a0a7042010004d30438c86b310000000450f95df100080000013c2a0b47c8010004d3043c1212f20000000450f95e2800080000013c2a0b7f73010004d3043ceb0b6d0000000450f95e3600080000013c2a0bada7010004d3043dae8e7f0000000450f95e4400080000013c2a0c1c03010004d3043f4e9d9d0000000450f95e5e00080000013c2a0c56da010004d3044033fc100000000450f95e6d00080000013c2a0d80f6010004d30444d6a5a50000000450f95ebc00080000013c2a0ed61a010004d30449f5c5020000000450f95f1100080000013c2a0f5454010004d3044bea3b8a0000000450f95f3300080000013c2a0faa64010004d3044d3306ca0000000450f95f4700080000013c2a111f74010004d30452e48b6a0000000450f95fa700080000013c2a1268d1010004d30457eaa1cc0000000450f95ffb00080000013c2a12fe6b010004d3045a3d464c0000000450f9602300080000013c2a142e15010004d3045ed4bb190000000450f9606f00080000013c2a14d217010004d30461605ba70000000450f9609b00080000013c2a15657f010004d304639626910000000450f960bf00080000013c2a15ca65010004d3046524cdec0000000450f960d900080000013c2a16a815010004d304688b12390000000450f9611300080000013c2a179f1a010004d3046c47a63d0000000450f9615100080000013c2a17f71f010004d3046d9efccd0000000450f9616700080000013c2a187c5e010004d3046fb366690000000450f9618b00080000013c2a19f074010004d3047555a89c0000000450f961e900080000013c2a1a4ef0010004d30476d13cef0000000450f9620200080000013c2a1a7ecb010004d3047780b8d20000000450f9620d00080000013c2a1abbc5010004d304786f63c00000000450f9621d00080000013c2a1b2147010004d30479fbe5c30000000450f9623700080000013c2a1b53d1010004d3047ac2083f0000000450f9624400080000013c2a1c258b010004d3047dfc6d940000000450f9627a00080000013c2a1cb4ad010004d304802370240000000450f9629e00080000013c2a1d4c99010004d3048274e3740000000450f962c500080000013c2a1df87b010004d304851d9aa10000000450f962f200080000013c2a247a34010004d3049e7f0a300000000450f9649b00080000013c2a254dc3010004d304a1c706ad0000000450f964d300080000013c2a25b539010004d304a34cdba00000000450f964ec00080000013c2a25ecc4010004d304a4292aa30000000450f964fa00080000013c2a2721cd010004d304a8e688770000000450f9654a00080000013c2a285fee010004d304adb8678c0000000450f9659b00080000013c2a2887f9010004d304ae54194a0000000450f965a500080000013c2a28f5e7010004d304b00e62940000000450f965c300080000013c2a292340010004d304b0b6b7540000000450f965cd00080000013c2a297ba2010004d304b20d19bd0000000450f965e300080000013c2a29cc25010004d304b346a2910000000450f965f800080000013c2a29f7d9010004d304b3f1d3c50000000450f9660300080000013c2a2ac9d1010004d304b73009b50000000450f9663b00080000013c2a2ca1a4010004d304be5f85080000000450f966b200080000013c2a2db74d010004d304c296a6860000000450f966f900080000013c2a2e73ba010004d304c57e12ab0000000450f9672a00080000013c2a2f126c010004d304c7e167d20000000450f9675200080000013c2a3047c3010004d304cca165100000000450f967a200080000013c2a30c9d6010004d304ce96cfbf0000000450f967c200080000013c2a31685a010004d304d10203260000000450f967eb00080000013c2a321d54010004d304d3cb277a0000000450f9681a00080000013c2a334dd9010004d304d86c25cb0000000450f9686700080000013c2a33799c010004d304d915abbc0000000450f9687200080000013c2a33bcd0010004d304da1b77570000000450f9688300080000013c2a33f362010004d304daf8ba800000000450f9689300080000013c2a347b60010004d304dd03d7a20000000450f968b400080000013c2a34eb32010004d304deb868030000000450f968d100080000013c2a3540e4010004d304e00911830000000450f968e700080000013c2a35c671010004d304e21d3e160000000450f9690b00080000013c2a361fbe010004d304e36e24a00000000450f9692000080000013c2a379d4a010004d304e949dcf50000000450f9698300080000013c2a37d60e010004d304ea1ec7cb0000000450f9699000080000013c2a397144010004d304f070c2eb0000000450f969fb00080000013c2a3a4ed6010004d304f3ca272c0000000450f96a3200080000013c2a3aec6f010004d304f62d02400000000450f96a5a00080000013c2a3b456d010004d304f79822f50000000450f96a7300080000013c2a3ba3da010004d304f8f940120000000450f96a8900080000013c2a3c674c010004d304fbf56a830000000450f96abb00080000013c2a3ca0eb010004d304fcd8298c0000000450f96aca00080000013c2a3d1a42010004d304febecbe10000000450f96aeb00080000013c2a3d8ed7010004d305007821040000000450f96b0700080000013c2a3dc624010004d305014f31310000000450f96b1500080000013c2a3def19010004d30501eef0940000000450f96b1f00080000013c2a3eedef010004d30505dd1c7a0000000450f96b6200080000013c2a3f2bb4010004d30506c51a580000000450f96b7100080000013c2a3f6207010004d305079af9550000000450f96b7f00080000013c2a405226010004d3050b4254fa0000000450f96bbc00080000013c2a40c3be010004d3050d05adb50000000450f96bda00080000013c2a417e26010004d3050fd6b0490000000450f96c0900080000013c2a41b11e010004d305109cd2c60000000450f96c1600080000013c2a429806010004d305142b627a0000000450f96c5200080000013c2a446c8d010004d3051b54e7db0000000450f96cca00080000013c2a44b7df010004d3051c737f670000000450f96cdc00080000013c2a44ed67010004d3051d4237410000000450f96cea00080000013c2a46254e010004d3052202eb9c0000000450f96d3a00080000013c2a474a69010004d305267c92ac0000000450f96d8500080000013c2a480e76010004d305297e76060000000450f96db700080000013c2a49e9d3010004d30530c2afa50000000450f96e3200080000013c2a4a4543010004d305321ffc270000000450f96e4800080000013c2a4aa093010004d3053384e9e00000000450f96e5f00080000013c2a4b5cc1010004d30536656bec0000000450f96e9000080000013c2a4bc05e010004d30537efc8970000000450f96eaa00080000013c2a4c2553010004d305397560810000000450f96ec300080000013c2a4d2793010004d3053d6443840000000450f96f0500080000013c2a4d9477010004d3053f181cc80000000450f96f2300080000013c2a4e3dc9010004d30541a5e2ad0000000450f96f4d00080000013c2a4e6b61010004d30542546a6a0000000450f96f5800080000013c2a4f0522010004d30544ad41e60000000450f96f7f00080000013c2a4f67f5010004d3054636aa6b0000000450f96f9900080000013c2a513d85010004d3054d6160fc0000000450f9701200080000013c2a52a845010004d30552e373120000000450f9706e00080000013c2a531299010004d305548a6c4a0000000450f9708a00080000013c2a53bb6d010004d30557160cd80000000450f970b400080000013c2a546329010004d30559a5bb0b0000000450f970df00080000013c2a54c7cf010004d3055b2db5560000000450f970f900080000013c2a551b40010004d3055c7367180000000450f9710e00080000013c2a55a476010004d3055e8b64460000000450f9713100080000013c2a55d72f010004d3055f5200d60000000450f9713e00080000013c2a56baac010004d30562d3366b0000000450f9717a00080000013c2a574f3d010004d30565111c9f0000000450f9719f00080000013c2a57a3c7010004d305665930c20000000450f971b400080000013c2a588ffe010004d30569fbc7a50000000450f971f200080000013c2a5a6495010004d305712088440000000450f9726a00080000013c2a5baaf3010004d305761981900000000450f972bd00080000013c2a5c392b010004d305784b01cc0000000450f972e200080000013c2a5ca67d010004d30579ec05110000000450f972fd00080000013c2a5d3c37010004d3057c368e470000000450f9732300080000013c2a5e0e7d010004d3057f753e4a0000000450f9735b00080000013c2a5e6b15010004d30580d4b0240000000450f9737000080000013c2a5edfca010004d305829b5f660000000450f9738e00080000013c2a5f137d010004d3058366099b0000000450f9739c00080000013c2a5fe304010004d30586962e4e0000000450f973d100080000013c2a60d602010004d3058a47ca940000000450f9740f00080000013c2a6180da010004d3058ce1f6720000000450f9743b00080000013c2a61b73d010004d3058dbb69000000000450f9744a00080000013c2a621e93010004d3058f4a4d640000000450f9746300080000013c2a62ec65010004d305926d55020000000450f9749800080000013c2a638c8f010004d30594e807df0000000450f974c200080000013c2a649eae010004d305990f6cdd0000000450f9750700080000013c2a64d176010004d30599d49b330000000450f9751400080000013c2a656193010004d3059c0f2adf0000000450f9753b00080000013c2a65afe3010004d3059d3aa2760000000450f9754d00080000013c2a664389010004d3059f793fc70000000450f9757200080000013c2a66ac76010004d305a112dee00000000450f9758d00080000013c2a6735bc010004d305a331c6270000000450f975b200080000013c2a690aef010004d305aa5dade90000000450f9762b00080000013c2a6950c2010004d305ab65dbe40000000450f9763b00080000013c2a6982b0010004d305ac2a90270000000450f9764800080000013c2a6ae0ae010004d305b18602190000000450f976a200080000013c2a6c0b18010004d305b60c89360000000450f976ed00080000013c2a6c9902010004d305b837996b0000000450f9771200080000013c2a6d2fa6010004d305ba8447f90000000450f9773800080000013c2a6d8549010004d305bbd215050000000450f9774e00080000013c2a6e308f010004d305be6ee04d0000000450f9777a00080000013c2a6e8aa6010004d305bfd4c22d0000000450f9779300080000013c2a6f5173010004d305c2d88dd40000000450f977c400080000013c2a6f798d010004d305c374b9a50000000450f977ce00080000013c2a6fa88d010004d305c42b99b50000000450f977da00080000013c2a701aa1010004d305c5e976900000000450f977f700080000013c2a704665010004d305c6942db10000000450f9780300080000013c2a707a18010004d305c75fcc0d0000000450f9781000080000013c2a718a24010004d305cb850bb40000000450f9785500080000013c2a71d576010004d305ccabfb930000000450f9786900080000013c2a720188010004d305cd5769d10000000450f9787400080000013c2a72324d010004d305ce1e80740000000450f9788300080000013c2a72f07e010004d305d0fc63150000000450f978b100080000013c2a73ec08010004d305d4d4256b0000000450f978f200080000013c2a746c36010004d305d6c7a7cd0000000450f9791200080000013c2a74e60b010004d305d8a4468b0000000450f9793200080000013c2a752297010004d305d99146350000000450f9794100080000013c2a754e1c010004d305da39d7ff0000000450f9794c00080000013c2a75dd4e010004d305dc6f65df0000000450f9797200080000013c2a76daeb010004d305e0481c5c0000000450f979b200080000013c2a77aff1010004d305e39018d90000000450f979ea00080000013c2a77e104010004d305e448a42d0000000450f979f500080000013c2a7984c6010004d305eab647b20000000450f97a6200080000013c2a79a5db010004d305eb3112360000000450f97a6900080000013c2a7a18ba010004d305ecf18e7c0000000450f97a8600080000013c2a7a643b010004d305ee18f86e0000000450f97a9a00080000013c2a7aba0d010004d305ef68adc80000000450f97ab000080000013c2a7b1751010004d305f0d448900000000450f97ac700080000013c2a7b419e010004d305f179c0dc0000000450f97ad200080000013c2a7babe2010004d305f31a87170000000450f97aee00080000013c2a7c0bc6010004d305f48eb73d0000000450f97b0600080000013c2a7ce1a7010004d305f7d39a3c0000000450f97b3d00080000013c2a7d2dd4010004d305f902a5650000000450f97b5100080000013c2a7dd437010004d305fb862aa90000000450f97b7b00080000013c2a7ed658010004d305ff7601d30000000450f97bbd00080000013c2a7f0180010004d306002264380000000450f97bc900080000013c2a7f8305010004d30602197a2b0000000450f97be900080000013c2a80529c010004d3060543a8ec0000000450f97c1e00080000013c2a80d77e010004d3060750ae5b0000000450f97c4100080000013c2a8146d3010004d30608fd9d850000000450f97c5d00080000013c2a82ab2a010004d3060e7161550000000450f97cb800080000013c2a836544010004d3061145ba720000000450f97ce800080000013c2a83c94f010004d30612cb525c0000000450f97d0100080000013c2a83f590010004d30613786bde0000000450f97d0d00080000013c2a84803d010004d3061598c15f0000000450f97d3000080000013c2a8521fd010004d306180dbb540000000450f97d5900080000013c2a856456010004d30619106d700000000450f97d6a00080000013c2a858c22010004d30619ac1f2e0000000450f97d7500080000013c2a85bbce010004d3061a6618bc0000000450f97d8100080000013c2a865476010004d3061cbfa7550000000450f97da900080000013c2a867d0d010004d3061d5970c60000000450f97db200080000013c2a86ce4b010004d3061e96ca340000000450f97dc700080000013c2a87048e010004d3061f6a83da0000000450f97dd500080000013c2a87700b010004d306210f94c40000000450f97df100080000013c2a87d658010004d306229f30450000000450f97e0b00080000013c2a882a55010004d30623ea9af00000000450f97e2100080000013c2a8aa1e4010004d3062d891a460000000450f97ec200080000013c2a8bd3ef010004d3063237ecca0000000450f97f1100080000013c2a91096e010004d306468e07210000000450f9806600080000013c2a91429f010004d306476cf58f0000000450f9807400080000013c2a929419010004d3064c9379180000000450f980cb00080000013c2a93253f010004d3064ecd149d0000000450f980f000080000013c2a981fb8010004d306623cdc5a0000000450f9823600080000013c2a98a3af010004d3066443ebd70000000450f9825800080000013c2a9c2212010004d30671e649850000000450f9833d00080000013c2a9c49ee010004d3067281fb440000000450f9834700080000013c2a9d9e18010004d30677b2bf6e0000000450f9839e00080000013c2a9e1462010004d3067980d2dc0000000450f983bc00080000013c2a9e8379010004d3067b320cb50000000450f983d900080000013c2a9f4881010004d3067e34a72c0000000450f9840b00080000013c2a9fdd50010004d3068078fd650000000450f9843100080000013c2aa016b0010004d30681595a0d0000000450f9844000080000013c2aa05b0d010004d306826427730000000450f9845100080000013c2aa0d08d010004d306842f21640000000450f9847000080000013c2aa11130010004d306852bdd8e0000000450f9848000080000013c2aa1cb0d010004d3068805ef930000000450f984b000080000013c2aa2ebf1010004d3068c6b15620000000450f984fa00080000013c2aa39fe2010004d3068f2c1e6c0000000450f9852800080000013c2aa5ecf5010004d306982656a70000000450f985bf00080000013c2aa66ef8010004d3069a2325830000000450f985e000080000013c2aa7498c010004d3069d7aa1760000000450f9861800080000013c2aa79dd7010004d3069ec23b860000000450f9862e00080000013c2aa88ad9010004d306a25f19800000000450f9866a00080000013c2aa8f954010004d306a40ea8150000000450f9868600080000013c2aa96de9010004d306a5d64b7e0000000450f986a400080000013c2aa9f75e010004d306a7eec2bf0000000450f986c700080000013c2aaa80b4010004d306aa07f11d0000000450f986eb00080000013c2aaaf2f7010004d306abc9618a0000000450f9870800080000013c2aac57bb010004d306b136b5530000000450f9876300080000013c2aaca6e6010004d306b26d61b30000000450f9877700080000013c2aad25cc010004d306b45cd66f0000000450f9879800080000013c2aadacd0010004d306b66c3e400000000450f987ba00080000013c2aae9c82010004d306ba172d760000000450f987f800080000013c2ab1834d010004d306c56a16cb0000000450f988b600080000013c2ab1fb76010004d306c73f78910000000450f988d500080000013c2ab22922010004d306c7f1a2c10000000450f988e000080000013c2ab30d7d010004d306cb6de4080000000450f9891b00080000013c2ab41ad1010004d306cf8ccebc0000000450f9896000080000013c2ab53e13010004d306d3fb71880000000450f989aa00080000013c2ab5ef79010004d306d6b6d76f0000000450f989d800080000013c2ab67e90010004d306d8df596c0000000450f989fc00080000013c2ab79e74010004d306dd4371060000000450f98a4600080000013c2ab7dd54010004d306de394abe0000000450f98a5600080000013c2ab8d7ad010004d306e20b729d0000000450f98a9600080000013c2ab94f42010004d306e3ddd7f50000000450f98ab500080000013c2ab9991e010004d306e501eeb90000000450f98ac800080000013c2ab9f980010004d306e67791620000000450f98ae000080000013c2abaf8c5010004d306ea5d70660000000450f98b2200080000013c2abb6de8010004d306ec2963c30000000450f98b4000080000013c2abc07ea010004d306ee81c80e0000000450f98b6700080000013c2abd42d2010004d306f3520a020000000450f98bb800080000013c2abf5c07010004d306fb80accc0000000450f98c4100080000013c2ac00818010004d306fe2187ed0000000450f98c6e00080000013c2ac0ec03010004d307019d07860000000450f98ca800080000013c2ac38361010004d3070bba652c0000000450f98d5200080000013c2ac46eae010004d3070f50d4a20000000450f98d8e00080000013c2ac8175f010004d3071d9c41d90000000450f98e7d00080000013c2ac83f1b010004d3071e39dbe50000000450f98e8800080000013c2ac8924d010004d3071f7cb1320000000450f98e9d00080000013c2ac90cde010004d307215afb350000000450f98ebc00080000013c2ac969f3010004d30722c695fd0000000450f98ed400080000013c2ac9dbc9010004d307248435ce0000000450f98ef100080000013c2aca1373010004d307255fcdb40000000450f98f0000080000013c2aca8307010004d3072711078d0000000450f98f1c00080000013c2acab802010004d30727dffc710000000450f98f2a00080000013c2acbe839010004d3072c876ac80000000450f98f7800080000013c2acf3981010004d307397995760000000450f9905100080000013c2acf91b4010004d3073ad573be0000000450f9906800080000013c2ad546d4010004d307511d65330000000450f991de00080000013c2ad57ff6010004d30751fc90aa0000000450f991ec00080000013c2ad6e4ba010004d3075771ffbe0000000450f9924800080000013c2adc1566010004d3076bb4ca040000000450f9939c00080000013c2adc631a010004d3076ce6b1a10000000450f993b000080000013c2ae58b23010004d30790ab91d10000000450f9960800080000013c2ae5e654010004d307920c34dc0000000450f9961f00080000013c2ae744b0010004d3079765be800000000450f9967900080000013c2ae88b9b010004d3079c624b5e0000000450f996cc00080000013c2ae8bd3b010004d3079d23a9190000000450f996d900080000013c2ae8e499010004d3079dbd72890000000450f996e300080000013c2ae9347f010004d3079ef8a6a10000000450f996f800080000013c2ae98494010004d307a02f15f60000000450f9970c00080000013c2ae9b307010004d307a0e3d0af0000000450f9971800080000013c2ae9daa4010004d307a17ecb500000000450f9972200080000013c2aea15d9010004d307a2668c240000000450f9973100080000013c2aeab6de010004d307a4db0c050000000450f9975b00080000013c2aeae3da010004d307a58ac4f20000000450f9976600080000013c2aeb4b11010004d307a71df4050000000450f9978000080000013c2aecddfa010004d307ad46af970000000450f997e800080000013c2aed6f8e010004d307af7cb78b0000000450f9980d00080000013c2aed9a96010004d307b0250c4b0000000450f9981800080000013c2aeeaee7010004d307b45bb3b60000000450f9985f00080000013c2aefb231010004d307b851bddc0000000450f998a100080000013c2af088ae010004d307bb9a71760000000450f998d800080000013c2af0d97e010004d307bcd380360000000450f998ed00080000013c2af25c6a010004d307c2bc92aa0000000450f9995000080000013c2af3448a010004d307c644ef630000000450f9998b00080000013c2af3a921010004d307c7cd26b70000000450f999a500080000013c2af42316010004d307c9a9c5750000000450f999c400080000013c2af46957010004d307cabc711c0000000450f999d600080000013c2af605a6010004d307d109e4840000000450f99a4000080000013c2af656e4010004d307d24424740000000450f99a5400080000013c2af7da9b010004d307d831bea10000000450f99ab800080000013c2affad51010004d307f6bdfc670000000450f99cb800080000013c2b010247010004d307fbf4797a0000000450f99d1000080000013c2b0207e2010004d307fff03c890000000450f99d5300080000013c2b02d70c010004d308031b5f700000000450f99d8800080000013c2b060923010004d3080f942dd50000000450f99e5900080000013c2b068162010004d308116d39020000000450f99e7800080000013c2b0b3aba010004d30823de5c470000000450f99fad00080000013c2b0b6257010004d3082478dcd50000000450f99fb800080000013c2b0bfee7010004d30826df4b7a0000000450f99fe000080000013c2b12ee40010004d30841f3a0250000000450f9a1a600080000013c2b135160010004d308437b5d670000000450f9a1c000080000013c2b2223fb010004d3087d5d81440000000450f9a58b00080000013c2b23cc70010004d30883d9ed230000000450f9a5f800080000013c2b2a4e68010004d3089d459d520000000450f9a7a300080000013c2b2b1f66010004d308a07679230000000450f9a7d800080000013c2b2d97d0010004d308aa19bd3a0000000450f9a87900080000013c2b2dbf4e010004d308aab400be0000000450f9a88400080000013c2b2eab95010004d308ae4f707e0000000450f9a8c000080000013c2b305ae0010004d308b4e46b440000000450f9a92f00080000013c2b309d87010004d308b5eaedfc0000000450f9a94000080000013c2b361bf7010004d308cb62f6670000000450f9aaa800080000013c2b365f88010004d308cc6622960000000450f9aab900080000013c2b37f04f010004d308d286ffe80000000450f9ab2000080000013c2b408416010004d308f404ad6b0000000450f9ad5200080000013c2b411868010004d308f64e7f840000000450f9ad7800080000013c2b41cf08010004d308f91396330000000450f9ada700080000013c2b428683010004d308fbdedfdf0000000450f9add600080000013c2b42ed1e010004d308fd724bfc0000000450f9adf000080000013c2b45b697010004d309085232650000000450f9aea600080000013c2b4667b8010004d3090b061e5a0000000450f9aed400080000013c2b46967a010004d3090bc3ab7a0000000450f9aee000080000013c2b46d47d010004d3090caec2d60000000450f9aeef00080000013c2b473461010004d3090e2963020000000450f9af0800080000013c2b47e804010004d30910e3fc070000000450f9af3600080000013c2b486beb010004d30912e9da530000000450f9af5800080000013c2b4a027e010004d309191b684c0000000450f9afc000080000013c2b4a4081010004d3091a0f8f190000000450f9afd000080000013c2b4aa8c2010004d3091ba4e3830000000450f9afeb00080000013c2b4ae3c8010004d3091c8abc0a0000000450f9affa00080000013c2b4bf069010004d30920a395b80000000450f9b03e00080000013c2b4c3a63010004d30921c4529b0000000450f9b05100080000013c2b4c93cf010004d3092322d04e0000000450f9b06800080000013c2b4d04da010004d30924dbe8670000000450f9b08500080000013c2b4de9be010004d309285ce0f20000000450f9b0c000080000013c2b4e8709010004d3092ac036190000000450f9b0e800080000013c2b4fbe64010004d3092f82d2c20000000450f9b13800080000013c2b5080fb010004d309327944490000000450f9b16a00080000013c2b51931a010004d30936aae9e80000000450f9b1b000080000013c2b54e24f010004d3094394f94d0000000450f9b28800080000013c2b551b70010004d309447424c40000000450f9b29700080000013c2b5f3fbf010004d3096c11415e0000000450f9b53000080000013c2b5f6a98010004d3096cb959140000000450f9b53b00080000013c2b5fd181010004d3096e4bd10a0000000450f9b55500080000013c2b603974010004d3096fe47bfc0000000450f9b57000080000013c2b612df8010004d309739d7c6f0000000450f9b5ae00080000013c2b620e2a010004d309770aaad50000000450f9b5e800080000013c2b6426bf010004d3097f38caa00000000450f9b67100080000013c2b65b7a5010004d30985596ae80000000450f9b6d800080000013c2b697af1010004d309940a66980000000450f9b7ce00080000013c2b6afe1b010004d30999f190bf0000000450f9b83100080000013c2b6b3653010004d3099ad0f9400000000450f9b84000080000013c2b78077a010004d309cce1bd0f0000000450f9bb8800080000013c2ba2246a010004d30a716211740000000450f9c650005672736b3a67726f75703a73656375726974793a496e74657276656e74696f6e3a486f6c643a65623338616139342d316630372d346430302d626363312d6130663330313130646535333a31323437323a3133303131350000000000003e4a7fffffff80000000000000000000024e00080000013c3bf02deb010004d34a223361b10000000450fdf2dd00080000013c3c435c16010004d34b672d8cb80000000450fe082b00080000013c3cccaee5010004d34d7f9bdfa00000000450fe2b5300080000013c3ccd910a010004d34d830392270000000450fe2b8b00080000013c3cd97d2d010004d34db1a16ea80000000450fe2e9900080000013c3d039a3d010004d34e561708580000000450fe396000080000013c3d34aa40010004d34f15b9859b0000000450fe45ef00080000013c3d350bca010004d34f173c41100000000450fe460900080000013c3d366032010004d34f1c69349f0000000450fe465f00080000013c3d369964010004d34f1d47e6030000000450fe466e00080000013c3d36e051010004d34f1e61f5d60000000450fe468100080000013c3d383dc2010004d34f23b2700a0000000450fe46d900080000013c3d387424010004d34f2486a3c30000000450fe46e700080000013c3d38b5b2010004d34f258a0cfd0000000450fe46f800080000013c3d390087010004d34f26ab06e90000000450fe470b00080000013c3d395000010004d34f27e0fc2b0000000450fe471f00080000013c3d3978b6010004d34f288172ac0000000450fe472a00080000013c3d39d5db010004d34f29ec194d0000000450fe474200080000013c3d3a10d2010004d34f2ad22ede0000000450fe475100080000013c3d3a4909010004d34f2badc6c30000000450fe475f00080000013c3d3a78f3010004d34f2c68b4780000000450fe476b00080000013c3d3af885010004d34f2e5bbcc60000000450fe478c00080000013c3d3b6970010004d34f301497d60000000450fe47a900080000013c3d3bae4a010004d34f312110800000000450fe47bb00080000013c3d3be393010004d34f31f1739e0000000450fe47c800080000013c3d3c5f9b010004d34f33dbe68f0000000450fe47e900080000013c3d3d612f010004d34f37c496950000000450fe482a00080000013c3d3d90eb010004d34f387f0a370000000450fe483600080000013c3d3df802010004d34f3a11822c0000000450fe485100080000013c3d3e33d4010004d34f3b02cc850000000450fe486100080000013c3d3ee97a010004d34f3dc0bc120000000450fe488e00080000013c3d3f544b010004d34f3f61bf570000000450fe48aa00080000013c3d3fb845010004d34f40e84b680000000450fe48c300080000013c3d3ff714010004d34f41dda3650000000450fe48d300080000013c3d41ddac010004d34f49514f8f0000000450fe495100080000013c3d4290f1010004d34f4c06e6c80000000450fe497e00080000013c3d42c1b6010004d34f4cc5a5180000000450fe498a00080000013c3d43b2a1010004d34f50764d370000000450fe49c900080000013c3d446430010004d34f5327d6cc0000000450fe49f500080000013c3d44f3df010004d34f555957070000000450fe4a1a00080000013c3d458737010004d34f579cb91a0000000450fe4a4100080000013c3d475c3b010004d34f5ec4d0410000000450fe4ab900080000013c3d493074010004d34f65ea85070000000450fe4b3100080000013c3d497ca1010004d34f670f4f8e0000000450fe4b4300080000013c3d4b0587010004d34f6d16a9d20000000450fe4ba900080000013c3d4c1fe4010004d34f715d0dbd0000000450fe4bf000080000013c3d4cda4d010004d34f743982240000000450fe4c2100080000013c3d4eaf8f010004d34f7b63448f0000000450fe4c9900080000013c3d50539f010004d34f81c79b9a0000000450fe4d0400080000013c3d508510010004d34f82905d810000000450fe4d1100080000013c3d50cd35010004d34f83a28f140000000450fe4d2300080000013c3d52586e010004d34f89af65370000000450fe4d8900080000013c3d544436010004d34f912b2cab0000000450fe4e0600080000013c3d560285010004d34f9801f5e60000000450fe4e7900080000013c3d562abf010004d34f989774a80000000450fe4e8200080000013c3d56e3d0010004d34f9b6ad99e0000000450fe4eb200080000013c3d57d670010004d34f9f222ecd0000000450fe4ef100080000013c3d58dcf6010004d34fa32017330000000450fe4f3300080000013c3d5921a1010004d34fa42bd8c00000000450fe4f4500080000013c3d59abe1010004d34fa64b77240000000450fe4f6900080000013c3d5a29cd010004d34fa834f5ee0000000450fe4f8900080000013c3d5b24) writing into /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-tmp-ia-4882-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:209)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:226)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:166)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:66)
 INFO 15:55:37,245 CFS(Keyspace='OpsCenter', ColumnFamily='pdps') liveRatio is 2.3633228082745292 (just-counted was 2.3633228082745292).  calculation took 2ms for 1256 columns
 INFO 15:56:30,149 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4787-Data.db to /10.80.90.53
 INFO 15:56:32,401 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4878-Data.db to /10.80.90.53
{code}

{code:title=cassandra01}
 INFO 15:53:43,296 Enqueuing flush of Memtable-MyBusinessHistoryCF@709374075(7410300/48461437 serialized/live bytes, 155356 ops)
 INFO 15:53:43,304 Writing Memtable-MyBusinessHistoryCF@709374075(7410300/48461437 serialized/live bytes, 155356 ops)
 INFO 15:53:43,688 Completed flushing /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4952-Data.db (4474554 bytes) for commitlog position ReplayPosition(segmentId=1358949134910, position=13864741)
 INFO 15:54:09,160 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4922-Data.db to /10.80.90.52
 INFO 15:54:11,605 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4922-Data.db to /10.80.90.53
ERROR 15:54:46,682 Exception in thread Thread[Streaming to /10.80.90.52:2,5,main]
java.lang.RuntimeException: java.io.IOException: Broken pipe
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Broken pipe
	at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
	at sun.nio.ch.FileChannelImpl.transferToDirectly(Unknown Source)
	at sun.nio.ch.FileChannelImpl.transferTo(Unknown Source)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:90)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more
{code}",31/Jan/13 00:31;markncooper;I'm seeing what looks like the same error every time I try and add a new node to my cluster.,"07/Feb/13 20:24;mkjellman;i'm wondering if the DecoratedKey RuntimeException is a manifestation but not the real issue here.

After unrelated IOExceptions during the repair while bootstrapping a new node, it appears that the IOException/broken pipe was thrown.

maybe when we retry it starts streaming from the wrong location in the sstable after attempting to recover from the original IOException?","12/Feb/13 22:29;yukim;I found one problem that can send extra chunk to destination which causes reading from wrong position.
This happens when the streaming section of sstable falls into the edge of compression chunks.
Test and fix attached.",14/Feb/13 05:49;mkjellman;+1 patch looks good. verified as fixed.,"14/Feb/13 20:00;yukim;Committed, thanks for testing!","27/Feb/13 08:10;alprema;Switched to 1.2.2 and re-enabled compression, repair now works like a charm.
Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system.peers.tokens is empty after node restart,CASSANDRA-5121,12626262,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,pchalamet,pchalamet,06/Jan/13 21:32,12/Mar/19 14:05,13/Mar/19 22:27,15/Jan/13 15:26,1.2.1,,,,,0,,,,,,,"Using a 2 nodes fresh cluster (127.0.0.1 & 127.0.0.2) running latest 1.2, I’m querying system.peers to get the nodes of the cluster and their respective token. But it seems there is a problem after either node restart.

When both node starts up, querying system.peers seems ok:

{code}
127.0.0.1> select * from system.peers;
+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------------------------------------+
| data_center     | host_id                                  | peer          | rack      | release_version     | rpc_address     | schema_version                           | tokens                                    |
+=================+==========================================+===============+===========+=====================+=================+==========================================+===========================================+
| datacenter1     | 4819cbb0-9741-4fe0-8d7d-95941b0247bf     | 127.0.0.2     | rack1     | 1.2.0               | 127.0.0.2       | 59adb24e-f3cd-3e02-97f0-5b395827453f     | 56713727820156410577229101238628035242    |
+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------------------------------------+
{code}

But as soon as one node is restarted (let’s say 127.0.0.2), tokens column is then empty:

{code}
127.0.0.1> select * from system.peers;
+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------+
| data_center     | host_id                                  | peer          | rack      | release_version     | rpc_address     | schema_version                           | tokens      |
+=================+==========================================+===============+===========+=====================+=================+==========================================+=============+
| datacenter1     | 4819cbb0-9741-4fe0-8d7d-95941b0247bf     | 127.0.0.2     | rack1     | 1.2.0               | 127.0.0.2       | 59adb24e-f3cd-3e02-97f0-5b395827453f     |             |
+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------+
{code}

{code}
Log server side:
DEBUG 22:08:01,608 Responding: ROWS [peer(system, peers), org.apache.cassandra.db.marshal.InetAddressType][data_center(system, peers), org.apache.cassandra.db.marshal.UTF8Type][host_id(system, peers), org.apache.cassandra.db.marshal.UUIDType][rack(system, peers), org.apache.cassandra.db.marshal.UTF8Type][release_version(system, peers), org.apache.cassandra.db.marshal.UTF8Type][rpc_address(system, peers), org.apache.cassandra.db.marshal.InetAddressType][schema_version(system,
peers), org.apache.cassandra.db.marshal.UUIDType][tokens(system, peers), org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type)]
 | 127.0.0.2 | datacenter1 | 4819cbb0-9741-4fe0-8d7d-95941b0247bf | rack1 | 1.2.0 | 127.0.0.2 | 59adb24e-f3cd-3e02-97f0-5b395827453f | null
{code}

Restarting the other node (127.0.0.1) restore back the tokens column.
",Windows 8 / Java 1.6.0_37-b06,,,,,,,,,,,,,,,,,,07/Jan/13 11:27;slebresne;5121.txt;https://issues.apache.org/jira/secure/attachment/12563554/5121.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-07 11:01:32.258,,,no_permission,,,,,,,,,,,,302848,,,Sat Jan 19 19:29:37 UTC 2013,,,,,,0|i175wv:,249926,brandon.williams,brandon.williams,,,,,,,,,,"07/Jan/13 11:01;soverton;In StorageService.handleStateNormal, when we see an endpoint come up which we already knew about: 

{noformat}
else if (endpoint.equals(currentOwner))
{
    // set state back to normal, since the node may have tried to leave, but failed and is now back up
    // no need to persist, token/ip did not change
{noformat}

I think the bug is that then we call 
{noformat}
SystemTable.updateTokens(endpoint, tokensToUpdateInSystemTable);
{noformat}
with an empty collection and SystemTable.updateTokens overwrites the current entry rather than adding to it.

Fix would be
{noformat}
- // no need to persist, token/ip did not change
+ if (!isClientMode)
+    tokensToUpdateInSystemTable.add(token);
{noformat}","07/Jan/13 11:27;slebresne;I agree on Sam's analysis but I would suggest the slightly different patch attached, because I think the intend was that if tokensToUpdateInSystemTable is empty, we don't update anything. In particular, in the case where we are relocating, I don't think we want remove the tokens from the system table either (and in the case where there is a token conflict I think the initial intent was also to leave things as they are, even though in that case maybe actually removing the token is not a bad idea?).
",15/Jan/13 15:11;brandon.williams;+1,"15/Jan/13 15:26;slebresne;Committed, thanks","19/Jan/13 06:32;edong;Hi, commit ec35427fdfbc46a8adeafc042651f552b9bcc1a0 breaks RelocateTest:

{noformat}
$ ant clean build test -Dtest.name=RelocateTest
...
    [junit] Testsuite: org.apache.cassandra.service.RelocateTest
    [junit] Tests run: 2, Failures: 2, Errors: 0, Time elapsed: 6.215 sec
    [junit] 
    [junit] Testcase: testWriteEndpointsDuringRelocate(org.apache.cassandra.service.RelocateTest):	FAILED
    [junit] removeTokens should be used instead
    [junit] junit.framework.AssertionFailedError: removeTokens should be used instead
    [junit] 	at org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:324)
    [junit] 	at org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:342)
    [junit] 	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1393)
    [junit] 	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)
    [junit] 	at org.apache.cassandra.service.RelocateTest.createInitialRing(RelocateTest.java:106)
    [junit] 	at org.apache.cassandra.service.RelocateTest.testWriteEndpointsDuringRelocate(RelocateTest.java:128)
    [junit] 
    [junit] 
    [junit] Testcase: testRelocationSuccess(org.apache.cassandra.service.RelocateTest):	FAILED
    [junit] removeTokens should be used instead
    [junit] junit.framework.AssertionFailedError: removeTokens should be used instead
    [junit] 	at org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:324)
    [junit] 	at org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:342)
    [junit] 	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1393)
    [junit] 	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)
    [junit] 	at org.apache.cassandra.service.RelocateTest.createInitialRing(RelocateTest.java:106)
    [junit] 	at org.apache.cassandra.service.RelocateTest.testRelocationSuccess(RelocateTest.java:177)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.service.RelocateTest FAILED

BUILD FAILED
...
{noformat}


After commit e6b6eaa583e8fc15f03c3e27664bf7fc06b3af0a, testWriteEndpointsDuringRelocate passes but testRelocationSuccess still fails:
{noformat}
$ ant clean build test -Dtest.name=RelocateTest
...
    [junit] Testcase: testRelocationSuccess(org.apache.cassandra.service.RelocateTest):	FAILED
    [junit] removeEndpoint should be used instead
    [junit] junit.framework.AssertionFailedError: removeEndpoint should be used instead
    [junit] 	at org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:316)
    [junit] 	at org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:334)
    [junit] 	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1394)
    [junit] 	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)
    [junit] 	at org.apache.cassandra.service.RelocateTest.testRelocationSuccess(RelocateTest.java:193)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.service.RelocateTest FAILED
...
{noformat}
","19/Jan/13 17:26;brandon.williams;You should update, this was fixed in 17adf8e4f72114d336140fac5157a35e63d1f53a","19/Jan/13 19:29;edong;Updated; test now passes, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select on composite partition keys are not validated correctly,CASSANDRA-5122,12626335,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,07/Jan/13 11:32,12/Mar/19 14:05,13/Mar/19 22:27,07/Jan/13 16:23,1.2.1,,,,,0,,,,,,,"From Kais Ahmed on the mailing list:
{noformat}
----------------------------------------------------------------------------------------------------------------------
[cqlsh 2.3.0 | Cassandra 1.2.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol
19.35.0]

cqlsh:test> CREATE TABLE foo (   a int,   b text,   c uuid,   PRIMARY KEY
((a, b)) );

cqlsh:test> INSERT INTO foo (a, b , c ) VALUES (  1 , 'aze',
'4d481800-4c5f-11e1-82e0-3f484de45426');
cqlsh:test> INSERT INTO foo (a, b , c ) VALUES (  1 , 'ert',
'693f5800-8acb-11e3-82e0-3f484de45426');
cqlsh:test> INSERT INTO foo (a, b , c ) VALUES (  1 , 'opl',
'd4815800-2d8d-11e0-82e0-3f484de45426');

-----------------------------------------------------------------------------------------------------------------------------------------------------

cqlsh:test> SELECT * FROM foo;

 a | b   | c
---+-----+--------------------------------------
 1 | ert | 693f5800-8acb-11e3-82e0-3f484de45426
 1 | opl | d4815800-2d8d-11e0-82e0-3f484de45426
 1 | aze | 4d481800-4c5f-11e1-82e0-3f484de45426

-----------------------------------------------------------------------------------------------------------------------------------------------------

cqlsh:test> SELECT * FROM foo where a=1;

 a | b   | c
---+-----+--------------------------------------
 1 | ert | 693f5800-8acb-11e3-82e0-3f484de45426
 1 | opl | d4815800-2d8d-11e0-82e0-3f484de45426
{noformat}

The last request should be invalid (since we don't have a good way to do it, at least not with a random partitioner).",,,,,,,,,,,,,,,,,,,07/Jan/13 13:39;slebresne;5122.txt;https://issues.apache.org/jira/secure/attachment/12563565/5122.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-07 15:43:38.885,,,no_permission,,,,,,,,,,,,302925,,,Mon Jan 07 16:23:37 UTC 2013,,,,,,0|i176hz:,250021,jbellis,jbellis,,,,,,,,,,07/Jan/13 15:43;jbellis;+1,"07/Jan/13 16:23;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow counters in collection (CQL3),CASSANDRA-5082,12624811,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,20/Dec/12 13:10,12/Mar/19 14:05,13/Mar/19 22:27,20/Dec/12 15:13,1.2.0,,,,,0,,,,,,,We don't support counters in collections but we don't throw an error when someone tries to create such a thing. Attaching patch to return a validation error.,,,,,,,,,,,,,,,,,,,20/Dec/12 13:10;slebresne;5082.txt;https://issues.apache.org/jira/secure/attachment/12561885/5082.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-20 14:39:50.779,,,no_permission,,,,,,,,,,,,301319,,,Thu Dec 20 15:13:29 UTC 2012,,,,,,0|i16rnj:,247612,jbellis,jbellis,,,,,,,,,,20/Dec/12 14:39;jbellis;+1,"20/Dec/12 15:13;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Murmur3Partitioner#describeOwnership calculates ownership% wrong,CASSANDRA-5076,12624529,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,18/Dec/12 20:37,12/Mar/19 14:05,13/Mar/19 22:27,19/Dec/12 18:44,1.2.0 rc2,,,,,0,,,,,,,"When I issued 'nodetool status' on Murmur3-partitioned cluster I got the following output:

{code}
$ bin/nodetool -p 7100 status                                                                                                                                                                                                                                                                                       (git)-[5065]-[~/Developments/workspace/cassandra]
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address           Load       Owns   Host ID                               Token                                    Rack
UN  127.0.0.1         24.78 KB   66.7%  ace7e54c-9fe1-4b23-83b0-949772b24c30  -9223372036854775808                     rack1
UN  127.0.0.2         29.22 KB   66.7%  67146442-dbfd-449c-82e1-26729b8ac89c  -3074457345618258603                     rack1
UN  127.0.0.3         6.19 KB    66.7%  3fab9f18-daf3-4452-8b9c-204ea0ee2e15  3074457345618258602                      rack1
{code}

Notice that 'Owns' percentages add up to 200%.

I think the problem is that Murmur3Partitioner#describeOwnership currently calculate ownership% based on [0, Long.MAX_VALUE] range, but we have to consider about negative tokens.",,,,,,,,,,,,,,,,,,,19/Dec/12 18:16;yukim;5076.txt;https://issues.apache.org/jira/secure/attachment/12561755/5076.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-19 18:34:41.493,,,no_permission,,,,,,,,,,,,300350,,,Wed Dec 19 18:44:36 UTC 2012,,,,,,0|i167dz:,244329,slebresne,slebresne,,,,,,,,,,"19/Dec/12 18:16;yukim;Patch attached to calculate ownership using BigInteger/BigDecimal for range (Long.MIN_VALUE, Long.MAX_VALUE].
This will also resolve problem described in CASSANDRA-4598.","19/Dec/12 18:34;slebresne;For the last range, I think the {{ti.subtract(tim1)}} part should be changed to {{(BigIntegerToken)start).token.subtract(ti)}} as done in RandomPartitionner.describeOwnership. Cause at the end of the loop, {{ti == tim1}}.

But +1 with that fixed.",19/Dec/12 18:44;yukim;Committed with fix for review. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing from higher to lower compaction throughput causes long (multi hour) pause in large compactions,CASSANDRA-5087,12625016,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jblangston@datastax.com,jblangston@datastax.com,jblangston@datastax.com,21/Dec/12 16:17,12/Mar/19 14:05,13/Mar/19 22:27,27/Dec/12 15:15,1.1.9,1.2.1,,,,0,,,,,,,"We're running a major compaction against a column family that is 2.1TB (yes, I know it's crazy huge, that's an entirely different discussion). During the evenings, we run a setcompactionthroughput 0 to unthrottle completely, and throttle again down to 20mb at the end of the maintenance window. 

Every morning we've come in to check progress, we find that the progress completely halts as soon as the compaction throttling command is issued. Eventually, compaction continues. I was looking at the throttling code, and I think I see the issue, but would like confirmation:

throttleDelta (org.apache.cassandra.utils.Throttle.throttleDelta) sets a sleep time based on the amount of data transferred since the last throttle time. Since we've gone from 20 MB to wide open, and back to 20MB, the wait that is calculated is based on an attempt to average the new throttling rate over the last 6.5 hours of running wide open.

I think this could be fixed by adding a reset of bytesAtLastDelay and timeAtLastDelay to the current values after the check at line 64:

Current:

        // if the target changed, log
        if (newTargetBytesPerMS != targetBytesPerMS) 
            logger.debug(""{} target throughput now {} bytes/ms."", this, newTargetBytesPerMS);
        targetBytesPerMS = newTargetBytesPerMS;

New:

 
        // if the target changed, log
        if (newTargetBytesPerMS != targetBytesPerMS) {
            logger.debug(""{} target throughput now {} bytes/ms."", this, newTargetBytesPerMS);
            if(newTargetBytesPerMS < targetBytesPerMS || targetBytesPerMS < 1) {
            	bytesAtLastDelay += bytesDelta;
            	timeAtLastDelay = System.currentTimeMillis();
                targetBytesPerMS = newTargetBytesPerMS;
            	return;
            }
            targetBytesPerMS = newTargetBytesPerMS;
        }

Some redundancies that can be removed there, but I wanted to keep the approach local to where I thought the problem was. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-12-27 15:15:35.48,,,no_permission,,,,,,,,,,,,301552,,,Thu Dec 27 15:15:35 UTC 2012,,,,,,0|i16umf:,248093,jbellis,jbellis,,,,,,,,,,"27/Dec/12 15:15;jbellis;Added comments and committed.  (Note: to 1.1.9 and 1.2.1, but not 1.2.0.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disallow bloom filter false positive chance of 0,CASSANDRA-5013,12618512,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,mdennis,mdennis,02/Dec/12 21:43,12/Mar/19 14:05,13/Mar/19 22:27,07/Jan/13 18:11,1.2.1,,Local/Config,,,0,,,,,,,"{pre}
ERROR [CompactionExecutor:16] 2012-11-30 08:44:32,546 SSTableWriter.java (line 414) Bloom filter FP chance of zero isn't supposed to happen
{pre}

when attempting to set it to zero, C* should either disallow the change or should just interpret 0 as ""make it the default"" and not continually log the above error message
",,,,,,,,,,,,,,,,,,,17/Dec/12 20:00;jbellis;5013-v2.txt;https://issues.apache.org/jira/secure/attachment/12561343/5013-v2.txt,07/Dec/12 14:16;brandon.williams;5013.txt;https://issues.apache.org/jira/secure/attachment/12559863/5013.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-07 14:12:15.016,,,no_permission,,,,,,,,,,,,293330,,,Mon Jan 07 18:11:20 UTC 2013,,,,,,0|i0swrr:,166786,brandon.williams,brandon.williams,,,,,,,,,,"07/Dec/12 14:12;brandon.williams;Since we're already converting zero to the default, it makes sense to maintain that behavior and simply remove the error.  Patch to do so.","07/Dec/12 16:44;jbellis;Where are we converting zero to default?  ISTM that setting to zero is likely to either mean

# user is asking for no false positives at all, which is impossible.  should give an error instead of changing to default.
# user is asking for no bloom filter at all, in which changing to default is also not what we want to do.","07/Dec/12 16:47;brandon.williams;If we detect zero we set fpChance to null, which then results in 15 target buckets.  I agree with both your points but for a point release in stable changing the behavior doesn't seem appropriate.",07/Dec/12 17:14;jbellis;then let's fix it in 1.2 :),"07/Dec/12 19:42;brandon.williams;Sure, but let's apply this patch for 1.1.8 :)","07/Dec/12 19:47;jbellis;I'm not sure at all this is going to work as designed.  The comments there say ""this isn't supposed to happen"" which makes me think that there is some code somewhere else that tries to get rid of zeros.

That complexity is gone in 1.2 (since we gave up on grandfathering in the pre-1.0? code that calls the 15 bucket path) so I'd rather stick with changes there than risk regressions.",17/Dec/12 20:00;jbellis;attached as v2,"17/Dec/12 20:04;andras.szerdahelyi@ignitionone.com;Thank you for your e-mail. I'm out of the office on vacation until Tuesday, the 8th of January 2013 with very little access to e-mail.
For technical questions or concerns, please write to tech-team@netmining.com.

happy holidays!
Andras Szerdahelyi
","17/Dec/12 21:11;andras;well, that wasn't entirely intentional, but happy holidays nevertheless! :-)

","04/Jan/13 12:59;brandon.williams;You need to check that bloomFilterFpChance isn't null before seeing if it's zero at the end to avoid NPE, but otherwise +1.","04/Jan/13 17:35;jbellis;Not sure what you mean, isn't that what I'm doing here?

{code}
.       // we disallow bFFPC==null starting in 1.2.1 but tolerated it before that
        return (bloomFilterFpChance == null || bloomFilterFpChance == 0)
               ? compactionStrategyClass == LeveledCompactionStrategy.class ? 0.1 : 0.01
               : bloomFilterFpChance;
{code}","04/Jan/13 17:39;brandon.williams;The problem is that if the compaction class is not LCS, it's still null, and then further down:

{noformat}
 if (bloomFilterFpChance == 0)
{noformat}

Throws an NPE.",07/Jan/13 18:11;jbellis;fixed + committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiget Supercolumns Sometimes Missing Results,CASSANDRA-5123,12626367,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,thobbs,thobbs,07/Jan/13 16:34,12/Mar/19 14:05,13/Mar/19 22:27,16/Jan/13 17:00,2.0 beta 1,,,,,0,,,,,,,"Starting approximately with commit c2812f3 (the January 3rd nightly build by DataStax Jenkins, #669), a few of the pycassa unit tests related to multigetting a particular supercolumn started failing periodically.  The nightly build is against Cassandra trunk.

You can reproduce with the pycassa unit tests fairly easily:
{noformat}
nosetests tests/test_columnfamily.py:TestSuperColumnFamily.test_multiget_supercolumn
{noformat}

It should fail within a few runs.

It looks like one of the requested keys isn't being returned at all.",,,,,,,,,,,,,,,,,,,14/Jan/13 13:38;slebresne;5123.txt;https://issues.apache.org/jira/secure/attachment/12564702/5123.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-14 13:38:35.334,,,no_permission,,,,,,,,,,,,302958,,,Wed Jan 16 17:00:13 UTC 2013,,,,,,0|i176s7:,250067,jbellis,jbellis,,,,,,,,,,"14/Jan/13 13:38;slebresne;This is the same problem that in CASSANDRA-4928, the slice filter was shared among multiple ReadCommand which is not correct in the current state of things. Attaching patch that remove the sharing.",16/Jan/13 08:26;jbellis;Why is it unsafe to share filter in SelectStatement for NQF?,"16/Jan/13 16:40;slebresne;bq. Why is it unsafe to share filter in SelectStatement for NQF?

It's not. Figured the cloneShallow was probably negligible enough to ignore in that case since it yield simpler code. That being said, since NQF is immutable (at least as far as cloning shallow is concerned), I suppose making its cloneShallow just return {{this}} is a valid option. ","16/Jan/13 16:49;jbellis;WFM either way, +1","16/Jan/13 17:00;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema push/pull race,CASSANDRA-5025,12618758,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,04/Dec/12 17:52,12/Mar/19 14:05,13/Mar/19 22:27,07/Dec/12 21:45,1.1.8,,,,,0,,,,,,,"When a schema change is made, the coordinator pushes the delta to the other nodes in the cluster.  This is more efficient than sending the entire schema.  But the coordinator also announces the new schema version, so the other nodes' reception of the new version races with processing the delta, and usually seeing the new schema wins.  So the other nodes also issue a pull to the coordinator for the entire schema.

Thus, schema changes tend to become O(n) in the number of KS and CF present.",,,,,,,,,,,,,,CASSANDRA-13061,,,,,06/Dec/12 19:10;jbellis;5025-v2.txt;https://issues.apache.org/jira/secure/attachment/12556480/5025-v2.txt,07/Dec/12 20:22;cherro;5025-v3.txt;https://issues.apache.org/jira/secure/attachment/12559943/5025-v3.txt,07/Dec/12 21:18;brandon.williams;5025-v4.txt;https://issues.apache.org/jira/secure/attachment/12559952/5025-v4.txt,10/Dec/12 00:49;cherro;5025-v5.txt;https://issues.apache.org/jira/secure/attachment/12560131/5025-v5.txt,04/Dec/12 21:01;jbellis;5025.txt;https://issues.apache.org/jira/secure/attachment/12555986/5025.txt,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-12-04 22:48:03.972,,,no_permission,,,,,,,,,,,,296006,,,Fri Jan 06 10:14:09 UTC 2017,,,,,,0|i144m7:,232211,xedin,xedin,,,,,,,,,,04/Dec/12 21:01;jbellis;patch attached to add a delay to rectifySchema to give concurrent changes a chance to propagate first,"04/Dec/12 22:48;cherro;For patch 5025.txt:

A single schema migration will result in N (num nodes) gossips of the new schema version (as before). Through MigrationManager.onChange()->rectifySchema(), those will each result in a delayed comparison of value 'theirVersion', but that value is now one minute old.

Further, if some new schema migration happens to be underway, the same effect of redundant repeat RowMutations will occur.

Schema migrations tend to happen in bursts - so this patch seems like it might reduce the problem but not eliminate it.

Would it not be better to have DefsTable.mergeSchema call Schema.instance.updateVersion instead of Schema.instance.updateVersionAndAnnounce and then deal with temporarily unavailable nodes by doing a MigrationManager.passiveAnnounce(version) if/when we see them come back online?",04/Dec/12 22:52;jbellis;I really don't want to reinvent HH poorly for schema migrations.  Maybe Pavel has a better suggestion.,"05/Dec/12 16:47;cherro;Clarifying for anyone else who encounters this issue:
* This problem was introduced in CASSANDRA-3931
* For use cases that involve creation/update/deletion of multiple keyspaces or column families, the symptom will be increasingly slow schema migrations as the KS/CF population grows. Depending on client RPC timeout config, schema change requests may fail. 
* In a test environment running stock C* 1.1.7, for a test that creates new CFs in sequence, we see the following CF creation times:
** Empty cluster: sub-second
** 200+ CFs: 15s ave.
** 400+ CFs: 30s+ with eventual failure due to 30s client side (Hector) RPC timeout.
* In the same test environment running 1.1.7 patched with 5025.txt:
** For the first 60s duration of the test, CF creation times are sub-second
** At 60s, the delayed rectifySchema migration calls kick in and creation times drop to 50s+ (including waits for schema agreement) with eventual failure due to 30s client side RPC timeout.

","05/Dec/12 23:34;xedin;[~cherro] I don't think that we every want to make such a frequent schema changes, it's not considered a good practice for a number of reasons, unless you are trying to do something like temp-tables which has it's own implications... Agree on [~jbellis] that it doesn't seem like a good idea to re-invent HH for schema until we have a good reason to do so.","06/Dec/12 19:10;jbellis;bq. Through MigrationManager.onChange()->rectifySchema(), those will each result in a delayed comparison of value 'theirVersion', but that value is now one minute old.

v2 attached that compares the current version after the delay.","06/Dec/12 20:28;cherro;[~jbellis]: patch 5025-v2.txt works better. For the same test, after 60s, the CF creation time drops from sub-second to 5 seconds average. Delayed rectifySchema work will still interfere with coincident schema migrations, but I think this is the right compromise. Thank you!

Minor: import for {{Callable}} was dropped, but is still referenced at line 229.

[~xedin]: This test was not endorsing a high rate of CF creation for real world use, the goal was to investigate if/why CF creation time was {{O(N)}}.",06/Dec/12 20:32;xedin;+1 with [~cherro] nit.,"06/Dec/12 22:16;jbellis;committed, thanks guys!","07/Dec/12 13:27;brandon.williams;This broke bootstrapping, the node thinks it has the schema when it does not, then of course streams nothing and joins the ring.

","07/Dec/12 15:50;cherro;Could StorageServer.joinTokenRing wait max(RING_DELAY, 1min) (the 1 min being the delay in MigrationManager.maybeScheduleSchemaPull? Or could MigrationManager.maybeScheduleSchemaPull use some multiple of RING_DELAY?

Related: is it correct that StorageServer.joinTokenRing calls Schema.instance.updateVersionAndAnnounce and MigrationManager.passiveAnnounce(Schema.instance.getVersion()) in quick succession?","07/Dec/12 16:50;cherro;From discussion on #cassandra-dev with [~brandon.williams], StorageServer.joinTokenRing could use Schema.emptyVersion as Schema UUID in order to allow the maybeScheduleSchemaPull delay to be skipped. Patch to follow...",07/Dec/12 20:22;cherro;Attached patch 3 proposing the use of Schema.emptyVersion to differentiate StorageServer.joinTokenRing from other scenarios so that migration delay can be skipped for bootstrapping.,"07/Dec/12 21:18;brandon.williams;Close, but maybeScheduleSchemaPull is actually called by the bootstrapping node, so the check needs to see if the current schema version is empty and if so pull.  v4 attached.",07/Dec/12 21:20;jbellis;+1,07/Dec/12 21:45;brandon.williams;Committed.,"10/Dec/12 00:49;cherro;(Following up on IRC discussion)
* My patch 3 incorrectly hardcoded Schema.emptyVersion for the announcement in SS.joinTokenRing. For actual bootstrap scenario, the schema version should be Schema.emptyVersion anyway. 
* Since Schema.updateVersion actually reads rows, I wondered if this will be equivalent to    Schema.emptyVersion (perhaps Schema tables themselves are represented already by this point in time?) Brandon said that he would check this.
* I had asked in a previous comment in this jira, and Brandon also noticed that SS.joinTokenRing had been calling Schema.updateVersionAndAnnounce and Schema.passiveAnnounce in quick succession. Brandon said that it should be removed.

I'm attaching patch 5 with these changes:
* Reverted my hardcoded Schema.emptyVersion in SS.joinTokenRing (back to original Schema.updateVersionAndAnnounce).
* Removed apparently redundant call to Schema.passiveAnnounce.

Brandon, could you please confirm whether it is safe to assume that Schema.updateVersionAndAnnounce would emit Schema.emptyVersion in a bootstrap scenario?
 ","10/Dec/12 00:56;xedin;bq. Since Schema.updateVersion actually reads rows, I wondered if this will be equivalent to Schema.emptyVersion (perhaps Schema tables themselves are represented already by this point in time?) Brandon said that he would check this.

If that is bootstrap and node is completely empty thus has newly created system tables Schema.updateVersion would emit ""emptyVersion"" as there is no data to be read and empty digest has a constant value.",10/Dec/12 02:07;brandon.williams;I fixed this up in 36389f7d8d59881cad309b078fc89df5864fd6d1 and allowed pulling immediately after a restart.,"10/Dec/12 02:54;cherro;Thanks [~brandon.williams], [~xedin].","20/Dec/16 15:59;pshirshov;Hi,

Looks like that the issue has not been resolved. 

I've just got this:

{code}
15:52:40.194 [cluster1-nio-worker-1] WARN  c.d.driver.core.RequestHandler - /127.0.0.1:9042 replied with server error (java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.exceptions.ConfigurationException: Column family ID mismatch (found 5230d9a0-c6cc-11e6-9ece-6d2c86545d91; expected 51d06a20-c6cc-11e6-9ece-6d2c86545d91)), defuncting connection.
{code}

On C* 3.9:

{code}
[cqlsh 5.0.1 | Cassandra 3.9 | CQL spec 3.4.2 | Native protocol v4]
Use HELP for help.
cqlsh>
{code}

with latest driver: {{""com.datastax.cassandra"" % ""cassandra-driver-core"" % ""3.1.2""}}

It's one-node setup and everything I'm doing is just running {{CREATE TABLE IF NOT EXISTS}} from several threads at the same time.

The only difference with previous version is the fact that applications starts hanging forever with stack like

{code}
""pool-4-thread-5-ScalaTest-running-CassandraMwsRestTest"" #37 prio=5 os_prio=31 tid=0x00007f7f7048a800 nid=0x5113 waiting on condition [0x0000700006010000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000796682630> (a com.datastax.driver.core.DefaultResultSetFuture)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:445)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:143)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:243)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:68)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:43)
{code}","06/Jan/17 10:14;webpoacher;I experience similar problems in a one node cluster. When I start 3 applications at the same time, they all try to migrate the cassandra schema (but will wait for each other using a locking table). However, they will check for this lock table using a CREATE TABLE IF NOT EXISTS

Cassandra driver 2.1.10.1

{noformat}
2017-01-03 09:57:22,372 · WARN · cluster2-nio-worker-1 · com.datastax.driver.core.RequestHandler · /127.0.0.1:9042 replied with server error (java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.exceptions.ConfigurationException: Column family ID mismatch (found a1c87c40-d192-11e6-a126-1d2c09c16740; expected a1c56f00-d192-11e6-a126-1d2c09c16740)), defuncting connection. ·  ·  ·  ·
2017-01-03 09:57:22,394 · ERROR · main · com.contrastsecurity.cassandra.migration.action.Migrate · Migration of keyspace ces2 to version 0.1.0.1 failed! Please restore backups and roll back database and code! ·  ·  ·  ·
2017-01-03 09:57:24,652 · ERROR · main · nl.mypackage.CassandraMigrationService · Error during migration ·  ·  ·  ·
com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9042 (com.datastax.driver.core.exceptions.DriverException: Timeout while trying to acquire available connection (you may want to increase the driver number of per-host connections)))
        at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84)
        at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)
        at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:217)
       at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:54)
        at com.contrastsecurity.cassandra.migration.dao.SchemaVersionDAO.tablesExist(SchemaVersionDAO.java:88)
{noformat}

What is the recommended way to perform schema migrations in a Cassandra cluster?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generated time-based UUID don't conform to the spec,CASSANDRA-5001,12618021,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,28/Nov/12 17:41,12/Mar/19 14:05,13/Mar/19 22:27,29/Nov/12 09:24,1.2.0 beta 3,,,,,0,,,,,,,"When UUIDGen layout the clock sequence and and node part of version 1 UUID, it does so with
{noformat}
private long getClockSeqAndNode(InetAddress addr)
{
    long lsb = 0;
    lsb |= (clock & 0x3f00000000000000L) >>> 56; // was 58?
    lsb |= 0x0000000000000080;
    lsb |= (clock & 0x00ff000000000000L) >>> 48;
    lsb |= makeNode(addr);
    return lsb;
}
{noformat}
This is not correct however, as this layout the clock seq (and variant) on the right-most part of the lsb while it should be on the left-most one.

At a minimum, the generated UUID don't fully respect the spec since the variant is not set correctly. But it also means that the clock seq bits end up being all 0's (as can be trivially seen in the string representation of the generated UUIDs).

Note that none of those is a huge huge deal as there is still largely enough random bytes to ensure that two different nodes won't end up with the same lsb. And having the variant wrong has probably no practical implementation. There is no reason not to fix those though.

One other small details is that the getAdjustedTimestamp as a sign error so that it returns completely broken timestamps. That being said the method is currently unused so that's not a big deal. I'm attaching a fix for that part too because that method might be useful someday but I won't shed a tear if we prefer just removing it.

I'm marking this for 1.2 because I'm not sure it's worth bothering with 1.1.",,,,,,,,,,,,,,,,,,,28/Nov/12 17:44;slebresne;5001.txt;https://issues.apache.org/jira/secure/attachment/12555199/5001.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-29 01:56:44.3,,,no_permission,,,,,,,,,,,,292623,,,Thu Nov 29 09:24:37 UTC 2012,,,,,,0|i0s9r3:,163056,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"29/Nov/12 01:56;vijay2win@yahoo.com;+1

Nit: it might be nice to move the calculation in getClockSeqAndNode into makeNode so we can cache it in nodeCache.","29/Nov/12 09:24;slebresne;Alright, committed, thanks

bq. Nit: it might be nice to move the calculation in getClockSeqAndNode into makeNode so we can cache it in nodeCache.

That true, but truth is, you're not supposed to generate uuid for another node (and in practice we never do) so that whole nodeCache is useless imho and the whole UUID lsb could be a static. So anyway, I'll open a separate ticket in a gif to fix those inefficiencies.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
user defined compaction is broken,CASSANDRA-5118,12626146,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,mkjellman,mkjellman,05/Jan/13 01:05,12/Mar/19 14:05,13/Mar/19 22:27,10/Jan/13 17:58,1.1.9,1.2.1,,,,0,,,,,,,"currently forceUserDefinedCompaction takes (keyspace, datafile)

cassandra tries to look for ks/ks-cf-hf-80-Data.db when the sstable actually exists at ks/cf/ks-cf-hf-80-Data.db

fix would be for user defined compaction to look for the sstable datafile in the correct location",Ubuntu 12.04,,,,,,,,,,,,,,,,,,09/Jan/13 20:03;yukim;5118-1.1.txt;https://issues.apache.org/jira/secure/attachment/12564021/5118-1.1.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-09 20:03:47.515,,,no_permission,,,,,,,,,,,,302729,,,Sat Jan 26 11:57:27 UTC 2013,,,,,,0|i17507:,249779,jbellis,jbellis,,,,,,,,,,"09/Jan/13 20:03;yukim;Looks like user defined compaction has not been working since data directory structure change in 1.1.
Patch fixes this by pointing proper directory based on given file names.",09/Jan/13 21:49;mkjellman;+1 verified this patch works against 1.2.0,"09/Jan/13 21:52;jbellis;LGTM.  Can you also add a quick test for forceUserDefinedCompaction to catch this regression?

Also created CASSANDRA-5139 for followup.",10/Jan/13 17:58;yukim;Committed with the test added to CompactionsTest.,"26/Jan/13 11:57;jjordan;FYI it worked, you just had to put the dir name in the string: so forceUserDefinedCompaction ks cf/ks-cf-hf-80-Data.db
Should put something in NEWS.txt for anyone who figured that out and was using forceUserDefinedCompaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to start when using Ec2Snitch,CASSANDRA-5212,12630222,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,jared.biel@bolderthinking.com,jared.biel@bolderthinking.com,31/Jan/13 21:37,12/Mar/19 14:05,13/Mar/19 22:27,01/Feb/13 20:38,,,,,,0,,,,,,,"Hello, we're using vanilla Cassandra in an EC2 environment and I just tested upgrading from 1.2.0 to 1.2.1 on two test instances. Cassandra fails to start because it's unable to load the Ec2Snitch. Version 1.2.0 was working OK. I have tried this on uninitialized/empty instances and received the same result. Cassandra successfully starts when switching to SimpleSnitch. Log output is below. We're using the official debian package from apache.org. Please let me know if you need any more details, thanks!

output.log
{code}
ERROR 21:25:06,684 Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Error instantiating snitch class 'org.apache.cassandra.locator.Ec2Snitch'.
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:475)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:525)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:338)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:122)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:151)
	at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:315)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:532)
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:457)
	... 10 more
Caused by: java.lang.ExceptionInInitializerError
	at org.apache.cassandra.locator.Ec2Snitch.<init>(Ec2Snitch.java:65)
	... 15 more
Caused by: java.lang.RuntimeException: Unable to read cassandra-rackdc.properties
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:39)
	... 16 more
Caused by: java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:435)
	at java.util.Properties.load0(Properties.java:354)
	at java.util.Properties.load(Properties.java:342)
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:35)
	... 16 more
Error instantiating snitch class 'org.apache.cassandra.locator.Ec2Snitch'.
Fatal configuration error; unable to start server.  See log for stacktrace.
Service exit with a return value of 1
{code}

system.log
{code}
INFO [main] 2013-01-31 21:25:05,016 CassandraDaemon.java (line 123) JVM vendor/version: OpenJDK 64-Bit Server VM/1.6.0_24
<SNIP>
ERROR [main] 2013-01-31 21:24:52,028 DatabaseDescriptor.java (line 509) Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Error instantiating snitch class 'org.apache.cassandra.locator.Ec2Snitch'.
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:475)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:525)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:338)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:122)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:151)
	at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:315)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:532)
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:457)
	... 10 more
Caused by: java.lang.ExceptionInInitializerError
	at org.apache.cassandra.locator.Ec2Snitch.<init>(Ec2Snitch.java:65)
	... 15 more
Caused by: java.lang.RuntimeException: Unable to read cassandra-rackdc.properties
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:39)
	... 16 more
Caused by: java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:435)
	at java.util.Properties.load0(Properties.java:354)
	at java.util.Properties.load(Properties.java:342)
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:35)
	... 16 more
<SNIP>
 INFO [main] 2013-01-31 21:25:06,656 DatabaseDescriptor.java (line 267) Global memtable threshold is enabled at 407MB
ERROR [main] 2013-01-31 21:25:06,684 DatabaseDescriptor.java (line 509) Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Error instantiating snitch class 'org.apache.cassandra.locator.Ec2Snitch'.
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:475)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:525)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:338)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:122)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:151)
	at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:315)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:532)
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:457)
	... 10 more
Caused by: java.lang.ExceptionInInitializerError
	at org.apache.cassandra.locator.Ec2Snitch.<init>(Ec2Snitch.java:65)
	... 15 more
Caused by: java.lang.RuntimeException: Unable to read cassandra-rackdc.properties
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:39)
	... 16 more
Caused by: java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:435)
	at java.util.Properties.load0(Properties.java:354)
	at java.util.Properties.load(Properties.java:342)
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:35)
	... 16 more
{code}","Ubuntu 12.04 x64
OpenJDK 64-Bit Server VM/1.6.0_24
m1.large EC2 Instance

MAX_HEAP_SIZE=""1242M""
HEAP_NEWSIZE=""200M""
",,,,,,,,,,,,,,,,,,01/Feb/13 00:39;vijay2win@yahoo.com;0001-CASSANDRA-5212.patch;https://issues.apache.org/jira/secure/attachment/12567467/0001-CASSANDRA-5212.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-31 23:34:37.207,,,no_permission,,,,,,,,,,,,310718,,,Fri Feb 01 20:38:28 UTC 2013,,,,,,0|i1hmg7:,311063,brandon.williams,brandon.williams,,,,,,,,,,"31/Jan/13 23:34;jbellis;It tells you what the problem is:

Caused by: java.lang.RuntimeException: Unable to read cassandra-rackdc.properties
","31/Jan/13 23:51;jared.biel@bolderthinking.com;Thanks - I didn't see that needle in the (hay) stack. It looks like as a result of CASSANDRA-5155 the file cassandra-rackdc.properties is now required to be in /etc/cassandra. However, this file is not distributed with the standard Debian Cassandra package. Maybe this should be documented in cassandra.yaml or the file should be distributed as part of the Debian package? Thanks for your help.","31/Jan/13 23:54;jbellis;Hmm, yes.  We should make it optional and/or add to the debian package.","01/Feb/13 12:42;brandon.williams;+1, but instead of silently swallowing the exception maybe a WARN would be more appropriate.","01/Feb/13 13:02;jasobrown;I agree with @driftx that we should add the WARN statement. Otherwise, +1","01/Feb/13 20:38;vijay2win@yahoo.com;Committed to 1.1, 1.2 and trunk, Thanks Brandon...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a better CQL error when table data does not conform to CQL metadata.,CASSANDRA-5138,12626841,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,boneill,boneill,09/Jan/13 20:26,12/Mar/19 14:05,13/Mar/19 22:27,24/Jul/13 15:04,2.0.0,,,,,0,,,,,,,"When you create a table via CQL, then insert into it via Thrift.  If you inadvertently leave out a component of the column name, in CQL you receive a:
TSocket read 0 bytes

Server-side the following exception is logged:
ERROR 15:19:18,016 Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: 3
	at org.apache.cassandra.cql3.statements.ColumnGroupMap.add(ColumnGroupMap.java:43)
	at org.apache.cassandra.cql3.statements.ColumnGroupMap.access$200(ColumnGroupMap.java:31)
	at org.apache.cassandra.cql3.statements.ColumnGroupMap$Builder.add(ColumnGroupMap.java:138)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:805)
	at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:145)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:134)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:61)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:132)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:140)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1686)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)

I'll submit a schema, and steps to reproduce.",Mac OS X running 1.2,,,,,,,,,,,,,,,,,,23/Jul/13 08:21;slebresne;5138-2.txt;https://issues.apache.org/jira/secure/attachment/12593656/5138-2.txt,22/Jul/13 14:51;slebresne;5138.txt;https://issues.apache.org/jira/secure/attachment/12593519/5138.txt,09/Jan/13 20:27;boneill;northpole.cql;https://issues.apache.org/jira/secure/attachment/12564030/northpole.cql,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-01-10 17:21:33.71,,,no_permission,,,,,,,,,,,,303532,,,Mon Sep 23 12:37:26 UTC 2013,,,,,,0|i17c67:,250940,jbellis,jbellis,,,,,,,,,,09/Jan/13 20:27;boneill;You can use this schema to reproduce the issue.  ,"09/Jan/13 20:29;boneill;Fire up cassandra-cli, and execute the following:
[default@unknown] use northpole;
Authenticated to keyspace: northpole
[default@northpole] set naughtyornicelist['naughty:USA']['PA:18964:michael.myers']='00';
Value inserted.
Elapsed time: 36 msec(s).

Then, go back to CQL and execute the following:
cqlsh> use northpole;
cqlsh:northpole> select * from naughtyornicelist ;
TSocket read 0 bytes
","10/Jan/13 17:21;slebresne;I don't think it is reasonable to have CQL validate on every read that no bad data has been added to non compact table through thrift: outside of not being particularly simple to implement and error prone, it would be silly performance wise.

The fact is, *non* compact CQL3 tables are a CQL3 things and in theory they should not be accessible at all through thrift (exactly because thrift has the potential of screwing things up). But I do understand the willingness to use thrift to look under the cover to see how things are implemented and that definitively have educational value. Which imo leaves us with the following options:
# do nothing and be clear that 'you shalt not mess up with non compact CQL3 table through thrift' and that if you do, things may break in unexpected ways.
# add validation on the thrift write path.
# refuse write access to non compact CQL3 tables from thrift by default with an option to deactivate that protection ""at your own risk"".

In an ideal world 2) would be the best solution. However, adding said validation is not 2 lines of code, far from it, and there would be the maintenance cost of keeping said validation up to date with the evolutions of CQL3. So, and especially given that accessing non compact CQL3 table from thrift is difficult anyway due to not all metadata being exposed, I'm not convinced that adding said validation would be the best use of our developer resources. Of course if someone is willing to step up to write and maintain that validation code, that's fine, but in the absence of that I would suggest 3) as a good enough compromise.
","10/Jan/13 17:42;jbellis;I thought #2 (validate on insert) was the plan for CASSANDRA-4377.

Unless it's really hairy I do think we should deliver on that.  Essentially nobody is going to be able to compose cql-inserts-from-thrift safely otherwise, and as crazy as it is people seem to want to do it.  If it makes people feel better about upgrading then I'm in favor.","10/Jan/13 18:16;slebresne;bq. I thought #2 (validate on insert) was the plan for CASSANDRA-4377.

CASSANDRA-4377 stopped at ""make sure well formed thrift query do not crash"".

Making sure you don't do something that would break CQL3 require a bit more. For instance, in theory we should refuse a query that adds a column in a CQL row that doesn't exist in the metadata (it could be that doing so doesn't really ""break"" anything but honestly I have no clue if that's the case in practice). Anyway, maybe doing some basic validation is not too hard. But I'm afraid it will be easy to forget validating something that breaks CQL3 in subtle ways. So ok for adding validation on a best effort basis, but I think we should still be clear that this is not encouraged and you have to do it at your own risk.

","11/Jan/13 03:27;boneill;Just to be clear, I'd be happy with a more informative message. (translate the ArrayOutOfBounds, to ""You may not have enough components in your column name."")

To elaborate on the craziness that is accessing tables from CQL that are populated by thrift...
We want to enable our ""data heads"" with cqlsh.  They are very excited about it because it looks like SQL, and to date Cassandra has been inaccessible to them.  (These are data gurus that are wizards at a SQL prompt, but do not have server-side access to Cassandra)  We have existing (and new) applications that use thrift to write (via Astyanax composite/compound columns using annotated classes).  We don't want to change our application development paradigm (yet), but we want cqlsh access to the data.  In that paradigm, we have encountered this error during development, especially if/when you get the translation wrong between a CQL schema and the thrift interpretation. (which isn't hard to do)","22/Jul/13 14:51;slebresne;Attaching relatively simple patch that simply assert a column name has the right number of components when inserting in a CQL3 table.

The patch does not validate that the CQL3 column inserted exists however. We could do it I suppose, but inserting such cell shouldn't really corrupt the data, the cell should simply be ignored by a select, so I figured it's probably not worth going further for thrift.","22/Jul/13 15:48;jbellis;IMO we should do that extra check (also type information, unless that's already taken care of by the other validation).  If you really wanted unchecked composite inserts, you'd presumably create the table from Thrift.  So here we should assume that the user is doing his best to create a valid CQL row and we should reject invalid ones.","22/Jul/13 15:59;slebresne;bq. IMO we should do that extra check

To complete the reason why I haven't add it, this is mostly because of DROP (CASSANDRA-3919). So in practice, people may have cell that don't correspond to a (currently) defined CQL3 column name, even if it's only temporary. For that, it could seem fair game to at least let user delete those from thrift. But I mean, I don't really care, I'm fine doing that extra check if you still think it's better, just wanted to present the full reasoning.

bq. also type information, unless that's already taken care of by the other validation

Thrift already does validation of the cell name and cell value (as well as partition key). So unless you were thinking of something else, we should be good.","22/Jul/13 17:07;jbellis;I don't think I follow -- when we implement DROP it will effectively delete columns without the user explicitly having to do so.  If you're worried about concurrency, I think your reasoning about ""Don't Do That"" from the keyspaces ticket applies.","23/Jul/13 08:21;slebresne;I guess all I meant is that since people tends to get touchy when you limit the thrift interface, doing the minimum validation so as not to crash CQL could be an option. Anyway, doesn't matter, attaching v2 patch that does the extra check.
",24/Jul/13 14:24;jbellis;+1,24/Jul/13 14:24;jbellis;... although we might want to make this 2.0-only to be on the safe side.,"24/Jul/13 15:04;slebresne;Agreed, committed to trunk only. Thanks","16/Aug/13 19:40;elprans;Guys, this completely broke writing to CQL3 tables from Thrift for me.

The name check is done against CFDefinition.columns, which seems to be populated with clustering key columns only.  Thus, it is not possible to write into columns that are outside the PRIMARY KEY.  Is this intentional?","19/Aug/13 09:20;slebresne;It is not allowed to insert cells (thrift columns if you will) that do not correspond to a declared CQL3 column no. But that's something that would qualify as wrong from a CQL3 point of view anyway so that does is intentional. Now from your description, it's hard to say if you were doing it wrong or if the added validation is indeed too restrictive, but at least looking quickly at the patch again I don't see anything wrong.

Maybe can you give a simple example of what you're trying to do and doesn't work (the CQL3 schema of the CF and and example of insertion that doesn't work with cassandra-cli for instance)? ","19/Aug/13 14:55;elprans;I understand that columns not declared in CQL3 are supposed to fail validation.  The problem is that even _defined_ columns in non-compact tables fail now.  Something as simple as

CREATE TABLE test (
    id     text,
    attr   text,
    value  text,
 
    PRIMARY KEY (id, attr)
);

It is no longer possible to write into ""value"" from Thrift, because it's not a part of the clustering key, which is apparently what CFMetadata.columns is populated from and against what the check is done.","21/Aug/13 07:04;slebresne;You're right, there was a typo in the initial patch, we were checking against the clustering keys at a place where we were suppose to check the non-PK columns. Anyway, I took the liberty to commit the trivial fix to 7a300c2 (and checked insertion in a table like the one above does work as expected).","19/Sep/13 16:54;elprans;Another related issue.  This check seems to make it impossible to insert proper records into CQL3 tables via Thrift, since it bails out on an attempt to write a column with an empty last component.

{code:sql} 
CREATE TABLE test (
    id text,
    attr text,
    value text,

    PRIMARY KEY (id, attr)
);
{code}

The storage column structure for the table would be:

|<key>|{<attr>,}|[empty]
|<key>|{<attr>,""value""}|<value>

There is no way to insert the first column (record demarcation) as it fails with ""Invalid cell for CQL3 table test. The CQL3 column component () does not correspond to a defined CQL3 column""
",19/Sep/13 17:48;elprans;I think the best fix would be to make CassandraServer insert row markers into CQL3 tables the same way CQL modification statements do.,"23/Sep/13 12:37;slebresne;You're right, but since this has been closed and has shipped already, opened CASSANDRA-6081 to fix letting row markers pass validation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
word count example fails with InvalidRequestException(why:Start key's token sorts after end token),CASSANDRA-5168,12628105,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,jeromatron,jeromatron,17/Jan/13 20:22,12/Mar/19 14:05,13/Mar/19 22:27,25/Jan/13 20:28,1.1.10,1.2.2,,,,0,,,,,,,Tried with the latest 1.2 branch (commit d64dc2eb3a1a3c3771bbe3218af9ce9629ec67bf) and got this error.  Seems related to but different than CASSANDRA-5106.,,,,,,,,,,,,,,,,,,,25/Jan/13 19:39;brandon.williams;5168.txt;https://issues.apache.org/jira/secure/attachment/12566541/5168.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-25 18:58:41.112,,,no_permission,,,,,,,,,,,,304956,,,Fri Jan 25 20:28:25 UTC 2013,,,,,,0|i17x3z:,254334,jbellis,jbellis,,,,,,,,,,"25/Jan/13 18:58;brandon.williams;{noformat}
java.lang.RuntimeException: InvalidRequestException(why:Start key's token sorts after end token)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:475)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:481)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:427)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getProgress(ColumnFamilyRecordReader.java:109)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.getProgress(MapTask.java:411)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:427)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: InvalidRequestException(why:Start key's token sorts after end token)
        at org.apache.cassandra.thrift.Cassandra$get_paged_slice_result.read(Cassandra.java:13685)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_paged_slice(Cassandra.java:731)
        at org.apache.cassandra.thrift.Cassandra$Client.get_paged_slice(Cassandra.java:715)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:460)
        ... 12 more
{noformat}

Only affects the wide row iterator.  In this case it's setting start key to -5551577223485402047 (key808) and comparing to -9223372036854775808",25/Jan/13 19:39;brandon.williams;I think we need to check that end token is not minimum here too.,25/Jan/13 20:21;jbellis;+1,25/Jan/13 20:28;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node isn't removed from system.peers after 'nodetool removenode',CASSANDRA-5167,12628028,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,ng@issuu.com,ng@issuu.com,17/Jan/13 12:20,12/Mar/19 14:05,13/Mar/19 22:27,18/Jan/13 16:20,1.2.1,,,,,0,,,,,,,"In a 3 node live cluster - After a replacement of a dead node, the old node remains in the system.peers table, even after running 'nodetool removenode <ID>'.

","Ubuntu 12.10, Java 1.7.0_09 (OpenJDK)",,,,,,,,,,,,,,,,,,18/Jan/13 15:57;brandon.williams;5167-v2.txt;https://issues.apache.org/jira/secure/attachment/12565490/5167-v2.txt,17/Jan/13 19:52;brandon.williams;5167.txt;https://issues.apache.org/jira/secure/attachment/12565366/5167.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-17 19:22:10.16,,,no_permission,,,,,,,,,,,,304876,,,Fri Jan 18 16:20:02 UTC 2013,,,,,,0|i17wjz:,254243,slebresne,slebresne,,,,,,,,,,17/Jan/13 19:22;brandon.williams;I'm not sure why we ever made a 'removeTokens' method instead of 'removeEndpoint' since we're never going to want to remove some subset of a node's tokens.  Patch to delete endpoints.,"18/Jan/13 09:37;slebresne;bq. I'm not sure why we ever made a 'removeTokens' method instead of 'removeEndpoint'

That slightly confused me too in CASSANDRA-4351, though I went for the lazy option of maintaining behavior blindly.

So definitely agreed on the idea. Two tiny remark on the patch though:
* in SS.handleStateRemoving, we call Gossiper.instance.removeEndpoint() but shouldn't we called SystemTable.removeEndpoint() too? In fact, it seems to me we may want to group both in a small utility method since at least when we call the Gossiper one, I think we always should call SystemTable one.
* in SS.handleStateNormal, ST.removeEndpoint should probably only be call if {{!isClient}} like in SS.excise().",18/Jan/13 15:57;brandon.williams;v2 incorporates both ideas.,18/Jan/13 16:08;slebresne;+1 with the nit that in excise I think we can call the new removeEndpoint instead of doing both call separately.,18/Jan/13 16:20;brandon.williams;Committed w/nit fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saved key cache is not loaded when opening ColumnFamily,CASSANDRA-5166,12627947,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,16/Jan/13 23:03,12/Mar/19 14:05,13/Mar/19 22:27,18/Jan/13 17:49,1.1.10,,,,,0,,,,,,,"_This bug happens on cassandra version 1.1.3 ~ 1.1.x only_

In order to load key cache in SSTableReader#open, keyCache has to be set by calling setTrackedBy before loading. CASSANDRA-4436 changed the order to load then setTrackedBy, so saved key cache is never loaded.
In 1.2, key cache stores both key and position, and is loaded out side of SSTableReader so the issue only happens on 1.1.x branch.",,,,,,,,,,,,,,,,,,,16/Jan/13 23:04;yukim;0001-key-cache-loading-test.patch;https://issues.apache.org/jira/secure/attachment/12565202/0001-key-cache-loading-test.patch,16/Jan/13 23:04;yukim;0002-fix-loading-key-cache.patch;https://issues.apache.org/jira/secure/attachment/12565203/0002-fix-loading-key-cache.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-18 00:25:27.936,,,no_permission,,,,,,,,,,,,304795,,,Fri Jan 18 17:49:12 UTC 2013,,,,,,0|i17w0n:,254156,jbellis,jbellis,,,1.1.3,,,,,,,16/Jan/13 23:04;yukim;Test and proposed fix attached.,18/Jan/13 00:25;jbellis;+1,18/Jan/13 17:49;yukim;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json doesn't check SIGPIPE,CASSANDRA-5150,12627231,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,pmirski,oberman,oberman,11/Jan/13 18:56,12/Mar/19 14:05,13/Mar/19 22:27,03/May/13 08:55,2.0 beta 1,,Legacy/Tools,,,0,lhf,,,,,,"I believe this explains the issue better than I can: http://stackoverflow.com/questions/11695500/how-do-i-get-java-to-exit-when-piped-to-head.

Basically, I expected that if I did: ""sstable2json SSTABLE | other-process"", and other-process had issues and/or died then the sstable2json process would die.  It doesn't.  

My workaround is using mkfifo FILE, and having sstable2json write to FILE, other-process read from FILE, and a 3rd overall process make sure the other two processes are working.  But, it would be _much_ simplier if sstable2json failed on SIGPIPE.

I looks like the fix is to periodically check System.out.checkError() in the Java.",,,,,,,,,,,,,,,,,,,27/Apr/13 13:25;pmirski;trunk-5150.txt;https://issues.apache.org/jira/secure/attachment/12580835/trunk-5150.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-27 13:25:28.613,,,no_permission,,,,,,,,,,,,304001,,,Thu May 02 16:49:50 UTC 2013,,,,,,0|i17hef:,251788,jbellis,jbellis,,,,,,,,,,27/Apr/13 13:25;pmirski;Patch in attachment. ,"27/Apr/13 13:30;pmirski;In patch there is checkError every entry write, which causes flush. Maybe better idea is to call checkError once per few entries to improve performance?","29/Apr/13 14:57;jbellis;Do you actually see a performance difference when you redirect to a file, for instance?  That's probably by far the most common use case.","30/Apr/13 14:35;pmirski;Tested on Windows, it seems that performance drop is minimal (about 1%).","02/May/13 16:49;jbellis;LGTM; committed.  Thanks, Pawel!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid streamId in cql binary protocol when using invalid CL,CASSANDRA-5164,12627737,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,pchalamet,pchalamet,15/Jan/13 21:13,12/Mar/19 14:05,13/Mar/19 22:27,27/Mar/13 10:28,1.2.4,,Legacy/CQL,,,0,,,,,,,"Execute a query using invalid CL (0x100 for example)

The response comes but does not use the request streamId (always 0).","Windows 8, java version ""1.6.0_37"" x86",,,,,,,,,,,,,,,,,,26/Mar/13 22:28;pchalamet;5164-2.txt;https://issues.apache.org/jira/secure/attachment/12575598/5164-2.txt,25/Mar/13 13:59;slebresne;5164.txt;https://issues.apache.org/jira/secure/attachment/12575314/5164.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-23 23:10:44.554,,,no_permission,,,,,,,,,,,,304524,,,Wed Mar 27 10:28:27 UTC 2013,,,,,,0|i17n4f:,252715,pchalamet,pchalamet,,,,,,,,,,23/Mar/13 23:10;jbellis;We should reject invalid CL.,"25/Mar/13 13:59;slebresne;Attaching patch to preserve the streamId in that case. I note that we don't do any effort to preserve the streamId if you screw up the frame format, but imo it's fair game. If you don't respect the protocol at that level, you have other problems to worry about than the streamId. ",26/Mar/13 22:28;pchalamet;fix invalid cast,"26/Mar/13 22:30;pchalamet;The patch would be better by inverting lines 202 & 203 in ErrorMessage.java:
{code}
public static ErrorMessage fromException(Throwable e)
 ...
 streamId = ((WrappedException)e).streamId;
 e = e.getCause();
{code}

Otherwise, an invalid cast is raised. Patch enclosed - I've tested it and exception nicely flows to the client using the right stream id.","27/Mar/13 10:28;slebresne;Oups, committed v2, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid serializing to byte[] on commitlog append,CASSANDRA-5199,12629836,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,30/Jan/13 04:04,12/Mar/19 14:05,13/Mar/19 22:27,05/Feb/13 14:46,2.0 beta 1,,,,,0,,,,,,,We used to avoid re-serializing RowMutations by caching the byte[] that we read off the wire.  We don't do that anymore since we fixed MessagingService to not create intermediate byte[].  So we should serialize the mutation directly to the commitlog.,,,,,,,,,,,,,,CASSANDRA-6714,,,,,30/Jan/13 04:06;jbellis;5199-1.2.txt;https://issues.apache.org/jira/secure/attachment/12567102/5199-1.2.txt,30/Jan/13 04:09;jbellis;5199-2.0.txt;https://issues.apache.org/jira/secure/attachment/12567103/5199-2.0.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-02-04 19:06:46.515,,,no_permission,,,,,,,,,,,,310332,,,Tue Feb 05 14:46:28 UTC 2013,,,,,,0|i1hk2n:,310677,yukim,yukim,,,,,,,,,,"30/Jan/13 04:06;jbellis;1.2 version just gets rid of the byte[] caching, since it's never actually re-used.","30/Jan/13 04:09;jbellis;2.0 version adds ByteBufferOutputStream and ChecksummedOutputStream to get rid of byte[] serialization entirely.  Also fixes mutation-length checksumming to include the entire length, not just the first eight bits.","04/Feb/13 19:06;yukim;+1 for both.
nit: you need the license header to new files in 2.0.",05/Feb/13 14:46;jbellis;Committed.  (I'll let RAT add the licenses.),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unbounded (?) thread growth connecting to an removed node,CASSANDRA-5175,12628423,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,jalkanen,jalkanen,19/Jan/13 19:52,12/Mar/19 14:05,13/Mar/19 22:27,23/Jan/13 06:48,1.1.10,1.2.1,,,,0,,,,,,,"The following lines started repeating every minute in the log file

{noformat}
 INFO [GossipStage:1] 2013-01-19 19:35:43,929 Gossiper.java (line 831) InetAddress /10.238.x.y is now dead.
 INFO [GossipStage:1] 2013-01-19 19:35:43,930 StorageService.java (line 1291) Removing token 170141183460469231731687303715884105718 for /10.238.x.y
{noformat}

Also, I got about 3000 threads which all look like this:

{noformat}
Name: WRITE-/10.238.x.y
State: WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1bb65c0f
Total blocked: 0  Total waited: 3

Stack trace: 
 sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:104)
{noformat}

A new thread seems to be created every minute, and they never go away.

The endpoint in question had been a part of the cluster weeks ago, and the node exhibiting the thread growth was added yesterday.

Anyway, assassinating the endpoint in question stopped thread growth (but kept the existing threads running), so this isn't a huge issue.  But I don't think the thread count is supposed to be increasing like this...","EC2, JDK 7u9, Ubuntu 12.04.1 LTS",,,,,,,,,,,,,,,,,,21/Jan/13 04:07;vijay2win@yahoo.com;0001-CASSANDRA-5175.patch;https://issues.apache.org/jira/secure/attachment/12565729/0001-CASSANDRA-5175.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-19 23:43:34.814,,,no_permission,,,,,,,,,,,,305577,,,Sat Jul 27 02:12:01 UTC 2013,,,,,,0|i18ffr:,257303,brandon.williams,brandon.williams,,,,,,,,,,"19/Jan/13 23:43;brandon.williams;Can you take a look, Vijay?",20/Jan/13 04:28;vijay2win@yahoo.com;I think the problem is that when we close the connection the thread will not be ended. Attached patch solves thread leak problem.,"22/Jan/13 19:06;brandon.williams;Hard to know for certain if this is truly the cause, but it sounds plausible. +1","23/Jan/13 06:48;vijay2win@yahoo.com;Committed to trunk, 1.1 and 1.2. Thanks!","23/Jan/13 07:46;vijay2win@yahoo.com;Also added a commit db8705294ba96fe2b746fea4f26a919538653ebd for test failure (dtest), basically we should not close the thread because we convicted the node. Thanks!","26/Jul/13 19:11;timiblossom;Hi Vijay,

I am using your commit db8705294ba96fe2b746fea4f26a919538653ebd but I think the logic in this commit is not the same as the attached patch.  Please take a look.

if (m == CLOSE_SENTINEL)
             {
                 disconnect();
+                if (!isStopped)
+                    break;
                 continue;
             }


I think it should be :

       if (isStopped)
           break;

Thanks.
","27/Jul/13 02:12;vijay2win@yahoo.com;Yes there was another commit on top the attached patch to fix the test cases, yes the logic has changed since calling close() is the only time we need to stop the thread.

Current code in the repo
{code}
            if (m == CLOSE_SENTINEL)
            {
                disconnect();
                if (isStopped)
                    break;
                continue;
            }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clqsh COPY is broken after strictening validation in 1.2.2 (quotes ints),CASSANDRA-5305,12634867,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,01/Mar/13 17:41,12/Mar/19 14:05,13/Mar/19 22:27,11/Mar/13 20:13,1.2.3,,Legacy/Tools,,,0,cqlsh,,,,,,"cqlsh COPY is quoting values when it shouldn't, and that's throwing IRE in 1.2.2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-02 07:51:51.415,,,no_permission,,,,,,,,,,,,315360,,,Mon Mar 11 20:13:33 UTC 2013,,,,,,0|i1if3b:,315704,brandon.williams,brandon.williams,,,,,,,,,,02/Mar/13 07:51;pushkarp;Any workaround for this issue? Not being able to use COPY cmd makes things difficult..,"02/Mar/13 16:13;iamaleksey;Not yet - fixing this turned out to be trickier than expected, but I'm on it.","06/Mar/13 23:46;iamaleksey;https://github.com/iamaleksey/cassandra/compare/5305

Updated COPY FROM to only quote ascii, text, timestamp, and inet values.
This also required changing the way we treat csv null, since we can no longer insert '' into, say, an int column.

Empty string (default null representation) is treated as a null now - the way PostgreSQL does it (""When using COPY FROM, any data item that matches this string will be stored as a null value, so you should make sure that you use the same string as you used with COPY TO."")

And since we need metadata from CQL3 system.schema_columns and system.schema_columnfamilies now, COPY requires cqlsh to be run in CQL3 mode (which still allows importing/exporting thrift/cql2 cfs, just using CQL3).","07/Mar/13 20:51;iamaleksey;Actually, I'm not sure if this treatment of nulls is enough or if we should go further and issue DELETEs for all the columns that are null (since we can't INSERT null). The suggested implementation simply omits these columns from the import INSERT queries.","07/Mar/13 20:52;iamaleksey;Yeah, I think that's how it used to work back when '' was a valid value for all the data types. Issuing DELETEs is in line with the old implementation.",09/Mar/13 17:37;iamaleksey;Added proper null handling in the second commit (https://github.com/iamaleksey/cassandra/compare/5305),09/Mar/13 17:39;iamaleksey;I think the only non-working part now is import of collections. But that can wait.,11/Mar/13 19:26;brandon.williams;+1,"11/Mar/13 20:13;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MalformedObjectNameException in ConnectionMetrics for IPv6 nodes because of "":"" characters",CASSANDRA-5298,12634583,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,michalm,michalm,michalm,28/Feb/13 09:59,12/Mar/19 14:05,13/Mar/19 22:27,04/Mar/13 10:46,1.2.3,,,,,0,,,,,,,"After upgrading node to 1.2.1, during C* startup, for all ConnectionMetrics I get exception like this one:

{noformat} WARN [GossipStage:1] 2013-02-27 12:14:55,431 JmxReporter.java (line 388) Error processing org.apache.cassandra.metrics:type=Connection,scope=2001:4c28:20:177:0:1:2:4,name=Timeouts
javax.management.MalformedObjectNameException: Invalid character ':' in value part of property
	at javax.management.ObjectName.construct(ObjectName.java:602)
	at javax.management.ObjectName.<init>(ObjectName.java:1403)
	at com.yammer.metrics.reporting.JmxReporter.onMetricAdded(JmxReporter.java:386)
	at com.yammer.metrics.core.MetricsRegistry.notifyMetricAdded(MetricsRegistry.java:516)
	at com.yammer.metrics.core.MetricsRegistry.getOrAdd(MetricsRegistry.java:491)
	at com.yammer.metrics.core.MetricsRegistry.newMeter(MetricsRegistry.java:240)
	at com.yammer.metrics.Metrics.newMeter(Metrics.java:245)
	at org.apache.cassandra.metrics.ConnectionMetrics.<init>(ConnectionMetrics.java:102)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.<init>(OutboundTcpConnectionPool.java:53)
	at org.apache.cassandra.net.MessagingService.getConnectionPool(MessagingService.java:481)
	at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:489)
	at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:612)
	at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:581)
	at org.apache.cassandra.gms.GossipDigestSynVerbHandler.doVerb(GossipDigestSynVerbHandler.java:85)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

(...){noformat}

Looking at ObjectName source code (e.g. http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/javax/management/ObjectName.java ) I can see that "":"" is not a valid character, so my idea for solving this problem is to use one of the following:

1) URLEncode.encode() - seems to be more ""proper"" solution, but produces a bit unreadable metric scope like: MBean org.apache.cassandra.metrics:type=Connection,scope=2001%3A4c28%3A10%3A168%3A0%3A2%3A3%3A6,name=CommandCompletedTasks

2) <String>.replaceAll() - we can simply replace "":"" with ""."" which wouldn't give us valid IPv6 address, but will be much more readable. 

Second one seems to be a better choice for me, but I attach two patches.",,,,,,,,,,,,,,,,,,,28/Feb/13 10:00;michalm;ipv6-connection-metrics-URLEncoder.patch;https://issues.apache.org/jira/secure/attachment/12571394/ipv6-connection-metrics-URLEncoder.patch,28/Feb/13 10:00;michalm;ipv6-connection-metrics-replaceAll.patch;https://issues.apache.org/jira/secure/attachment/12571393/ipv6-connection-metrics-replaceAll.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-04 10:46:21.863,,,no_permission,,,,,,,,,,,,315076,,,Mon Mar 04 10:46:21 UTC 2013,,,,,,0|i1idc7:,315420,jbellis,jbellis,,,,,,,,,,"01/Mar/13 06:50;michalm;D'oh, I attached the files, but did not mark this issue as ""Patch available""!",04/Mar/13 10:46;jbellis;committed (and added a comment for) the replaceAll patch.  thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes can be marked up after gossip sends the goodbye command,CASSANDRA-5254,12632412,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,14/Feb/13 17:09,12/Mar/19 14:05,13/Mar/19 22:27,06/Mar/13 22:10,1.1.11,,,,,0,,,,,,,"Finally tracked this down on dtestbot after setting the rpc_timeout to ridiculous levels:

{noformat}
==> logs/last/node1.log <==
 INFO [FlushWriter:1] 2013-02-14 10:01:10,311 Memtable.java (line 305) Completed flushing /tmp/dtest-iaYzzR/test/node1/data/system/schema_columns/system-schema_columns-hf-2-Data.db (558 bytes) for commitlog position ReplayPosition(segmentId=1360857665931, position=4770)
 INFO [MemoryMeter:1] 2013-02-14 10:01:10,974 Memtable.java (line 213) CFS(Keyspace='ks', ColumnFamily='cf') liveRatio is 20.488836662749705 (just-counted was 20.488836662749705).  calculation took 96ms for 144 columns
 INFO [GossipStage:1] 2013-02-14 10:01:12,119 Gossiper.java (line 831) InetAddress /127.0.0.3 is now dead.

==> logs/last/node2.log <==
 INFO [GossipStage:1] 2013-02-14 10:01:12,119 Gossiper.java (line 831) InetAddress /127.0.0.3 is now dead.
 INFO [GossipStage:1] 2013-02-14 10:01:12,238 Gossiper.java (line 817) InetAddress /127.0.0.3 is now UP
 INFO [GossipTasks:1] 2013-02-14 10:01:26,386 Gossiper.java (line 831) InetAddress /127.0.0.3 is now dead.

==> logs/last/node3.log <==
 INFO [StorageServiceShutdownHook] 2013-02-14 10:01:11,115 Gossiper.java (line 1134) Announcing shutdown
 INFO [StorageServiceShutdownHook] 2013-02-14 10:01:12,118 MessagingService.java (line 549) Waiting for messaging service to quiesce
 INFO [ACCEPT-/127.0.0.3] 2013-02-14 10:01:12,119 MessagingService.java (line 705) MessagingService shutting down server thread.
{noformat}

node2 receives the goodbye command from node3, and node1 has already marked node3 down, but some kind of signal is still coming from node3 to node2 marking it up again.",,,,,,,,,,,,,,,,,,,06/Mar/13 20:28;brandon.williams;5254-v2.txt;https://issues.apache.org/jira/secure/attachment/12572386/5254-v2.txt,05/Mar/13 19:40;brandon.williams;5254.txt;https://issues.apache.org/jira/secure/attachment/12572150/5254.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-06 04:51:42.357,,,no_permission,,,,,,,,,,,,312908,,,Wed Mar 06 22:10:48 UTC 2013,,,,,,0|i1hzyv:,313254,jasobrown,jasobrown,,,,,,,,,,"05/Mar/13 19:40;brandon.williams;This is a pernicious thing to debug, since the timing condition is so tight; enabling DEBUG or TRACE even on just the gossiper does not let it reproduce.  However, careful examination of the INFO messages tells us that handleMajorStateChange is not being called since there is no 'node restarted' message, which means applyStateLocally is the only other option, and that is called in the ack/ack2 handlers. This tells us that we're in the middle of a gossip round when we send the shutdown message, so the easiest thing to do is sleep for more than one round.  Trivial patch to do so, which has solved this on the dtests.",06/Mar/13 04:51;vijay2win@yahoo.com;+1,"06/Mar/13 20:28;brandon.williams;v2 adds just a touch more protection and ignores ack2 when gossip is disabled, which we were already doing in ack/syn.",06/Mar/13 21:48;jasobrown;+1 on v2,06/Mar/13 22:10;brandon.williams;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible assertion triggered in SliceFromReadCommand,CASSANDRA-5284,12633925,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,25/Feb/13 15:54,12/Mar/19 14:05,13/Mar/19 22:27,26/Feb/13 10:03,1.1.11,,,,,0,,,,,,,"In SliceFromReadCommand.maybeGenerateRetryCommand, the following assertion
{noformat}
assert maxLiveColumns <= count;
{noformat}
may actually fail. Namely, it asserts that no node has returned more columns that what was asked for, which in general is true, but can not be if an expiring column is counted as dead by the replica (but still send as a tombstone) but, due to clock difference, is actually counted live by the coordinator.

I note that this is similar to CASSANDRA-5149 in that fixing CASSANDRA-5149 would fix this too, but in the meantime, this edge case is harmless so there is probably not much point in keeping the assertion.",,,,,,,,,,,,,,,,,,,25/Feb/13 16:02;slebresne;5284.txt;https://issues.apache.org/jira/secure/attachment/12570806/5284.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-25 19:06:34.221,,,no_permission,,,,,,,,,,,,314419,,,Tue Feb 26 10:03:34 UTC 2013,,,,,,0|i1i9af:,314764,jbellis,jbellis,,,,,,,,,,25/Feb/13 19:06;jbellis;+1,"26/Feb/13 10:03;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PropertyFileSnitch default DC/Rack behavior is broken in 1.2,CASSANDRA-5285,12633955,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,25/Feb/13 17:46,12/Mar/19 14:05,13/Mar/19 22:27,01/Mar/13 16:05,1.2.3,,,,,0,,,,,,,"CASSANDRA-4728 added a check that the local node was in the property file but didn't took a potential default into account.

Typically, using the shipped cassandra-topology.properties (that has a default DC and rack) with a node on localhost raises a ConfigurationException on startup).",,,,,,,,,,,,,,,,,,,25/Feb/13 17:56;slebresne;5285.txt;https://issues.apache.org/jira/secure/attachment/12570825/5285.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-01 14:55:03.674,,,no_permission,,,,,,,,,,,,314449,,,Fri Mar 01 16:05:52 UTC 2013,,,,,,0|i1i9gv:,314793,jbellis,jbellis,,,,,,,,,,25/Feb/13 17:56;slebresne;Attaching simple fix.,01/Mar/13 14:55;jbellis;+1,"01/Mar/13 16:05;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatabaseDescriptor.hasExistingNoSystemTables return true even with only system keyspace,CASSANDRA-5289,12634066,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,cywjackson,cywjackson,26/Feb/13 05:43,12/Mar/19 14:05,13/Mar/19 22:27,07/Mar/13 21:18,,,,,,0,,,,,,,"The hasExistingNoSystemTables method in DatabaseDescriptor checks for directory only. 

On a new start, system KS would be created. This method current return true because of it, resulting incorrect/confusing log:

 logger.info(""Found table data in data directories. Consider using the CLI to define your schema."")     ;",,,,,,,,,,,,,,,,,,,26/Feb/13 05:47;cywjackson;5289.diff;https://issues.apache.org/jira/secure/attachment/12570933/5289.diff,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-04 13:18:28.926,,,no_permission,,,,,,,,,,,,314559,,,Thu Mar 07 21:18:03 UTC 2013,,,,,,0|i1ia5b:,314903,jbellis,jbellis,,,,,,,,,,"26/Feb/13 05:47;cywjackson;patch to add check the name not equal to Table.SYSTEM_KS

patch based on 1.1 branch

note for trunk, Table.SYSTEM_KS is changed to Table.SYSTEM_TABLE
","04/Mar/13 13:18;jasobrown;Honestly, you could debate whether we need that check at all. In either case, all we do after calling hasExistingNoSystemTables() is tell the user to create some CFs - I think that should be obvious by now :). I think we can just eliminate that method and the not overwhelmingly helpful logging, and have startup be nanoseonds more efficient.","07/Mar/13 01:29;cywjackson;I understand your point. 

We had a EC2 node outage last week, and subsequently the ops team rebuilt a node to replace it. 

However, I am not part of the ops team and so wasn't aware of such happened. But I was reviewing the logs for other issues, and reaching here was really confusing. (i saw node was started with new token indicating a new start, but then saw the log msg seems suggesting there were data).

Hence I would prefer keep the log but fix the bug to make it clear, instead of seeking for that sub nanoseconds improvement. But I would agree taking the log out would make it less confusing and gain that sub nanoseconds improvement.

Either way, your call :)","07/Mar/13 21:18;jbellis;I still see people copying their data files to a new cluster and wondering why Cassandra reports no table definitions.  So I'd prefer to fix the message than remove it.

Jackson's fix LGTM.  Committed, and in 1.2+ fixed the method name typo (should be Non-.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when executing a file contains CQL statement in cqlsh,CASSANDRA-5235,12631397,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,shamim_ru,shamim_ru,08/Feb/13 08:25,12/Mar/19 14:05,13/Mar/19 22:27,08/Feb/13 18:56,1.1.10,1.2.2,Legacy/Tools,,,0,,,,,,,"When executing a file contains CQL statement returns following error:
cqlsh> source '/tmp/src/xyz.cql';
Shell instance has no attribute 'cql_ver_tuple'","Redhat linux 5, windows 7",,,,,,,,,,,,,,,,,,08/Feb/13 16:24;iamaleksey;5235.txt;https://issues.apache.org/jira/secure/attachment/12568595/5235.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-08 16:52:37.334,,,no_permission,,,,,,,,,,,,311893,,,Sat Feb 09 12:03:24 UTC 2013,,,,,,0|i1htpb:,312239,brandon.williams,brandon.williams,,,,,,,,,,"08/Feb/13 16:52;iamaleksey;1.1.9 has the same indentation issue, but it doesn't actually affect SOURCE. Still, should probably be fixed there as well, just in case.",08/Feb/13 18:54;brandon.williams;+1,"09/Feb/13 12:03;shamim_ru;it works, thankx",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AnitEntropy/MerkleTree Error,CASSANDRA-5245,12631964,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,drohr,drohr,12/Feb/13 15:43,12/Mar/19 14:05,13/Mar/19 22:27,12/Mar/13 18:03,1.2.3,,,,,2,,,,,,,"We are seeing AntiEntropy errors when performing repair jobs in one of our Cassandra clusters. It seems to have started with 1.2. (maybe an issue with vnodes) The exceptions occur almost every time we try to do a repair on all column families in the cluster. Doing the same task on 1.1 does not trigger this.

6 nodes cluster (vnodes, murmur3, rf:3)
very low activity
running a nodetool repair -pr loop on the cluster nodes
nodetool hangs, and same big stacktrace in logs.

root 11025 0.0 0.0 106100 1436 pts/0 S+ Feb11 0:00 _ /bin/sh /usr/bin/nodetool -h HOST -p 7199 -pr repair KEYSPACE COLUMN_FAMILY

ERROR [AntiEntropyStage:3] 2013-02-11 17:08:12,630 CassandraDaemon.java (line 133) Exception in thread Thread[AntiEntropyStage:3,5,main]
java.lang.AssertionError
	at org.apache.cassandra.utils.MerkleTree.inc(MerkleTree.java:137)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:245)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.difference(MerkleTree.java:227)
	at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer.run(AntiEntropyService.java:982)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

This issue was partially solved earlier but seems to be back with vnodes: https://issues.apache.org/jira/browse/CASSANDRA-3014
",,,,,,,,,,,,,,,,,,,06/Mar/13 18:50;yukim;5245-1.2.txt;https://issues.apache.org/jira/secure/attachment/12572367/5245-1.2.txt,07/Mar/13 14:00;slebresne;5245-diffHelper.txt;https://issues.apache.org/jira/secure/attachment/12572536/5245-diffHelper.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-02-12 15:55:48.863,,,no_permission,,,,,,,,,,,,312460,,,Tue Mar 12 18:03:47 UTC 2013,,,,,,0|i1hx73:,312806,yukim,yukim,,,,,,,,,,"12/Feb/13 15:55;jbellis;That assertion is over 3 years old, so it's not as simple as ""1.2 added a bogus assert.""  (Which is not what you claimed, of course.)","12/Feb/13 16:11;drohr;What do you mean? That assertion is from my logs yesterday? Been able to repro it 3-4 times during the last couple of days. I couldn't repro it on 1.1. (at least not so far). If it is a common assertion during repairs, then it should not be marked as closed.

The question was asked in the Cassandra User list to open a new bug or just edit the old one. Thats why I created a new one. (http://www.mail-archive.com/user@cassandra.apache.org/msg27686.html)",27/Feb/13 13:50;anttiko;We have the same problem even with Cassandra 1.2.2. ,"06/Mar/13 03:39;joeyi;I have also run into this issue today.

1.2.2, Vnodes, Murmur3Partitioner

3 Nodes, RF3

All nodes have the following stacktrace 1+ times:

{noformat}
ERROR [AntiEntropyStage:10] 2013-03-06 01:46:21,159 CassandraDaemon.java (line 132) Exception in thread Thread[AntiEntropyStage:10,5,main]
java.lang.AssertionError
	at org.apache.cassandra.utils.MerkleTree.inc(MerkleTree.java:137)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:245)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.difference(MerkleTree.java:227)
	at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer.run(AntiEntropyService.java:983)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{noformat}

Another example of stacktrace:
{noformat}
ERROR [AntiEntropyStage:9] 2013-03-05 22:24:41,730 CassandraDaemon.java (line 132) Exception in thread Thread[AntiEntropyStage:9,5,main]
java.lang.AssertionError
        at org.apache.cassandra.utils.MerkleTree.inc(MerkleTree.java:137)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:245)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
{noformat}","06/Mar/13 13:55;joeyi;I am able to reproduce this after restarting the cluster and trying to run nodetool repair -pr (instead of just nodetool repair)

{noformat}
        
ERROR [AntiEntropyStage:8] 2013-03-06 07:56:24,794 CassandraDaemon.java (line 132) Exception in thread Thread[AntiEntropyStage:8,5,main]
java.lang.AssertionError
        at org.apache.cassandra.utils.MerkleTree.inc(MerkleTree.java:137)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:245)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.difference(MerkleTree.java:227)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer.run(AntiEntropyService.java:983)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{noformat}","06/Mar/13 14:23;yukim;I just look at the code and found this part: https://github.com/apache/cassandra/blob/cassandra-1.2.2/src/java/org/apache/cassandra/service/AntiEntropyService.java#L299

I don't test anything and I'm not sure this is related yet, but if we are using M3P, then we do manually splitting the tree based on the key samples between given range. We can have not enough splits initially.","06/Mar/13 15:51;slebresne;Using the key sample for M3P was definitively not the intention. That instanceof should be changed to a call to preserveOrder() really. I'm less surprised that the code for the ordering partition may get in an infinite recursion, though we should obviously fix it nonetheless.","06/Mar/13 18:50;yukim;Patch to fix as Sylvain's suggestion.
I think this is enough to fix AssertionError when using M3P.","07/Mar/13 02:42;mkjellman;+1 on patch.

Reproduced repair issue with ccm and stress pretty easily with 256 tokens and M3. Calling preserveOrder() fixes this.","07/Mar/13 14:00;slebresne;Yes, +1 on Yuki's patch because that's a problem. But this doesn't fix the fact that differenceHelper might recurse one time too much if the tree ends up being Byte.MAX_VALUE-1 deep (which can happen with the method used to build the tree for OrderPreservingPartitioner, which was mistakenly used for M3P). So attaching a 2nd patch that I believe should fix the recursion problem.
","07/Mar/13 15:55;yukim;+1. Sylvain, could you commit both?",12/Mar/13 17:49;joeyi;Any chance these fixes will make it into the 1.2.3 release?,"12/Mar/13 18:03;slebresne;Yep, committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-all 1.2.0 pom missing netty dependency,CASSANDRA-5181,12628827,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,carllerche,carllerche,23/Jan/13 00:20,12/Mar/19 14:05,13/Mar/19 22:27,04/Mar/13 17:27,1.2.3,,Packaging,,,0,,,,,,,"It seems that cassandra depends on netty now, however the pom excludes this dependency.",,,,,,,,,,,,,,,,,,,23/Jan/13 11:01;slebresne;5181.txt;https://issues.apache.org/jira/secure/attachment/12566113/5181.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-23 11:01:11.846,,,no_permission,,,,,,,,,,,,308305,,,Mon Mar 04 17:27:22 UTC 2013,,,,,,0|i1b1mn:,272562,dbrosius,dbrosius,,,,,,,,,,"23/Jan/13 11:01;slebresne;I think the attached patch fixes that (emphasis on the ""think"").",02/Mar/13 06:14;dbrosius;+1 lgtm,"04/Mar/13 17:27;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scripts fail when paths contain space,CASSANDRA-5338,12636639,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,amichai,amichai,12/Mar/13 19:31,12/Mar/19 14:05,13/Mar/19 22:27,08/Jul/13 19:33,,,Legacy/Tools,,,0,,,,,,,The shell scripts fail when the cassandra or java dirs contain spaces.,"Kubuntu 12.10 (GNU bash 4.2.37), and Windows XP (msysgit GNU bash 3.1.0)",,,,,,,,,,,,,,,,,,12/Mar/13 19:32;amichai;fix_spaces_in_paths.patch;https://issues.apache.org/jira/secure/attachment/12573396/fix_spaces_in_paths.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-13 15:32:55.32,,,no_permission,,,,,,,,,,,,317131,,,Mon Jul 08 19:33:46 UTC 2013,,,,,,0|i1ipzr:,317472,urandom,urandom,,,,,,,,,,12/Mar/13 19:32;amichai;The patch needs testing on other platforms.,"13/Mar/13 09:22;amichai;btw, this problem would be easily alleviated if there wasn't so much duplication in the scripts, some of which get updated and others forgotten... why not consolidate the duplication into a single master script? For backwards compatibility, the existing scripts can continue to exist but just call the master script with appropriate parameters.",13/Mar/13 15:32;brandon.williams;See CASSANDRA-5301 for the duplication.,01/Apr/13 22:52;jbellis;So this is redundant post-5301?,"02/Apr/13 06:50;amichai;The duplication thing is a related thought that could prevent the issue from creeping back again in the future, and making the fix easier, but the actual bug is the whitespace handling, which is unrelated to bug #5301 (which is resolved as wontfix in any case).","08/Jul/13 19:33;urandom;committed; thanks, and sorry for the long delay",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add a note to cassandra-cli ""show schema"" explaining that it is cql-oblivious",CASSANDRA-5309,12635109,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jayadevan.maymala@ibsplc.com,jayadevan.maymala@ibsplc.com,04/Mar/13 11:37,12/Mar/19 14:05,13/Mar/19 22:27,05/Mar/13 04:05,1.2.3,,Legacy/Tools,,,0,,,,,,,"If I create a table using cqlsh, that table  does not show when I do a describe in cassandra-cli. For example, I create a table in cqlsh.
cqlsh:system> CREATE KEYSPACE testkp WITH  replication =  {'class':'SimpleStrategy', 'replication_factor':2};

cqlsh:testkp> CREATE TABLE test (
          ...   k int PRIMARY KEY,
          ...   v1 int,
          ...   v2 int
          ... );
cqlsh:testkp>

In cassandra-cli, I get this 

[default@testkp] show schema;
create keyspace testkp
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = {replication_factor : 2}
  and durable_writes = true;

use testkp;




[default@testkp] describe;
Keyspace: testkp:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:2]
  Column Families:

No Column Family is shown. But if I do 
[default@testkp] create column family test;
Cannot add already existing column family ""test"" to keyspace ""testkp""
[default@testkp] list test;
Using default limit of 100
Using default column limit of 100
-------------------
RowKey: 1
=> (column=, value=, timestamp=1362395851188000)
=> (column=v1, value=00000002, timestamp=1362395851188000)
=> (column=v2, value=00000003, timestamp=1362395851188000)

1 Row Returned.
Elapsed time: 105 msec(s).

So obviously the table/column family is there.","Linus (CentOS), 64 bit.",,,,,,,,,,,,,,,,,,05/Mar/13 01:01;iamaleksey;5309.txt;https://issues.apache.org/jira/secure/attachment/12572006/5309.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-04 14:02:44.054,,,no_permission,,,,,,,,,,,,315602,,,Tue Mar 05 04:04:34 UTC 2013,,,,,,0|i1igkn:,315945,jbellis,jbellis,,,,,,,,,,04/Mar/13 14:02;jbellis;We should build a notice into the cli since this keeps coming up.,"04/Mar/13 14:23;karpa13a;created CF with cqlsh -3 not visible(and unusable) in thrift api (phpcassa and cassandra-cli)
but CF created in phpcassa/cassandra-cli visible and accessible in cqlsh -3 and phpcassa/cassandra-cli
C* 1.2.2

related to this ticket?
or this is feature?",04/Mar/13 14:26;iamaleksey;This is the intended behavior.,05/Mar/13 03:12;jbellis;+1,05/Mar/13 03:24;iamaleksey;Committed.,05/Mar/13 03:30;karpa13a;may be add this warning after 'create column family ...' statement in cqlsh -3 too?,"05/Mar/13 03:37;jayadevan.maymala@ibsplc.com;I am a newcomer in the world of NoSQL databases. Whether we create a column family using cli or a table using cqlsh, aren't the same things happening under the covers? So why is an object created using one interface not visible in the other?",05/Mar/13 03:42;iamaleksey;[~jayadevan.maymala@ibsplc.com] CASSANDRA-4377,"05/Mar/13 03:52;brandon.williams;Hang on, printing to stdout means you can no longer round-trip a 'show schema' through a file.  stderr seems a better choice.","05/Mar/13 04:04;iamaleksey;bq. Hang on, printing to stdout means you can no longer round-trip a 'show schema' through a file. stderr seems a better choice.

This is good - this way when schema fails to replay they'll definitely notice the warning.
Just kidding - fixed in b1abbf2d05d8cdf46c08a7a6e5289c9ade98dd3b. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests broken on Java7,CASSANDRA-5315,12635648,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,krummas,krummas,06/Mar/13 19:47,12/Mar/19 14:05,13/Mar/19 22:27,06/Mar/13 22:26,1.1.11,,Legacy/Testing,,,0,,,,,,,"Tests are broken when running them with java7

Seems to be related to this:
http://intellijava.blogspot.se/2012/05/junit-and-java-7.html
(getDeclaredMethods now returns the methods without any ordering, in java6 it was returned in the same order as they were defined in the .java file)",,,,,,,,,,,,,,,,,,,06/Mar/13 20:13;krummas;0001-fix-tests-for-java7.patch;https://issues.apache.org/jira/secure/attachment/12572381/0001-fix-tests-for-java7.patch,06/Mar/13 21:32;krummas;5315-cassandra1.1.patch;https://issues.apache.org/jira/secure/attachment/12572403/5315-cassandra1.1.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-06 22:26:04.605,,,no_permission,,,,,,,,,,,,316141,,,Wed Mar 06 22:26:04 UTC 2013,,,,,,0|i1ijwf:,316484,brandon.williams,brandon.williams,,,,,,,,,,"06/Mar/13 20:13;krummas;this adds @RunWith on all tests that were failing for me, there might be more",06/Mar/13 21:32;krummas;patch against 1.1,"06/Mar/13 22:26;brandon.williams;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift CQLPreparedResult don't include type arguments (for collection in particular),CASSANDRA-5311,12635162,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,04/Mar/13 17:31,12/Mar/19 14:05,13/Mar/19 22:27,04/Mar/13 17:56,1.2.3,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,04/Mar/13 17:34;slebresne;5311.txt;https://issues.apache.org/jira/secure/attachment/12571898/5311.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-04 17:42:10.191,,,no_permission,,,,,,,,,,,,315655,,,Mon Mar 04 17:56:58 UTC 2013,,,,,,0|i1igwf:,315998,jbellis,jbellis,,,,,,,,,,04/Mar/13 17:34;slebresne;Trivial patch attached.,04/Mar/13 17:42;jbellis;+1,"04/Mar/13 17:56;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hanging system after OutOfMemory. Server cannot die due to uncaughtException handling,CASSANDRA-5273,12633165,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,ignaced,ignaced,20/Feb/13 10:32,12/Mar/19 14:05,13/Mar/19 22:27,22/May/13 15:33,1.2.6,,,,,0,,,,,,,"On out of memory exception, there is an uncaughtexception handler that is calling System.exit(). However, multiple threads are calling this handler causing a deadlock and the server cannot stop working. See http://www.mail-archive.com/user@cassandra.apache.org/msg27898.html. And see stack trace in attachement.","linux, 64 bit",,,,,,,,,,,,,,,,,,24/Apr/13 10:00;krummas;0001-CASSANDRA-5273-add-timeouts-to-the-blocking-commitlo.patch;https://issues.apache.org/jira/secure/attachment/12580265/0001-CASSANDRA-5273-add-timeouts-to-the-blocking-commitlo.patch,24/Apr/13 07:44;krummas;0001-CASSANDRA-5273-add-timeouts-to-the-blocking-commitlo.patch;https://issues.apache.org/jira/secure/attachment/12580250/0001-CASSANDRA-5273-add-timeouts-to-the-blocking-commitlo.patch,22/May/13 13:59;jbellis;5273-v2.txt;https://issues.apache.org/jira/secure/attachment/12584307/5273-v2.txt,22/May/13 15:27;jbellis;5273-v3.txt;https://issues.apache.org/jira/secure/attachment/12584316/5273-v3.txt,20/Feb/13 10:46;ignaced;CassHangs.txt;https://issues.apache.org/jira/secure/attachment/12570109/CassHangs.txt,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2013-03-23 21:58:16.984,,,no_permission,,,,,,,,,,,,313661,,,Wed May 22 15:33:04 UTC 2013,,,,,,0|i1i4lz:,314006,krummas,krummas,,,,,,,,,,20/Feb/13 10:46;ignaced;Stack trace hanging system,23/Mar/13 21:58;jbellis;Do we have any better options than just adding a timeout to the appendingThread.join call?,24/Apr/13 07:44;krummas;this is very hard to reproduce (and i have never seen it happen in any of our clusters) so i simply added timeouts to the Thread#join methods in CommitLog#shutdownBlocking,"24/Apr/13 07:56;ignaced;Just an idea : one could say that the problem is caused by the java runtime that is holding a lock during System.exit(). At the same time, the cassandra code (the uncaught exception handler) is potentially calling System.exit() many times. Would it not be more safe and clean for the code in the handler to call at most once System.exit(), avoiding the jre lock and letting everything die in a 'normal' way?","24/Apr/13 10:00;krummas;adds a lock to make sure only one thread calls System.exit

the threads that would have called System.exit will block (unclear (to me) what would happen if they would be allowed to continue to run where they should have exited)","29/Apr/13 14:29;jbellis;bq. the threads that would have called System.exit

I don't think we're very rigorous about calling Thread.setDaemon, so I think this will actually deadlock it -- System.exit will wait for daemon threads to die, and the daemon threads will park at the lock acquisition.",21/May/13 15:48;krummas;[~jbellis] you think the timeouts would be enough?,"22/May/13 13:59;jbellis;Thinking about it more, I think adding a lock doesn't change anything.  System.exit already locks/synchronizes the important parts.  So we still have the deadlock problem, which we can hack around with timeouts but I'd rather not.

Patch attached against 1.2 to call System.exit from a new thread instead.",22/May/13 15:27;jbellis;v3 preallocates the Thread.,22/May/13 15:29;krummas;lgtm,22/May/13 15:33;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not insert an empty map.,CASSANDRA-5141,12626959,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,krzysztof cieslinski,krzysztof cieslinski,10/Jan/13 15:29,12/Mar/19 14:04,13/Mar/19 22:27,11/Jan/13 17:01,1.2.1,,,,,0,,,,,,,"It is not possible to insert an empty map. It looks like the ""{}"" is reserved only for Set.

So when for table:

{code}
CREATE TABLE users (
    id text PRIMARY KEY,
    surname text,
    favs map<text, text>
)
{code}

I try to insert map without any elements:

{code}
cqlsh:test> insert into users(id,surname,favs) values('aaa','aaa',{});
{code}

I get:

{code}
 Bad Request: Set operations are only supported on Set typed columns, but org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type) given.
text could not be lexed at line 1, char 63
{code}
",,,,,,,,,,,,,,,,,,,11/Jan/13 14:46;slebresne;5141.txt;https://issues.apache.org/jira/secure/attachment/12564410/5141.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-11 07:40:07.399,,,no_permission,,,,,,,,,,,,303651,,,Thu Feb 07 15:22:10 UTC 2013,,,,,,0|i17dhb:,251153,jbellis,jbellis,,,,,,,,,,"11/Jan/13 07:40;dbrosius;cql.g's 

set_literal and map_literal can't differentiate {} without outside knowledge, and so just choose set.","11/Jan/13 14:46;slebresne;Yes, the parser can't distinguish between empty set and empty map, so it always pick empty set and delegate the real choice to when we have type information. Now there used to be code that was handling that in UpdateStatement but it seems to have gone away (haven't found when but haven't look very hard).

Anyway, attaching code that adds back the code to handle that.","11/Jan/13 16:35;jbellis;s/differenciate/differentiate/

otherwise +1 :)","11/Jan/13 17:01;slebresne;bq. s/differenciate/differentiate/

Damn, my cover is blown. Committed with that fixed, thanks","07/Feb/13 09:23;marcinszymaniuk;Its letting me to execute the insert with an empty mup but that doesnt really inserting the map. Example:
{code:sql}insert into users(id,surname,favs) values('key123','justAString',{});
select * from users where id='key123';
{code}
{code}id     | favs | surname
--------+------+-------------
key123 | null | justAString
{code}
The question is- is this what you aggreed for? Its a bit misleading for me - inserting an empty map means that it has some semantic but I loose it. 

The MapOperation.doPut method doesnt seem to care about empty map at all - its just iterating through a map. If its empty it does nothing.","07/Feb/13 09:45;slebresne;In Cassandra, an empty map/set/list is equivalent to the column being null. In other words a collection only exists if it has elements. The reason for that semantic (versus having null being different from empty) is honestly mainly dictated by implementation concerns (internally a collection *does not* indeed exist unless it has elements). I.e. it's not because we think it is intrinsically better that way, but it is also true that, at least as far as I'm concern, I don't think it is intrinsically worst that way either.","07/Feb/13 15:22;marcinszymaniuk;gotcha, thanks Sylvain",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rewrite RandomAccessReader to use FileChannel / nio to address Windows file access violations,CASSANDRA-4050,12546471,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,JoshuaMcKenzie,jn,jn,14/Mar/12 20:22,12/Mar/19 14:04,13/Mar/19 22:27,07/Apr/14 20:52,2.2.0 beta 1,,Legacy/Local Write-Read Paths,,,1,Windows,,,,,,"On Windows w/older java I/O libraries the files are not opened with FILE_SHARE_DELETE.  This causes problems as hard-links cannot be deleted while the original file is opened - our snapshots are a big problem in particular.  The nio library and FileChannels open with FILE_SHARE_DELETE which should help remedy this problem.

Original text:
I'm using Cassandra 1.0.8, on Windows 7.  When I take a snapshot of the database, I find that I am unable to delete the snapshot directory (i.e., dir named ""{datadir}\{keyspacename}\snapshots\{snapshottag}"") while Cassandra is running:  ""The action can't be completed because the folder or a file in it is open in another program.  Close the folder or file and try again"" [in Windows Explorer].  If I terminate Cassandra, then I can delete the directory with no problem.

I expect to be able to move or delete the snapshotted files while Cassandra is running, as this should not affect the runtime operation of Cassandra.",Windows 7,,,,,,,,,,,,,CASSANDRA-6283,,,,,26/Mar/14 19:36;JoshuaMcKenzie;CASSANDRA-4050_v1.patch;https://issues.apache.org/jira/secure/attachment/12636982/CASSANDRA-4050_v1.patch,07/Apr/14 17:51;JoshuaMcKenzie;CASSANDRA-4050_v2.patch;https://issues.apache.org/jira/secure/attachment/12639025/CASSANDRA-4050_v2.patch,07/Apr/14 20:28;JoshuaMcKenzie;CASSANDRA-4050_v3.patch;https://issues.apache.org/jira/secure/attachment/12639053/CASSANDRA-4050_v3.patch,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-03-14 21:08:27.212,,,no_permission,,,,,,,,,,,,231629,,,Mon Apr 07 20:52:03 UTC 2014,,,,,,0|i05h8v:,29891,benedict,benedict,,,,,,,,,,"14/Mar/12 21:08;jbellis;Currently we take the snapshot using mklink /H, but I've experimented with the Java7 Files.createLink and see the same behavior.  It may simply be normal behavior for Windows that links are considered ""open"" until their creator is closed.

","15/Mar/12 04:31;jn;Yeah I'm starting to think that might be true -- unfortunately.  I haven't found anything definitive, but the following postings imply a hardlink cannot be deleted while another hardlink to the same file is locked:

http://superuser.com/questions/387136/is-it-possible-to-delete-a-hardlink-to-a-locked-file
http://superuser.com/questions/301303/one-hardlink-is-locked-how-do-i-remove-the-other

Perhaps the data structure that records the lock is in the file object and not the hardlink object.","05/Oct/12 23:00;akashirin;I have the same problem with Cassandra 1.1.5 on Windows 7 and Windows Server 2008 R2:

The nodetool stably creates snapshots that can't be deleted by the ""clearsnapshot"" command later (- IOException: Failed to delete ...). The bare facts are below.
 
For some (not for all) snapshot files (= hard links):
 
1) WinExplorer->Delete says ""The process cannot access the file because it is being used by another process"".
2) Command interpreter writes ""Access denied"" on ""del"" command.
3) Troubleshooting tools (- ""Process Explorer"" and ""Unlocker"") do not find open file handles. Moreover, Unlocker is able to delete these files without problems.
4) The files become deletable by the rest tools just after Cassandra-server has been stopped.
5) After restart, the same files become locked again.
6) Everything repeats after computer has been restarted.


","06/Oct/12 04:06;jbellis;Right, so NTFS is locking the underlying data since the ""original"" sstable is still open in Cassandra, so the behavior described by Jim applies.  We'd have to add a workaround like the one given on superuser.com -- move the links to a ""garbage"" location to clean up on restart.

This is pretty low priority for me but I'd be glad to point someone interested in the right direction.","25/Feb/14 21:51;JoshuaMcKenzie;There's been some discussion on CASSANDRA-6283 concerning this.  From the comments:

With our current io implementation on Windows, snapshots won't be deletable as long as the original sstable is locked. It's going to take a new FileDataInput based on FileChannel w/jdk7 using the FILE_SHARE_DELETE flag to allow deletion of hard links while the original file is open.

Bug with behavior: http://bugs.java.com/view_bug.do?bug_id=6607535
jdk7 support: http://www.docjar.com/html/api/sun/nio/fs/WindowsChannelFactory.java.html",14/Mar/14 17:44;JoshuaMcKenzie;I did some testing to confirm - nio.2 resolves both deleting hard-links and deleting of files with other handles currently open on Windows.  It should be straightforward to convert to a FileChannel and ByteBuffer in RAR and wrap the FileDataInput interface over to the ByteBuffer's.,"17/Mar/14 21:57;JoshuaMcKenzie;After changing RAF over to nio.2-based I'm still seeing snapshot deletion errors.  A little digging turned up:
http://bugs.java.com/view_bug.do?bug_id=4715154

As this applies to deleting memory mapped files but not necessarily hard-links with memory mapped segments in the original I figured I'd test it.  I've confirmed that if the original file has any data that's in a MappedByteBuffer even if the original RAF it was associated with is closed, Windows refuses to delete the hard-link.

Some local logging indicates that we have MmappedSegmentedFile Segments open on the original keyspaces in question when deletion is attempted which isn't surprising.  The following implies that there *might* be a way around this but it involves turning off all page caching for these files which isn't what we want for SSTableReaders on Windows: http://www.osronline.com/showThread.cfm?link=64732","18/Mar/14 15:53;JoshuaMcKenzie;Sanity checked - with nio.2 on RAR and bypassing MappedByteBuffers in MmappedSegmentedFile, snapshot-based repairs on Windows work and clean up without issue.  Not a solution long-term obviously but it helps support the hypothesis.

I think it makes sense to 1) convert RAR to nio.2 and 2) add a SnapshotDeletingTask similar to the SSTableDeletingTask to delete snapshot files once the original sstables are compacted and unmapped.  Alternatively we could allow snapshot files from repair to accrue and flag them for deletion on shutdown of the jvm and/or system reboot.",18/Mar/14 16:40;jbellis;I've said elsewhere that I'm fine with dropping the mapped i/o path in 3.0.  It just isn't used often enough (and the performance difference isn't large enough) to justify the extra complexity.,"18/Mar/14 18:00;JoshuaMcKenzie;I'd like to drop the mapped i/o separately from converting the RAR over to nio.2 (assuming we want to go that route).  You want me to open a new ticket for that and hammer that out as a pre-req to this?

edit: also - do we have perf #'s from when we added the memory mapped i/o into the code-path?","18/Mar/14 21:29;jbellis;bq. I'd like to drop the mapped i/o separately

WFM.

bq. do we have perf #'s from when we added the memory mapped i/o into the code-path

It was a long time ago (CASSANDRA-408, CASSANDRA-669).  It was a pretty big win at the time (10%?  20%?) because we were basically doing zero-copy reads from the mapped buffers.  The problem is that we had to give that up when we started manually unmapping obsolete sstables (CASSANDRA-2521) -- too hard to push refcounting all the way up the read path, so we gave up and just copy to a new buffer after all.","19/Mar/14 20:25;JoshuaMcKenzie;Makes sense - it would require pushing some things pretty far back in the stack to hold a ref on a memory mapped segment on the read path.

If we're thinking 3.X release for removing mmap I can throw a workaround on this ticket to always return a BufferedPoolingSegmentedFile from SegmentedFile's getBuilder if the platform is Windows.  That + nio.2 should get us working snapshots and less weird file handle behaviors on Windows in 2.0.X without having to wait on clean-up of the old mmap code.",19/Mar/14 21:01;jbellis;Rewriting to nio2 is the same scope as removing mmap.  Probably riskier actually.  So 3.0 for both.,"19/Mar/14 21:19;JoshuaMcKenzie;Fair enough.  RAR is only the root of all our file i/o, after all.  ;)  We should probably pursue either making snapshot deletion a quiet failure rather than deleteWithConfirm or disabling snapshot-based repair on Windows in 2.0.x.  I'm inclined to go with the latter since I'd rather not disrupt the *nix ecosystem based on Windows file-system eccentricities in our current stabilization-phase.",22/Mar/14 01:03;jbellis;Created CASSANDRA-6907 for that.,"26/Mar/14 19:35;JoshuaMcKenzie;Attaching 1st run at converting to nio.2.  Test results on both Windows and linux at blends of 3/1, 50/1, 100/1 write/read ratios and the inverse look to be within margin of error, though we're not getting any huge gains out of this change.  3/1 sample:
{code:title=3/1 w/r test numbers}
         4050 mmap 3/1 r/w:
                        id, ops       ,    op/s,adj op/s,   key/s,    mean,     med,     .95,     .99,    .999,     max,   time,   stderr
             4 threadCount, 934400    ,   31029,   31030,   31029,     0.1,     0.1,     0.2,     0.2,     0.7,    48.3,   30.1,  0.01072
             8 threadCount, 1259400   ,   41576,   41607,   41576,     0.2,     0.2,     0.2,     0.4,     1.1,    37.9,   30.3,  0.01139
            16 threadCount, 1478350   ,   48565,   48592,   48565,     0.3,     0.3,     0.5,     1.0,     7.0,    73.6,   30.4,  0.01197
            24 threadCount, 1523350   ,   49177,      -0,   49177,     0.5,     0.4,     0.7,     1.5,    19.1,    71.8,   31.0,  0.01668
            36 threadCount, 1518900   ,   48679,   48718,   48679,     0.7,     0.6,     1.1,     2.3,    22.6,    92.7,   31.2,  0.01425
            54 threadCount, 1541050   ,   48020,   48113,   48020,     1.1,     0.9,     1.8,     4.1,    28.6,   212.6,   32.1,  0.03217
         trunk mmap 3/1 r/w:
                        id, ops       ,    op/s,adj op/s,   key/s,    mean,     med,     .95,     .99,    .999,     max,   time,   stderr
             4 threadCount, 926400    ,   30764,   30765,   30764,     0.1,     0.1,     0.2,     0.2,     0.7,    24.3,   30.1,  0.00997
             8 threadCount, 1283250   ,   42495,      -0,   42495,     0.2,     0.2,     0.2,     0.3,     0.9,    44.4,   30.2,  0.01254
            16 threadCount, 1478250   ,   48509,      -0,   48509,     0.3,     0.3,     0.5,     0.9,     4.1,    68.0,   30.5,  0.00912
            24 threadCount, 1507900   ,   48553,   48594,   48553,     0.5,     0.4,     0.8,     1.7,    21.2,   132.1,   31.1,  0.01290
            36 threadCount, 1515150   ,   48079,      -0,   48079,     0.7,     0.6,     1.2,     2.7,    23.3,   103.8,   31.5,  0.01531
            54 threadCount, 1517600   ,   47826,      -0,   47826,     1.1,     0.9,     1.6,     3.2,    25.0,   194.4,   31.7,  0.01819
{code}

I mention mmap in these results as using BufferedPoolingSegmentedFiles on both trunk and on this patch had a noticeable negative impact on throughput, more on nio.2 than on the byte[] raw usage.  On trunk with read-heavy workloads I'm seeing anywhere from a 30-40% hit in stress results on read performance.  1/50 r/w ratio stress w/BufferedPoolingSegmentedFiles was still 16% slower than my testing using MmappedSegmentedFiles.  I'll be attaching a sample of the perf #'s I've been getting to CASSANDRA-6890.

I put some yammer timers inside the RAR code on both trunk and on this branch and it looks like #'s are comparable up to about the 60th percentile or so across all major read or rebuffer operations - then they balloon.  In the territory of a max timestamp of 100+ms on a simple channel seek vs. .01 on mmap'ed.  GC count during stress is roughly double at a glance - I'll look into that further on 6890 but heap stress due to more activity on the heap is to be expected.

As noted earlier - in order to fully resolve this issue, either CASSANDRA-6890 will need to be resolved or some alternative solution for Windows if we keep mmap'ing in.",27/Mar/14 16:24;jbellis;[~benedict] to review,"31/Mar/14 13:00;benedict;I've uploaded a tidied up version [here|https://github.com/belliottsmith/cassandra/tree/4050-nio2]

I've eliminated some unnecessary variables, simplified a couple of loops/conditions, and unified the AbstractDataInput/Small hierarchy. Also fixed a minor ""bug"" with getPosition() in RAR after close(), and CRAR now ensures that the current position is restored after rebuffer() - whilst currently this wouldn't cause any problems, it seems like an oversight.","31/Mar/14 17:50;JoshuaMcKenzie;Good catch on getPosition - I accounted for that in current() but that hadn't triggered on any testing and was an oversight.

I kept AbstractDataInput and AbstractDataInputSmall separate in the type heirarchy because I didn't want to push the int -> long signature change down to all the classes that implemented the base.  I'm not sure if the added footprint justifies the added complexity or not - I was trying to minimize changes to unrelated classes due to the loss of RAF code.  I didn't like it, but I also don'e like the alternative that much.  It looks like we run the risk of Bad Things if someone does a MemoryInputStream.skipBytes that pushes the position past Max Int - this impl has us casting off the remainder on a seek call so you could end up in negative territory.

As for the tidying up - looks good to me.  Thanks for taking the time to do that - clean idiomatic usage of the nio API's clearly makes things easier to parse.

Tests on linux look good, snapshots on Windows behave w/benedict's revisions and no mmap, and read performance looks comparable so I +1 the changes with the above caveat.","31/Mar/14 17:58;benedict;bq. It looks like we run the risk of Bad Things if someone does a MemoryInputStream.skipBytes that pushes the position past Max Int - this impl has us casting off the remainder on a seek call so you could end up in negative territory.

How so? The MemoryInputStream defines what its limit is, and the skipBytes method ensures it never goes above this. So seek() can never be called with a value that is out of range (since it is a protected method). We could put in an assert if we want to be doubly certain, however, and that's probably not a bad idea for simple declaration of intent.

I think the reduced code duplication (from readLine and skipBytes now being shared), and cleaner hierarchy is preferable, especially as ADISmall is not a very clear distinction from ADI. Think the overall footprint is reduced rather than increased...?

bq. Thanks for taking the time to do that - clean idiomatic usage of the nio API's clearly makes things easier to parse.

I find the NIO library tough to parse at the best of times, and wanted to be sure I was reading it right, so it was a freebie to change as I reviewed :)","31/Mar/14 18:16;JoshuaMcKenzie;{quote}
the skipBytes method ensures it never goes above this
{quote}

How is skipBytes protecting against blowing past our limit?  (note: me just being dense here is not out of the question)
{code:java, title=skipBytes}
 64     public int skipBytes(int n) throws IOException
 65     {
 66         if (n <= 0)
 67             return 0;
 68         seek(getPosition() + n);
 69         return position;
 70     }
{code}

It looks like this exposes seek() to the outside world with a protection against negative inputs but not much else.  That being said - the old code looks like it has the same potential problem:

{code:java, title=old code}
    public int skipBytes(int n) throws IOException
    {
        seekInternal(getPosition() + n);
        return position;
    }
{code}","31/Mar/14 18:19;benedict;Ah, this is my failure to delete the skipBytes method from MIS, as it now occurs in ADI (in a safe manner).","31/Mar/14 18:20;benedict;In fact, it looks like that is simply a bug that has always been present - the new behaviour is no worse than the old, but deleting it is still the correct fix.

Good spot.",31/Mar/14 18:23;JoshuaMcKenzie;Sure enough.  given the docs for skipBytes are 0-n bytes skipped I think the code in ADI looks good.  I'd much rather we not add more types to the hierarchy in this context.,31/Mar/14 21:12;benedict;Sounds like we're in agreement then? Any further changes you want to make after my update?,31/Mar/14 21:18;JoshuaMcKenzie;No other changes.  Depending on where we fall on CASSANDRA-6890 I may want to expand this ticket to cover using SSTableDeletingTasks on snapshot files to work around deleting memory mapped file segments on Windows in order to fix the issue that caused us to go down this path in the first place.  I don't like low-level I/O rewrites being under the guise of a Windows snapshot file ticket so I may do some housekeeping here.,"31/Mar/14 21:47;benedict;Rename/describe the ticket? :)

I had been thinking the same thing.",04/Apr/14 21:53;JoshuaMcKenzie;I'll rebase your branch against trunk and post a revised patch early next week.  I know how much you love rebasing and I figure I owe you one for the house-cleaning on this patch.  ;),07/Apr/14 17:52;JoshuaMcKenzie;New patch attached.  Passes the same tests trunk does and perf is in line.,"07/Apr/14 18:53;benedict;Might want to delete skipBytes from MIS as we discussed, otherwise LGTM and ready for commit.","07/Apr/14 20:09;JoshuaMcKenzie;Odd.  I wonder if I didn't pull down that [commit|https://github.com/josh-mckenzie/cassandra/commit/51cf8e74db1d452d99ac554b6666c86829376d91] before I rebased locally.  Odd since I thought I saw that on the difftool run;  I'll fix it and repost.
",07/Apr/14 20:28;JoshuaMcKenzie;v3 attached.  I removed the CommitLogTest changes that snuck on on the remove skipBytes from MIS commit from you as they looked unrelated to this ticket.,07/Apr/14 20:30;benedict;+1,07/Apr/14 20:52;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,
Make scrub and cleanup operations throttled,CASSANDRA-4100,12548569,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,28/Mar/12 22:42,12/Mar/19 14:04,13/Mar/19 22:27,11/Apr/12 00:49,1.1.1,1.2.0 beta 1,,,,0,compaction,,,,,,Looks like scrub and cleanup operations are not throttled and it will be nice to throttle else we are likely to run into IO issues while running it on live cluster.,,,,,,,,,,,,,,,,,,,09/Apr/12 23:38;vijay2win@yahoo.com;0001-CASSANDRA-4100-v2.patch;https://issues.apache.org/jira/secure/attachment/12522040/0001-CASSANDRA-4100-v2.patch,10/Apr/12 17:53;vijay2win@yahoo.com;0001-CASSANDRA-4100-v3.patch;https://issues.apache.org/jira/secure/attachment/12522138/0001-CASSANDRA-4100-v3.patch,03/Apr/12 19:15;vijay2win@yahoo.com;0001-CASSANDRA-4100.patch;https://issues.apache.org/jira/secure/attachment/12521194/0001-CASSANDRA-4100.patch,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-04-04 15:01:09.365,,,no_permission,,,,,,,,,,,,233666,,,Wed Apr 11 00:49:10 UTC 2012,,,,,,0|i0grz3:,95962,yukim,yukim,,,,,,,,,,"03/Apr/12 19:15;vijay2win@yahoo.com;Attached patch adds the throttle for cleanup and scrub. 
Note: compaction_throughput_mb_per_sec is used for the throttling, not sure if making a seperate property for cleanup/scrub is better....","04/Apr/12 15:01;yukim;Looks like patch is against trunk, though fix version is marked as 1.0.10.
Anyway, I think Throttle object in CompcationController should be non-static since compactions may run in parallel.","04/Apr/12 15:36;vijay2win@yahoo.com;>>> I think Throttle object in CompcationController should be non-static since compactions may run in parallel.
Exactly thats why static is better, Parallel compaction is not a problem per say (ParallelCompactionIterable.getReduced() will take care of it), but compaction running one after the other (lot of small compactions).

Let me know if everything else is ok i will rebase to 1.0.10 and move away from static (I am ok either ways), if needed. Thanks!","09/Apr/12 21:18;yukim;OK, so static Throttle is fine here since one compaction_throughput_mb_per_sec is used for all compactions. Then, do we need to divide that by number of active compactions? I'm referring the code inside the implementation of ThroughputFunction:

{code}
totalBytesPerMS / Math.max(1, CompactionManager.instance.getActiveCompactions());
{code}",09/Apr/12 23:38;vijay2win@yahoo.com;Fixed. Thanks!,"10/Apr/12 07:27;slebresne;bq. OK, so static Throttle is fine

I'll have to disagree. I'm pretty sure this patch break throttling. If more than one compaction share the Throttle object, they also share the Throttle.timeAtLastDelay field. Which means that as soon as there is more than 1 compaction running at any given time, the interval on which throttling is computing is bogus, and thus throttling will be bogus.

More generally, I'm -1 on changing code that does not have any known problem on the 1.0 branch (and as far as I know, current throttling works well) as 1.0 is getting really stable and we should start being conservative there (but I'd be fine with a patch that just add throttling to scrub and cleanup for 1.0).","10/Apr/12 07:37;slebresne;And to be clear, I'm not saying that throttling cannot be improved (it is true that currently it is too conservative and could throttle a thread even though the total throughput is below the threshold), but I'm saying that 1) this patch does not fix that correctly and 2) in any case this should be another ticket that shouldn't be targeted at 1.0.","10/Apr/12 17:53;vijay2win@yahoo.com;Alright, v3 simply adds throttle to scrub and cleanup... Simple refactor to move the throttle to compaction controller.","10/Apr/12 21:59;yukim;v3 looks good to me, but as Sylvain said, I'm +1 to put this to version 1.1.1 instead of 1.0.10.",11/Apr/12 00:49;vijay2win@yahoo.com;Committed to 1.1 and trunk. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal error processing get_slice (NullPointerException),CASSANDRA-4095,12548427,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jlaban,jlaban,28/Mar/12 03:09,12/Mar/19 14:04,13/Mar/19 22:27,30/Mar/12 15:24,1.0.9,1.1.0,,,,0,,,,,,,"I get this pretty regularly.  It seems to happen transiently on multiple nodes in my cluster, every so often, and goes away.


ERROR [Thrift:45] 2012-03-26 19:59:12,024 Cassandra.java (line 3041) Internal error processing get_slice
java.lang.NullPointerException
        at org.apache.cassandra.db.SliceFromReadCommand.maybeGenerateRetryCommand(SliceFromReadCommand.java:76)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:724)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:564)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:283)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:365)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:326)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:3033)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)



The line in question is (I think) the one below, so it looks like the column family reference for a row can sometimes be null?

    int liveColumnsInRow = row != null ? row.cf.getLiveColumnCount() : 0;



Here is my column family (on 1.0.8):

    ColumnFamily: WorkQueue (Super)
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type/org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Row Cache Provider: org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider
      Key cache size / save period in seconds: 0.0/0
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.0
      Replicate on write: false
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
","Java(TM) SE Runtime Environment (build 1.6.0_30-b12)
",,,,,,,,,,,,,,,,,,29/Mar/12 18:45;jbellis;4095.txt;https://issues.apache.org/jira/secure/attachment/12520481/4095.txt,30/Mar/12 08:28;slebresne;4095_v2.txt;https://issues.apache.org/jira/secure/attachment/12520581/4095_v2.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-03-29 18:45:19.694,,,no_permission,,,,,,,,,,,,233524,,,Fri Mar 30 15:24:21 UTC 2012,,,,,,0|i0grx3:,95953,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"29/Mar/12 18:45;jbellis;Thanks for the report, John.  I think your analysis is spot on.  Patch attached that does not assume row.cf is non-null.",29/Mar/12 18:56;vijay2win@yahoo.com;+1,"30/Mar/12 08:28;slebresne;The patch doesn't compile, it's calling a method that doesn't exist. Attaching v2 that I think adds the intended method.","30/Mar/12 14:06;jbellis;I wrote the patch against 1.1, which already has that method.  Sorry for not being clear.","30/Mar/12 14:52;vijay2win@yahoo.com;I Should have also verified against 1.0, thanks!","30/Mar/12 15:24;slebresne;Alright, I've committed v2 to 1.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mlockall() returned code is ignored w/o assertions,CASSANDRA-4096,12548428,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,scode,scode,28/Mar/12 03:13,12/Mar/19 14:04,13/Mar/19 22:27,30/Mar/12 15:41,1.0.9,1.1.0,,,,0,jna,,,,,,"We log that mlockall() was successful only based on the lack of an assertion failure, so for anyone running w/o {{-ea}} we are lying about mlockall() succeeding.",,,,,,,,,,,,,,,,,,,29/Mar/12 18:08;jbellis;4096.txt;https://issues.apache.org/jira/secure/attachment/12520463/4096.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-29 18:08:34.014,,,no_permission,,,,,,,,,,,,233525,,,Fri Mar 30 15:41:53 UTC 2012,,,,,,0|i0grxj:,95955,scode,scode,,,,,,,,,,29/Mar/12 18:08;jbellis;The assert is redundant anyway since JNA will check return value and errno for us. Patch attached.,"30/Mar/12 02:42;scode;Good point. If that's the behavior of JNA, then definitely +1.",30/Mar/12 15:41;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IncomingTCPConnection recognizes from by doing socket.getInetAddress() instead of BroadCastAddress,CASSANDRA-4099,12548549,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,28/Mar/12 20:54,12/Mar/19 14:04,13/Mar/19 22:27,06/Jun/12 02:23,1.0.9,1.1.0,,,,0,,,,,,,"change ""this.from = socket.getInetAddress()"" to understand the broad cast IP, but the problem is we dont know until the first packet is received, this ticket is to work around the problem until it reads the first packet.",,,,,,,,,,,,,,,,,,,28/Mar/12 23:48;vijay2win@yahoo.com;0001-CASSANDRA-4099-v2.patch;https://issues.apache.org/jira/secure/attachment/12520343/0001-CASSANDRA-4099-v2.patch,29/Mar/12 18:39;vijay2win@yahoo.com;0001-CASSANDRA-4099-v3.patch;https://issues.apache.org/jira/secure/attachment/12520479/0001-CASSANDRA-4099-v3.patch,30/Mar/12 01:33;vijay2win@yahoo.com;0001-CASSANDRA-4099-v4.patch;https://issues.apache.org/jira/secure/attachment/12520546/0001-CASSANDRA-4099-v4.patch,28/Mar/12 21:59;vijay2win@yahoo.com;0001-CASSANDRA-4099.patch;https://issues.apache.org/jira/secure/attachment/12520332/0001-CASSANDRA-4099.patch,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-03-28 23:00:42.739,,,no_permission,,,,,,,,,,,,233646,,,Wed Jun 06 02:23:36 UTC 2012,,,,,,0|i0gryn:,95960,brandon.williams,brandon.williams,,,,,,,,,,"28/Mar/12 23:00;brandon.williams;This doesn't look like a perfect solution since all nodes will have to stream to all other nodes in order to learn the correct version, and thus be able to use newer-version features.  I'm not sure there's currently a way around this, though.  I created CASSANDRA-4101 to get us started there, but I'll look more closely here tomorrow.","28/Mar/12 23:06;vijay2win@yahoo.com;Thanks Brandon, CASSANDRA-4101 looks like a better solution but not only does the Streaming sets the version Gossip or any connunication does set it, the following does it

{code} 
            from = msg.getFrom(); // why? see => CASSANDRA-4099
            if (version > MessagingService.current_version)
            {
                // save the endpoint so gossip will reconnect to it
                Gossiper.instance.addSavedEndpoint(from);
                logger.info(""Received "" + (isStream ? ""streaming "" : """") + ""connection from newer protocol version. Ignoring"");
            }
            else if (msg != null)
            {
                Gossiper.instance.setVersion(from, version);
                logger.debug(""set version for {} to {}"", from, version);
            }
{code} ",28/Mar/12 23:11;brandon.williams;Won't 'from' still always be wrong in your configuration unless streaming occurs?,"28/Mar/12 23:16;vijay2win@yahoo.com;Nope there is 2 places where we set the from one is during the stream just before we start. the second place is when it is not a stream and when we recive the first message so we should be ok and should never be null.

1)

{code}
                    // why? see => CASSANDRA-4099
                    from = streamHeader.broadcastAddress;
                    stream(streamHeader, input);
{code}

2)

{code}

Message msg = receiveMessage(input, version);
            from = msg.getFrom(); // why? see => CASSANDRA-4099
{code}

Hope it makes sense.","28/Mar/12 23:22;brandon.williams;I think I see, in your situation the version is correct for everything except streaming, hence 1)?  It seems like the problem here is it will still accept streams from a lesser version, which is always version-specific.","28/Mar/12 23:28;vijay2win@yahoo.com;I think we can remove

{code}
                if (version <= MessagingService.current_version)
                {
                    int size = input.readInt();
                    byte[] headerBytes = new byte[size];
                    input.readFully(headerBytes);
                    StreamHeader streamHeader = StreamHeader.serializer().deserialize(new DataInputStream(new FastByteArrayInputStream(headerBytes)), version);
                    // why? see => CASSANDRA-4099
                    from = streamHeader.broadcastAddress;
{code}

Because the version will be already set by the Gossip hopefully :) but I still dont understand why we streaming needs to be version specific though.","28/Mar/12 23:48;vijay2win@yahoo.com;IRC Brandon explained the changes in flowcontrol which will not allow us to stream data from other versions, v2 removes those changes from v1","29/Mar/12 14:22;brandon.williams;I'm confused, how does 'from' differ from 'msg.getFrom' in this patch?  It seems like a no-op.","29/Mar/12 15:25;vijay2win@yahoo.com;getFrom gets the IP from the message header and the message header is set by the caller, which uses FB.BCA and the receiving machine sets it. Plz look at  Message.getInternalReply for example. Thanks!","29/Mar/12 16:13;brandon.williams;I see, it's injecting another getFrom call.  +1 (though this version only applies to trunk)","29/Mar/12 17:56;jbellis;I still don't get it.

{noformat}
+            from = msg.getFrom(); // why? see => CASSANDRA-4099
-                Gossiper.instance.setVersion(msg.getFrom(), version);
+                Gossiper.instance.setVersion(from, version);
{noformat}

How is the first getFrom, not the same as the one we've de-inlined?","29/Mar/12 18:19;brandon.williams;It overrides where from is set in the constructor:

{code}
this.from = socket.getInetAddress();
{code}","29/Mar/12 18:22;jbellis;I see.

Looking at the usages of ITC.from, it looks like we can drop that constructor initialization entirely...","29/Mar/12 18:27;brandon.williams;I agree, we should get out of the habit of examining sockets directly due to broadcast_address.","29/Mar/12 18:39;vijay2win@yahoo.com;Attached version incorporate's the comments... Note that the streaming a file will not remove or add the version, which i think is a better option IMHO. Plz let me know if you think otherwise. Thanks!","29/Mar/12 18:50;jbellis;+1 on v3 for 1.0+

I'm fine w/ continuing to ignore version on streaming for now",29/Mar/12 23:51;vijay2win@yahoo.com;Committed to 1.0 and trunk (as CASSANDRA-4101 is closed).,30/Mar/12 01:19;vijay2win@yahoo.com;Looks like there is a case we will run into NPE when the other node which is sending a higher version we ignore it but the problem the msg object is not available.,30/Mar/12 01:26;vijay2win@yahoo.com;v4 is on top of the v3 and it fixes the NPE,30/Mar/12 16:20;brandon.williams;+1,30/Mar/12 18:51;vijay2win@yahoo.com;Committed back Thanks!,"05/Jun/12 22:50;jbellis;I think this is still broken.  if A forwards a message from B to C, then {{Gossiper.instance.setVersion(from, version)}} C will mark B's version (B == from) to the version that A sent.  But A sends its own version, not B's.  (Which is correct for the purpose of message forwarding, since A re-serializes instead of passing what B sent verbatim.)

I don't think we can accommodate both message forwarding, and broadcast address != socket address, without a protocol change to include a ""here is my reply-to broadcast_address"" piece of information when the connection is first established, distinct from Message.from.","05/Jun/12 23:37;vijay2win@yahoo.com;Hi Jonathan, IMO the right fix for it with a similar approach as in the ticket CASSANDRA-4101 (we can also remove the ConcurrentMap.get() to compare the versions), Agree?",05/Jun/12 23:45;brandon.williams;I agree that something like CASSANDRA-4101 is the right way to do this,06/Jun/12 02:23;jbellis;I kind of think we've reached the limits of what we can do by band-aiding this.  Opened CASSANDRA-4311 for a deeper fix.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bogus MemoryMeter liveRatio calculations,CASSANDRA-4065,12547162,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,doubleday,doubleday,doubleday,20/Mar/12 08:39,12/Mar/19 14:04,13/Mar/19 22:27,18/Apr/12 23:14,1.1.0,,,,,0,,,,,,,"I get strange cfs.liveRatios.

A couple of mem meter runs seem to calculate bogus results: 

{noformat}
Tue 09:14:48 dd@blnrzh045:~$ grep 'setting live ratio to maximum of 64 instead of' /var/log/cassandra/system.log
 WARN [MemoryMeter:1] 2012-03-20 08:08:07,253 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:09,160 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:13,274 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:22,032 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:12:41,057 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 67.11787351054079
 WARN [MemoryMeter:1] 2012-03-20 08:13:50,877 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 112.58547951925435
 WARN [MemoryMeter:1] 2012-03-20 08:15:29,021 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 193.36945063589877
 WARN [MemoryMeter:1] 2012-03-20 08:17:50,716 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 348.45008340969434
{noformat}

Because meter runs never decrease liveRatio in Memtable (Which seems strange to me. If past calcs should be included for any reason wouldn't averaging make more sense?):

{noformat}
cfs.liveRatio = Math.max(cfs.liveRatio, newRatio);
{noformat}

Memtables are flushed every couple of secs:

{noformat}
ColumnFamilyStore.java (line 712) Enqueuing flush of Memtable-BlobStore@935814661(1874540/149963200 serialized/live bytes, 202 ops)
{noformat}

Even though a saner liveRatio has been calculated after the bogus runs:

{noformat}
INFO [MemoryMeter:1] 2012-03-20 08:19:55,934 Memtable.java (line 198) CFS(Keyspace='SmeetBlob', ColumnFamily='BlobStore') 
   liveRatio is 64.0 (just-counted was 2.97165811895841).  calculation took 124ms for 58 columns
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-03-20 15:43:27.732,,,no_permission,,,,,,,,,,,,232320,,,Wed Apr 18 23:14:34 UTC 2012,,,,,,0|i0grjz:,95894,jbellis,jbellis,,,,,,,,,,"20/Mar/12 15:43;jbellis;bq. meter runs never decrease liveRatio in Memtable, which seems strange to me. If past calcs should be included for any reason wouldn't averaging make more sense?

I'm open to ways to improve this, but the idea is that the penalty for overestimating how big the memtable is (frequent flushes) is less severe than the penalty for underestimating (running out of memory and dying).",20/Mar/12 15:44;jbellis;I wonder if we could use some kind of constantly-updated estimate instead of scanning the whole memtable periodically...,"20/Mar/12 17:02;doubleday;Maybe a simple solution would suffice: 

Don't use jamm but do some simplified estimation as 

estimate_size = $raw_size + $row_count * ROW_OVERHEAD + $column_count * COL_OVERHEAD

Since we know the used data structures ... and to keep it easy use 64b overhead since this will be the usual case anyway.

","20/Mar/12 17:30;jbellis;That's probably possible in theory, but there isn't a fixed-size overhead for structures like CLHM.","20/Mar/12 21:55;doubleday;bq. That's probably possible in theory, but there isn't a fixed-size overhead for structures like CLHM.

Well yes - I though of an approximation.

But maybe even easier (still conservative but able to heal):

{noformat}
if (newRatio > cfs.liveRatio) {
    cfs.liveRatio = newRatio;
} else {
    cfs.liveRatio = (cfs.liveRatio + newRatio) / 2.0;
}
{noformat}

 ","18/Apr/12 23:14;jbellis;sounds reasonable to me.  committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
decom should shut thrift down,CASSANDRA-4086,12548228,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,27/Mar/12 02:12,12/Mar/19 14:04,13/Mar/19 22:27,27/Mar/12 17:31,1.0.9,1.1.0,,,,0,,,,,,,"If you decom a node an then try to use it, you get nothing but timeouts.  Instead let's just kill thrift so intelligent clients can move along.",,,,,,,,,,,,,,,,,,,27/Mar/12 16:28;brandon.williams;4086.txt;https://issues.apache.org/jira/secure/attachment/12520145/4086.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-27 16:32:08.955,,,no_permission,,,,,,,,,,,,233325,,,Tue Mar 27 17:31:38 UTC 2012,,,,,,0|i0grsv:,95934,jbellis,jbellis,,,,,,,,,,27/Mar/12 16:28;brandon.williams;Trivial patch to shut thrift down just before gossip/MS.,27/Mar/12 16:32;jbellis;should we just have it call drain() instead of partially reimplementing it?,"27/Mar/12 16:56;brandon.williams;I've tried that, but it feels like it makes the logic for decom much less clear, and has a side effect that drain shuts the node down, which we don't want.",27/Mar/12 17:10;jbellis;Why don't we want that?,"27/Mar/12 17:16;brandon.williams;Historically I think the reasoning is you may have packaging that automatically restarts the process, which is something you don't really want with decom, but isn't a huge problem for drain.  David apparently ran into this problem on CASSANDRA-1483.","27/Mar/12 17:20;jbellis;WFM then, +1",27/Mar/12 17:31;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflowError when upgrading to 1.0.8 from 0.8.10,CASSANDRA-4078,12547796,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,wenjun@openf.in,wenjun@openf.in,23/Mar/12 13:44,12/Mar/19 14:04,13/Mar/19 22:27,02/Apr/12 21:36,1.0.10,,,,,0,,,,,,,"Hello

I am trying to upgrade our 1-node setup from 0.8.10 to 1.0.8 and seeing the following exception when starting up 1.0.8.  We have been running 0.8.10 without any issues.
 
Attached is the entire log file during startup of 1.0.8.  There are 2 exceptions:

1. StackOverflowError (line 2599)
2. InstanceAlreadyExistsException (line 3632)

I tried ""run scrub"" under 0.8.10 first, it did not help.  Also, I tried dropping the column family which caused the exception, it just got the same exceptions from another column family.

Thanks
","OS: Linux xps.openfin 2.6.35.13-91.fc14.i686 #1 SMP Tue May 3 13:36:36 UTC 2011 i686 i686 i386 GNU/Linux

Java: JVM vendor/version: Java HotSpot(TM) Server VM/1.6.0_31
",,,,,,,,,,,,,,,,,,02/Apr/12 20:55;jbellis;4078-asserts-v3.txt;https://issues.apache.org/jira/secure/attachment/12521035/4078-asserts-v3.txt,23/Mar/12 20:34;thepaul;4078.add-asserts.txt;https://issues.apache.org/jira/secure/attachment/12519707/4078.add-asserts.txt,02/Apr/12 20:41;thepaul;4078.patch2.txt;https://issues.apache.org/jira/secure/attachment/12521032/4078.patch2.txt,26/Mar/12 22:46;wenjun@openf.in;cassandra.yaml.1.0.8;https://issues.apache.org/jira/secure/attachment/12520027/cassandra.yaml.1.0.8,26/Mar/12 22:46;wenjun@openf.in;cassandra.yaml.8.10;https://issues.apache.org/jira/secure/attachment/12520028/cassandra.yaml.8.10,30/Mar/12 20:33;wenjun@openf.in;keycheck.txt;https://issues.apache.org/jira/secure/attachment/12520664/keycheck.txt,23/Mar/12 13:46;wenjun@openf.in;system.log;https://issues.apache.org/jira/secure/attachment/12519621/system.log,26/Mar/12 16:57;wenjun@openf.in;system.log.0326;https://issues.apache.org/jira/secure/attachment/12519973/system.log.0326,26/Mar/12 18:20;wenjun@openf.in;system.log.0326-02;https://issues.apache.org/jira/secure/attachment/12519987/system.log.0326-02,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,2012-03-23 16:14:11.991,,,no_permission,,,,,,,,,,,,232893,,,Mon Apr 02 21:36:31 UTC 2012,,,,,,0|i0grpj:,95919,jbellis,jbellis,,,,,,,,,,23/Mar/12 13:46;wenjun@openf.in;startup log from version 1.0.8,23/Mar/12 16:14;jbellis;Looks like a problem w/ the intervaltree creation.,"23/Mar/12 20:34;thepaul;Wenjun, would it be possible to run your 1.0.8 version again (with DEBUG logging, as before) with this patch applied?

I haven't been able to figure out yet how this could get into an infinite recursion, but if we can identify an invalid assumption somewhere, that ought to help a lot.

If it's easier to get at, this change is also available in my github fork at git@github.com:thepaul/cassandra.git (branch named 4078).","26/Mar/12 16:56;wenjun@openf.in;Paul
when I tried running code from git@github.com:thepaul/cassandra.git and I am getting the following errors.  The only changes I made in cassandra.yaml are path to data files and local IP address.

java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.

I don't really use git so I may have done something wrong to get your code.  The command I used is ""git clone git@github.com:thepaul/cassandra.git"".  I noticed your changes in IntervalNode.java were not included in the cloned code.  I copied the changes from 4078.add-aserts.txt.  ant was able to build without any issues.  How do I make sure the new classe files are included in cassandra runtime?

Attached is the complete log for startup.

Thanks
Wenjun
","26/Mar/12 17:20;thepaul;Ah, you got trunk by default (unstable code for future Cassandra 1.2). You need to do ""git checkout 4078"" in that directory to switch to the 4078 branch.","26/Mar/12 18:19;wenjun@openf.in;I am able to download the branch from github site.  Attached is the log file.

Thanks","26/Mar/12 18:50;thepaul;Ok. It looks like there are indeed 6 places where something is trying to create an Interval where the min is  greater than the max. The following are taken from your log, right before the AssertionError, with some spaces added and other valid intervals cut out:

{noformat}
Interval(DecoratedKey(f3f31fef-9b24-4fe3-be8e-b54a43ccf867, f3f31fef9b244fe3be8eb54a43ccf867),
         DecoratedKey(62476171-4693-4a81-bd7c-01726bfca4c1, 6247617146934a81bd7c01726bfca4c1)),
Interval(DecoratedKey(c60e0fbc-dcc6-472c-8d5c-8b3cd041e4c9, c60e0fbcdcc6472c8d5c8b3cd041e4c9),
         DecoratedKey(7dd77e1f-92e0-4bc7-a417-2f9b0b97170d, 7dd77e1f92e04bc7a4172f9b0b97170d)),
Interval(DecoratedKey(920c1919-98c4-4da8-88da-325067f9ece3, 920c191998c44da888da325067f9ece3),
         DecoratedKey(40d6f31d-4b37-492e-b06c-9de0088de83f, 40d6f31d4b37492eb06c9de0088de83f)),
Interval(DecoratedKey(a5f8d3d8-d26a-47fd-904c-8d55784776fc, a5f8d3d8d26a47fd904c8d55784776fc),
         DecoratedKey(762f9402-e5f6-44bb-824c-f93170c2f508, 762f9402e5f644bb824cf93170c2f508)),
Interval(DecoratedKey(a27d2f3b-8255-42fb-a8d5-5f0fe488dadb, a27d2f3b825542fba8d55f0fe488dadb),
         DecoratedKey(7582809c-34b3-4736-8dfe-19811303bbfa, 7582809c34b347368dfe19811303bbfa)),
Interval(DecoratedKey(d72c5963-ebe8-4e13-a74b-5584ca1d053c, d72c5963ebe84e13a74b5584ca1d053c),
         DecoratedKey(4962307f-261d-469a-b48b-1de7cd7a7700, 4962307f261d469ab48b1de7cd7a7700)),
{noformat}

This shouldn't ever happen, so I'll do some poking around to see where these come from.","26/Mar/12 22:24;thepaul;These values come right from the SSTables. I've been looking through code to see if I can find any possibility of bad values being written to an SSTable, but there might be a simpler explanation:

Is there any possibility that you have configured a different partitioner for Cassandra 1.0.8 from the one you used when writing the SSTables (0.8.10, I suppose)?",26/Mar/12 22:47;wenjun@openf.in;I just attached yaml files for both 0.8.10 and 1.0.8 (from your branch).,"27/Mar/12 02:05;thepaul;Ok, looks like both are RandomPartitioner. I'll keep investigating.","29/Mar/12 20:52;wenjun@openf.in;Paul, 

so you know: I just tried 1.1.0-beta2 and seeing the same error.","29/Mar/12 22:24;thepaul;Yeah, the problem is almost certainly either (a) some important difference in the way it's being used now versus the way it was used before, or (b) some sort of bug in 0.8.10. I don't think 1.0.8 or 1.1.0 are acting improperly given the data and configuration here; I'm just hoping I can help you sort out whatever datatype expectation mismatch or invalid sstables you have.","30/Mar/12 00:02;thepaul;Wenjun, are you able to tell from the UUID keys shown which ColumnFamily is having these troubles? I can't quite tell for sure from the logs. If necessary, we can add more asserts to identify the right place, but we can skip that step if you already know which one it is.","30/Mar/12 16:21;wenjun@openf.in;Paul, yes, finally I am able to narrow it down to this one CF.  I truncated data from all other CFs, ran 'cleanp' and 'scrub', and it is still happening.  What else can I do to help debugging this issue?  The followings are definition of this CF:

    ColumnFamily: UserAgreementStatus
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Row Cache Provider: org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.45468749999999997/1440/97 (millions of ops/minutes/MB)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: [UserAgreementStatus.userId]
      Column Metadata:
        Column Name: agreementName
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: response
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: userId
          Validation Class: org.apache.cassandra.db.marshal.LexicalUUIDType
          Index Name: userId
          Index Type: KEYS
        Column Name: viewTime
          Validation Class: org.apache.cassandra.db.marshal.LongType

Log right before the exception:
DEBUG [SSTableBatchOpen:2] 2012-03-30 12:09:20,277 SSTableReader.java (line 190) INDEX LOAD TIME for /home/cassandra/var/lib/cassandra/d
ata/Appoji/UserAgreementStatus.userId-f-56: 3 ms.
DEBUG [SSTableBatchOpen:2] 2012-03-30 12:09:20,277 SSTableReader.java (line 193) key cache contains 0/0 keys


","30/Mar/12 19:23;thepaul;Wenjun- it would help at this point to be sure whether or not your sstables really do have keys which are out of order. Could you run the sstablekeys tool on the data file for that UserAgreementStatus CF? It looks like it is at {{/home/cassandra/var/lib/cassandra/data/Appoji/UserAgreementStatus/Appoji-UserAgreementStatus.userId-g-*-Data.db}}. We want to check that the byte sequences are in order:

{noformat}
$ for s in /home/cassandra/var/lib/cassandra/data/Appoji/UserAgreementStatus/Appoji-UserAgreementStatus.userId-g-*-{Data,Index}.db; do
>     echo ""checking $s""
>     bin/sstablekeys ""$s"" | sort -c
> done
{noformat}

If that doesn't give any output, then maybe there really is a bug in the 1.0/1.1 code that we need to look at.","30/Mar/12 20:33;wenjun@openf.in;Paul, attache is results from sstablekeys

Thanks","30/Mar/12 21:33;thepaul;Ok, so there are definitely out-of-order keys in your sstables. I have no good theories as to how that happened- I don't think anyone else has run into this. Were you possibly using any sort of custom types or parameterized types with 0.8.9? Custom anything?

Either way, the thing to be done now is probably to get your data sorted. Maybe we can come up with a one-off tool of some kind to read the values and write them back out in a sorted way.","30/Mar/12 22:35;yukim;I don't have a clue about the cause, but since corrupted files are index column families, I think work around is to remove all those corrupted index sstables, upgrade C*, then rebuild index using 'nodetool rebuild_index'.","02/Apr/12 14:37;thepaul;Yuki- there is an index sstable with out-of-order keys, and that one could be fixed as you suggest, but the actual data sstable also has out-of-order keys.","02/Apr/12 15:34;wenjun@openf.in;I am able to fix the issue by doing the followings:

1. ran original command for creating the CF (replacing create with update) in cassandra-cli, got an error ""cannot modify index name"". (I looked it up in Cassandra mailing list and did not found any reference, so not sure what it means).

2. ran the command again WITHOUT index in column_metadata.

3. ran the command WITH index in column_metadata.

Now I am able to start 1.0.8 and 1.1.0-beta2.  I double-checked that CF and it seems all the data and index are fine.  So, let me know if I need to check anything else.

Thanks


","02/Apr/12 15:46;thepaul;Oh, good. I'm glad that was doable.

Before we close the ticket, is there anything at all from your experience or Cassandra usage that may have been unusual- hardware, platform, storage method, custom data or index types, etc? I would really like at least to get some clues about how this happened, if possible.","02/Apr/12 16:37;wenjun@openf.in;
I found something over the weekend: it seems 0.8.10 does not allow duplicate index names for different CFs.  We do have CFs having same name (such as userId) for their indexes, which means some earlier version must have allowed it (we started from 0.7.4).  Can this be the cause?

","02/Apr/12 16:37;wenjun@openf.in;
I found something over the weekend: it seems 0.8.10 does not allow duplicate index names for different CFs.  We do have CFs having same name (such as userId) for their indexes, which means some earlier version must have allowed it (we started from 0.7.4).  Can this be the cause?

","02/Apr/12 20:41;thepaul;Ok. We'll keep that info around in case we ever see something like this again. Thanks.

I do recommend that these asserts (patch attached, also at my pending/4078-2 tag in github) get committed to mainline Cassandra, though, to help protect against this sort of situation. The asserts in this patch will not catch all possible updates to SSTable.last, but they surround some of the more important (and less frequent) places.

Also, the asserts inside the IntervalTree creation should avoid getting into confusing StackOverflowError situations.",02/Apr/12 20:55;jbellis;alternate patch attached that converts the ordering check in SSTW.beforeAppend to an assert,"02/Apr/12 21:03;thepaul;{noformat}
assert decoratedKey == null : ""Keys must not be null"";
{noformat}

(-1)",02/Apr/12 21:36;jbellis;committed w/ that fixed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL 3.0 does not work in cqlsh with uppercase SELECT,CASSANDRA-4161,12551208,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dohse,dohse,dohse,17/Apr/12 12:20,12/Mar/19 14:04,13/Mar/19 22:27,19/Apr/12 19:02,1.1.0,,Legacy/Tools,,,0,cql3,cqlsh,,,,,"Uppercase SELECT prevents usage of CQL 3.0 features like ORDER BY

Example:

select * from test ORDER BY number; # works
SELECT * from test ORDER BY number; # fails",cqlsh,,,,,,,,,,,,,,,,,,17/Apr/12 12:22;dohse;0001-Allow-CQL-3.0-with-uppercase-SELECT-statement.patch;https://issues.apache.org/jira/secure/attachment/12522949/0001-Allow-CQL-3.0-with-uppercase-SELECT-statement.patch,17/Apr/12 21:03;thepaul;4161.patch.txt;https://issues.apache.org/jira/secure/attachment/12523021/4161.patch.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-17 21:03:42.568,,,no_permission,,,,,,,,,,,,236072,,,Thu Apr 19 19:02:53 UTC 2012,,,,,,0|i0gsp3:,96079,thepaul,thepaul,,,,,,,,,,17/Apr/12 12:22;dohse;Fix problem by converting command to lower case,"17/Apr/12 21:03;thepaul;Good catch, thanks. I do think that a slightly better solution is to do the downcasing in a more specific place, though, as attached. I would like to keep original case intact in cmdword, even if it's just for error messages right now.

Does this work for you?

(also available in my 4161 branch in github: https://github.com/thepaul/cassandra/tree/4161)",19/Apr/12 08:30;ctavan;I'm wondering why CQL is being parsed in the client at all? Couldn't we just handle the exceptions thrown by cassandra? That way we wouldn't have to keep cqlsh in sync with CQL development on the C*-side.,19/Apr/12 09:06;dohse;Works for me™,"19/Apr/12 15:39;thepaul;bq. I'm wondering why CQL is being parsed in the client at all? Couldn't we just handle the exceptions thrown by cassandra? That way we wouldn't have to keep cqlsh in sync with CQL development on the C*-side.

cqlsh has to attempt to parse input in order to recognize keyspace switches, provide tab-completion, implement the cqlsh-specific commands, separate multiple statements, and (in the future) to allow things like CASSANDRA-3799.

Yes, of course, if cqlsh can identify a CQL statement but can't parse it, and it doesn't recognize the command word as being cqlsh-specific, it should pass the CQL on untouched to Cassandra. The problem in this ticket was with cqlsh deciding incorrectly that the user intended to give a cqlsh-only command.",19/Apr/12 15:39;thepaul;+1 for 4161.patch.txt.,19/Apr/12 16:40;dohse;+1 for 4161.patch.txt,19/Apr/12 19:02;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SStableImport and SStableExport does not serialize row level deletion,CASSANDRA-4054,12546566,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dr-alves,hanzhu,hanzhu,15/Mar/12 10:47,12/Mar/19 14:04,13/Mar/19 22:27,12/Jul/12 15:41,1.2.0 beta 1,,Legacy/Tools,,,0,,,,,,,SSTableImport and SSTableExport does not serialize/de-serialize the row-level deletion info to/from the json file. This brings back the deleted data after restore from the json file.,,,,,,,,,,,,,,,,,,,11/Jul/12 16:59;dr-alves;4054.patch;https://issues.apache.org/jira/secure/attachment/12536064/4054.patch,29/Jun/12 23:13;dr-alves;4054.patch;https://issues.apache.org/jira/secure/attachment/12534069/4054.patch,21/Jun/12 04:31;dr-alves;4054.patch;https://issues.apache.org/jira/secure/attachment/12532821/4054.patch,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-03-15 21:17:58.096,,,no_permission,,,,,,,,,,,,231724,,,Thu Jul 12 16:09:47 UTC 2012,,,,,,0|i0grf3:,95872,yukim,yukim,,,,,,,,,,"15/Mar/12 21:17;jbellis;I think we could fix that by splitting the row into {'key': key, 'metadata': {...}, 'columns': [...]} instead of the current {key: [...]}.

Do you want to take a stab at that, Zhu?",15/Mar/12 21:19;jbellis;Changing fix version to 1.1.0 since this would be backwards-incompatible.  If we miss 1.1.0 we can push to 1.2.,"21/Jun/12 04:36;dr-alves;Patch implements jbellis suggestion.
also:
- refactored SSTableImport's methods away from static (class had static internal state that was breaking tests in some configurations)
- added several tests to import that were missing (import sorted was only tested in the case we indicated we wanted to import sorted but there were unsorted keys).
- deletion info is serialized/deserialized with the same data as in DeletionInfo.Serializer.serializeToSSTable","21/Jun/12 16:31;yukim;SSTableImport seems it does not handle SuperColumn deletion as well. (It has been commented out for long time.)
How about extending writeMeta/parseCFMetadata to accept SuperColumn (or more precisely, AbstractColumnContainer) and use them when handling both row and SuperColumn?
Also, it would be great if you remove whitespaces on empty line and always put braces on new line.",29/Jun/12 23:13;dr-alves;additionally implements Yuki's suggestion. also deals with formatting and whitespace issues.,"29/Jun/12 23:20;jbellis;bq. I think we could fix that by splitting the row into {'key': key, 'metadata': {...}, 'columns': [...]} 

Thinking about it more ... do we really want a meta[data] object with just a single sub-object?  should we just pull deletion_info out and make it a top-level field and dispense with the meta container?

Nit: would prefer to spell out ""columns"" vs ""cols""","29/Jun/12 23:33;dr-alves;the only point I can think of for the metadata container is to ease maintenance/compatibility down the line (i.e. writing/parsing metadata can change without having impact on the overall format), then again the less verbose the output the better (previous format had that in mind and that's why I used ""meta"" and ""cols"" instead of ""metadata"" and ""columns""). I'm happy either way. wdyt?
","10/Jul/12 14:43;dr-alves;Yuki is the implementation on the latest patch what you had in mind?
wrt to pulling deletionInfo up and abandoning metadata +0, I'm happy either way.","11/Jul/12 14:58;yukim;David, 
As you stated above, I think it is ok to leave metadata container.
So, I'm +1 here, only if it were ""metadata"" and ""columns"".
Could you make those changes and repost the patch?",11/Jul/12 16:59;dr-alves;patch without abbreviated names,"12/Jul/12 15:40;yukim;+1 and committed. Thanks, David!","12/Jul/12 16:09;hudson;Integrated in Cassandra #1691 (See [https://builds.apache.org/job/Cassandra/1691/])
    new json format with row level deletion; patch by David Alves, reviewed by yukim for CASSANDRA-4054 (Revision d569f873de40f0336a9a34e260c1942866e48950)

     Result = ABORTED
yukim : 
Files : 
* test/resources/SuperCF.json
* test/resources/UnsortedCF.json
* test/resources/SimpleCF.oldformat.json
* test/resources/CounterCF.json
* src/java/org/apache/cassandra/tools/SSTableExport.java
* CHANGES.txt
* test/unit/org/apache/cassandra/tools/SSTableImportTest.java
* test/resources/SimpleCFWithDeletionInfo.json
* test/resources/SimpleCF.json
* test/unit/org/apache/cassandra/tools/SSTableExportTest.java
* src/java/org/apache/cassandra/tools/SSTableImport.java
* test/resources/UnsortedSuperCF.json
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Preserve commitlog size cap when recycling segments at startup,CASSANDRA-4201,12553420,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,pchalamet,pchalamet,30/Apr/12 21:28,12/Mar/19 14:04,13/Mar/19 22:27,23/May/12 21:34,1.1.1,,,,,0,commitlog,,,,,,"1. Create a single node cluster, use default configuration, use cassandra.bat to start the server:

2. run the following commands in cli:
{code}
create keyspace toto;
use toto;
create column family titi;
truncate titi;
{code}

3. the node dies with this error:
{code}
ERROR 23:23:02,118 Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Map failed
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:202)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:159)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Map failed
        at sun.nio.ch.FileChannelImpl.map(Unknown Source)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:119)
        ... 5 more
Caused by: java.lang.OutOfMemoryError: Map failed
        at sun.nio.ch.FileChannelImpl.map0(Native Method)
        ... 7 more
 INFO 23:23:02,122 Stop listening to thrift clients
 INFO 23:23:02,123 Waiting for messaging service to quiesce
 INFO 23:23:02,125 MessagingService shutting down server thread.
{code}","Windows 7 x64, 4Gb, JRE 1.6.0_31 (x86)",,,,,,,,,,,,,,,,,,01/May/12 22:01;jbellis;4201.txt;https://issues.apache.org/jira/secure/attachment/12525229/4201.txt,30/Apr/12 21:28;pchalamet;cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12525111/cassandra.yaml,01/May/12 21:50;pchalamet;log.txt;https://issues.apache.org/jira/secure/attachment/12525227/log.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-04-30 21:47:36.202,,,no_permission,,,,,,,,,,,,237579,,,Wed May 23 21:34:12 UTC 2012,,,,,,0|i0gt6f:,96157,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,30/Apr/12 21:47;jbellis;I can't reproduce.  Are you using a 32bit jvm?,01/May/12 07:52;pchalamet;Yes it is a 32 bits jvm running on x64 system.,01/May/12 14:01;jbellis;How big is your heap?,"01/May/12 14:32;pchalamet;1 gig. Using jvm options in cassandra.bat :

set JAVA_OPTS=-ea^
 -javaagent:""%CASSANDRA_HOME%\lib\jamm-0.2.5.jar""^
 -Xms1G^
 -Xmx1G^
 -XX:+HeapDumpOnOutOfMemoryError^
 -XX:+UseParNewGC^
 -XX:+UseConcMarkSweepGC^
 -XX:+CMSParallelRemarkEnabled^
 -XX:SurvivorRatio=8^
 -XX:MaxTenuringThreshold=1^
 -XX:CMSInitiatingOccupancyFraction=75^
 -XX:+UseCMSInitiatingOccupancyOnly^
 -Dcom.sun.management.jmxremote.port=7199^
 -Dcom.sun.management.jmxremote.ssl=false^
 -Dcom.sun.management.jmxremote.authenticate=false^
 -Dlog4j.configuration=log4j-server.properties^
 -Dlog4j.defaultInitOverride=true","01/May/12 21:50;pchalamet;The first time truncate is called, CommitLogAllocator (line 104) calls createFreshSegment() : a new CommitLogSegment is created. It is working fine.

It seems that everything collapes when the segment is recycled. CommitLogSegment failed line 119
{code}
     buffer = logFileAccessor.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, CommitLog.SEGMENT_SIZE);
{code}
The map() function is failing with ""java.io.IOException: Map failed"" with an OutOfMemory as inner exception.

I've also attached a log in debug mode.","01/May/12 22:01;jbellis;The problem is you don't have enough contiguous address space to mmap the commitlog segements.

1.1.1 already has a configuration parameter to change the commitlog segment size; dropping that to 16, and setting commitlog_total_space_in_mb to the same, works for me.

I did find a related bug, that Cassandra keeps extra commitlog segments around at startup time.  Patch attached to fix that.",18/May/12 00:29;vijay2win@yahoo.com;+1 ,23/May/12 21:34;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor CQL3 fixes,CASSANDRA-4185,12552645,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,25/Apr/12 13:08,12/Mar/19 14:04,13/Mar/19 22:27,03/May/12 07:30,1.1.1,,Legacy/CQL,,,0,cql3,,,,,,"The goal of this ticket is to be the home for a number of minor fixes/improvements in CQL3 that I didn't felt warranted a ticket each. It includes 4 patches:
* The first one fixes the grammar for float constants, so as to not recognize 3.-3, but to actually allow 3. (i.e, with radix point but with the fractional part left blank)
* The second one correctly detect the (invalid) case where a table is created with COMPACT STORAGE but without any 'clustering keys'.
* The third one fixes COUNT, first by making sure both COUNT(*) and COUNT(1) are correctly recognized and also by ""processing"" the internal row before counting, are there isn't a 1-to-1 correspondence between internal rows and CQL rows in CQL3. The grammar change in this patch actually rely on CASSANDRA-4184
* The fourth and last patch disallows the counter type for keys (i.e. any column part of the PRIMARY KEY) as it is completely non-sensical and will only led to confusion.
",,,,,,,,,,,,,,,,,,CASSANDRA-4184,25/Apr/12 13:08;slebresne;0001-Fix-float-parsing.txt;https://issues.apache.org/jira/secure/attachment/12524265/0001-Fix-float-parsing.txt,25/Apr/12 13:08;slebresne;0002-Fix-compact-storage-validation.txt;https://issues.apache.org/jira/secure/attachment/12524266/0002-Fix-compact-storage-validation.txt,25/Apr/12 13:08;slebresne;0003-Fix-COUNT-in-select.txt;https://issues.apache.org/jira/secure/attachment/12524267/0003-Fix-COUNT-in-select.txt,25/Apr/12 14:11;slebresne;0004-Disallow-counters-for-PRIMARY-KEY-part-v2.txt;https://issues.apache.org/jira/secure/attachment/12524273/0004-Disallow-counters-for-PRIMARY-KEY-part-v2.txt,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-05-02 17:45:51.436,,,no_permission,,,,,,,,,,,,236791,,,Thu May 03 07:30:41 UTC 2012,,,,,,0|i0gsz3:,96124,jbellis,jbellis,,,,,,,,,,25/Apr/12 14:11;slebresne;Realized the last patch was missing some parts. v2 attached with those missing parts.,02/May/12 17:45;jbellis;+1,"03/May/12 07:30;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop on CF with ColumnCounter columns fails,CASSANDRA-4181,12552177,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,marcocova,marcocova,marcocova,23/Apr/12 18:22,12/Mar/19 14:04,13/Mar/19 22:27,23/Apr/12 19:06,1.1.1,,,,,0,,,,,,,"Accessing CounterColumn from Hadoop fails with an exception:

{noformat} 
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:456)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:462)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:409)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.nextKeyValue(ColumnFamilyRecordReader.java:184)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator$WideColumnIterator.computeNext(ColumnFamilyRecordReader.java:500)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator$WideColumnIterator.computeNext(ColumnFamilyRecordReader.java:472)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at com.google.common.collect.Iterators$PeekingImpl.hasNext(Iterators.java:1080)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:449)
        ... 11 more
{noformat}",,,,,,,,,,,,,,,,,,,23/Apr/12 18:36;marcocova;cassandra-4181.patch;https://issues.apache.org/jira/secure/attachment/12523836/cassandra-4181.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-23 19:06:40.34,,,no_permission,,,,,,,,,,,,236793,,,Mon Apr 23 19:06:40 UTC 2012,,,,,,0|i0gsxb:,96116,brandon.williams,brandon.williams,,,,,,,,,,23/Apr/12 18:36;marcocova;Small fix: unthriftify the column before accessing its name,"23/Apr/12 19:06;brandon.williams;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Setup"" section of tools/stress/README.txt needs update",CASSANDRA-4168,12551487,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,tpatterson,tpatterson,18/Apr/12 16:14,12/Mar/19 14:04,13/Mar/19 22:27,01/May/12 19:00,1.0.10,1.1.1,Legacy/Tools,,,0,stress,,,,,,"The README.txt file states ""Run `ant` from the Cassandra source directory, then Run `ant` from the contrib/stress directory.""

The file needs to reflect the changes in the way stress is built.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-04-18 16:45:42.55,,,no_permission,,,,,,,,,,,,236295,,,Tue May 01 19:00:22 UTC 2012,,,,,,0|i0gssf:,96094,bcoverston,bcoverston,,,,,,,,,,18/Apr/12 16:45;brandon.williams;Done in 2d029e86d99e74c8d0eeb321d821daeaa27b1b52,23/Apr/12 17:42;tpatterson;I vote that the readme should mention the need to run 'ant jar' or 'ant stress-build',23/Apr/12 18:15;brandon.williams;I suggest that it's time we made 'ant jar' the default target instead.,01/May/12 02:03;bcoverston;+1 ant jar should be the default target.,01/May/12 19:00;brandon.williams;Changed all current branches to 'jar' by default.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when using sstableloader with PropertyFileSnitch configured,CASSANDRA-4145,12550785,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jicheng,jicheng,jicheng,13/Apr/12 07:08,12/Mar/19 14:04,13/Mar/19 22:27,13/Apr/12 15:33,1.0.10,1.1.0,Legacy/Tools,,,0,bulkloader,,,,,,"I got a NullPointerException when using sstableloader on 1.0.6. The cluster is using PropertyFileSnitch. The same configuration file is used for sstableloader. 

The problem is if StorageService is initialized before DatabaseDescriptor, PropertyFileSnitch will try to access StorageService.instance before it finishes initialization.


{code}
 ERROR 01:14:05,601 Fatal configuration error
org.apache.cassandra.config.ConfigurationException: Error instantiating snitch class 'org.apache.cassandra.locator.PropertyFileSnitch'.
        at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:607)
        at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:454)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:306)
        at org.apache.cassandra.service.StorageService.<init>(StorageService.java:187)
        at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:190)
        at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:183)
        at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:106)
        at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:62)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
        at java.lang.reflect.Constructor.newInstance(Unknown Source)
        at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:589)
        ... 7 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.locator.PropertyFileSnitch.reloadConfiguration(PropertyFileSnitch.java:170)
        at org.apache.cassandra.locator.PropertyFileSnitch.<init>(PropertyFileSnitch.java:60)
        ... 12 more
Error instantiating snitch class 'org.apache.cassandra.locator.PropertyFileSnitch'.
Fatal configuration error; unable to start server.  See log for stacktrace.
{code}",,,,,,,,,,,,,,,,,,,13/Apr/12 07:13;jicheng;4145.txt;https://issues.apache.org/jira/secure/attachment/12522536/4145.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-13 15:33:25.841,,,no_permission,,,,,,,,,,,,235649,,,Fri Apr 13 15:33:25 UTC 2012,,,,,,0|i0gsif:,96049,jbellis,jbellis,,,,,,,,,,"13/Apr/12 15:33;jbellis;lgtm, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include stress tool in debian packaging,CASSANDRA-4256,12556269,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,17/May/12 18:40,12/Mar/19 14:04,13/Mar/19 22:27,21/May/12 17:28,1.1.1,,Legacy/Tools,,,0,,,,,,,"The stress tool isn't included in the debian packaging. We need to update that to grab the stress shell script as well as put the stress.jar file in lib.

Also the stress shell script needs to be updated to include looking in /usr/share/cassandra... when searching for the stress jar so it will run in packaged installations.",,,,,,,,,,,,,,,,,,,18/May/12 16:25;nickmbailey;0001-Include-stress-in-debian-package-V2.patch;https://issues.apache.org/jira/secure/attachment/12528097/0001-Include-stress-in-debian-package-V2.patch,18/May/12 17:55;nickmbailey;0001-Include-stress-in-debian-package-V3.patch;https://issues.apache.org/jira/secure/attachment/12528111/0001-Include-stress-in-debian-package-V3.patch,21/May/12 16:06;nickmbailey;0001-Include-stress-in-debian-package-V4.patch;https://issues.apache.org/jira/secure/attachment/12528447/0001-Include-stress-in-debian-package-V4.patch,17/May/12 19:42;nickmbailey;0001-Include-stress-in-debian-package.patch;https://issues.apache.org/jira/secure/attachment/12527896/0001-Include-stress-in-debian-package.patch,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-05-18 02:49:34.267,,,no_permission,,,,,,,,,,,,255998,,,Mon May 21 17:28:50 UTC 2012,,,,,,0|i0gtt3:,96259,thepaul,thepaul,,,,,,,,,,17/May/12 19:42;nickmbailey;Includes stress stuff in debian package. Also slightly reorganizes the stress script so it will work better in packaged installs.,"17/May/12 19:45;nickmbailey;Also I'll note it would be cool to get a 1.1.0-2 debian package out if we can. I guess this can't completely be considered solely a packaging change since the stress script itself changed, but I guess I'll leave that up to the committers.",18/May/12 02:49;thepaul;+1.,"18/May/12 10:22;slebresne;bq. Also I'll note it would be cool to get a 1.1.0-2 debian package out if we can

Honestly I'd better wait on 1.1.1 to be released as we can't really do a new debian package without doing a full release (as otherwise the md5 in the packages wouldn't match).

Other than that, I wonder if it wouldn't make sense to rename stress to something like cassandra-stress? I wonder if stress is not a tad generic to put in /usr/bin like that.",18/May/12 15:46;nickmbailey;Would you propose changing it in general or just in the packaging?,"18/May/12 16:05;slebresne;I don't know :)

Probably it's enough to change it in the packaging (if we consider it a valid concern), but changing the script name in general could make it more likely to get uniformity across other (externally handled) packaging.",18/May/12 16:14;nickmbailey;I'm against only changing it in the packaging :). I guess I'd be fine changing it everywhere. I'll update the patch.,18/May/12 16:25;nickmbailey;V2 attached.,"18/May/12 16:52;thepaul;I see just a few places that ought to be updated to reflect the change:

1. A few bits of text in tools/stress/README
2. The stress tool's idea of its own name (in tools/stress/src/org/apache/cassandra/stress/Stress.java, after ""Usage:"")
3. The rename is probably big enough to warrant a mention in CHANGES.

+1 otherwise.",18/May/12 17:55;nickmbailey;V3 attached,21/May/12 15:40;thepaul;+1,21/May/12 16:05;nickmbailey;V4 which renames stress.bat as well as stressd is attached.,"21/May/12 17:28;slebresne;Alright, committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 range query with secondary index fails,CASSANDRA-4257,12556274,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,sbillig,sbillig,17/May/12 19:06,12/Mar/19 14:04,13/Mar/19 22:27,24/May/12 14:45,1.1.1,,Feature/2i Index,Legacy/CQL,,0,cql3,index,,,,,"This query fails:
select * from indextest where setid = 0 and row < 1;
when there's a secondary index on 'setid'; row isn't the primary key.

{code:title=CQL3}
bin$ ./cqlsh --cql3
Connected to Git at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.31.0]
Use HELP for help.
cqlsh> use warehouse1;
cqlsh:warehouse1> create table indextest (id int primary key, row int, setid int);
cqlsh:warehouse1> create index indextest_setid_idx on indextest (setid);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (0, 0, 0);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (1, 1, 0);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (2, 2, 0);
cqlsh:warehouse1> select * from indextest where setid = 0;
 id | row | setid
----+-----+-------
  0 |   0 |     0
  1 |   1 |     0
  2 |   2 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row = 1;
 id | row | setid
----+-----+-------
  1 |   1 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row < 1;
TSocket read 0 bytes
{code}

{code:title=Error message}
ERROR 13:36:23,544 Error occurred during processing of message.
java.lang.NullPointerException
  at org.apache.cassandra.cql3.statements.SelectStatement.getIndexExpressions(SelectStatement.java:546)
  at org.apache.cassandra.cql3.statements.SelectStatement.multiRangeSlice(SelectStatement.java:253)
  at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:132)
  at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
  at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
  at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
  at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
  at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
  at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:680)
{code}

Works fine in CQL2:
{code:title=CQL2}
bin$ ./cqlsh_uuid --cql2
Connected to Git at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.31.0]
Use HELP for help.
cqlsh> use warehouse1;
cqlsh:warehouse1> select * from indextest where setid = 0 and row < 1;
 id | row | setid
----+-----+-------
  0 |   0 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row < 2;
 id | row | setid
----+-----+-------
  0 |   0 |     0
  1 |   1 |     0
{code}","Cassandra 1.1.0 and git cassandra-1.1 branch, as of commit fd92c09d95a53d582cb8c4b0e77ac47fdd884935",,,,,,,,,,,,,,,,,,19/May/12 16:36;slebresne;4257.txt;https://issues.apache.org/jira/secure/attachment/12528260/4257.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-19 16:36:06.55,,,no_permission,,,,,,,,,,,,255999,,,Thu May 24 14:45:36 UTC 2012,,,,,,0|i0gttj:,96261,xedin,xedin,,,,,,,,,,"19/May/12 16:36;slebresne;That was kind of a typo. When 2ndary indexes was used, the code was expecting that a column that had a lesser-than clause on it, also had a greater-than one. Simple patch attached to fix. I've pushed a regression test to the dtests too.",23/May/12 15:34;xedin;+1,"24/May/12 14:45;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
concurrent modif ex when repair is run on LCS,CASSANDRA-4255,12556268,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,17/May/12 18:29,12/Mar/19 14:04,13/Mar/19 22:27,21/May/12 16:52,1.0.11,1.1.1,,,,0,compaction,lcs,,,,,"came across this, will try to figure a way to systematically reprod this. But the problem is the sstable list in the manifest is changing as the repair is triggered:

{panel}
Exception in thread ""main"" java.util.ConcurrentModificationException 
 at java.util.AbstractList$Itr.checkForComodification(Unknown Source)
 at java.util.AbstractList$Itr.next(Unknown Source)
 at org.apache.cassandra.io.sstable.SSTable.getTotalBytes(SSTable.java:250)
 at org.apache.cassandra.db.compaction.LeveledManifest.getEstimatedTasks(LeveledManifest.java:435)
 at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getEstimatedRemainingTasks(LeveledCompactionStrategy.java:128)
 at org.apache.cassandra.db.compaction.CompactionManager.getPendingTasks(CompactionManager.java:1063)
 at sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
 at java.lang.reflect.Method.invoke(Unknown Source)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
 at com.sun.jmx.mbeanserver.PerInterface.getAttribute(Unknown Source)
 at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(Unknown Source)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(Unknown Source)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(Unknown Source)
 at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
 at java.lang.reflect.Method.invoke(Unknown Source)
 at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
 at sun.rmi.transport.Transport$1.run(Unknown Source)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.Transport.serviceCall(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
 at java.lang.Thread.run(Unknown Source)
{panel}

maybe we could change the list to a copyOnArrayList? just a suggestion, haven't investigated much yet:

{code:title=LeveledManifest.java}
generations[i] = new ArrayList<SSTableReader>();
{code}",,,,,,,,,,,,,,,,,,,17/May/12 19:13;jbellis;4255.txt;https://issues.apache.org/jira/secure/attachment/12527888/4255.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-17 19:13:54.097,,,no_permission,,,,,,,,,,,,255997,,,Mon May 21 16:52:46 UTC 2012,,,,,,0|i0gtsn:,96257,slebresne,slebresne,,,,,,,,,,"17/May/12 19:13;jbellis;patch attached to synchronize getEstimatedTasks.  copyOnWriteArrayList would probably work too, but we already synchronize other accesses (to prevent cross-level races) so that's simplest here.","17/May/12 20:23;jbellis;Looks like this only affects getEstimatedTasks, not actually repair.",21/May/12 10:21;slebresne;+1,21/May/12 16:52;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
long-test broken due to incorrect config option,CASSANDRA-4270,12556774,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tpatterson,tpatterson,tpatterson,21/May/12 22:46,12/Mar/19 14:04,13/Mar/19 22:27,23/May/12 21:53,1.0.11,1.1.1,Legacy/Testing,,,0,test,,,,,,"the long-test fails:
{code}
BUILD FAILED
/home/tahooie/datastax/cassandra/build.xml:1125: Problem: failed to create task or type jvmarg
Cause: The name is undefined.
{code}
The problem is that the build.xml file has the jvmarg outside the <testmacro> tag instead of inside it. A patch is forthcoming.",,,,,,,,,,,,,,,,,,,21/May/12 22:48;tpatterson;CASSANDRA-4270.patch;https://issues.apache.org/jira/secure/attachment/12528522/CASSANDRA-4270.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-22 15:50:29.127,,,no_permission,,,,,,,,,,,,256012,,,Tue May 22 15:50:29 UTC 2012,,,,,,0|i0gtz3:,96286,jbellis,jbellis,,,,,,,,,,"22/May/12 15:50;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't specify certain keyspace properties in CQL,CASSANDRA-4278,12557052,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,23/May/12 19:24,12/Mar/19 14:04,13/Mar/19 22:27,29/May/12 08:48,1.1.1,,,,,0,cql,cql3,,,,,"A user using EC2MultiRegionSnitch, where the datacenter name has to match the AWS region names, will not be able to specify a keyspace's replica counts for those datacenters using CQL. AWS region names contain hyphens, which are not valid identifiers in CQL, and CQL keyspace/columnfamily properties must be identifiers or identifiers separated by colons.

Example:

{noformat}
CREATE KEYSPACE Foo
  WITH strategy_class = 'NetworkTopologyStrategy'
      AND strategy_options:""us-east""=1
      AND strategy_options:""us-west""=1;
{noformat}

(see http://mail-archives.apache.org/mod_mbox/cassandra-user/201205.mbox/browser for context)

..will not currently work, with or without the double quotes.

CQL should either allow hyphens in COMPIDENT, or allow quoted parts of a COMPIDENT token.",,,,,,,,,,,,,,,,,,,24/May/12 14:34;slebresne;4278.txt;https://issues.apache.org/jira/secure/attachment/12529072/4278.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-24 14:34:00.889,,,no_permission,,,,,,,,,,,,256020,,,Tue May 29 08:48:18 UTC 2012,,,,,,0|i0gu2n:,96302,xedin,xedin,,,,,,,,,,"24/May/12 14:34;slebresne;Attaching patch that cleans up (I think) the definition of property names. Not only does it allow quoted identifiers, it also make the non quoted ones case insensitive. I'll note that it remove the support integer without quotes, i.e, one can't write ""strategy_options:4 = ..."", but I'm pretty sure this was neither used, nor is it useful.",26/May/12 12:49;xedin;+1,"29/May/12 08:48;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL 3.0 prepare_cql_query fails on ""BEGIN BATCH""",CASSANDRA-4202,12553427,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,sbillig,sbillig,30/Apr/12 22:03,12/Mar/19 14:04,13/Mar/19 22:27,03/May/12 07:30,1.1.1,,Legacy/CQL,,,0,c++,cql3,thrift,,,,"Preparing the following (contrived) statement with the C++ Thrift bindings 
throws a TTransportException (""No more data to read."" from TTransport.h:41)

q = ""begin batch insert into crashtest (id, val) values (?, ?); apply batch"";
client.prepare_cql_query(pr, q, Compression::NONE);

{code:title=crashtest.cpp}
#include <protocol/TBinaryProtocol.h>
#include <thrift/transport/TSocket.h>
#include <thrift/transport/TTransportUtils.h>
#include ""Cassandra.h""

using namespace std;
using namespace apache::thrift;
using namespace apache::thrift::protocol;
using namespace apache::thrift::transport;
using namespace org::apache::cassandra;
using namespace boost;

int main(int argc, char **argv) {
    shared_ptr<TTransport> socket(new TSocket(""127.0.0.1"", 9160));
    shared_ptr<TTransport> transport(new TFramedTransport(socket));
    shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));

    CassandraClient client(protocol);

    try {
        transport->open();
        client.set_keyspace(""test1"");
        client.set_cql_version(""3.0.0"");

        CqlResult cr;
        CqlPreparedResult pr;

        // In cqlsh: create table crashtest (id int primary key, val text);
        const char *q;
        // q = ""insert into crashtest (id, val) values (?, ?)""; // This works fine
        q = ""begin batch insert into crashtest (id, val) values (?, ?); apply batch"";

        client.prepare_cql_query(pr,  q, Compression::NONE);

        vector<string> vtypes = pr.variable_types;
        vector<string>::iterator it;

        for (it = vtypes.begin(); it != vtypes.end(); it++) {
            cout << *it << endl;
        }
    } catch (TException &tx) {
        cerr << ""TException ERROR: "" << tx.what() << endl;
    }
}
{code}

{code:title=backtrace}
#0  0x00007fff901800e9 in __cxa_throw ()
#1  0x0000000100009ab9 in apache::thrift::transport::readAll<apache::thrift::transport::TBufferBase> (trans=@0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TTransport.h:41
#2  0x0000000100009c1d in apache::thrift::transport::TBufferBase::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TBufferTransports.h:82
#3  0x0000000100009c5b in apache::thrift::transport::TFramedTransport::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TBufferTransports.h:390
#4  0x0000000100004b45 in apache::thrift::transport::TVirtualTransport<apache::thrift::transport::TFramedTransport, apache::thrift::transport::TBufferBase>::readAll_virt (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TVirtualTransport.h:99
#5  0x00000001000034c1 in apache::thrift::transport::TTransport::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TTransport.h:126
#6  0x0000000100009f4c in apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>::readI32 (this=0x100401370, i32=@0x7fff5fbff020) at TBinaryProtocol.h:372
#7  0x000000010000b5bf in apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>::readMessageBegin (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TBinaryProtocol.h:203
#8  0x0000000100006b07 in apache::thrift::protocol::TVirtualProtocol<apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>, apache::thrift::protocol::TProtocolDefaults>::readMessageBegin_virt (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TVirtualProtocol.h:432
#9  0x00000001000abe78 in apache::thrift::protocol::TProtocol::readMessageBegin (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TProtocol.h:518
#10 0x0000000100069a98 in org::apache::cassandra::CassandraClient::recv_prepare_cql_query (this=0x7fff5fbff5b0, _return=@0x7fff5fbff4c0) at Cassandra.cpp:10231
#11 0x000000010003bf3f in org::apache::cassandra::CassandraClient::prepare_cql_query (this=0x7fff5fbff5b0, _return=@0x7fff5fbff4c0, query=@0x7fff5fbff6b0, compression=org::apache::cassandra::Compression::NONE) at Cassandra.cpp:10206
#12 0x00000001000020ea in main (argc=1, argv=0x7fff5fbff8c8) at crashtest.cpp:36
{code}

{code:title=server error message}

ERROR 17:13:55,089 Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.cassandra.cql3.statements.UpdateStatement.prepare(UpdateStatement.java:278)
	at org.apache.cassandra.cql3.statements.BatchStatement.prepare(BatchStatement.java:157)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:207)
	at org.apache.cassandra.cql3.QueryProcessor.prepare(QueryProcessor.java:158)
	at org.apache.cassandra.thrift.CassandraServer.prepare_cql_query(CassandraServer.java:1260)
	at org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query.getResult(Cassandra.java:3484)
	at org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query.getResult(Cassandra.java:3472)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{code}","OSX 10.7.2, Cassandra 1.1.0, Thrift 0.7",,,,,,,,,,,,,,,,,,02/May/12 11:36;slebresne;4202.txt;https://issues.apache.org/jira/secure/attachment/12525277/4202.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-30 22:06:53.087,,,no_permission,,,,,,,,,,,,237587,,,Thu May 03 07:30:59 UTC 2012,,,,,,0|i0gt6v:,96159,xedin,xedin,,,,,,,,,,30/Apr/12 22:06;jbellis;Does the server log any errors?,"30/Apr/12 22:16;sbillig;Oops, forgot the most important part.  Added it to the description.",02/May/12 11:36;slebresne;Patch attached to fix (we need the statements inside the batch to use the bounded variables from the batch itself).,"02/May/12 13:45;jbellis;Is this an artifact of ""everything is a prepared statement?""  Does that really make sense for batches?","02/May/12 13:58;slebresne;Depends what you mean by that :). To some extent it's an artifact of how we index bound variables in the parser. What happened is that the parser computes the index of the prepared variables (incrementing the index while parsing each time it sees a bound variable) and only the top-level statement knows how many variables there is at the end. For a batch, it means that individual inserts/deletes inside a batch needs to refer to the batch to know how many variables there is in total, which is what was not done. I don't know how clear that is and if I have answered your question though.  ",02/May/12 20:16;xedin;+1,"03/May/12 07:30;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PFS should give a friendlier error message when a node has not been configured,CASSANDRA-4349,12560856,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,16/Jun/12 03:06,12/Mar/19 14:04,13/Mar/19 22:27,18/Jun/12 19:15,1.1.2,,,,,0,,,,,,,see CASSANDRA-4345,,,,,,,,,,,,,,,,,,,16/Jun/12 03:16;jbellis;4349.txt;https://issues.apache.org/jira/secure/attachment/12532288/4349.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-18 16:07:03.157,,,no_permission,,,,,,,,,,,,256084,,,Mon Jun 18 19:15:45 UTC 2012,,,,,,0|i0guvj:,96432,brandon.williams,brandon.williams,,,,,,,,,,16/Jun/12 03:16;jbellis;Patch attached.,18/Jun/12 16:07;brandon.williams;+1,18/Jun/12 19:15;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken streaming after CASSANDRA-4311,CASSANDRA-4360,12595278,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,20/Jun/12 15:38,12/Mar/19 14:04,13/Mar/19 22:27,20/Jun/12 19:17,1.2.0 beta 1,,,,,0,streaming,,,,,,"CASSANDRA-4311 made change in message exchange, that the message will contain header only at the beginning of exchange. This causes FileStreamTask to fail since it expects message header in StreamReply message.",,,,,,,,,,,,,,,,,,,20/Jun/12 15:41;yukim;4360.txt;https://issues.apache.org/jira/secure/attachment/12532713/4360.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-20 19:10:18.527,,,no_permission,,,,,,,,,,,,256093,,,Wed Jun 20 21:06:50 UTC 2012,,,,,,0|i0gv07:,96453,brandon.williams,brandon.williams,,,,,,,,,,"20/Jun/12 15:41;yukim;Patch attached to remove assumption that replay message contains message header.
This removal is fine since nodes already reject streaming from different versions.",20/Jun/12 19:10;brandon.williams;+1,"20/Jun/12 19:16;yukim;Committed, thanks!","20/Jun/12 21:06;hudson;Integrated in Cassandra #1540 (See [https://builds.apache.org/job/Cassandra/1540/])
    fix streaming for messaging change patch by yukim, reviewed by brandonwilliams for CASSANDRA-4360 (Revision d59be21e8a936d5c893a67e8a3c98505739a7279)

     Result = ABORTED
yukim : 
Files : 
* src/java/org/apache/cassandra/streaming/FileStreamTask.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Oversize integer in CQL throws NumberFormatException,CASSANDRA-4291,12558264,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,ssadler,ssadler,27/May/12 06:08,12/Mar/19 14:04,13/Mar/19 22:27,31/May/12 00:40,1.0.11,1.1.1,Legacy/CQL,,,0,cql,,,,,,"In CQL, the parser does not handle an oversize Integer, the client socket get closed and an exception is output in the log.

{noformat}cqlsh:TEST1> select count(*) from Items limit 10000000000000;
TSocket read 0 bytes
cqlsh:TEST1> select count(*) from Items limit 1;
TSocket read 0 bytes{noformat}

{noformat}
ERROR 02:51:28,600 Error occurred during processing of message.
java.lang.NumberFormatException: For input string: ""10000000000""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:461)
	at java.lang.Integer.parseInt(Integer.java:499)
	at org.apache.cassandra.cql.CqlParser.selectStatement(CqlParser.java:631)
	at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:221)
	at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:951)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:873)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1234)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

The INTEGER type in Cql.g matches digits but not to any particular limit.",,,,,,,,,,,,,,,,,,,28/May/12 16:39;dbrosius@apache.org;4291_catch_misc_excs_processing_statements.txt;https://issues.apache.org/jira/secure/attachment/12529971/4291_catch_misc_excs_processing_statements.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-28 16:39:26.318,,,no_permission,,,,,,,,,,,,256032,,,Thu May 31 00:20:40 UTC 2012,,,,,,0|i0gu87:,96327,xedin,xedin,,,,,,,,,,"28/May/12 16:39;dbrosius@apache.org;catch misc runtime exceptions from cql.getStatement and return InvalidRequestExceptions with 'useful messages' for these so that the connection from cql to the server doesn't get into a bad state.

against trunk","30/May/12 15:46;xedin;This one is related to 1.0 CQL(2) and also CQL3 in 1.1. +1, so just add a patch for cql3 and you are good to go commit it to 1.0 and 1.1","31/May/12 00:20;dbrosius@apache.org;committed to cassandra-1.0 as commit edfc06fdd0794831cf7d1401fa98ff38bb4e4210

added fix for cql3 in cassandra-1.1 as commit 59f349de705f38daa8869c64817e27e9657ccc45",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3: create table don't always validate access to the right keyspace,CASSANDRA-4296,12558575,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,30/May/12 10:41,12/Mar/19 14:04,13/Mar/19 22:27,30/May/12 14:06,1.1.1,,Legacy/CQL,,,0,,,,,,,"Create table allows (like other queries) to override the currently set keyspace ({{CREATE TABLE foo.bar ...}}). However, when we do that, the access check is done on the wrong keyspace. In particular if no keyspace was set, this end up in a NPE.",,,,,,,,,,,,,,,,,,,30/May/12 10:44;slebresne;4296.patch;https://issues.apache.org/jira/secure/attachment/12530186/4296.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-30 10:59:12.788,,,no_permission,,,,,,,,,,,,256036,,,Wed May 30 14:06:53 UTC 2012,,,,,,0|i0gua7:,96336,jbellis,jbellis,,,,,,,,,,30/May/12 10:44;slebresne;Patch attached,30/May/12 10:59;jbellis;+1,"30/May/12 14:06;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in CLI when updating keyspace,CASSANDRA-4322,12559863,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,richardlow,richardlow,08/Jun/12 16:03,12/Mar/19 14:04,13/Mar/19 22:27,11/Sep/12 11:18,1.1.6,,,,,0,,,,,,,"To repro:

1. Open the cli
2. Create a keyspace:
  create keyspace ks1 with placement_strategy = SimpleStrategy and strategy_options = {replication_factor:1};
3. Update the keyspace:
  update keyspace ks1 with strategy_options = {replication_factor:3};

The output is:

[default@unknown] create keyspace ks1 with placement_strategy = SimpleStrategy and strategy_options = {replication_factor:1};                               
8ecd5e16-e0f7-37e7-850e-38ee1a3a510e
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] update keyspace ks1 with strategy_options = {replication_factor:3};                                        
857af387-6677-3e39-bdf6-e1132673c25b
Waiting for schema agreement...
... schemas agree across the cluster
org.apache.thrift.protocol.TProtocolException: Required field 'keyspace' was not present! Struct: describe_keyspace_args(keyspace:null)
[default@unknown]

The problem is that the patch in CASSANDRA-4052 assumes the CLI is authenticated to a working keyspace.  getKSMetaData in executeUpdateKeySpace is called with keySpace, which is null.

Changing this to keyspaceName partially solves it, we now get:

[default@unknown] update keyspace ks1 with strategy_options = {replication_factor:3};
Not authenticated to a working keyspace.
18d750fc-19d9-30f0-b8b9-18b2e4a0a0d4
Waiting for schema agreement...
... schemas agree across the cluster
Not authenticated to a working keyspace.

This comes from replayAssumptions in getKSMetaData.

It seems that the refresh code needs to be reworked slightly to not assume the CLI is authenticated to a keyspace.",,,,,,,,,,,,,,,,,,,08/Sep/12 18:59;xedin;CASSANDRA-4322.patch;https://issues.apache.org/jira/secure/attachment/12544362/CASSANDRA-4322.patch,09/Jun/12 00:14;dbrosius@apache.org;auth_for_mod_ks.txt;https://issues.apache.org/jira/secure/attachment/12531471/auth_for_mod_ks.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-06-09 00:14:04.36,,,no_permission,,,,,,,,,,,,256059,,,Tue Sep 11 11:18:47 UTC 2012,,,,,,0|i0guk7:,96381,dbrosius@apache.org,dbrosius@apache.org,,,,,,,,,,09/Jun/12 00:14;dbrosius@apache.org;don't allow updating or dropping of a keyspace unless you authenticate to it (use ks).,"11/Jun/12 09:25;richardlow;This patch breaks backwards compatibility, so now operations that used to not require setting a keyspace do.  Do we aim to keep the cassandra-cli interface compatible within the same x.x release?",08/Sep/12 18:59;xedin;patch allows to create keyspace without being authorized to one + fixes describe.,11/Sep/12 02:45;dbrosius@apache.org;+1 patch LGTM,11/Sep/12 11:18;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool compactionstats fails with NullPointerException,CASSANDRA-4318,12559766,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,sj.climber,sj.climber,07/Jun/12 22:25,12/Mar/19 14:04,13/Mar/19 22:27,09/Jun/12 15:37,1.1.2,,,,,0,caching,,,,,,"Test uses Column family C defined as follows:

create column family C with caching = 'keys_only' and key_validation_class = 'LongType' and compression_options = { sstable_compression: SnappyCompressor, chunk_length_kb: 64 } and max_compaction_threshold=0; 

max_compaction_threshold is set to 0 to disable auto compaction.

SSTables are streamed via sstableloader, after which a major compaction is triggered using ""nodetool compact MyKeyspace C"".

Thereafter, attempts to request compaction stats via ""nodetool compactionstats"" fail with the following exception:

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.compaction.CompactionInfo.asMap(CompactionInfo.java:103)
        at org.apache.cassandra.db.compaction.CompactionManager.getCompactions(CompactionManager.java:1115)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:303)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662) ","2 node cluster running on Ubuntu 10.10 64-bit
16GB Max Heap allocated to each node.
Test keyspace using replication factor 2",,,,,,,,,,,,,,,,,,08/Jun/12 03:14;jbellis;4318.txt;https://issues.apache.org/jira/secure/attachment/12531362/4318.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-08 03:14:20.977,,,no_permission,,,,,,,,,,,,256056,,,Sat Jun 09 15:37:45 UTC 2012,,,,,,0|i0guin:,96374,xedin,xedin,,,,,,,,,,"08/Jun/12 03:14;jbellis;patch attached to deal with cfm directly instead of looking up by id, and create a dummy cfm for cache saving instead of leaving null",08/Jun/12 11:34;xedin;+1,09/Jun/12 15:37;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL should support CL.TWO,CASSANDRA-4156,12551084,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,thepaul,thepaul,16/Apr/12 16:44,12/Mar/19 14:04,13/Mar/19 22:27,17/Apr/12 07:15,1.1.0,,Legacy/CQL,,,0,cql,cql3,,,,,CL.TWO was overlooked in creating the CQL language spec. It should probably be added.,,,,,,,,,,,,,,,,,,,16/Apr/12 19:23;mdennis;cassandra-trunk-4156.txt;https://issues.apache.org/jira/secure/attachment/12522839/cassandra-trunk-4156.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-16 19:24:57.68,,,no_permission,,,,,,,,,,,,235948,,,Tue Apr 17 07:15:43 UTC 2012,,,,,,0|i0gsn3:,96070,slebresne,slebresne,,,,,,,,,,16/Apr/12 19:24;mdennis;tested on EC2 with multiple nodes,"17/Apr/12 07:15;slebresne;+1. Committed for 1.1.0, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly catch exception when Snappy cannot be loaded,CASSANDRA-4400,12596570,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,02/Jul/12 07:08,12/Mar/19 14:04,13/Mar/19 22:27,05/Jul/12 16:09,1.1.3,,,,,0,,,,,,,"From the mailing list, on C* 1.1.1:
{noformat}
INFO 14:22:07,600 Global memtable threshold is enabled at 35MB
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:317)
        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:219)
        at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:55)
        at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:37)
        at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:76)
        at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:79)
        at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:439)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:118)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Caused by: java.lang.UnsatisfiedLinkError: no snappyjava in java.library.path
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1681)
        at java.lang.Runtime.loadLibrary0(Runtime.java:840)
        at java.lang.System.loadLibrary(System.java:1047)
        at org.xerial.snappy.SnappyNativeLoader.loadLibrary(SnappyNativeLoader.java:52)
        ... 17 more
ERROR 14:22:09,934 Exception encountered during startup
org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
        at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:55)
        at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:37)
        at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:76)
        at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:79)
        at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:439)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:118)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
        at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:55)
        at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:37)
        at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:76)
        at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:79)
        at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:439)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:118)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Exception encountered during startup: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
{noformat}",,,,,,,,,,,,,,,,,,,02/Jul/12 07:10;slebresne;4400.txt;https://issues.apache.org/jira/secure/attachment/12534183/4400.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-02 15:01:09.205,,,no_permission,,,,,,,,,,,,256128,,,Mon Dec 10 15:29:38 UTC 2012,,,,,,0|i0gvgv:,96528,acobley,acobley,,,,,,,,,,02/Jul/12 07:10;slebresne;Patch attached to be a little bit more thorough in which errors we catch.,"02/Jul/12 15:01;acobley;I added your fix to the source of apache-cassandra-1.1.1-src and it does allow a startup if SnappyJava is missing.  The only thing I not is the log from the startup is a bit messy with exceptions thrown, but I'm guessing this acceptable ?

INFO 15:42:57,873 Global memtable threshold is enabled at 35MB
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:317)
	at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:219)
	at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
	at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:46)
	at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:56)
	at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:38)
	at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:76)
	at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:79)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:439)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:118)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Caused by: java.lang.UnsatisfiedLinkError: no snappyjava in java.library.path
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1738)
	at java.lang.Runtime.loadLibrary0(Runtime.java:823)
	at java.lang.System.loadLibrary(System.java:1028)
	at org.xerial.snappy.SnappyNativeLoader.loadLibrary(SnappyNativeLoader.java:52)
	... 17 more
 WARN 15:42:59,586 Cannot initialize native Snappy library. Compression on new tables will be disabled.
 INFO 15:43:00,141 Initializing key cache with capacity of 5 MBs.","04/Jul/12 10:03;slebresne;Actually, I'd like to get rid of that exception at startup but I'm not sure why that InvocationTargetException is not caught by the 'catch (Exception e)' in SnappyCompressor.isAvailable(). Unless it's printed in the log by the snappy library itself.","04/Jul/12 10:27;acobley;I think it's being printed by :

static {
        try {
            impl = SnappyLoader.load();
        }
        catch (Exception e) {
            e.printStackTrace();
        }

in Snappy.java line 47.  I can experiment and confirm 
","04/Jul/12 12:26;slebresne;:(

So I guess the current output is the best we can do and we can open a ticket on Snappy-Java to have them remove that.","04/Jul/12 12:33;acobley;let me confirm thats the problem if thats the case, yes a ticket should be opened.
","04/Jul/12 21:25;acobley;Sylvian,
I've done a little digging into the snappy source code, looks like they are not catching a java.lang.reflect.InvocationTargetException at line 319 of SnappyLoader:

 loadMethod.invoke(null, nativeLib.getAbsolutePath());

I'll try and raise a ticket with them.

Are you happy to commit your changes for 1.1.3?","05/Jul/12 14:18;slebresne;Yeah so there is not much more we can do in the meantime. @Andy you tested the attached patch succesfully, right? If so I'll commit that.","05/Jul/12 14:51;acobley;Yep Sylvian, I've tested it and am happy.","05/Jul/12 16:09;slebresne;Alright, committed, thanks","06/Aug/12 06:53;trajano;Although it catches it properly, it still throws a large stack trace.  Is it possible to get rid of it?","06/Aug/12 10:36;slebresne;As explain above, the large stack trace is thrown by the Snappy library itself, so until they fix that there is nothing we can do about it, sorry.","13/Nov/12 16:29;miggi;Hey Guys,
Was it fixed in 1.1.6 ? 
See the same trace: 

Caused by: java.lang.UnsatisfiedLinkError: no snappyjava in java.library.path
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860)
	at java.lang.Runtime.loadLibrary0(Runtime.java:845)
	at java.lang.System.loadLibrary(System.java:1084)
	at org.xerial.snappy.SnappyNativeLoader.loadLibrary(SnappyNativeLoader.java:52)
	... 17 more

Cassandra-1.1.6
OSX 10.8 JDK ""1.7.0_07""
Any Ideas ?
","13/Nov/12 16:39;slebresne;As said above, this is logged by SnappyJava itself so there is nothing we can do about that. But this is only cosmetic, you should disregard the trace itself (it does indicate that you won't have compression enable but that's a totally different problem).",13/Nov/12 17:04;yukim;[~miggi] I  think you are seeing this https://github.com/xerial/snappy-java/issues/6.,"08/Dec/12 18:49;drew_kutchar;Is there a reason Cassandra's not using the pure Java version of Snappy? https://github.com/dain/snappy
The performance numbers are very similar. https://github.com/ning/jvm-compressor-benchmark/wiki","10/Dec/12 15:29;slebresne;bq. Is there a reason Cassandra's not using the pure Java version of Snappy?

An very quick look at those benchmarks seems to suggest that the JNI impl is still faster on read, which is what we mainly care about. That being said, it wouldn't be crazy to fallback to the pure Java version if the JNI one can't be loaded, provided this is not too complicated to do. At least patches are welcome :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction invalidates row cache,CASSANDRA-4364,12595422,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,krummas,krummas,21/Jun/12 12:29,12/Mar/19 14:04,13/Mar/19 22:27,02/Jul/12 07:52,1.2.0 beta 1,,,,,0,compaction,,,,,,"Compactions invalidate row cache after CASSANDRA-3862

https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionIterable.java#L87",,,,,,,,,,,,,,,,,,,01/Jul/12 06:23;jbellis;4364.txt;https://issues.apache.org/jira/secure/attachment/12534143/4364.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-21 18:28:36.475,,,no_permission,,,,,,,,,,,,256096,,,Thu Nov 15 01:39:41 UTC 2012,,,,,,0|i0gv1j:,96459,slebresne,slebresne,,,,,,,,,,21/Jun/12 18:28;jbellis;What do you sugggest instead?,"21/Jun/12 18:37;krummas;we discussed this a bit on irc, and the reason that the cached rows were invalidated was to get rid of expired tombstones in the cached rows

maybe it is not a big deal to just leave them there?

","22/Jun/12 07:00;slebresne;I think that at the very least we could simply remove expired tombstones for the in-heap cache as we were doing before. At least I don't understand why that was removing since I don't see a problem with that. For the off-heap cache, I guess the choice is between keeping the invalidation or leaving the expired tombstone in cache. Not sure what is best, it will probably depend a bit on the use cases, but my hunch is that just leaving the tombstone in cache is probably a win most of the time.","01/Jul/12 06:23;jbellis;Agreed, the earlier compromise (removeDeletedInCache for on-heap cache; ignore for serializing) is probably best.  Patch attached to restore that.

See CASSANDRA-2304 and CASSANDRA-3921 for more background.",02/Jul/12 06:18;slebresne;+1,02/Jul/12 07:52;jbellis;committed,"15/Nov/12 01:39;rcoli;Affects Version/s: 1.2.0 beta 1 
Fix Version/s: 1.1.3

It affects versions after the version it was fixed in? Seems unlikely? Are these transposed? :)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bug when trying to describe a cf in a pre cql3 case sensitive keyspace,CASSANDRA-4385,12596170,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,al@ooyala.com,al@ooyala.com,27/Jun/12 21:36,12/Mar/19 14:04,13/Mar/19 22:27,26/Jul/12 17:40,1.1.3,,,,,0,cqlsh,,,,,,"I can't describe column families in my schema defined via cassandra-cli. Update also seems to fail for the same CF's.

CREATE KEYSPACE Hastur
  with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
  and strategy_options = {replication_factor:2};

CREATE COLUMN FAMILY LookupByKey
  with compaction_strategy = 'LeveledCompactionStrategy'
  and compression_options = null;

Then later, https://gist.github.com/3006886","Linux, Hotspot JDK6, Cassandra 1.1.0 from tarball unmodified, stock config.",,,,,,,,,,,,,,,,,,09/Jul/12 22:48;thepaul;0001-cqlsh-Fix-error-reporting-for-unknown-CFs.patch;https://issues.apache.org/jira/secure/attachment/12535756/0001-cqlsh-Fix-error-reporting-for-unknown-CFs.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-30 00:41:56.101,,,no_permission,,,,,,,,,,,,256114,,,Thu Jul 26 17:40:12 UTC 2012,,,,,,0|i0gvaf:,96499,brandon.williams,brandon.williams,,,,,,,,,,29/Jun/12 20:08;al@ooyala.com;Recreating the keyspace and CF via cqlsh instead of cassandra-cli resolves the issue. This isn't really practical on my production systems.,30/Jun/12 00:41;brandon.williams;You need to quote the cf/ks in cql3 since it is case insensitive.,30/Jun/12 01:17;al@ooyala.com;Quoted is also broken.,"30/Jun/12 20:39;thepaul;(Pasting the contents of the gist for posterity.)

{noformat}
cqlsh:system> use Hastur;
Bad Request: Keyspace 'hastur' does not exist
cqlsh:system> use ""Hastur"";
cqlsh:""Hastur""> describe columnfamily lookupbykey;

Keyspace NotFoundException() not found.
cqlsh:""Hastur""> describe columnfamily LookupByKey;

Keyspace NotFoundException() not found.
cqlsh:""Hastur""> describe columnfamily ""LookupByKey"";

Keyspace NotFoundException() not found.

cqlsh:system> use system;
cqlsh:system> desc columnfamily NodeIdInfo;

CREATE COLUMNFAMILY NodeIdInfo (
  KEY blob PRIMARY KEY
) WITH
  comment='nodeId and their metadata' AND
  comparator='TimeUUIDType' AND
  read_repair_chance=0.000000 AND
  gc_grace_seconds=0 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write=True AND
  compaction_strategy_class='SizeTieredCompactionStrategy';
{noformat}","09/Jul/12 22:45;thepaul;The root cause of this should be fixed in the 1.1 branch already by CASSANDRA-4173 and CASSANDRA-4198, but the error reporting is clearly also broken here. ""{{Keyspace NotFoundException() not found.}}"" is messed up.

Al, you can safely use a more recent cqlsh from the 1.1 branch if you'd like.

I'll put up a fix for the error reporting.","09/Jul/12 22:48;thepaul;Attached patch fixes error reporting when a columnfamily is not found.

Also available in my github, in the 4385 branch; current version tagged pending/4385.

https://github.com/thepaul/cassandra/tree/pending/4385","10/Jul/12 01:38;al@ooyala.com;If I 'desc schema' in cqlsh --cql3 using a build of either HEAD or 4385 from your tree, I still can't see my application keyspace, likely because it's called ""Hastur"". If I run cqlsh --cql2 everything shows up.

It could be that the answer is, ""rebuild your schema using CQL 3"" ...","10/Jul/12 01:42;al@ooyala.com;The ""Hastur"" schema was created with this file: https://github.com/ooyala/hastur-server/blob/master/tools/cassandra/create_keyspace.cass

Weirdly, the ""HasturTrigger"" keyspace shows up in CQL3 but ""Hastur"" does not.
","10/Jul/12 04:30;thepaul;Interesting. Do you see any errors, or does it just not show up?

There are some related issues with DESC that are fixed in CASSANDRA-4380, which will probably help too. I've merged the 4380 branch into the 4385 one in my github (new tag is pending/4385-2). Give that a try.",26/Jul/12 17:40;brandon.williams;Worked for me.  Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HHOM.scheduleAllDeliveries still expects tokens as row keys,CASSANDRA-4389,12596244,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,soverton,soverton,soverton,28/Jun/12 11:44,12/Mar/19 14:04,13/Mar/19 22:27,28/Jun/12 21:20,,,,,,0,,,,,,,scheduleAllDeliveries needs updating to expect hostIds instead of tokens in the HINTS_CF,,,,,,,,,,,,,,,,,,,28/Jun/12 11:46;soverton;4389.patch;https://issues.apache.org/jira/secure/attachment/12533812/4389.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-28 21:20:00.374,,,no_permission,,,,,,,,,,,,256118,,,Thu Jun 28 21:20:00 UTC 2012,,,,,,0|i0gvc7:,96507,urandom,urandom,,,,,,,,,,28/Jun/12 11:46;soverton;Attached patch,28/Jun/12 21:20;urandom;committed; thanks Sam!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
use data size ratio in liveRatio instead of live size : serialized throughput,CASSANDRA-4399,12596527,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,01/Jul/12 06:59,12/Mar/19 14:04,13/Mar/19 22:27,04/Jul/12 09:49,1.1.3,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,02/Jul/12 18:25;jbellis;4399-v2.txt;https://issues.apache.org/jira/secure/attachment/12534307/4399-v2.txt,01/Jul/12 07:24;jbellis;4399.txt;https://issues.apache.org/jira/secure/attachment/12534144/4399.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-07-02 08:30:10.18,,,no_permission,,,,,,,,,,,,256127,,,Wed Jul 04 09:49:39 UTC 2012,,,,,,0|i0gvgf:,96526,slebresne,slebresne,,,,,,,,,,01/Jul/12 07:24;jbellis;patch attached.  primarily this means that overwritten columns no longer artificially inflate liveRatio.,"02/Jul/12 08:30;slebresne;We'll need to reinitialize sizeDelta to 0 in the do-loop in AtomicSortedColumns.addAllColumns otherwise we'll report buggy numbers when there is contention.

Otherwise lgtm, except maybe that we might want to add in the comment on ISortedColumns.addAll() that the returned long is a delta and is not always defined correctly (I am not a fan  how having the return value not always be a valid value but I guess it might a necessary evil for performance).",02/Jul/12 18:25;jbellis;Patch to split out addAllWithSizeDelta to make it clear when we try to do an unsupported operation.,04/Jul/12 09:49;slebresne;This was committed by Jonathan as 8674784932ee80 while JIRA was down.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Subcolumns not removed when compacting tombstoned super column,CASSANDRA-4396,12596424,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,nickmbailey,nickmbailey,29/Jun/12 17:34,12/Mar/19 14:04,13/Mar/19 22:27,10/Jul/12 13:36,1.0.11,1.1.3,,,,1,compaction,,,,,,"When we compact a tombstone for a super column with the old data for that super column, we end up writing the deleted super column and all the subcolumn data that is now worthless to the new sstable. This is especially inefficient when reads need to scan tombstones during a slice.

Here is the output of a simple test I ran to confirm:

insert supercolumn, then flush
{noformat}
Nicks-MacBook-Pro:12:20:52 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-1-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": -9223372036854775808, ""subColumns"": [[""737562636f6c31"",""7468697320697320612074657374"",1340990212532000]]}}
}
{noformat}

delete supercolumn, flush again

{noformat}
[Nicks-MacBook-Pro:12:20:59 cassandra-1.0] cassandra$ bin/nodetool -h localhost flush
[Nicks-MacBook-Pro:12:22:41 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-2-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": 1340990544005000, ""subColumns"": []}}
}
{noformat}

compact and check resulting sstable

{noformat}
[Nicks-MacBook-Pro:12:22:55 cassandra-1.0] cassandra$ bin/nodetool -h localhost compact 
[Nicks-MacBook-Pro:12:23:09 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-3-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": 1340990544005000, ""subColumns"": [[""737562636f6c31"",""7468697320697320612074657374"",1340990212532000]]}}
}
[Nicks-MacBook-Pro:12:23:20 cassandra-1.0] cassandra$ 
{noformat}",,,,,,,,,,,,,,,,,,,06/Jul/12 23:02;jbellis;4396-2.txt;https://issues.apache.org/jira/secure/attachment/12535455/4396-2.txt,29/Jun/12 23:01;jbellis;4396.txt;https://issues.apache.org/jira/secure/attachment/12534068/4396.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-06-29 18:04:15.246,,,no_permission,,,,,,,,,,,,256124,,,Tue Jul 10 13:36:00 UTC 2012,,,,,,0|i0gvf3:,96520,slebresne,slebresne,,,,,,,,,,"29/Jun/12 18:04;jbellis;This code in removeDeletedSuper is intended to address this scenario:

{code}
.               // remove subcolumns if
                // (a) the subcolumn itself is tombstoned or
                // (b) the supercolumn is tombstoned and the subcolumn is not newer than it
                if (subColumn.timestamp() <= minTimestamp
                    || subColumn.getLocalDeletionTime() < gcBefore)
                {
                    subIter.remove();
                }
{code}

Unclear why it's not actually working here.","29/Jun/12 21:43;nickmbailey;This is also a problem with simply flushing super column deletes:

From a fresh cluster I can create a supercolumn with subcolumns, delete that supercolumn, trigger a flush with nodetool, and observe the subcolumn data in the flushed sstable with sstable2json.","29/Jun/12 22:04;jbellis;I'm okay with not calling removeDeleted on flush, in fact I think it's probably the right tradeoff given that the extra overhead will be a no-op most of the time, but compaction should definitely evict it.","29/Jun/12 23:01;jbellis;This was introduced by CASSANDRA-3234.  Fix attached to allow dropping columns shadowed by supercolumn (and row) tombstones that are not yet expired.

Reducing gcgs is one way to work around the problem without this patch.  This only affects in-memory compactions, so reducing in_memory_compaction_limit_in_mb could also mitigate it.",02/Jul/12 07:00;slebresne;+1,02/Jul/12 08:05;jbellis;committed,"06/Jul/12 21:52;nickmbailey;[~jbellis] I still see the same behavior with this patch applied. Major compaction of an sstable with subcolumns and an sstable with a tombstone for the relevant super column, produces a new sstable with the super column tombstoned but the subcolumns still present.",06/Jul/12 23:02;jbellis;patch to fix mixing user-provided timestamps and local deletion time.  includes unit test.,06/Jul/12 23:03;jbellis;(also fixes < to <=),"06/Jul/12 23:05;jbellis;patch is against 1.0, merging to trunk fixed the bug there...",07/Jul/12 00:12;ctrahman;v2 is working here - thanks!,"10/Jul/12 10:19;slebresne;v2 lgtm, +1.",10/Jul/12 13:36;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cqlsh tab completion error in ""CREATE KEYSPACE""",CASSANDRA-4334,12560374,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,tpatterson,tpatterson,12/Jun/12 16:25,12/Mar/19 14:04,13/Mar/19 22:27,26/Sep/12 15:09,1.1.6,,,,,0,,,,,,,"The following:
{code}
cqlsh> CREATE KEYSPACE test WITH strategy_class = 'S<TAB>
{code}
will tab complete like this:
{code}
cqlsh> CREATE KEYSPACE test WITH strategy_class = 'SimpleStrategy '
{code}
Note the extra space after SimpleStrategy. Not a big deal to remove, but it could be misleading to people.","ubuntu, git:cassandra-1.1. I see the error in cqlsh with cql2 and cql3.",,,,,,,,,,,,,,,,,,25/Sep/12 15:53;iamaleksey;4334-v2-patch.txt;https://issues.apache.org/jira/secure/attachment/12546534/4334-v2-patch.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-25 06:19:54.326,,,no_permission,,,,,,,,,,,,256070,,,Wed Sep 26 15:09:12 UTC 2012,,,,,,0|i0gup3:,96403,brandon.williams,brandon.williams,,,,,,,,,,"25/Sep/12 06:19;thepaul;Aleksey: nice, this was the same solution I came up with tonight before realizing you'd already solved it. You'll also want to check, though, that {{lasttype != 'unclosedName'}} - the same bug will manifest for double-quoted names in cql3 mode otherwise.","25/Sep/12 15:40;iamaleksey;Thanks, Paul. I'll update the patch.",26/Sep/12 15:09;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove RangeKeySample from attributes in jmx,CASSANDRA-4452,12599522,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jendap,jendap,jendap,19/Jul/12 20:15,12/Mar/19 14:04,13/Mar/19 22:27,25/Jul/12 18:13,1.1.3,,,,,0,jmx,,,,,,"RangeKeySample in org.apache.cassandra.db:type=StorageService MBean can be really huge (over 200MB in our case). That's a problem for monitoring tools as they're not build for that. Recommended and often used mx4j may be killer in this situation.

It would be good enough to make RangeKeySample ""operation"" instead of ""attribute"" in jmx. Looking at how MBeanServer.registerMBean() works we can do one of the following:
a) add some dummy parameter to getRangeKeySample
b) name it differently - not like getter (next time somebody will rename it back)
c) implement MXBean instead of MBean (a lot of work)

Any of those work. All of them are ""hacks"". Any better idea?



BTW: It's blocker for some installations. Our update to 1.1.2 caused downtime, downgrade back to 1.0.x, repairs, etc.",,,,,,,,,,,,,,,,,,,24/Jul/12 23:33;jendap;cassandra-1.1.2-4452.txt;https://issues.apache.org/jira/secure/attachment/12537768/cassandra-1.1.2-4452.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-19 22:42:31.911,,,no_permission,,,,,,,,,,,,256168,,,Wed Jul 25 18:13:26 UTC 2012,,,,,,0|i0gw13:,96619,jbellis,jbellis,,,,,,,,,,19/Jul/12 22:42;jbellis;I would be fine with simply renaming it.,20/Jul/12 09:21;slebresne;Same here. Renaming it to be an operation with a comment to explain why it shouldn't be renamed back to an attribute.,"24/Jul/12 23:33;jendap;""Sample"" is also verb. We can rename it from getRangeKeySample() to sampleKeyRange(). Ok?","25/Jul/12 18:13;jbellis;LGTM, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If processor is missing from /proc/cpuinfo, cassandra will not start",CASSANDRA-4401,12596665,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,acobley,acobley,02/Jul/12 15:17,12/Mar/19 14:04,13/Mar/19 22:27,11/Jul/12 14:30,1.1.3,,Packaging,,,0,,,,,,,"cassandra.env.sh does an egrep on /proc/cpuinfo in order to find the number of processors on the system.  If /proc/cpuinfo does not contain a processor :# line then the script will fail because of a divide  by 0 error.  Changing the Linux section of cassandra.env.sh to:


Linux)
            system_memory_in_mb=`free -m | awk '/Mem:/ {print $2}'`
            system_cpu_cores=`egrep -c 'processor([[:space:]]+):.*' /proc/cpuinfo`
            if [ ""$system_cpu_cores"" -lt ""1"" ]
            then
               system_cpu_cores=""1""
            fi
is a possible fix",,,,,,,,,,,,,,,,,,,05/Jul/12 12:53;acobley;4401.txt;https://issues.apache.org/jira/secure/attachment/12535192/4401.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-11 14:30:02.313,,,no_permission,,,,,,,,,,,,256129,,,Wed Jul 11 14:45:27 UTC 2012,,,,,,0|i0gvhb:,96530,brandon.williams,brandon.williams,,,,,,,,,,"05/Jul/12 12:54;acobley;Patch attached, hope I've done it right !","11/Jul/12 14:30;brandon.williams;Committed, but move outside of the linux-specific block in case this happens elsewhere, since zero cores is always invalid.","11/Jul/12 14:45;acobley;Good idea !
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cleanup uses global partitioner to estimate ranges in index sstables,CASSANDRA-4403,12596836,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Jul/12 22:19,12/Mar/19 14:04,13/Mar/19 22:27,04/Jul/12 02:29,1.1.3,1.2.0 beta 1,,,,0,compaction,,,,,,"Introduced in CASSANDRA-1404, CleanupTest is showing this on trunk (on stderr, so test doesn't actually fail):

{noformat}
    [junit] java.lang.ClassCastException: org.apache.cassandra.dht.Token$KeyBound cannot be cast to org.apache.cassandra.dht.Token
    [junit]     at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:24)
    [junit]     at org.apache.cassandra.dht.Range$1.compare(Range.java:386)
    [junit]     at org.apache.cassandra.dht.Range$1.compare(Range.java:383)
    [junit]     at java.util.Arrays.mergeSort(Arrays.java:1270)
    [junit]     at java.util.Arrays.sort(Arrays.java:1210)
    [junit]     at java.util.Collections.sort(Collections.java:159)
    [junit]     at org.apache.cassandra.dht.Range.normalize(Range.java:382)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:570)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.estimatedKeysForRanges(SSTableReader.java:549)
    [junit]     at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:111)
    [junit]     at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:136)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
{noformat}

This doesn't happen on the 1.1 branch (less robust test?) but the problem is still there.",,,,,,,,,,,,,,,,,,,02/Jul/12 22:25;jbellis;4403.txt;https://issues.apache.org/jira/secure/attachment/12534468/4403.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-04 02:29:05.235,,,no_permission,,,,,,,,,,,,256130,,,Wed Jul 04 02:29:05 UTC 2012,,,,,,0|i0gvhz:,96533,yukim,yukim,,,,,,,,,,"02/Jul/12 22:25;jbellis;Actually, looks like there are two places to fix: one in getExpectedCompactedFileSize that I've attached a fix for, and another in STCS from CASSANDRA-4022.  Not sure off the top of my head how to fix that one so I'll leave to Yuki. :)",02/Jul/12 22:27;jbellis;Decided to split the second part out to CASSANDRA-4404 since it affects only 1.2.,04/Jul/12 02:29;yukim;Reviewed and committed during today's JIRA crisis. Closing as Fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
leveled compaction generates too small sstables,CASSANDRA-4419,12597744,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,noa,noa,06/Jul/12 10:55,12/Mar/19 14:04,13/Mar/19 22:27,12/Jul/12 17:52,1.0.11,,,,,0,,,,,,,"When I set sstable_size_in_mb to 96 I end up with sstable data files no larger than 60M.

This in turn messes up the LeveledManifest calculation since it finds compaction candidates by summing up the size of sstables at a particular level and comparing it to the configured size multiplied by the desired number of sstables at a level, resulting in ~20 sstables in level 1 instead of the 10 that one would expect from looking at LeveledManifest.

Some additional logging here reveals that the position parameter passed to LeveledCompactionTask.newSSTableSegmentThresholdReached() is significantly higher than the size of the output file.",,,,,,,,,,,,,,,,,,,11/Jul/12 08:48;slebresne;4419.txt;https://issues.apache.org/jira/secure/attachment/12536000/4419.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-06 13:41:49.26,,,no_permission,,,,,,,,,,,,256141,,,Thu Jul 12 17:52:51 UTC 2012,,,,,,0|i0gvnr:,96559,jbellis,jbellis,,,,,,,,,,"06/Jul/12 13:41;slebresne;The problem is that compaction is considering uncompressed size for position (and thus cut segments at 96MB of uncompressed data) while the manifest consider compressed sizes for level sizes. This was fixed by CASSANDRA-4341 for cassandra-1.1, but I guess we might want to backport that part of the patch to 1.0.",11/Jul/12 08:48;slebresne;Patch that extract the relevant part from CASSANDRA-4341.,11/Jul/12 20:24;jbellis;+1,12/Jul/12 17:52;slebresne;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool drain sometimes doesn't mark commitlog fully flushed,CASSANDRA-4446,12599347,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,rcoli,rcoli,18/Jul/12 21:30,12/Mar/19 14:04,13/Mar/19 22:27,15/Jan/13 15:14,1.2.1,,Tool/nodetool,,,2,,,,,,,"I recently wiped a customer's QA cluster. I drained each node and verified that they were drained. When I restarted the nodes, I saw the commitlog replay create a memtable and then flush it. I have attached a sanitized log snippet from a representative node at the time. 

It appears to show the following :
1) Drain begins
2) Drain triggers flush
3) Flush triggers compaction
4) StorageService logs DRAINED message
5) compaction thread excepts
6) on restart, same CF creates a memtable
7) and then flushes it [1]

The columnfamily involved in the replay in 7) is the CF for which the compaction thread excepted in 5). This seems to suggest a timing issue whereby the exception in 5) prevents the flush in 3) from marking all the segments flushed, causing them to replay after restart.

In case it might be relevant, I did an online change of compaction strategy from Leveled to SizeTiered during the uptime period preceding this drain.

[1] Isn't commitlog replay not supposed to automatically trigger a flush in modern cassandra?","ubuntu 10.04 64bit
Linux HOSTNAME 2.6.32-345-ec2 #48-Ubuntu SMP Wed May 2 19:29:55 UTC 2012 x86_64 GNU/Linux
sun JVM
cassandra 1.0.10 installed from apache deb",,,,,,,,,,,,,,,,,,11/Jan/13 23:46;jbellis;4446.txt;https://issues.apache.org/jira/secure/attachment/12564518/4446.txt,18/Jan/13 23:46;rcoli;CASSANDRA-4446--1.0.12_to_1.1.8.txt;https://issues.apache.org/jira/secure/attachment/12565581/CASSANDRA-4446--1.0.12_to_1.1.8.txt,18/Jul/12 21:35;rcoli;cassandra.1.0.10.replaying.log.after.exception.during.drain.txt;https://issues.apache.org/jira/secure/attachment/12537076/cassandra.1.0.10.replaying.log.after.exception.during.drain.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-09-08 04:02:32.275,,,no_permission,,,,,,,,,,,,245708,,,Fri Jan 18 23:45:47 UTC 2013,,,,,,0|i06ctz:,35013,yukim,yukim,,,,,,,,,,"08/Sep/12 04:02;scode;In general, nodetool drain never seems to completely eliminate on-startup log replay. I observe this all the time on all clusters. It certainly cuts down the amount of replay done, but either never or fairly seldom eliminates it completely - at least not based on log messages indicating replay.

Never had time to investigate.","08/Oct/12 23:40;kmueller;Also seeing this in an upgrade from 1.0.xx to 1.1.15:

 INFO 16:29:17,486 completed pre-loading (3 keys) key cache.
 INFO 16:29:17,495 Replaying /data2/commit-cassandra/CommitLog-1349727956484.log
 INFO 16:29:17,503 Replaying /data2/commit-cassandra/CommitLog-1349727956484.log
 INFO 16:29:18,495 GC for ParNew: 3506 ms for 4 collections, 1963062320 used; max is 17095983104
 INFO 16:29:18,498 Finished reading /data2/commit-cassandra/CommitLog-1349727956484.log
 INFO 16:29:18,499 Log replay complete, 0 replayed mutations


This is a standard upgrade process which includes a drain","15/Oct/12 15:41;omid;I also experience this every time I drain / restart (up until latest 1.1.6 but not on 1.1.6 itself any more) and getting this message in log:

{quote}
2012-10-12_15:50:36.92191  INFO 15:50:36,921 Log replay complete, N replayed mutations   
{quote}

with N being non-zero. I wonder if this is a cause of double-counts for Counter mutations.","21/Nov/12 06:36;tamarfraenkel;I had the same experience, when I upgraded my cluster from 1.0.9 to 1.0.11. I ran drain before the upgrade, upgrade on the node finished and node restarted at 2012-11-20 10:20:58, but then I see in the logs reply of commit log:
{quote} 
 INFO [main] 2012-11-20 09:41:13,918 CommitLog.java (line 172) Replaying /raid0/cassandra/commitlog/CommitLog-1353402218337.log
 INFO [main] 2012-11-20 09:41:20,360 CommitLog.java (line 179) Log replay complete, 0 replayed mutations
 INFO [main] 2012-11-20 10:11:35,635 CommitLog.java (line 167) No commitlog files found; skipping replay
 INFO [main] 2012-11-20 10:21:11,631 CommitLog.java (line 172) Replaying /raid0/cassandra/commitlog/CommitLog-1353404473899.log
 INFO [main] 2012-11-20 10:21:18,119 CommitLog.java (line 179) Log replay complete, 6413 replayed mutations
 INFO [main] 2012-11-20 10:55:46,435 CommitLog.java (line 172) Replaying /raid0/cassandra/commitlog/CommitLog-1353406871619.log
 INFO [main] 2012-11-20 10:55:54,139 CommitLog.java (line 179) Log replay complete, 3 replayed mutations
{quote} 
This caused over increment of counters
",21/Nov/12 12:00;jbellis;This is going to stand as a known limitation with 1.0.x; so far it looks like it is fixed in latest 1.1.,"04/Jan/13 00:16;mkjellman;did a nodetool drain before 1.1.7 -> 1.2.0. upon starting 1.2.0 every node in my cluster still replayed the commit logs and created mutations.

{code}
 INFO 13:17:06,529 DRAINING: starting drain process
 INFO 13:17:06,529 Stop listening to thrift clients
 INFO 13:17:06,532 Announcing shutdown
 INFO 13:17:07,536 Waiting for messaging service to quiesce
 INFO 13:17:07,537 MessagingService shutting down server thread.

... normal startup stuff..
 INFO 13:20:20,182 Replaying /ssd/commitlog/CommitLog-1355265349912.log
 INFO 13:20:24,166 Finished reading /ssd/commitlog/CommitLog-1355265349912.log
 INFO 13:20:24,166 Replaying /ssd/commitlog/CommitLog-1355265349914.log
 INFO 13:20:26,700 Finished reading /ssd/commitlog/CommitLog-1355265349914.log
 INFO 13:20:26,701 Replaying /ssd/commitlog/CommitLog-1355265349915.log
 INFO 13:20:28,118 Finished reading /ssd/commitlog/CommitLog-1355265349915.log
... more replay lines ...
 INFO 13:22:00,061 Log replay complete, 8052 replayed mutations
 INFO 13:22:00,358 Possible old-format hints found. Truncating
 INFO 13:22:00,370 Enqueuing flush of Memtable-local@1908923620(402/402 serialized/live bytes, 13 ops)
 INFO 13:22:00,372 Writing Memtable-local@1908923620(402/402 serialized/live bytes, 13 ops)
 INFO 13:22:00,494 Cassandra version: 1.2.0
 INFO 13:22:00,495 Thrift API version: 19.35.0
 INFO 13:22:00,495 CQL supported versions: 2.0.0,3.0.0 (default: 3.0.0)
 INFO 13:22:00,534 Loading persisted ring state
 INFO 13:22:00,537 Starting up server gossip
 WARN 13:22:00,557 No host ID found, created dd3a40e2-fef1-4574-87b8-e2929fd80235 (Note: This should happen exactly once per node).
{code}",04/Jan/13 10:11;slebresne;Let's reopen then if it doesn't sound like it's fixed in recent releases.,04/Jan/13 10:59;arodrime;+1. Good to see this ticket reopen. Drain didn't work for a while. I remove all the commit logs files before a restart to avoid counters operations to replay.,"04/Jan/13 20:11;tpatterson;It always replays 3 mutations when I follow these steps:
{code}
bin/cassandra
tools/bin/cassandra-stress --operation=INSERT --num-keys=100000
bin/nodetool drain
bin/cassandra
{code}

This is on trunk, commit acf30622","11/Jan/13 23:46;jbellis;System tables were not getting flushed.  This is the source of the extra replaying.  Patch attached to fix this, and also parallelize flushing.",14/Jan/13 20:28;yukim;+1,"14/Jan/13 22:07;jbellis;Committed.

Note that earlier releases can workaround by manually running flush against system KS before drain.","18/Jan/13 00:23;rcoli;While I'm sure that this does fix one real cause of drain not working in trunk (yay!), one of the symptoms I've heard reported in the 1.0.x - 1.1.5 timeframe is that ""my counters over-counted on upgrade, despite drain"". Most recent report was 1.0.12->1.1.8 with drain being run as part of the upgrade process.

NEWS.txt says :

""If you using counters and upgrading from a version prior to 1.1.6, you should drain existing Cassandra nodes prior to the upgrade to prevent overcount during commitlog replay (see CASSANDRA-4782).  For non-counter uses, drain is not required but is a good practice to minimize restart time.""

If drain in these versions can't be counted on (heh) to actually work for this purpose (which reports suggest it cannot), then I propose changing this line to read ""drain existing nodes and remove their commitlog"".","18/Jan/13 00:34;jbellis;No counter mutations are double-counted, only the unflushed system changes are replayed.","18/Jan/13 01:04;rcoli;If only unflushed system changes are replayed, how do you account for :

""I upgraded from 1.0.12 to 1.1.8, using drain, and I noticed overcounting counters"" ?

It's quite possible that upgrading from 1.1.x to 1.1.y>x does not in fact replay anything other than system keyspace and does not incur double counting of counters. I am however pretty confident based on multiple reports of the above quoted issue that counter increments may be over-replayed if one uses drain (as NEWS.txt suggests) while upgrading from 1.0.x to >1.1.6.

If this is being dealt with as ""known limitation of 1.0.x"", then I continue to suggest the above change to NEWS.txt, as otherwise people using counters in 1.0.x WILL incur double-increment while upgrading per the instructions in NEWS.txt.","18/Jan/13 01:11;jbellis;Show me how to reproduce it and I will re-evaluate my position, but as near as I can tell the advice in NEWS is still best practice.","18/Jan/13 23:45;rcoli;How to reproduce it, from the multiple reports :

1) Drain and stop cluster with counters on 1.0.x
2) Start same cluster on 1.1.x
3) Notice commitlog replay of the counter columnfamily and that your counters have over-counted

Attached is a log from the latest reporter, CASSANDRA-4446--1.0.12_to_1.1.8.txt. It shows the following.

1) Drain starts and completes on 1.0.12
2) Cluster then starts on 1.1.8, and replays the commit log
3) As part of commitlog replay, it flushes various CFs including titan3/RMEntityCount/, which is a counter columnfamily; machine has 4gb of heap and the flush is while thrift is down and the node has not jumped state to normal, so it seems reasonable to conjecture this flush is part of commitlog replay
4) It then logs ""10698 replayed mutations"", which adds further support to the idea that these Counts are part of replay
5) Operator then noticed a significant percentage of records had overcounted in this columnfamily",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Updating column family using cassandra-cli results in ""Cannot modify index name""",CASSANDRA-4439,12598764,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,a.schultz,a.schultz,13/Jul/12 22:01,12/Mar/19 14:04,13/Mar/19 22:27,18/Jul/12 22:16,1.1.3,,Legacy/Tools,,,0,cli,schema,,,,,"Using cassandra-cli the following update to a column family worked in 1.1.0:
{code}
create keyspace testing;
use testing;

create column family Album
with comparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_type: KEYS}
];

update column family Album
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

After upgrading to 1.1.2, the update statement fails with the following exception in system.log:
{quote}
ERROR [Thrift:16] 2012-07-13 14:51:54,558 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:188)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:161)
        at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1063)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3520)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3508)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
        ... 11 more
Caused by: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:676)
        at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:463)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:407)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.ColumnDefinition.apply(ColumnDefinition.java:214)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:758)
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:672)
        ... 9 more
ERROR [MigrationStage:1] 2012-07-13 14:51:54,561 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:676)
        at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:463)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:407)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.ColumnDefinition.apply(ColumnDefinition.java:214)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:758)
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:672)
        ... 9 more
{quote}


After further testing the following works in 1.1.2:
{code}
create keyspace testing;
use testing;
create column family Album
with comparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_name: 'Album_profileId_idx', index_type: KEYS}
];

update column family Album
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_name: 'Album_profileId_idx', index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

So it appears that if you did not specify an index_name when creating the column originally, you cannot update the column family anymore.","cassandra 1.1.2, RHEL6.3, running under java-1.6.0-sun",,,,,,,,,,,,,,,,,,17/Jul/12 14:22;xedin;CASSANDRA-4439.patch;https://issues.apache.org/jira/secure/attachment/12536821/CASSANDRA-4439.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-17 15:02:54.349,,,no_permission,,,,,,,,,,,,256159,,,Mon Jul 23 17:04:28 UTC 2012,,,,,,0|i0gvw7:,96597,yukim,yukim,,,,,,,,,,"16/Jul/12 12:57;a.schultz;For some extra information I modified ./src/java/org/apache/cassandra/config/ColumnDefinition.java:214 to show me what the index name issue was and it appears if you don't specify the index on the update, the index is automatically created at ""Album_profileId_idx_2"".  

{quote}
ERROR [Thrift:1] 2012-07-16 05:50:11,566 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:188)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:161)
        at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1063)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3520)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3508)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
        ... 11 more
Caused by: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:676)
        at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:463)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:407)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at org.apache.cassandra.config.ColumnDefinition.apply(ColumnDefinition.java:214)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:758)
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:672)
        ... 9 more
ERROR [MigrationStage:1] 2012-07-16 05:50:11,570 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:676)
        at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:463)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:407)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at org.apache.cassandra.config.ColumnDefinition.apply(ColumnDefinition.java:214)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:758)
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:672)
        ... 9 more
{quote}


I was able to work around the issue if I specify the auto-generated index name in the update statement.
{code}
create keyspace testing;
use testing;
create column family Album
with comparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_type: KEYS}
];

update column family Album
and column_metadata =
[
    {column_name: 'profileId', validation_class: UTF8Type, index_name: 'Album_profileId_idx', index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

Also for anyone wondering, you can find the auto-generated index name by doing a show schema on the keyspace.  I'm going to drop the severity since I can work around the issue by adjusting my update statements.","16/Jul/12 13:41;a.schultz;Also despite the fact that it says you cannot rename the index, it will rename the index if you run the command and restart cassandra.  The exception does not stop the schema from being updated.  Also if you attempt to update the schema again with a corrected index name after it throws the exception, it will not accept it until you restart.  At which point your index name has already been changed to the first index name you attempted.

An example of this is:
{code}
create keyspace testing;
use testing;
create column family Album
with comparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_type: KEYS}
];

update column family Album
and column_metadata =
[
    {column_name: 'profileId', validation_class: UTF8Type, index_name: 'badindex', index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

The previous update will throw an exception and if you attempt to run the following it will still throw the exception:
{code}
update column family Album
and column_metadata =
[
    {column_name: 'profileId', validation_class: UTF8Type, index_name: 'Album_profileId_idx', index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

Also if you restart and do a show schema, your index is now named ""badindex"".
{code}

[default@testing] show schema;
create keyspace testing
  with placement_strategy = 'NetworkTopologyStrategy'
  and strategy_options = {datacenter1 : 1}
  and durable_writes = true;

use testing;

create column family Album
  with column_type = 'Standard'
  and comparator = 'UTF8Type'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'BytesType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and column_metadata = [
    {column_name : 'profileId',
    validation_class : UTF8Type,
    index_name : 'badindex',
    index_type : 0},
    {column_name : 'postedDate',
    validation_class : LongType}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.SnappyCompressor'};
{code}",17/Jul/12 15:02;jbellis;Isn't existingIndexNames supposed to take care of this case?,"17/Jul/12 15:08;xedin;No, it doesn't do that.",17/Jul/12 15:14;a.schultz;I have verified this patch fixes my issue.  thanks.,"17/Jul/12 18:45;xedin;Thanks, Alex. addDefaultIndexNames() works just fine when cql where we have all of the metadata (index_names/index_type) but for cli, where we need to provide all of the column attributes by hand, it wasn't because index_name should be filled in by user every time even if it was auto generated, so just checking that index_name is not set there (when no explicit name were given) is not good enough for thrift access.",18/Jul/12 01:58;yukim;+1,18/Jul/12 22:16;xedin;Committed.,"23/Jul/12 17:04;hudson;Integrated in Cassandra #1768 (See [https://builds.apache.org/job/Cassandra/1768/])
    fixes small bug introduced by CASSANDRA-4439 (Revision e220efa2a87c8232d87bdca9f2a02cfcef9f1a4c)

     Result = ABORTED
xedin : 
Files : 
* src/java/org/apache/cassandra/config/CFMetaData.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool fail to setcompactionthreshold,CASSANDRA-4455,12599710,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jasont,jasont,21/Jul/12 10:17,12/Mar/19 14:04,13/Mar/19 22:27,25/Jul/12 19:03,1.1.4,,Tool/nodetool,,,0,,,,,,,"first change compaction threshold from 4/32 to 2/2
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 2 2
It successful
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 4 32
Exception in thread ""main"" java.lang.RuntimeException: The min_compaction_threshold cannot be larger than the max.
        at org.apache.cassandra.db.ColumnFamilyStore.setMinimumCompactionThreshold(ColumnFamilyStore.java:1697)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeSetter(MBeanIntrospector.java:238)
        at com.sun.jmx.mbeanserver.PerInterface.setAttribute(PerInterface.java:84)
        at com.sun.jmx.mbeanserver.MBeanSupport.setAttribute(MBeanSupport.java:240)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.setAttribute(DefaultMBeanServerInterceptor.java:762)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.setAttribute(JmxMBeanServer.java:699)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.setAttribute(RMIConnectionImpl.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:303)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

The tool first try to set min then max, so it failed, since orign max is smaller the new min.
The work around is:
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 2 32
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 4 32
",Cassandra 1.0.3,,,,,,,,,,,,,,,,,,25/Jul/12 16:06;xedin;CASSANDRA-4455-v2.patch;https://issues.apache.org/jira/secure/attachment/12537847/CASSANDRA-4455-v2.patch,25/Jul/12 13:45;iamaleksey;CASSANDRA-4455.patch;https://issues.apache.org/jira/secure/attachment/12537832/CASSANDRA-4455.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-07-25 16:06:31.697,,,no_permission,,,,,,,,,,,,256171,,,Wed Jul 25 19:03:34 UTC 2012,,,,,,0|i0gw2f:,96625,xedin,xedin,,,,,,,,,,"25/Jul/12 16:06;xedin;Attaching alternative version which adds setCompactionThresholds(int, int) method instead of doing branch checking for old min/max values in nodetool. What do you think, Aleksey?","25/Jul/12 18:06;iamaleksey;I think mine is acceptable, but your version is preferable.
The only thing I don't like is the inconsistency in the argument names between the old setters and the new one - min, max, minCompationThreshold and maxCompationThreshold. The latter ones are redundant in the context - I'd just use 'threshold'.
But this is bikeshedding on my part, doesn't really matter.
Go with v2.","25/Jul/12 19:03;xedin;Committed with names changed to {min,max}Thresholds.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes Don't Restart: Assertion Error on Serializing Cache provider,CASSANDRA-4463,12600072,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,arya,arya,24/Jul/12 21:47,12/Mar/19 14:04,13/Mar/19 22:27,18/Apr/13 18:32,1.1.11,1.2.5,,,,2,,,,,,,"I stopped Cassandra on one of our 1.1.2 nodes and I couldn't start it any more. System.log didn't have much useful info but output.log had this:

java.lang.AssertionError
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:43)
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:39)
        at org.apache.cassandra.cache.SerializingCache.serialize(SerializingCache.java:116)
        at org.apache.cassandra.cache.SerializingCache.put(SerializingCache.java:174)
        at org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:45)
        at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:430)
        at org.apache.cassandra.db.Table.open(Table.java:124)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        at com.netflix.priam.cassandra.NFThinCassandraDaemon.init(NFThinCassandraDaemon.java:41)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Cannot load daemon
Service exit with a return value of 3


Deleting the stuff in saved_caches folder fixed the problem.","Ubuntu 12.04 Precise
Cassandra 1.1.5
Oracle Java 6",,,,,,,,,,,,,,,,,,09/Apr/13 15:01;jbellis;4463.txt;https://issues.apache.org/jira/secure/attachment/12577809/4463.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-09 15:01:29.146,,,no_permission,,,,,,,,,,,,248197,,,Wed Apr 10 02:26:38 UTC 2013,,,,,,0|i09men:,54063,dbrosius,dbrosius,,,,,,,,,,"13/Oct/12 00:43;arya;This is still happening in 1.1.5. I just had to perform some maintenance and restart service and I got this again:

java.lang.AssertionError
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:43)
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:39)
        at org.apache.cassandra.cache.SerializingCache.serialize(SerializingCache.java:116)
        at org.apache.cassandra.cache.SerializingCache.put(SerializingCache.java:174)
        at org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:45)
        at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:446)
        at org.apache.cassandra.db.Table.open(Table.java:124)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:206)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:290)
        at com.netflix.priam.cassandra.NFThinCassandraDaemon.init(NFThinCassandraDaemon.java:41)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Cannot load daemon
Service exit with a return value of 3","09/Apr/13 15:01;jbellis;Null CFs are not allowed in the row cache.  The normal read path checks this as follows:

{code}
.           ColumnFamily data = getTopLevelColumns(QueryFilter.getIdentityFilter(filter.key, new QueryPath(columnFamily)),
                                                   Integer.MIN_VALUE,
                                                   true);
            if (sentinelSuccess && data != null)
                CacheService.instance.rowCache.replace(key, sentinel, data);
{code}

We need to check this during cache population at startup as well, since the cache is only saved intermittently -- a row that contained data when the cache was saved, and subsequently invalidated, will read as null on the following startup.","10/Apr/13 00:55;dbrosius;extraneous semi at end of line.

do you want to increment cachedRowsRead if the data is null?

otherwise +1","10/Apr/13 02:26;jbellis;I'm not 100% sure why we track cachedRowsRead to be honest.  If I were to guess it might be some measure of how much work cache loading cost us at startup, in which case I think we do want to increment.  (This is more explicit in 1.2, where we log the time taken if rows read > 0.)

Fixed EOL and committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyRecordReader hadoop integration fails with ghost keys,CASSANDRA-4466,12600449,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,ndrummond@rim.com,ndrummond@rim.com,27/Jul/12 10:25,12/Mar/19 14:04,13/Mar/19 22:27,31/Jul/12 15:52,1.0.12,1.1.4,,,,0,,,,,,,"When running hadoop-cassandra jobs with range queries over ghost keys, the ColumnFamilyRecordReader throws an exception if the last key in a slice_range query is a ghost key. 

This seems to be related to changes made in CASSANDRA-2855 to prevent ghost keys appearing in a hadoop map. 

The call stack trace is attached.

I made a one-line change to ColumnFamilyRecordReader.java, which seems to solve this issue for us.
",,,,,,,,,,,,,,,,,,,27/Jul/12 10:31;ndrummond@rim.com;4466-stack;https://issues.apache.org/jira/secure/attachment/12538146/4466-stack,27/Jul/12 10:29;ndrummond@rim.com;4466-v0.patch;https://issues.apache.org/jira/secure/attachment/12538145/4466-v0.patch,30/Jul/12 17:46;jbellis;4466.txt;https://issues.apache.org/jira/secure/attachment/12538378/4466.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-07-27 19:21:45.785,,,no_permission,,,,,,,,,,,,256180,,,Tue Jul 31 15:52:40 UTC 2012,,,,,,0|i0gw6f:,96643,ndrummond@rim.com,ndrummond@rim.com,,,,,,,,,,27/Jul/12 10:31;ndrummond@rim.com;exception call stack,"27/Jul/12 19:21;jbellis;setting rows=null is the signal to stop looping over that inputsplit, so that's not going to work very well in the general case.","30/Jul/12 08:35;ndrummond@rim.com;Ah ok, thanks for the input. What's the best approach to solving this ?",30/Jul/12 17:46;jbellis;I think we can just add back the last-seen row.  Patch attached.,31/Jul/12 10:32;ndrummond@rim.com;I can confirm the patch works for our dataset. Thanks for this.,31/Jul/12 14:29;brandon.williams;+1,31/Jul/12 15:52;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Drain causes incorrect error messages: ""Stream took more than 24H to complete; skipping""",CASSANDRA-4484,12601241,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,canadianveggie,canadianveggie,canadianveggie,02/Aug/12 16:41,12/Mar/19 14:04,13/Mar/19 22:27,02/Aug/12 17:57,1.1.4,,,,,0,,,,,,,"After calling drain on a node, there are a bunch of incorrect error messages in the cassandra log file: ""Stream took more than 24H to complete; skipping"".

The problem is in MessagingService.waitForStreaming. It is logging an error if ThreadPoolExecutor.awaitTermination returns true, but if a timeout happens it returns false. See http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html#awaitTermination%28long,%20java.util.concurrent.TimeUnit%29",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-08-02 17:56:32.164,,,no_permission,,,,,,,,,,,,256191,,,Thu Aug 02 17:57:01 UTC 2012,,,,,,0|i0gwcf:,96670,jbellis,jbellis,,,,,,,,,,02/Aug/12 16:43;canadianveggie;Fixed with this pull request: https://github.com/apache/cassandra/pull/12,02/Aug/12 17:56;jbellis;(Introduced in CASSANDRA-3679 for 1.1.0.),"02/Aug/12 17:57;jbellis;patch lgtm, committed.  thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh needs to use system.local instead of system.Versions,CASSANDRA-4491,12601482,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,05/Aug/12 03:27,12/Mar/19 14:04,13/Mar/19 22:27,18/Sep/12 07:32,1.2.0 beta 1,,Legacy/Tools,,,0,cqlsh,,,,,,"Apparently the system.Versions table was removed as part of CASSANDRA-4018. cqlsh in 1.2 should use system.local preferentially, and fall back on system.Versions to keep backwards compatibility with older c*.

Also changed in 4018: all the system.schema_* CFs now use columns named ""keyspace_name"", ""columnfamily_name"", and ""column_name"" instead of ""keyspace"", ""columnfamily"", and ""column"".

While we're at it, let's update the cql3 table structure parsing and the DESCRIBE command for the recent Cassandra changes too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-09-13 18:32:41.567,,,no_permission,,,,,,,,,,,,256196,,,Tue Sep 18 07:32:24 UTC 2012,,,,,,0|i0gwf3:,96682,brandon.williams,brandon.williams,,,,,,,,,,"12/Sep/12 19:12;thepaul;Changes made in my github clone:

https://github.com/thepaul/cassandra/tree/4491

Current version tagged pending/4491.",13/Sep/12 18:32;brandon.williams;Committed.,"18/Sep/12 04:42;thepaul;Sorry, my fault for not being clear: this change involved multiple commits. Three, in this case. The last one got in right.",18/Sep/12 07:32;slebresne;I've committed the first 2 patches then.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HintsColumnFamily compactions hang when using multithreaded compaction,CASSANDRA-4492,12601486,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,carlyeks,alienth,alienth,05/Aug/12 04:12,12/Mar/19 14:04,13/Mar/19 22:27,18/Dec/12 21:15,1.1.9,1.2.0,,,,0,,,,,,,"Running into an issue on a 6 node ring running 1.0.11 where HintsColumnFamily compactions often hang indefinitely when using multithreaded compaction. Nothing of note in the logs. In some cases, the compaction hangs before a tmp sstable is even created.

I've wiped out every hints sstable and restarted several times. The issue always comes back rather quickly and predictably. The compactions sometimes complete if the hint sstables are very small. Disabling multithreaded compaction stops this issue from occurring.

Compactions of all other CFs seem to work just fine.

This ring was upgraded from 1.0.7. I didn't keep any hints from the upgrade.

I should note that the ring gets a huge amount of writes, and as a result the HintedHandoff rows get be quite wide. I didn't see any large-row compaction notices when the compaction was hanging (perhaps the bug was triggered by incremental compaction?). After disabling multithreaded compaction, several of the rows that were successfully compacted were over 1GB.

Here is the output I see from compactionstats where a compaction has hung. The 'bytes compacted' column never changes.

{code}
pending tasks: 1
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction          systemHintsColumnFamily          268082       464784758     0.06%
{code}


The hung thread stack is as follows: (full jstack attached, as well)

{code}
""CompactionExecutor:37"" daemon prio=10 tid=0x00000000063df800 nid=0x49d9 waiting on condition [0x00007eb8c6ffa000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000050f2e0e58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Deserializer.computeNext(ParallelCompactionIterable.java:329)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Deserializer.computeNext(ParallelCompactionIterable.java:281)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:147)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:126)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:100)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:101)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:88)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:141)
        at org.apache.cassandra.db.compaction.CompactionManager$7.call(CompactionManager.java:395)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}",,,,,,,,,,,,,,,,,,,19/Dec/12 00:26;carlyeks;4492-patch2.patch;https://issues.apache.org/jira/secure/attachment/12561612/4492-patch2.patch,18/Dec/12 23:49;carlyeks;4492-patch2.patch;https://issues.apache.org/jira/secure/attachment/12561608/4492-patch2.patch,18/Dec/12 19:21;carlyeks;4492.patch;https://issues.apache.org/jira/secure/attachment/12561544/4492.patch,05/Aug/12 04:59;alienth;jstack.txt;https://issues.apache.org/jira/secure/attachment/12539176/jstack.txt,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-08-05 14:02:41.621,,,no_permission,,,,,,,,,,,,249088,,,Wed Dec 19 06:10:53 UTC 2012,,,,,,0|i0a4on:,57032,jbellis,jbellis,,,,,,,,,,05/Aug/12 14:02;brandon.williams;MT compaction is unfortunately a) not highly used and b) known to be suspect to issues.  The best course of action right now is to just not use it. :(,"16/Oct/12 18:01;tjake;I hit this recently as well.

Though mine I was able to reproduce.  If you call truncate while a compaction is currently going on it hangs both the truncate and the parallel compaction iterator.  

",17/Oct/12 00:45;mkjellman;hit this as well on both 1.1.5 and 1.1.6. Turned multhreaded compaction off and HintsColumnFamily finished very quickly. Nothing in the logs of interest and not reproducible.,"29/Nov/12 21:16;tvachon;This actually is severe.  Since they hang, it blocks all schema changes.",11/Dec/12 16:48;jbellis;If you can give us a set of sstables that reliably cause the hang (snapshot before compaction option should be useful here) then we can troubleshoot.  Nothing is obviously wrong from just eyeballing things.,"11/Dec/12 17:03;jbellis;bq.  If you call truncate while a compaction is currently going on it hangs both the truncate and the parallel compaction iterator. 

Truncate took some big changes for 1.2 (CASSANDRA-4096) so I doubt this is still the case.  (If it is, I'm not sure it's related to compaction alone causing the hang.)",11/Dec/12 19:26;tjake;Using 1.2 with LeveledCompaction I have not hit any deadlocks,11/Dec/12 19:34;tvachon;[~jbellis] no I can't.  We turned off MT compaction and they have been compacted away.,11/Dec/12 19:50;jbellis;Does your workload also involve truncates [~tvachon] [~mkjellman] [~alienth]?,"11/Dec/12 20:01;tvachon;No, update heavy though ","11/Dec/12 20:05;mkjellman;no, minimal (if any) deletes.","18/Dec/12 15:34;tjake;This is still happening:

Looking at the code it seems there are two places where HintedHandoffManager calls a user defined compact() for all sstables.

Multithreaded compaction would allow this to race since I see no check to avoid multiple calls to user defined compaction for the same sstables

","18/Dec/12 15:35;tvachon;{quote}Looking at the code it seems there are two places where HintedHandoffManager calls a user defined compact() for all sstable{quote}

Well that would explain why everytime I restart and I get hints, I get every sstable compacted",18/Dec/12 15:40;jbellis;Jake: I think you're missing how markCompacting works in submitUserDefined,"18/Dec/12 15:48;tjake;Ah, I needed to scroll down more.  Well I can confirm this is sill happening in 1.2 but only for hints CF  ","18/Dec/12 19:06;carlyeks;Here is what I think is happening (with help from Jake):

For simplicity, we are compacting two SSTables, sstable-1 and sstable-2.

- Read a row from sstable-1, which is empty
  - We don't call close on the LazilyCompactedRow because only Write or Update calls close; this means that the NotifyingSSTableIdentityIterator never signals the condition.
- Read a row from sstable-2, which is not empty
- Call hasNext() in CompactionTask's runWith() on the iterator for sstable-1, which was never triggered

This means that we are now deadlocked in ParallelCompactionIterable.Deserializer's as waiting for the signal and waiting for another row. We never return because we have no way of closing sstable-1's NotifyingSSTableIdentityIterator, and moving to the next row.","18/Dec/12 19:22;carlyeks;This patch add the Closeable interface to AbstractCompactedRow, and calls row.close() if the row is empty.",18/Dec/12 21:15;jbellis;I think you've nailed it.  Committed to 1.1 and 1.2.0.  Nice work!,"18/Dec/12 23:49;carlyeks;One more place that wasn't handling the close on empty - was hitting this case as well.

I've only included the additional patch.",19/Dec/12 02:48;jbellis;fixed formatting and committed,19/Dec/12 06:10;mkjellman;[~carlyeks] thanks for your work on this,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snappy 1.0.4 doesn't work on OSX / Java 7,CASSANDRA-4958,12616012,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,coltnz,coltnz,14/Nov/12 03:10,12/Mar/19 14:03,13/Mar/19 22:27,17/May/13 15:30,1.2.6,,,,,0,,,,,,,"Fixed in 1.0.5-M3 see :

https://github.com/xerial/snappy-java/issues/6

",,,,,,,,,,,,,,,,,,,17/May/13 14:58;yukim;0001-CASSANDRA-4958-1.2.patch;https://issues.apache.org/jira/secure/attachment/12583662/0001-CASSANDRA-4958-1.2.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-14 09:21:08.046,,,no_permission,,,,,,,,,,,,257625,,,Wed Jul 24 14:14:07 UTC 2013,,,,,,0|i0k3n3:,115407,slebresne,slebresne,,,,,,,,,,"14/Nov/12 09:21;slebresne;I'm fine with upgrading snappy-java, though is 1.0.5-M3 really released? I doesn't appear in http://code.google.com/p/snappy-java/downloads/list for instance. Does someone know if you're supposed to bake it yourself?","14/Nov/12 12:27;jbellis;It looks like the M stands for Milestone, so it's beta (alpha?) quality.",14/Nov/12 20:28;kzadorozhny;We have to compile M3 ourselves for our cluster on FreeBSD. It runs ok with 1.1.6 and openjdk7.,"14/Nov/12 21:40;coltnz;M3 is in maven repo:
http://search.maven.org/#search%7Cga%7C1%7Ca%3A%22snappy-java%22

The git history seems pretty stable https://github.com/xerial/snappy-java/commits/develop but maybe 1.5 isn't too far away.

The workaround for those who use the cassandra-maven-plugin is an explicit dependency:
{noformat} 
                <dependencies>
                    <dependency>
                        <groupId>org.xerial.snappy</groupId>
                        <artifactId>snappy-java</artifactId>
                        <version>1.0.5-M3</version>
                        <type>jar</type>
                    </dependency>
                </dependencies>
{noformat} ","14/Dec/12 10:14;slebresne;I'm not too fan of using a beta version, no matter how stable the git history looks like. So I see two way of moving forward:
# maybe M3 is indeed pretty stable and the author of snappy-java just didn't take the time to do a stable release. But imo only the snappy-java author knows that and we should maybe bug him to see if he would be good doing a stable release or if not, when we can expect one.
# there's apparently a pure java implem of snappy (https://github.com/dain/snappy). It's slightly slower on decompression so I'm not sure we want to have it the default, but we could use it as a fallback when loading the native version fails. Which would also give us a fallback for more exotic platform where the native version is just not available.

I note that I list both possibilities, but I'd lie if I said any of them was on my priority list. So feel free to bug the snappy-java author if you want this issue resolved :)",14/Dec/12 14:40;tjake;You can also look at CASSANDRA-5038  I've been using it without issue and it should work fine with Java7.  ,"17/May/13 14:40;yukim;Finally snappy-java 1.0.5 is out.

http://search.maven.org/#artifactdetails%7Corg.xerial.snappy%7Csnappy-java%7C1.0.5%7Cbundle

Time to upgrade?","17/May/13 14:52;slebresne;bq. Time to upgrade?

Sure, mind giving it a shot?","17/May/13 14:58;yukim;Here it is, against 1.2 branch.",17/May/13 15:04;slebresne;I'd have trust you going ninja style on that one :). But +1.,17/May/13 15:30;yukim;Committed. :),"17/May/13 18:00;mkjellman;[~yukim] best ninja fix, maybe ever! thank you!","06/Jul/13 03:03;pmonfette;I believe this fix (going to snappy 1.0.5) broke Cassandra on CentOS 5 for us. We're using the RPM package.

I upgraded from Cassandra 1.2.5 (which was using snappy 1.0.4) and I couldn't get 1.2.6 to start up properly and I had to rollback to 1.2.5.

Here's the error I was getting when starting Cassandra 1.2.6 after the upgrade.

{code}
ERROR [SSTableBatchOpen:5] 2013-07-05 16:35:22,466 CassandraDaemon.java (line 192) Exception in thread Thread[SSTableBatchOpen:5,5,main]
java.lang.RuntimeException: Cannot create CompressionParameters for stored parameters
        at org.apache.cassandra.io.compress.CompressionMetadata.<init>(CompressionMetadata.java:99)
        at org.apache.cassandra.io.compress.CompressionMetadata.create(CompressionMetadata.java:63)
        at org.apache.cassandra.io.util.CompressedSegmentedFile$Builder.complete(CompressedSegmentedFile.java:51)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:411)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:201)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:154)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:241)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.exceptions.ConfigurationException: SnappyCompressor.create() threw an error: java.lang.NoClassDefFoundError Could not initialize class org.xerial.snappy.Snappy
        at org.apache.cassandra.io.compress.CompressionParameters.createCompressor(CompressionParameters.java:179)
        at org.apache.cassandra.io.compress.CompressionParameters.<init>(CompressionParameters.java:71)
        at org.apache.cassandra.io.compress.CompressionMetadata.<init>(CompressionMetadata.java:95)
        ... 12 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.cassandra.io.compress.CompressionParameters.createCompressor(CompressionParameters.java:156)
        ... 14 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        ... 19 more
{code}

I did try to implement the following documented fix that seemed related to our issue but it did not work:

http://www.datastax.com/docs/1.0/troubleshooting/index#snappy

Furthermore, I suspect this is not the exact same problem since 1.2.5 was working perfectly fine for us without the above fix. We only did an update of the Cassandra package to 1.2.6, diffed and merged the yaml config file and tried to start it back up, like all the other previous upgrades we did as version 1.2.x came out.

I also fully updated all server packages in case it had something to do with an old library somwehere else or something but it did not work either.

We're running the latest CentOS 5.9, 64 bits, fully updated.

We're running java 1.6.0 from Sun (not OpenJDK)

{code}
java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)
{code}

Here is the content of my /tmp folder on the server (those get created when I start up Cassandra. The 1.0.5 is the one that got created when I was trying to start up Cassandra 1.2.6. It seems quite small compared to version 1.0.4 (which was created by version 1.2.5).

{code}
-rwxr-xr-x  1 cassandra       cassandra       991112 Jul  5 16:38 snappy-1.0.4.1-libsnappyjava.so
-rwxr-xr-x  1 cassandra       cassandra        48432 Jul  5 16:35 snappy-1.0.5-libsnappyjava.so
{code}

The only thing I haven't tried is to update our java version to a later 1.6.x release or go to 1.7.x or even try OpenJDK as this created some issues with other softwares we had since the server was coming up but queries to the keyspaces were coming back as errors or unknown keyspaces...

Let me know if you need more information, I'll be glad to help out.

Thanks guys !","06/Jul/13 03:47;pmonfette;Me again, I believe I found the issue, it requires a much too recent glibc version than what is available for CentOS 5. Only CentOS 6 seems to have it for now.

{code}
 INFO 03:32:04,175 Not using multi-threaded compaction
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:322)
        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
        at org.xerial.snappy.Snappy.<clinit>(Snappy.java:48)
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:55)
        at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:37)
        at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:82)
        at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:81)
        at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:468)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:123)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:211)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:441)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:484)
Caused by: java.lang.UnsatisfiedLinkError: /tmp/snappy-1.0.5-libsnappyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9' not found (required by /tmp/snappy-1.0.5-libsnappyjava.so)
        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1807)
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1703)
        at java.lang.Runtime.load0(Runtime.java:770)
        at java.lang.System.load(System.java:1003)
        at org.xerial.snappy.SnappyNativeLoader.load(SnappyNativeLoader.java:39)
        ... 17 more
 WARN 03:32:04,200 Cannot initialize native Snappy library. Compression on new tables will be disabled.
{code}

Hopefully this will help you fix this issue.

Thanks.","06/Jul/13 04:10;pmonfette;Here's what CentOS 5 seems to provide (very latest update), very close but not 3.4.9 :-)

rpm -q --provides libstdc++
{code}
libstdc++ = 4.1.1-52.el5
libstdc++.so.6()(64bit)
libstdc++.so.6(CXXABI_1.3)(64bit)
libstdc++.so.6(CXXABI_1.3.1)(64bit)
libstdc++.so.6(GLIBCXX_3.4)(64bit)
libstdc++.so.6(GLIBCXX_3.4.1)(64bit)
libstdc++.so.6(GLIBCXX_3.4.2)(64bit)
libstdc++.so.6(GLIBCXX_3.4.3)(64bit)
libstdc++.so.6(GLIBCXX_3.4.4)(64bit)
libstdc++.so.6(GLIBCXX_3.4.5)(64bit)
libstdc++.so.6(GLIBCXX_3.4.6)(64bit)
libstdc++.so.6(GLIBCXX_3.4.7)(64bit)
libstdc++.so.6(GLIBCXX_3.4.8)(64bit)
libstdc++ = 4.1.2-54.el5
{code}",06/Jul/13 15:45;jbellis;I imagine you can just drop the old 1.0.4 library back in.,"19/Jul/13 03:59;pmonfette;Hello,

is there a fix for this issue for an upcoming release or is support for CentOS/RHEL 5 dropped starting with version 1.2.6 ?

I saw Jonathan's comment but I definitely would prefer not to go this way since we are using the rpm package, I would prefer that everything remains as distributed/packaged.

Should I open up a new bug for this instead of commenting here ?

Thanks.","24/Jul/13 14:14;jbellis;Just to be clear, if we had known 1.0.5 would break RHEL5 we would have waited for 2.0.  But since we didn't find out until 1.2.6 was out in the wild, I think you can consider RHEL5 support dropped in Apache Cassandra, since reverting to the older version would break OS X users.

However, I note that DataStax Enterprise 3.1 (based on 1.2.x) still packages the old Snappy (and does not support OS X).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Starting Cassandra throws EOF while reading saved cache,CASSANDRA-5252,12632291,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,drew_kutchar,drew_kutchar,13/Feb/13 22:11,12/Mar/19 14:03,13/Mar/19 22:27,24/Mar/13 01:38,1.2.2,,,,,1,,,,,,,"I just saw this exception happen on Cassandra 1.2.1. I thought this was fixed by CASSANDRA-4916. Wasn't CASSANDRA-4916 part of the 1.2.1 release?

I'm on Mac OS X 10.8.2, Oracle JDK 1.7.0_11, using snappy-java 1.0.5-M3 from Maven (not sure if that's the cause).
I'm attaching my data and log directory as data.zip.


{code}
 WARN [main] 2013-02-12 17:50:11,714 AutoSavingCache.java (line 160) error reading saved cache /Users/services/cassandra/data/saved_caches/system-schema
_columnfamilies-KeyCache-b.db
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:277)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:364)
        at org.apache.cassandra.db.Table.initCf(Table.java:337)
        at org.apache.cassandra.db.Table.<init>(Table.java:280)
        at org.apache.cassandra.db.Table.open(Table.java:110)
        at org.apache.cassandra.db.Table.open(Table.java:88)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:421)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:177)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
 INFO [SSTableBatchOpen:1] 2013-02-12 17:50:11,722 SSTableReader.java (line 164) Opening /Users/services/cassandra/data/data/system/schema_columns/syste
m-schema_columns-ib-6 (193 bytes)
 INFO [SSTableBatchOpen:2] 2013-02-12 17:50:11,722 SSTableReader.java (line 164) Opening /Users/services/cassandra/data/data/system/schema_columns/syste
m-schema_columns-ib-5 (3840 bytes)
 INFO [main] 2013-02-12 17:50:11,725 AutoSavingCache.java (line 139) reading saved cache /Users/services/cassandra/data/saved_caches/system-schema_colum
ns-KeyCache-b.db
 WARN [main] 2013-02-12 17:50:11,725 AutoSavingCache.java (line 160) error reading saved cache /Users/services/cassandra/data/saved_caches/system-schema
_columns-KeyCache-b.db
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:277)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:364)
        at org.apache.cassandra.db.Table.initCf(Table.java:337)
        at org.apache.cassandra.db.Table.<init>(Table.java:280)
        at org.apache.cassandra.db.Table.open(Table.java:110)
        at org.apache.cassandra.db.Table.open(Table.java:88)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:421)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:177)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
 INFO [SSTableBatchOpen:1] 2013-02-12 17:50:11,736 SSTableReader.java (line 164) Opening /Users/services/cassandra/data/data/system/local/system-local-i
b-14 (458 bytes)
 INFO [main] 2013-02-12 17:50:11,738 AutoSavingCache.java (line 139) reading saved cache /Users/services/cassandra/data/saved_caches/system-local-KeyCac
he-b.db
 WARN [main] 2013-02-12 17:50:11,739 AutoSavingCache.java (line 160) error reading saved cache /Users/services/cassandra/data/saved_caches/system-local-
KeyCache-b.db
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:277)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:364)
        at org.apache.cassandra.db.Table.initCf(Table.java:337)
        at org.apache.cassandra.db.Table.<init>(Table.java:280)
        at org.apache.cassandra.db.Table.open(Table.java:110)
        at org.apache.cassandra.db.Table.open(Table.java:88)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:421)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:177)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
{code}",,,,,,,,,,,CASSANDRA-4916,,,CASSANDRA-4916,,,,,13/Feb/13 22:11;drew_kutchar;data.zip;https://issues.apache.org/jira/secure/attachment/12569277/data.zip,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-23 22:06:14.302,,,no_permission,,,,,,,,,,,,312787,,,Sun Mar 24 01:38:23 UTC 2013,,,,,,0|i1hz7z:,313133,,,,,,,,,,,,"23/Mar/13 22:06;jbellis;Is it possible there's still a problem here, Dave?","24/Mar/13 01:37;dbrosius;well the problem was this fix didn't go in till 1.2.2. 

so you won't see the above problem in 1.2.2. 

Of course if the write gets interrupted on shutdown or such, you may have this issue, but the exception is no longer at error, and so it's unlikely you'll see it.

fix was documented in 4916. will close this one too.",24/Mar/13 01:38;dbrosius;fixed by 4916,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Index fails to be created on all nodes in cluster, restart resolves",CASSANDRA-4465,12600253,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,heffergm,heffergm,26/Jul/12 01:15,12/Mar/19 14:02,13/Mar/19 22:27,24/Mar/13 06:27,,,,,,2,,,,,,,"On a production cluster, under load, creating an index on a column resulted in the index being successfully created on 4 of 21 nodes. All nodes received the schema agreement and were in concert. There were no errors logged on any of the nodes that failed to build the index.

A rolling restart of the cluster resulted in the nodes which had previously failed to build the index doing so when coming back up from a restart.","21 node cluster, Ubuntu Linux 11.10 in a virtualized environment, Apache cassandra community release, binary distribution",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-08-01 16:26:37.933,,,no_permission,,,,,,,,,,,,256179,,,Sun Mar 24 06:27:32 UTC 2013,,,,,,0|i0gw67:,96642,,,,,,,,,,,,"01/Aug/12 16:26;cnlwsu;I have also seen this on a 7 node (Ubuntu 10.04 in Rackspace, and 10.04 in ESX VM) cluster.  In fact this has happened to me often.  I have seen it when I have created a new column family (wait for schema agreement) then add the index (via solr schema.xml http post) but I cannot reproduce on single node cluster.",24/Mar/13 06:27;jbellis;Should be fixed by new schema propagation code in 1.1 and 1.2.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Appending/Prepending items to list using BATCH,CASSANDRA-4835,12612680,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,krzysztof cieslinski,krzysztof cieslinski,19/Oct/12 14:03,12/Mar/19 14:02,13/Mar/19 22:27,25/Oct/12 06:52,1.2.0 beta 2,,,,,0,,,,,,,"As I know, there is no any guarantee that commands that are inside BATCH block will execute in same order, as they are stored in the BATCH block. But...

I have made two tests:
First appends some items to the empty list, and the second one, prepends items, also to the empty list. Both of them are using UPDATE commands stored in the BATCH block. 

Results of those tests are as follow:
First:
      When appending new items to list, USING commands are executed in the same order as they are stored i BATCH.

Second:
      When prepending new items to list, USING commands are executed in random order.  

So, in other words below code:
{code:xml}
BEGIN BATCH
 UPDATE... list_name = list_name + [ '1' ]  
 UPDATE... list_name = list_name + [ '2' ]
 UPDATE... list_name = list_name + [ '3' ] 
APPLY BATCH;{code}

 always results in [ '1', '2', '3' ],
 but this code:
{code:xml}
BEGIN BATCH
 UPDATE... list_name = [ '1' ] + list_name   
 UPDATE... list_name = [ '2' ] + list_name
 UPDATE... list_name = [ '3' ] + list_name
APPLY BATCH;{code}

results in randomly ordered list, like [ '2', '1', '3' ]    (expected result is [ '3', '2', '1' ])

So somehow, when appending items to list, commands from BATCH are executed in order as they are stored, but when prepending, the order is random.",,,,,,,,,,,,,,,,,,,24/Oct/12 08:49;slebresne;0001-Fix-prepends-within-same-millis.txt;https://issues.apache.org/jira/secure/attachment/12550605/0001-Fix-prepends-within-same-millis.txt,24/Oct/12 08:49;slebresne;0002-Ensure-same-timestamp-in-batches.txt;https://issues.apache.org/jira/secure/attachment/12550606/0002-Ensure-same-timestamp-in-batches.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-19 14:10:23.914,,,no_permission,,,,,,,,,,,,249929,,,Thu Oct 25 06:52:58 UTC 2012,,,,,,0|i0an87:,60036,jbellis,jbellis,,,,,,,,,,"19/Oct/12 14:10;jbellis;You should think of multiple prepends in a batch as guaranteed to be prepended before any existing data, but not ordered among themselves.  Otherwise we could not parallelize within the batch.  (It's random chance that your appends appear to maintain batch order.)

If you want to retain order you should combine into one update: {{list = [3, 2, 1] + list}}, which will also be more performant.","19/Oct/12 21:15;krzysztof cieslinski;Ok, thanks, but I'm afraid that the fact, that my appends are in same order as in BATCH is not a result of a random chance, due the fact that i did this test using BATCH that contains 5000 update commands. And all of them(these 5000 values in list) are in same order as update commands in BATCH(i have executed this test ~10 times and result was always same). However prepending new items is totally random even for BATCH that contains 10 or less updates.. So this shows that for sure, order of updates execution is different for BATCH with prependings and BATCH with appendings. ","22/Oct/12 15:57;jbellis;You should consider this an implementation quirk (possibly even a bug), not a guarantee.",23/Oct/12 20:18;jbellis;Sylvain points out that we actually do expect update order to be preserved *within the same row*.  Reopening.,"24/Oct/12 08:49;slebresne;Alright, this is in fact a legit bug in prepend and is not specific to batches (though it's probably harder to reproduce without them). Basically the logic in prepend to make sure we were always generating a decreasing keys even in the same millisecond was broken. It was working only for the same update, but was broke for successive update in the same millisecond. Patch attached to fix that.

That being said, I do think that people should be very careful in assuming that statements in a batch are applied in order *even within the same row* because that's just not true in general. Batch applies everything ""at the same time"".  So for instance:
{noformat}
BEGIN BATCH
  UPDATE user SET name = 'Goo' WHERE userid = 1;
  UPDATE user SET name = 'Foo' WHERE userid = 1;
APPLY BATCH
{noformat}
will always (that's not quite true currently, see below) end up setting 'Goo' as the name because the way the reconciliation rules work, the biggest value wins for equal timestamp. Similarly,
{noformat}
BEGIN BATCH
  DELETE FROM user WHERE userid = 1;
  UPDATE user SET name = 'Foo' WHERE userid = 1;
APPLY BATCH
{noformat}
will always (again, see below) end up with the user deleted because on timestamp ties, tombstone wins.

In other words, there was indeed a bug with prepend, and append/prepend do respect the order in batches within the same partition key because we happen to process the statements of a batch in order and there is no good reason to do otherwise, but I don't think we should make that a guarantee either (as in, it's true now, it could change tomorrow, it's an implementation detail). And so user shouldn't rely on it, and if the order is important, they should combine into one statement.

Now, it is unrelated to lists, but when I said that
{noformat}
BEGIN BATCH
  UPDATE user SET name = 'Goo' WHERE userid = 1;
  UPDATE user SET name = 'Foo' WHERE userid = 1;
APPLY BATCH
{noformat}
will always end up with 'Goo', it's not quite true currently, because batches don't guarantee that all update will use the same timestamp (in other words, the result of the batch above randomly depends of the timing of the operation).  I think that *that* is a guarantee we should provide: that unless the timestamp is user provided, all statement of a batch uses the same timestamp. I'm attaching a second patch that implements that.
",24/Oct/12 20:26;jbellis;+1,"25/Oct/12 06:52;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deal with broken ALTER DROP support in CQL3,CASSANDRA-4929,12615198,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,07/Nov/12 17:29,12/Mar/19 14:02,13/Mar/19 22:27,16/Nov/12 11:37,1.2.0 beta 3,,,,,0,,,,,,,"Currently, {{ALTER DROP}} only remove the metadata for the column, making it unavailable, but don't reclaim the data. This is unintuive and CASSANDRA-3919 is opened to fix it. However, that later issue won't make it for 1.2, and I think we should be very careful into shipping 1.2 with the current behavior because 1) it's unintuitive and 2) as unintuitive as it is, we don't want people to start relying on that behavior. So I thing we should do one of:
* remove support for {{ALTER DROP}} until CASSANDRA-3919 reintroduce it. After all, there is no real performance impact in keeping a colum that you don't use and if you really really want to get rid of the metadata, you still have the workaround of trashing the schema and recreating it without that column (obviously not user friendly, but at least it's vaguely possible).
* add a specific syntax for the current behavior of {{ALTER DROP}}, one that clearly imply that the data is not deleted, if we consider that this behavior can be sometimes useful (that is, even after CASSANDRA-3919 is resolved). One such syntax could one of (not sure which one I prefer):
{noformat}
ALTER TABLE foo DROP my_column SCHEMA ONLY
ALTER TABLE foo DROP my_column KEEP DATA
{noformat}

I have a slight preference for solution 2, but honestly because it will it easier to drop a column you've just added but maybe mispelled the name until CASSANDRA-3919. Once CASSANDRA-3919 is in, I'm not sure this will be so useful anymore.
",,,,,,,,,,,,,,,,,,,14/Nov/12 14:18;slebresne;4929.txt;https://issues.apache.org/jira/secure/attachment/12553495/4929.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-07 18:00:51.796,,,no_permission,,,,,,,,,,,,255796,,,Fri Nov 16 11:37:52 UTC 2012,,,,,,0|i0fuyv:,90615,,,,,,,,,,,,"07/Nov/12 18:00;jbellis;Doesn't solution 2 have the problem that we'd be stuck with a ""wart"" on the language that doesn't make sense anymore post-1.2?  (""mispelled a column"" should properly be dealt with by supporting ALTER ... RENAME, for instance.)","07/Nov/12 18:14;slebresne;bq. Doesn't solution 2 have the problem that we'd be stuck with a ""wart"" on the language that doesn't make sense anymore post-1.2?

Yes, that's what I meant by ""I'm not sure this will be so useful anymore"". But I don't maybe it does have some narrow usefulness: say you're not sure you want to remove a column for real. You could do a 'DROP ... KEEP DATA'. If it turns out that it was a bad, you can just add it back and you're in your previous state. And if after some testing you're convince that's the correct things to do, you'd add it back and then do a real DROP just afterwards. I mean, that's definitively not an uber important things to have and there is a risk nobody will never used that, but I figured, maybe it's worth making life easier for 1.2 if there is an even remote chance that the feature can have some use from 1.3 onwards (of course I'm only suggesting that because supporting this will be 3 lines of code, and if all come to worst and nobody ever use it when 1.3 is out, then we won't have lost much). But again, I'm just suggesting.

bq. ""mispelled a column"" should properly be dealt with by supporting ALTER ... RENAME

While it would be nice, making that work for column not part of the PRIMARY KEY (the only ones concerned by CASSANDRA-3919 and this ticket) is about as much work as CASSANDRA-3919 (wouldn't be crazy to do it as part of CASSANDRA-3919 though).","08/Nov/12 15:53;jbellis;If we think we could do 3919 for 1.2.1 or .2 then my preferred option would be ""leave DROP out entirely for now.""",08/Nov/12 17:32;jeromatron;I think there's a valid use case in wanting to drop metadata for individual column definitions.  That could go into a different form if the drop method is too confusing though.,"14/Nov/12 14:24;slebresne;bq. If we think we could do 3919 for 1.2.1 or .2 then my preferred option would be ""leave DROP out entirely for now.""

As said above, I'm fine with that. Worst case scenario, if #3919 proves trickier than we sought and/or if we find some compelling use case for a drop that only drops the metadata, we can add that later. For now, attaching patch to remove the support (the patch simply makes the syntax invalid, I figured there is no reason to remove code internally that we'll need for CASSANDRA-3919).

bq. I think there's a valid use case in wanting to drop metadata for individual column definitions

Which would be?","15/Nov/12 16:19;jbellis;Let's go with ""remove support in 1.2.0 and add a note to NEWS that we expect to re-add it 'right' in 1.2.1"" then.","16/Nov/12 11:37;slebresne;Alright, committed the removal with a note in the NEWS file (and in the CQL3 documentation).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix CQL3 loose type validation of constants,CASSANDRA-5198,12629829,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,appodictic,appodictic,30/Jan/13 03:23,12/Mar/19 14:02,13/Mar/19 22:27,04/Feb/13 09:53,1.2.2,,,,,0,,,,,,,"This works as it should.

{noformat}
cqlsh:movies> select * from users where token (username) > token('') ;

 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
    bsmith |         null |  null |       bob |    smith |     null
 scapriolo |         null |  null |    stacey | capriolo |     null
 ecapriolo |         null |  null |    edward | capriolo |     null

cqlsh:movies> select * from users where token (username) > token('bsmith') ;

 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 scapriolo |         null |  null |    stacey | capriolo |     null
 ecapriolo |         null |  null |    edward | capriolo |     null

cqlsh:movies> select * from users where token (username) > token('scapriolo') ;

 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 ecapriolo |         null |  null |    edward | capriolo |     null

{noformat}

But look what happens when you supply numbers into the token function.


{noformat}
qlsh:movies> select * from users where token (username) > token(0) ;
 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 ecapriolo |         null |  null |    edward | capriolo |     null
cqlsh:movies> select * from users where token (username) > token(1134314) ;

 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
    bsmith |         null |  null |       bob |    smith |     null
 scapriolo |         null |  null |    stacey | capriolo |     null
 ecapriolo |         null |  null |    edward | capriolo |     null

cqlsh:movies> select * from users where token (username) > token(113431431) ;
 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 scapriolo |         null |  null |    stacey | capriolo |     null
 ecapriolo |         null |  null |    edward | capriolo |     null

cqlsh:movies> select * from users where token (username) > token(1134) ;
 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 ecapriolo |         null |  null |    edward | capriolo |     null
cqlsh:movies> select * from users where token (username) > token(1134434) ;
 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 scapriolo |         null |  null |    stacey | capriolo |     null
{noformat}

This does not make sense to me. The token function is apparently converting integers to strings leading to seemingly unpredictable results. 

However I find this syntax odd, I feel like I should be able to say 
'token(username) > 0 and token(username) < 10' because from a thrift side I can page tokens or I can page keys. In this case, I guess, I am only able to page keys because the token is not returned to the user.

Is token 0 = ''? How do I arrive at the minimal token for and int column. 

Should the token() function at least be smart enough to reject integers for string columns?",,,,,,,,,,,,,,,,,,,30/Jan/13 17:00;slebresne;0001-Respect-CQL3-constant-types.txt;https://issues.apache.org/jira/secure/attachment/12567160/0001-Respect-CQL3-constant-types.txt,30/Jan/13 17:00;slebresne;0002-Improve-printing-of-type-in-error-message.txt;https://issues.apache.org/jira/secure/attachment/12567161/0002-Improve-printing-of-type-in-error-message.txt,30/Jan/13 17:00;slebresne;0003-Respect-partitioner-type-for-Token-function.txt;https://issues.apache.org/jira/secure/attachment/12567162/0003-Respect-partitioner-type-for-Token-function.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-01-30 09:25:05.315,,,no_permission,,,,,,,,,,,,310325,,,Mon Feb 04 09:52:58 UTC 2013,,,,,,0|i1hk13:,310670,iamaleksey,iamaleksey,,,,,,,,,,"30/Jan/13 09:25;slebresne;So what happens is that {{token(0)}} is basically interpreted as the equivalent of {{token('0')}}.

Now this is not specific to the token method at all. With the table used above, you can do:
{noformat}
INSERT INTO users(username, firstname, lastname) VALUES (12, 42, 0)
{noformat}
and that will have the same effect than
{noformat}
INSERT INTO users(username, firstname, lastname) VALUES ('12', '42', '0')
{noformat}
In the same spirit, you can insert value {{'12'}} in an int column.

Now is that a good idea? I'm not sure indeed. This is not really intentional and is more of an oversight (more precisely it's an inheritance of CQL2 that has never been fixed).

I'm fine fixing it (thus fixing the token special case), and in fact in favor of fixing it, but of course that will break anyone that relies on this loose validation (which may be no-one). Though I guess ""not validating types"" is more of a bug than a feature.


bq. I feel like I should be able to say 'token(username) > 0 and token(username) < 10'

You can with the caveat that currently the token needs to be quoted, so {{token(username) > '0' and token(username) < '10'}}. The vague rational was that tokens are not always ints (i.e they are not in the case of OPP) so we only accept a string and pass that to the partitionner fromString method, oblivious to what the token type actually is. *But* this is definitively neither intuitive, nor coherent with the behavior described above. So I suggest that if we do the change suggested above of actually doing type validation, we also use the occasion for properly typing tokens.

bq. because the token is not returned to the user

On that part I'll not that it is true with thrift too. If you want to page tokens thrift side, you have to compute the token from the keys returned. That being said, I'm not opposed to allow the token function in select clause so you can do
{noformat}
SELECT username, token(username) FROM ...
{noformat}
to save the token computation client side.
","30/Jan/13 17:00;slebresne;Attached 3 patches related to the proposed changes above:
# the first one adds proper type validation. In other word, it rejects a string value when the column is int, or reject an int value when the column is a blob (instead of interpreting it as an hex value which I'm pretty sure is counter-intuitive). This does however also reject a string value when the column is a blob, because I'm far from convince than interpreting the content of the string as an hex value is particularly intuitive. But to allow inserting blobs, it allow a new type of hex constants (that must start with '0x'). In other words, if b is a blob column:
{noformat}
UPDATE ... SET b = '00ff' ...
{noformat}
is not valid anymore, but
{noformat}
UPDATE ... SET b = 0x00ff ...
{noformat}
is. I note that the patch ain't tiny because it required a few refactoring here and there
to be done properly, but overall I think those refactor actually improve the
code.
# the second patch is mainly of cosmetic and make sure we use CQL3 type in CQL3 error message. I.e. 'map<text, int>' rather than 'org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type.Int32Type)'.
# the third patch make sure we take the partitioner token type into account. So if your partitioner is M3P you should provide a bigint value, if it's RP a varint one and if it's OPP a blob one.

Those patches don't add yet support for the token function in select clause that I talk above. I also want to add conversion function that allow to say convert a string or a uuid to a blob, but I want to refactor a bit the (currently ugly) handling of functions to do that so that will follow later (and it can be done in another ticket).
","30/Jan/13 20:34;appodictic;I like #1.
I advocate proper type validation. We recently had a MySQL update that was wrapping booleans in 'T', 'F', 'true' and based on your database the results are different or non-intuitive. Personally, I do not like ""loose validation"" it encourages ambiguity. Hive went through something similar: http://grokbase.com/t/hive/dev/125sw56a78/non-string-partition-columns, there was much ambiguity and misconceptions around ""loose validation"" and it became tech-dept that was hard to dig out of.",30/Jan/13 22:28;iamaleksey;+1 to the first three patches.,"31/Jan/13 11:24;slebresne;Alight, since we seem to be in agreement, committed (and I've updated the title to reflect the slightly broader scope).

I'll open a separate ticket for adding conversion functions.","01/Feb/13 19:55;tjake;This is a breaking change for us or anyone who is using CQL with blobs.

Since we no longer accept '' hex but 0x didn't previously work till now, there is no way to upgrade 1.2.x to 1.2.2 due to this change.

I suggest you allow both formats for a single release and add a note in NEWS.txt, that way someone can deploy 1.2.2 then update their app to the new syntax","01/Feb/13 19:58;iamaleksey;bq. I suggest you allow both formats for a single release and add a note in NEWS.txt, that way someone can deploy 1.2.2 then update their app to the new syntax

+1",04/Feb/13 09:52;slebresne;Agreed. I've re-allowed strings-as-blobs in commit b251e7aec03273ac14eeae79bee13422068d508b while (hopefully) making it clear it is deprecated (in particular it logs a warning (only once to avoid flooding) if you use a string as blob).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to trim DC and RACK names in cassandra-topology.properties file,CASSANDRA-5165,12627819,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,azotcsit,azotcsit,azotcsit,16/Jan/13 12:11,12/Mar/19 14:02,13/Mar/19 22:27,16/Jan/13 20:46,1.2.1,,,,,0,snitch,,,,,,"Some misprints in cassandra-topology.properties file can be painful for debugging. For example extra symbols at the end of line like a space. So difference between ""DC1:RACK1"" and ""DC1:RACK1 "" couldn't be detected using nodetool ring command.

I think that symbols like a space shouldn't be allowed in DC/RACK names.  So I suggest to trim them.

The patch has been attached.
",cassandra-1.1.6 (DataStax distribution).,,,,,,,,,,,,,,,,,,16/Jan/13 12:12;azotcsit;cassandra-1.2-5165_trim_spaces.txt;https://issues.apache.org/jira/secure/attachment/12565111/cassandra-1.2-5165_trim_spaces.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-16 20:46:45.576,,,no_permission,,,,,,,,,,,,304607,,,Fri Jan 18 11:00:14 UTC 2013,,,,,,0|i17nnr:,252802,brandon.williams,brandon.williams,,,,,,,,,,"16/Jan/13 20:46;brandon.williams;I don't see a problem with having a space in the middle, like ""San Antonio"", but you're right about trimming. Committed with a small change to GPFS to match.",18/Jan/13 11:00;azotcsit;Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"after a IOException is thrown during streaming, streaming tasks hang in netstats",CASSANDRA-5229,12631334,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,mkjellman,mkjellman,07/Feb/13 20:44,12/Mar/19 14:02,13/Mar/19 22:27,06/May/13 07:08,1.2.5,,,,,0,,,,,,,"After an IOExcpetion, streaming tasks marked as ""successful"" in the logs are hung in netstats

With TRACE debugging on streaming on the receiving node everything about the sstable in the log (not very much)

{code}
 INFO [AntiEntropyStage:1] 2013-02-07 11:23:44,717 StreamOut.java (line 151) Stream context metadata [/data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-5-Data.db sections=3068 progress=0/2785204713 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-25-Data.db sections=2696 progress=0/758409465 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-60-Data.db sections=3099 progress=0/238876436 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-63-Data.db sections=1166 progress=0/2125323 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-38-Data.db sections=2507 progress=0/515992757 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-26-Data.db sections=3153 progress=0/994857654 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-57-Data.db sections=3116 progress=0/129398170 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-58-Data.db sections=217 progress=0/72286 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-59-Data.db sections=3146 progress=0/3357709019 - 0%], 27 sstables.
 INFO [AntiEntropyStage:1] 2013-02-07 11:23:52,964 StreamOut.java (line 151) Stream context metadata [/data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-5-Data.db sections=2930 progress=0/2799914560 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-25-Data.db sections=2590 progress=0/761266059 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-60-Data.db sections=2956 progress=0/241362497 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-63-Data.db sections=1153 progress=0/2125323 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-38-Data.db sections=2422 progress=0/522126371 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-26-Data.db sections=3004 progress=0/998401202 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-57-Data.db sections=2974 progress=0/129722346 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-58-Data.db sections=220 progress=0/72286 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-59-Data.db sections=2998 progress=0/3375554099 - 0%], 27 sstables.
{code}

node that is streaming out thinks that the streaming session was successful 
{code}
 INFO [MiscStage:1] 2013-02-07 11:23:38,022 StreamOut.java (line 151) Stream context metadata [/data/cassandra/evidence/fingerprints/evidence-fingerprints-ia-472-Data.db sections=1727 progress=0/210208515 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-919-Data.db sections=1746 progress=0/119438030 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-920-Data.db sections=1681 progress=0/54498226 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-922-Data.db sections=16 progress=0/13490 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-918-Data.db sections=632 progress=0/70019542 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-921-Data.db sections=1644 progress=0/39870238 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-497-Data.db sections=1569 progress=0/208331077 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-923-Data.db sections=1572 progress=0/30870478 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-925-Data.db sections=167 progress=0/1845123 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-703-Data.db sections=1574 progress=0/287386471 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-913-Data.db sections=811 progress=0/103776521 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-915-Data.db sections=1539 progress=0/141864261 - 0%], 12 sstables.
 INFO [MiscStage:1] 2013-02-07 11:23:49,938 StreamOut.java (line 151) Stream context metadata [/var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-919-Data.db sections=3153 progress=0/994857654 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-918-Data.db sections=2507 progress=0/515992757 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-923-Data.db sections=3153 progress=0/131969347 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-703-Data.db sections=3068 progress=0/2784807967 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-913-Data.db sections=2696 progress=0/758409465 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ia-472-Data.db sections=3150 progress=0/1792868996 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-920-Data.db sections=3123 progress=0/240094510 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-929-Data.db sections=18 progress=0/29468 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-922-Data.db sections=217 progress=0/13490 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-921-Data.db sections=3124 progress=0/130320291 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-497-Data.db sections=3055 progress=0/1749539598 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-925-Data.db sections=1608 progress=0/13357410 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-915-Data.db sections=3096 progress=0/1241397203 - 0%], 13 sstables.
 INFO [Streaming to /10.8.25.132:2] 2013-02-07 11:24:18,780 StreamReplyVerbHandler.java (line 44) Successfully sent /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-919-Data.db to /10.8.25.132
{code}

node that is being streamed to has nothing in the logs about this particular sstable


{code}
Streaming from: /10.8.30.13
evidence: /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-919-Data.db sections=3153 progress=218225400/994857654 - 21%

_______

Streaming from: /10.138.12.10
evidence: /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-26-Data.db sections=3153 progress=218225400/994857654 - 21%
{code}",Ubuntu 12.04,,,,,,,,,,CASSANDRA-5105,,,,,,,,03/May/13 17:03;yukim;5229-1.2.txt;https://issues.apache.org/jira/secure/attachment/12581712/5229-1.2.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-10 02:19:12.066,,,no_permission,,,,,,,,,,,,311830,,,Mon May 06 07:08:59 UTC 2013,,,,,,0|i1htbb:,312176,slebresne,slebresne,,,,,,,,,,10/Feb/13 02:19;rbranson;Seeing this as well.,"11/Feb/13 18:15;yukim;Do you have stack trace of IOException?

Is the last log above is from /10.8.25.132?
It seems that both 919( and 26) sstables are sent twice for different ranges. And the ""Successfully sent"" line in the second log can be one of those files.","11/Feb/13 19:34;mkjellman;ERROR [Streaming to /10.8.25.114:4] 2013-02-09 11:02:09,992 CassandraDaemon.java (line 135) Exception in thread Thread[Streaming to /10.8.25.114:4,5,main]
java.lang.RuntimeException: java.io.IOException: Broken pipe
        at com.google.common.base.Throwables.propagate(Throwables.java:160)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.IOException: Broken pipe
        at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
        at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:420)
        at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:552)
        at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:90)
        at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        ... 3 more","12/Feb/13 23:13;yukim;Looks like the problem is streaming session silently hangs if the exception thrown is not IOException(like RuntimeException in CASSANDRA-5105). When the error was IOException, then the node would send retry or session failure message.",14/Feb/13 05:50;mkjellman;fixed.,14/Feb/13 05:56;jbellis;Re-opening since it sounds like we should still fix the silent hang on unexpected exception.,"03/May/13 17:03;yukim;I think the best we can do here is to catch RuntimeException and let session fail.
Patch attach for that.",06/May/13 07:08;slebresne;+1 (took the liberty to commit it since I'd like to re-roll 1.2.5 shortly).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describeOwnership() in Murmur3Partitioner.java doesn't work for close tokens,CASSANDRA-4598,12605780,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,julienlambert,julienlambert,31/Aug/12 20:50,12/Mar/19 14:02,13/Mar/19 22:27,19/Dec/12 21:56,1.2.0 rc2,,,,,0,,,,,,,"On a 2 node-cluster, if the two tokens are close enough, the ownership information displayed will be 0.00% for each, instead of ~0% for one and ~100% for the other. The number of replicas displayed is then 0, even if you have more.

Reproduce:
- Create a 2-node cluster, using Murmur3Partitioner
- Move the tokens to two consecutive values
- Display ring with nodetool

Problem:
This line causing this problem is in {{describeOwnership()}} of {{Murmur3Partitioner.java}} (lines 117 and 123):

{{float age = ((ti - tim1 + ri) % ri) / ri;}}

If {{ti - tim1}} (the difference of the two consecutive tokens) is too small, then the precision of the float isn't enough to represent the exact numbers (because {{ri}}, the total range of the ring, is a very big number). 

For example, {{(float) (ri + 1) = (float) (ri - 1) = (float) ri = 9.223372E18}}, so that {{((ri+1)%ri)/ri = ((ri-1)%ri)/ri = (ri%ri)/ri = 0}}. Whereas with a correct precision, the exact value for {{(ri-1)%ri}} should be {{ri-1}} and {{(ri-1)/ri ~ 1.0 (100%)}} instead of 0%.

Also, as the number of replica is determined by NodeCmd using the ownership percentages, it is wrong too.

Solution:
We might want to use BigInteger or BigDecimal somewhere?",,,,,,,,,,,,,,CASSANDRA-5076,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-12-19 21:56:16.131,,,no_permission,,,,,,,,,,,,255197,,,Wed Dec 19 21:56:16 UTC 2012,,,,,,0|i0epzz:,83974,,,,,,,,,,,,"19/Dec/12 21:56;yukim;We fixed desribeOwnership bug in CASSANDRA-5076, and the above issue was resolved also. Closing this as 'Fixed'.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Preparing UPDATE queries with collections returns suboptimal metadata,CASSANDRA-5017,12618587,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,iamaleksey,iamaleksey,03/Dec/12 16:35,12/Mar/19 14:02,13/Mar/19 22:27,10/Dec/12 16:34,1.2.0 rc1,,,,,0,,,,,,,"CQL3, binary protocol.

collections (id int primary key, amap map<int, varchar>);

preparing ""UPDATE test.collections SET amap[?] = ? WHERE id = ?"" returns the following metadata:
[{column,<<""test"">>,<<""collections"">>,<<""amap"">>,
                       {map,int,varchar}},
               {column,<<""test"">>,<<""collections"">>,<<""amap"">>,
                       {map,int,varchar}},
               {column,<<""test"">>,<<""collections"">>,<<""id"">>,int}]

Ideally it should return [int, varchar, int] types. Less ideally [{map, int, varchar}, int] and expect an encoded map with a single key-value pair. But certainly not what it currently returns.",,,,,,,,,,,,,,,,,,,04/Dec/12 13:54;slebresne;0001-Return-correct-metadata-when-preparing-map.txt;https://issues.apache.org/jira/secure/attachment/12555935/0001-Return-correct-metadata-when-preparing-map.txt,04/Dec/12 13:54;slebresne;0002-Fix-prepared-list-index-and-using-integer-for-map-keys.txt;https://issues.apache.org/jira/secure/attachment/12555936/0002-Fix-prepared-list-index-and-using-integer-for-map-keys.txt,04/Dec/12 13:54;slebresne;0003-Fix-handling-of-prepared-marker-for-deletes.txt;https://issues.apache.org/jira/secure/attachment/12555937/0003-Fix-handling-of-prepared-marker-for-deletes.txt,05/Dec/12 08:22;slebresne;0004-Update-validation-of-ListOperation.txt;https://issues.apache.org/jira/secure/attachment/12556075/0004-Update-validation-of-ListOperation.txt,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-12-04 13:54:51.212,,,no_permission,,,,,,,,,,,,293424,,,Mon Dec 10 16:34:56 UTC 2012,,,,,,0|i0szov:,167259,iamaleksey,iamaleksey,,,,,,,,,,"04/Dec/12 13:54;slebresne;Attaching 3 small patches that fixes a number of problems with prepared queries and collections, including this one, the fact that you weren't allowed to prepare a list index and the fact that delete wasn't working correctly when either a list index or a map key was prepared.

I note we have a small ""problem"" for the metadata return in all those cases, in that if you do
{noformat}
UPDATE foo SET amap[?] = ? WHERE ...
{noformat}
then it's unclear what name to return for the prepared variables in the metadata. The choice made by the patch is to return 'key(amap)' and 'value(amap)' respectively (and when preparing a list index, we return 'index(alist)').
","05/Dec/12 00:08;iamaleksey;CASSANDRA-5018 in indeed no longer an issue. Also I do get the right columns in my metadata now:

{ok, R} = seestar_client:prepare(Pid, ""update test.collections set amap[?] = ? WHERE id = ?"").
{ok,{prepared,<<21,71,235,65,143,167,200,103,53,75,239,64,
                92,235,243,145>>,
              [{column,<<""test"">>,<<""collections"">>,<<""key(amap)"">>,int},
               {column,<<""test"">>,<<""collections"">>,<<""value(amap)"">>,
                       varchar},
               {column,<<""test"">>,<<""collections"">>,<<""id"">>,int}]}}

However, when I try to execute the query (or just run it in cqlsh), I get ""Bad Request: List operations are only supported on List typed columns, but org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.UTF8Type) given"" error.","05/Dec/12 08:22;slebresne;Right, forgot to update the validation of ListType. Fourth patch attached to fix that.",05/Dec/12 17:08;iamaleksey;Everything works now (prepared and unprepared). +1,"10/Dec/12 16:34;slebresne;My bad, forgot to resolve, but that has been committed a few days ago.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE while loading Saved KeyCache,CASSANDRA-5253,12632384,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,ahmedgc,ahmedgc,14/Feb/13 14:11,12/Mar/19 14:02,13/Mar/19 22:27,20/Feb/13 02:43,,,,,,0,,,,,,,"This bug occurred in the Beta version and was marked as fixed in this Jira: CASSANDRA-4553

However it seems to have reoccurred in the production 1.2.1 release. This is the first install I have made of Cassandra (so a clean install), which I downloaded prepackaged from http://www.apache.org/dyn/closer.cgi?path=/cassandra/1.2.1/apache-cassandra-1.2.1-bin.tar.gz

I have created a keyspace but not inserted any data, so that is not the issue either.

Here is a sample from the logs all the way from startup
{code}
 INFO [main] 2013-02-07 19:48:54,109 CassandraDaemon.java (line 101) Logging initialized
 INFO [main] 2013-02-07 19:48:54,125 CassandraDaemon.java (line 123) JVM vendor/version: Java HotSpot(TM) Client VM/1.7.0_11
 INFO [main] 2013-02-07 19:48:54,125 CassandraDaemon.java (line 124) Heap size: 1067057152/1067057152
 INFO [main] 2013-02-07 19:48:54,126 CassandraDaemon.java (line 125) Classpath: C:\Cassandra\\conf;C:\Cassandra\\lib\antlr-3.2.jar;C:\Cassandra\\lib\apache-cassandra-1.2.1.jar;C:\Cassandra\\lib\apache-cassandra-clientutil-1.2.1.jar;C:\Cassandra\\lib\apache-cassandra-thrift-1.2.1.jar;C:\Cassandra\\lib\avro-1.4.0-fixes.jar;C:\Cassandra\\lib\avro-1.4.0-sources-fixes.jar;C:\Cassandra\\lib\commons-cli-1.1.jar;C:\Cassandra\\lib\commons-codec-1.2.jar;C:\Cassandra\\lib\commons-lang-2.6.jar;C:\Cassandra\\lib\compress-lzf-0.8.4.jar;C:\Cassandra\\lib\concurrentlinkedhashmap-lru-1.3.jar;C:\Cassandra\\lib\guava-13.0.1.jar;C:\Cassandra\\lib\high-scale-lib-1.1.2.jar;C:\Cassandra\\lib\jackson-core-asl-1.9.2.jar;C:\Cassandra\\lib\jackson-mapper-asl-1.9.2.jar;C:\Cassandra\\lib\jamm-0.2.5.jar;C:\Cassandra\\lib\jline-1.0.jar;C:\Cassandra\\lib\json-simple-1.1.jar;C:\Cassandra\\lib\libthrift-0.7.0.jar;C:\Cassandra\\lib\log4j-1.2.16.jar;C:\Cassandra\\lib\metrics-core-2.0.3.jar;C:\Cassandra\\lib\netty-3.5.9.Final.jar;C:\Cassandra\\lib\servlet-api-2.5-20081211.jar;C:\Cassandra\\lib\slf4j-api-1.7.2.jar;C:\Cassandra\\lib\slf4j-log4j12-1.7.2.jar;C:\Cassandra\\lib\snakeyaml-1.6.jar;C:\Cassandra\\lib\snappy-java-1.0.4.1.jar;C:\Cassandra\\lib\snaptree-0.1.jar;C:\Cassandra\\build\classes\main;C:\Cassandra\\build\classes\thrift;C:\Cassandra\\lib\jamm-0.2.5.jar
 INFO [main] 2013-02-07 19:48:54,130 CLibrary.java (line 61) JNA not found. Native methods will be disabled.
 INFO [main] 2013-02-07 19:48:54,147 DatabaseDescriptor.java (line 131) Loading settings from file:/C:/Cassandra/conf/cassandra.yaml
 INFO [main] 2013-02-07 19:48:54,515 DatabaseDescriptor.java (line 150) 32bit JVM detected.  It is recommended to run Cassandra on a 64bit JVM for better performance.
 INFO [main] 2013-02-07 19:48:54,516 DatabaseDescriptor.java (line 190) DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
 INFO [main] 2013-02-07 19:48:54,516 DatabaseDescriptor.java (line 204) disk_failure_policy is stop
 INFO [main] 2013-02-07 19:48:54,524 DatabaseDescriptor.java (line 267) Global memtable threshold is enabled at 339MB
 INFO [main] 2013-02-07 19:48:55,099 CacheService.java (line 111) Initializing key cache with capacity of 50 MBs.
 INFO [main] 2013-02-07 19:48:55,109 CacheService.java (line 140) Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO [main] 2013-02-07 19:48:55,110 CacheService.java (line 154) Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
 INFO [main] 2013-02-07 19:48:55,117 CacheService.java (line 166) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,452 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_keyspaces\system-schema_keyspaces-ib-1 (258 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,484 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_keyspaces\system-schema_keyspaces-ib-3 (262 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,489 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_keyspaces\system-schema_keyspaces-ib-2 (262 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,517 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-ib-1 (4420 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,522 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-ib-3 (4424 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,525 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-ib-2 (4424 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,543 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columns\system-schema_columns-ib-3 (3750 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,548 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columns\system-schema_columns-ib-1 (3747 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,553 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columns\system-schema_columns-ib-2 (3748 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,588 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\local\system-local-ib-16 (119 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,594 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\local\system-local-ib-18 (436 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,600 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\local\system-local-ib-17 (109 bytes)
 INFO [main] 2013-02-07 19:48:55,610 AutoSavingCache.java (line 139) reading saved cache C:\Cassandra\saved_caches\system-local-KeyCache-b.db
 WARN [main] 2013-02-07 19:48:55,614 AutoSavingCache.java (line 160) error reading saved cache C:\Cassandra\saved_caches\system-local-KeyCache-b.db
java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:277)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:364)
	at org.apache.cassandra.db.Table.initCf(Table.java:337)
	at org.apache.cassandra.db.Table.<init>(Table.java:280)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.db.Table.open(Table.java:88)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:421)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:177)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:56,212 SSTableReader.java (line 164) Opening C:\Cassandra\data\system_auth\users\system_auth-users-ib-1 (72 bytes)
 INFO [main] 2013-02-07 19:48:56,242 CassandraDaemon.java (line 224) completed pre-loading (3 keys) key cache.
{code}","JVM vendor/version: Java HotSpot(TM) Client VM/1.7.0_11
OS: Windows 7 Enterprise SP1, x64.",,,,,,,,,,CASSANDRA-4916,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-02-20 02:37:28.738,,,no_permission,,,,,,,,,,,,312880,,,Wed Feb 20 02:37:28 UTC 2013,,,,,,0|i1hzsn:,313226,,,,,,,,,,,,"20/Feb/13 02:37;dbrosius;applied CASSANDRA-4916 to cassandra-1.2, which was mistakenly only applied to trunk until now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User rpc_address for binary protocol and change default port,CASSANDRA-4751,12610056,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,03/Oct/12 09:41,12/Mar/19 14:02,13/Mar/19 22:27,04/Oct/12 09:12,1.2.0 beta 2,,,,,0,,,,,,,"The events mechanism of the binary protocol require that we know the address on which other nodes can be joined (for the binary protocol). Hence CASSANDRA-4501. However, in 1.2 we've already burned all the padding in 1.1 gossip, so we can't gossip a new info (the binary protocol address), so CASSANDRA-4501 will have to move to 1.3.

But we do already gossip the rpc_address value, so an option is to make the binary protocol bind on the rpc_address (but a specific port) rather than having it's own setting. This ticket suggests to do that. Imo, there is little downside to do it: the thrift and binary transport are not meant to be used together except during the transition from one to the other, and even then having to use the same network interface is hardly a limitation (in other words, even for 1.3, we might want to hold on CASSANDRA-4501 until someone comes with a compelling use case for it).",,,,,,,,,,,,,,,,,,,03/Oct/12 09:43;slebresne;4751.txt;https://issues.apache.org/jira/secure/attachment/12547514/4751.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-03 17:52:35.773,,,no_permission,,,,,,,,,,,,240241,,,Thu Oct 04 09:12:49 UTC 2012,,,,,,0|i00yp3:,3554,jbellis,jbellis,,,,,,,,,,03/Oct/12 09:43;slebresne;Attaching patch for this. The patch also change the default port from 8000 since it's a bit crowded as a port number.,"03/Oct/12 17:52;jbellis;I don't follow {{InetAddress.getByName(StorageService.instance.getRpcaddress(endpoint));}} -- why convert it to a String, to convert back w/ getByName?","03/Oct/12 18:27;slebresne;{{endpoint}} is the listen_address of the node, but for event, we want to provide the address on which the node can be joined by a client, hence StorageService.instance.getRpcaddress.","03/Oct/12 21:29;jbellis;Feels a bit hackish but since we're not doing it in performance critical areas, +1","04/Oct/12 09:12;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot create keyspace with specific keywords through cli,CASSANDRA-4129,12549881,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,mkmainali,mkmainali,06/Apr/12 13:42,12/Mar/19 14:02,13/Mar/19 22:27,06/Apr/12 16:46,1.0.10,,,,,0,,,,,,,"Keyspaces cannot be create when the keyspace name which are used as keywords in the cli, such as 'keyspace', 'family' etc., through CLI. Even when surrounding the keyspace with quotation does not solve the problem. However, such keyspaces can be created through other client such as Hector.

This is similar to the issue CASSANDRA-3195, in which the column families could not be created. Similar to the solution of CASSANDRA-3195, using String keyspaceName = CliUtil.unescapeSQLString(statement.getChild(0).getText()) in executeAddKeySpace would solve the problem. ",,,,,,,,,,,,,,,,,,,06/Apr/12 14:46;xedin;CASSANDRA-4129.patch;https://issues.apache.org/jira/secure/attachment/12521680/CASSANDRA-4129.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-06 14:12:00.154,,,no_permission,,,,,,,,,,,,234872,,,Fri Apr 06 16:46:49 UTC 2012,,,,,,0|i0gsbj:,96018,jbellis,jbellis,,,,,,,,,,"06/Apr/12 13:58;mkmainali;To be more specific, creating keyspaces like 'create keyspace keyspace' or 'create keyspace 'keyspace'' or 'create keyspace family' would fail and raise syntax error error.",06/Apr/12 14:12;jbellis;cli keywords must be quoted,"06/Apr/12 14:27;mkmainali;For keyspace creation quotation, both single and double, does not work.

Here is a sample of error I get when creating keyspace

[default@unknown]create keyspace 'column';
Invalid keyspace name: 'column'

[default@unknown]create keyspace ""column"";
Syntax error at position 16: unexpected """""" for `create keyspace ""column"";`.

Using quotation works for cf, but not for ks. Anything specific I need to do?",06/Apr/12 14:46;xedin;patch against cassandra-1.0,06/Apr/12 16:35;jbellis;+1,06/Apr/12 16:46;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exit status of bin/cassandra without -f is wrong,CASSANDRA-4271,12556789,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thobbs,thobbs,22/May/12 01:27,12/Mar/19 14:02,13/Mar/19 22:27,23/Jul/12 16:46,1.1.3,,,,,0,,,,,,,"The launch_service() function returns {{$?}} after exec'ing java, and the script then exits with that same status.

The problem is that we do a {{[ ! -z ""$pidpath""] && ...}} conditional statment after exec'ing when the foreground flag isn't set.  The value of {{$?}} then depends on that conditional and the statement, typically returning 1, because {{$pidpath}} isn't set.  So, even if everything appears to execute normally, you will get an exit status of 1 for the whole script.

I suspect the right thing to do is just return 0 when backgrounding.",,,,,,,,,,,,,,,,,,,13/Jul/12 20:56;thepaul;0001-startup-script-returns-0-after-backgrounding.patch;https://issues.apache.org/jira/secure/attachment/12536455/0001-startup-script-returns-0-after-backgrounding.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-13 20:56:49.279,,,no_permission,,,,,,,,,,,,256013,,,Mon Jul 23 16:46:51 UTC 2012,,,,,,0|i0gtzj:,96288,brandon.williams,brandon.williams,,,,,,,,,,"13/Jul/12 20:56;thepaul;Patch attached; also present in the 4271 branch in my github. Current version tagged pending/4271.

https://github.com/thepaul/cassandra/tree/4271",23/Jul/12 16:46;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL3: cqlsh exception running ""describe schema""",CASSANDRA-4309,12559451,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,cdaw,cdaw,05/Jun/12 21:45,12/Mar/19 14:02,13/Mar/19 22:27,13/Jul/12 15:46,1.1.3,,Legacy/Tools,,,0,cqlsh,,,,,,"{code}
cqlsh> describe schema;

CREATE KEYSPACE system WITH strategy_class = 'LocalStrategy';

USE system;

Traceback (most recent call last):
  File ""./cqlsh"", line 811, in onecmd
    self.handle_statement(st, statementtext)
  File ""./cqlsh"", line 839, in handle_statement
    return custom_handler(parsed)
  File ""./cqlsh"", line 1329, in do_describe
    self.describe_schema()
  File ""./cqlsh"", line 1264, in describe_schema
    self.print_recreate_keyspace(k, sys.stdout)
  File ""./cqlsh"", line 1091, in print_recreate_keyspace
    self.print_recreate_columnfamily(ksname, cf.name, out)
  File ""./cqlsh"", line 1114, in print_recreate_columnfamily
    layout = self.get_columnfamily_layout(ksname, cfname)
  File ""./cqlsh"", line 706, in get_columnfamily_layout
    layout = self.fetchdict()
  File ""./cqlsh"", line 605, in fetchdict
    return dict(zip([d[0] for d in desc], row))
TypeError: 'NoneType' object is not iterable
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-06-05 21:47:54.926,,,no_permission,,,,,,,,,,,,256049,,,Fri Jul 13 15:46:14 UTC 2012,,,,,,0|i0gufz:,96362,brandon.williams,brandon.williams,,,,,,,,,,05/Jun/12 21:47;thepaul;Occurs when using CQL3 and when the keyspace has no columnfamilies. (Workaround: make a columnfamily :),09/Jul/12 23:44;thepaul;This should be fixed for the system keyspace case by CASSANDRA-4380. Can't reproduce my earlier theory that this was caused by empty keyspaces in general; they seem to be working fine in 1.1.1 and on the 1.1 tip.,"13/Jul/12 15:46;brandon.williams;Resolving since CASSANDRA-4380 is in.  Cathy, if you still experience this on 1.1 tip please feel free to reopen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh doesn't show correct timezone when SELECTing a column of type TIMESTAMP,CASSANDRA-5046,12623040,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,btoddb,btoddb,09/Dec/12 03:24,12/Mar/19 14:02,13/Mar/19 22:27,10/Dec/12 15:28,1.1.8,1.2.0 rc1,,,,0,,,,,,,"trying to figure out if i'm doing something wrong or a bug.  i am
creating a simple schema, inserting a timestamp using ISO8601 format,
but when retrieving the timestamp, the timezone is displayed
incorrectly.  i'm inserting using GMT, the result is shown with
""+0000"", but the time is for my local timezone (-0800)

tried with 1.1.6 (DSE 2.2.1), and 1.2.0-rc1-SNAPSHOT

here's the trace:

bin/cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 2.3.0 | Cassandra 1.2.0-rc1-SNAPSHOT | CQL spec 3.0.0 | Thrift
protocol 19.35.0]
Use HELP for help.
cqlsh> CREATE KEYSPACE btoddb WITH replication =
{'class':'SimpleStrategy', 'replication_factor':1};
cqlsh>
cqlsh> USE btoddb;
cqlsh:btoddb> CREATE TABLE test (
          ...   id uuid PRIMARY KEY,
          ...   ts TIMESTAMP
          ... );
cqlsh:btoddb>
cqlsh:btoddb> INSERT INTO test
          ...   (id, ts)
          ...   values (
          ...     '89d09c88-40ac-11e2-a1e2-6067201fae78',
          ...     '2012-12-07T10:00:00-0000'
          ...   );
cqlsh:btoddb>
cqlsh:btoddb> SELECT * FROM test;

 id                                   | ts
--------------------------------------+--------------------------
 89d09c88-40ac-11e2-a1e2-6067201fae78 | 2012-12-07 02:00:00+0000

cqlsh:btoddb>",cassandra 1.1.6 (DSE 2.2.1) or cassandra RC1 (from tip),,,,,,,,,,,,,,,,,,09/Dec/12 16:47;iamaleksey;5046-1.1.txt;https://issues.apache.org/jira/secure/attachment/12560096/5046-1.1.txt,09/Dec/12 16:46;iamaleksey;5046-1.2.txt;https://issues.apache.org/jira/secure/attachment/12560095/5046-1.2.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-09 16:51:02.075,,,no_permission,,,,,,,,,,,,296632,,,Mon Dec 10 15:27:47 UTC 2012,,,,,,0|i14b2f:,233256,jbellis,jbellis,,,,,,,,,,"09/Dec/12 16:51;iamaleksey;Weird. I remember correcting this before attaching the original patches for CASSANDRA-4746, but apparently I attached the patches with this mistake back then.",10/Dec/12 14:56;jbellis;+1,"10/Dec/12 15:27;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't prepare an UPDATE query with a counter column (CQL3),CASSANDRA-5022,12618606,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,iamaleksey,iamaleksey,03/Dec/12 18:24,12/Mar/19 14:02,13/Mar/19 22:27,05/Dec/12 08:03,1.2.0 rc1,,,,,0,,,,,,,"CQL3, binary protocol:

demo(id int primary key, counter counter)

Preparing ""UPDATE test.counters SET counter = counter + ? WHERE id = ?"" yields 8704, ""Invalid operation for commutative columnfamily counters"" error.",,,,,,,,,,,,,,,,,,,04/Dec/12 18:06;slebresne;5002.txt;https://issues.apache.org/jira/secure/attachment/12555958/5002.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-04 18:06:15.506,,,no_permission,,,,,,,,,,,,295764,,,Wed Dec 05 08:03:39 UTC 2012,,,,,,0|i13xtr:,231110,iamaleksey,iamaleksey,,,,,,,,,,04/Dec/12 18:06;slebresne;Attaching patch to fix that. The patch also move some small bit of validation that were done at execution time to preparation time (since there is no good reason not to do it there).,04/Dec/12 23:42;iamaleksey;+1,"05/Dec/12 08:03;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Auth.setup() is called too early,CASSANDRA-5049,12623189,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,10/Dec/12 18:26,12/Mar/19 14:02,13/Mar/19 22:27,10/Dec/12 19:41,1.2.0 rc1,,,,,0,,,,,,,"Auth.setup() triggers a request against the system_auth keyspace, request that is not an internal one, so it at least require TokenMetadata to be set up. However, Auth.setup() is call much too early, even before the commit log is replayed. The only reason this doesn't trigger an assertionError everytime is because Auth.setup() actually only schedul it's request after RING_DELAY, but still, replaying the commit log can take much more than that, and even without that I suspect this would be racy with normal bootstrap.",,,,,,,,,,,,,,,,,,,10/Dec/12 19:17;iamaleksey;5049-v2.txt;https://issues.apache.org/jira/secure/attachment/12560242/5049-v2.txt,10/Dec/12 18:41;slebresne;5049.txt;https://issues.apache.org/jira/secure/attachment/12560234/5049.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-12-10 18:53:36.133,,,no_permission,,,,,,,,,,,,296800,,,Mon Dec 10 19:40:50 UTC 2012,,,,,,0|i14hdj:,234278,,,,,,,,,,,,"10/Dec/12 18:41;slebresne;Attaching a naive patch that call Auth.setup() only once we know it will work. I however suspect that the authenticator and authorizer should be setup much sooner than that. So the right approach is probably to split that setup() into two methods, but I'll wait for someone knowledgeable of the Auth thingy to confirm out of laziness.","10/Dec/12 18:53;iamaleksey;Right, IAuthenticator#setup and IAuthorizer#setup should be called before that (where Auth#setup used to be), but superuser part should be scheduled after commit-log replay.","10/Dec/12 19:31;slebresne;Auth.setupSuperuser needs to be called in the 'isSurveyMode' of joinRing too, but otherwise +1.","10/Dec/12 19:40;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't allow prepared marker inside collections,CASSANDRA-4890,12614336,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,01/Nov/12 10:39,12/Mar/19 14:02,13/Mar/19 22:27,01/Nov/12 14:32,1.2.0 beta 2,,,,,0,,,,,,,"Currently the parser don't disallow preparing queries like (where l is a list<string>):
{noformat}
INSERT INTO test (k, l) VALUES (0, [1, ?, 2])
{noformat}

However, we don't handler it correctly. And in fact we can't really handle it properly currently since we return the name of the prepared column during prepare and here the marker don't correspond to a column (concretely, the code currently return l and list<string> for the name and type of the prepared value, which is incorrect). We also don't handle it during execute, though that last could in theory be fixed with some effort.

But overall I don't think allowing that kind of things is really useful (you can of course prepare the whole collection), so I suggest just refusing it for now.",,,,,,,,,,,,,,,,,,,01/Nov/12 10:40;slebresne;4890.txt;https://issues.apache.org/jira/secure/attachment/12551694/4890.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-01 13:15:42.392,,,no_permission,,,,,,,,,,,,253580,,,Thu Nov 01 14:32:44 UTC 2012,,,,,,0|i0dvnj:,79058,jbellis,jbellis,,,,,,,,,,01/Nov/12 13:15;jbellis;+1,"01/Nov/12 14:32;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
no need to keep tombstones in HintsColumnFamily,CASSANDRA-4892,12614400,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,mdennis,mdennis,01/Nov/12 17:13,12/Mar/19 14:02,13/Mar/19 22:27,01/Nov/12 19:19,1.2.0 beta 2,,,,,0,,,,,,,"Once a hint is delivered, it is removed from the HintsColumnFamily.  Because it is local and would only be deleted after expiration or after a correct delivery, there is no need to keep any tombstones (i.e. gc_grace_seconds should be zero)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-11-01 19:19:17.425,,,no_permission,,,,,,,,,,,,253724,,,Thu Nov 01 19:19:17 UTC 2012,,,,,,0|i0e4tb:,80542,,,,,,,,,,,,"01/Nov/12 19:19;jbellis;Done in d1dd9a10295e408daf9107d4fe4c47dbece75195 for trunk -- this was a regression in the 1.2 hints rewrite, it is already set to zero in earlier releases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong bloom_filter_fp_chance for newly created CFs with LeveledCompactionStrategy,CASSANDRA-5093,12625373,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,27/Dec/12 23:58,12/Mar/19 14:02,13/Mar/19 22:27,28/Dec/12 15:38,1.2.0,,,,,0,,,,,,,"0.1 is supposed to be the default bloom_filter_fp_chance for LeveledCompactionStrategy (and 0.01 for all other strategies).
However, CFPropDefs#applyToCFMetadata() sets bloom_filter_fp_chance before setting compaction strategy class, so the default bloom_filter_fp_chance is always 0.01 no matter what the compaction strategy is.

The fix is to move cfm#bloomFilterFpChance() call below cfm#compressionParameters().

The attached patch also kills dead default consistency level-related code.",,,,,,,,,,,,,,,,,,,28/Dec/12 00:00;iamaleksey;5093.txt;https://issues.apache.org/jira/secure/attachment/12562532/5093.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-28 15:20:16.01,,,no_permission,,,,,,,,,,,,301915,,,Fri Dec 28 15:37:58 UTC 2012,,,,,,0|i16xef:,248543,jbellis,jbellis,,,,,,,,,,28/Dec/12 15:20;jbellis;+1,"28/Dec/12 15:37;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unit test failing under long-test,CASSANDRA-4810,12611905,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,bbucher,bbucher,15/Oct/12 20:59,12/Mar/19 14:02,13/Mar/19 22:27,16/Oct/12 16:47,1.2.0 beta 2,,Legacy/Testing,,,0,,,,,,,"the following failure occurs when running ant long-test

junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionsTest
    [junit] Tests run: 5, Failures: 1, Errors: 0, Time elapsed: 31.28 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionsTest: sstables=2 rowsper=1 colsper=200000: 2173 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionsTest: sstables=2 rowsper=200000 colsper=1: 4531 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionsTest: sstables=100 rowsper=800 colsper=5: 1864 ms
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStandardColumnCompactions(org.apache.cassandra.db.compaction.LongCompactionsTest):	FAILED
    [junit] expected:<9> but was:<99>
    [junit] junit.framework.AssertionFailedError: expected:<9> but was:<99>
    [junit] 	at org.apache.cassandra.db.compaction.CompactionsTest.assertMaxTimestamp(CompactionsTest.java:207)
    [junit] 	at org.apache.cassandra.db.compaction.LongCompactionsTest.testStandardColumnCompactions(LongCompactionsTest.java:141)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LongCompactionsTest FAILED
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-10-16 01:59:46.897,,,no_permission,,,,,,,,,,,,248821,,,Tue Oct 16 16:47:37 UTC 2012,,,,,,0|i0a07b:,56306,,,,,,,,,,,,16/Oct/12 01:59;jbellis;is this 1.1 or trunk?,16/Oct/12 02:53;bbucher;trunk.,"16/Oct/12 16:47;yukim;AssertionError is caused by one of tests moved from unit test recently in trunk.
testStandardColumnCompactions should have cleared SSTables generated in other tests in order to run properly.
Committed fix in 8c471240d0ffafeef53df2fd69693294257ed730.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
leveled compaction does less work in L0 than intended,CASSANDRA-4778,12610898,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,08/Oct/12 22:40,12/Mar/19 14:02,13/Mar/19 22:27,09/Oct/12 03:49,1.1.6,,,,,0,compaction,lcs,,,,,"We have this code in the candidate loop:

{code}
.               if (SSTable.getTotalBytes(candidates) > maxSSTableSizeInBytes)
                {
                    // add sstables from L1 that overlap candidates
                    candidates.addAll(overlapping(candidates, generations[1]));
                    break;
                }
{code}

thus, as soon as we have enough to compact to make one L1 sstable's worth of data, we stop collecting candidates.",,,,,,,,,,,,,,,,,,,08/Oct/12 22:41;jbellis;4778.txt;https://issues.apache.org/jira/secure/attachment/12548322/4778.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-08 23:03:17.52,,,no_permission,,,,,,,,,,,,245693,,,Wed Oct 10 17:04:37 UTC 2012,,,,,,0|i06cqf:,34997,yukim,yukim,,,,,,,,,,08/Oct/12 22:41;jbellis;patch to move combine-with-L1 logic outside the candidates loop,08/Oct/12 22:42;jbellis;(introduced by CASSANDRA-4341),08/Oct/12 23:03;yukim;+1,09/Oct/12 03:49;jbellis;committed,10/Oct/12 17:04;jbellis;Also added a fix in f34bd79b9a92f23c1fc5e185e074d7faa880fc0b to avoid infinite compaction on a single L0 sstable that isn't large enough to promote.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TTL/WRITETIME function against collection column returns invalid value,CASSANDRA-4992,12617624,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,yukim,yukim,26/Nov/12 15:02,12/Mar/19 14:02,13/Mar/19 22:27,26/Nov/12 18:23,1.2.0 beta 3,,,,,0,,,,,,,"Since we cannot query individual content of collection in 1.2, TTL/WRITETIME function on collection column does not make sense. But currently we can perform those function on collection and get deserialization error like:

{code}
value '\x00\x03\x00\x01c\x00\x01b\x00\x01a' (in col 'writetime(l)') can't be deserialized as bigint: unpack requires a string argument of length 8
{code}

Looks like it tries to deserialize whole list/set/map content as bigint for WRITETIME and int for TTL.",,,,,,,,,,,,,,,,,,,26/Nov/12 16:27;slebresne;4992.txt;https://issues.apache.org/jira/secure/attachment/12554873/4992.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-26 16:27:04.155,,,no_permission,,,,,,,,,,,,292127,,,Mon Nov 26 18:23:37 UTC 2012,,,,,,0|i0rryn:,160174,yukim,yukim,,,,,,,,,,26/Nov/12 16:27;slebresne;Simple patch attached to just refuse said function on collection columns.,26/Nov/12 16:50;yukim;+1,"26/Nov/12 18:23;slebresne;Commited, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion error while delivering the hints.,CASSANDRA-4320,12559780,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,08/Jun/12 00:25,12/Mar/19 14:02,13/Mar/19 22:27,15/Jun/12 03:04,1.1.2,1.2.0 beta 1,,,,0,,,,,,,"java.lang.AssertionError
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:351)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:269)
        at org.apache.cassandra.db.HintedHandOffManager.access$400(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:442)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

Did some digging and looks like we just need to skip the deleted columns.",,,,,,,,,,,,,,,,,,,08/Jun/12 03:47;vijay2win@yahoo.com;0001-CASSANDRA-4320-v2.patch;https://issues.apache.org/jira/secure/attachment/12531366/0001-CASSANDRA-4320-v2.patch,08/Jun/12 00:31;vijay2win@yahoo.com;0001-CASSANDRA-4320.patch;https://issues.apache.org/jira/secure/attachment/12531352/0001-CASSANDRA-4320.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-06-08 02:27:13.584,,,no_permission,,,,,,,,,,,,256058,,,Wed Jun 13 18:58:20 UTC 2012,,,,,,0|i0gujj:,96378,jbellis,jbellis,,,,,,,,,,"08/Jun/12 02:27;jbellis;So...  I guess the deletion during one page, doesn't get purged by removeDeleted in the next since it's happening in the same ms?

What if we just increased the gcBefore in the removeDeleted call to MAX_VALUE?",08/Jun/12 03:47;vijay2win@yahoo.com;ahaaa that works too :) Plz see the attached. Thanks!,"08/Jun/12 17:46;vijay2win@yahoo.com;Actually v2 wont work because RowMutation.hintFor we do 
{code}
ttl = Math.min(ttl, cf.metadata().getGcGraceSeconds()); 
{code}

so v1 is still better.","09/Jun/12 15:25;jbellis;I don't follow, using max_value means that if an ExpiringColumn has expired we will ignore it, if it has not then we don't want to.","09/Jun/12 17:49;vijay2win@yahoo.com;Sorry i should have been clear, because of 

CFS.removeDeletedSuper
{code}
if (subColumn.getLocalDeletionTime() < gcBefore
                    || cf.deletionInfo().isDeleted(c.name(), subColumn.timestamp())
                    || c.deletionInfo().isDeleted(subColumn))
                {
                    subIter.remove();
                }
{code}

Hence we will remove everything and never replay any column if we set the max_value. Makes sense?",13/Jun/12 18:58;jbellis;makes sense.  committed v1 with a comment.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe commands fail in cql3 when previously created with cql2,CASSANDRA-5101,12625812,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,mkjellman,mkjellman,03/Jan/13 05:33,12/Mar/19 14:02,13/Mar/19 22:27,04/Jan/13 16:27,1.2.1,,,,,0,,,,,,,"column families and keyspaces created with cassandra-cli/cql2 cannot be described with cql3

describe table cfname fails with: ""expected string or buffer""
describe schema fails with ""expected string or buffer"" as well",,,,,,,,,,,,,,,,,,,04/Jan/13 15:50;iamaleksey;5101-v2.txt;https://issues.apache.org/jira/secure/attachment/12563297/5101-v2.txt,03/Jan/13 21:42;iamaleksey;5101.txt;https://issues.apache.org/jira/secure/attachment/12563165/5101.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-03 13:32:01.353,,,no_permission,,,,,,,,,,,,302358,,,Tue Jan 08 22:22:15 UTC 2013,,,,,,0|i170cv:,249025,brandon.williams,brandon.williams,,,,,,,,,,03/Jan/13 05:41;mkjellman;interesting -- if i drop an index in cqlsh in cql2 mode and then recreate it in cql3 describe commands work for that cf. all non-modified cf's post upgrade still fail though.,"03/Jan/13 13:32;brandon.williams;From the error it sounds like you're using cqlsh, not the cli?",03/Jan/13 15:51;mkjellman;Using cqlsh in 1.2. Cf were created in cassandra-cli and cql2 describes them just fine in cqlsh in 1.2,03/Jan/13 16:59;iamaleksey;[~mkjellman]Can you attach the CQL2 CREATE statement you used for the CF that can't be described?,"03/Jan/13 17:06;mkjellman;cassandra-cli create statement examples. let me know if you need more:

create keyspace evidence with strategy_class = NetworkTopologyStrategy AND strategy_options:DC1 = 2;
create column family messages with key_validation_class = BytesType AND comparator = BytesType AND compression_options={sstable_compression:SnappyCompressor, chunk_length_kb:64} AND bloom_filter_fp_chance = 0.5 and compaction_strategy = SizeTieredCompactionStrategy;

__

create column family domain_metadata_history with key_validation_class = UTF8Type AND default_validation_class = UTF8Type AND comparator = 'CompositeType(UTF8Type, UTF8Type, IntegerType)' AND compression_options = {sstable_compression:SnappyCompressor, chunk_length_kb:64} WITH compaction_strategy=LeveledCompactionStrategy AND compaction_strategy_options={sstable_size_in_mb: 100};","03/Jan/13 17:32;iamaleksey;[~mkjellman] Are these the one that fail for you or the ones that work? Because my 1.2.0 cqlsh describes them just fine in both default and legacy modes.

default (CQL3):
{noformat}
cqlsh:evidence> DESC KEYSPACE;

CREATE KEYSPACE evidence WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'DC1': '2'
};

USE evidence;

CREATE TABLE domain_metadata_history (
  key text,
  column1 text,
  column2 text,
  column3 varint,
  value text,
  PRIMARY KEY (key, column1, column2, column3)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.100000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  compaction={'sstable_size_in_mb': '100', 'class': 'LeveledCompactionStrategy'} AND
  compression={'chunk_length_kb': '64', 'sstable_compression': 'SnappyCompressor'};

CREATE TABLE messages (
  key blob,
  column1 blob,
  value blob,
  PRIMARY KEY (key, column1)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.500000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'chunk_length_kb': '64', 'sstable_compression': 'SnappyCompressor'};
{noformat}

legacy (CQL2):
{noformat}
cqlsh:evidence> DESC KEYSPACE;

CREATE KEYSPACE evidence WITH strategy_class = 'NetworkTopologyStrategy'
  AND strategy_options:DC1 = '2';

USE evidence;

CREATE TABLE domain_metadata_history (
  KEY text PRIMARY KEY
) WITH
  comment='' AND
  comparator='org.apache.cassandra.db.marshal.CompositeType'<text, text, varint> AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='LeveledCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='100' AND
  compression_parameters:chunk_length_kb='64' AND
  compression_parameters:sstable_compression='SnappyCompressor';

CREATE TABLE messages (
  KEY blob PRIMARY KEY
) WITH
  comment='' AND
  comparator=blob AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:chunk_length_kb='64' AND
  compression_parameters:sstable_compression='SnappyCompressor';
{noformat}","03/Jan/13 17:37;mkjellman;Those fail for me.

{code:title=""CQL3""}
[cqlsh 2.3.0 | Cassandra 1.2.0 | CQL spec 3.0.0 | Thrift protocol 19.35.0]
Use HELP for help.
cqlsh> describe schema

CREATE KEYSPACE evidence WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'DC2': '0',
  'DC1': '2'
};

USE evidence;

expected string or buffer
cqlsh>
{code}

{code:title=""CQL2""}
[cqlsh 2.3.0 | Cassandra 0.0.0 | CQL spec 2.0.0 | Thrift protocol 19.35.0]
Use HELP for help.
cqlsh> describe schema

CREATE KEYSPACE evidence WITH strategy_class = 'NetworkTopologyStrategy'
  AND strategy_options:DC2 = '0'
  AND strategy_options:DC1 = '2';

USE evidence;

CREATE TABLE fingerprints (
  KEY blob PRIMARY KEY
) WITH
  comment='' AND
  comparator=blob AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=172800 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

...and the rest of the schema...
{code}","03/Jan/13 17:42;iamaleksey;[~mkjellman] Created with CQL2 from your example, described with CQL3:

{noformat}
CREATE TABLE fingerprints (
  key blob,
  column1 blob,
  value blob,
  PRIMARY KEY (key, column1)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=172800 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
{noformat}

I'm afraid I still can't reproduce :(","03/Jan/13 17:46;mkjellman;are you creating in 1.1.7 though, upgrading and then describing?

edit: i should clarify these column families were created prior to 1.2 on a variety of versions. when i modify the schema for a cf in cql2 on 1.2 describe for that cf works in 1.2. the non modified cf's still fail.","03/Jan/13 17:49;iamaleksey;Nope. That was never mentioned in the issue, though.","03/Jan/13 17:50;mkjellman;yeah, my bad. sorry. minor detail ;)","03/Jan/13 18:52;iamaleksey;Yep, there is an issue with key_aliases column of system.schema_columnfamilies cf for 1.1 tables. It returns null when it should be returning '[]', and json-loading it fails with 'expected string or buffer' error.","04/Jan/13 07:45;slebresne;For the record, I think I'd prefer fixing cqlsh to be less fragile and to handle both null and [] the same way (it should probably do that for column_aliases too btw). Adding code at startup to rewrite nulls to [] feels highly overkill here, especially since having no key_aliases column is no wrong per se.

I note that the fact we have 2 way to mean ""no key aliases"" is unfortunate but just a downside of using a json-encoded list internally and that'll be fix once we switch to true lists.","04/Jan/13 13:10;iamaleksey;I'm checking now, but it seems like there is another issue with key aliases for upgraded 1.1 instances - if they have a key alias, it's going to be in the key_alias column, which we are no longer using.

That would make key aliases even less consistent (need to look at key_alias column, need to check key_aliases for null). I think pushing this complexity to the clients is a bad thing (cqlsh is not the only consumer of system.shema_* tables) - the better way would be to convert non-null key_alias to key_aliases singleton list and null key_alias to an empty list on startup, once.

I'm not a fat of either solution, but there is no third one, and I slightly prefer dealing with this in C* and not pushing the complexity to all the clients.","04/Jan/13 13:23;iamaleksey;Yep, that would be described incorrectly as well.

I think I dislike the overkill-conversion to 1.2-style on startup *slightly* less than I dislike that added extra-logic in every client using schema_columnfamilies metadata.

Another opinion, maybe?","04/Jan/13 13:51;slebresne;Converting things at startup is not as easy as it sound, because the schema is distributed. Typically, removing key_alias on startup on newly upgraded node would potentially propagate and screw up old, not-yet upgraded nodes (and/or new nodes would pull back the ""key_alias"" from older nodes after startup).

I hear you about not pushing the complexity to the clients, but at the same time we have to be extra careful with the schema table, as screwing things there is the one way to screw up a full cluster, and so making those tables convenient to read for clients is not a top priority (which is not saying we should make it hard on purpose but ...).

Also, converting the information from the schema table to a CQL3 definition is already far from trivial and I'm not sure that little details like the one on this issue really make much quantitative difference here. Don't get me wrong, this already existing complexity is not a good thing, and maybe exposing the system tables directly is not the best way to expose the schema for CQL3 clients. But my point is, as far as this ticket is concerned, I'd still rather avoid messing with schema at startup, especially to save a very small amount of complexity to clients library. But I'm all for discussing how we could fix the more general issue that the information exposed in the system schema is hard to make sense of for a CQL3 client.  ","04/Jan/13 13:56;iamaleksey;You are right.

Will attach a v2 with a cqlsh-fix instead and let's deal with the more general issue later.",04/Jan/13 16:15;brandon.williams;+1,"04/Jan/13 16:27;iamaleksey;Thanks, committed.",08/Jan/13 22:22;mkjellman;confirmed that this commit fixes the issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
some confusion around KEY pseudocolumn from Thrift tables,CASSANDRA-4955,12615924,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,jbellis,jbellis,13/Nov/12 17:50,12/Mar/19 14:02,13/Mar/19 22:27,29/Nov/12 19:54,1.2.0 beta 3,,,,,0,,,,,,,"Inserting into the schema created by cassandra-stress.  cqlsh {{DESCRIBE TABLE}} says

{code}
CREATE TABLE ""Standard1"" (
  ""KEY"" blob PRIMARY KEY,
  ""C0"" blob,
  ""C1"" blob,
  ""C2"" blob,
  ""C3"" blob,
  ""C4"" blob
) WITH COMPACT STORAGE AND
  comment='' AND
  caching='KEYS_ONLY' AND
  read_repair_chance=0.100000 AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  replicate_on_write='true';
{code}

but that casing doesn't actually work:

{noformat}
cqlsh:Keyspace1> insert into ""Standard1"" (""KEY"", ""C0"", ""C1"", ""C2"", ""C3"", ""C4"") values ('FF', '00', '11', '22', '33', '44');
Bad Request: Unknown identifier KEY
{noformat}

lowercase does work:

{noformat}
cqlsh:Keyspace1> insert into ""Standard1"" (""key"", ""C0"", ""C1"", ""C2"", ""C3"", ""C4"") values ('FF', '00', '11', '22', '33', '44');
{noformat}
",,,,,,,,,,,,,,,,,,,29/Nov/12 19:37;iamaleksey;4955.txt;https://issues.apache.org/jira/secure/attachment/12555390/4955.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-29 19:42:47.587,,,no_permission,,,,,,,,,,,,257504,,,Thu Nov 29 19:54:31 UTC 2012,,,,,,0|i0k2vz:,115285,brandon.williams,brandon.williams,,,,,,,,,,"29/Nov/12 19:42;brandon.williams;+1, this fixes the casing problem with the key (but CASSANDRA-4827 gets in the way now)","29/Nov/12 19:54;iamaleksey;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Truncate doesn't clear row cache,CASSANDRA-4940,12615547,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jjordan,jjordan,09/Nov/12 20:25,12/Mar/19 14:02,13/Mar/19 22:27,12/Nov/12 14:29,1.2.0 beta 3,,,,,0,,,,,,,"Truncate doesn't clear the row cache.  select * from <table> which skips the row cache returns no data, but selecting by key does.

cqlsh:temp> select v1..v3 from temp2 where k in (3,2,1);
 v1 | v2 | v3
----+----+----
 16 | 17 | 18
 12 | 13 | 14
  8 |  9 | 10

cqlsh:temp> truncate temp2;
cqlsh:temp> select v1..v3 from temp2 where k in (3,2,1);
 v1 | v2 | v3
----+----+----
 16 | 17 | 18
 12 | 13 | 14
  8 |  9 | 10

cqlsh:temp> select * from temp2;
cqlsh:temp> select v1..v3 from temp2 where k in (3,2,1);
 v1 | v2 | v3
----+----+----
 16 | 17 | 18
 12 | 13 | 14
  8 |  9 | 10

",,,,,,,,,,,,,,,,,,,12/Nov/12 03:52;jbellis;4940.txt;https://issues.apache.org/jira/secure/attachment/12553068/4940.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-12 03:52:26.56,,,no_permission,,,,,,,,,,,,256835,,,Mon Nov 12 14:29:48 UTC 2012,,,,,,0|i0iy1r:,108610,slebresne,slebresne,,,,,,,,,,12/Nov/12 03:52;jbellis;patch attached.,12/Nov/12 14:00;slebresne;+1 (I've pushed a test in dtest for this),12/Nov/12 14:29;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Starting Cassandra throws EOF while reading saved cache,CASSANDRA-4916,12614862,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,mkjellman,mkjellman,05/Nov/12 22:28,12/Mar/19 14:02,13/Mar/19 22:27,08/Jan/13 05:21,1.2.2,,,,,0,,,,,,,"Currently seeing nodes throw an EOF while reading a saved cache on the system schema when starting cassandra

 WARN 14:25:54,896 error reading saved cache /ssd/saved_caches/system-schema_columns-KeyCache-b.db
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:278)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:393)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:365)
	at org.apache.cassandra.db.Table.initCf(Table.java:334)
	at org.apache.cassandra.db.Table.<init>(Table.java:272)
	at org.apache.cassandra.db.Table.open(Table.java:102)
	at org.apache.cassandra.db.Table.open(Table.java:80)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:320)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:203)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:395)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:438)


to reproduce delete all data files, start a cluster, leave cluster up long enough to build a cache. nodetool drain, kill cassandra process. start cassandra process in foreground and note EOF thrown (see above for stack trace)",,,,,,,,,,,,,,,,,,,13/Nov/12 06:14;dbrosius;4916.txt;https://issues.apache.org/jira/secure/attachment/12553274/4916.txt,13/Feb/13 02:00;drew_kutchar;data.zip;https://issues.apache.org/jira/secure/attachment/12569138/data.zip,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-11-12 23:17:02.897,,,no_permission,,,,,,,,,,,,255360,,,Wed Feb 20 05:30:51 UTC 2013,,,,,,0|i0eran:,84184,,,,,,,,,,,,"12/Nov/12 23:17;jbellis;Wouldn't be the first time I've seen InputStream.available lie.  But we don't want to write the number of items in the cache at the start of the file (the approach we usually take) because that would require making a copy of the cache's keySet which might be more memory than we can afford.

Suggested workarounds: write some kind of EOF value when we're done instead of just closing the file, and check for that on read.  Alternatively, just catch the EOF and log it at debug; a partially-read cache is harmless.","13/Nov/12 05:58;jbellis;Michael, can you test Dave's patch?",13/Nov/12 06:14;dbrosius;oops fix formatting,"13/Nov/12 16:12;mkjellman;[~jbellis] yes, will test now",13/Nov/12 20:59;mkjellman;patch looks good. Ship it!,08/Jan/13 05:21;dbrosius;committed to trunk as 1d641f5111613a5a049042a8723d0dd9ffc29c02,"13/Feb/13 01:59;drew_kutchar;I just saw this same exception happen on Cassandra 1.2.1. Was this part of the 1.2.1 release?
I'm on Mac OS X 10.8.2, Oracle JDK 1.7.0_11, using snappy-java 1.0.5-M3 from Maven (not sure if that's the cause).

I'm attaching my data and log directory as data.zip.
",20/Feb/13 02:36;dbrosius;no mistakenly only applied to trunk... It is now applied to the cassandra-1.2 branch,20/Feb/13 05:30;jbellis;Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
comments and error messages use wrong method names,CASSANDRA-4688,12608242,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,crotwell,crotwell,crotwell,19/Sep/12 13:50,12/Mar/19 14:02,13/Mar/19 22:27,21/Sep/12 17:21,1.1.6,,,,,0,,,,,,,comments and error messages in ColumnFamilyInputFormat do not reflect the actual method names in ConfigHelper,,,,,,,,,,,,,,,,,,,19/Sep/12 13:52;crotwell;trunk-4688.txt;https://issues.apache.org/jira/secure/attachment/12545732/trunk-4688.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-21 17:21:49.506,,,no_permission,,,,,,,,,,,,256337,,,Fri Sep 21 17:21:49 UTC 2012,,,,,,0|i0gygv:,97014,jbellis,jbellis,,,,,,,,,,19/Sep/12 13:52;crotwell;patch,19/Sep/12 13:52;crotwell;patch file attached to fix comments and error messages,"21/Sep/12 17:21;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We should emit number of sstables in each level from JMX,CASSANDRA-4537,12603335,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,kohlisankalp,kohlisankalp,13/Aug/12 21:32,12/Mar/19 14:02,13/Mar/19 22:27,11/Sep/12 22:24,1.2.0 beta 1,,,,,0,compaction,leveled,,,,,"We should add methods to this Mbean org.apache.cassandra.db.ColumnFamilyStoreMBean

These metrics will be helpful to see how sstables are distributed in different levels and how they move to higher level with time. 
Currently we can see this by looking at the json file but with JMX, we can monitor the historic values over time using any monitoring tool.  ",,43200,43200,,0%,43200,43200,,,,,,,,,,,,10/Sep/12 20:09;yukim;4537-v2.txt;https://issues.apache.org/jira/secure/attachment/12544519/4537-v2.txt,07/Sep/12 20:57;yukim;4537.txt;https://issues.apache.org/jira/secure/attachment/12544289/4537.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-14 02:08:07.153,,,no_permission,,,,,,,,,,,,256226,,,Tue Mar 12 16:11:11 UTC 2013,,,,,,0|i0gwv3:,96754,jbellis,jbellis,,,,,,,,,,14/Aug/12 02:08;brandon.williams;I would go one further and say it should be available via nodetool compactionstats.  I can't count the number of times I've had to paste a python script that prints level counts.,"07/Sep/12 20:57;yukim;Attaching patch that adds SSTableCountPerLevel to ColulmnFamilyStoreMBean, which returns number of SSTable for each level in _int[]_ and prints it with nodetool cfstats.

Output looks like below:
{code}
...
    Column Family: Standard1
    SSTable count: 19
    SSTables in each level: [0, 8, 11, 0, 0, 0, 0, 0]
    Space used (live): 100923462
    Space used (total): 101012612
...
{code}

compactionstats displays info only when compaction is happening, so I chose cfstats to show info.

cfstats output is long, but let CASSANDRA-4191 handle it.","07/Sep/12 21:11;jbellis;Idea: show number of sstables / desired max, when we exceed the desired threshold?  This would give you a quick eyeball of ""here's where compaction is behind.""

SSTables in each level: [43/0, 8, 102/100, 123, 0, 0, 0, 0]
",10/Sep/12 20:09;yukim;v2 attached. It displays max threshold of level when exceeded.,"11/Sep/12 19:31;jbellis;+1, lgtm

nit: getLevelSize call is a little confusing since it handles i > generations.length, inlining to generations[i].size might be more clear",11/Sep/12 22:24;yukim;Committed with above nit fixed.,"11/Sep/12 23:18;hudson;Integrated in Cassandra #2090 (See [https://builds.apache.org/job/Cassandra/2090/])
    Add SSTable count per level to cfstats patch by yukim; reviewed by jbellis for CASSANDRA-4537 (Revision c64d975cdf9eebddb04801573035a7272f779fed)

     Result = ABORTED
yukim : 
Files : 
* src/java/org/apache/cassandra/db/compaction/LeveledManifest.java
* src/java/org/apache/cassandra/tools/NodeCmd.java
* src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java
* CHANGES.txt
* src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java
","07/Mar/13 15:57;cscetbon;Whenever I try to access this attribute I get an empty string in version 1.2.2 I was expecting that even if I have only stables in level 0 I would get an array like [x,0,0,0,..]","07/Mar/13 16:09;yukim;[~cscetbon] Do you mean SSTableCountPerLevel attribute in ColumnFamilyStoreMBean? It should return int[] even if there are only level 0s or no sstables at all when ColumnFamily is configured to use leveled compaction. When you use size tiered compaction, then it returns null.",12/Mar/13 16:11;cscetbon;why is this information not available when we are using size tiered compaction ? It would be interesting to know how much different similar-sized stables exist.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra 1.2 should not accept CQL version ""3.0.0-beta1""",CASSANDRA-4649,12607145,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,11/Sep/12 23:10,12/Mar/19 14:02,13/Mar/19 22:27,25/Sep/12 12:38,1.2.0 beta 2,,Legacy/CQL,,,0,cql3,,,,,,"During Cassandra 1.1's whole lifecycle, the CREATE KEYSPACE syntax was pretty dramatically and incompatibly different from what is there now for 1.2. That's ok, since we explicitly said there could be breaking changes in the syntax before 3.0.0 final, but at least we should make it clear that 3.0.0 is not compatible with the 3.0.0-beta1 syntax we had out for quite a while.

If we don't want to reject connections asking for 3.0.0-beta1, we should bump the version number to 3.0.1 or something.",,,,,,,,,,,,,,,,,,,12/Sep/12 13:13;slebresne;4649.txt;https://issues.apache.org/jira/secure/attachment/12544814/4649.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-12 13:13:52.763,,,no_permission,,,,,,,,,,,,255191,,,Tue Sep 25 12:38:20 UTC 2012,,,,,,0|i0epyn:,83968,thepaul,thepaul,,,,,,,,,,"12/Sep/12 13:13;slebresne;Patch attached that
* changes the version to 3.0.0 (we don't have anything in store that would need to break syntax further and we're about to roll 1.2.0 beta1 soon (hopefully) so I think it's indeed time to drop the beta).
* Makes 1.2 refuse 3.0.0-beta1 version (with an hopefully useful error message).

I note however that 1.1 was happily accepting 3.0.0 as a version, and I don't think we have advertised much that people should use 3.0.0-beta1 in 1.1, so most people will be surprised that their CREATE KEYSPACE query is invalid all of a sudden anyway (including user of cqlsh). We do have a NEWS entry to explain that however so not sure there is more we can/need to do. ",20/Sep/12 16:37;thepaul;+1.,"25/Sep/12 12:38;slebresne;Oups, forgot that one. Committed now. Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json always returns default value validator,CASSANDRA-5134,12626727,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,cangeli,cangeli,09/Jan/13 09:12,12/Mar/19 14:02,13/Mar/19 22:27,09/Jan/13 12:53,1.2.1,,Legacy/Tools,,,0,,,,,,,"When exporting to JSON tables created using cqlsh, values are always exported to hexa : the serializeColumn function fails to get the correct value validator from the cfMetaData, since getColumnDefinition(column) returns null.

I'm not a java expert and doesn't really understand the use of ByteBuffer for Map : the workaround I found is to pass to cfMetaData.getValueValidator a new wrapped ByteBuffer of the column.name argument, then the validator is correctly found from the cfMetaData.column_metadata.",,,,,,,,,,,,,,,,,,,09/Jan/13 11:16;slebresne;5134.txt;https://issues.apache.org/jira/secure/attachment/12563928/5134.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-09 11:16:48.278,,,no_permission,,,,,,,,,,,,303335,,,Wed Jan 09 12:53:33 UTC 2013,,,,,,0|i179if:,250509,brandon.williams,brandon.williams,,,,,,,,,,09/Jan/13 11:16;slebresne;Attaching patch to fix. We weren't really using the right method post-CQL3.,09/Jan/13 12:04;brandon.williams;+1,"09/Jan/13 12:53;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""show schema"" command in cassandra-cli generates wrong ""index_options"" values.",CASSANDRA-5008,12618350,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,thboileau,thboileau,30/Nov/12 17:42,12/Mar/19 14:02,13/Mar/19 22:27,07/Dec/12 17:47,1.1.8,1.2.0 rc1,,,,0,,,,,,,"Using cassandra-cli, launch the ""show schema"" command and save the output to a file.
Try to import it in order to recreate the schema, it fails with error message :
""Syntax error at position 626: no viable alternative at input '}'""

",ubuntu 12.04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-12-07 17:47:10.924,,,no_permission,,,,,,,,,,,,293011,,,Fri Dec 07 17:47:10 UTC 2012,,,,,,0|i0srcf:,165907,brandon.williams,brandon.williams,,,,,,,,,,"30/Nov/12 17:45;thboileau;After some investigations, it appears that the ""index_options"" attributes are badly generated when there is actually no index options :

create column family Test
  with column_type = 'Standard'
  and comparator = 'UTF8Type'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'UTF8Type'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and column_metadata = [
    {column_name : 'email',
    validation_class : UTF8Type,
    index_name : 'test_email',
    index_type : 0,
    index_options : {

}},
    {column_name : 'enabled',
    validation_class : UTF8Type,
    index_name : 'test_enabled',
    index_type : 0,
    index_options : {

}},
    {column_name : 'roles',
    validation_class : UTF8Type,
    index_name : 'test_roles',
    index_type : 0,
    index_options : {

}}]

When removing these empty ""index_options"" attributes, the script is correctly handled.","30/Nov/12 17:52;thboileau;I guess the fix is located in method ""showColumnMeta"" of class ""org.apache.cassandra.cli.CliClient"". It could be something like this:

if (colDef.index_options != null && !colDef.index_options.entrySet().isEmpty())
            {
                sb.append(TAB + TAB + ""index_options : {""+NEWLINE);        
                for (Map.Entry<String, String> entry : colDef.index_options.entrySet())
                {
                    sb.append(TAB + TAB + TAB + CliUtils.escapeSQLString(entry.getKey()) + "": '"" + CliUtils.escapeSQLString(entry.getValue()) + ""',"" + NEWLINE);
                }
                sb.append(""}"");
            }","07/Dec/12 17:47;yukim;Thanks Thierry, committed your !isEmpty check in 1ac90589ead4b95a922d783ead75412dd46c0f28",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update sstable2json for 1.1.x,CASSANDRA-4817,12612088,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,j.casares,j.casares,16/Oct/12 19:17,12/Mar/19 14:02,13/Mar/19 22:27,20/Sep/13 22:40,,,,,,0,datastax_qa,,,,,,"This format is still needed in 1.1.5:

bin/json2sstable -K KS -c CF CF.json KS-CF-he-2-Data.db

to comply with this method:

https://github.com/apache/cassandra/blob/cassandra-1.1.5/src/java/org/apache/cassandra/io/sstable/Descriptor.java#L160

even though 1.1.x now uses the directory structure KS/CF/CF-he-2-Data.db.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,249104,,,2012-10-16 19:17:25.0,,,,,,0|i0a4sn:,57050,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh timestamp formatting is broken - displays wrong timezone info (at least on Ubuntu),CASSANDRA-4746,12609853,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,02/Oct/12 02:09,12/Mar/19 14:02,13/Mar/19 22:27,30/Oct/12 18:51,1.1.7,1.2.0 beta 2,,,,0,cqlsh,,,,,,"cqlsh> create keyspace test with strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
cqlsh> use test;
cqlsh:test> create table ts (id int primary key, ts timestamp);
cqlsh:test> insert into ts (id, ts) values (1, '2012-05-14 07:53:20+0000');
cqlsh:test> select * from ts;
 id | ts
----+--------------------------
  1 | 2012-05-14 10:53:20+0000


Should've been 2012-05-14 10:53:20+0300.

cqlsh formats timestamps using '%Y-%m-%d %H:%M:%S%z' format-string and 'the %z escape that expands to the preferred hour/minute offset is not supported by all ANSI C libraries'. In this case it's just replaced with all zeroes.","64-bit Ubuntu 12.04, python 2.7.3",,,,,,,,,,,,,,,,,,26/Oct/12 12:59;iamaleksey;4746-1.1.txt;https://issues.apache.org/jira/secure/attachment/12550958/4746-1.1.txt,25/Oct/12 17:21;iamaleksey;4746.txt;https://issues.apache.org/jira/secure/attachment/12550811/4746.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-26 11:31:01.177,,,no_permission,,,,,,,,,,,,246153,,,Tue Oct 30 19:02:44 UTC 2012,,,,,,0|i07hs7:,41648,brandon.williams,brandon.williams,,,,,,,,,,26/Oct/12 11:31;brandon.williams;Is there any reason not to put this in 1.1?,26/Oct/12 13:00;iamaleksey;Not anymore.,26/Oct/12 13:48;brandon.williams;Committed.,30/Oct/12 15:45;jbellis;reopening because this was not merged to trunk.  (I got a conflict on merge so I just kept the trunk version as a temporary fix.),30/Oct/12 18:31;iamaleksey;Huh? I have it in trunk. Seems like it was merged just fine.,30/Oct/12 19:02;brandon.williams;I used both patches and did a 'merge -s ours' on the second one for trunk which may have caused confusion.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an official way to disable compaction,CASSANDRA-5074,12624486,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,jbellis,jbellis,18/Dec/12 16:13,12/Mar/19 14:02,13/Mar/19 22:27,09/Apr/13 16:38,2.0 beta 1,,,,,0,,,,,,,"We've traditionally used ""min or max compaction threshold = 0"" to disable compaction, but this isn't exactly intuitive and it's inconsistently implemented -- allowed from jmx, not allowed from cli.",,,,,,,,,,,,,,,,,,,05/Apr/13 13:49;krummas;0001-CASSANDRA-5074-make-it-possible-to-disable-autocompa.patch;https://issues.apache.org/jira/secure/attachment/12577213/0001-CASSANDRA-5074-make-it-possible-to-disable-autocompa.patch,09/Apr/13 10:25;krummas;0001-CASSANDRA-5074-v2.patch;https://issues.apache.org/jira/secure/attachment/12577774/0001-CASSANDRA-5074-v2.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-02-15 17:12:27.995,,,no_permission,,,,,,,,,,,,300035,,,Mon Dec 02 22:27:29 UTC 2013,,,,,,0|i165fr:,244013,slebresne,slebresne,,,,,,,,,,18/Dec/12 16:13;jbellis;Adding a NoCompactionStrategy may be the simplest approach.,"15/Feb/13 17:12;a.gazzarini;Hi, as far as I understood, that requires writing a NullObject implementation of AbstractCompactionStrategy. Is that enough? I tried in my local Cassandra installation 

> create column family with compaction_strategy=NoCompactionStrategy 

and effectively the compaction never run...but to be honest I'm a Cassandra newbie so I'm wondering if that is enough. In this case I could submit the patch, otherwise please point me to the right direction, i'd like to give a hand. ","17/Feb/13 13:51;a.gazzarini;It seems things are not so easy as described in my previous post. At the moment there are two possible strategies: Leveled and SizeTiered compaction. Looking at their code it seems that a disabled compaction should affect only getNextBackgroundTask(...) and not getMaximalTask and getUserDefinedTask (although this latter is not allowed in LeveredCompaction). 
So the question is: what kind of compaction should run when I'm using a NoCompactionStrategy and those two methods are invoked? SizeTiered, Leveled or another that maybe will be implemented tomorrow and plugged in?  

The obvious thing that comes in my mind is to think about NoCompactionStrategy as a decorator; something like

> create column family X with compaction_strategy=NoCompactionStrategy AND compaction_strategy_options={delegate_strategy: LeveledCompactionStrategy, <other options>};
> create column family X with compaction_strategy=NoCompactionStrategy AND compaction_strategy_options={delegate_strategy: SizeTieredCompactionStrategy, <other options>};

So basically the getNextBackgroundTask will do nothing, effectively disabling (minor) compaction, and the getMaximalTask and getUserDefinedTask will delegate to a wrapped strategy that has been injected (the ""delegate_strategy"" option). Now, I have a problem with the design of the XXXCompactionStrategy: in general compaction strategies (this is valid both for SizeTiered and Leveled) 

- have a lot of initialization code in their constructor, this makes hard to use those strategies as an ""inner"" state of a decorator, I mean something like : decoratee.init() --> decoratee.getMaximalTask() --> decoratee.close();   
- receive a shared instance of ColumnFamilyStore and eventually (e.g. SizeTieredCompactionStrategy) modify it; the initialization code assumes no other compaction strategy are associated with the column family; so, based on this assumption, the concrete strategy instance is free to modify the state of its context (e.g. column family store)     

All above makes difficult to implement a decorator pattern because the decoratee instance will modify the same column family store of the decorator (for example the SizeTieredCompactionStrategy modifies the min and max threshod which are supposed to be 0 in case of compaction disabled).    

For example, the ColumnFamilyStore has a method isCompactionDisabled that basically checks if min or max threshols are set to 0. Now, when instantiating the NoCompactionStrategy, one of the first thing that shoujld be done is 

store.setCompactionThresholds(0,0);

after that, the wrapped instance should be initialized. Suppose for example that the decoratee instance is a SizeTieredCompactionStrategy; creating a new instance will immediately reset the previous thresholds to 4 and 32 because the STCS has this code in its constructor:

cfs.setCompactionThresholds(cfs.metadata.getMinCompactionThreshold(), cfs.metadata.getMaxCompactionThreshold());

Nothing, just to say that a simple NullObject is not enough, I'm trying to solve that; once did, I'll submit a patch.  ","17/Feb/13 19:14;slebresne;To be honest, I'm not completely convinced about that NoCompactionStrategy idea. I agree with the ticket premises than using min/max thresholds is adhoc, especially since those threshold have no other meaning for leveled compaction.  However, as Andrea says, we only want to disable automatic (background) compaction, not the user triggered ones, so replacing the strategy entirely feels overkill to me. Overall, a simple boolean in ColumnFamilyStore would seem much simpler to me. And since we already have the disableAutoCompaction call in JMX (for some reason it doesn't seem the enableAutoCompaction is expose in JMX so we should add it), I'd suggest using that as the sole way to enable/disable auto compactions.
","18/Feb/13 08:48;a.gazzarini;Right, I agree...the simplest thing that could possibly work; but in my opinion there's still something that needs to be changed: setting the min and / or max threshold, strictly speaking doesn't disable the compaction, because is up to the concrete strategy implementor to check that doing 

if (cfs.isCompactionDisabled()) {
...
}

I think it should be better to move this responsibility to the superlayer. 
So my suggestion is to change a little bit the AbstractCompactionStrategy in order to use a template method. Something like this:

(AbstractCompactionStrategy)

{noformat}
final synchronized AbstractCompactionTask getNextBackgroundTask(final int gcBefore)
{
    if (!cfs.isCompactionDisabled()) { 
       doGetNextBackgroundTask(gcBefore);
    }
}
...

abstract AbstractCompactionTask doGetNextBackgroundTask(final int gcBefore);
{noformat}
","18/Feb/13 17:27;slebresne;In case that wasn't clear, I didn't said there is nothing to change. On the contrary, we should *stop* relying on setting the min/max compaction threshold to disable compaction (I'd be in favor of always considering that setting them to 0 is disallowed, but we'd have to leave that to 2.0 to not break it in a minor release). So instead I would add a per-cfs 'isAutoCompactionEnable' boolean. As for the call to isCompactionDisabled, it should indeed be moved, but I would move it to CompactionManager directly (where I would put 2 checks, one prior to submitting to the compaction executor, to avoid submitting useless task in the first place, and then again before the call to getNextBackgroundTask, just in case the runnable has been sitting for too long on the executor and has missed the first check).","21/Feb/13 12:08;a.gazzarini;Probably that wasn't clear, but honestly I believe it's a problem of mine: I have just a little bit of experience on Cassandra source code and its internal architecture. 

Ok, now I think I got the point about your idea, so I will continue investigating the code in order to implement that. 

A question: in case the logic is moved on the CompactionMananger, what should be, from client point of view, a nice and good way to indicate that the (background) compaction must be disabled? I believe the command line of my previous post is no longer appropriate

> create column family XYZ with compaction_strategy=NoCompactionStrategy 

   ","05/Apr/13 13:49;krummas;* adds nodetool commands (disableautocompaction, enableautocompaction)
* makes it possible to set via schema, ""update cf x with auto_compaction_disabled=true""
* should be backwards compactible, if someone has disabled compactions with max_compaction_threshold, it will disable compactions using the new way (maybe should output a warning or similar though)","08/Apr/13 09:59;slebresne;The general idea lgtm. A few remarks though:
* There is no handling of CQL3 (which is in the package org.apache.cassandra.cql3). I'll not that for CQL3, this setting should probably be a 'compaction' option, not at top-level one. Thruth being told, maybe it would be cleaner to have that be a compaction option in the code too (handled by AbstractCompactionStrategy)?
* Currently, whether compaction is disabled or not is checked by the compaction strategy themselves. In particular, SizeTiered directly check the min and max thresholds (rather than calling isCompactionDisabled) in getNextBackgroundSSTables, so that needs to be fixed. Furthermore, those checks are redundant with the newly added checks in CompactionManager. And since there's the new 'isActive' flag that also mean ""don't create a compaction task"", maybe it would be simpler to create an AbstractCompactionStrategy.isEnabled() method that would return 'isActive && !isAutoCompactionDisabled()' and use that exclusively.
* We should probably now refuse setting the min/max thresholds to 0 everywhere (and add a mention in the NEWS file).
* In nodeCmd, the convention for methods that take a keyspace and column family argument is that if they are not given, the method applies to all keyspace/CF. Could be handy here too.
* Nit: For the enableAutoCompaction with an argument, we can add a @VisibleForTesting annotation.
","09/Apr/13 10:25;krummas;+1 on all the comments

now it is a compaction_strategy_option in cli, and a compaction = {..} option in CQL.

i also removed a call to cfs.setCompactionThresholds in the STCS constructor, couldn't find a reason it was there.

and, if there is a better way of disallowing min/max thresholds = 0 than checking in cql, cql3 and cli that does not break existing schemas, let me know","09/Apr/13 14:45;slebresne;bq. i also removed a call to cfs.setCompactionThresholds in the STCS constructor, couldn't find a reason it was there

Back in the days (like 1.0 old, when dinosaurs where roaming the earth), we used to set the min/max thresholds to 0/Integer.MAX_VALUE in LCS. So STCS needed to restore the settings to sane levels in case we were switching from LCS to STCS. But since then we've fixed our ways (in CASSANDRA-4233 apparently, thanks git pickaxe) so this is just some leftover. It's fine removing it and you can even remove the comment the patch adds imo.

bq. if there is a better way of disallowing min/max thresholds = 0 than checking in cql, cql3 and cli

Not really.

Otherwise, I've just realized that following CASSANDRA-3430, we need to check the ACS.isActive flag *within* getNextBackgroundTask (with the synchronized block in particular) otherwise it could be racy. So I think we should move back the isAutoCompactionDisabled check from CompactionManager to STCS/LCS.getNextBackgroundTask (it's fine to keep the check at the beginning of CompactionManager.submitBackground however, no point is pushing tasks that will do nothing on the executor). My bad for suggesting otherwise.

But with the above fixed, +1.

Nit: could be nice to preserve the comment in CFS.disableAutoCompaction(), it's still useful.
","09/Apr/13 16:38;krummas;thanks, fixed the comments and pushed as ebefb77c6e8a5046a8c1b1bb0edd258aaf0ad8b7","02/Dec/13 17:56;vongocminh;Hello,

Could you please confirm that KS and CF are ""optional"" parameter in nodetool disableautocompaction?

We are running C* v2.0.3 and the command nodetool does not seem working. We disable the auto compaction on all nodes via nodetool and the servers continue to compact CFs.
{quote}
INFO 18:45:03,594 Compacted 4 sstables to [D:\AtlasData\titan\pdl_identity\titan-pdl_identity.pdl_identity_portfolio_idx-jb-449,].  183á596 bytes to 183á289 (~99% of original) in 546ms = 0,320143MB
{quote}
Thanks for your help.
Best regards,
Minh","02/Dec/13 18:46;krummas;yes, without ks/cf it disables for all column families

note that it resets on restart though, if you want to disable it forever, you need to set the compaction strategy option ""enabled"" to false

if it does not disable all compactions, it is a bug

","02/Dec/13 19:15;vongocminh;I'm kind of lost here: with nodetool in C* v2.0.1, *disableautocompaction* does no effect (servers continue compressing SSTables); with v2.0.3, the ""feature"" is no longer recorgnized by nodetool:
{quote}
C:\Workspace\apache-cassandra-2.0.3\bin>nodetool -h parw00146880 disableautocompaction
Starting NodeTool
Unrecognized command: disableautocompaction
usage: java org.apache.cassandra.tools.NodeCmd --host <arg> <command>
...
{quote}","02/Dec/13 19:33;krummas;uh that is weird, just downloaded the 2.0.3 tarball and disableautocompaction exists

btw, nodetool disableautocompaction disables for all *existing* column families, so if you disableautocompaction and then start stress (for example), it will not be disabled for the cfs it creates","02/Dec/13 22:27;vongocminh;Thanks a lot for your quick answer.
It is indeed very weird. I downloaded the binary for Windows from this address last week:
http://archive.apache.org/dist/cassandra/2.0.3/

I will recheck it tomorrow ...
(edit)
The cqlsh shows this:
{quote}
[cqlsh 4.1.0 | Cassandra 2.0.3 | CQL spec 3.1.1 | Thrift protocol 19.38.0]
{quote}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timeout reporter writes hints for the local node,CASSANDRA-4753,12610089,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,iamaleksey,iamaleksey,03/Oct/12 15:47,12/Mar/19 14:01,13/Mar/19 22:27,19/Dec/12 20:56,1.2.1,,,,,0,,,,,,,"MessagingService.java:330 calls StorageProxy.scheduleLocalHint() without checking if the local node is the target. This causes StorageProxy.scheduleLocalHint to throw AssertionError sometimes.

Got this error when some batches are timed out. This can happen because even local batches now go through StorageProxy.sendMessages and aren't just rm.apply()'d.",,,,,,,,,,,,,,,,,,,19/Nov/12 09:00;jbellis;4753-v2.txt;https://issues.apache.org/jira/secure/attachment/12554140/4753-v2.txt,19/Nov/12 11:03;iamaleksey;4753-v3.txt;https://issues.apache.org/jira/secure/attachment/12554154/4753-v3.txt,19/Nov/12 03:15;iamaleksey;4753.txt;https://issues.apache.org/jira/secure/attachment/12554108/4753.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-10-03 15:54:38.424,,,no_permission,,,,,,,,,,,,250367,,,Wed Dec 19 20:56:54 UTC 2012,,,,,,0|i0axxb:,61769,iamaleksey,iamaleksey,,,,,,,,,,"03/Oct/12 15:51;iamaleksey;See https://issues.apache.org/jira/browse/CASSANDRA-4542?focusedCommentId=13451479&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13451479
and https://issues.apache.org/jira/browse/CASSANDRA-4542?focusedCommentId=13451486&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13451486

",03/Oct/12 15:54;iamaleksey;Special-casing for local inserts might be worth it now that abm is the default and we went even futher with CASSANDRA-4738.,03/Oct/12 15:54;jbellis;Sounds like the right solution is to add a local-only apply the way we do for normal Mutations.,"19/Nov/12 03:17;iamaleksey;That particular error has been fixed some time ago (same issue that was causing batchlog timeouts in beta1). So there is really no need to special-case timeout-reporter for local node - such cases just should not happen.

However, I added a check for local node to StorageProxy.shouldHint so that no issue like this one pops in the future.","19/Nov/12 09:00;jbellis;It's valid for local writes to timeout, but we don't want to just drop them on the floor if they do.  v2 attached.","19/Nov/12 10:48;iamaleksey;You are right. There is still the option of special-casing local endpoint for syncWriteToBatchlog and asyncRemoveFromBatchlog. I know single-node clusters are not a pirority, but still. That would also make this issue go away.
",19/Nov/12 11:03;iamaleksey;Something like v3.,"19/Nov/12 15:19;jbellis;Hmm.  We actually have a couple problems here.

# We do allow local writes to be dropped, but
# Nobody writes hints for dropped local writes
# If local hints were attempted, they would error out (what v2 was trying to fix)

We actually do want to hint batchlog inserts to maintain the contract of ""timed out means it's in progress, you do not have to retry.""

I'm not sure what the best way to add hinting (actually, retry-on-hint-stage) code for insertLocal is though.  One option would be to just make local writes non-droppable, but then we lose our backpressure mechanism of a full hint stage causing OverloadedException, and a poorly behaving client could OOM the coordinator with a lot of local writes.","20/Nov/12 21:23;jbellis;Here is one solution: http://github.com/jbellis/cassandra/branches/4753-5

- Created LocalMutationRunnable that retries with hint backpressure if it gets dropped
- Added an updated v3, with an additional assert to make sure we don't introduce a bug if we decide to allow using the coordinator as a batchlog member in non-single-node clusters",19/Dec/12 20:45;iamaleksey;+1,19/Dec/12 20:56;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: ASSUME functionality broken by CASSANDRA-4198 fix,CASSANDRA-4352,12594967,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,18/Jun/12 17:10,12/Mar/19 14:01,13/Mar/19 22:27,18/Jun/12 18:11,1.1.2,,Legacy/Tools,,,0,cqlsh,,,,,,"All uses of the {{ASSUME}} command in cqlsh now appear to be wholly ineffective at affecting subsequent value output.

This is due to a change in the grammar definition introduced by the fix for CASSANDRA-4198, upon which definition the ASSUME functionality relied.

All that's needed to fix is to update the token-binding names used.",,,,,,,,,,,,,,,,,,,18/Jun/12 17:12;thepaul;4352.patch.txt;https://issues.apache.org/jira/secure/attachment/12532427/4352.patch.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-18 18:11:58.701,,,no_permission,,,,,,,,,,,,256086,,,Mon Jun 18 18:11:58 UTC 2012,,,,,,0|i0guwn:,96437,brandon.williams,brandon.williams,,,,,,,,,,"18/Jun/12 17:12;thepaul;Fix attached, or also available in the 4352 patch in my github:

https://github.com/thepaul/cassandra/tree/4352

Current revision of patch is tagged as pending/4352.",18/Jun/12 18:11;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
http link is broken in cassandra-env.sh,CASSANDRA-4106,12549178,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,chipitsine,chipitsine,02/Apr/12 16:21,12/Mar/19 14:01,13/Mar/19 22:27,02/Apr/12 16:23,,,Packaging,,,0,documentation,,,,,,"# jmx: metrics and administration interface
#
# add this if you're having trouble connecting:
# JVM_OPTS=""$JVM_OPTS -Djava.rmi.server.hostname=<public name>""
#
# see
# http://blogs.sun.com/jmxetc/entry/troubleshooting_connection_problems_in_jconsole
# for more on configuring JMX through firewalls, etc. (Short version:
# get it working with no firewall first.)


link to https://blogs.sun.com leads to 404",any,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,234169,,,Mon Apr 02 16:23:52 UTC 2012,,,,,,0|i0gs1z:,95975,,,,,,,,,,,,02/Apr/12 16:23;chipitsine;please change link with https://blogs.oracle.com/jmxetc/entry/troubleshooting_connection_problems_in_jconsole,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MalformedObjectNameException: Invalid character ':' in value part of property in StreamingMetrics for IPv6,CASSANDRA-5328,12636178,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,michalm,michalm,michalm,09/Mar/13 08:36,12/Mar/19 14:01,13/Mar/19 22:27,11/Mar/13 14:17,,,,,,0,,,,,,,"See CASSANDRA-5298 for the details - it's the same case, but in different Metrics' class.
Attaching patch.",,,,,,,,,,,,,,,,,,,09/Mar/13 08:37;michalm;5328.patch;https://issues.apache.org/jira/secure/attachment/12572906/5328.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-11 14:17:48.284,,,no_permission,,,,,,,,,,,,316670,,,Mon Mar 11 14:17:48 UTC 2013,,,,,,0|i1in5r:,317012,,,,,,,,,,,,11/Mar/13 14:17;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dsnitch severity is not correctly set for compaction info,CASSANDRA-5255,12632418,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,brandon.williams,brandon.williams,14/Feb/13 17:30,12/Mar/19 14:01,13/Mar/19 22:27,02/Mar/13 02:45,1.2.3,,,,,0,,,,,,,"We're doing two things wrong in CI.  First, load can change between calls, which can cause a negative severity even though it meant to subtract whatever it added before.  Second, we should report based on how much IO we're using, since a 1T throttled to 5MB/s is less impactful than a 100MB running at full speed.",,,,,,,,,,,,,,,,,,,18/Feb/13 06:47;vijay2win@yahoo.com;0001-CASSANDRA-5255.patch;https://issues.apache.org/jira/secure/attachment/12569774/0001-CASSANDRA-5255.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-15 19:35:49.146,,,no_permission,,,,,,,,,,,,312914,,,Sat Mar 02 02:45:57 UTC 2013,,,,,,0|i1i007:,313260,jbellis,jbellis,,,,,,,,,,15/Feb/13 19:35;jbellis;Another improvement (maybe for another ticket) would be to incorporate streaming MBps into severity as well.,"18/Feb/13 06:44;vijay2win@yahoo.com;Attached patch fixes the negative severity.

{quote}
 we should report based on how much IO we're using, since a 1T throttled to 5MB/s is less impactful than a 100MB running at full speed
{quote}

Well I am not sure about that, 100MB without throttle may be on SSD's and 1TB may be on Spindles.... So looking at the throttle might not help and we should assume the user did the right thing on throttle.

Another possible option: Create a way to get IOStat data into the JVM and measure the IOWait time but the problem is how do we support MS Windows? not sure if there is a generic way to do so.

I would suggest we do both Streaming and IOWait monitor in another ticket, Makes sense?","18/Feb/13 15:18;jbellis;bq. Another possible option: Create a way to get IOStat data into the JVM and measure the IOWait time but the problem is how do we support MS Windows? not sure if there is a generic way to do so.

I like that idea a lot better than trying to special-case streaming and compaction.

No idea how to get that info on Windows though.  Guess we could fall back to MBPs compaction/streaming there.  (Yes, this is not perfect if you mix SSDs + HDDs, but it's a LOT better than total MB of compaction.)",01/Mar/13 15:05;jbellis;+1 on the fix for negative severity.  Please open followup tickets per above discussion.,"02/Mar/13 02:45;vijay2win@yahoo.com;Committed to 1.2 and trunk, opened CASSANDRA-5306 as a followup. Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Directories.migrateFile() does not handle -old or -tmp LDB manifests,CASSANDRA-5242,12631802,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,amorton,amorton,11/Feb/13 19:40,12/Mar/19 14:01,13/Mar/19 22:27,02/Apr/13 18:08,1.1.11,,,,,0,,,,,,,"During LDB compaction a -old.json file is created with the previous manifest.

Directories.migrateFile() only checks for the .json extension and uses the length to determine the CF name. 

This can result in the -old.json manifest getting copied to a CF-old directory in the new layout. 

see http://www.mail-archive.com/user@cassandra.apache.org/msg27583.html for an example ",,,,,,,,,,,,,,,,,,,24/Mar/13 13:58;krummas;0001-CASSANDRA-5242-v2.patch;https://issues.apache.org/jira/secure/attachment/12575229/0001-CASSANDRA-5242-v2.patch,24/Mar/13 07:43;krummas;0001-CASSANDRA-5242.patch;https://issues.apache.org/jira/secure/attachment/12575211/0001-CASSANDRA-5242.patch,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-23 22:07:37.392,,,no_permission,,,,,,,,,,,,312298,,,Tue Apr 02 18:08:51 UTC 2013,,,,,,0|i1hw73:,312644,iamaleksey,iamaleksey,,,,,,,,,,23/Mar/13 22:07;jbellis;Should probably fix even though it's obsolete in 2.0.,24/Mar/13 07:43;krummas;patch against 1.1,"24/Mar/13 13:58;krummas;didnt read title properly it seems, this handles -tmp files as well","24/Mar/13 14:21;jbellis;Can you review, [~amorton]?",02/Apr/13 18:08;iamaleksey;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport on-startup manifest repair to 1.2,CASSANDRA-5327,12636099,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,08/Mar/13 19:00,12/Mar/19 14:01,13/Mar/19 22:27,08/Mar/13 23:43,1.2.3,,,,,0,compaction,,,,,,Initially added to trunk for CASSANDRA-4872,,,,,,,,,,,,,,,,,,,08/Mar/13 19:00;jbellis;5327.txt;https://issues.apache.org/jira/secure/attachment/12572798/5327.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-08 19:42:53.887,,,no_permission,,,,,,,,,,,,316591,,,Fri Mar 08 23:43:43 UTC 2013,,,,,,0|i1imo7:,316933,krummas,krummas,,,,,,,,,,08/Mar/13 19:42;krummas;lgtm,08/Mar/13 23:43;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove SimpleTransportFactory,CASSANDRA-5162,12627715,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jasobrown,jasobrown,15/Jan/13 19:03,12/Mar/19 14:01,13/Mar/19 22:27,15/Jan/13 19:06,2.0 beta 1,,,,15/Jan/13 00:00,0,,,,,,,"We don't support non-framed thrift transport, so remove as o.a.c.cli.transport. SimpleTransportFactory it's an unneeded class",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-01-15 19:06:13.704,,,no_permission,,,,,,,,,,,,304502,,,Tue Jan 15 19:06:13 UTC 2013,,,,,,0|i17mxz:,252686,,,,,,,,,,,,15/Jan/13 19:05;jasobrown;Jonathan will remove it - no patch necessary :),15/Jan/13 19:06;jbellis;done in 83c014777a8d4597ca9b6cfc0f3dd9c77bd39d50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option to disable tcp_nodelay,CASSANDRA-5148,12627147,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,krummas,krummas,11/Jan/13 09:34,12/Mar/19 14:01,13/Mar/19 22:27,11/Jan/13 22:16,1.2.1,,,,,0,,,,,,,"Add option to disable TCP_NODELAY for cross-dc communication.

Reason is we are seeing huge amounts of packets being sent over our poor firewalls.

For us, disabling this for inter-dc communication increases average packet size from ~400 bytes to ~1300 bytes.",,,,,,,,,,,,,,,,,,,11/Jan/13 09:35;krummas;0001-Add-option-to-disable-TCP_NODELAY-for-inter-dc-commu.patch;https://issues.apache.org/jira/secure/attachment/12564384/0001-Add-option-to-disable-TCP_NODELAY-for-inter-dc-commu.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-11 22:16:14.808,,,no_permission,,,,,,,,,,,,303916,,,Fri Jan 11 22:16:14 UTC 2013,,,,,,0|i17gv3:,251701,jbellis,jbellis,,,,,,,,,,11/Jan/13 09:35;krummas;patch for this against trunk,"11/Jan/13 22:16;jbellis;My initial reaction was that I'd prefer to manage the buffering explicitly instead of relying on nagle to get it right, but on further thought, I don't think it matters much: if the connection is busy, nagle will send out packets as soon as max segment size is reached; if it is not, then worst case 200-500ms delay from delayed ack is still tolerable if we are not treating cross-dc requests synchronously.

Committed defaulting to true for 1.2.1 and to false for 2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make sure SSTables left over from compaction get deleted and logged,CASSANDRA-5137,12626804,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,09/Jan/13 17:31,12/Mar/19 14:01,13/Mar/19 22:27,11/Jan/13 19:18,1.1.9,1.2.1,,,,0,,,,,,,"When opening ColumnFamily, cassandra checks SSTable files' ancestors and skips loading already compacted ones. Those files are expected to be deleted, but currently that never happens.
Also, there is no indication of skipping loading file in the log, so it is confusing especially doing upgradesstables.",,,,,,,,,,,,,,CASSANDRA-5116,,,,,10/Jan/13 21:49;yukim;5137-1.1-v2.txt;https://issues.apache.org/jira/secure/attachment/12564275/5137-1.1-v2.txt,11/Jan/13 17:38;yukim;5137-1.1-v3.txt;https://issues.apache.org/jira/secure/attachment/12564451/5137-1.1-v3.txt,09/Jan/13 17:32;yukim;5137-1.1.txt;https://issues.apache.org/jira/secure/attachment/12563965/5137-1.1.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-01-09 17:58:34.292,,,no_permission,,,,,,,,,,,,303422,,,Fri Jan 11 19:18:16 UTC 2013,,,,,,0|i17b2v:,250763,,,,,,,,,,,,09/Jan/13 17:32;yukim;We need to mark skipping SSTable as compacted to be removed.,"09/Jan/13 17:58;jbellis;Hmm.

This patch is correct as far as it goes but I think the existing assumption is broken: that if we have any sstable with ancestor X, then X is safe to delete.

Specifically, LCS will create multiple sstables from a given set of ancestors, so unless we know that we finished the compaction (and finished writing all the resulting descendant sstables), we could lose data if we delete the ancestors themselves.

One possible fix:

# Add a flag to SSTM for ""this was the final sstable in the compaction""
# When we scan sstables, we can delete ancestors if we find that marker in any of the descendants
# Otherwise, we should delete the *descendants* and leave the ancestors alone (so we don't doublecount data for counters)","09/Jan/13 18:13;slebresne;I think you're right.

A fourth pseudo-solution could be to wait the end of the compaction to rename all the newly created writers (i.e remove the tmp markers). It's not bulletproof though as I don't think we can rename multiple files atomically but just wanted to mention it.

Maybe at least for 1.1, 3. is the best/simplest option. On the longer term, maybe 1. is better.","09/Jan/13 18:33;jbellis;You're right, you don't actually need a marker since if compaction completes the next step is to remove the ancestors.  I think ""if ancestors are still alive, assume compaction didn't finish and delete the descendants"" is good enough.","09/Jan/13 18:56;slebresne;bq. I think ""if ancestors are still alive, assume compaction didn't finish and delete the descendants"" is good enough.

yeah agreed.","09/Jan/13 19:01;slebresne;Hum wait, it only works if we have all ancestors though. What if just one ancestor don't get deleted for some reason (or only some of the SSTableDeletingTask have executed before a crash)? It's easy enough to check that we have *all* ancestors, but what if we don't? We're back to square one :(","09/Jan/13 20:13;jbellis;You're right.  Guess we need a compaction-finished flag after all.

Instead of storing it in sstable metadata, maybe we could store it in system.local the way we do with truncation information.  Unfortunately 1.1 doesn't support Maps so we'd be doing two separate implementations for 1.1 and 1.2.

Should we just say that for 1.1 we'll retain all sstables (counter users will get overcounts, everyone else just gets extra compaction work) and fix it better in 1.2?","09/Jan/13 20:16;jbellis;Something like this...

{code}
create table compaction_log (
  id uuid primary key,
  inputs set<int>,
  outputs set<int>
);
{code}

When we start a compaction, we add it to the log.  When we finish, we remove it.  If we restart and compaction_log is not empty, we remove any sstables from the outputs set.","10/Jan/13 08:57;slebresne;bq. Should we just say that for 1.1 we'll retain all sstables

For 1.1, I'd suggest doing my fourth pseudo-solution above, i.e. moving the renaming of newly created writes at the end of the compaction (it's trivial). Then at startup, we could indeed retain all sstables for normal CF, but for counter we would keep removing the predecessors as we do now. It wouldn't totally fix the risk of losing counters, but it would make it very unlikely (you'd need to fail exactly in the middle of the bulk renaming a newly create sstable writers), while just retaining all sstables would make it very easy to have overcounts.

For 1.2, you compaction_log solution does seem reasonable.","10/Jan/13 21:49;yukim;V2 implements Sylvain's idea that renames written SSTables at the end of compaction.

For 1.2, let's open different issue for Jonathan's suggestion.","11/Jan/13 15:17;slebresne;The code of v2 looks alright, but let's also disable the filtering in ColumnFamilyStore.ctor for non-counter CFs so we take zero chance of losing data (and since reusing an already compacted sstable is a bit inefficient but harmless).

bq. For 1.2, let's open different issue for Jonathan's suggestion

Agreed.",11/Jan/13 17:38;yukim;Attached v3 that also changes the filtering part only for counter CF.,"11/Jan/13 17:47;slebresne;+1 (though do commit your v1 along the way, no way in keeping sstable we're not going to use, even if it's only for counters).","11/Jan/13 19:18;yukim;Committed v1 + v3, and opened CASSANDRA-5151 for better solution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop keyspace argument from forceUserDefinedCompactions,CASSANDRA-5139,12626858,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,jbellis,jbellis,09/Jan/13 21:52,12/Mar/19 14:01,13/Mar/19 22:27,14/Feb/13 03:28,2.0 beta 1,,,,,0,jmx,,,,,,Redundant now that keyspace is encoded in filename.,,,,,,,,,,,,,,,,,,,06/Feb/13 05:24;yukim;5139-2.0.txt;https://issues.apache.org/jira/secure/attachment/12568166/5139-2.0.txt,15/Jan/13 18:48;yukim;5139.txt;https://issues.apache.org/jira/secure/attachment/12564970/5139.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-15 18:48:59.739,,,no_permission,,,,,,,,,,,,303548,,,Thu Feb 14 03:28:48 UTC 2013,,,,,,0|i17c9z:,250957,jbellis,jbellis,,,,,,,,,,"15/Jan/13 18:48;yukim;Patch attached for new user defined compaction method. New method's behavior is slightly different from existing one, in the way that new method accepts mixed ks/cf files.
I'm not sure if it is OK to drop current API so I just deprecate it and create new method with one arg. On the other hand, I feel having two different methods with different args is confusing though.",15/Jan/13 18:52;jbellis;I guess we should keep the API stable for 1.2.  Retargeting for 2.0 so we can rip out the old one.  I think nodetool will need an update?,"15/Jan/13 18:57;yukim;OK. I will update the patch for 2.0.
And I don't think nodetool has command for user defined compaction.",06/Feb/13 05:24;yukim;Attaching patch for 2.0.,12/Feb/13 22:24;jbellis;+1,"14/Feb/13 03:28;yukim;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Composites index bug,CASSANDRA-4884,12614187,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,31/Oct/12 14:50,12/Mar/19 14:01,13/Mar/19 22:27,31/Oct/12 15:41,1.2.0 beta 2,,,,,0,,,,,,,Indexes on composite tables is currently broken (due to CASSANDRA-2897). Attaching patch to fix.,,,,,,,,,,,,,,,,,,,31/Oct/12 14:52;slebresne;4884.txt;https://issues.apache.org/jira/secure/attachment/12551552/4884.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-31 15:35:49.891,,,no_permission,,,,,,,,,,,,253379,,,Wed Oct 31 15:41:49 UTC 2012,,,,,,0|i0dkrr:,77294,jbellis,jbellis,,,,,,,,,,31/Oct/12 15:35;jbellis;+1,"31/Oct/12 15:41;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stress for cql3 is broken on 1.2/trunk,CASSANDRA-4979,12617116,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Nov/12 15:23,12/Mar/19 14:01,13/Mar/19 22:27,21/Nov/12 16:35,1.2.0 beta 3,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,21/Nov/12 15:24;slebresne;4979.txt;https://issues.apache.org/jira/secure/attachment/12554508/4979.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-21 15:33:01.334,,,no_permission,,,,,,,,,,,,259316,,,Wed Nov 21 16:35:35 UTC 2012,,,,,,0|i0lobb:,124589,jbellis,jbellis,,,,,,,,,,"21/Nov/12 15:33;jbellis;It's actually broken pretty deeply...  it gives explicit names to each cell for instance, when the more natural way to interpret ""make me a row with 100,000 cells"" is to make a wide row for slicing.

OTOH I could understand wanting to test explicit name queries on 5-cell rows.  Not sure how to generalize both, I think we might need more options. :(","21/Nov/12 15:40;slebresne;Yes, cql3 support by stress is funky. It also only ever test compact storage, so it would be good to also allow testing compact storage. It would also make sense to allow testing the binary protocol, but the current code has thrift hardcoded in quite a few places. Lastly, the code is littered with code repetition. So imho, we should refactor the hell out of it.

But the code we have now doesn't even run with the -L3 option, and that's what this patch fixes.",21/Nov/12 16:20;jbellis;+1,"21/Nov/12 16:35;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Some cqlsh help topics don't work (select, create, insert and anything else that is a cql statement)",CASSANDRA-4811,12611952,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,16/Oct/12 02:37,12/Mar/19 14:01,13/Mar/19 22:27,25/Oct/12 10:51,1.1.7,1.2.0 beta 2,,,,0,cqlsh,,,,,,"cqlsh> help select
Improper help command.

Same will happen if you look up a help topic for any other cql statement.
38748b43d8de17375c7cc16e7a4969ca4c1a2aa1 broke it (#4198) 5 months ago.
",,,,,,,,,,,,,,,,,,,20/Oct/12 19:12;iamaleksey;CASSANDRA-4811.txt;https://issues.apache.org/jira/secure/attachment/12550149/CASSANDRA-4811.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-25 10:51:56.3,,,no_permission,,,,,,,,,,,,248887,,,Thu Oct 25 10:51:56 UTC 2012,,,,,,0|i0a29j:,56640,brandon.williams,brandon.williams,,,,,,,,,,"20/Oct/12 19:15;iamaleksey;There is a deep underlying issue with lexing/parsing, but I gave up trying to fix it. If something like this ever happens again, I'll make changes, otherwise it's not worth it.

The patch is simple, fixes the issue's symptoms perfectly and should be enough.",25/Oct/12 10:51;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"binary protocol: when an invalid event type is watched via a REGISTER message, the response message does not have an associated stream id",CASSANDRA-4719,12609115,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,25/Sep/12 20:25,12/Mar/19 14:01,13/Mar/19 22:27,08/Nov/12 11:19,1.2.0,,Legacy/CQL,,,0,binary_protocol,,,,,,"I tried sending a REGISTER message with an eventlist including the string ""STATUS_FOO"", in order to test error handling in the python driver for that eventuality. But the response from the server (a ""Server error"" with a message of ""java.lang.IllegalArgumentException: No enum const class org.apache.cassandra.transport.Event$Type.STATUS_FOO"") had a stream_id of 0, so the driver was not able to associate it with the request.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-11-08 11:19:30.338,,,no_permission,,,,,,,,,,,,239452,,,Thu Nov 08 11:19:30 UTC 2012,,,,,,0|i00he7:,751,,,,,,,,,,,,08/Nov/12 11:19;slebresne;I've fixed that some times ago in a ninja-commit so closing this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress percentile label does not match what is returned,CASSANDRA-5288,12634010,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,cburroughs,cburroughs,25/Feb/13 22:33,12/Mar/19 14:01,13/Mar/19 22:27,28/Aug/13 03:46,1.2.10,2.0.1,Legacy/Tools,,,0,,,,,,,"We say it's the 99th: https://github.com/apache/cassandra/blob/trunk/tools/stress/src/org/apache/cassandra/stress/StressAction.java#L65

But return 99.9th https://github.com/apache/cassandra/blob/trunk/tools/stress/src/org/apache/cassandra/stress/StressAction.java#L145

Not sure which is intended.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-27 20:11:13.446,,,no_permission,,,,,,,,,,,,314504,,,Wed Aug 28 03:46:04 UTC 2013,,,,,,0|i1i9t3:,314848,,,,,,,,,,,,26/Aug/13 13:29;cburroughs;[~Ryan McGuire] I think you might have originally added these in CASSANDRA-5243.  Could you comment on if 99 or 99.9 was intended?  I'd also like to fix the header to it uses all commas for easier parsing while we are at it.,"27/Aug/13 20:11;enigmacurry;As far as I know, it's actually reporting 99.9th percentile, so I agree, the heading is mislabled.","27/Aug/13 20:14;enigmacurry;As in, we should change the heading, not the statistic. Too many things rely on that number the way it is.",28/Aug/13 03:46;jbellis;Ninja'd.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Old-style mapred interface only populates row key for first column when using wide rows,CASSANDRA-4834,12612496,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bkempe,bkempe,bkempe,18/Oct/12 21:37,12/Mar/19 14:01,13/Mar/19 22:27,06/Nov/12 12:25,1.1.7,,,,,0,,,,,,,"When using the ColumnFamilyRecordReader with the old-style Hadoop interface to iterate over wide row columns, the row key is only populated on the first column.
See attached tests.

",,,,,,,,,,,,,,,,,,,18/Oct/12 21:38;bkempe;TestJob.java;https://issues.apache.org/jira/secure/attachment/12549758/TestJob.java,18/Oct/12 21:38;bkempe;TestJobOldHadoop.java;https://issues.apache.org/jira/secure/attachment/12549759/TestJobOldHadoop.java,05/Nov/12 16:01;bkempe;cassandra-1.1-CASSANDRA-4834.txt;https://issues.apache.org/jira/secure/attachment/12552117/cassandra-1.1-CASSANDRA-4834.txt,25/Oct/12 23:50;bkempe;trunk-CASSANDRA-4834.txt;https://issues.apache.org/jira/secure/attachment/12550891/trunk-CASSANDRA-4834.txt,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-11-02 14:32:08.806,,,no_permission,,,,,,,,,,,,249675,,,Tue Nov 06 12:25:30 UTC 2012,,,,,,0|i0agbr:,58918,brandon.williams,brandon.williams,,,,,,,,,,"18/Oct/12 21:38;bkempe;TestJob is using the new mapred interface and produces the correct output.
TestJobOldHadoop does not populate the row key for columns after the first.",18/Oct/12 21:40;bkempe;patch,"25/Oct/12 23:50;bkempe;lastColumn also needs a duplicated ByteBuffer.
Otherwise, if the column name is consumed in the map/reduce code, the expression in
https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java#L458
will evaluate to false and the CFRR can get into an infinite loop.","02/Nov/12 14:32;brandon.williams;[~bkempe] can you rebase to 1.1?  I don't see any reason to not include this there.  Looks good otherwise, thanks!",05/Nov/12 16:01;bkempe;added patch for cassandra-1.1 branch,"06/Nov/12 12:25;brandon.williams;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"replication, compaction, compression? options are not validated",CASSANDRA-4795,12611434,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,brandon.williams,brandon.williams,11/Oct/12 21:21,12/Mar/19 14:01,13/Mar/19 22:27,05/Mar/13 11:05,1.2.1,,,,,0,,,,,,,"When creating a keyspace and specifying strategy options, you can pass any k/v pair you like.",,,,,,,,,,,,,,,,,,,12/Feb/13 12:02;slebresne;0001-Reallow-unexpected-strategy-options-for-thrift.txt;https://issues.apache.org/jira/secure/attachment/12568965/0001-Reallow-unexpected-strategy-options-for-thrift.txt,12/Feb/13 12:02;slebresne;0002-Reallow-unexpected-strategy-options-for-thrift.txt;https://issues.apache.org/jira/secure/attachment/12568966/0002-Reallow-unexpected-strategy-options-for-thrift.txt,12/Feb/13 12:02;slebresne;0003-Adds-application_metadata-field-to-ks-metadata.txt;https://issues.apache.org/jira/secure/attachment/12568967/0003-Adds-application_metadata-field-to-ks-metadata.txt,15/Jan/13 07:24;dbrosius;4795.compaction_strategy.txt;https://issues.apache.org/jira/secure/attachment/12564894/4795.compaction_strategy.txt,01/Dec/12 01:48;dbrosius;4795.replication_strategy.txt;https://issues.apache.org/jira/secure/attachment/12555602/4795.replication_strategy.txt,23/Jan/13 05:53;dbrosius;4795_compaction_strategy_v2.txt;https://issues.apache.org/jira/secure/attachment/12566094/4795_compaction_strategy_v2.txt,24/Jan/13 05:59;dbrosius;4795_compaction_strategy_v3.txt;https://issues.apache.org/jira/secure/attachment/12566255/4795_compaction_strategy_v3.txt,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2012-10-11 21:22:54.727,,,no_permission,,,,,,,,,,,,247798,,,Tue Mar 05 11:05:28 UTC 2013,,,,,,0|i08pxr:,48801,slebresne,slebresne,,,,,,,,,xedin,11/Oct/12 21:22;jbellis;We've been WARNing about bad options since 1.1.1 (CASSANDRA-4046) so I would think making it stricter would be fair game for 1.2.,11/Oct/12 22:46;brandon.williams;I didn't realize we were warning about it already.  It's probably better to leave it at that since we've allowed this for so long in case someone is relying on it.,"11/Oct/12 23:01;jbellis;I still want to fix it -- as we found out today, it's definitely confusing when you think an option belongs to replication strategy but it actually belongs to the keyspace -- but we can push it out past 1.2.","31/Oct/12 18:50;slebresne;I think that ticket was initially targeting the replication strategy, but there is also a lack of validation for the compaction strategies. In that case, we don't even validate that the value of a know option is correct (or rather, for some of the options we log a warning in the log but just carry on, and AbstractCompactionStrategy.tombstoneThreshold is not validated at all. I do not that in the case of the compaction strategy, we don't currently build the compaction strategy until it's actually needed (i.e. when migration are apply), so we shouldn't ""just"" throw an exception in the constructor: we need to do as we do for the replication strategy in CreateKeyspaceStatement, i.e. do a trial run of building the strategy to make sure everything's ok. 

I do think we should correctly validate options (and by that I mean doing more than a warning in the log) for both replication and compaction strategy for 1.2. At least we should do it for CQL3 if we've extremely afraid of breaking thrift (but I don't really buy that people would rely on our lack of validation).  ","12/Nov/12 05:02;jbellis;Can you take a stab at Sylvain's proposal, Dave?","01/Dec/12 01:47;dbrosius;oops, just saw this...

part1, replication strategy validation
4795.replication_strategy.txt

(more to come)","09/Jan/13 10:20;slebresne;+1 on part1, committed).

Dave, are you still planning on working on the compaction and compression validation? ","11/Jan/13 06:29;dbrosius;yeah, sorry. fell asleep at the wheel.","15/Jan/13 04:28;dbrosius;so, for compaction options, it appears to me that you can't create a CompactionStrategy object to do validation, because the ColumnFamilyStore needs to exist for the ACS constructor, which means it's too late to safe guard invalid options, right?

The validation would need to be in CFMetaData i think, and CFMetaData would validate options needed by fetching the valid set from a static call thru reflection on the compactionStrategyClass....

which seems ugly... so am i missing something easier?","15/Jan/13 04:59;jbellis;What if we required a static {{validate}} method on ACS subclasses that we call with the options and maybe CFMetaData?  Ugly, since there's no polymorphism on statics, but since we're doing ACS creation via reflection anyway it's not really a problem.

Edit: which is pretty much what you suggested with the difference being where the logic lives.  Either way WFM.","15/Jan/13 05:22;dbrosius;the 'right'?? solution perhaps is to remove the cfs from AbstractCompactionStrategy ctor, and instead pass it into the various methods that need it. Problem there is that you'd need a 'postCreateInitialize(ColumnFamilyStore cfs)' on ACS, as SizeTiered and Leveled do things with cfs in the ctor, that would need to move to that method.

It would also break backwards compatibility with potential client ACS's.","15/Jan/13 05:29;jbellis;That's what I thought at first but ACS is free to maintain state as well, e.g. LCS's manifest.  It would suck to turn those all into Map<CFS, X>.  Alternativley you could split off the construction into a StrategyFactory, which IMO is clunkier than the reflection idea.","15/Jan/13 05:34;dbrosius;ok, and just fail with ConfigurationException if that static method doesn't exist?",15/Jan/13 05:43;jbellis;I'd no-op it for backwards compatibility with user-provided ACS implementations.,"15/Jan/13 07:25;dbrosius;validate compaction options. in 4795.compaction_strategy.txt

pretty ugly... if you have suggestions, please do.","15/Jan/13 10:39;slebresne;On the compaction strategy patch:
* I would move the call to {{validateCompactionOptions}} in {{CFPropDefs.validate()}}.  First because ALTER also need the validation and putting it in CFPropDefs handles both ""for free"". And also because there is no guarantee that {{CFPropDefs.compactionStrategyClass}} won't be {{null}} which I think is not handled correctly by the patch (and moving things to {{CFPropDefs.validate()}} makes it easier to deal with).
* We should not only validate that there is no unknown option provided, but we should also move the validation of the options themselves from the ctors to the validateOptions method. Also, I'd prefer adding a validateOptions to AbstractCompactionStrategy to handle the tombstone related option and have subclasses call that method explicitly rather than handling everything in the sublcasses.
","23/Jan/13 03:19;dbrosius;not sure where CFPropDefs.validate gets used with cql3

ah, never mind. two CFPropDefs.","23/Jan/13 10:23;slebresne;Patch looks good but a few remaining remarks/nits:
* I think the comment in AbstractCompactionStrategy ctor is a bit confusing as we don't really fully repeat all the ""checks"" (we don't catch NumberFormatException nor validate the tombstoneThreshold value). So I agree we should probably repeat the checks, but let's repeat them fully.
* In SizeTiered validation, we might want to catch NumberFormatException.
* Since validateOptions is supposed to return a map of the options it don't know about, let's maybe check it's empty at the end of CFMetaData.validateCompactionOptions and get rid of the individual checks in Leveled and SizeTiered (which avoid needing VALID_OPTIONS).
* Might I suggest adding a few static getInt(map, property_name, defaut_value), getDouble, ... helper methods to simplify stuff like
{noformat}
try
{
    String optionValue = options.get(MIN_SSTABLE_SIZE_KEY);
    long minSSTableSize = optionValue == null ? DEFAULT_MIN_SSTABLE_SIZE : Long.parseLong(optionValue);
}
catch (NumberFormatException e)
{
    ...
}
{noformat}
* In cql3.CFPropDefs, we should move the validateCompactionOptions line inside the preceding 'if' statement.
* In cql3.CreateColumnFamilyStatement, let's remove the validate method override since it's not doing anything.

As a side note, there's a bit too much line changed not related to the patch to my taste. I understand this is your editor doing that automatically, but while reordering imports is ok, removing static imports to inline the class name in the code (in CFMetaData) is less justified imo.
","23/Jan/13 13:27;dbrosius;{quote}Since validateOptions is supposed to return a map of the options it don't know about, let's maybe check it's empty at the end of CFMetaData.validateCompactionOptions and get rid of the individual checks in Leveled and SizeTiered (which avoid needing VALID_OPTIONS).{quote}


If user supplied compaction strategies derive from these classes, that won't work.","23/Jan/13 13:40;slebresne;bq. If user supplied compaction strategies derive from these classes

Why? tTe contract of validateOptions would be that it should return a list with *only* the options it doesn't know about (which is kind of what it is supposed to do (and does) in your patch already anyway). Thus if a user supplied compaction derives from an existing one, it would have to either 1) return an empty map if he wants to ignore on purpose options or 2) call the super class validateOptions first (which is what Leveled and SizeTiered do with AbstractStrategy). Of course, in the case where the compaction strategy do not implement a validateOptions, we should skip the check for invalid options, but that's no different from what the patch does now.","23/Jan/13 15:19;dbrosius;yeah, your right. i shouldn't form opinions early in the morning.","24/Jan/13 17:05;slebresne;Alright, +1, committed (I did move the unrecognized option check inside CFMetadata so all interface gets it).","11/Feb/13 18:55;xedin;Ok, so changing this in 1.2.1 probably was a bit unfair to people, because e.g. Titan relies on providing custom strategy option 'titan-version' and it can't do custom strategy because there is no control of what version people install, can we please start warning people about unrecognized options in the strategy again instead of restricting them, because those options are sometimes very convenient to safe meta-information.","11/Feb/13 19:29;xedin;Also, All of the old users how were adding custom properties to replication strategy (at least) wouldn't be able to start-up after upgrade to 1.2.1 because validation is also done on Table initialization.

{noformat}
java.lang.RuntimeException: org.apache.cassandra.exceptions.ConfigurationException: Unrecognized strategy option {titan-version} passed to SimpleStrategy for keyspace titan
	at org.apache.cassandra.db.Table.<init>(Table.java:269)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.db.Table.open(Table.java:88)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:223)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:379)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:422)
Caused by: org.apache.cassandra.exceptions.ConfigurationException: Unrecognized strategy option {titan-version} passed to SimpleStrategy for keyspace titan
	at org.apache.cassandra.locator.AbstractReplicationStrategy.validateExpectedOptions(AbstractReplicationStrategy.java:281)
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:72)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:234)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:288)
	at org.apache.cassandra.db.Table.<init>(Table.java:265)
	... 5 more
{noformat}","12/Feb/13 07:26;slebresne;bq. changing this in 1.2.1 probably was a bit unfair to people

If you mean ""in a minor release"", then I agree, mea culpa.

But on the principle, I disagree that we should allow people to shove random metadata in the stategy options. Especially because that means not validating correctly the actual strategy options (and no, warning in the log is not a good enough because 1) application developer may not have easy access to the server log and 2) asking people to check the server log when they do query to see if they hadn't mispelled some option is a ridiculously bad user experience).

If application want to store data, they should create a column family and store data in there. But to be clear, I'm open to discussing adding some new application_metadata field to the column family metadata that would explicitely be uninterpreted by Cassandra (not that I'm particularly thrilled by the idea, but I'm at least open to it), but the strategy options is just not the right place for that.

So I'm fine reverting this from 1.2 (in which case I'd still prefer keeping the validation at least on the CQL3 side), not so much from trunk.
","12/Feb/13 07:42;xedin;bq. If you mean ""in a minor release"", then I agree, mea culpa.

That's exactly what I mean.

bq. 1) application developer may not have easy access to the server log 2) asking people to check the server log when they do query to see if they hadn't mispelled some option is a ridiculously bad user experience

If the original intention was to protect people from mistypes then maybe we need to put more work and throw exception only if typed in is similar to what it should be, like git does e.g. ""option rplicatian_factor is not recognized maybe you mean 'replication_factor'?"" ? Requiring those options to be typed in by the user especially with such a long names that we have already is a ""ridiculously bad user experience"", especially when those names/options change frequently between releases.

bq. If application want to store data, they should create a column family and store data in there. But to be clear, I'm open to discussing adding some new application_metadata field to the column family metadata that would explicitely be uninterpreted by Cassandra (not that I'm particularly thrilled by the idea, but I'm at least open to it), but the strategy options is just not the right place for that.

So in the case of Titan when they just want to save one 'last seen version' attribute, creating a separate column family or even having that per-cf doesn't make any sense but it's pretty convenient to keep it in the keypsace strategy options (without have any other alternatives) because it's not updated or even read that often.","12/Feb/13 08:00;slebresne;bq. If the original intention was to protect people from mistypes

How can that *not* be one's intent?

bq. like git does e.g. ""option rplicatian_factor is not recognized maybe you mean 'replication_factor'?""

Sure, that would be great: ""reject errors with fix suggestion in the error message"" is indeed even better than ""reject errors"" that is itself better than ""not reject errors silently"". So be my guest, do open a ticket to do even better than what this patch does. But that patch is still progress.

bq. it's pretty convenient to keep it in the keypsace strategy options (without have any other alternatives)

Again, I'm ok talking about alternatives that would be right if there is a need, but you will not convince me that ""shoving random application data inside the strategy options"" is right, or even a good idea.
","12/Feb/13 08:17;xedin;I don't think this is about right or wrong but rather about living by means instead of re-inventing broken bicycle. I'm not arguing or trying to convince, I'm simply saying that committed patch made situation even worse and it should be reconsidered all together. Also, if somebody ever had mistyped and didn't fix or used unrecognized attributes in replication strategy or compaction, after upgrade to 1.2.1 Cassandra just *wouldn't start up* which is also a ""ridiculously bad user experience"", this is why I think this patch should be reverted.

Edit: I think that concerns DSE as well, because AFAIK they used custom attributes in the compaction strategy as well as in keyspace replication.","12/Feb/13 12:02;slebresne;I'm a little bit confused about what you are trying to say. If you're saying we've broke stuff in a minor release, it's bad, we should revert the breakage, then I though I had made it pretty clear that I agree. But for 2.0 onwards, allowing to store random data in the replication strategy option *is* wrong: it doesn't even work with NTS (which will interpret the random data as a datacenter).

But anyway, less arguing more doing, I'm attaching patches that do the following:
* patch 00001 re-allow unknown options in the replication strategy options on the thrift side (but not for CQL3 as I doubt there is any legacy CQL3 app using this since CQL3 is kind of new). It also allows startup if there was unknown options in the first place (that part was definitevely an oversight, the compaction strategy is clearly making sure to only log a warning in that case for instance).
* patch 00002 does the same for the compaction strategy.
* patch 00003 is my suggestion for an alternative to the problem ""I want to attach some tiny piece of metadata to a keyspace and creating a CF for that is overkill"" (for that patch the thrift file must be regen but I don't attach that). Could totally go into a separate ticket.

Those patches are against 1.2 but to be clear, my intent is to keep disallowing unknown options even for thrift in trunk.","12/Feb/13 15:36;jbellis;bq. So in the case of Titan when they just want to save one 'last seen version' attribute, creating a separate column family or even having that per-cf doesn't make any sense but it's pretty convenient to keep it in the keypsace strategy options (without have any other alternatives) because it's not updated or even read that often.

No doubt everyone practicing this extremely awful hack has a perfectly good excuse. :)

But fundamentally creating a last_seen_versions table with cfname (ksname?) is simply not that onerous and is the Right Thing To Do.

bq. patch 00003 is my suggestion for an alternative to the problem ""I want to attach some tiny piece of metadata to a keyspace and creating a CF for that is overkill""

-1 from me, I get that change is painful for people who are abusing schema right now but we really shouldn't be encouraging this.","12/Feb/13 22:33;xedin;bq. But fundamentally creating a last_seen_versions table with cfname (ksname?) is simply not that onerous and is the Right Thing To Do.

For that specific case it's actually not worth it because it's always a single version as there is no reason to keep older version around, so creating separate CF for one row with empty value is overkill as specially as it's even not read that often and query wouldn't be as convenient as it was with describe keyspace for that single piece of data.

bq. -1 from me, I get that change is painful for people who are abusing schema right now but we really shouldn't be encouraging this.

It's usually one/two attributes that go there so I think it's pretty convenient to store it that way instead of dealing with separate CF/querying, but I'm welcome to new ideas",12/Feb/13 22:39;jbellis;Show me an existing database that lets you attach random crap to a table definition and I will reconsider my position. :),13/Feb/13 08:03;slebresne;Do we agree on the first 2 patches in the meantime?,"01/Mar/13 13:38;slebresne;So, does the absence of review of my 2 first patches mean that everyone decided the status quo was ok? Cause I could live with that. But if we think this should be reverted from 1.2 then let's do that sooner than later.","01/Mar/13 19:24;xedin;bq. Show me an existing database that lets you attach random crap to a table definition and I will reconsider my position. 

What is HBase?

bq. So, does the absence of review of my 2 first patches mean that everyone decided the status quo was ok? Cause I could live with that. But if we think this should be reverted from 1.2 then let's do that sooner than later.

I'm fine with those, I'm fine even with removing those as soon as system doesn't crap on startup.","04/Mar/13 17:39;jbellis;bq. I'm fine with those

To clarify, is that ""I'm fine in principle"" or ""I've reviewed it, +1?""
","04/Mar/13 20:52;xedin;I've reviewed those two, +1.","05/Mar/13 11:05;slebresne;Alright. I committed the 2 first patch. For trunk however, I've kept the behavior of rejecting unknown options (though Cassandra won't refuse to start if there is unknown option in the first place), and I've added a proper not in the NEWS file for that.",,,,,,,,,,,,,,,,,,,,,
Clean out STREAM_STAGE vestiges,CASSANDRA-4764,12610366,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,nickmbailey,nickmbailey,04/Oct/12 17:46,12/Mar/19 14:01,13/Mar/19 22:27,23/Oct/12 14:57,1.2.0 beta 2,,,,,0,streaming,,,,,,Currently it appears as though bulk loading operations don't run in any stage. Seems like they should be running in STREAM_STAGE.,,,,,,,,,,,,,,,,,,,22/Oct/12 19:14;jbellis;4764-v2.txt;https://issues.apache.org/jira/secure/attachment/12550338/4764-v2.txt,23/Oct/12 14:15;jbellis;4764-v3.txt;https://issues.apache.org/jira/secure/attachment/12550465/4764-v3.txt,04/Oct/12 21:07;jbellis;4764.txt;https://issues.apache.org/jira/secure/attachment/12547824/4764.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-10-04 21:07:14.927,,,no_permission,,,,,,,,,,,,240576,,,Tue Oct 23 14:57:58 UTC 2012,,,,,,0|i013mv:,4358,yukim,yukim,,,,,,,,,,"04/Oct/12 21:07;jbellis;Looks like post-CASSANDRA-3494 Stage.STREAM is obsolete, since MessagingService has a per-target Executor for streams.  Patch attached to clean out the vestiges.","04/Oct/12 21:13;nickmbailey;So the reason i noticed this is because we bulk loads weren't showing up in the stream stage mbean. It sounds like nothing will show up there though. Removing the stage is good so that the mbean doesn't show up at all anymore but should we make a ticket to expose similar stats from messaging service?

I guess the only thing that mbean provided that the streaming service mbean doesn't is completed tasks. So maybe just add a counter there?",04/Oct/12 21:22;jbellis;Do we need it?  Would we count sessions or files or ranges?,"04/Oct/12 21:29;nickmbailey;Well just the number of completed tasks probably isn't particularly useful you are right.

I feel like we need some additional tracking around streaming though. Currently if streaming fails mid stream for bootstrap or move or decom and the like there isn't necessarily any indication that something went wrong unless you examine the logs. So more useful woud be successful count and failed count on a per session basis or something like that.

That is perhaps best decided in a different ticket though.","04/Oct/12 21:41;jbellis;Yeah, it sounds like we're not really sure what we need.  Maybe just return session id to the user so they can ask ""hey, how's session X doing?""","22/Oct/12 17:58;yukim;For removing STREAM stage, +1.
Attached patch seems against 1.1 and contains diff from different issue though.",22/Oct/12 19:14;jbellis;clean v2 attached against trunk,22/Oct/12 19:50;yukim;+1,22/Oct/12 21:29;jbellis;committed,"23/Oct/12 14:00;slebresne;That has completely broke bootstrap by removing Verb.STREAM_REQUEST from the list of known verbs in MessagingService (and StreamRequests were executed on teh stream stage, which we can change I suppose).",23/Oct/12 14:13;jbellis;Reverted.,23/Oct/12 14:15;jbellis;v3 attached w/ fix.,"23/Oct/12 14:52;yukim;+1
(StreamReply is actually not handled in MISC stage but in FileStreamTask and streamExecutor now, but let's leave it here for later fix.)","23/Oct/12 14:57;jbellis;committed, w/ comment for STREAM_REPLY",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove uses of SchemaDisagreementException,CASSANDRA-4487,12601434,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,03/Aug/12 22:21,12/Mar/19 14:01,13/Mar/19 22:27,16/Aug/12 22:56,1.2.0 beta 1,,Legacy/CQL,,,0,,,,,,,"Since we can handle concurrent schema changes now, there's no need to validateSchemaAgreement before modification now.",,,,,,,,,,,,,,,,,,,07/Aug/12 13:47;xedin;0001-code-changes.patch;https://issues.apache.org/jira/secure/attachment/12539572/0001-code-changes.patch,07/Aug/12 13:47;xedin;0002-re-generated-thrift.patch;https://issues.apache.org/jira/secure/attachment/12539573/0002-re-generated-thrift.patch,15/Aug/12 11:08;xedin;CASSANDRA-4487-v2.patch;https://issues.apache.org/jira/secure/attachment/12541035/CASSANDRA-4487-v2.patch,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-08-07 20:48:02.107,,,no_permission,,,,,,,,,,,,256194,,,Thu Aug 16 22:56:30 UTC 2012,,,,,,0|i0gwdr:,96676,jbellis,jbellis,,,,,,,,,,"07/Aug/12 20:32;jbellis;We actually want to leave SDE in the thrift interface (but remove throws declarations), so clients can support old C* versions easily.","07/Aug/12 20:48;slebresne;We might also want to keep the validateSchemaIsSettled for the thrift side, otherwise we're changing the semantic of the operation. I mean, it's fine for CQL3, but if we really want to be conservative on the thrift side .... (typically waiting on validateSchemaIsSettled means that you know you can start inserting data in your newly created CF). ",07/Aug/12 21:15;xedin;So we want to limit the scope of the to CQL3?,"07/Aug/12 21:32;jbellis;bq. typically waiting on validateSchemaIsSettled means that you know you can start inserting data in your newly created CF

I don't understand -- we don't do that on the normal request path even in thrift, just for schema mutations.  ","07/Aug/12 21:38;xedin;The thing I don't understand is how it's different for CQL3 comparing to Thrift/CQL, in either case if we to remove the check users won't be able to run any operations safely except to the localhost...","07/Aug/12 21:54;slebresne;bq. we don't do that on the normal request path even in thrift, just for schema mutations

Hum, I could have swear we did that for thrift but apparently not. However we do do it for CQL (that is both CQL2 and for CQL3). Maybe that just a relic of old time, I don't know.

bq. how it's different for CQL3 comparing to Thrift/CQL, in either case if we to remove the check users won't be able to run any operations safely except to the localhost

I think the waiting for schema settled shouldn't be part of the request path, but should be done separately by clients if they want to. But if we were doing the wait in thrift, I would have suggested to keep it just for the sake of not changing the behavior. But anyway, if we don't do it, I guess there is no problem.","07/Aug/12 22:04;xedin;bq. Hum, I could have swear we did that for thrift but apparently not. However we do do it for CQL (that is both CQL2 and for CQL3). Maybe that just a relic of old time, I don't know.

It should be, because it only makes sense to check it for schema migrations.

bq. I think the waiting for schema settled shouldn't be part of the request path, but should be done separately by clients if they want to. But if we were doing the wait in thrift, I would have suggested to keep it just for the sake of not changing the behavior. But anyway, if we don't do it, I guess there is no problem.

I'm not sure if that hurts that much to leave such validation on schema migration patch, definitely would avoid problems related to change contention or errors on the read/write patch with CL > ONE if things weren't settled properly. But I think if we are to remove checks we probably should remove all of them to keep behavior consistent.

",15/Aug/12 11:08;xedin;Attaching v2 which removes all schema agreement validation but leafs SDE in place for Thrift API backward compatibility.,16/Aug/12 16:45;jbellis;LGTM,16/Aug/12 22:56;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
old-style mapred interface doesn't set key limit correctly,CASSANDRA-4534,12603250,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,13/Aug/12 14:54,12/Mar/19 14:01,13/Mar/19 22:27,14/Aug/12 21:02,1.0.12,1.1.4,,,,0,,,,,,,"{{next(ByteBuffer key, SortedMap<ByteBuffer, IColumn> value)}} calls clear/put/rewind, but not flip or limit.",,,,,,,,,,,,,,,,,,,13/Aug/12 14:59;jbellis;4534.txt;https://issues.apache.org/jira/secure/attachment/12540666/4534.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-13 15:01:22.65,,,no_permission,,,,,,,,,,,,256224,,,Tue Aug 14 21:02:38 UTC 2012,,,,,,0|i0gwtz:,96749,brandon.williams,brandon.williams,,,,,,,,,,13/Aug/12 14:59;jbellis;fix attached,13/Aug/12 15:01;brandon.williams;+1,14/Aug/12 21:02;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo fix: key_valiation_class -> key_validation_class,CASSANDRA-4089,12548263,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,ash211,ash211,27/Mar/12 06:32,12/Mar/19 14:01,13/Mar/19 22:27,27/Mar/12 22:46,1.0.0,,Legacy/Documentation and Website,,,0,,,,,,,There is a typo in the Cli help docs for the update column family command.,,,,,,,,,,,,,,,,,,,27/Mar/12 06:33;ash211;typo-fix-01.txt;https://issues.apache.org/jira/secure/attachment/12520070/typo-fix-01.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-27 22:46:46.766,,,no_permission,,,,,,,,,,,,233360,,,Wed Mar 28 01:10:25 UTC 2012,,,,,,0|i0gru7:,95940,,,,,,,,,,,,"27/Mar/12 06:33;ash211;diff --git a/src/resources/org/apache/cassandra/cli/CliHelp.
index 9e417dc..017b54e 100644
--- a/src/resources/org/apache/cassandra/cli/CliHelp.yaml
+++ b/src/resources/org/apache/cassandra/cli/CliHelp.yaml
@@ -692,7 +692,7 @@ commands:
           It is also valid to specify the fully-qualified c
           extends org.apache.cassandra.db.marshal.AbstractT
 
-        - key_valiation_class: Validator to use for keys.
+        - key_validation_class: Validator to use for keys.
           Default is BytesType which applies no validation.
 
           Supported values are:","27/Mar/12 22:46;xedin;Committed, thanks!",28/Mar/12 01:01;jbellis;Doesn't affect 1.0+?,"28/Mar/12 01:10;ash211;I observed it on 1.0.8

On Tue, Mar 27, 2012 at 6:03 PM, Jonathan Ellis (Commented) (JIRA) <

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure Jackson dependency matches lib,CASSANDRA-5126,12626386,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,zznate,zznate,zznate,07/Jan/13 18:43,12/Mar/19 14:01,13/Mar/19 22:27,09/Jan/13 03:06,1.2.1,,,,,0,,,,,,,"Older version of Jackson ASL has a concurrency issue. See http://jira.codehaus.org/browse/JACKSON-237

This can be triggered in some environments when running M/R tasks and the wrong version of jackson gets picked up. ",,,,,,,,,,,,CASSANDRA-4183,,,,,,,08/Jan/13 21:44;amorton;5126-2.txt;https://issues.apache.org/jira/secure/attachment/12563832/5126-2.txt,07/Jan/13 18:46;zznate;5126.txt;https://issues.apache.org/jira/secure/attachment/12563607/5126.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-08 20:29:14.179,,,no_permission,,,,,,,,,,,,302978,,,Wed Jan 09 03:06:00 UTC 2013,,,,,,0|i176xb:,250090,amorton,amorton,,,,,,,,,,07/Jan/13 18:46;zznate;Updates Jackson core and asl to latest 1.9.x,08/Jan/13 20:29;amorton;Fixed in CASSANDRA-4183,08/Jan/13 21:01;zznate;Not so - looks like *only* 1.1 has the patch applied. cassandra-1.2 and trunk both still reference older versions.,"08/Jan/13 21:16;zznate;So - looks like the jars were updated in the lib dir, but not in the build.xml.

The ramifications of this are that *anything* referencing cassandra on maven central will have the older versions of jackson brought in as dependencies.","08/Jan/13 21:44;amorton;Second version, sets the dependancy to 1.9.2 to match the file in /lib",09/Jan/13 03:06;amorton;Applied to 1.2 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't insert only a key in CQL3,CASSANDRA-5040,12622913,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,07/Dec/12 15:18,12/Mar/19 14:01,13/Mar/19 22:27,10/Dec/12 08:49,1.2.0 rc1,,,,,0,,,,,,,"The following should work but well, doesnt:
{noformat}
cqlsh:k> CREATE TABLE t (k int PRIMARY KEY, v int);
cqlsh:k> INSERT INTO t (k) VALUES (0);
Bad Request: line 1:27 required (...)+ loop did not match anything at input ')'
{noformat}

The reason is just that the parser for INSERT has never been updated from the time where providing only a key was illegal.",,,,,,,,,,,,,,,,,,,07/Dec/12 15:19;slebresne;5040.patch;https://issues.apache.org/jira/secure/attachment/12559873/5040.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-07 16:46:07.415,,,no_permission,,,,,,,,,,,,296491,,,Mon Dec 10 08:49:25 UTC 2012,,,,,,0|i149qv:,233042,jbellis,jbellis,,,,,,,,,,07/Dec/12 15:19;slebresne;Trivial patch attached,07/Dec/12 16:46;jbellis;+1,"10/Dec/12 08:49;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
o.a.c.hadoop.ConfigHelper should support setting Thrift frame and max message sizes.,CASSANDRA-5188,12629380,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,xedin,xedin,26/Jan/13 05:49,12/Mar/19 14:01,13/Mar/19 22:27,28/Jan/13 18:47,1.1.10,1.2.2,,,,0,,,,,,,Without such support people will be running into problems like https://github.com/thinkaurelius/faunus/issues/99 without any work around when custom frame and/or max message sizes are used.,,,,,,,,,,,,,,,,,,,26/Jan/13 05:52;xedin;CASSANDRA-5188.patch;https://issues.apache.org/jira/secure/attachment/12566615/CASSANDRA-5188.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-28 18:26:10.294,,,no_permission,,,,,,,,,,,,309812,,,Mon Jan 28 18:47:52 UTC 2013,,,,,,0|i1haw7:,309190,brandon.williams,brandon.williams,,,,,,,,,,28/Jan/13 18:26;brandon.williams;+1,28/Jan/13 18:47;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli should exit with error-exit status on all errors which cause it to exit.,CASSANDRA-5247,12632103,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,steve_p,rthille,rthille,12/Feb/13 22:23,12/Mar/19 14:00,13/Mar/19 22:27,29/Jun/13 18:38,1.2.7,,Legacy/Tools,,,0,lhf,,,,,,"running cassandra-cli with a --file argument which does not exist returns success:
ubuntu@host:~$ cassandra-cli --file does-not-exist ; echo $?
does-not-exist (No such file or directory)
0
",,,,,,,,,,,,,,,,,,,25/Jun/13 03:51;steve_p;CASSANDRA-5247.txt;https://issues.apache.org/jira/secure/attachment/12589542/CASSANDRA-5247.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-22 12:21:31.982,,,no_permission,,,,,,,,,,,,312599,,,Sat Jun 29 18:38:40 UTC 2013,,,,,,0|i1hy1z:,312945,jbellis,jbellis,,,,,,,,,,"22/Apr/13 12:21;lakemove;inFileMode swallows the exception (which should be thrown to outer)
{code}
if(sessionState.isFileMode(){
  FileReader fileReader;
  try {
    fileReader = new FileReader(sessionState.filename);
  }catch(IOException e) {
    sessionState.err.println(e.getMessage());
    return;//here returns main method and exit JVM. should re-throw exception instead.
  }
}
{code}","25/Jun/13 03:55;steve_p;This patch resolves the problem mentioned in the ticket by doing a ""System.exit(1)"" instead of a return as mentioned in the first comment.  Also the code after the try...catch is moved into the try due to errors thrown by the compiler complaining that fileReader was now possibly undefined.  This should be OK since evaluateFileStatements() also throws IOExceptions so errors thrown while reading the input file would now be caught by the catch that now has the exit().","29/Jun/13 18:38;jbellis;Committed, thanks!

(Ignored the rebuild of Thrift code with an older? newer? compiler.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate login for USE queries,CASSANDRA-5207,12630185,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,31/Jan/13 18:39,12/Mar/19 14:00,13/Mar/19 22:27,31/Jan/13 19:09,1.2.2,,,,,0,,,,,,,"CASSANDRA-5144 added login validation to Thrift set_keyspace method. Same should be done for CQL2 and CQL3 USE queries, otherwise C* will leak keyspace existence to strangers even when the configured authenticator requires login.",,,,,,,,,,,,,,,,,,,31/Jan/13 18:40;iamaleksey;5207.txt;https://issues.apache.org/jira/secure/attachment/12567404/5207.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-31 18:49:42.301,,,no_permission,,,,,,,,,,,,310681,,,Thu Jan 31 19:09:31 UTC 2013,,,,,,0|i1hm7z:,311026,jbellis,jbellis,,,,,,,,,,31/Jan/13 18:49;jbellis;+1,"31/Jan/13 19:09;iamaleksey;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress tool hangs forever on timeout or error,CASSANDRA-4128,12549798,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,tpatterson,tpatterson,05/Apr/12 21:42,12/Mar/19 14:00,13/Mar/19 22:27,06/Apr/12 20:35,1.1.0,,Legacy/Tools,,,0,stress,,,,,,"The stress tool hangs forever if it encounters a timeout or exception. CTRL-C will kill it if run from a terminal, but when running it from a script (like a dtest) it hangs the script forever. It would be great for scripting it if a reasonable error code was returned when things go wrong.

To duplicate, clear out /var/lib/cassandra and then run ""stress --operation=READ"".","This happens in every version of the stress tool, that I know of, including calling it from the dtests.",,,,,,,,,,,,,,,,,,06/Apr/12 19:32;xedin;CASSANDRA-4128.patch;https://issues.apache.org/jira/secure/attachment/12521715/CASSANDRA-4128.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-06 20:14:01.787,,,no_permission,,,,,,,,,,,,234789,,,Fri Apr 06 20:35:02 UTC 2012,,,,,,0|i0gsb3:,96016,brandon.williams,brandon.williams,,,,,,,,,,06/Apr/12 20:14;brandon.williams;+1,06/Apr/12 20:35;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove token generator,CASSANDRA-5261,12632530,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,snazy,jbellis,jbellis,15/Feb/13 05:44,12/Mar/19 14:00,13/Mar/19 22:27,26/Oct/15 12:40,3.0.0,,Legacy/Tools,,,0,triaged,,,,,,Obsoleted by vnodes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-10-26 12:23:56.82,,,no_permission,,,,,,,,,,,,313026,,,Mon Oct 26 12:40:49 UTC 2015,,,,,,0|i1i0p3:,313372,iamaleksey,iamaleksey,,,,,,,,,,"26/Oct/15 12:23;iamaleksey;+1, but don't forget to add a NEWS.txt entry.","26/Oct/15 12:40;snazy;Thanks!
Committed as db7feb4c252f3ed12fbc40beb2d465a7070f9b0d",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix validation of dates,CASSANDRA-4441,12598998,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,16/Jul/12 17:04,12/Mar/19 14:00,13/Mar/19 22:27,16/Jul/12 18:34,1.2.0 beta 1,,,,,0,,,,,,,"Our date validation (timestamp type) doesn't validate that the date is correct, i.e. it allows dates like 2011-42-42, because DateUtils.parseDate() doesn't do the validation (don't ask me how it can generate a timestamp from bogus date, apparently it can).

The easy fix is to use DateUtils.parseDateStrictly(), which does the validation. This does require to update commons-lang to >= 2.5 (we have 2.4).",,,,,,,,,,,,,,,,,,,16/Jul/12 17:10;slebresne;4441.txt;https://issues.apache.org/jira/secure/attachment/12536658/4441.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-16 17:15:35.358,,,no_permission,,,,,,,,,,,,256161,,,Mon Jul 16 18:34:09 UTC 2012,,,,,,0|i0gvx3:,96601,jbellis,jbellis,,,,,,,,,,16/Jul/12 17:10;slebresne;Attached trivial patch. As said this require commons-lang 2.6 (which I'll add when committing),"16/Jul/12 17:15;jbellis;+1, but I'd rather push this to 1.2 only, since DateType has behaved this way since 0.8.","16/Jul/12 18:34;slebresne;You're right, committed to trunk only.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 CREATE TABLE with set and counter causes java.lang.IllegalArgumentException,CASSANDRA-4706,12608789,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,titanous,titanous,23/Sep/12 15:40,12/Mar/19 14:00,13/Mar/19 22:27,26/Sep/12 12:10,1.2.0 beta 2,,,,,0,,,,,,,"Running a freshly compiled cassandra with no data, and a brand new keyspace (SimpleStrategy, replication_factor 1)


{noformat}
cqlsh:test> CREATE TABLE test (id bigint PRIMARY KEY, count counter, things set<text>);
TSocket read 0 bytes
{noformat}

{noformat}
ERROR 11:25:54,926 Error occurred during processing of message.
java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:247)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:50)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:59)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getString(AbstractCompositeType.java:143)
	at org.apache.cassandra.config.CFMetaData.validate(CFMetaData.java:1064)
	at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:123)
	at org.apache.cassandra.cql3.statements.CreateColumnFamilyStatement.announceMigration(CreateColumnFamilyStatement.java:100)
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:83)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:116)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1677)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}","rev 60bf68ca (tagged 1.2.0-beta1-tentative)

cqlsh 2.2.0 | Cassandra 1.2.0-beta1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.34.0

OS X 10.8.2
java version ""1.6.0_35""
Java(TM) SE Runtime Environment (build 1.6.0_35-b10-428-11M3811)
Java HotSpot(TM) 64-Bit Server VM (build 20.10-b01-428, mixed mode)

Ubuntu 12.04.1 LTS
java version ""1.7.0_07""
Java(TM) SE Runtime Environment (build 1.7.0_07-b10)
Java HotSpot(TM) 64-Bit Server VM (build 23.3-b01, mixed mode)",,,,,,,,,,,,,,,,,,25/Sep/12 13:56;slebresne;4706.txt;https://issues.apache.org/jira/secure/attachment/12546515/4706.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-24 13:10:23.888,,,no_permission,,,,,,,,,,,,256347,,,Wed Sep 26 12:10:54 UTC 2012,,,,,,0|i0gyn3:,97042,jbellis,jbellis,,,,,,,,,,24/Sep/12 13:10;slebresne;The code lacks proper validation and I'll look into it but let me note that mixing counter column with non-counter columns (set being non-counter) is not allowed (not more than it is on thrift).,"24/Sep/12 14:37;titanous;Ah, thanks, I missed that in the docs.",25/Sep/12 13:56;slebresne;We were actually validating correctly but the IAE was triggered while building the InvalidRequestException. Attached simple patch to fix.,25/Sep/12 16:53;jbellis;+1,"26/Sep/12 12:10;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalStateException thrown when running new installation with old data directories,CASSANDRA-5196,12629800,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,rstrickland,rstrickland,29/Jan/13 21:51,12/Mar/19 13:59,13/Mar/19 22:27,05/Feb/13 23:43,1.2.2,,,,,0,,,,,,,"If you install 1.2.1 when there are existing data directories, the scrub operation fails, throwing this exception:

ERROR [main] 2013-01-29 15:05:06,564 FileUtils.java (line 373) Stopping the gossiper and the RPC server
ERROR [main] 2013-01-29 15:05:06,564 CassandraDaemon.java (line 387) Exception encountered during startup
java.lang.IllegalStateException: No configured daemon
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:314)
	at org.apache.cassandra.io.util.FileUtils.handleFSError(FileUtils.java:375)
	at org.apache.cassandra.db.Directories.<init>(Directories.java:113)
	at org.apache.cassandra.db.Directories.create(Directories.java:91)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:403)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:174)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)

This condition should produce a more reasonable exception.",CentOS 5.5,,,,,,,,,,,,,,,,,,05/Feb/13 20:52;iamaleksey;5196.txt;https://issues.apache.org/jira/secure/attachment/12568084/5196.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-31 23:22:15.645,,,no_permission,,,,,,,,,,,,310296,,,Tue Feb 05 23:43:22 UTC 2013,,,,,,0|i1hjun:,310641,brandon.williams,brandon.williams,,,,,,,,,,"31/Jan/13 23:22;iamaleksey;The exception is thrown by this method:

{noformat}
    public static void createDirectory(File directory)
    {
        if (!directory.exists())
        {
            if (!directory.mkdirs())
                throw new FSWriteError(new IOException(""Failed to mkdirs "" + directory), directory);
        }
    }
{noformat}

So it seems like the directory didn't exist, it tried to create one, but failed (I assume it had something to do with permissions).

Can't reproduce. Can you, [~rstrickland]?",04/Feb/13 17:52;iamaleksey;Pretty sure this particular issue was caused by not having enough permissions to create the directories. Please reopen if that's not the case and you can reproduce.,"05/Feb/13 03:27;haifeng;Issue reproduced on my machine.
Environment:
Redhat Linux, Cassandra 1.2.1 unzipped to a NFS mounted folder (/mnt/storage/cassandra). 
Create data/commitlog folder manually in the NFS folder(mkdir -p /mnt/storage/cassandra/data, mkdir -p /mnt/storage/cassandra/commitlog).
config file updated.
Start Cassandra, ""bin/cassandra -f"", same error found.
{quote}
 INFO 22:15:06,283 Found table data in data directories. Consider using the CLI to define your schema.
ERROR 22:15:06,495 Stopping the gossiper and the RPC server
ERROR 22:15:06,505 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.IllegalStateException: No configured daemon
        at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:314)
        at org.apache.cassandra.io.util.FileUtils.handleFSError(FileUtils.java:375)
        at org.apache.cassandra.db.Directories.<init>(Directories.java:113)
        at org.apache.cassandra.db.Directories.create(Directories.java:91)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:379)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:364)
        at org.apache.cassandra.db.Table.initCf(Table.java:337)
        at org.apache.cassandra.db.Table.<init>(Table.java:280)
        at org.apache.cassandra.db.Table.open(Table.java:110)
        at org.apache.cassandra.db.Table.open(Table.java:88)
        at org.apache.cassandra.db.Table$1.apply(Table.java:82)
        at org.apache.cassandra.db.Table$1.apply(Table.java:79)
        at com.google.common.collect.Iterators$9.transform(Iterators.java:893)
        at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1664)
        at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:114)
        at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:41)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
 INFO 22:15:06,560 No commitlog files found; skipping replay
{quote}

Remove above folders(data/commitlog) and restart Cannandra, issue resolved.",05/Feb/13 03:34;iamaleksey;What OS user was used when you created the /mnt/storage/cassandra/data directory? Did you use sudo? Did you run bin/cassandra -f as the same user? ,"05/Feb/13 03:42;haifeng;RHEL 6.2, 64bit.
All operations were done by root user.
/mnt/storage/cassandra not removed. data and commitlog folders removed.
{quote}

drwxr-xr-x 4 root root      4096 Feb  4 22:18 cassandra
{quote}","05/Feb/13 04:20;iamaleksey;For some reason Java fails to create the directories. Or at least directory.mkdirs() call returns false. Unfortunately Java's file api won't give you any details, other than it thinks it failed - I don't see what we can do about those cases.

I'd like you to try one other thing, though.

remove /mnt/storage/cassandra/data

Start Cassandra (if I'm reading you comment right - it should start fine).

Stop Cassandra. Remove /mnt/storage/cassandra/data/* - all the sstable directories inside data, but NOT /mnt/storage/cassandra/data/ itself. Then start Cassandra again ant tell me if it works all right or fails as if you created it manually. Thanks. ","05/Feb/13 06:24;haifeng;{quote}
Stop Cassandra. Remove /mnt/storage/cassandra/data/* - all the sstable directories inside data, but NOT /mnt/storage/cassandra/data/ itself. Then start Cassandra again ant tell me if it works all right or fails as if you created it manually. Thanks.
{quote}
Run Cassandra successfully, then stop it, remove ""/mnt/storage/cassandra/data/*"" and run Cassandra again, the Cassandra started without error.

Then I did following test, no error found.
Remove all folders/files under /mnt/storage/cassandra/, create data folder manually, start Cassandra. 

","05/Feb/13 14:07;rstrickland;In my case the directories were owned by ""cassandra"" and the service was started as ""cassandra"" using init.  Even starting as root resulted in the same failure.  Only deleting the directories entirely resolved the issue.  This worked in my case because it was an old installation and I didn't need the data, but it took me a while to figure this out since the exception was so misleading.","05/Feb/13 20:55;iamaleksey;While I don't think we can fix the directory creation itself in these cases, we can and should make failure handling user-friendlier. The attached patch changes the following things:
- doesn't attempt to stop RPC if it's not already started - gets rid of the non-obvious IllegalStateException
- separately logs directory creation failure
- stops native protocol as well if disk_failure_policy is 'stop'
- reduces log-spam when we fail to create multiple directories","05/Feb/13 23:16;brandon.williams;+1 on the patch, but I don't understand what the directory creation problem is in the case of it existing as user X, but then even running as root we raise an error.","05/Feb/13 23:20;iamaleksey;bq. but I don't understand what the directory creation problem is in the case of it existing as user X, but then even running as root we raise an error.

I don't understand either - and I can't reproduce it (I tried).",05/Feb/13 23:33;brandon.williams;We can take a closer with the patch in place if it happens again.,05/Feb/13 23:43;iamaleksey;Yep. Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Truncate operation doesn't delete rows from HintsColumnFamily.,CASSANDRA-4655,12607241,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,azotcsit,azotcsit,12/Sep/12 13:48,12/Mar/19 13:59,13/Mar/19 22:27,23/May/13 15:07,1.2.6,,,,,0,hintedhandoff,truncate,,,,,"Steps to reproduce:
1. Start writing of data to some column family, let name it 'MyCF'
2. Stop 1 node
3. Wait some time (until some data will be collected in HintsColumnFamily)
4. Start node (HintedHandoff will be started automatically for 'MyCF')
5. Run 'truncate' command for 'MyCF' column family from command from cli
6. Wait until truncate will be finished
7. You will see that 'MyCF' is not empty because HintedHandoff is copying data 

So, I suggest to clean HintsColumnFamily (for truncated column family) before we had started to discard sstables. 
I think it should be done in CompactionManager#submitTrucate() method. I can try to create patch but I need to know right way of cleaning HintsColumnFamily. Could you clarify it?
","Centos 6.2, Cassandra 1.1.4 (DataStax distribution), three-nodes cluster.",,,,,,,,,,,,,,,,,,22/May/13 23:24;jbellis;4655-v3.txt;https://issues.apache.org/jira/secure/attachment/12584401/4655-v3.txt,06/Dec/12 11:48;azotcsit;cassandra-1.2-4655-hints_truncation-v2.txt;https://issues.apache.org/jira/secure/attachment/12556263/cassandra-1.2-4655-hints_truncation-v2.txt,26/Sep/12 08:22;azotcsit;cassandra-1.2-4655-hints_truncation.txt;https://issues.apache.org/jira/secure/attachment/12546659/cassandra-1.2-4655-hints_truncation.txt,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-09-21 16:35:56.191,,,no_permission,,,,,,,,,,,,250235,,,Thu May 23 15:07:46 UTC 2013,,,,,,0|i0awfj:,61527,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,,"21/Sep/12 16:35;jbellis;Maybe it would be easier to record truncate time and drop hints older than that on replay, than to make truncate a two-pass operation.",24/Sep/12 08:57;azotcsit;Good idea. I'll try to implement it.,26/Sep/12 08:24;azotcsit;Patch has been attached. Please review it.,"26/Sep/12 15:20;jbellis;Two things:

# Need to store last truncation time across server restart.  Storing these as a Map in {{system.local}} is one option.  ({{system}} columnfamilies are defined in CFMetaData.)
# Instead of skipping the entire hint if any component CF has been truncated, should generate a new RowMutation with only untruncated CF data in it.","07/Nov/12 21:33;jbellis;Are you still working on this, Alexey?","06/Dec/12 11:48;azotcsit;Jonathan, I'm sorry for so long delay and a silence from my side. 
I've created another patch against current trunk (46f1c7fabd6a7102a7a363bcfdc40c62a3bdc461). It uses local column family as you suggested. I tested it on three-nodes ccm cluster with replication factor 2. Steps:
1. load 150KB per node
2. stop one of them
3. wait until both live nodes will have about 20MB of data
4. stop loading of data
5. start downed node
6. wait 3-5 seconds after start of hints delivery process and truncate data
7. wait until data will be successfully truncated
Results:
Nodes which were not stopped are completely empty. Another node [that was stopped] contains about several tens rows but all their were marked as deleted. Note that the both nodes which weren't stopped before truncate operation has about 5-6 thousands of rows.

Please review the patch.",26/Feb/13 06:43;azotcsit;Is there any progress on review?,"27/Feb/13 09:47;vijay2win@yahoo.com;Overall LGTM, 

Do we really need to remove removeTruncationTime? IMHO it might be a useful information for the user to query.
Instead of caching the truncatedAt in CFS can we just cache it in deliverHintsToEndpointInternal? coz it wont be thread safe in CFS anyways.
",25/Mar/13 15:54;jbellis;Ping [~azotcsit],"22/May/13 23:24;jbellis;v3 attached.  I've retained removing obsolete truncation information; otherwise, in the case of creating many temporary tables we will cause problems eventually.

Moved caching to HHOM.  Also combined the new hint time with the existing truncated_at blob for simplicity and to make sure both get updated as a group.",23/May/13 02:33;vijay2win@yahoo.com;+1,23/May/13 15:07;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SizeTieredCompactionStrategy.getBuckets is quadradic in the number of sstables,CASSANDRA-4287,12558177,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,25/May/12 19:47,12/Mar/19 13:59,13/Mar/19 22:27,26/May/12 05:37,1.0.11,1.1.1,,,,0,compaction,,,,,,"getBuckets first sorts the sstables by size (N log N) then adds each sstable to a bucket (N**2 in the worst case of all sstables the same size, because we use the bucket's contents as a hash key).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-25 20:58:38.837,,,no_permission,,,,,,,,,,,,256028,,,Sat May 26 05:37:20 UTC 2012,,,,,,0|i0gu6f:,96319,thobbs,thobbs,,,,,,,,,,25/May/12 20:23;jbellis;Changesets up on https://github.com/jbellis/cassandra/branches/4287,25/May/12 20:58;thobbs;+1,26/May/12 05:37;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 Descrepancies,CASSANDRA-4968,12616472,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,tjake,tjake,16/Nov/12 21:19,12/Mar/19 13:59,13/Mar/19 22:27,17/Nov/12 17:30,1.2.0 beta 3,,,,,0,,,,,,,"I upgraded an environment from 1.1.6 to 1.2 beta

We found the following differences between cql3 versions:


The CLUSTERING ORDER BY requires all columns be specified in 1.2 whereas 1.1 defaulted to ASC

{code}
CREATE TABLE table1(
   col1 text,
   classtype ascii,
   kt bigint,
   id uuid,
   PRIMARY KEY (col1,classtype,kt)
) WITH COMPACT STORAGE
AND CLUSTERING ORDER BY(classtype ASC, kt DESC)
{code}

1.1 didn't require classtype in the ORDER BY, 1.2 does.


----------

In docs/cql3.textile there is mention of 'compaction_strategy' CREATE option but the actual option is 'compaction'

----------

cqlsh for some reason renders all bytes fields as strings in 1.2 whereas in 1.1 they were hex.






",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-11-17 17:30:39.19,,,no_permission,,,,,,,,,,,,258338,,,Sat Nov 17 17:30:39 UTC 2012,,,,,,0|i0krhr:,119271,,,,,,,,,,,,"17/Nov/12 17:30;jbellis;ORDER BY change is a bug fix.  leaving classtype implicit implies to SQL users that kt is the primary ordering/clustering which is incorrect.

Ninja-fixed compaction_strategy in 2806dedd9c412d3be6ae6979a86b155497284263.

Created CASSANDRA-4970 to deal with cqlsh bug.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3: Allow renaming PK columns to ease upgrade from thrift,CASSANDRA-4822,12612166,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,17/Oct/12 07:56,12/Mar/19 13:59,13/Mar/19 22:27,25/Oct/12 16:00,1.2.0 beta 2,,Legacy/CQL,,,0,,,,,,,"Say you have a clicks CF in thrift storing for each user a timeline of which links it clicked on. It may have a definition like:
{noformat}
create column family clicks with key_validation_class = UUIDType and comparator = TimeUUIDType and default_validation_class = UTF8Type
{noformat}

In CQL3, you can access that thrift created CF as if it had been defined by:
{noformat}
CREATE TABLE clicks (
  key uuid,
  column timeuuid,
  value text,
  PRIMARY KEY (key, column)
) WITH COMPACT STORAGE
{noformat}
In other words, CQL3 will pick default names for the key_alias, column_aliases and value_alias metadata. It's ok but it would be more user friendly to use if the user could rename those to something better. Today, the only solution would be to remove the schema and re-create the table in CQL3. We can make that simpler by adding support for:
{noformat}
ALTER TABLE clicks RENAME key to user_id;
ALTER TABLE clicks RENAME column to insertion_time;
ALTER TABLE clicks RENAME value to url_clicked; 
{noformat}

Of course such rename statement won't be applicable to all columns. Namely, we can only allow renaming PK columns and in some compact storage cases the value. But that's probably still worth adding.",,,,,,,,,,,,,,,,,,,25/Oct/12 15:29;slebresne;4822-2.txt;https://issues.apache.org/jira/secure/attachment/12550784/4822-2.txt,17/Oct/12 08:33;slebresne;4822.txt;https://issues.apache.org/jira/secure/attachment/12549467/4822.txt,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-10-24 20:13:01.674,,,no_permission,,,,,,,,,,,,249229,,,Thu Oct 25 16:00:20 UTC 2012,,,,,,0|i0a68f:,57283,jbellis,jbellis,,,,,,,,,,"24/Oct/12 20:13;jbellis;I think this is backwards:

+                newList.add(i < l.size() ? null : l.get(i));

Nits: would prefer {{l}} to be given a more meaningful name, like {{oldNames}}.  Comments for the aliases lists that they can be null-padded would be nice.","25/Oct/12 15:29;slebresne;Attaching v2 of the patch. It addresses the remarks above but now that I've been able to test it, it also includes a few new stuffs. Basically we weren't very accurate in our detection of whether the table was compact or not. Let recall that we don't ""store"" if a table is compact or not, we infer that from the comparator and how many column aliases exist. We probably should have store something to distinguish cql3 tables from other ones from day one, but unless we want to break all existing CQL3 table now we have to stick to our little detection dance.

Anyway, I've updated said detection code (in CFDefinition) to be more precise.  However, there is still a small catch if we allow users to provide column aliases, which is that if a thrift user with a composite type declare all but the last column alias, we will interpret the table as non-compact but that's not correct. I don't know how to fix that, but to make sure this have little change to happen in practice, I've modified the rename to allow renaming multiple columns in the same ALTER statement. If User upgrading from thrift alter all the columns at once, they'll always be fine.
",25/Oct/12 15:49;jbellis;+1,"25/Oct/12 16:00;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
no error propagated to client when updating a column family with an invalid column def,CASSANDRA-4353,12594970,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,soverton,soverton,soverton,18/Jun/12 17:31,12/Mar/19 13:59,13/Mar/19 22:27,19/Jun/12 00:00,1.1.2,,Legacy/CQL,,,0,,,,,,,"CASSANDRA-3761 appears to have introduced a regression which is exposed by test_system_column_family_operations in test/system/test_thrift_server.py

The test fails with this stack trace:
{noformat}
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_system_column_family_operations
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/nose/case.py"", line 183, in runTest
    self.test(*self.arg)
  File ""/opt/acunu/tests/cassandra-tests.hg/thrift/system/test_thrift_server.py"", line 1469, in test_system_column_family_operations
    _expect_exception(fail_invalid_field, InvalidRequestException)
  File ""/opt/acunu/tests/cassandra-tests.hg/thrift/system/test_thrift_server.py"", line 209, in _expect_exception
    r = fn()
  File ""/opt/acunu/tests/cassandra-tests.hg/thrift/system/test_thrift_server.py"", line 1468, in fail_invalid_field
    client.system_update_column_family(modified_cf)
  File ""/usr/lib/python2.6/site-packages/cassandra/Cassandra.py"", line 1892, in system_update_column_family
    return self.recv_system_update_column_family()
  File ""/usr/lib/python2.6/site-packages/cassandra/Cassandra.py"", line 1903, in recv_system_update_column_family
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File ""/usr/lib64/python2.6/site-packages/thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
    sz = self.readI32()
  File ""/usr/lib64/python2.6/site-packages/thrift/protocol/TBinaryProtocol.py"", line 203, in readI32
    buff = self.trans.readAll(4)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 272, in read
    self.readFrame()
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 276, in readFrame
    buff = self.__trans.readAll(4)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TSocket.py"", line 108, in read
    raise TTransportException(type=TTransportException.END_OF_FILE, message='TSocket read 0 bytes')
TTransportException: TSocket read 0 bytes

----------------------------------------------------------------------

{noformat}

The logs have the following stack trace:

{noformat}
ERROR [Thrift:1] 2012-06-18 18:17:27,865 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 16
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:47)
        at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:115)
        at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1303)
        at org.apache.cassandra.config.CFMetaData.columnMetadata(CFMetaData.java:228)
        at org.apache.cassandra.config.CFMetaData.fromThrift(CFMetaData.java:648)
        at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1061)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3520)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3508)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

",,,,,,,,,,,,,,,,,,,18/Jun/12 17:41;soverton;4353.patch;https://issues.apache.org/jira/secure/attachment/12532431/4353.patch,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-19 00:00:05.166,,,no_permission,,,,,,,,,,,,256087,,,Tue Jun 19 00:00:05 UTC 2012,,,,,,0|i0gux3:,96439,xedin,xedin,,,,,,,,,,18/Jun/12 17:41;soverton;Attached patch which re-throws MarshalException as InvalidRequestException in CFMetaData.fromThrift,19/Jun/12 00:00;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate compression parameters,CASSANDRA-4266,12556747,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,jbellis,jbellis,21/May/12 20:32,12/Mar/19 13:58,13/Mar/19 22:27,23/May/12 13:46,1.0.11,1.1.1,,,,0,,,,,,,compression_parameters doesn't warn when unknown options are specified; see  http://ac31004.blogspot.co.uk/2012/05/snappy-compression-fails-for-apache.html,,,,,,,,,,,,,,,,,,,22/May/12 16:27;slebresne;4266.txt;https://issues.apache.org/jira/secure/attachment/12528617/4266.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-22 16:27:36.206,,,no_permission,,,,,,,,,,,,256008,,,Wed May 23 13:46:50 UTC 2012,,,,,,0|i0gtxj:,96279,jbellis,jbellis,,,,,,,,,,"22/May/12 16:27;slebresne;Attaching patch that does 2 things:
# Refuse unknown compression options
# Check at startup whether Snappy is supported (since after all it use native code underneath) and if it doesn't, it logs a warning and default table creation to umcompressed ones.",22/May/12 16:29;slebresne;Note that the patch is against 1.1 but we can commit to 1.0 too.,22/May/12 20:46;jbellis;+1,"23/May/12 13:46;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Track maximum ttl and use to expire entire sstables,CASSANDRA-5228,12631278,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,jbellis,jbellis,07/Feb/13 15:29,12/Mar/19 13:58,13/Mar/19 22:27,22/Mar/13 10:43,2.0 beta 1,,,,,1,,,,,,,It would be nice to be able to throw away entire sstables worth of data when we know that it's all expired.,,,,,,,,,,,,,,CASSANDRA-5685,,,,,07/Mar/13 13:06;krummas;0001-track-max-local-deletiontime-v2.patch;https://issues.apache.org/jira/secure/attachment/12572529/0001-track-max-local-deletiontime-v2.patch,07/Mar/13 14:19;krummas;0001-track-max-local-deletiontime-v3.patch;https://issues.apache.org/jira/secure/attachment/12572539/0001-track-max-local-deletiontime-v3.patch,22/Mar/13 07:35;krummas;0001-track-max-local-deletiontime-v4.patch;https://issues.apache.org/jira/secure/attachment/12574976/0001-track-max-local-deletiontime-v4.patch,06/Mar/13 09:40;krummas;0001-track-max-ttl-v1.patch;https://issues.apache.org/jira/secure/attachment/12572297/0001-track-max-ttl-v1.patch,11/Mar/13 15:14;krummas;0002-CASSANDRA-5228-add-a-nodetool-command-that-drops-ent.patch;https://issues.apache.org/jira/secure/attachment/12573099/0002-CASSANDRA-5228-add-a-nodetool-command-that-drops-ent.patch,13/Mar/13 20:00;krummas;0002-CASSANDRA-5228-drop-entire-sstables-if-all-tombstone-v2.patch;https://issues.apache.org/jira/secure/attachment/12573565/0002-CASSANDRA-5228-drop-entire-sstables-if-all-tombstone-v2.patch,22/Mar/13 07:35;krummas;0002-CASSANDRA-5228-v3.patch;https://issues.apache.org/jira/secure/attachment/12574975/0002-CASSANDRA-5228-v3.patch,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2013-02-08 08:52:54.316,,,no_permission,,,,,,,,,,,,311774,,,Fri Mar 22 10:43:15 UTC 2013,,,,,,0|i1hsyv:,312120,slebresne,slebresne,,,,,,,,,,"07/Feb/13 15:33;jbellis;I'm skeptical that this would be useful with our general-purpose compaction strategies.  Certainly LCS and probably STCS as well are good enough at combining newer data with old that we'd end up with a mix of expirations in most sstables.

But, if we have an append-mostly workload of ttl data, we could create a separate compaction strategy that doesn't bother merging sstables, just throws out expired ones (and relies on the bloom filters until then to avoid checking too many sstables on reads).","08/Feb/13 08:52;slebresne;bq. I'm skeptical that this would be useful with our general-purpose compaction strategies

For the record, I don't fully share that pessimism :). Just to say that imo it's worth separating what the title says (and doing that first) from creating a new compaction strategy. Could be you didn't intended otherwise anyway but just wanted to chime in.","08/Feb/13 15:53;carlyeks;I think this would really help with the Hints cf. When all of the hints in an sstable have been handed off, then we can easily delete the whole SSTable rather than running a compaction.

Also, think we need to track maximum ttl.","13/Feb/13 19:00;jbellis;Hints is a different case; I'd rather re-code the delivery mechanism to operate sstable-at-a-time, than keep them around until the TTL expires (which is usually much, much longer than ""until we deliver it"").

You're right about wanting to track max ttl; edited.","20/Feb/13 19:50;batalbot;It sounds like this would be an ""Age Tiered Compaction Strategy""?

One of our busiest use cases involves logging performance data with the same TTL for all columns of a row.  The row is never updated once all columns are written to once.  For example, metrics per-second might be kept for 3 days, metrics per-minute kept for 3 weeks, and metrics per-hour or day kept longer.  It would be great to avoid all the excess compaction IO work on rows that never change and haven't expired yet.  This sounds like it would greatly extend the performance of cassandra (by reducing compaction-IO overhead) for us and many other similar workloads.
","06/Mar/13 09:40;krummas;first attempt at a first part that just tracks max ttl on sstables, figured this ticket could be split in 2 patches

next part will drop sstables after maxTimestamp + maxTTL + gc_grace_seconds - we need to take gc_grace in account here since deletes don't have a ttl, meaning we could drop tombstones too early. (another approach might be to say that DeletedColumn TTL is gc_grace_seconds?)","06/Mar/13 10:11;slebresne;bq. another approach might be to say that DeletedColumn TTL is gc_grace_seconds?

Kind of, but I wouldn't put it that way exactly. I don't think we should track ttl per-se, but rather the maximum time at which all columns in the sstable are deleted. I.e. we should track the max localDeletionTime (which is MAX_VALUE for normal columns, which is what we want). So there shouldn't be any special casing of tombstone versus expired column in that ticket in that the Colum.getLocalDeletionTime() does the right thing already.

And thus dropping sstable should just be checking if current_time > max(localDeletionTime). Though for the record, I note that having all column GCable is *not* sufficient to drop the sstable, we'd typically need to check that the sstable maxTimestamp is smaller than all other sstable minTimestamp. Otherwise, we might drop tombstone (or expiring columns for that matter) that are GCable but shard another column in another sstable.","06/Mar/13 12:14;krummas;ah nice, that will make this patch a lot cleaner, will fix","07/Mar/13 13:06;krummas;track max local deletion time instead

again, only tracks the information, does not yet use it","07/Mar/13 14:13;slebresne;I've only skimmed over the patch quickly, but while that looks fine on principle, one thing that is not ok is:
{noformat}
+            int maxLocalDeletionTime = desc.version.tracksMaxLocalDeletionTime ? dis.readInt() : Integer.MIN_VALUE;
{noformat}
If !tracksMaxLocalDeletionTime, this should return Integer.MAX_VALUE, because MIN_VALUE means ""everything in that sstable is GCable"".
","07/Mar/13 14:19;krummas;nice catch, old maxttl-patch leftover","11/Mar/13 15:14;krummas;adds a nodetool command to expire entire sstables if ttl has expired (and maxtimestamp is less than mintimestamp of all other sstables)

applies on top of the v3 track max local deletiontime patch.

unsure if doing this as a nodetool command is the right way, suggestions where to put if not as a nodetool command it are welcome","12/Mar/13 16:53;slebresne;I suspect we'd rather make that automatic rather a nodetool command if possible. In fact, we could do that for every compaction. Typically, when we create the compaction iterator, we could skip sstables that can be dropped fully (but still consider them as compacted).

We may also want to tweak slightly AbstractCompactionStrategy.worthDroppingTombstones so it return yes if the maxLocalDeletion time is less than gcBefore (though the current version should work reasonably well already).","13/Mar/13 20:00;krummas;tries to drop sstables in CompactionTask#runWith

also checks if it is possible to drop sstable in AbstractCompactionStrategy#worthDroppingTombstones

still applies on top of v3 of the max deletiontime patch","21/Mar/13 15:55;slebresne;* In SSTableMetadata default ctor, we need to use MAX_VALUE, not MIN_VALUE, same as when we read metadata that don't track the deletion time.
* Nit: In the Descriptor version, it uses version ""ic"" but trunk version is now ""ja"".
* Nit: I don't know why CompactionController don't already keep a reference to the compacted sstable, but let's do it. It's weird to have to pass it to getTTLExpiredSSTables even though CompactionController has been created with the sstables in the first place.
* In CompactionTask, we could use actuallyCompact for the getPreheatKeyCache branch. But we shouldn't use it in the createCompactionWriter call.
* Concerning getTTLExpiredSSTables:
** The {{candidate.getMaxTimestamp() > minTimestamp}} check should use a '>=' because tombstone wins over normal insert on a timestamp tie.
** we should pass gcBefore as argument for the static version, and use the one of the controller for the non-static version, rather than recomputing it from scratch.
** Nit: Let's rename the method to say getFullyExpiredSSTables (or maybe getDroppableSSTables). This is not TTL specific, but about gcable tombstones (that might not come from TTL).
** I don't think it's ok to drop sstables without having done the min_timestamp check, unless overlapping is empty (and I'm not sure it's worth special casing). Overall, I find the method a bit hard to follow. I would suggest a slightly refactored version like:
{noformat}
List<SSTableReader> candidates = new ArrayList<SSTableReader>();
long minTimestamp = Integer.MAX_VALUE;

for (SSTableReader sstable : overlapping)
    minTimestamp = Math.min(minTimestamp, sstable.getMinTimestamp());

for (SSTableReader candidate : compacting)
{
    if (candidate.maxLocalDeletionTime() < gcBefore)
        candidates.add(candidate);
    else
        minTimestamp = Math.min(minTimestamp, sstable.getMinTimestamp());
}

// we still need to keep candidates that might shadow something in a
// non-candidate sstable. And if we remove a sstable from the candidates, we
// must take it's timestamp into account (hence the sorting below).
Collections.sort(candidates, SSTable.maxTimestampComparator);

Iterator<SSTableReader> iterator = candidates.iterator();
while (iterator.hasNext())
{
    SSTableReader candidate = iterator.next();
    if (candidate.getMaxTimestamp() >= minTimestamp)
    {
        minTimestamp = Math.min(candidate.getMinTimestamp(), minTimestamp);
        iterator.remove();
    }
    else
    {
        logger.debug(""Dropping TTL Expired SSTable {} (maxLocalDeletionTime={}, localTime={})"",
                candidate, candidate.getSSTableMetadata().maxLocalDeletionTime, localTimeSeconds);
    }
}
return new HashSet<SSTableReader>(candidates);
{noformat}
","22/Mar/13 07:35;krummas;attaching a rebased track local deletiontime patch as well

the updated drop-sstable patch addresses the comments and:
* fixes the shaky mutate sstablelevel unit test that seemed to break 50% of the times i ran it
* updates current_version to ""ja"" in Descriptor
* fixes the BootstrapTest which needs a current-version filename to pass","22/Mar/13 10:43;slebresne;Alright, lgtm. Committed, thanks.

Note that I do think we might want to be more aggressive in checking when there is droppable sstable. Currently, we do it as part of worthDroppingTombstone, which is fine, but that in turn is only checked if there is no other compaction to do. And we might want to check if there is fully droppable sstable before we check for other compactions, though that check wouldn't be totally free so it's unclear if it's desirable in general. Anyway, this a matter for a later ticket anyway (an alternative being the specific compaction strategy suggested by Jonathan, that would not really compact, just use the droppable check). ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set operation mode to MOVING earlier,CASSANDRA-4252,12556136,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,nickmbailey,nickmbailey,16/May/12 22:15,12/Mar/19 13:58,13/Mar/19 22:27,18/May/12 16:33,1.1.1,,,,,0,,,,,,,Right now when moving a node we set the OperationMode only once we've calculated the necessary ranges to transfer and if there actually are ranges to transfer. Due to the sleep for ring settling this means there are 30 seconds where the node is moving but the operation mode isn't set accordingly. Additionally if it turns out no data needs to be transferred then the move will complete without ever switching the OperationMode to moving.,,,,,,,,,,,,,,,,,,,17/May/12 22:50;jbellis;4252.txt;https://issues.apache.org/jira/secure/attachment/12527946/4252.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-17 22:01:02.147,,,no_permission,,,,,,,,,,,,255994,,,Fri May 18 16:33:42 UTC 2012,,,,,,0|i0gtrb:,96251,nickmbailey,nickmbailey,,,,,,,,,,17/May/12 22:01;jbellis;What difference does it make as long as we set MOVING before streamRanges / requestRanges?,"17/May/12 22:21;nickmbailey;From a monitoring application perspective its just nicer to set the mode immediately so that when a move is initiated the operation mode of the node changes immediately rather than waiting 30ish seconds. Also so in the admittedly uncommon case of no data being transferred the operation mode gets set to moving at all, which it currently won't.","17/May/12 22:33;jbellis;bq. just nicer to set the mode immediately so that when a move is initiated the operation mode of the node changes immediately rather than waiting 30ish seconds

Fair enough.

bq. in the admittedly uncommon case of no data being transferred the operation mode gets set to moving at all

I think it's inside that check to avoid logging ""fetching new ranges and streaming old ranges"" when there's nothing to stream, which is also misleading.

Since we don't ""push"" modes I don't think there's any difference between not setting it at all, and setting it only to set back to NORMAL a nanosecond later.","17/May/12 22:42;nickmbailey;Ah you are right. I was under the impression we did the RING_DELAY sleep no matter what, but apparently not.

Regarding the 'fetching new ranges...' log message, we can call setMode multiple times (a pattern we use for other modes). So we can just call setMode earlier on with a more accurate log message in addition to the later call.",17/May/12 22:50;jbellis;Patch attached.,18/May/12 16:15;nickmbailey;+1,18/May/12 16:33;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix sstable blacklisting for LCS,CASSANDRA-4343,12560701,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,14/Jun/12 23:56,12/Mar/19 13:58,13/Mar/19 22:27,15/Jun/12 17:28,1.1.2,,,,,0,compaction,lcs,,,,,,,,,,,,,,,,,,,,,,,,15/Jun/12 01:08;jbellis;4343.txt;https://issues.apache.org/jira/secure/attachment/12532151/4343.txt,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-15 16:48:26.177,,,no_permission,,,,,,,,,,,,256078,,,Fri Jun 15 17:28:06 UTC 2012,,,,,,0|i0gusv:,96420,yukim,yukim,,,,,,,,,,"15/Jun/12 01:08;jbellis;CompactionsTest failed after CASSANDRA-4341 but the cause was an existing bug in the blacklisting code.

Consider L0 sstables A and B and L1 sstable C.  A and B overlap with C, and C is suspect/blacklisted.  getCandidatesFor will return {A, B, C}, and getCompactionCandidates will then remove C from the candidate list.  A and B will be compacted to D in L1, so L1 will now contain two overlapping sstables C and D.

Patch to fix by check blacklist in getCandidatesFor.  If an overlapping sstable is suspect, we'll proceed to the next possible range instead of doing an incorrect compaction.  Overlapping suspect sstables in L0 are also handled.","15/Jun/12 16:48;yukim;+1 (I didn't see any test failure without patch, though.)",15/Jun/12 17:28;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
