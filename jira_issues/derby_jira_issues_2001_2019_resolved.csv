Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Reference),Outward issue link (Reference),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bug behavior facts),Custom field (Bug behavior facts),Custom field (Bug behavior facts),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue URL),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Issue & fix info),Custom field (Issue & fix info),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Severity),Custom field (Severity),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Urgency),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
The default Network Server security policy file could be trimmed down somewhat.,DERBY-6987,13146085,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,bryanpendleton,bryanpendleton,bryanpendleton,18/Mar/18 23:49,25/Mar/18 15:57,13/Mar/19 22:39,19/Mar/18 00:55,10.14.1.0,10.14.2.0,,Network Server,,,0,,"As described here (https://db.apache.org/derby/docs/10.14/security/rsecnetservbasic.html), if you start the Network Server without specifying a security manager, the Network Server will install a default Java security manager that enforces a basic policy.

This basic security policy could be trimmed down and made simpler.

Users who desire a more complex and sophisticated Network Server security policy already have the ability to provide one, as described here (https://db.apache.org/derby/docs/10.14/security/csecjavasecurity.html), so trimming down the basic security policy does not affect such deployments.",,,,,,,,,,,,,19/Mar/18 00:45;bryanpendleton;releaseNote.html;https://issues.apache.org/jira/secure/attachment/12915079/releaseNote.html,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,9223372036854775807,,,Release Note Needed,,2018-03-18 23:49:53.0,,,,,,0|i3rgtj:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Network Server COMMAND_TESTCONNECTION need not try to open a database,DERBY-6986,13146084,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,bryanpendleton,bryanpendleton,bryanpendleton,18/Mar/18 23:45,25/Mar/18 15:57,13/Mar/19 22:42,19/Mar/18 00:56,10.14.1.0,10.14.2.0,,Network Server,,,0,,"The Network Server has a special protocol which can be used to send it commands (e.g., ""stop"", ""ping"", ""sysinfo"", etc.). The ""ping"" command, which internally is known in the code as COMMAND_TESTCONNECTION, allows a custom client program to request that the Network Server attempt to open a Derby database specified as an argument in the COMMAND_TESTCONNECTION packet.

This is very old code, probably included during the initial development of the Network Server as a development-time diagnostic tool, and is not required for any current Network Server functionality, so it should be removed.",,,,,,,,,,,,,19/Mar/18 00:38;bryanpendleton;releaseNote.html;https://issues.apache.org/jira/secure/attachment/12915078/releaseNote.html,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,9223372036854775807,,,Release Note Needed,,2018-03-18 23:45:27.0,,,,,,0|i3rgtb:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQLSTATE: XJ001, SQLERRMC: java.lang.NullPointerExceptionXJ001.U",DERBY-6981,13134953,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,bryanpendleton,masterchief24,masterchief24,31/Jan/18 02:07,08/Feb/18 02:50,13/Mar/19 22:42,08/Feb/18 02:50,10.12.1.1,10.15.1.3,,JDBC,,,0,newbie,"Original error mesage:

 
{code:java}
java.sql.SQLException: DERBY SQL error: ERRORCODE: 0, SQLSTATE: XJ001, SQLERRMC: java.lang.NullPointerExceptionXJ001.U at org.apache.derby.client.am.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.client.am.SqlException.getSQLException(Unknown Source) at org.apache.derby.client.am.ClientResultSet.next(Unknown Source) at org.ziptie.provider.credentials.internal.test1.main(test1.java:43) Caused by: ERROR XJ001: DERBY SQL error: ERRORCODE: 0, SQLSTATE: XJ001, SQLERRMC: java.lang.NullPointerExceptionXJ001.U at org.apache.derby.client.am.ClientResultSet.completeSqlca(Unknown Source) at org.apache.derby.client.net.NetResultSetReply.parseFetchError(Unknown Source) at org.apache.derby.client.net.NetResultSetReply.parseCNTQRYreply(Unknown Source) at org.apache.derby.client.net.NetResultSetReply.readFetch(Unknown Source) at org.apache.derby.client.net.ResultSetReply.readFetch(Unknown Source) at org.apache.derby.client.net.NetResultSet.readFetch_(Unknown Source) at org.apache.derby.client.net.NetResultSet.flowFetch(Unknown Source) at org.apache.derby.client.net.NetCursor.getMoreData_(Unknown Source) at org.apache.derby.client.am.Cursor.stepNext(Unknown Source) at org.apache.derby.client.am.Cursor.next(Unknown Source) at org.apache.derby.client.am.ClientResultSet.nextX(Unknown Source)
{code}
DERBY-6735 is very similar bug.

DDL

 
{code:java}
CREATE TABLE TEST1
(
ID int PRIMARY KEY NOT NULL,
LASTUPDATE timestamp
){code}
 

I have attached test Code:[^derby-test.java]

test Code with embeded derby : [^Main.java]

^Full code on my intellij : [^Test1.zip]:^

Another database (PostgreSQL) has no problem.

I also put a workaround in the code.","OSX,CentOS",,,,,,,,,,,,31/Jan/18 06:37;masterchief24;Main.java;https://issues.apache.org/jira/secure/attachment/12908510/Main.java,31/Jan/18 06:39;masterchief24;Test1.zip;https://issues.apache.org/jira/secure/attachment/12908511/Test1.zip,31/Jan/18 06:34;masterchief24;derby-test.java;https://issues.apache.org/jira/secure/attachment/12908509/derby-test.java,01/Feb/18 05:32;bryanpendleton;patch1.diff;https://issues.apache.org/jira/secure/attachment/12908723/patch1.diff,31/Jan/18 05:24;bryanpendleton;test1.java;https://issues.apache.org/jira/secure/attachment/12908498/test1.java,,,,,5.0,,,,,,,,,,,,Crash,Performance,,,,,,,,,2018-01-31 05:26:35.628,,,no_permission,,,,,,,,,,,,9223372036854775807,,,Newcomer,Workaround attached,Thu Feb 08 02:50:05 UTC 2018,,,,,,0|i3pknj:,9223372036854775807,,,,,,,,,"31/Jan/18 05:26;bryanpendleton;I took the provided derby-test.java and tweaked it slightly. I ran it as follows:

 

    javac -cp .;\users\bryan\derby\trunk\classes test1.java

    java -cp .;\users\bryan\derby\trunk\classes test1

 

It produced the following output:

 

Insert 3
3: 1517299200000 -> 1517376190493

 

I'm not really sure what the program was supposed to do, but it didn't produce a NullPointerException for me.

 

However, an important difference is that I ran the program in Embedded mode, not in Client/Server mode.","31/Jan/18 06:43;masterchief24;Hi. I have attached the version of EmbeddedDriver for easy testing.
 Test results are the same between Server/Client and EmbeddedDriver.","31/Jan/18 14:36;bryanpendleton;Yes, it reproduces for me now. I see the following in my derby.log:

 

Wed Jan 31 06:35:24 PST 2018 Thread[main,5,main] (XID = 212), (SESSIONID = 1), (DATABASE = test), (DRDAID = null), Faile
d Statement is: SELECT id, LASTUPDATE FROM test1 WHERE id = ? FOR UPDATE OF id, LASTUPDATE with 1 parameters begin parameter #1: 3 :end parameter 
java.lang.NullPointerException
 at org.apache.derby.iapi.store.access.BackingStoreHashtable.remove(BackingStoreHashtable.java:968)
 at org.apache.derby.impl.sql.execute.TableScanResultSet.getNextRowCore(TableScanResultSet.java:520)
 at org.apache.derby.impl.sql.execute.IndexRowToBaseRowResultSet.getNextRowCore(IndexRowToBaseRowResultSet.java:346)
 at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.getNextRow(BasicNoPutResultSetImpl.java:488)
 at org.apache.derby.impl.jdbc.EmbedResultSet.movePosition(EmbedResultSet.java:450)
 at org.apache.derby.impl.jdbc.EmbedResultSet.next(EmbedResultSet.java:394)
 at Main.main(Main.java:123)","01/Feb/18 04:53;bryanpendleton;I don't fully understand what's going wrong here, but I (sort of) know how to fix it.

The problem involves:
 # The use of a CONCUR_UPDATABLE result set and a SELECT ... FOR UPDATE statement, which triggers some tricky code in TableScanResultSet
 # The fact that we execute the query multiple times, closing and reopening result sets on the same prepared statement
 # A bug (I think) in the TableScanResultSet.close() logic.

 

The tricky code in TableScanResultSet is very very old code:
{code:java}
/** 
 * This field is used by beetle 3865, updateable cursor using index. It 
 * is a hash table containing updated rows that are thrown into future 
 * direction of the index scan, and as a result we'll hit it again but 
 * should skip it. The hash table will spill to disk if it grows too big 
 * to be kept in memory. 
 */ 
protected BackingStoreHashtable past2FutureTbl;
{code}
And the bug (I think) in TableScanResultSet.close() is that when the past2FutureTbl object is lazy-initialized, so when we close() it, we also have to null out the variable, so that the next time we reopen this result set, we will re-allocate and re-initialize a new past2FutureTbl hashtable.

That is, I think the fix is:
{code:java}
Index: java/engine/org/apache/derby/impl/sql/execute/TableScanResultSet.java =================================================================== 
--- java/engine/org/apache/derby/impl/sql/execute/TableScanResultSet.java (revision 1805254) 
+++ java/engine/org/apache/derby/impl/sql/execute/TableScanResultSet.java (working copy) 
@@ -625,6 +625,7 @@
 if (past2FutureTbl != null)
 { 
past2FutureTbl.close(); 
+ past2FutureTbl = null;
 }
 }
 else
{code}
 

I haven't run any real tests on this, just verified that if I null-out past2FutureTbl at this point, this particular test case passes.

 

Rick, what do you think?","01/Feb/18 05:34;bryanpendleton;Attached 'patch1.diff' is a patch proposal, containing the proposed change to TableScanResultSet.close as well as the reproduction script from this issue, refactored into the UpdatableResultSetTest suite.

The test case fails without the change to TableScanResultSet.java, and passes with it.

I haven't done any other testing (my environment is a bit flaky right now).

If this patch proposal seems like a reasonable path forward, I'll try to run more tests.","01/Feb/18 20:40;rhillegas;Hi Bryan,

Thanks for digging into this. Your analysis sounds solid to me. It definitely looks like a bug to close the hashtable without nulling out that variable immediately afterward. Thanks.","03/Feb/18 16:09;jira-bot;Commit 1823037 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1823037 ]

DERBY-6981: NullPointerException when re-executing PreparedStatement query.

TableScanResultSet's past2FutureTbl is a hash table containing updated
rows that are thrown into the future direction of the index scan, so
that the scan knows it's seen these rows already and should skip them
subsequently.

When the TableScanResultSet.close() method was called, it was closing the
past2FutureTbl, but not clearing the pointer, which caused the lazy
initialization of the past2FutureTbl to be incorrectly performed the
next time the same TableScanResultSet was opened and scanned, resulting
in the NullPointerException in the underlying BackingStoreHashtable code
in that second scan.

The fix is to clear the old instance and freshly initialize a new instance,
each time the TableScanResultSet is closed and reopened.","08/Feb/18 02:50;bryanpendleton;Thanks Rick for the review! I'm not planning any additional work at this time, resolving.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SET CYCLE fails to let an identity column cycle if the range is already exhausted,DERBY-6961,13101837,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,rhillegas,rhillegas,rhillegas,13/Sep/17 02:15,18/Sep/17 02:56,13/Mar/19 22:42,18/Sep/17 01:31,10.14.1.0,10.14.1.0,,SQL,,,0,,"If a NO CYCLE identity column exhausts its range, then...

  ALTER TABLE ALTER COLUMN $columnName SET CYCLE

...will not revive the identity column. No more rows can be inserted into the table. This violates the SQL Standard and is very surprising behavior after the ALTER TABLE command completed successfully.

The problem is that the exhausted sequence generator has a next value of NULL, signifying that it is done. After the ALTER TABLE command, the next value of the sequence generator should be the minimum value (for an ascending sequence generator) or the maximum value (for a descending sequence generator) according to the 2016 SQL Standard, section 4.27.2 (Operations involving sequence generators), quoted here in full:

""When a <next value expression> is applied to a sequence generator SG, SG issues a value V taken from SG's current cycle such that V is expressible as the current base value of SG plus N multiplied by the increment of SG, where N is a non-negative number.

Thus a sequence generator will normally issue all of the values in its cycle and these will normally be in increasing or decreasing order (depending on the sign of the increment) but within that general ordering separate subgroups of ordered values may occur.

If the sequence generator's cycle is exhausted (i.e., it cannot issue a value that meets the criteria), then a new cycle is created with the current base value set to the minimum value of SG (if SG is an ascending sequence generator) or the maximum value of SG (if SG is a descending sequence generator).

If a new cycle is created and the descriptor of SG includes NOCYCLE, then an exception condition is raised.

If there are multiple instances of <next value expression>s specifying the same sequence generator within a single SQL-statement, all those instances return the same value for a given row processed by that SQL-statement.""


The following script shows this problem:

{noformat}
connect 'jdbc:derby:memory:db;create=true';

------------------------------------------------
--
-- Exhaust a NO CYCLE identity column.
-- SET CYCLE does not allow the sequence generator
-- to continue processing.
--
------------------------------------------------

create table t_noCycleExhaust(a int generated always as identity (start with 2147483646 no cycle), b int);
values syscs_util.syscs_peek_at_identity('APP', 'T_NOCYCLEEXHAUST');

insert into t_noCycleExhaust(b) values (1);
values syscs_util.syscs_peek_at_identity('APP', 'T_NOCYCLEEXHAUST');
insert into t_noCycleExhaust(b) values (2);
-- the sequence generator has NULL as its next value
values syscs_util.syscs_peek_at_identity('APP', 'T_NOCYCLEEXHAUST');

-- should fail
insert into t_noCycleExhaust(b) values (3);
values syscs_util.syscs_peek_at_identity('APP', 'T_NOCYCLEEXHAUST');
select * from t_noCycleExhaust order by b;

alter table t_noCycleExhaust alter column a set cycle;
-- the sequence generator still has NULL as its next value. this is the bug.
values syscs_util.syscs_peek_at_identity('APP', 'T_NOCYCLEEXHAUST');

-- incorrectly fails
insert into t_noCycleExhaust(b) values (3);
values syscs_util.syscs_peek_at_identity('APP', 'T_NOCYCLEEXHAUST');
select * from t_noCycleExhaust order by b;


------------------------------------------------
--
-- Do NOT exhaust a NO CYCLE identity column.
-- Then SET CYCLE. The sequence generator will
-- wrap around.
--
------------------------------------------------

create table t_noCycleDoNotExhaust(a int generated always as identity (start with 2147483646 no cycle), b int);
values syscs_util.syscs_peek_at_identity('APP', 'T_NOCYCLEDONOTEXHAUST');

insert into t_noCycleDoNotExhaust(b) values (1);
values syscs_util.syscs_peek_at_identity('APP', 'T_NOCYCLEDONOTEXHAUST');

alter table t_noCycleDoNotExhaust alter column a set cycle;
values syscs_util.syscs_peek_at_identity('APP', 'T_NOCYCLEDONOTEXHAUST');

insert into t_noCycleDoNotExhaust(b) values (2);
values syscs_util.syscs_peek_at_identity('APP', 'T_NOCYCLEDONOTEXHAUST');
insert into t_noCycleDoNotExhaust(b) values (3);
values syscs_util.syscs_peek_at_identity('APP', 'T_NOCYCLEDONOTEXHAUST');
select * from t_noCycleDoNotExhaust order by b;
{noformat}
",,,,,,,,,,DERBY-6904,,,17/Sep/17 23:38;rhillegas;derby-6961-01-aa-recyclingExhaustedIdentityColumns.diff;https://issues.apache.org/jira/secure/attachment/12887573/derby-6961-01-aa-recyclingExhaustedIdentityColumns.diff,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,2017-09-13 03:00:24.425,,,no_permission,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 18 02:56:34 UTC 2017,,,,,,0|i3jzyn:,9223372036854775807,,,,,,,,Blocker,"13/Sep/17 03:00;bryanpendleton;Thanks for catching this, Rick.

I definitely don't think this was intentional behavior. I think we just missed it during the implementation and testing.

Does it seem hard to fix?
","14/Sep/17 00:51;rhillegas;Hey Bryan,

I haven't looked at the code. But I think that it should be easy to fix. The underlying sequence generator needs to be queried, the value in the system tables needs to be updated, and the sequence generator needs to be destroyed so that it can be recreated after the DBA commits the transaction which runs the ALTER TABLE command.

Thanks,
-Rick","14/Sep/17 01:21;rhillegas;Here's another, similar problem: If a CYCLE identity column is on the verge of wrapping around and then the column is ALTERed to NO CYCLE, then the next insert into the table should fail. But that insert succeeds, incorrectly. The following script shows this:

{noformat}
connect 'jdbc:derby:memory:db;create=true';

create table t1(a int generated always as identity (start with 2147483646 cycle), b int);
insert into t1(b) values (1);
alter table t1 alter column a set no cycle;
insert into t1(b) values (2);
-- fails as expected
insert into t1(b) values (3);

select * from t1 order by b;

------

create table t2(a int generated always as identity (start with 2147483646 cycle), b int);
insert into t2(b) values (1);
insert into t2(b) values (2);
alter table t2 alter column a set no cycle;

-- should fail but does not
insert into t2(b) values (3);

select * from t2 order by b;
{noformat}","17/Sep/17 23:39;rhillegas;Attaching derby-6961-01-aa-recyclingExhaustedIdentityColumns.diff. This patch addresses both of the problems scripted above. I am running full tests now.

Touches the following files:

---------------------------

M       java/engine/org/apache/derby/impl/sql/execute/AlterTableConstantAction.java

Correctly handle ALTER TABLE...SET [NO] CYCLE for exhausted identity columns which are at their rollover point.

---------------------------

M       java/testing/org/apache/derbyTesting/functionTests/tests/lang/AlterTableTest.java

Tests.
",18/Sep/17 01:29;rhillegas;Tests passed cleanly on derby-6961-01-aa-recyclingExhaustedIdentityColumns.diff.,"18/Sep/17 01:31;jira-bot;Commit 1808668 from [~rhillegas] in branch 'code/trunk'
[ https://svn.apache.org/r1808668 ]

DERBY-6961: Correctly handle ALTER TABLE...SET [NO] CYCLE on exhausted identity columns; commit derby-6961-01-aa-recyclingExhaustedIdentityColumns.diff.",18/Sep/17 02:56;bryanpendleton;Thanks Rick! It's good to see that it was more-or-less as straightforward as you were hoping.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE from trigger,DERBY-6726,12738827,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,bryanpendleton,tdudgeon,tdudgeon,03/Sep/14 15:40,16/Sep/17 19:50,13/Mar/19 22:42,02/Oct/16 17:05,10.10.1.1,10.12.1.2,10.13.1.0,SQL,,,1,,"Saw this strange exception when doing an insert to a table with a trigger
{code}
Tue Sep 02 13:39:09 BST 2014 Thread[SQLExecution,1,system] (XID = 62693), (SESSIONID = 1), (DATABASE = C:/Users/timbo/Documents/IJCProjects/mini-regs/Vanilla Oracle/.config/derby-minireg-01-sep/db), (DRDAID = null), Failed Statement is: UPDATE samples SET sample_code = 'S123456' WHERE sample_id = CAST (org.apache.derby.iapi.db.Factory::getTriggerExecutionContext().getNewRow().getObject(1) AS INTEGER)
java.lang.NullPointerException
    at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.getTriggerActionString(Unknown Source)
    at org.apache.derby.iapi.sql.dictionary.TriggerDescriptor.getActionSPS(Unknown Source)
    at org.apache.derby.impl.sql.execute.GenericTriggerExecutor.getAction(Unknown Source)
    at org.apache.derby.impl.sql.execute.RowTriggerExecutor.fireTrigger(Unknown Source)
    at org.apache.derby.impl.sql.execute.TriggerEventActivator.notifyEvent(Unknown Source)
    at org.apache.derby.impl.sql.execute.UpdateResultSet.fireAfterTriggers(Unknown Source)
    at org.apache.derby.impl.sql.execute.UpdateResultSet.open(Unknown Source)
    at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
    at org.apache.derby.impl.sql.GenericPreparedStatement.executeSubStatement(Unknown Source)
    at org.apache.derby.impl.sql.execute.GenericTriggerExecutor.executeSPS(Unknown Source)
    at org.apache.derby.impl.sql.execute.RowTriggerExecutor.fireTrigger(Unknown Source)
    at org.apache.derby.impl.sql.execute.TriggerEventActivator.notifyEvent(Unknown Source)
    at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source)
    at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
    at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
    at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
    at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
    at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
    at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
{code}

The trigger definition is this:
{code}
CREATE TRIGGER samples_code_trg
AFTER INSERT ON samples
REFERENCING NEW AS newrow FOR EACH ROW MODE DB2SQL
UPDATE samples SET sample_code = 'S123456'
WHERE samples.sample_id = newrow.sample_id;
{code}

As mentioned here: http://mail-archives.apache.org/mod_mbox/db-derby-user/201408.mbox/%3Cltq5hl$kps$1@ger.gmane.org%3E
it could be that its caused by another AFTER UPDATE trigger that's on the table.

Unfortunately I rebuilt all the tables and triggers and not the problem doesn't happen, so I can't provide a test case.

",,,,,,,,,,,,,29/Sep/16 02:28;bryanpendleton;TriggerTest.diff;https://issues.apache.org/jira/secure/attachment/12830836/TriggerTest.diff,28/Sep/16 19:01;bruehlicke;derbytrig.zip;https://issues.apache.org/jira/secure/attachment/12830768/derbytrig.zip,01/Oct/16 02:10;bryanpendleton;fixesRepro.diff;https://issues.apache.org/jira/secure/attachment/12831207/fixesRepro.diff,01/Oct/16 14:20;bryanpendleton;getTableDescriptor.diff;https://issues.apache.org/jira/secure/attachment/12831224/getTableDescriptor.diff,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,2014-09-03 22:10:43.422,,,no_permission,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 02 17:05:53 UTC 2016,,,,,,0|i1zmsv:,9223372036854775807,,,,,,,,,"03/Sep/14 22:10;dagw;Working from the DDL shown, I have not been able to reproduce this on trunk yet.","28/Sep/16 02:38;bruehlicke;Just had similar stack trace when using derby 10.8.1.2 or newer. Works fine with 10.7.1.1 or below.  Fails on an UPDATE trigger. I am working on a test app to reproduce the error.

",28/Sep/16 19:01;bruehlicke;NetBeans 8.2RC project with a single JUnit test showing this error when using 10.8.1.2.  Replacing the jars with 10.7.1.1 will make the code working. Using any version higher than 10.8 will also fail.,"28/Sep/16 19:01;bruehlicke;I was able to crystallize out the code causing the error in a JUNIT 4 test. Attached a NetBeans 8.2RC project with all setup causing the error. The README.txt file also tells how to make the error go away, i.e. which lines in the code causing it.

I hope this will help to find the problem. The code was tested with JDK 1.8.0_102 64 bit.","28/Sep/16 19:02;bruehlicke;The Netbeans project is is the file ""derbytrig.zip""","29/Sep/16 02:28;bryanpendleton;Thank you for the test case. It reproduces the problem for me!

Attached 'TriggerTest.diff' is your same test case (I believe),
edited into the format used by the Derby regression test suites.

When I run the modified TriggerTest on the current Derby trunk, I get:

Caused by: java.lang.NullPointerException
	at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.examineTriggerNodeAndCols(DataDictionaryImpl.java:4773)
	at org.apache.derby.iapi.sql.dictionary.TriggerDescriptor.getSPS(TriggerDescriptor.java:407)
	at org.apache.derby.iapi.sql.dictionary.TriggerDescriptor.getActionSPS(TriggerDescriptor.java:339)
	at org.apache.derby.impl.sql.execute.GenericTriggerExecutor.getAction(GenericTriggerExecutor.java:120)
	at org.apache.derby.impl.sql.execute.GenericTriggerExecutor.executeWhenClauseAndAction(GenericTriggerExecutor.java:346)
	at org.apache.derby.impl.sql.execute.RowTriggerExecutor.fireTrigger(RowTriggerExecutor.java:113)
	at org.apache.derby.impl.sql.execute.TriggerEventActivator.notifyEvent(TriggerEventActivator.java:272)
	at org.apache.derby.impl.sql.execute.UpdateResultSet.fireAfterTriggers(UpdateResultSet.java:870)
	at org.apache.derby.impl.sql.execute.UpdateResultSet.open(UpdateResultSet.java:289)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(GenericPreparedStatement.java:472)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeSubStatement(GenericPreparedStatement.java:338)
	at org.apache.derby.impl.sql.execute.GenericTriggerExecutor.executeSPS(GenericTriggerExecutor.java:216)
	at org.apache.derby.impl.sql.execute.GenericTriggerExecutor.executeWhenClauseAndAction(GenericTriggerExecutor.java:346)
	at org.apache.derby.impl.sql.execute.RowTriggerExecutor.fireTrigger(RowTriggerExecutor.java:113)
	at org.apache.derby.impl.sql.execute.TriggerEventActivator.notifyEvent(TriggerEventActivator.java:272)
	at org.apache.derby.impl.sql.execute.UpdateResultSet.fireAfterTriggers(UpdateResultSet.java:870)
	at org.apache.derby.impl.sql.execute.UpdateResultSet.open(UpdateResultSet.java:289)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(GenericPreparedStatement.java:472)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(GenericPreparedStatement.java:351)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1339)
","29/Sep/16 13:58;bruehlicke;Super ! Thanx, it is always great to hear if a problem can be reproduced. This stack trace I get using 10.12.1.1. When using 10.8.1.2 - 10.11.1.1 a get a slightly different and with 10.7.1.1 and lower version, all works perfect without any error.","29/Sep/16 20:39;bruehlicke;Trying to understand more on this and going through the Derby code, it looks to me that the initial guess of ""it could be that its caused by another AFTER UPDATE trigger that's on the table.""  could be a guess in the right direction. In the test I provided there are 2 AFTER UPDATE triggers - both on the same table. Disabling any of them makes the code work - but having both make the code fail.

","30/Sep/16 02:34;bryanpendleton;That seems like a very important insight, thanks. It seems to me that this
indicates that the TriggerEventActivator is possibly the important class
to investigate, as that appears to be the frame in the call stack where
the different triggers are being tracked, ordered, and scheduled for execution.","30/Sep/16 14:02;bruehlicke;I have tracked down the problem,

First I was comparing sources changes between 7.1.1 and 8.1.2 to reduce amount of source to be checked and as I know it worked in 7.1.1. and failed in 8.1.2.  GenericTriggerExecutor, RowTriggerExecutor. TriggerEventActivator, UpdateResultSet and GenericPreparedStatement where the same - so no problem should be in those.

The problem happens in :   iapi/sql/dictionary/TriggerDescriptor in the method getActionSPS.  (In 12.1.1. this is getSPS).

In 8.1.2 code was added to the method as part of DERBY-4874. When I comment out this extra code in the 8.1.2 code base - the Test works !  I do not currently understand what this extra code is doing, but it is for sure the culprit of the problem.

Now looking at the 12.1.1 code we have the same or similar code in TriggerDescriptor.getSPS(...). Again, commenting out the addition made for DERBY-4874 and just have it do the same as back in the 7.1.1 days - the code works also on 12.1.1

I will try to understand what the getSPS(..) is trying to do and why it falls over having a double set of AFTER UPDATE triggers, but maybe someone closer to the code can help with ideas as the problem is now pinned down to a few lines of source.



I will now go into the 12.1.1 code base and try to understand the logic here ","30/Sep/16 16:56;bruehlicke;Looks like the rabbit hole goes deeper ... looks like the core of the problem is hidden way deeper but hard to pin down not knowing the code. I debugged the test and found that the TriggerEventActivator seem to do its job correctly. It finds both triggers and executes both after each other. Going up the stack trace it become more blurry. I will try to debug, below what I have found so far ... maybe someone has a light bulb going on after seeing this ... for me it is still deep darkness what is going on.

TriggerDescriptor.getSPS fails because the TableDescriptor is null
    it worked in 7.1.1 as no TableDescriptor was needed

TriggerEventActivator correctly identified both triggers as eventNumber = 5 with i=0 the first trigger and i=2 the second.
   .notifyEvent(..) line 272 though calls the fireTrigger with ""colsReadFromTable"" which is null and which should contain the columns required from the Trigger Table


UpdateResultSet.fireAfterTriggers() calls the TriggerEventActivator passing down ""constants.getBaseRowReadMap()"" which is null
   ""constants"" is a UpdateConstantAction which is a WriteCursorConstantAction which is a ConstantAction (Interface) and is passed into the class as an ConstantAction from a GenericResultSetFactory.getUpdateResultSet it gets created  new UpdateResultSet(source, generationClauses, checkGM, activation);

UpdateResultSet constructor will call its this with the activation as well as activation.getConstantAction() as ""passedInConstantAction"" which will be the ""constants"" which returns null when called on getBaseRowReadMap().","30/Sep/16 18:36;bruehlicke;OK - after 7 hours debugging the reason is found - Bryan your instincts serve you well ... The main error is in TriggerEventActivator.

for 10.12.1.1 it is in the line 86 which has
tableName = triggerInfo.triggerArray[0].getTableDescriptor().getQualifiedName();

Note that the triggerInfo.triggerArray is HARDCODED to use index 0. Now this gives the needed tableName, BUT the ""getTableDiscriptor()"" method in TriggerDescriptor is actually a SETTER !!! It SETS  td if td is null,  That's why it works when just one trigger is in the test code and fails if 2 as the second trigger's td is never set. This bug must have been here for a long time and never surfaced.

So,  in the TriggerEventActivator line 86 the triggerInfo.triggerArray has correctly the 2 triggers but onlyindex 0 gets its ""td"" set via the ""getTableDescriptor()"".  To test this just add code which will loop for the length of the triggerArray and call the getTableDescritptor on each element.

I.e;  HACK to make it work (assuming the tableName is the same for each of the triggerInfo TableDescription ), replace line 86 with this chunk of code and it will work, it is of course the ""setting"" behavior of the getTableDescriptor which is the ugly part here,

tableName = triggerInfo.triggerArray[0].getTableDescriptor().getQualifiedName();
for(int i=1;i<triggerInfo.triggerArray.length;i++){
      triggerInfo.triggerArray[i].getTableDescriptor();
}




","01/Oct/16 00:12;bryanpendleton;Bernd, firstly, thank you VERY much for putting this energy into this issue! Also, thank
you for the clear and detailed notes about your investigations, it is extremely helpful.

I will try to find some time this weekend to replicate your findings in detail, and to see
if I can add any additional clarity to your observations so far.

My instinct is that the initialization loop that you describe is not really a hack, it is a
reasonable approach to correctly setting up the runtime data structures, including
the table descriptors.

I agree with you that it is disturbing that ""getTableDescriptor()"" is the routine that
gets used to initialize the table descriptor, although this sort of ""initialize upon first
reference"" is not all that uncommon in the Derby codebase.

The other question is where, precisely, we want that initialization loop to be, but as
a first guess, line 86 of TriggerEventActivator seems like a place to start.

As I said, I will try to find a few hours soon to dig through the code and catch up
with your findings and give you some more feedback about whatever I might learn.
","01/Oct/16 02:10;bryanpendleton;With Bernd's proposed change to the TriggerEventActivator constructor,
the reproduction script passes, and the other tests in TriggerTest
pass as well. Attached 'fixesRepro.diff' contains that diff.

I'll do some more testing with this patch proposal.",01/Oct/16 13:39;bryanpendleton;The entire regression suite passes with 'fixesRepro.diff'.,"01/Oct/16 13:42;bryanpendleton;It seems like another potential way to fix this problem is tied to Bernd's observation
that TriggerDescriptor's 'td' member is a lazy-initialize field, which is set when it
is first referenced by the getTableDescriptor() method.

But in the TriggerDescriptor code itself, we don't always go through getTableDescriptor();
sometimes we directly reference 'td'. Such code will fail if td is still NULL at that point,
which is precisely the description of the test case scenario.

So perhaps, instead of the initialization loop in TriggerEventActivator, we could
go through all of TriggerDescriptor, and every time we reference 'td', we instead
reference it by calling getTableDescriptor(), thus allowing 'td' to auto-initialize properly.

I'll give that a try and see how it behaves.

bryan
","01/Oct/16 14:19;bruehlicke;Yep - sounds better to me. I did not like my little hack. It is at the ""wrong"" place and the loop starts with i=1 and no-one would understand what is going on when reading it (if no comments are added why this was added).  How would the td auto-initialization approach work in a multi threaded environment ? Is Derby actually (in theory) thread safe ?

Thinking about the stack trace I cannot see any other point where td is used directly and not just passed along. I wonder if this idea will work with the above test case.  Exciting to hear if it worked for you.","01/Oct/16 14:20;bryanpendleton;Attached 'getTableDescriptor.diff' is an alternate patch proposal.

In some ways, this patch seems more precise and desirable, as it
fixes the problem closer to the symptom, that is, inside TriggerDescriptor
itself, which is the class that **ought** to encapsulate the knowledge of
the lazy-initialization of the 'td' member field. The other patch could be
critiqued because it allowed that knowledge to ""leak out"" into the
TriggerEventActivator class.

The new patch passes the reproduction script; I'll do more testing.","01/Oct/16 14:42;bruehlicke;Looked at the getTableDescriptor.diff - if all tests pass, it for sure has my vote. As you are stating so correctly it keeps the knowledge of the initialization internal allowing to user to be ""clueless"" which is good for any API development.  Thank you for this Bryan ! I can now use 12.1.1 + patch and move forward.","01/Oct/16 17:54;jira-bot;Commit 1763024 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1763024 ]

DERBY-6726: NPE from trigger

When there are multiple triggers on the same table, it is crucial that the
TriggerDescriptor class always uses the getTableDescriptor() getter method
to access its 'td' member field, so that the field can be lazy-initialized
if it has not yet been set.","01/Oct/16 17:57;bryanpendleton;Thank you for the code review Bernd, and thanks once again for all the help on
tracking this down. It seems that we've had this issue for a while, and it's nice to
feel like we may have addressed it.

Please do let us know of your further experiences running with this fix in your environment.

I've left the issue open for now, because I intend to back-port it to (at least) the
10.12 and 10.13 branches. We could certainly back-port it all the way back to the 10.8
branch, but I'm not planning to do that at this time.","01/Oct/16 22:16;jira-bot;Commit 1763034 from [~bryanpendleton] in branch 'code/branches/10.13'
[ https://svn.apache.org/r1763034 ]

DERBY-6726: NPE from trigger

Merged from main via svn merge of 1763024 from trunk (no conflicts)

When there are multiple triggers on the same table, it is crucial that the
TriggerDescriptor class always uses the getTableDescriptor() getter method
to access its 'td' member field, so that the field can be lazy-initialized
if it has not yet been set.","02/Oct/16 16:39;jira-bot;Commit 1763083 from [~bryanpendleton] in branch 'code/branches/10.12'
[ https://svn.apache.org/r1763083 ]

DERBY-6726: NPE from trigger

Merged from main via svn merge of 1763024 from trunk (no conflicts)

When there are multiple triggers on the same table, it is crucial that the
TriggerDescriptor class always uses the getTableDescriptor() getter method
to access its 'td' member field, so that the field can be lazy-initialized
if it has not yet been set.",02/Oct/16 17:05;bryanpendleton;We've completed all the intended work on this issue. Resolving.,,,,,,,,,,,,,,,
Create table as Select cannot copy Decimal columns,DERBY-6956,13094495,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,,MuratC,MuratC,14/Aug/17 14:52,20/Aug/17 14:24,13/Mar/19 22:42,20/Aug/17 14:24,10.13.1.1,10.14.1.0,,JDBC,SQL,,1,newbie,"I create a test table with the following query:

CREATE TABLE ""DERBYTEST"" (""STRINGCOLUMN"" varchar(255), ""INTEGERCOLUMN"" integer, ""SHORTCOLUMN"" varchar(255), ""LONGCOLUMN"" bigint, ""DOUBLECOLUMN"" double, ""FLOATCOLUMN"" double, ""DECIMALCOLUMN"" decimal(31, 6), ""BOOLEANCOLUMN"" smallint, ""DATECOLUMN"" timestamp, ""DATETIMECOLUMN"" timestamp, ""ID"" integer, ""LASTMODTIME"" timestamp, PRIMARY KEY (""ID""))

this query completes successfully

I later try to copy the table with the following query:

CREATE TABLE ""DERBYTEST_TEMP"" AS SELECT * FROM DERBYTEST WITH NO DATA

This throws the following exception:

[42X71][30000] Invalid data type 'DECIMAL(31, 6)' for column 'DECIMALCOLUMN'

Everything works perfectly fine if I remove the decimal column.","Windows 7, DataGrip",,,,,,,,,,,,15/Aug/17 23:50;rhillegas;derby-6956-01-aa-removeSpuriousCheck.diff;https://issues.apache.org/jira/secure/attachment/12882049/derby-6956-01-aa-removeSpuriousCheck.diff,16/Aug/17 01:11;bryanpendleton;withTest.diff;https://issues.apache.org/jira/secure/attachment/12882061/withTest.diff,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,2017-08-14 15:23:02.559,,,no_permission,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 22:22:22 UTC 2017,,,,,,0|i3irfr:,9223372036854775807,,,,,,,,,14/Aug/17 15:23;klementh;Urgenttttttttttttttttttttt! I'm facing the same issue.,"15/Aug/17 01:41;bryanpendleton;Reproduces for me with the head of trunk.

The stack trace is:

{code:java}
Mon Aug 14 18:35:53 PDT 2017 Thread[main,5,main] (XID = 168), (SESSIONID = 1), (DATABASE = memory:test), (DRDAID = null)
, Failed Statement is: CREATE TABLE ""DERBYTEST_TEMP"" AS SELECT * FROM DERBYTEST WITH NO DATA^M
ERROR 42X71: Invalid data type 'DECIMAL(31, 6)' for column 'DECIMALCOLUMN'.
    at org.apache.derby.iapi.error.StandardException.newException(StandardException.java:290)
    at org.apache.derby.iapi.error.StandardException.newException(StandardException.java:285)
    at org.apache.derby.impl.sql.compile.CreateTableNode.bindStatement(CreateTableNode.java:338)
    at org.apache.derby.impl.sql.GenericStatement.prepMinion(GenericStatement.java:401)
    at org.apache.derby.impl.sql.GenericStatement.prepare(GenericStatement.java:99)
    at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(GenericLanguageConnectionContext.java:1114)
    at org.apache.derby.impl.jdbc.EmbedStatement.execute(EmbedStatement.java:689)
    at org.apache.derby.impl.jdbc.EmbedStatement.execute(EmbedStatement.java:637)
    at org.apache.derby.impl.tools.ij.ij.executeImmediate(ij.java:372)
    at org.apache.derby.impl.tools.ij.utilMain.doCatch(utilMain.java:533)
    at org.apache.derby.impl.tools.ij.utilMain.runScriptGuts(utilMain.java:375)
    at org.apache.derby.impl.tools.ij.utilMain.go(utilMain.java:251)
    at org.apache.derby.impl.tools.ij.Main.go(Main.java:229)
    at org.apache.derby.impl.tools.ij.Main.mainCore(Main.java:184)
    at org.apache.derby.impl.tools.ij.Main.main(Main.java:75)
    at org.apache.derby.tools.ij.main(ij.java:59)
{code}

And line 338 of CreateTableNode.java is this ""if"" statement:

{code:java}
				DataTypeDescriptor dtd = rc.getExpression().getTypeServices();
				if ((dtd != null) && !dtd.isUserCreatableType())
				{
					throw StandardException.newException(
							SQLState.LANG_INVALID_COLUMN_TYPE_CREATE_TABLE,
							dtd.getFullSQLTypeName(),
							rc.getName());
				}
{code}

This appears to indicate that dtd.isUserCreatableType() returned false for the DECIMAL(31,6) data type column.","15/Aug/17 01:49;bryanpendleton;DataTypeDescriptor.isUserCreatableType was added by DERBY-2605, and dealt with the problem described here: http://article.gmane.org/gmane.comp.apache.db.derby.devel/40881

","15/Aug/17 02:01;bryanpendleton;And in the particular code path tickled by the CREATE TABLE AS SELECT statement, in the code:


{code:java}
    public boolean isUserCreatableType() throws StandardException
    {
        switch (typeId.getJDBCTypeId())
        {
            case Types.JAVA_OBJECT:
                return getTypeId().getBaseTypeId().isAnsiUDT();
            case Types.DECIMAL:
                return
                (getPrecision() <= typeId.getMaximumPrecision()) &&
                (getScale() <= typeId.getMaximumScale()) &&
                (getMaximumWidth() <= typeId.getMaximumMaximumWidth());
            default: break;
        }
        return true;
    }
{code}

the problem is that getMaximumWidth() returns 33, which is larger than the 31 limit for DECIMAL.

So, somehow, the statement thinks it's trying to create a field of width 33, even though the source table is DECIMAL(31,6).

And, sure enough, if I change the repro so that the DECIMALCOLUMN column is DECIMAL(29,6), the test passes and the error is not thrown.
","15/Aug/17 02:07;rhillegas;The CREATE TABLE AS SELECT statement fails because we flunk the following check...

                (getMaximumWidth() <= typeId.getMaximumMaximumWidth());

at line 1859 in DataTypeDescriptor.java. That, in turn is because the maximum width accounts for a sign byte and a decimal point byte in addition to the maximum number of digits in the decimal. I don't know yet why that check is necessary.","15/Aug/17 04:13;bryanpendleton;The stack trace at the moment that CREATE TABLE AS SELECT decides to create a new decimal type of maximum width 33 is: 


{code:java}
        at org.apache.derby.catalog.types.TypeDescriptorImpl.<init>(TypeDescriptorImpl.java:88)
        at org.apache.derby.iapi.types.DataTypeDescriptor.<init>(DataTypeDescriptor.java:450)
        at org.apache.derby.impl.sql.compile.SQLParser.getDataTypeServices(SQLParser.java:203)
        at org.apache.derby.impl.sql.compile.SQLParser.exactNumericType(SQLParser.java:3855)
        at org.apache.derby.impl.sql.compile.SQLParser.numericType(SQLParser.java:3763)
        at org.apache.derby.impl.sql.compile.SQLParser.dataTypeCommon(SQLParser.java:3445)
        at org.apache.derby.impl.sql.compile.SQLParser.dataTypeDDL(SQLParser.java:3381)
        at org.apache.derby.impl.sql.compile.SQLParser.columnDefinition(SQLParser.java:3247)
        at org.apache.derby.impl.sql.compile.SQLParser.tableElement(SQLParser.java:3212)
        at org.apache.derby.impl.sql.compile.SQLParser.tableElementList(SQLParser.java:3199)
        at org.apache.derby.impl.sql.compile.SQLParser.tableDefinition(SQLParser.java:10274)
        at org.apache.derby.impl.sql.compile.SQLParser.createStatements(SQLParser.java:2029)
        at org.apache.derby.impl.sql.compile.SQLParser.StatementPart(SQLParser.java:1923)
        at org.apache.derby.impl.sql.compile.SQLParser.Statement(SQLParser.java:1823)
        at org.apache.derby.impl.sql.compile.ParserImpl.parseStatementOrSearchCondition(ParserImpl.java:170)
        at org.apache.derby.impl.sql.compile.ParserImpl.parseStatement(ParserImpl.java:130)
        at org.apache.derby.impl.sql.GenericStatement.prepMinion(GenericStatement.java:359)
        at org.apache.derby.impl.sql.GenericStatement.prepare(GenericStatement.java:99)
        at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(GenericLanguageConnectionContext.java:1114)
        at org.apache.derby.impl.jdbc.EmbedStatement.execute(EmbedStatement.java:689)
        at org.apache.derby.impl.jdbc.EmbedStatement.execute(EmbedStatement.java:637)
        at org.apache.derby.impl.tools.ij.ij.executeImmediate(ij.java:372)
        at org.apache.derby.impl.tools.ij.utilMain.doCatch(utilMain.java:533)
        at org.apache.derby.impl.tools.ij.utilMain.runScriptGuts(utilMain.java:375)
        at org.apache.derby.impl.tools.ij.utilMain.go(utilMain.java:251)
        at org.apache.derby.impl.tools.ij.Main.go(Main.java:229)
        at org.apache.derby.impl.tools.ij.Main.mainCore(Main.java:184)
        at org.apache.derby.impl.tools.ij.Main.main(Main.java:75)
        at org.apache.derby.tools.ij.main(ij.java:59)
{code}
","15/Aug/17 04:18;bryanpendleton;I suspect that this code in sqlgrammar.jj is the source of the extra 2 bytes in the maximum width:


{code:java}
        /*
        ** If we have a decimal point, need to count it
        ** towards maxwidth.  Max width needs to account
        ** for the possible leading '0' and '-' and the
        ** decimal point.  e.g., DEC(1,1) has a maxwidth
        ** of 4 (to handle ""-0.1"").
        */
        maxWidth = DataTypeUtilities.computeMaxWidth( precision, scale);
{code}

This is further confirmed by looking at this code in DataTypeUtilities.java:


{code:java}
    public static int computeMaxWidth( int precision, int scale)
    {
    // There are 3 possible cases with respect to finding the correct max
    // width for DECIMAL type.
    // 1. If scale = 0, only sign should be added to precision.
    // 2. scale=precision, 3 should be added to precision for sign, decimal and an additional char '0'.
    // 3. precision > scale > 0, 2 should be added to precision for sign and decimal.
    return (scale ==0) ? (precision +1) : ((scale == precision) ? (precision + 3) : (precision + 2));
    }
{code}

And that code in DataTypeUtilities, in turn, goes back to DERBY-836.","15/Aug/17 04:20;bryanpendleton;To summarize: my opinion is that DERBY-836 and DERBY-2605 interacted to create this problem.

DERBY-836 means that sometimes the maximum width is ""illegal"", and DERBY-2605 added the check that we should not create a new table with a column of ""illegal"" maximum width.

Rick, what do you think?","15/Aug/17 14:21;bryanpendleton;It's not clear to me why we make this check for CREATE TABLE AS SELECT, but not for simple CREATE TABLE.

Anyway, two possibilities occur to me:
# Increase the maximum width for DECIMAL to 33, since clearly we desire to allow 31 digits, an optional sign, and an optional decimal point
# Remove the check against maximum width, if, as Rick suggested, we aren't sure why we're making that check.","15/Aug/17 23:51;rhillegas;Attaching derby-6956-01-aa-removeSpuriousCheck.diff. This patch removes the offending extra check for maximum width. Tests passed cleanly against this patch.

Thanks for digging into the code, Bryan. I think that the extra check is just a mistake. The datatype rejected by CREATE TABLE AS SELECT is exactly the same datatype which was used by CREATE TABLE. The remaining checks for DECIMAL look adequate to me.

I am inclined to commit this patch.
","16/Aug/17 00:31;bryanpendleton;Thanks for the patch, Rick.  I agree with your assessment.

+1 to commit from me.

If I have some spare time in the next week or two, I'll take the repro case from the issue description, as well as a couple variants (e.g., DECIMAL(29,6), etc.) and drop them into our regression suite, just to close the loop and satisfy myself.
","16/Aug/17 01:12;bryanpendleton;Hi Rick, attached 'withTest.diff' includes a few new test cases for regression testing.

thanks,

bryan
","17/Aug/17 01:27;jira-bot;Commit 1805249 from [~rhillegas] in branch 'code/trunk'
[ https://svn.apache.org/r1805249 ]

DERBY-6956: Commit patch derby-6956-01-aa-removeSpuriousCheck.diff, removing a needless restriction on the kind of DECIMAL types which can be column types in a CREATE TABLE AS SELECT statement.","17/Aug/17 01:28;rhillegas;Thanks, Bryan. I have committed the derby-6956-01-aa-removeSpuriousCheck.diff patch.","18/Aug/17 01:32;jira-bot;Commit 1805356 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1805356 ]

DERBY-6956: CREATE TABLE AS SELECT cannot copy DECIMAL(31,6)

This change adds several test tests to serve as regression tests.","19/Aug/17 22:22;rhillegas;Hi Bryan,

Thanks for the additional tests. I don't plan to backport this fix to any other codelines. I think that the issue can be resolved. What are your thoughts?",,,,,,,,,,,,,,,,,,,,,,,
Problem with schema name starting with number followed by a dot,DERBY-6918,13023586,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,bryanpendleton,hoomanv,hoomanv,27/Nov/16 16:38,04/Dec/16 08:59,13/Mar/19 22:42,03/Dec/16 03:55,10.13.1.1,10.14.1.0,,SQL,,,0,,"It seems that there is a problem with schema names that start with a number followed by a dot.
Please take a look at the following script. Everything works fine but the last line which is a delete statement. Note that having the foreign key constraints was necessary to make it reproduce the problem.

create schema ""1.a"";

create table ""1.a"".""role""
(
""id"" integer generated always as identity,
""name"" varchar(255) not null
);

alter table ""1.a"".""role"" add constraint ""role_pk""
primary key (""id"");

create table ""1.a"".""user""
(
""id"" integer generated always as identity,
""name"" varchar(255) not null
);

alter table ""1.a"".""user"" add constraint ""user_pk""
primary key (""id"");

create table ""1.a"".""user_role""
(
""role"" integer not null,
""user"" integer not null
);

alter table ""1.a"".""user_role"" add constraint ""user_role_fk1""
foreign key (""role"")
references ""1.a"".""role"" (""id"")
on delete cascade;

alter table ""1.a"".""user_role"" add constraint ""user_role_fk2""
foreign key (""user"")
references ""1.a"".""user"" (""id"")
on delete cascade;

alter table ""1.a"".""user_role"" add constraint ""user_role_u1""
unique (""user"", ""role"");

insert into ""1.a"".""role"" (""name"") values ('r1');
insert into ""1.a"".""user"" (""name"") values ('u1');
insert into ""1.a"".""user_role"" (""role"",""user"") values (1,1);

select * from ""1.a"".""user"";

delete from ""1.a"".""user"";

Last delete statement fails. The error is:
Schema '1' does not exist [SQL State=42Y07, DB Errorcode=20000] ",,,,,,,,,,,,,27/Nov/16 19:11;bryanpendleton;schemaNames.diff;https://issues.apache.org/jira/secure/attachment/12840591/schemaNames.diff,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,2016-11-27 17:48:24.787,,,no_permission,,,,,,,,,,,,9223372036854775807,,,,,Sun Dec 04 08:59:27 UTC 2016,,,,,,0|i36thr:,9223372036854775807,,,,,,,,Normal,"27/Nov/16 17:48;bryanpendleton;Reproduces for me with the current head of trunk.

Here's the precise stack trace where the problem occurs:
{quote}
ERROR 42Y07: Schema '1' does not exist
	at org.apache.derby.iapi.error.StandardException.newException(StandardException.java:290)
	at org.apache.derby.iapi.error.StandardException.newException(StandardException.java:285)
	at org.apache.derby.impl.sql.catalog.DataDictionaryImpl.getSchemaDescriptor(DataDictionaryImpl.java:1709)
	at org.apache.derby.iapi.sql.StatementUtil.getSchemaDescriptor(StatementUtil.java:138)
	at org.apache.derby.impl.sql.compile.QueryTreeNode.getSchemaDescriptor(QueryTreeNode.java:1146)
	at org.apache.derby.impl.sql.compile.QueryTreeNode.getSchemaDescriptor(QueryTreeNode.java:1121)
	at org.apache.derby.impl.sql.compile.TableName.bind(TableName.java:245)
	at org.apache.derby.impl.sql.compile.FromBaseTable.bindNonVTITables(FromBaseTable.java:2338)
	at org.apache.derby.impl.sql.compile.FromList.bindTables(FromList.java:348)
	at org.apache.derby.impl.sql.compile.SelectNode.bindNonVTITables(SelectNode.java:490)
	at org.apache.derby.impl.sql.compile.DMLStatementNode.bindTables(DMLStatementNode.java:190)
	at org.apache.derby.impl.sql.compile.DeleteNode.bindStatement(DeleteNode.java:151)
	at org.apache.derby.impl.sql.compile.DeleteNode.bindStatement(DeleteNode.java:374)
	at org.apache.derby.impl.sql.GenericStatement.prepMinion(GenericStatement.java:401)
	at org.apache.derby.impl.sql.GenericStatement.prepare(GenericStatement.java:99)
	at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(GenericLanguageConnectionContext.java:1114)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(EmbedStatement.java:684)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(EmbedStatement.java:632)
	at org.apache.derby.impl.tools.ij.ij.executeImmediate(ij.java:372)
	at org.apache.derby.impl.tools.ij.utilMain.doCatch(utilMain.java:533)
	at org.apache.derby.impl.tools.ij.utilMain.runScriptGuts(utilMain.java:375)
	at org.apache.derby.impl.tools.ij.utilMain.go(utilMain.java:251)
	at org.apache.derby.impl.tools.ij.Main.go(Main.java:229)
	at org.apache.derby.impl.tools.ij.Main.mainCore(Main.java:184)
	at org.apache.derby.impl.tools.ij.Main.main(Main.java:75)
	at org.apache.derby.tools.ij.main(ij.java:59)
{quote}

It's possible that the problem arises in this code in DeleteNode.java, where we are
generating an internal ""delete"" statement to perform the cascading delete:
{code}
			//In case of cascade delete , create nodes for
			//the ref action  dependent tables and bind them.
			if(fkTableNames != null)
			{
				String currentTargetTableName = targetTableDescriptor.getSchemaName() +
						 ""."" + targetTableDescriptor.getName();

{code}
This code does not seem to be properly using delimited identifiers to manage
the targetTableDescriptor's schema name and table name; it's just pasting them
together, resulting in the simple names ""1.a.user"" and ""1.a.role"" being used
for the generated cascaded deletes. ","27/Nov/16 18:01;bryanpendleton;Yuck. It's not that simple. Even adjusting the half-dozen places where
DeleteNode simply pastes together the schema name and table name without
considering delimited identifiers doesn't work because there is other
code that later bursts those names back apart by looking for the first dot:
{code}
	private StatementNode getDependentTableNode(String tableName, int refAction,
												ColumnDescriptorList cdl) throws StandardException
	{
        DMLModStatementNode node = null;

		int index = tableName.indexOf('.');
		String schemaName = tableName.substring(0 , index);
		String tName = tableName.substring(index+1);
{code}

So fixing this is a bit of a project.

Regardless, thanks very much for the clear and precise reproduction script,
it is VERY helpful!
","27/Nov/16 19:11;bryanpendleton;Attached ""schemaNames.diff"" is a small patch that
cleans up the handling of schema names in the
foreign key maintenance logic in DMLModStatementNode
and DeleteNode: rather than pasting the schema name
and table name together, then bursting them back apart,
the schema name and table name are simply treated
separately, which seems cleaner and simpler to me.

The test case from the repro script is also added to the
regression test suites.

From a quick scan of the Subversion history, it seems like
this part of the code dates back to the original donation
of the Derby code by IBM. So this is old, old code; it's a
bit interesting that we hadn't encountered this problem
until now. I didn't take the repro script and hunt back through
old releases to confirm that it's ""always"" been broken, though.

But if this patch proves to work, it could certainly be
back-ported to older Derby releases, if anyone so desired.

I'm running the full set of tests and will report back on my findings.",28/Nov/16 00:49;bryanpendleton;No problems were revealed by running the complete regression suite.,"03/Dec/16 03:54;jira-bot;Commit 1772428 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1772428 ]

DERBY-6918: Problem with schema name containing a period

When processing DELETE statements in a database schema including
referential integrity constraints that specify ON DELETE CASCADE,
it is easiest to keep the schema name and table name separate,
rather than pasting them together into a compound name joined with
a period, because pasting them together results in ambiguity about
which period separates the schema name from the table name, and
which period was simply part of the schema name proper.

Adjusted the logic in DeleteNode and DMLModStatementNode accordingly.","03/Dec/16 03:55;bryanpendleton;No additional improvements to the patch occurred to me over the
last few days, so I've committed it to the trunk and resolved the job.","03/Dec/16 09:31;hoomanv;Bryan thanks a lot for fixing this.
Is it possible to release it as a hotfix for 10.13? Otherwise we have to wait for 10.14 to be released on maven central.","03/Dec/16 22:28;jira-bot;Commit 1772493 from [~bryanpendleton] in branch 'code/branches/10.13'
[ https://svn.apache.org/r1772493 ]

DERBY-6918: Problem with schema name containing a period

Merged: svn merge -r 1772427:1772428 https://svn.apache.org/repos/asf/db/derby/code/trunk

No merge conflicts; no additional changes.","03/Dec/16 22:30;bryanpendleton;The code merge was straightforward, so I've merged the fix back to 10.13
and committed it.

I'm not sure when the next release from the 10.13 branch might occur;
one thing that would help would be if you could build a copy of 10.13
from source yourself, and perform some additional testing to verify
that this fix is working properly in your environment.

",04/Dec/16 08:59;hoomanv;Thank you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect example for new SYSCS_UTIL.SYSCS_IMPORT_TABLE_BULK procedure,DERBY-6914,13011125,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,bryanpendleton,rhillegas,rhillegas,10/Oct/16 17:39,11/Oct/16 13:41,13/Mar/19 22:42,11/Oct/16 01:53,10.13.1.0,10.13.1.1,,Documentation,,,0,,"The example in the Reference Guide section ""SYSCS_UTIL.SYSCS_IMPORT_TABLE_BULK system procedure"" gives the name of the new procedure as CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE rather than SYSCS_UTIL.SYSCS_IMPORT_TABLE_BULK.",,,,,,,,,,,DERBY-4555,DERBY-6895,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,2016-10-11 01:30:53.52,,,no_permission,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 11 13:41:40 UTC 2016,,,,,,0|i34oqv:,9223372036854775807,,,,,,,,Blocker,"11/Oct/16 01:30;jira-bot;Commit 1764188 from [~bryanpendleton] in branch 'docs/trunk'
[ https://svn.apache.org/r1764188 ]

DERBY-6914: Incorrect example for new SYSCS_UTIL.SYSCS_IMPORT_TABLE_BULK procedure","11/Oct/16 01:52;jira-bot;Commit 1764189 from bpendleton@apache.org in branch 'docs/branches/10.13'
[ https://svn.apache.org/r1764189 ]

DERBY-6914: Incorrect example for new SYSCS_UTIL.SYSCS_IMPORT_TABLE_BULK procedure","11/Oct/16 01:53;bryanpendleton;Thanks Rick for the eagle eyes!

I fixed the example text in main and merged it back to the 10.13 docs branch so we'll pick it up when I build the next release candidate.","11/Oct/16 13:41;jira-bot;Commit 1764241 from [~bryanpendleton] in branch 'code/branches/10.13'
[ https://svn.apache.org/r1764241 ]

DERBY-6911: Update release notes to note that 10.13.1.1 supports JDBC 4.2, also include DERBY-6914 in notes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER TABLE DROP COLUMN corrupts secondary index collation information,DERBY-6890,12973322,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,bryanpendleton,akoyro,akoyro,27/May/16 06:57,15/Sep/16 15:25,13/Mar/19 22:42,18/Jun/16 16:02,10.12.1.1,10.13.1.0,,SQL,,,0,,"For a database with ""collation=/territory="" information configured via
the JDBC Connection URL at database connection time, individual
columns in tables and indexes in the database have collation identification
which is stored as part of the table/index conglomerate.

When an ALTER TABLE DROP COLUMN statement is run against
such a database, the drop column processing performs logic which
re-builds all of the (remaining) secondary indexes for that table
to reflect their new state following the removal of that column.

This index rebuild process does not properly re-configure the
collation information for column(s) in those index(es), leaving
the indexes in a corrupt state.

As a workaround, following the ALTER TABLE DROP COLUMN, the
damaged secondary indexes can be dropped and recreated explicitly.

== Original issue description below ==

After issue https://issues.apache.org/jira/browse/DERBY-3888 was fixed, we want to use the 'GENERATED BY DEFAULT' feature 
for our tables.  

To migrate our tables, we use this sql: 
     ALTER TABLE MODULE ADD COLUMN ID_TEMP BIGINT GENERATED BY DEFAULT AS IDENTITY;
     UPDATE MODULE SET ID_TEMP = ID;
     ALTER TABLE MODULE ALTER COLUMN ID_TEMP NOT NULL;
     ALTER TABLE MODULE DROP ID;
     RENAME COLUMN MODULE.ID_TEMP TO ID;

But after I applied it, I started to get this exception:
Caused by: org.apache.derby.shared.common.sanity.AssertFailure: ASSERT FAILED type of inserted column[0] = org.apache.derby.iapi.types.CollatorSQLVarchartype of template column[0] = org.apache.derby.iapi.types.SQLVarchar
	at org.apache.derby.shared.common.sanity.SanityManager.THROWASSERT(SanityManager.java:162)
	at org.apache.derby.shared.common.sanity.SanityManager.THROWASSERT(SanityManager.java:147)
	at org.apache.derby.impl.store.access.btree.OpenBTree.isIndexableRowConsistent(OpenBTree.java:519)
	at org.apache.derby.impl.store.access.btree.BTreeController.doIns(BTreeController.java:679)
	at org.apache.derby.impl.store.access.btree.BTreeController.insert(BTreeController.java:1372)
	at org.apache.derby.impl.store.access.btree.index.B2IController.insert(B2IController.java:210)
	at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(IndexChanger.java:565)
	at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(IndexChanger.java:393)
	at org.apache.derby.impl.sql.execute.IndexChanger.insert(IndexChanger.java:713)
	at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(IndexSetChanger.java:268)
	at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(RowChangerImpl.java:458)
	at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(InsertResultSet.java:881)
	at org.apache.derby.impl.sql.execute.InsertResultSet.open(InsertResultSet.java:452)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(GenericPreparedStatement.java:473)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(GenericPreparedStatement.java:352)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1340)
	... 30 more

I attached Test.groovy class which shows this issue. 

also I found this workaround: 
we need to drop all indexes and create them again, after we applied this pk column update.





","Mac OS X 10.11.5
JDK: Oracle 1.8.0_92",,,,,,,,,,,,03/Jun/16 00:58;bryanpendleton;CollationTest.diff;https://issues.apache.org/jira/secure/attachment/12807870/CollationTest.diff,11/Jun/16 23:52;bryanpendleton;CollationTest2.diff;https://issues.apache.org/jira/secure/attachment/12809653/CollationTest2.diff,27/May/16 07:00;akoyro;Test.groovy;https://issues.apache.org/jira/secure/attachment/12806581/Test.groovy,02/Jun/16 15:08;akoyro;Test.java;https://issues.apache.org/jira/secure/attachment/12807739/Test.java,05/Jun/16 20:02;bryanpendleton;doesntPassTests.diff;https://issues.apache.org/jira/secure/attachment/12808216/doesntPassTests.diff,05/Jun/16 20:59;bryanpendleton;fixIndexCollation.diff;https://issues.apache.org/jira/secure/attachment/12808217/fixIndexCollation.diff,07/Jun/16 14:17;bryanpendleton;proposed.diff;https://issues.apache.org/jira/secure/attachment/12808683/proposed.diff,18/Jun/16 03:27;bryanpendleton;ready.diff;https://issues.apache.org/jira/secure/attachment/12811510/ready.diff,01/Jun/16 23:30;bryanpendleton;testRepro.diff;https://issues.apache.org/jira/secure/attachment/12807556/testRepro.diff,9.0,,,,,,,,,,,,,,,,,,,,,,2016-06-01 23:30:23.906,,,no_permission,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 15 15:25:09 UTC 2016,,,,,,0|i2yl1r:,9223372036854775807,,,,,,,,,"01/Jun/16 23:30;bryanpendleton;I tried to edit the reproduction script into the
AlterTableTest.java test suite in the Derby regression tests.

The new test case (attached as testRepro.diff) does not
reproduce the problem, however.

I am not sure if I made a simple mistake in translating from
the Groovy-language test program into pure Java, or if there
is something more subtle going on.

Perhaps somebody else can compare my testRepro.diff against
Test.groovy and see how to make a reproducible test case in pure Java.","02/Jun/16 09:41;andrey.koyro;I will chech it today




-- 
Best regards
Andrey
","02/Jun/16 15:07;akoyro;I made a few experiments and see that the exception depends on number of INSERTs in one commit, or may be it depends on length (or chars) of this 'title'. 
I am not sure. 

Anyway, I have adjusted this test, on my env it throws the exception always. I have ported it to java, See Test.java.
","03/Jun/16 00:41;bryanpendleton;Hi Andrei,

Thanks very much for the updated Test.java. It reproduces the problem for me, too, now.

I think the problem may be closely related to this: "";collation=TERRITORY_BASED:SECONDARY""

When I remove the ""collation="" from the JDBC Connection URL, the Test.java program no longer
reproduces the problem for me.

Can you confirm that finding when you get a chance?
","03/Jun/16 00:58;bryanpendleton;Since it appears that this problem is related to collation sequence handling,
I moved the test case from the AlterTableTest suite to the CollationTest suite.

The modified test reliably reproduces the problem for me.

I didn't experiment with backing down the statement count from 295;
perhaps fewer INSERT statements would work.

Regardless, the crucial point is that I now see the same failure that
Andrei described.

Unfortunately, collation sequence handling is not all that familiar
to me, so I will have to study the code in more detail to understand
more about what is going wrong.","03/Jun/16 02:27;akoyro;Hi Bryan

Yes, you are right, if I use the default collation I don't get this exception.  But TERRITORY_BASED:SECONDARY allows case insensitive queries and I should create my db with this collation.

And one more, in my test there are two commented out statements:
//          !!!!!!!!!!! WORKAROUND !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
//		s.execute(""DROP INDEX Module_title"");
//		s.execute(""CREATE INDEX Module_title ON Module(title)"");

If you uncomment these lines this exception goes away. 

","04/Jun/16 00:07;bryanpendleton;The Btree code is complaining because it has two objects, one which is a SQLVarchar,
and one which is a CollatorSQLVarchar. Below are the stack traces of the places where
I think those two different objects are being created.

{quote}
java.lang.Exception: CollatorSQLVarchar constructor
	at org.apache.derby.iapi.types.CollatorSQLVarchar.<init>(CollatorSQLVarchar.java:57)
	at org.apache.derby.iapi.types.SQLVarchar.getValue(SQLVarchar.java:91)
	at org.apache.derby.iapi.types.DataTypeDescriptor.getNull(DataTypeDescriptor.java:1024)
	at org.apache.derby.impl.sql.execute.NormalizeResultSet.getCachedDestination(NormalizeResultSet.java:401)
	at org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeRow(NormalizeResultSet.java:378)
	at org.apache.derby.impl.sql.execute.NormalizeResultSet.getNextRowCore(NormalizeResultSet.java:191)
	at org.apache.derby.impl.sql.execute.DMLWriteResultSet.getNextRowCore(DMLWriteResultSet.java:148)
	at org.apache.derby.impl.sql.execute.InsertResultSet.getNextRowCore(InsertResultSet.java:1082)
	at org.apache.derby.impl.sql.execute.InsertResultSet.open(InsertResultSet.java:451)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(GenericPreparedStatement.java:472)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(GenericPreparedStatement.java:351)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1339)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(EmbedPreparedStatement.java:1709)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(EmbedPreparedStatement.java:320)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(EmbedPreparedStatement.java:309)
	at Test.testDerby6890(Test.java:49)
	at Test.main(Test.java:73)

java.lang.Exception: Simple SQLVarchar constructor
	at org.apache.derby.iapi.types.SQLVarchar.<init>(SQLVarchar.java:117)
	at org.apache.derby.iapi.types.DataValueFactoryImpl.getNullDVDWithUCS_BASICcollation(DataValueFactoryImpl.java:1125)
	at org.apache.derby.iapi.types.DataValueFactoryImpl.getNull(DataValueFactoryImpl.java:1073)
	at org.apache.derby.impl.store.access.conglomerate.TemplateRow.allocate_objects(TemplateRow.java:96)
	at org.apache.derby.impl.store.access.conglomerate.TemplateRow.newRow(TemplateRow.java:199)
	at org.apache.derby.impl.store.access.conglomerate.OpenConglomerateScratchSpace.get_template(OpenConglomerateScratchSpace.java:210)
	at org.apache.derby.impl.store.access.btree.BTreeController.doIns(BTreeController.java:675)
	at org.apache.derby.impl.store.access.btree.BTreeController.insert(BTreeController.java:1372)
	at org.apache.derby.impl.store.access.btree.index.B2IController.insert(B2IController.java:210)
	at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(IndexChanger.java:565)
	at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(IndexChanger.java:393)
	at org.apache.derby.impl.sql.execute.IndexChanger.insert(IndexChanger.java:713)
	at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(IndexSetChanger.java:268)
	at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(RowChangerImpl.java:458)
	at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(InsertResultSet.java:881)
	at org.apache.derby.impl.sql.execute.InsertResultSet.open(InsertResultSet.java:452)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(GenericPreparedStatement.java:472)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(GenericPreparedStatement.java:351)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1339)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(EmbedPreparedStatement.java:1709)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(EmbedPreparedStatement.java:320)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(EmbedPreparedStatement.java:309)
	at Test.testDerby6890(Test.java:49)
	at Test.main(Test.java:73)
{quote}","04/Jun/16 16:30;bryanpendleton;I haven't really developed a good intuitive theory about this bug yet.

It seems that the problem lies with the Module_title index, for several reasons:
1) The proximate cause of the problem is that when we go to manufacture
   a ""template"" row for the Module_title index, the template is manufactured
   with a simple SQLVarchar datatype, rather than a CollatorSQLVarchar datatype.
2) If we drop and recreate the Module_title index, the problem goes away

And surely the problem must be with one of the alteration statements that
are issued (ALTER TABLE, RENAME COLUMN).

But none of those alteration statements are performed against the Module_title
index, so I don't have a clear idea about how this (unrelated?) secondary
index is getting damaged by the alter statements that should only be affecting
(a) the base table, and (b) the system-generated primary key constraint index.

And another big problem with my intuitive grasp of this bug is that the collation
and territory information for a database is supposed to be database-wide, not
table-by-table and index-by-index, so I don't understand how we could get
into a situation where some of the tables and indexes in the database think
they have one sort of collation/territory configuration, while other tables/indexes
think they have a different configuration.

Perhaps the next step is to back WAY up, and study what happens with the
'collation=' attribute on the connection URL at database creation time.
","04/Jun/16 22:21;bryanpendleton;Possibly things go wrong in the ALTER TABLE DROP COLUMN ID statement,
which causes us to run this section of code:

	at org.apache.derby.impl.sql.execute.AlterTableConstantAction.updateIndex(AlterTableConstantAction.java:2780)
	at org.apache.derby.impl.sql.execute.AlterTableConstantAction.updateAllIndexes(AlterTableConstantAction.java:2694)
	at org.apache.derby.impl.sql.execute.AlterTableConstantAction.compressTable(AlterTableConstantAction.java:2456)
	at org.apache.derby.impl.sql.execute.AlterTableConstantAction.dropColumnFromTable(AlterTableConstantAction.java:1725)

In AlterTableConstantAction.compressTable, there is some special code for
the DROP COLUMN case which adjusts the collation IDs for the base table 
to reflect the impact that dropping this column has on the remaining columns.

(That is, we remove the collation ID for the dropped column, and slide all
subsequent collation ids for the remaining columns in the base table down
by one to compensate).

But I don't see, yet, any similar code which adjusts the collation ids for
the columns of the secondary indexes.

So my working theory is that it's AlterTableConstantAction.compress, when
called for the DROP COLUMN case, that is corrupting the collation ids
for the secondary index columns.","05/Jun/16 01:30;bryanpendleton;The support for adjusting the collation identifiers
during ALTER TABLE DROP COLUMN was part
of DERBY-2537, so linking this issue to that.","05/Jun/16 18:12;bryanpendleton;OK, I think I see the problem now, though I don't yet know how to fix it.

The test performs an ALTER TABLE DROP COLUMN ID statement. Before
that statement, the Module table has columns (ID, TITLE, ID_TEMP).
After that statement, the Module table will have columns (TITLE, ID_TEMP).

Near the end of ALTER TABLE DROP COLUMN ID, we are going to
rebuild all the indexes on table Module. At this point, since we are dropping the
primary key column ID, we are also discarding the system-generated index
for the primary key, so there is only 1 index on table Module, namely the
Module_title index on Module(title).

This index processing sets up a sorter for each remaining index, in
preparation to scanning the base table and reloading each index. This
work is done by AlterTableConstantAction.setUpAllSorts().

But the data structures at this point are confused. The table descriptor
(td.getColumnDescriptorList) is still showing (ID,TITLE,ID_TEMP), but
the collation ids are expecting to work with the columns (TITLE,ID_TEMP).
When we go to set up the sorter for the Module_title index, we look at
the information about the first column, expecting that to be the information
for the TITLE column, but instead the column we get from the column
descriptor list is the first column in the **old** table, which is the ID column,
and hence we rebuild the Module_title index using the collation information
from the ID column, rather than the TITLE column.

Since ID is a BIGINT, not a string, it has no collation information, so the
new index that we build says that the first column has no collation
information, but then when the test later tries to update the index, the
store (rightfully) complains because column TITLE is a string column
and so it **should** have had collation information.

So either, at this point, we should be using collation information that
is still based off the original table column description list, or we should
be referencing the new (post-DROP COLUMN) column descriptor list.

We call",05/Jun/16 19:20;bryanpendleton;Updated issue title and description.,"05/Jun/16 20:02;bryanpendleton;Attached 'doesntPassTests.diff' is not for commit, but may
show a possible path forward.

The idea of this patch is to move the computation of
the index collation ids out of 'setUpAllSorts' and into
'getAffectedIndexes'. This allows us to compute the index
collation ids before we have re-numbered the index
columns, thus getting the correct collation id information.

With this patch in place, the reproduction script passes,
which is nice. However, there is a test failure in AlterTableTest,
in test ""testDropColumn"", at line 3096 of AlterTableTest.java,
with the output:

org.apache.derby.shared.common.sanity.AssertFailure: ASSERT FAILED sizeof_ids = 2;collationIds.length = 4
        at org.apache.derby.shared.common.sanity.SanityManager.THROWASSERT(SanityManager.java:162)
        at org.apache.derby.shared.common.sanity.SanityManager.THROWASSERT(SanityManager.java:147)
        at org.apache.derby.impl.store.access.conglomerate.ConglomerateUtil.createCollationIds(ConglomerateUtil.java:227)
        at org.apache.derby.impl.store.access.btree.index.B2I.create(B2I.java:592)
        at org.apache.derby.impl.store.access.btree.index.B2IFactory.createConglomerate(B2IFactory.java:225)
        at org.apache.derby.impl.store.access.RAMTransaction.createConglomerate(RAMTransaction.java:803)
        at org.apache.derby.impl.store.access.RAMTransaction.recreateAndLoadConglomerate(RAMTransaction.java:880)
        at org.apache.derby.impl.store.access.RAMTransaction.createAndLoadConglomerate(RAMTransaction.java:844)
        at org.apache.derby.impl.sql.execute.AlterTableConstantAction.updateIndex(AlterTableConstantAction.java:2786)
        at org.apache.derby.impl.sql.execute.AlterTableConstantAction.updateAllIndexes(AlterTableConstantAction.java:2700)
        at org.apache.derby.impl.sql.execute.AlterTableConstantAction.compressTable(AlterTableConstantAction.java:2462)
        at org.apache.derby.impl.sql.execute.AlterTableConstantAction.dropColumnFromTable(AlterTableConstantAction.java:1725)

But maybe this is progress anyway, so I'm recording it here.","05/Jun/16 20:59;bryanpendleton;Attached 'fixIndexCollation.diff' is a patch proposal.

It passes both AlterTableTest and CollationTest, as well as
the reproduction script (which is included as a new test in
CollationTest by this patch).

I'm running a more complete set of tests to see what else pops out.

The idea of the fix is as described before, to move the collation
id computation for the indexes from setUpAllSorts into getAffectedIndexes,
with the additional complexity that if an index includes the column
which is being dropped, we must (carefully) omit that column from
the collation id array that we build. This fixup is now performed at
the same point in the code where we fixup the column position data,
which seems more appropriate than where it was before this patch.

This is complex, subtle code. But maybe this patch is viable.","05/Jun/16 23:21;bryanpendleton;The patch isn't completely viable; the tests show that certain code paths
through SYSCS_COMPRESS_TABLE now fail because the collation ids
aren't getting set at all, and thus we get a NullPointerException.

But overall, the patch seems like it has a lot of potential. I'll need to
track down those NPEs, though.","07/Jun/16 14:17;bryanpendleton;Attached 'proposed.diff' passes regression tests and is, I think,
a viable candidate for commit.

Although the actual number of lines of code changes is small,
the code paths through ALTER TABLE are complex, and
affect at least SYSCS_COMPRESS_TABLE, TRUNCATE TABLE,
and ALTER TABLE DROP COLUMN.

If anyone has the time to review the patch, that would be great.","08/Jun/16 02:19;rhillegas;Hi Bryan,

I have taken a quick look at the patch and it seems to be complicated enough for the problem described. But this is tricky code and I always make mistakes myself when I'm in AlterTableConstantAction. Fortunately, the existing regression tests usually flag my blunders. I recommend adding more test cases. Maybe a test case for ALTER TABLE ADD COLUMN in the presence of an index on a column with a collation. Thanks.","08/Jun/16 04:43;bryanpendleton;Thanks for the review, Rick. I agree that some additional test cases would
be valuable in this area. I will think on this for a bit.","10/Jun/16 01:40;bryanpendleton;I spent some time looking through the test cases in CollationTest2.java. From what
I can tell, they are a pretty extensive set of test cases for this area of the code.

However, those test cases didn't reveal this bug.

I'm going to try to dig into the specific ALTER TABLE related test cases in CollationTest2
and try to understand why the existing test cases didn't trip this bug.

And then my hope is to extend and augment those test cases, rather than writing
a bunch of new test cases, because that seems a bit simpler overall.

I think that we want to cover:
- ALTER TABLE DROP COLUMN
- SYSCS_COMPRESS_TABLE
- TRUNCATE TABLE
other ALTER TABLE actions, such as ADD COLUMN, ALTER COLUMN, RENAME COLUMN
- different patterns of secondary indexes, both system-generated and user defined
","11/Jun/16 23:52;bryanpendleton;Yay! After a few false starts, I was able to modify CollationTest2.java
so that it, too, reproduces this bug. Attached CollationTest2.diff is
the test change.

I am still only able to demonstrate problems in the DROP COLUMN case;
the various other tests in CollationTest2.java (TRUNCATE TABLE,
ADD COLUMN, COMPRESS, etc.) are not revealing any additional issues.

Next step will be to unify these test changes with the already-constructed
patch to get a more complete patch with better test coverage.",18/Jun/16 03:27;bryanpendleton;Attached 'ready.diff' is the change I propose to commit. Tests are clean.,"18/Jun/16 16:00;jira-bot;Commit 1749069 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1749069 ]

DERBY-6890: ALTER TABLE DROP COLUMN corrupts secondary index collation data.

During ALTER TABLE DROP COLUMN, we rebuild all the indexes on the table. Some
indexes may be entirely dropped, some indexes may be rebuilt with a subset
of columns, some indexes are simply rebuild with essentially the same content.

The issue is that the index rebuild logic was incorrectly setting the
collation data for each index. So the indexes had the right data, but the
wrong collation information, causing them to be damaged.

This change moves the computation of the index collation ids out of the
setUpAllSorts method, into the getAffectedIndexes method, where it is simpler
to compute the index collation ids appropriately, because the code in that
location already has logic to manipulate both the old (pre-DROP) and new
(post-DROP) table descriptions, and so it is straightforward to compute the
correct collation ids there.

As part of this change, an old test case in CollationTest2, which was marked
with the comment ""this test does not work yet"", and was disabled, is changed
(un-commented) and is now enabled.

I found no new problems with this test case. I believe that, at the time
that comment was written, there were bugs in Derby that have since been
repaired, and hence it is appropriate to enable this test case at this time.","18/Jun/16 16:02;bryanpendleton;I think this patch is as good as I can make it for now, and so I'm
committing it to get additional experience with it in the trunk.","15/Sep/16 01:54;ari;Will there be a 10.13 release in 2016, or perhaps a backport of bug fixes to another 10.12 release?","15/Sep/16 15:25;rhillegas;Hi Ari,

I believe that Bryan has volunteered to manage a 10.13.1 release this fall. See http://apache-database.10148.n7.nabble.com/Proposal-for-a-Derby-10-13-release-this-fall-td146404.html. He has not proposed a schedule yet: http://wiki.apache.org/db-derby/DerbyTenThirteenOneRelease.

Thanks,
-Rick",,,,,,,,,,,,,,
Engine deadlock between XA timeout handling and cleanupOnError,DERBY-6879,12954263,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,bbergquist,bbergquist,bbergquist,29/Mar/16 11:04,09/Aug/16 00:04,13/Mar/19 22:42,09/Aug/16 00:04,10.10.2.0,10.13.1.0,,Services,,,0,,"Deadlock between XA timer cleanup task and the ContextManager.cleanupOnError

Found one Java-level deadlock:
=============================
""DRDAConnThread_34"":
  waiting to lock monitor 0x0000000104b14d18 (object 0xfffffffd9090f058, a org.apache.derby.jdbc.XATransactionState),
  which is held by ""Timer-0""
""Timer-0"":
  waiting to lock monitor 0x00000001038b96e8 (object 0xfffffffd9090d8b0, a org.apache.derby.impl.jdbc.EmbedConnection40),
  which is held by ""DRDAConnThread_34""
 
Java stack information for the threads listed above:
===================================================
""DRDAConnThread_34"":
     at org.apache.derby.jdbc.XATransactionState.cleanupOnError(Unknown Source)
     - waiting to lock <0xfffffffd9090f058> (a org.apache.derby.jdbc.XATransactionState)
     at org.apache.derby.iapi.services.context.ContextManager.cleanupOnError(Unknown Source)
     at org.apache.derby.impl.jdbc.TransactionResourceImpl.cleanupOnError(Unknown Source)
     at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
     at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
     at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
     at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
     - locked <0xfffffffd9090d8b0> (a org.apache.derby.impl.jdbc.EmbedConnection40)
     at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
     at org.apache.derby.impl.jdbc.EmbedPreparedStatement.execute(Unknown Source)
     at org.apache.derby.iapi.jdbc.BrokeredPreparedStatement.execute(Unknown Source)
     at org.apache.derby.impl.drda.DRDAStatement.execute(Unknown Source)
     at org.apache.derby.impl.drda.DRDAConnThread.parseEXCSQLSTTobjects(Unknown Source)
     at org.apache.derby.impl.drda.DRDAConnThread.parseEXCSQLSTT(Unknown Source)
     at org.apache.derby.impl.drda.DRDAConnThread.processCommands(Unknown Source)
     at org.apache.derby.impl.drda.DRDAConnThread.run(Unknown Source)
""Timer-0"":
     at org.apache.derby.impl.jdbc.EmbedConnection.xa_rollback(Unknown Source)
     - waiting to lock <0xfffffffd9090d8b0> (a org.apache.derby.impl.jdbc.EmbedConnection40)
     at org.apache.derby.jdbc.XATransactionState.cancel(Unknown Source)
     - locked <0xfffffffd9090f058> (a org.apache.derby.jdbc.XATransactionState)
     at org.apache.derby.jdbc.XATransactionState$CancelXATransactionTask.run(Unknown Source)
     at java.util.TimerThread.mainLoop(Timer.java:555)
     at java.util.TimerThread.run(Timer.java:505)
 
Found 1 deadlock.


This deadlock caused Derby to create 18000 transaction recovery logs because of the XA transaction that did not cleanup in the timeout.  Rebooting the system would cause a 50 hour boot up time to process the transaction logs so recovery had to be done by going to a backup database before the issue occurred.


",Solaris 10.5 on Oracle M5000 ,,,,,,,,,,,,05/Jul/16 20:34;bbergquist;derby-6879-2016-07-05.diff;https://issues.apache.org/jira/secure/attachment/12816294/derby-6879-2016-07-05.diff,08/Jul/16 13:18;bbergquist;derby-6879-2016-07-08.diff;https://issues.apache.org/jira/secure/attachment/12816821/derby-6879-2016-07-08.diff,18/Jul/16 00:49;bbergquist;derby-6879-2016-07-17.diff;https://issues.apache.org/jira/secure/attachment/12818485/derby-6879-2016-07-17.diff,26/Jun/16 19:24;bbergquist;derby-6879-test.diff;https://issues.apache.org/jira/secure/attachment/12813568/derby-6879-test.diff,19/Jul/16 00:16;bryanpendleton;diff-0717-ignore-whitespace.diff;https://issues.apache.org/jira/secure/attachment/12818713/diff-0717-ignore-whitespace.diff,05/Jul/16 20:34;bbergquist;svnstatus.txt;https://issues.apache.org/jira/secure/attachment/12816295/svnstatus.txt,16/Jul/16 18:28;bryanpendleton;testFail.zip;https://issues.apache.org/jira/secure/attachment/12818415/testFail.zip,,,7.0,,,,,,,,,,,,Crash,Data corruption,,,,,,,,,2016-06-23 00:39:06.387,,,no_permission,,,,Patch,,,,,,,,9223372036854775807,,,,,Tue Aug 09 00:04:33 UTC 2016,,,,,,0|i2vc7b:,9223372036854775807,,,,,,,,,"29/Mar/16 11:07;bbergquist;I am examining the locking pattern to determine a different locking pattern to eliminate the deadlock.  Initially it looks like the timer task should grab the lock on the connection before locking the XATransactionState.

","21/Jun/16 21:19;bbergquist;In XATransactionState.java, we have:

        /**
         * Runs the cancel task of the global transaction
         */
        public void run() {
            try {
                    xaState.cancel(MessageId.CONN_XA_TRANSACTION_TIMED_OUT);
            } catch (Throwable th) {
                Monitor.logThrowable(th);
            }
        }

and the ""cancel"" method looks like:

    synchronized void cancel(String messageId) throws XAException {
...
            try {
                conn.xa_rollback();
            } catch (SQLException sqle) {

and the ""xa_rollback"" method looks like:

	public final void xa_rollback() throws SQLException {
		synchronized (getConnectionSynchronization())
		{

So when the timer expires, the lock pattern is to lock the XATransactionState and then the EmbedConnection object.   

When an exception such as a lock timeout occurs when using a XA transaction and cleanupOnError code is called, the call stack looks like:

""DRDAConnThread_3"" #15 prio=5 os_prio=64 tid=0x0000000103d5b000 nid=0x19 waiting for monitor entry [0xffffffff1a9fe000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.derby.jdbc.XATransactionState.cleanupOnError(XATransactionState.java:155)
	- waiting to lock <0xffffffff4d0aead8> (a org.apache.derby.jdbc.XATransactionState)
	at org.apache.derby.iapi.services.context.ContextManager.cleanupOnError(ContextManager.java:343)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.cleanupOnError(TransactionResourceImpl.java:461)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(TransactionResourceImpl.java:344)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(EmbedConnection.java:2400)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(ConnectionChild.java:85)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1437)
	- locked <0xffffffff4d01b698> (a org.apache.derby.impl.jdbc.EmbedConnection40)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(EmbedStatement.java:711)
	- locked <0xffffffff4d01b698> (a org.apache.derby.impl.jdbc.EmbedConnection40)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeLargeUpdate(EmbedStatement.java:190)
	at org.apache.derby.iapi.jdbc.BrokeredStatement.executeLargeUpdate(BrokeredStatement.java:633)
	at org.apache.derby.impl.drda.DRDAConnThread.parseEXCSQLIMM(DRDAConnThread.java:5393)
	at org.apache.derby.impl.drda.DRDAConnThread.processCommands(DRDAConnThread.java:759)
	at org.apache.derby.impl.drda.DRDAConnThread.run(DRDAConnThread.java:295)

So here the lock pattern is locking the EmbedConnection40 (which is an EmbedConnection) and then the XATransactionState.   

So we have just the opposite pattern and a deadlock occurs.   

I have been able to get this to reproduce in a test application.   Using a normal non-XA connection, I query and lock many (10000) rows in a large table.   Using a separate XA connection, I try to delete those same rows.  I have ""derby.jdbc.xaTransactionTimeout=30"" in my derby.properties to have a very short transaction timeout.  I have the ""derby.locks.waitTimeout=60"" in my derby.properties.

When I run the test application, it locks up the 10000 rows in one transaction that is not committed through the first non-XA connection.   The second XA connection tries to delete these rows.  Since the ""derby.jdbc.xaTransactionTimeout"" is 30 seconds, the XA transaction is going to timeout at 30 seconds, but the ""derby.locks.waitTimeout"" is going to cause the XA transaction to try wait for 60 seconds to obtain the locks that it needs. 

The timeout of the XA transaction is going to cause a SQLTransactionRollbackException to be recognized on the XA connection and it will try to then call the cleanup code.   But alas, the EmbedConnection is locked and it will try to lock the XATransactionState in the cleanup code and the XATransactionState is lock in the timeout task and will attempt to call ""xa_rollback"" on the XA connection.  Deadlock.

I have a simple workaround and that is to change

        /**
         * Runs the cancel task of the global transaction
         */
        public void run() {
            try {
                    xaState.cancel(MessageId.CONN_XA_TRANSACTION_TIMED_OUT);
            } catch (Throwable th) {
                Monitor.logThrowable(th);
            }
        }

to

        public void run() {
            try {
                synchronized (xaState.conn) {
                    xaState.cancel(MessageId.CONN_XA_TRANSACTION_TIMED_OUT);
                }
            } catch (Throwable th) {
                Monitor.logThrowable(th);
            }
        }

This changes the timeout task to first lock on the connection and then XATransactionState.   

I would actually prefer to somehow add a new method to ""EmbedConnection"" that would acquire its lock which would then call back into XATransactionState.cancel to perform the cancel which would also change the lock pattern, but I am struggling to find out how to get from an EmbedConnection back to the XATransactionState.

","23/Jun/16 00:39;bryanpendleton;Hi Brett, thanks for all the time and effort on this!

The evidence seems indisputable, and your analysis and explanation seem airtight.

I agree with your conclusion that CancelXATransactionTask.run needs to lock the
connection before it calls the xaState.cancel method.

I guess that your remaining questions boil down to:
1) How can we clean up the proposed change so that the locking on the connection
is a bit more elegantly encapsulated in the EmbedConnection class, and
2) How can we wrap all this up to get it committed

Taking the second question first, it seems like your testing approach is a pretty
good one. Is it conceivable that you could package that up as a new test case
for the existing XA suites? Perhaps lowering the lock timeout so that the
test runs slightly faster?

W.r.t. the coding aspect of expressing the synchronization point, would it
help to provide a new interface here?

For example, could we make a ""Cancelable"" interface, with a single method
named ""cancel( MessageId msg )"". Then we could make XATransactionState
implement Cancelable, and we could add a new method to EmbedConnection
along the lines of:
{code}
    synchronized void cancel( Canceable c, MessageId m ) {
        c.cancel( m );
    }
{code}
and then CancelXATransactionTask.run turns into
{code}
    public void run() {
        try {
            xaState.conn.cancel( xaState, MessageId.CONN_XA_TRANSACTION_TIMED_OUT ); 
        } catch ( Throwable th ) { Monitor.logThrowable(th); }
{code}

I hope this is all making (some) sense.","23/Jun/16 10:44;bbergquist;I like your suggestion for the Cancelable which ""feels"" right in having the control be from the Embed connection.  I will pursue that.

As for testing, the condition can be replicated simply and quickly:

1. Set the ""derby.locks.waitTimeout"" so some value.
2. Set the ""derby.jdbc.xaTransactionTimeout"" to a value less than ""derby.locks.waitTimeout"" (in my last test, I used 60 second and 30 seconds but these could easily be 20 seconds and 10 seconds).
3. Lock a table exclusively in one statement with autoCommit disabled using a non-XA connection
4. Try to lock the same table exclusively with another statement using a XA connection

What I am having trouble with is detecting the actual Java deadlock.   Normally I use ""jstack"" to determine such.  I looked through the test cases and don't see any such usage but may have missed it.  Any ideas in this area for the test case will be appreciated.

",23/Jun/16 15:26;bbergquist;I think I can write up a test using the capability in the ThreadMXBean.  This has the ability to look for deadlocks.  I am looking at the current tests to see how this might be done.,"23/Jun/16 19:35;bbergquist;I am questioning if this is the final correct answer.  In this one case, the cleanUpOnError is being called by connection that is executing a statement so the locking is connection and then XATransactionState.  When a timeout occurs, the locking is XATransactionState and then the connection.  When both occur at the same time the deadlock occurs.  My proposed fix handles this situation.

There is other possibilities.   So for example, code calls XAResource.commit.   This has the locking pattern of XATransactionState and then  connection.   So if the timeout were to occur during this time, then there would now be a deadlock here because the proposed solution is going to lock the connection and then the XATransacttionState.

The original deadlock problem can only occur if the XATransactionState.cleanupOnError is called while the connection is locked and executing and a cleanup needs to be performed.","26/Jun/16 19:24;bbergquist;Here is a patch that adds a test for this issue.  Once the test is applied, then the XATest suite will fail as the added test will trigger a Java level deadlock in Derby. 

The patch also adds a DeadlockWatchdog.java class that is used by the new test to detect the Java level deadlock and exit the test using System.exit(1) so that the test run does not hang.

Once DERBY-6879 is fixed then the deadlock will no longer occur and the XATest suite will pass.","26/Jun/16 20:54;bryanpendleton;I downloaded and applied the patch successfully, and tried running XATest. Here's what I see:
{quote}
junit-single:
    [junit] Running org.apache.derbyTesting.functionTests.tests.jdbcapi.XATest
    [junit] Watchdog failed: java.security.AccessControlException: access denied
 (""java.lang.management.ManagementPermission"" ""monitor"")
    [junit] Running org.apache.derbyTesting.functionTests.tests.jdbcapi.XATest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] Test org.apache.derbyTesting.functionTests.tests.jdbcapi.XATest FAILED (crashed)
{quote}","26/Jun/16 22:04;bbergquist;Brian I just downloaded the trunk fresh on my Mac (I developed the patch on Windows 7) and downloaded and applied the patch and ran:

ant -Dderby.junit.testclass=org.apache.derbyTesting.functionTests.tests.jdbcapi.XATest junit-clean all buildjars junit-single junit-html

and it worked for me.  This is the tail end of that run:

...
junit-single:
    [junit] Running org.apache.derbyTesting.functionTests.tests.jdbcapi.XATest
    [junit] Deadlock detected
    [junit] Running org.apache.derbyTesting.functionTests.tests.jdbcapi.XATest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] Test org.apache.derbyTesting.functionTests.tests.jdbcapi.XATest FAILED (crashed)

junit-init-nocp:

junit-init:

emit-junit-classpath-jars:
     [echo] Running with jars: /Users/brett/Development/svn/derby/trunk/jars/sane
     [echo] CLASSPATH (environment variable): ${env.CLASSPATH}

emit-junit-classpath:

junit-sysinfo:

junit-html:

...

So maybe there is an environmental issue.  I am using

Bretts-MacBook-Pro:trunk brett$ java -version
java version ""1.8.0_92""
Java(TM) SE Runtime Environment (build 1.8.0_92-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode)
Bretts-MacBook-Pro:trunk brett$ 

","26/Jun/16 22:33;bbergquist;The error that you got is the error that happens without the 

 permission java.lang.management.ManagementPermission ""monitor"";

in 

./java/testing/org/apache/derbyTesting/functionTests/tests/jdbcapi/XATest.policy","27/Jun/16 00:16;bbergquist;After examining the code for quite some time and still not completely understanding the context stack, I believe that the best solution is in XATransactionState to always synchronize on the connection first and then the XATransactionState instance for all of the methods that interact with the connection.  My reasoning is that it will be very difficult to change the locking pattern whereby an EmbedConnection is executing a SQL statement where the connection is synchronized on when executing a statement, and then detects an error and the context stack is processed calling cleanupOnError (which in XATransactionState is going to synchronize on the XATransactionState.

So if that locking pattern cannot be changed, the XATransactionState locking pattern needs to be changed to first synchronize on the connection and then on the XATransactionState.  But all synchronization needs to be changed to that pattern so that another possible deadlock is not introduced.
","27/Jun/16 01:32;bryanpendleton;I agree, it's as though the policy isn't being deployed as part of the test.

To be honest, the mechanism that the test harness uses for custom policies
like these is a bit murky to me.

In my previous experiment, I was running with classes. So I ran 'ant buildjars'
and then ran the test again, and it worked perfectly.

So then I ran 'ant cleanjars', and ran the test again, and it got the
security policy problem.

So maybe it's a build error on my part, but it seems like something about
the policy is causing it not to be picked up in all cases on my machine.

By the way, when I did 'ant buildjars', I had to run 'ant refreshjardriftcheck',
because the new DeadlockWatchdog class in derbyTesting.jar was noticed by the
jar drift detector. I think that's unrelated to why I was getting the
security policy error in the non-jar configuration, but I mention it anyway.","29/Jun/16 13:45;bbergquist;My thoughts about always synchronizing on the connection first will not work.  It does fix the immediate problem, but now when XA transaction is going to be to be timed out, the ""cancel"" waits for the connection to be able to be synchronized.  If the connection is executing a long running SQL statement, then the cancel actually waits for the completion before the ""cancel"" attempts to cancel.  So while it does solve the deadlock, it introduces another anomaly and as such is not the correct solution.
","05/Jul/16 16:16;bbergquist;I have attached a revised patch with an updated test case and the resolution of the issue.

The final solution I came up with was to modify XATransactionState.cancel.   The change removes the method level synchronization and instead synchronizes on the instance for handle the cancelling and rollback work against the XATransactionState, releases the synchronization state and performs the connection level rollback (which synchronizes on the connection), and then again synchronizes on the XATransactionState to return the connection.

The change ensure that XATransactionState.cancel does not hold a lock on the XATransactionState instance while waiting for the lock on the EmbedConnection.
","05/Jul/16 16:19;bbergquist;The patch also makes the CancelXATransactionTask.cancel and the CancelXATransactionTask.run methods synchronized.  Without this there is the possibility that the ""xaState"" could become null within ""cancel"" but be accessed within ""run"" and a NPE could occur.",05/Jul/16 19:39;bbergquist;The patch was no good so I removed it.   I will upload another shortly.,"05/Jul/16 20:34;bbergquist;The patch ""derby-6897-2016-07-05.diff"" is now ready for review.  I had an issue with EOL with the previous incarnation of the patch that I deleted.  

I also attached the output of the ""svn status"" to show the files modified/added.",06/Jul/16 18:41;bbergquist;All tests passed with this patch in place for me.,"07/Jul/16 03:31;bryanpendleton;Hi Brett, thanks for the patch, and for the detailed explanation of your thinking on this.

I don't really feel qualified to offer very deep comments on your patch, but here are
a few cursory comments:

1) I believe we generally don't include @author tags in our code in Derby. Assuming that's
   OK, maybe you could remove that tag?

2) It seems like the change to derbyTesting.jar.lastcontents is bigger than it needs to
   be; it looks like the contents of that file got sorted differently and so org.apache.derby
   classes ended up in a different order relative to org.apache.derbyTesting. I wonder
   if that's an artifact of the platform you are working on -- do you use Windows?

3) I think that there is also a 'insane' version of derbyTesting.jar.lastcontents, which
   needs to have the same trivial change as the 'sane' version -- that is, DeadlockWatchdog.class
   needs to be added there, too.

4) It seems like, at around line 410 of XATransactionState.java, you added braces
   around the call to Monitor.logTextMessage. Was that just a stylistic thing? Or is
   there a deeper change there that I'm not understanding?

Is there any other particular review you feel would be useful at this point? Assuming
that I can successfully apply your patch, and build and run the tests, is there more
work that you think is needed prior to commiting your change?
","08/Jul/16 12:57;bbergquist;I will update the patch Brian.

""1"" snuck in there via my IDE.

""2"" I am working on windows and the change to ""derbyTesting.jar.lastcontents"" was done by running the ANT target ""refreshjardriftcheck"" which was prompted when it saw the new DeadlockWatchdog.class"".   So any re-ordering was done by that target, possibly because of being developed on Windows.   I can manually fix that file so only have the DeadlockWatchdog.class being added.

""3"" Is there some document on the ""sane"" versus ""insane"" builds?   I understand what they are doing and by looking at the ""build.xml"" I see how to trigger each, but I can only find one reference in the ""BUILDING.html"".   There seems to me to be a lot of ""magic"" on building/testing that I found as an impediment to contributing.

""4' the IDE snuck this in when reformatting this section according to my coding guidelines. Looking through the history of the code when I was comparing the source from 10.10.2.0 to the trunk I braces being removed for no apparent reason.   On the change between revisions 1364916 and 1364917, I see a brace being added for a bare ""if"".   

Personally since doing software development since 1984, I have come to the conclusion that having the braces always has no downside and always has an upside when code is added to an ""if"" (it ensures that the code is added to content of the ""if"" and you don't have to add anything else).  Also according to the coding standard referenced on

http://wiki.apache.org/db-derby/DerbyContributorChecklist

which links to:

http://www.oracle.com/technetwork/java/javase/documentation/codeconventions-142311.html#449

having the ""braces"" is according to the coding standards.  I will remove the added braces however.   

As far as further review, just a sanity check on the change is good.  

The original problem to be solved is that a connection in that is performing a XA transaction that discovers an error that must be cleaned up is going to have a lock on a EmbedConnection and will then require a lock on the XATransactionState and if the same XA transaction times out (because of the XA transaction timer value) while the original XA transaction is being worked, then the timeout handling is going to have a lock on the XATransactionState and then require a lock on the EmbedConnection, triggering the deadlock.

The change in the patch fixes this problem is the smallest way that I know of by altering the locking on the XATransactionState when invoked via the timeout by removing the synchronization on the ""cancel"" method and inline synchronizing on the ""XATransactionState"" to while its internals are being altered, then releasing the lock to allow the EmbedConnection.rollback to be invoked without the lock and finally acquiring the lock again to finish cleaning up the ""XATransactionState"".
","08/Jul/16 13:18;bbergquist;New patch file attached that fixes the issues that Brian mentioned in his review.  I also added a couple of comments to the ""cancel"" method to indicate the new synchronization change and reference to DERBY-6879 of what the change is for.","08/Jul/16 13:39;bryanpendleton;Thanks Brett!

I'm no fan of brace or whitespace wars, either, sorry if I left that impression. I was
simply doing a close reading and trying to make sure I didn't overlook anything.

The note in BUILDING.html is the only doc I know of w.r.t. sane vs insane. From a
coding point of view, a sane build causes code like:
{code}
            if (SanityManager.DEBUG) {
                SanityManager.ASSERT(stream != null);
            }
{code}
to actually do something, while in an insane build that code is neutered, and
in fact with modern compilers is essentially eliminated entirely. If you compare
the file size of the class files and jars in a sane build vs an insane build, you
should see a non-trivial difference, and test runs should have different elapsed
times, too.

It sounds like you're ready to go. I'll get to work on commiting this patch soon,
probably this weekend.
","09/Jul/16 00:05;jira-bot;Commit 1751977 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1751977 ]

DERBY-6879: Engine deadlock between XA timeout handling and cleanupOnError

This patch was contributed by Brett Bergquist (brett at thebergquistfamily dot com)

The original problem to be solved is that a connection that is performing a
XA transaction that discovers an error that must be cleaned up is going to
have a lock on a EmbedConnection and will then require a lock on the
XATransactionState.

And if the same XA transaction times out (because of the XA transaction
timer value) while the original XA transaction is being worked, then the
timeout handling is going to have a lock on the XATransactionState and then
require a lock on the EmbedConnection, triggering the deadlock.

The change in the patch fixes this problem by altering the locking on
the XATransactionState when invoked via the timeout by removing the
synchronization on the ""cancel"" method and instead using inline
synchronization on the ""XATransactionState"" while its internals are being
altered.

Then, it releases the XATransactionState lock to allow the
EmbedConnection.rollback method to be invoked without the lock.

Finally, it acquires the lock again to finish cleaning up the XATransactionState.","09/Jul/16 00:06;bryanpendleton;I had no further suggestions to offer after re-reading Brett's latest patch,
and all of my test results were clean, so I've committed the patch.

Thank you for this contribution, Brett! And, more importantly, thanks
again for all the hard work to diagnose the problem, build new test cases
and testing infrastructure, and really going the extra distance to contribute
as thorough an improvement as this.","09/Jul/16 01:25;jira-bot;Commit 1751978 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1751978 ]

DERBY-6879: Engine deadlock between XA timeout handling and cleanupOnError

This patch was contributed by Brett Bergquist (brett at thebergquistfamily dot com)

The original problem to be solved is that a connection that is performing a
XA transaction that discovers an error that must be cleaned up is going to
have a lock on a EmbedConnection and will then require a lock on the
XATransactionState.

And if the same XA transaction times out (because of the XA transaction
timer value) while the original XA transaction is being worked, then the
timeout handling is going to have a lock on the XATransactionState and then
require a lock on the EmbedConnection, triggering the deadlock.

The change in the patch fixes this problem by altering the locking on
the XATransactionState when invoked via the timeout by removing the
synchronization on the ""cancel"" method and instead using inline
synchronization on the ""XATransactionState"" while its internals are being
altered.

Then, it releases the XATransactionState lock to allow the
EmbedConnection.rollback method to be invoked without the lock.

Finally, it acquires the lock again to finish cleaning up the XATransactionState.","09/Jul/16 01:26;bryanpendleton;Had to do follow-on commit 1751978 because I forgot to 'svn add'
the two new files. Mea culpa.","16/Jul/16 18:28;bryanpendleton;Hi Brett,

While running the overall suite overnight I had a test failure
with the new test. I zipped up the output and attached it as
'testFail.zip'.

Can you see anything in there that explains what happened?

thanks,

bryan","16/Jul/16 19:49;bbergquist;The exception is a XAER_DUPID being reported from line 1271 on XaTest.java which looks like:

        // Get a second connection and global xact
        // and try to select causing lock timeout
        XAConnection xaconn2 = xads.getXAConnection();
        XAResource xar2 = xaconn2.getXAResource();
        xar2.setTransactionTimeout(2);
        Xid xid2 = XATestUtil.getXid(6879, 11, 51);
        Connection conn2 = xaconn2.getConnection();
        // Set to serializable so we get lock timeout
        conn2.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);
        xar2.start(xid2, XAResource.TMNOFLAGS);

So the ""xar2.start"" is the one failing with a duplicate XID.   I don't know how this could happen unless the test were being run twice at the same time.   Not that this code is before anything that triggers the DERBY-6879 issue.  It is just creating and starting a XA transaction with a specific XID.   
","16/Jul/16 20:11;bbergquist;Brian, it looks like there is a timing issue in XATransactionState.cancel with the new code.

       if (needsRollback) {
            // While the rollback is performed on the connection, 
            //  this XATransactionState is ont synchronized to work around 
            //  the issue reported in DERBY-6879
            try {
                // Rollback the global transaction
                conn.xa_rollback();
            } catch (SQLException sqle) {
                XAException ex = new XAException(XAException.XAER_RMERR);
                ex.initCause(sqle);
                throw ex;
            }
        }

The ""conn.xa_rollback"" is throwing a SQL exception.  

For now, I think we should rollback the patch and disable this test until I can reproduce this issue consistently and debug it.","16/Jul/16 22:58;bryanpendleton;Good, I'm glad you have a theory to consider.

I don't see any need to rollback your previous work just yet. It's only in main,
not in any released builds, and so it's only being seen by developers like us who
are building and working with main.

I'm happy to leave the code as it is while we think about the issues for the time being.

The intermittent test failure is not causing anyone any problems that I am aware of.","18/Jul/16 00:46;bbergquist;I was able to get this to reproduce periodically by causing stress on my test system while running the tests over and over.   

There is a timing issue that is opened up because of the lack of synchronization on the XATransactionState inside of 'cancel' while calling the ""conn.rollback"".  

 I have taken a different approach for a fix.  

The problem occurs if a timeout occurs calling ""cancel"" and if an error occurs on the clients connection causing the ""cleanupOnError"" to be called at the same time.  Recognizing this, the patch in the ""cancel"" method checks to see if the ""cleanUpOnError"" is being invoked and if so, the cancel is skipped.  This makes sense as if ""cleanupOnError"" is being called, then the transaction will end anyways as there the error handling code on the client is being processed, so it does not need to be cancelled.

The patch also adds a check in the ""cleanupOnError"" method to check to see if the ""cancel"" is being invoked.  If so, then the cleanup on the XATransactionState by this method is skipped.   this makes sense as if the transaction is being cancelled, then there is no need to mark the XATransactionState with the cleanup error information.

The patch also disassociates the XID from ResourceAdapter earlier in the ""cancel"".   The logic behind this is that once the ""cancel"" starts processing any client code that access the XA transaction, really should not see the XA transaction.   So a call to  ""XATransaction.end"" for example, after the XA transaction times out and is cancelled, the client code is going to received a XAException.XAER_NOTA.   Note that this is not a change in except in timing of where the XID is removed in ""cancel"" in that it previously was done at the end of ""cancel"" and now it is being done in the beginning of ""cancel"".  This eliminate any possibly that client code can access a XA transaction after it is starting to be cancelled.  Also note that this does not change the error that a client would receive if the XA transaction were cancelled and then seconds later after the cancel completed, the client were to access the XA transaction; it would receive a XAException.XAER_NOTA.

The patch creates a new private static class that is used to track if ""cancel"" or ""cleanupOnError"" has been invoked.  The methods are synchronized so that there is no timing issue on checking and recording the state.

","18/Jul/16 00:49;bbergquist;The patch that fixes the issue.   I have run all tests in the ""rg.apache.derbyTesting.functionTests.tests.jdbcapi._Suite"" over and over while stressing the system.  

I will run the full test suite now and update but I believe that there will be no issues as the XATransactionState is only accessed by tests in this suite.
",18/Jul/16 11:22;bbergquist;Both the junit based and the old test harness tests passed with no errors.,"19/Jul/16 00:16;bryanpendleton;I found the 07-17 diff to be a bit hard to concentrate on,
because the re-indentation of some code led to a large
diff file. Attached 'diff-0717-ignore-whitespace.diff' is
the result of doing 'svn diff -x -w' after applying the
'derby-6879-2016-07-17.diff'; it is significantly smaller
and might help anyone who is looking at this diff.","19/Jul/16 03:55;jira-bot;Commit 1753333 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1753333 ]

DERBY-6879: Engine deadlock between XA timeout handling and cleanupOnError

This patch was contributed by Brett Bergquist (brett at thebergquistfamily dot com)

This change is a follow-up to revision 1751977.

The problem with that revision occurs if a timeout occurs calling ""cancel""
and if an error occurs on the clients connection causing the ""cleanupOnError""
to be called at the same time.

This change creates a new private static class that is used to track if
""cancel"" or ""cleanupOnError"" has been invoked. The methods are synchronized
so that there is no timing issue on checking and recording the state.","19/Jul/16 03:57;bryanpendleton;I read through the patch, and also built it and ran the jdbcapi tests myself multiple times.

I have no additional improvements to suggest, so I committed the patch.

Brett, thanks for the quick follow-up.",08/Aug/16 05:14;kristwaa;Hi. What's the state of this issue now? It's still reported as unresolved and unassigned by JIRA.,"08/Aug/16 15:05;bbergquist;I don't know the proper procedure on this Kristian.   All of the code is complete and been committed by Bryan and it there have been no issues in the regression tests that I know of.  Also, I have put a patched version of this into our production environments and there have been no issues there either.

I think it is ready to be marked as resolved.","09/Aug/16 00:04;bryanpendleton;Thanks for the reminder, Kristian. As Brett noted, we've completed
all the intended work for this issue. Marking resolved as fixed in 10.13."
SYSCS_IMPORT_TABLE_LOBS_FROM_EXTFILE can't import more than Integer.MAX_VALUE bytes of blob data,DERBY-6884,12959665,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,bryanpendleton,ehowe,ehowe,18/Apr/16 14:37,03/May/16 03:22,13/Mar/19 22:42,03/May/16 03:22,10.11.1.1,10.13.1.0,,SQL,,,0,,"Using SYSCS_EXPORT_TABLE_LOBS_TO_EXTFILE to export a table containing a blob column, SYSCS_IMPORT_TABLE_LOBS_FROM_EXTFILE  will fail with a NumberFormatException if the offset for a blob record is > Integer.MAX_VALUE.  This is because ImportReadData.initExternalLobFile() is parsing the offset as an Integer.

The stack trace and a program to reproduce are below.

java.lang.NumberFormatException: For input string: ""2147483770""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ~[na:1.8.0_45]
	at java.lang.Integer.parseInt(Integer.java:583) ~[na:1.8.0_45]
	at java.lang.Integer.parseInt(Integer.java:615) ~[na:1.8.0_45]
	at org.apache.derby.impl.load.ImportReadData.initExternalLobFile(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.load.ImportReadData.getBlobColumnFromExtFile(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.load.ImportAbstract.getBlob(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.load.Import.getBlob(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.iapi.types.SQLBlob.setValueFromResultSet(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.sql.execute.VTIResultSet.populateFromResultSet(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.sql.execute.VTIResultSet.getNextRowCore(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.sql.execute.NormalizeResultSet.getNextRowCore(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.sql.execute.NoPutResultSetImpl.getNextRowFromRowSource(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.store.access.heap.HeapController.load(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.store.access.heap.Heap.load(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.store.access.RAMTransaction.loadConglomerate(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.store.access.RAMTransaction.recreateAndLoadConglomerate(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.sql.execute.InsertResultSet.bulkInsertCore(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source) ~[derby-10.11.1.1.jar:na]
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source) ~[derby-10.11.1.1.jar:na]
	... 36 common frames omitted

==================================
package blob;

import java.io.BufferedInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.sql.*;

public final class DerbyIssue {
    // derby url
    public static final String DBURL = ""jdbc:derby:testdb;create=true"";
    // any random binary file such as a large image or document
    public static final String BLOB_DATA_FILE = ""..."";
    public static final String EXPORT_TABLE_FILE = ""table-data"";
    public static final String EXPORT_BLOB_FILE = ""blob-data"";

    public static void main(String... args) throws Exception {
        final DerbyIssue test = new DerbyIssue();
        test.run();
    }

    public void run() throws Exception {
        Class.forName(""org.apache.derby.jdbc.ClientDriver"").getConstructor().newInstance();

        try(final Connection con = DriverManager.getConnection(DBURL)) {
            try (final Statement stmt = con.createStatement()) {
                stmt.execute(""CREATE TABLE TESTBLOB(id BIGINT, content BLOB)"");
            }

            System.out.printf(""inserting test data%n"");

            try (final PreparedStatement pstmt = con.prepareStatement(""INSERT INTO TESTBLOB (id, content) VALUES (?, ?)"")) {
                long id = 1;
                long byteCount = 0;
                final File content = new File(BLOB_DATA_FILE);
                while (byteCount < Integer.MAX_VALUE) {
                    insertBlob(pstmt, id, content);
                    id++;
                    byteCount += content.length();
                    if (id % 100 == 0) {
                        System.out.printf(""%d%n"", byteCount);
                    }
                }
                insertBlob(pstmt, id, content);
                byteCount += content.length();

                System.out.printf(""%d bytes written to testblob table%n"", byteCount);
            }

            final File exportFile = new File(EXPORT_TABLE_FILE);
            final File blobFile = new File(EXPORT_BLOB_FILE);
            try (final CallableStatement stmt = con.prepareCall(
                    ""CALL SYSCS_UTIL.SYSCS_EXPORT_TABLE_LOBS_TO_EXTFILE (null, ?, ?, null, null, null, ?)"")) {
                stmt.setString(1, ""TESTBLOB"");
                stmt.setString(2, exportFile.toString());
                stmt.setString(3, blobFile.toString());
                stmt.execute();
            }

            System.out.printf(""testblob table exported%n"");

            try (final Statement stmt = con.createStatement()) {
                stmt.execute(""TRUNCATE TABLE TESTBLOB"");
            }

            System.out.printf(""testblob table truncated%n"");

            try (final CallableStatement stmt = con.prepareCall(
                    ""CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE_LOBS_FROM_EXTFILE (null, ?, ?, null, null, null, 0)"")) {
                stmt.setString(1, ""TESTBLOB"");
                stmt.setString(2, exportFile.toString());
                stmt.execute();
            }

            System.out.printf(""testblob data imported%n"");
        }
    }

    private void insertBlob(PreparedStatement pstmt, long id, File content) throws IOException, SQLException {
        try(BufferedInputStream contentStream = new BufferedInputStream(new FileInputStream(content))) {
            pstmt.setLong(1, id);
            pstmt.setBinaryStream(2, contentStream);
            pstmt.executeUpdate();
        }
    }
}",,,,,,,,,,,,,19/Apr/16 01:37;bryanpendleton;DerbyIssue.java;https://issues.apache.org/jira/secure/attachment/12799412/DerbyIssue.java,01/May/16 16:36;bryanpendleton;JustChangeOffset.diff;https://issues.apache.org/jira/secure/attachment/12801657/JustChangeOffset.diff,29/Apr/16 01:56;bryanpendleton;firstTryAtTest.diff;https://issues.apache.org/jira/secure/attachment/12801368/firstTryAtTest.diff,01/May/16 03:47;bryanpendleton;testForLargeDataSuite.diff;https://issues.apache.org/jira/secure/attachment/12801627/testForLargeDataSuite.diff,19/Apr/16 02:49;bryanpendleton;trivial.diff;https://issues.apache.org/jira/secure/attachment/12799422/trivial.diff,,,,,5.0,,,,,,,,,,,,Seen in production,,,,,,,,,,2016-04-19 01:37:29.931,,,no_permission,,,,,,,,,,,,9223372036854775807,,,Newcomer,Repro attached,Tue May 03 03:22:34 UTC 2016,,,,,,0|i2w9if:,9223372036854775807,,,,,,,,Normal,"19/Apr/16 01:37;bryanpendleton;The problem reproduces for me, just as described, using the
current head of trunk on Windows, with JDK 1.8.0_77-b03

I attached a re-formatted version of the repro program, which
was easier for me to read and follow, as ""DerbyIssue.java"".

I also removed the explicit load of the Derby ClientDriver which
appears to be unnecessary with the repro program, as it uses
the EmbeddedDriver and hence can run with just derby.jar.

Also, to be clear: to run the repro program, you need to edit
the program text to replace the three dots in the next line with
the name of a valid file in your test directory.

    public static final String BLOB_DATA_FILE = ""..."";

I used a 75 MB PDF file that I happened to have sitting around.

The program cleverly loops, counting the size of the blobs
that it has inserted, until it has more than 2 GB of them, so it
doesn't really matter what file you use, but you have to pick a file.

It would be nice to figure out a clever way to have a smaller repro,
as this repro takes several minutes to run on my system, but
for the purposes of demonstrating the bug the repro was great -- thanks!
","19/Apr/16 02:49;bryanpendleton;I'm not really familiar with the code in this part of Derby, but I
took the famous approach of trying to ""do the simplest thing
that could possibly work"", and made the trivial change to the
""lobOffset"" and ""lobLength"" fields in the ImportReadData class.

Attached 'trivial.diff' is the result.

With this patch applied, the test program passes.

I'm not willing to say this is the correct answer; in particular,
I had to down-cast the offset and length fields to int in order
to pass them to ImportLobFile.getString(), which expects int
values for accessing clob data.

But it did make the test program pass, so maybe it's a
start toward a fix of some sort.

I did no other testing at all.","20/Apr/16 13:40;bryanpendleton;With the patch applied, I ran the tools test suite, and there were no failures.

This suggests that a simple path forward might be:
1) Convert the repro into a new test case in ImportExportLobTest.java
2) Commit the new test case, and the trivial diff

I do worry that there may be other similar problems lurking though, so I
intend to (at least) produce a variant of the test case which uses a set of CLOB
columns rather than a set of BLOB columns, to check to see if there is a CLOB
variant of this job lurking.
","21/Apr/16 12:07;knutanders;I agree that the casts from long to int look a bit suspicious. Wouldn't they end up as negative values? The test case doesn't seem to exercise that part of the code, so it's difficult to verify that it works correctly. (I replaced the modified lines in getClobColumnFromExtFileAsString() with ""throw new RuntimeException()"", and the test case still passed.)","29/Apr/16 01:53;bryanpendleton;I finally got some time to try to develop a ""clob"" version of the repro
program, and, as I think we all expected, it fails in a similar fashion:

Caused by: java.lang.NumberFormatException: For input string: ""2147487744""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Integer.parseInt(Integer.java:583)
        at java.lang.Integer.parseInt(Integer.java:615)
        at org.apache.derby.impl.load.ImportReadData.initExternalLobFile(ImportReadData.java:1040)
        at org.apache.derby.impl.load.ImportReadData.getClobColumnFromExtFileAsString(ImportReadData.java:953)
        at org.apache.derby.impl.load.ImportAbstract.getString(ImportAbstract.java:167)
        at org.apache.derby.impl.load.Import.getString(Import.java:45)
        at org.apache.derby.iapi.types.SQLChar.setValueFromResultSet(SQLChar.java:1466)
        at org.apache.derby.impl.sql.execute.VTIResultSet.populateFromResultSet(VTIResultSet.java:688)
        at org.apache.derby.impl.sql.execute.VTIResultSet.getNextRowCore(VTIResultSet.java:461)
        at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(ProjectRestrictResultSet.java:287)
        at org.apache.derby.impl.sql.execute.NormalizeResultSet.getNextRowCore(NormalizeResultSet.java:188)
... (I truncated the stack trace)

So there is clearly more work to be done, to address the issues on the CLOB side.","29/Apr/16 01:56;bryanpendleton;Attached is my first try at writing a regression test for these problems.

Unfortunately, although this regression test appears to demonstrate
the problem with ""clob"" data when the external file exceeds Integer.MAX_VALUE
in size, the test is problematic: it takes more than 1 hour to run.

I hope that I can improve the test program, because obviously
a test that takes an hour is not appropriate to put into our
test suite.

My first ideas are (a) to not commit so often, and (b) to write a
smaller number of larger clob objects.

I'll try some of those ideas, and see if the runtime of the test
is improved at all.

Once I get a reliable test, including the ""blob"" version should
be straightforward.","29/Apr/16 15:19;mikem;there use to be a test suite for tests like this - especially clob/blob testing.  Even if you make it run fast, not everyone/everywhere will have the disk space to run it.  The suite was meant to at least be run once per release and more often by some nightly testing framework if possible - not sure if anyone runs it any more.","30/Apr/16 00:07;bryanpendleton;I think the test suite you might be referring to is the ""largeDataTests""

Unfortunately, all the references I can find to those tests are > 5 years old,
and my memory of how to run the old ""runall"" tests is fading.

Still, I agree with you in principle; I'll do more research here.
","30/Apr/16 00:13;bryanpendleton;Maybe something like:

  java org.apache.derbyTesting.functionTests.harness.RunSuite largeData

might work?
","30/Apr/16 20:39;bryanpendleton;I believe I've got the test able to reproduce both the CLOB and BLOB issues.

However, it still takes a very long time to run.

And, I don't have enough disk space on my (virtual) machine to run the test anymore,
so I'll need to add some more disk space.

It's clear that these tests don't belong in the regular test suite, so I'll look into
placing these tests into the ""largeData"" suite suggested by Mike.","01/May/16 03:47;bryanpendleton;I've moved the test cases to their own test program, so that
they won't be run by the regular test suites, but can still be
run as desired.

Just as Knut Anders predicted, with 'trivial.diff' applied, the
behavior of the CLOB test case changes from the integer
parse error to a ""Negative seek offset"" error, due to casting
the value from a long to an int producing a negative value.

I'll try turning my attention to that detail later; for now I just
wanted to record the progress I'd made.

A snip from the stack trace is below.

Caused by: java.io.IOException: Negative seek offset
        at java.io.RandomAccessFile.seek(RandomAccessFile.java:555)
        at org.apache.derby.impl.load.ImportFileInputStream.seek(ImportLobFile.java:266)
        at org.apache.derby.impl.load.ImportLobFile.getString(ImportLobFile.java:132)
        at org.apache.derby.impl.load.ImportReadData.getClobColumnFromExtFileAsString(ImportReadData.java:959)
","01/May/16 16:36;bryanpendleton;After thinking about it some more in the clear light of morning, I took
a closer look at the reference pages for the CLOB and BLOB data types.

Both data types are limited to 2GB as their maximum length.

So I think the only actual problem here is the offset in the external
file, which needs to be a long to allow for external files > 2GB in size.

The 'JustChangeOffset.diff' patch contains a modification to the offset
field only; the length field is left as an int.

This makes the code diff very small.

I'll run various tests and see how it behaves.","02/May/16 13:41;knutanders;JustChangeOffset.diff looks good to me. Except that jardriftcheck fails when building the jar files, because of the new class in derbyTesting.jar. +1 to commit when that's fixed.

A couple of nits:

{noformat}
--- java/engine/org/apache/derby/impl/load/ImportLobFile.java	(revision 1741376)
+++ java/engine/org/apache/derby/impl/load/ImportLobFile.java	(working copy)
@@ -128,7 +128,7 @@
      * @param length  length of the the data.
      * @exception  IOException  on any I/O error.     
      */
-    public String getString(int offset, int length) throws IOException {
+    public String getString(long offset, int length) throws IOException {
         lobInputStream.seek(offset);
         lobLimitIn.clearLimit();
         lobLimitIn.setLimit((int) length);
{noformat}

While you're at it, maybe also remove the redundant cast of length to int in the above call to setLimit(), so that readers don't have to spend cycles figuring out what the purpose of the cast is?

You might also want to clean up the indentation in the test case. It uses a mix of tabs and spaces, and it doesn't always seem to agree with itself if tabs are 4 or 8 characters wide.

{noformat}
+        PreparedStatement ps = getConnection().prepareStatement(
+                     ""insert into DERBY_6884_TESTCLOB values(? , ?)"" );
{noformat}

BaseJDBCTestCase has a helper method for preparing statments, so the above could have been replaced with the slightly simpler

{code}
        PreparedStatement ps = prepareStatement(
                     ""insert into DERBY_6884_TESTCLOB values(? , ?)"" );
{code}

Then you don't have to close the prepared statement manually in the test case.","03/May/16 03:20;jira-bot;Commit 1742057 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1742057 ]

DERBY-6884: SYSCS_IMPORT_TABLE_LOBS_FROM_EXTFILE can't import lob data

This change modifies the ImportLobFile.getString() and
ImportReadData.initExternalLobFile() methods so that they use a Java ""long""
variable for the offset into the external lob file; prior to this change
they were using a Java ""int"" variable and hence would malfunction when the
lob offsets exceeded Integer.MAX_VALUE ( 2,147,483,647 ).

The regression test which demonstrates these problems is a bit slow to run;
on my system, it takes approximately 15 minutes to execute, and requires
about 10 GB of available disk space during the test run. Therefore, the
test cases are placed in a new test program (Derby6884Test), which is not
listed in the ""standard"" system test suites, but rather is only added to the
""largedata"" suite. The new test can also be run by itself, e.g.:

ant -Dderby.junit.testclass=org.apache.derbyTesting.functionTests.tests.largedata.Derby6884Test junit-clean junit-single","03/May/16 03:22;bryanpendleton;Thanks Knut Anders for the careful review and good suggestions.

The blank/tab thing continues to confound my editor settings, I'm afraid.
Since the test program is entirely new source, I decided to use only
spaces in that (small) source file. That way, it is at least internally
consistent.

The code changes in question are small, and could easily be
back-ported to earlier releases, but I am not planning to do that
at this time.",,,,,,,,,,,,,,,,,,,,,,,,
Update failing with java.sql.SQLDataException ,DERBY-6880,12954458,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Major,Fixed,bryanpendleton,simonzee,simonzee,29/Mar/16 23:19,28/Apr/16 11:59,13/Mar/19 22:42,28/Apr/16 02:44,10.12.1.1,10.13.1.0,,,,,0,,"When updating a single column in a table using executeUpdate() via Vert.x I am receiving the following exception:

java.sql.SQLDataException: Invalid character string format for type long.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(SQLExceptionFactory.java:84)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Util.java:233)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(TransactionResourceImpl.java:424)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(TransactionResourceImpl.java:353)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(EmbedConnection.java:2405)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(ConnectionChild.java:88)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1432)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(EmbedPreparedStatement.java:1709)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(EmbedPreparedStatement.java:320)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(EmbedPreparedStatement.java:309)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:410)
	at io.vertx.ext.jdbc.impl.actions.JDBCUpdate.execute(JDBCUpdate.java:50)
	at io.vertx.ext.jdbc.impl.actions.JDBCUpdate.execute(JDBCUpdate.java:34)
	at io.vertx.ext.jdbc.impl.actions.AbstractJDBCAction.handle(AbstractJDBCAction.java:48)
	at io.vertx.ext.jdbc.impl.actions.AbstractJDBCAction.handle(AbstractJDBCAction.java:33)
	at io.vertx.core.impl.ContextImpl.lambda$executeBlocking$15(ContextImpl.java:296)
	at io.vertx.core.impl.OrderedExecutorFactory$OrderedExecutor.lambda$new$261(OrderedExecutorFactory.java:91)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ERROR 22018: Invalid character string format for type long.
	at org.apache.derby.iapi.error.StandardException.newException(StandardException.java:290)
	at org.apache.derby.iapi.error.StandardException.newException(StandardException.java:285)
	at org.apache.derby.iapi.types.SQLChar.getLong(SQLChar.java:447)
	at org.apache.derby.impl.sql.execute.UpdateResultSet.collectAffectedRows(UpdateResultSet.java:534)
	at org.apache.derby.impl.sql.execute.UpdateResultSet.open(UpdateResultSet.java:272)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(GenericPreparedStatement.java:473)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(GenericPreparedStatement.java:352)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(EmbedStatement.java:1340)
	... 13 more

Further details and discussion can be found here:

https://mail-archives.apache.org/mod_mbox/db-derby-user/201603.mbox/%3CCAHbUnCXkHMKE1u9R3D-z9Njp8goAV7%2B0vPOmgafH8DCqG8mSpQ%40mail.gmail.com%3E

Notably, I have tried executing the same update via other means (for example, manually via SquirrelSQL, and via the ORMLite framework) and the update succeeds. This may be due to the exact JDBC APIs they are using (for example, SquirrelSQL is not using a prepared statement, and ORMLite updates all the columns when it updates a table rather than just some of them).

I have created a complete but minimal example that illustrates the problem in the following GitHub project:

https://github.com/ssadedin/DerbyDebug

My derby / system information is as follows:

$ java -cp 'lib/*' org.apache.derby.tools.sysinfo
------------------ Java Information ------------------
Java Version:    1.8.0_72
Java Vendor:     Oracle Corporation
Java home:       /Library/Java/JavaVirtualMachines/jdk1.8.0_72.jdk/Contents/Home/jre
Java classpath:  lib/derby.jar:lib/derbyclient.jar
OS name:         Mac OS X
OS architecture: x86_64
OS version:      10.11.1
Java user name:  simon
Java user home:  /Users/simon
Java user dir:   /Users/simon/Documents/workspace/BrokenDerby
java.specification.name: Java Platform API Specification
java.specification.version: 1.8
java.runtime.version: 1.8.0_72-b15
--------- Derby Information --------
[/Users/simon/Documents/workspace/BrokenDerby/lib/derby.jar] 10.12.1.1 - (Unversioned directory)
[/Users/simon/Documents/workspace/BrokenDerby/lib/derbyclient.jar] 10.12.1.1 - (Unversioned directory)
------------------------------------------------------
----------------- Locale Information -----------------
------------------------------------------------------
------------------------------------------------------
",,,,,,,,,,,,,01/Apr/16 00:44;bryanpendleton;repro.diff;https://issues.apache.org/jira/secure/attachment/12796438/repro.diff,02/Apr/16 00:51;bryanpendleton;standalone.java;https://issues.apache.org/jira/secure/attachment/12796634/standalone.java,02/Apr/16 14:59;bryanpendleton;undo6742.diff;https://issues.apache.org/jira/secure/attachment/12796677/undo6742.diff,10/Apr/16 16:31;bryanpendleton;undoMoreTests.diff;https://issues.apache.org/jira/secure/attachment/12797923/undoMoreTests.diff,,,,,,4.0,,,,,,,,,,,,Crash,Regression,Seen in production,,,,,,,,2016-04-01 00:44:15.249,,,no_permission,,,,,,,,,,,,9223372036854775807,,,Repro attached,,Thu Apr 28 11:59:11 UTC 2016,,,,,,0|i2vden:,9223372036854775807,,,,,,,,,"01/Apr/16 00:44;bryanpendleton;Attached 'repro.diff' adds a new test case to the
GeneratedColumnsTest suite which demonstrates
the ""Invalid character string format for type long"" error.
","01/Apr/16 01:02;bryanpendleton;The code which calls getLong, provoking the exception,
is new code added by DERBY-6742, so marking this
issue as linked to that one.","01/Apr/16 03:17;bryanpendleton;In the code:

{code}
                for(col=1;col<=maxColumns;col++)
                {
                    ColumnDescriptor cd = td.getColumnDescriptor(col);
                    if(cd.isAutoincrement())
                    {
                        break;
                    }
                }

                if(col <= maxColumns)
                {
                    DataValueDescriptor dvd = row.cloneColumn(col);
                    identityVal = dvd.getLong();
                }
{code}

is where we go astray. The for loop locates the correct column descriptor,
it is for the ""id"" column of table ""pipeline_command"".

But the table descriptor in ""td"" is not accurate for use with the data in ""row"".

In this particular case, ""td"" describes the underlying ""pipeline_command"" table,
and it appears to have all the relevant columns just as you would expect, and
so column 1 is indeed the autogenerated ""ID"" column.

However, column 1 of ""row"" is nothing the same at all. ""row"" is a ValueRow,
and has only 3 columns, in my case the three columns are:
  - the old value of the ""status"" column
  - the new value of the ""status"" column
  - some sort of value that is displayed as ""(1,8)"", which I haven't decoded yet.

I think that ""row"", at the time when we are running, is some sort of intermediate
object, and the code in UpdateResultSet is getting all confused because it
is trying to use the underlying table descriptor to interpret the values in ""row"",
but ""row"" is not a row from the base ""pipeline_command"" table anymore, it is
some intermediate ""old-value,new-value"" row that has been constructed by the query.

Anyway, that's as far as I got today fooling around with the reproduction script.
","02/Apr/16 00:51;bryanpendleton;standalone.java is the same reproduction program, but as a
standalone JDBC program, so easier to run under debuggers, etc.","02/Apr/16 01:17;bryanpendleton;Here's a snip from the change that was submitted for DERBY-6414:

{code}
+     *  The 2nd array, rla has a spot for each of the columns in the table, 
+     *  with non null value for auto generated column. But in case of Update,
+     *  resultDescription does not include all the columns in the table. It
+     *  only has the columns being touched by the Update statement(the rest of
+     *  the columns in the table will retain their original values), and for 
+     *  each of those touched columns, it has a duplicate entry in 
+     *  resultDescription in order to have before and after values for the 
+     *  changed column values. Lastly, it has a row location information for 
+     *  the row being updated. This difference in array content of 
+     *  resultDescription requires us to have separate implementation of this
+     *  method for insert and update.
{code}

I think this description (before and after values for the changed column values,
plus the row location information for the row being updated) exactly describes
the 3-column ValueRow object that I am seeing when I run the reproduction.

I have the before and after values for the 'status' column as columns 1 and 2
of my 3-column row, and the third column (1,8) is the row location information.
","02/Apr/16 01:23;bryanpendleton;One possibility is to back-out the change to UpdateResultSet made by DERBY-6742 in 
revision https://svn.apache.org/r1628596. Of course, that would re-open DERBY-6742.
","02/Apr/16 14:59;bryanpendleton;As an experiment, I deleted the section of code added to
UpdateResultSet.collectAffectedRows() by DERBY-6742,
and I ran the complete regression test suite, including the
new test of the DERBY-6880 reproduction script.

All tests were successful.

So apparently reverting that section of the DERBY-6742
changes does not cause any of our existing regression
test suites to fail.

I'm not sure what to conclude from this. There were a substantial
number of new tests added by https://svn.apache.org/r1625884
but it appears that none of those test cases directly exercised
the code in UpdateResultSet.collectAffectedRows, whereas
the new reproduction script from DERBY-6880 does exercise
that code.

Frankly, it's hard to tell whether removing that section of code
would actually re-open DERBY-6742 or not.

I may just let this one sit for a bit, until others have had a chance
to think about it.

Simon, as an experiment (not sure if you can do this), you could
take this patch (undo6742.diff) and apply it to your own copy of
either the Derby 10.12 source tree or the Derby mainline source tree,
build yourself a custom version of derby.jar, and confirm that this
change resolves the problem you are seeing with your application
in the Vert.x framework.","03/Apr/16 01:43;simonzee;Thanks for all your work on this Bryan!

I applied the undo patch that you attached and that resolved the problem for me. I will leave this patched version in place and work with it over the next few days and report back if there are any regressions. ","05/Apr/16 08:54;peen;If I understand the javadoc describing the section you've tried commenting out, I think you should be able to get the identity value of the updated row after executing an update affecting a single row, like the one in the test.

So if you expand the test to also check the result of ustmt.getGeneratedKeys(), I think it will fail (always returns 0) when the section of code in UpdateResultSet.collectAffectedRows() is commented out.","06/Apr/16 13:55;bryanpendleton;That's a really interesting perspective, thank you for suggesting it!

The JavaDoc in question, I think, is for Statement.getGeneratedKeys:
https://docs.oracle.com/javase/7/docs/api/java/sql/Statement.html#getGeneratedKeys()

It says:
{quote}
    Retrieves any auto-generated keys created as a result of executing this 
    Statement object. If this Statement object did not generate any keys, 
    an empty ResultSet object is returned.

    Note:If the columns which represent the auto-generated keys were not 
    specified, the JDBC driver implementation will determine the columns 
    which best represent the auto-generated keys.
{quote}
Which isn't a lot of detail. But, to me, it clearly uses the phrases
{quote}
    auto-generated keys created as a result
{quote}
and
{quote}
    generate any keys
{quote}
To me, that specifically means that the update must cause a *NEW* value for
a column which is ""GENERATED ... AS IDENTITY"" to be generated by the
statement, and the reproduction's statement:
{quote}
    update pipeline_command set status = 'WAIT RESULT' where id = ?
{quote}
only updates the ""status"" column, and does not cause any new values to
be generated for the ""id"" column.

However, while doing a bit of web searching, I came across this page from
the DB2 documentation set:

    https://www.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.apdv.java.doc/src/tpc/imjcc_t0057054.html

which suggests a thoroughly different reading of how Statement.getGeneratedKeys
should behave, stating that:
{quote}
    The following code names the EMPNO column as an automatically 
    generated key, updates the thirty rows in the EMP_BONUS table, 
    and retrieves the values of EMPNO for the updated rows. 
{quote}
The statement in the DB2 example, like the statement in this issue's reproduction,
does not modify nor create new values for any automatically generated keys, but
the DB2 example suggests that Statement.getGeneratedKeys is intended to
return the generated key values of *other* columns in the rows which were updated,
even if those columns were not actually created or updated by the statement.

It's a bit of a puzzle, actually. But, your comment does suggest some new test
cases that I might write (thank you!)

I'll try to find some time to dig into that some more.

In the meantime, if anyone knows of any more information about how Statement.getGeneratedKeys
is intended to behave, that would be welcomed.","07/Apr/16 14:11;peen;In case you could use this info; 
I tried changing the PreparedStatement in the test to this:

{quote}
PreparedStatement ustmt = conn.prepareStatement(
				""update pipeline_command set status = 'WAIT RESULT'"" +
						""       where id = ?"", new String[] \{""ID""\});
{quote}

It results in the same error on Derby as before. But running this against DB2 makes Statement.getGeneratedKeys return the ID of the updated row (In this test '2'). Running the original PreparedStatement against DB2 makes Statement.getGeneratedKeys return an empty resultsset.","09/Apr/16 18:33;bryanpendleton;Soren, does DB2 allow
{code}
PreparedStatement ustmt = conn.prepareStatement(
""update pipeline_command set status = 'WAIT RESULT'"" +
"" where id = ?"", new String[] {""STATUS""});
{code}","10/Apr/16 16:31;bryanpendleton;I added some more tests experimenting with the variant
of Connection.prepareStatement() that allows the program
to specify an array of column names to indicate the 
auto-generated columns of interest for getGeneratedKeys.

It seems like the array of column names is case-sensitive.
Is this wrong? Shouldn't these column names be handled
in the typical case-insensitive manner of SQL, given that
they aren't ""delimited identifiers""? The Javadoc for
java.sql.Connection didn't provide any guidance on that.

Derby throws the same X0X0F exception in all three cases:
- column name is unknown
- column name is valid, but is in the wrong case
- column name is valid, but does not specify a IDENTITY column

Anyway, I still feel like backing out the code in UpdateResultSet
is the right thing to do. The DB2 behaviors described by Soren Peen
are fascinating, but it would be a substantial change to the Derby
runtime model to implement them.

And, the code in UpdateResultSet seems fundamentally incorrect
to me, as it is attempting to use the column descriptions from the
full table descriptor for the UPDATE statement's target table to
access the columns in the transient, projected ""row"" which has
the subset of (before,after,rowid) values that are used by things
like trigger execution. Some alternate sort of data structures, as
well as a different code-generation strategy, might be needed in
order to allow us to capture the values of IDENTITY columns
during an UPDATE statement.","28/Apr/16 02:39;jira-bot;Commit 1741380 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1741380 ]

DERBY-6880: Update failing with java.sql.SQLDataException

This change reverts part of the changes made by revision 1628596 for DERBY-6742.

Specifically, the section of code added to UpdateResultSet.collectAffectedRows
is removed. That code caused problems with certain SQL UPDATE statements
which were compiled with Statement.RETURN_GENERATED_KEYS. The new test cases
added by this change include several examples of such SQL statements.

The JDBC documentation for the intended behavior of UPDATE statements with the
RETURN_GENERATED_KEYS option is unclear; the intended behavior is much
clearer with INSERT statements. Given that I don't understand the intended
behavior, it seems safer to me to return Derby to the previous state for
UPDATE statements; namely, that no attempt is made to compute the set of
generated keys for an UPDATE statement.","28/Apr/16 02:44;bryanpendleton;I've gone as far on this issue as I care to, for the time being. My feeling
is that this patch is an incremental improvement: the new test cases
pass, including several test cases which worked in older Derby releases
and broke in the latest release, and no known test cases are broken by
this patch.

It's clear that I don't really understand what is meant by the JDBC
documentation for Statement.RETURN_GENERATED_KEYS for an
UPDATE SQL statement, and it's also clear that I don't know how to
implement the behavior that is exhibited by other systems (notably
the DB2 behaviors that Soren Peen described).

But, for the time being, those are not my itch to scratch, whereas
fixing the regression is, so I decided to commit this patch and mark
this issue resolved.

We certainly could back-port this change to 10.12, if anyone is planning
to make a patch release of 10.12, but I'm not intending to back-port
this change at this time.","28/Apr/16 11:59;simonzee;Bryan, thanks for your persistence and diligence on this issue!

I think you've made the right call. While there was clearly an intention behind the code changes that were made, the regression caused seems worse than whatever subtle feature of the JDBC APIs they were intended to implement.",,,,,,,,,,,,,,,,,,,,,,,
Community page describing contribution process has stale testing section,DERBY-6965,13106243,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Minor,Fixed,bryanpendleton,bryanpendleton,bryanpendleton,30/Sep/17 17:31,01/Oct/17 17:16,13/Mar/19 22:42,01/Oct/17 17:16,,10.15.1.3,,Web Site,,,0,,"The ""Contribute Code or Documentation"" page at http://db.apache.org/derby/derby_comm.html#Contribute+Code+or+Documentation

is out of date, specifically w.r.t. the testing process. Running the old derbyall test harness is no longer the standard process (although it does no harm, it doesn't really run the important set of tests).

The following changes should be made:
# The page should contain a link to the wiki testing page: https://wiki.apache.org/db-derby/DerbyTopLevelJunitTests
# The item ""run the tests"" on the page should say to run 'ant junitreport'.
# We could still include the suggestion to run derbyall with the old harness, but I suggest that we remove that, because it's pretty arcane and (IMO) not really necessary.",,,,,,,,,,,,,30/Sep/17 18:31;bryanpendleton;derby_comm.html;https://issues.apache.org/jira/secure/attachment/12889863/derby_comm.html,30/Sep/17 18:32;bryanpendleton;derby_source.html;https://issues.apache.org/jira/secure/attachment/12889862/derby_source.html,30/Sep/17 18:30;bryanpendleton;siteDocs.diff;https://issues.apache.org/jira/secure/attachment/12889864/siteDocs.diff,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,2017-10-01 16:24:29.475,,,no_permission,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 01 17:16:24 UTC 2017,,,,,,0|i3kqy7:,9223372036854775807,,,,,,,,,"30/Sep/17 18:33;bryanpendleton;Attached 'siteDocs.diff' contains the updates I propose to improve the ""testing"" sections of the derby web site docs.

I also attached the built versions of derby_comm and derby_source so you can click on them and let me know what you think!","01/Oct/17 16:24;jira-bot;Commit 1810268 from [~bryanpendleton] in branch 'site/trunk'
[ https://svn.apache.org/r1810268 ]

DERBY-6965: Community page has outdated testing instructions

This change updates derby_comm.html so that it instructs
developers to use 'ant junitreport' to run tests of their change.

It also points to the wiki for further resources.","01/Oct/17 17:16;bryanpendleton;The updated pages appear to be showing up on the web site.

I'm always a little uncertain about the web site updates, because I get a lot of ""fake"" diffs due to things like Forrest version numbers embedded in the output, etc.

But the web site appears to be displaying properly, so marking as resolved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix links to CI test results,DERBY-6886,12964039,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Minor,Fixed,bryanpendleton,bryanpendleton,bryanpendleton,30/Apr/16 02:31,03/May/16 08:06,13/Mar/19 22:42,30/Apr/16 15:27,,10.13.1.0,,Web Site,,,0,,"The information on https://db.apache.org/derby/derby_tests.html
is out of date.

The best current test results are the Apache Jenkins test runs at:

https://builds.apache.org/job/Derby-trunk-suites.All/

The derby_tests.html page should be updated to reflect that status.",,,,,,,,,,,,,30/Apr/16 02:34;bryanpendleton;updateSite.diff;https://issues.apache.org/jira/secure/attachment/12801576/updateSite.diff,30/Apr/16 02:34;bryanpendleton;updateSource.diff;https://issues.apache.org/jira/secure/attachment/12801575/updateSource.diff,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,2016-04-30 15:24:58.333,,,no_permission,,,,,,,,,,,,9223372036854775807,,,,,Tue May 03 08:06:41 UTC 2016,,,,,,0|i2wzwn:,9223372036854775807,,,,,,,,,"30/Apr/16 02:34;bryanpendleton;This patch removes the links to the IBM and Oracle test results,
since they are hard to reach from the Open Source community,
and replaces them by a link to the Apache Jenkins latest test results.","30/Apr/16 15:24;jira-bot;Commit 1741778 from [~bryanpendleton] in branch 'site/trunk'
[ https://svn.apache.org/r1741778 ]

DERBY-6886: Update links to test results

Neither IBM nor Oracle host externally-available machines with current
Derby test results, and the links to the old locations of such results
are no longer operational, so this change removes those links from the
Derby web site.

The change also modifies the site to include a link to the Apache Jenkins
test results for Derby.","30/Apr/16 15:27;bryanpendleton;For some background discussion on this, please see:

http://mail-archives.apache.org/mod_mbox/db-derby-dev/201510.mbox/%3C561307C9.6070508%40gmail.com%3E

and

http://mail-archives.apache.org/mod_mbox/db-derby-dev/201604.mbox/%3C9c76aedd-ea26-c3dc-29bc-725c00a89b88%40gmail.com%3E

I've updated the page to show the Apache Jenkins test results link,
and to remove the no-longer-operational links to other test results.","03/May/16 08:06;jira-bot;Commit 1742066 from [~knutanders] in branch 'site/trunk'
[ https://svn.apache.org/r1742066 ]

DERBY-6886: Fix links to CI test results

Also add link to the full list of Derby test jobs on builds.apache.org.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unload EmbeddedDriver by using System.gc()?,DERBY-6878,12952472,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Minor,Fixed,,martin stanik,martin stanik,22/Mar/16 15:11,29/Mar/16 06:44,13/Mar/19 22:42,27/Mar/16 18:55,,10.5.1.1,,Documentation,,,0,easyfix,"derby development doc suggests one can 'unload driver class':
> You cannot explicitly request that the JVM unload a class, but you can ensure that the EmbeddedDriver class is unloaded by using a System.gc() ....

this advice is unfortunate, because:
1. it is wrong - one can not unload single class. 
    only whole classloader with all it's classes might be unloaded (this needs be organized ahead)
2. one may get false impression unloading driver is good practice/desirable/necessary.
    i believe it is not.

i suggest to remove the advice unless there is provided a scenario when unloading driver is needed +  explanation how to do it properly (derby driver needs be loaded into extra classloader).
    ",,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,2016-03-24 02:14:21.142,,,no_permission,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 29 06:44:35 UTC 2016,,,,,,0|i2v15r:,9223372036854775807,,,,,,,,Low,"24/Mar/16 02:14;bryanpendleton;Just to be clear, which exact documentation are you referring to? Can you post an exact link?","24/Mar/16 08:22;martin stanik;originally, i found it in https://db.apache.org/derby/docs/10.0/manuals/develop/develop12.html
however, google provided more links:
https://db.apache.org/derby/docs/10.4/devguide/rdevcsecure26537.html
http://db.apache.org/derby/docs/10.4/devguide/tdevdvlp20349
https://db.apache.org/derby/docs/10.4/devguide/derbydev.pdf
https://db.apache.org/derby/docs/10.1/devguide/derbydev.pdf
https://db.apache.org/derby/docs/10.1/devguide/tdevdvlp20349.html

i see this are all links related to 10.1 and 10.4.
the problem was then fixed by https://issues.apache.org/jira/browse/DERBY-3585
","27/Mar/16 18:50;bryanpendleton;It seems a bit inaccurate to mark this as a duplicate of DERBY-3585, particularly
since the removal of the incorrect GC suggestion in DERBY-3585 was a bit of
a tangent from the actual purpose of DERBY-3585.

However, it does seem that DERBY-3585 has adequately addressed this
issue, so we no longer need to leave this issue open.","27/Mar/16 18:55;bryanpendleton;Marked as fixed by subversion revision 644555
","29/Mar/16 06:44;martin stanik;thanks, Bryan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
insertRow() and updateRow() fail with syntax error when column has an alias,DERBY-1773,12348792,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Minor,Fixed,bryanpendleton,knutanders,knutanders,29/Aug/06 07:09,13/Mar/16 02:28,13/Mar/19 22:42,13/Mar/16 02:28,10.2.1.6,10.13.1.0,,JDBC,,,0,derby_triage10_5_2,"When the select query used in an updatable result set has column aliases, a syntax error is thrown when executing ResultSet.insertRow() and ResultSet.updateRow(). The problem is seen on embedded and client. Repro is attached.

Exception in thread ""main"" ERROR 42X14: 'A1' is not a column in table or VTI 'APP.T'.
        at org.apache.derby.iapi.error.StandardException.newException(StandardException.java:316)
        at org.apache.derby.impl.sql.compile.ResultColumn.bindResultColumnByName(ResultColumn.java:677)
        at org.apache.derby.impl.sql.compile.ResultColumnList.bindResultColumnsByName(ResultColumnList.java:682)
        at org.apache.derby.impl.sql.compile.ResultSetNode.bindResultColumns(ResultSetNode.java:683)
        at org.apache.derby.impl.sql.compile.SelectNode.bindResultColumns(SelectNode.java:742)
        at org.apache.derby.impl.sql.compile.UpdateNode.bind(UpdateNode.java:349)
        at org.apache.derby.impl.sql.GenericStatement.prepMinion(GenericStatement.java:345)
        at org.apache.derby.impl.sql.GenericStatement.prepare(GenericStatement.java:111)
        at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(GenericLanguageConnectionContext.java:723)
        at org.apache.derby.impl.jdbc.EmbedResultSet.updateRow(EmbedResultSet.java:3734)
        at Alias.main(Alias.java:15)",,,,,,,,,,,,,29/Aug/06 07:09;knutanders;ASF.LICENSE.NOT.GRANTED--Alias.java;https://issues.apache.org/jira/secure/attachment/12339745/ASF.LICENSE.NOT.GRANTED--Alias.java,15/Feb/10 18:16;bryanpendleton;NoUpdatesToAliasedColumnsWithTest.diff;https://issues.apache.org/jira/secure/attachment/12435890/NoUpdatesToAliasedColumnsWithTest.diff,05/Mar/16 21:08;bryanpendleton;anyVsAll.diff;https://issues.apache.org/jira/secure/attachment/12791636/anyVsAll.diff,12/Mar/16 19:29;bryanpendleton;fixedComments.diff;https://issues.apache.org/jira/secure/attachment/12792964/fixedComments.diff,02/Mar/10 18:39;bryanpendleton;updatedToHeadMarch2010.diff;https://issues.apache.org/jira/secure/attachment/12437626/updatedToHeadMarch2010.diff,05/Mar/16 21:08;bryanpendleton;updatedToHeadMarch2016.diff;https://issues.apache.org/jira/secure/attachment/12791635/updatedToHeadMarch2016.diff,,,,6.0,,,,,,,,,,,,,,,,,,,,,,2010-02-13 17:39:50.405,,,no_permission,,,,,,,,,,,,22695,,,Repro attached,,Sun Mar 13 02:28:18 UTC 2016,,,,,,0|i06ny7:,36814,,,,,,,,Normal,09/Jul/09 09:30;knutanders;Triaged for 10.5.2.,"13/Feb/10 17:39;bryanpendleton;I've been investigating this issue a bit, and want to note 
what I've found, as well as several ideas.

Firstly, the core of the issue occurs in EmbedResultSet.updateRow, which is
attempting to construct and execute, on the fly, a statement like:

  update t set x = 2 where current of my-cursor-name

The problem arises due to the fact that the select statement:

  select * from t as a(a1)

has introduced aliases for both the table (T => A) and the column (X => A1)

Now, when EmbedResultSet.updateRow processes the table, it is careful
to use the true underlying base table name (T):

               updateWhereCurrentOfSQL.append(getFullBaseTableName(targetTable));//got the underlying (schema.)table name

The problem arises a bit later, when it tries to access the columns which
are being updated:

            for (int i=1; i<=resultDescription.getColumnCount(); i++) { //in this for loop we are constructing columnname=?,... part of the update sql
                if (columnGotUpdated[i-1]) { //if the column got updated, do following
                    if (foundOneColumnAlready)
                        updateWhereCurrentOfSQL.append("","");
                    //using quotes around the column name to preserve case sensitivity
                    updateWhereCurrentOfSQL.append(IdUtil.normalToDelimited(
                            resultDescription.getColumnDescriptor(i).getName()) + ""=?"");
                    foundOneColumnAlready = true;
                }
            }

The issue here is that resultDescription.getColumnDescriptor(i) is a
GenericColumnDescriptor, which was constructed from a ResultColumnDescriptor
by the method ResultColumnList.makeResultDescriptors(), and this code doesn't
have enough ""smarts"" to understand the difference between the column's ""exposedName"" (A1)
and its true base column name (X).

So, I can see two possibilities:
1) Enhance GenericColumnDescriptor, ResultColumnDescriptor, and (probably) ResultColumn,
so that we have a new method GenericColumnDescriptor.getBaseColumnName(), and call
that new method from EmbedResultSet.updateRow.
2) Refuse the statement. Change (probably) SelectNode.isUpdatableCursor() so that it
looks through its ResultColumnList to see if any of the result columns have been aliased,
and state that a SELECT with aliased column names is NOT a legal updatable SELECT statement.

I'll investigate both possibilities further, just wanted to note my results so far.","13/Feb/10 20:03;bryanpendleton;Interestingly, UpdatableResultSetTest.testDeleteRowWithCorrelationForColumnName()
has some very similar, but not identical, scenarios.

That test case appears to state that updating a column which has an alias should
not be allowed.

I'll study this test case in more detail, and try to figure out why we refuse the statement
in that case, but not in the scenario from the repro script.
","14/Feb/10 15:37;knutanders;Could the difference be explicit vs implicit FOR UPDATE clause? Ref manual says that correlation name cannot be used on updatable columns. http://db.apache.org/derby/docs/dev/ref/rrefcorrelationname.html

Updatable result sets used to require FOR UPDATE until DERBY-231. The test case in UpdatableResultSetTest appended ""FOR UPDATE of c1"" to the select statement, whereas the repro doesn't use FOR UPDATE.","14/Feb/10 15:42;knutanders;It made no difference to add FOR UPDATE to the repro. When I added FOR UPDATE OF X, I got NPE/assert failure.","14/Feb/10 16:05;bryanpendleton;Thanks Knut, I think you are exactly right. I'll pursue the ""refuse it with an error""
approach, and I'll try to include test cases both with and without FOR UPDATE.

Thanks for the pointer to DERBY-231, the behavior is making more sense now.
","15/Feb/10 18:16;bryanpendleton;Attached is a patch proposal. I'm still running tests, and would
appreciate suggestions for more tests.

This patch makes two basic changes:
1) It modifies ResultColumnList.markUpdatableByCursor so that
    it only marks a ResultColumn as updatable if it is not aliased,
    that is, if the actual column name of its ColumnReference matches
    the ResultColumn name.
2) It modifies SelectNode.isUpdatableCursor to call the new method
     FromTable.columnsAreUpdatable, which checks the columns to
    see if they have been aliased, in which case the select is NOT updatable.

There are various aspects to this code that I'm not completely clear on.
For one thing, I'm not clear on the differences between the following
two techniques for introducing column aliasing:

      SELECT c1 as a1, c2 as a2 from t1 as abcde

versus

      select * from t1 as a(a1,a2)

They seem to take substantially different paths through the code and
the resulting data structures are different, so the two basic changes that
I made seemed necessary, as the first change handles the first statement,
and the second change handles the second statement.

Secondly, I'm not clear on the overall semantics of ""the ResultSet is updatable""
versus ""this particular column is updatable"". It seems like the FOR UPDATE OF
clause allows for the specification of a particular *subset* of columns which
are updatable, but one can also simply say ""FOR UPDATE"", or can in fact
omit the clause entirely (this is DERBY-231), in which case the implication is,
I guess, that all columns must be updatable in order for the ResultSet to be updatable?

Or is it more dynamic? Is the intent that, if I say ""FOR UPDATE OF C1,C2"", then
*only* C1 and C2 are updatable, but if I say just ""FOR UPDATE"" or omit it entirely,
then so long as at least one column is updatable, the ResultSet should be updatable,
and the computation of updatability must be deferred until the actual attempt to
perform an update (by calling rs.updateXXX) is performed?

As I said, it would be great to have suggestions for more test cases to add!
","18/Feb/10 11:28;knutanders;My understanding is that ""FOR UPDATE OF C1,C2"" means that the cursor should be updatable, but you can only update the columns C1 and C2 using that cursor. If there's an alias on C1 or C2, the statement should fail. Aliases on other columns than C1 and C2 should be allowed, and the cursor should still be updatable if they are aliased. If I read the patch correctly, it will make the cursor read only as long as there is at least one column that's aliased. I don't think it's a big usability issue if we choose to disallow aliases altogether on updatable cursors, but that would require changes to the reference manual [1] and probably also warrant a release note, so I'd prefer not to impose that restriction.

I agree that the two different ways of aliasing (C2 AS A2 vs T1 AS A(A1,A2)) are equivalent with regard to this issue and should be handled the same way.

I also agree that FOR UPDATE with no column list, or an implicit FOR UPDATE clause (DERBY-231), means that all columns are updatable. Perhaps the alternative, more dynamic semantics are more convenient in some cases, but we already have syntax for making just a subset of the columns updatable, so I don't think it adds much value.

[1] http://db.apache.org/derby/docs/dev/ref/rrefsqlj41360.html#rrefsqlj41360__sqlj15384","02/Mar/10 18:39;bryanpendleton;Just updated the patch to the latest trunk head to resolve a few merges, and re-attached it.","05/Mar/16 21:08;bryanpendleton;Uhm, I guess I lost track of this issue, for it's been 6 years! :)

The first thing I did was to bring the previous patch up to date,
since a lot can change in 6 years. 'updatedToHeadMarch2016.diff'
is the result, and it builds and passes tests with the head of trunk.

Upon closely re-reading Knut Anders's comments from 6 years ago,
I remembered that I had not addressed his observation that it
would be possible for some, but not all, of the columns in the
result list to be aliased, and only those columns which are aliased
should be non-updatable; the unaliased columns should still be
updatable.

After thinking about that for a while, it seemed to me that this
merely required changing the proposed ResultColumnList.columnsAreUpdatable()
method so that it tested whether *any* columns in the list were
updatable, as opposed to requiring that *all* columns in the
list were updatable.

So I did that, and added a few additional test cases, and the
results appear to be satisfactory.

The revised patch is attached as anyVsAll.diff.

I'm running tests on it now, to see how it behaves.

Sorry for the 6 year gap, but if anyone is still interested
in this issue, feedback would be gratefully received.","12/Mar/16 19:29;bryanpendleton;Attached 'fixedComments.diff' is essentially identical
to the previous patch, but I updated some of the comments
to be more accurate, and changed the string text in
a SanityManager,Debug statement. So, no functional
change, just cleanup.

All my regression tests have been clean.","12/Mar/16 23:19;jira-bot;Commit 1734744 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1734744 ]

DERBY-1773: cursor updates fail with syntax error when column has an alias

This change enhances the runtime column analysis code so that an updatable
cursor can make a more nuanced decision about whether a column update is
or is not allowed.

Specifically, certain columns may not be updated, if they have been aliased.

Prior to this change, a confusing syntax error message would be delivered
when attempting to update an aliased column. Now, a more clear error message
is delivered, pointing at the fact that the aliased column is not in the
FOR UPDATE list of the cursor.

So the net result is (at least, should be) that the same set of queries are
accepted, but those that are not accepted have a slightly more clear message
issued when they are detected.",13/Mar/16 02:28;bryanpendleton;I've completed the work I intended for this issue; resolving.,,,,,,,,,,,,,,,,,,,,,,,,,,
ResultSetMetaData.getScale returns inconsistent values for DOUBLE type.,DERBY-853,12327925,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Trivial,Fixed,Dnj,djd,djd,21/Jan/06 04:28,04/Aug/16 02:26,13/Mar/19 22:42,04/Aug/16 02:26,10.2.1.6,10.13.1.0,,JDBC,,,0,derby_triage10_5_2,"If a DOUBLE column is returned in the result set then getScale() returns 0.
If a DOUBLE expression is returned and the expression is the result of a DOUBLE combined with a DECIMAL then it seems the scale from the decimal sometimes affects the result set metadata.

E.g. DECIMAL(10,2) - DOUBLE returns a DOUBLE with getScale() returning 2.

See the test output for jdbcapi/metadata.java

double -- precision: 15 scale: 0 display size: 22 type name: DOUBLE
double precision - dec(10,2) -- precision: 15 scale: 0 display size: 22 type name: DOUBLE
dec(10,2) - double precision -- precision: 15 scale: 2 display size: 22 type name: DOUBLE

First line is a DOUBLE column, second is DOUBLE - DECIMAL, third is DECIMAL - DOUBLE

I assume the scale  should always be zero for a DOUBLE, as it holds no meaning, but I can't see any proof of that in JDBC spec, javadoc or tutorial book.

",,,,,,,,,,,,,03/Aug/16 03:20;bryanpendleton;BryanPossibleIdea.diff;https://issues.apache.org/jira/secure/attachment/12821760/BryanPossibleIdea.diff,31/Jul/16 07:08;Dnj;Derby-853.diff;https://issues.apache.org/jira/secure/attachment/12821203/Derby-853.diff,31/Jul/16 22:48;Dnj;Derby-853_2.diff;https://issues.apache.org/jira/secure/attachment/12821236/Derby-853_2.diff,02/Aug/16 17:35;Dnj;Derby-853_3.diff;https://issues.apache.org/jira/secure/attachment/12821656/Derby-853_3.diff,28/Dec/12 19:31;knutanders;Derby853.java;https://issues.apache.org/jira/secure/attachment/12562601/Derby853.java,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,2009-07-21 16:15:35.72,,,no_permission,,,,,,,,,,,,22174,,,Newcomer,Repro attached,Thu Aug 04 02:26:23 UTC 2016,,,,,,0|i06obz:,36876,,,,,,,,Normal,21/Jul/09 16:15;rhillegas;Triaged for 10.5.3: assigne normal urgency.,"28/Dec/12 19:31;knutanders;Attaching Derby853.java which reproduces the problem. It subtracts a DOUBLE from a DECIMAL(10,2) and prints the scale and the type name of the result. On trunk, it prints:

scale: 2
type: DOUBLE","30/Jul/16 17:15;Dnj;I think the problem happens in getScale() method in NumericTypeCompiler.java . 

In this method, getStoredFormatIdFromTypeId() is not equal to Decimal it returns
the scale value of leftType.
","31/Jul/16 07:08;Dnj;I attached a patch. Could you please give me any comments about this patch.

thanks.","31/Jul/16 16:05;bryanpendleton;It seems like a good approach -- did it fix the Derby853.java test?

I think you are right that the problem is that we are using 'leftType',
which isn't correct in this case, because the type precedence rules
mean that at line 258 we have noticed that combining a DECIMAL and a
DOUBLE means we have a result of DOUBLE.

I wonder if there is a way to fix this somewhere near line 258 instead,
because it seems like at line 258 we shouldn't even be calling the
getScale() method if higherType is not a DECIMAL type, so maybe that
call get getScale() should be inside the ""if"" test at line 260?

bryan","31/Jul/16 17:01;Dnj;{quote}did it fix the Derby853.java test?{quote}

Yes. It fixed Derby853.java test.

{quote}I wonder if there is a way to fix this somewhere near line 258 instead,
because it seems like at line 258 we shouldn't even be calling the
getScale() method if higherType is not a DECIMAL type, so maybe that
call get getScale() should be inside the ""if"" test at line 260?{quote}

Yes. We shouldn't call getScale() is it is not decimal. 
I'll check it and attach a new patch.",31/Jul/16 22:48;Dnj;I attached doing changes. This patch also passes the test in Derby853.java,01/Aug/16 05:49;Dnj;All tests are clean with this patch,"01/Aug/16 14:29;bryanpendleton;Which test suite do you think would be a good place to add the test
cases from the Derby853.java code?","01/Aug/16 14:47;Dnj;I think this suits for
java/testing/org/apache/derbyTesting/functionTests/tests/jdbcapi/ResultSetMiscTest.java

I'll add it with a new method.","02/Aug/16 17:35;Dnj;This patch adds the tests to test this bug.

This patch touches
M       java/engine/org/apache/derby/impl/sql/compile/NumericTypeCompiler.java
M       java/testing/org/apache/derbyTesting/functionTests/tests/jdbcapi/ResultSetMiscTest.java
","03/Aug/16 03:20;bryanpendleton;Hi Danoja,

I was wondering if you could look at a slight variation on your patch,
which I have attached as ""BryanPossibleIdea.diff"".

It is very similar to your patch, with two differences:

1) If the higher-precedence type isn't DECIMAL, I changed it to
simply set the scale to 0. I don't think it matters terribly much whether
the alternate type is DOUBLE, or BIGINT, or any other type; if it's
not DECIMAL, the scale should be 0.

2) I also added a test of getPrecision to the test case that you
added to confirm that, whichever order the arguments are in
the test case, the precision still comes back 15. I didn't discover
any new problem; I just added the additional assertion on getPrecision

What do you think about this idea?

For me, it passes your new test case, and it also passes all
the existing test suites.

thanks,

bryan
","03/Aug/16 04:39;Dnj;Hi Bryan,

I think since scale of all other types(that are supported for operations like ""-"") should be 0, this is a simple and good approach.

thanks.",03/Aug/16 23:55;Dnj;All test are clean in my system too.,"04/Aug/16 01:29;jira-bot;Commit 1755133 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1755133 ]

DERBY-853: ResultSetMetaData.getScale returns inconsistent values

This patch was contributed by Danoja Dias (danojadias at gmail dot com)

When a SQL statement contains arithmetic expressions, the result of the
expression may be of a different type than the operands to the expression,
due to type precedence rules which may require promoting the operand
values during evaluation of the expression.

For example, subtracting a DOUBLE from a DECIMAL results in a DOUBLE.

In some of these cases, Derby was reporting that the result column
had a non-zero scale, although the result column was not of DECIMAL type.

This change modifies the NumericTypeCompiler so that it only computes
a non-zero scale for the result column when it is of DECIMAL type.","04/Aug/16 02:20;Dnj;I think, we can resolve this issue.",04/Aug/16 02:26;bryanpendleton;We've completed all the planned work for this issue. Marking fixed.,,,,,,,,,,,,,,,,,,,,,,
isNullable on ResultSetMetaData from DatabaseMetaData.getBestRowIdentifier values are opposite when there is no rows in ResultSet vs. when there is a row.,DERBY-3181,12381968,Bug,Resolved,DERBY,Derby,software,fuzzylogic,,http://db.apache.org/derby/,Trivial,Fixed,Dnj,myrna,myrna,06/Nov/07 23:23,01/Aug/16 06:23,13/Mar/19 22:42,01/Jun/16 17:07,10.4.1.3,10.13.1.0,,JDBC,,,0,derby_triage10_5_2,"With code like the following: 

           DatabaseMetaData dmd = conn.getMetaData(); 
            ResultSet rs = dmd.getBestRowIdentifier(null,""APP"",""a"",3,true);
            ResultSetMetaData rsmd = rs.getMetaData(); 
            int actualCols = rsmd.getColumnCount(); 
            for (int i = 0; i < actualCols; i++) 
            { 
                 System.out.print(""getColumnName: "" + rsmd.getColumnName(i+1) + "", isNullable: ""); 
                 System.out.println(rsmd.isNullable(i+1)); 
            } 
The printed values for isNullable returned are opposite of what they are when the getBestRowIdentifier call looks like this:
            ResultSet rs = dmd.getBestRowIdentifier(null,""APP"",""a"",1,true);

In the latter case, the values are:
getColumnName: SCOPE, isNullable: 0
getColumnName: COLUMN_NAME, isNullable: 1
getColumnName: DATA_TYPE, isNullable: 0
getColumnName: TYPE_NAME, isNullable: 1
getColumnName: COLUMN_SIZE, isNullable: 0
getColumnName: BUFFER_LENGTH, isNullable: 0
getColumnName: DECIMAL_DIGITS, isNullable: 0
getColumnName: PSEUDO_COLUMN, isNullable: 0

In the first case, the values are:
getColumnName: SCOPE, isNullable: 1
getColumnName: COLUMN_NAME, isNullable: 0
getColumnName: DATA_TYPE, isNullable: 1
getColumnName: TYPE_NAME, isNullable: 1
getColumnName: COLUMN_SIZE, isNullable: 1
getColumnName: BUFFER_LENGTH, isNullable: 1
getColumnName: DECIMAL_DIGITS, isNullable: 1
getColumnName: PSEUDO_COLUMN, isNullable: 1

The isNullable value should be stable. 
It's probably worthwhile verifying what the value *should* be in the first place.
",,,,,,,,,,,,,06/May/16 04:44;Dnj;Derby-3181.diff;https://issues.apache.org/jira/secure/attachment/12802603/Derby-3181.diff,24/May/16 09:16;Dnj;Derby3181.diff;https://issues.apache.org/jira/secure/attachment/12805840/Derby3181.diff,26/May/16 06:39;Dnj;deleteQuery.diff;https://issues.apache.org/jira/secure/attachment/12806304/deleteQuery.diff,06/Nov/07 23:24;myrna;repro.java;https://issues.apache.org/jira/secure/attachment/12369062/repro.java,06/May/16 14:35;bryanpendleton;testChange.diff;https://issues.apache.org/jira/secure/attachment/12802686/testChange.diff,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,2009-07-06 15:31:12.171,,,no_permission,,,,,,,,,,,,23483,,,Newcomer,Repro attached,Wed Jun 01 17:07:35 UTC 2016,,,,,,0|i06o73:,36854,,,,,,,,Normal,06/Jul/09 15:31;kmarsden;Triaged for 10.5.2. Set urgency to normal,"03/May/16 05:31;Dnj;the argument scope of getBestRowIdentifier(String catalog,String schema,String table,int scope,boolean nullable) method must be 0 or 1 or 2. If not it is not a valid argument and It returns an empty resultset. 
In the latter part of repro.java, scope is 3 and it execute the following prepared query in metadata.properties file.
 getBestRowIdentifierEmpty=\
	SELECT SCOPE, COLUMN_NAME, DATA_TYPE, TYPE_NAME, COLUMN_SIZE, \
		BUFFER_LENGTH, DECIMAL_DIGITS, PSEUDO_COLUMN \
	FROM (VALUES \
		(CAST (2  AS SMALLINT), \
		 CAST ('' AS VARCHAR(128)), \
		 0, \
		 CAST ('INT' AS VARCHAR(128)), \
		 0, \
		 0,	\
		 CAST (0 AS SMALLINT), \
		 CAST (0 AS SMALLINT)) \
		) AS BESTROWIDENTIFIER( \
		SCOPE, COLUMN_NAME, DATA_TYPE, TYPE_NAME, COLUMN_SIZE, BUFFER_LENGTH, \
		DECIMAL_DIGITS, PSEUDO_COLUMN) \
	WHERE (1=0)

I think the bug occurs according to the resultset thst returns here.","04/May/16 07:20;Dnj;In the first case of repro.java 
it runs following query for getBestRowIdentifier method,
getBestRowIdentifierAllColumns=\
	SELECT \
		CAST (java.sql.DatabaseMetaData::bestRowSession AS SMALLINT) AS SCOPE, \
		COLS.COLUMNNAME AS COLUMN_NAME, \
		COLS.COLUMNDATATYPE.getJDBCTypeId() AS DATA_TYPE, \
		CAST (COLS.COLUMNDATATYPE.getTypeName() AS VARCHAR(128)) AS TYPE_NAME, \
		COLS.COLUMNDATATYPE.getMaximumWidth() AS COLUMN_SIZE, \
		CAST (NULL AS INT) AS BUFFER_LENGTH, \
		CAST ((CASE WHEN (COLS.COLUMNDATATYPE.getJDBCTypeId() IN ( \
			java.sql.Types::DECIMAL, java.sql.Types::NUMERIC, \
			java.sql.Types::INTEGER, java.sql.Types::SMALLINT, \
			java.sql.Types::TINYINT, java.sql.Types::BIGINT, \
			java.sql.Types::DATE, java.sql.Types::TIME, \
			java.sql.Types::TIMESTAMP)) \
				THEN COLS.COLUMNDATATYPE.getPrecision() \
				ELSE CAST (NULL AS SMALLINT) END) AS SMALLINT) \
			AS DECIMAL_DIGITS, \
		CAST (java.sql.DatabaseMetaData::bestRowNotPseudo AS SMALLINT) AS PSEUDO_COLUMN \
	FROM SYS.SYSSCHEMAS SCHEMAS, SYS.SYSTABLES TABS,  \
		SYS.SYSCOLUMNS COLS  \
	WHERE COLS.REFERENCEID = TABS.TABLEID  \
	  AND TABS.SCHEMAID = SCHEMAS.SCHEMAID  \
	  AND ((1=1) OR % IS NOT NULL)  \
	  AND (SCHEMAS.SCHEMANAME LIKE APP)  \
	  AND (TABS.TABLENAME=a) \
	  AND 1 BETWEEN 0 AND 2  \
	  AND (1<>0 OR NOT COLS.COLUMNDATATYPE.isNullable())","05/May/16 00:25;bryanpendleton;I had completely misunderstood that this job involved the behavior when
the ""scope"" argument was invalid -- thanks for pointing that out!

Frankly, I don't understand why we are returning a ResultSet at all
when the scope is invalid. Why not throw a SQLException instead?

I looked through the Subversion history, and that behavior of
getBestRowIdentifier is present in the original version of EmbedDatabaseMetaData.java
in the original contribution from IBM.

So this is very old behavior.

It would be interesting, though, to try this experiment:
1) Try changing EmbedDatabaseMetaData.getBestRowIdentifier to
    throw a SQLException if the scope is out of range, rather than
   executing getBestRowIdentifierEmpty
2) Run all the regression tests (ant junit-core) and see if any
   of the regression tests fails due to this change

That might give us some clues about which sorts of applications, if any,
depend on being able to call getBestRowIdentifier with an invalid scope value.
","06/May/16 04:42;Dnj;Hi Bryan,

I attached the diff file.
I changed to throw an exception when scope argument is invalid. I think this is enough to check whether there is any depend on being able to call getBestRowIdentifier with an invalid scope value. 

Should I improve the way it throws the exception. ","06/May/16 05:08;bryanpendleton;This looks great. I will run some tests on my system, 
and let you know what I see, so you can compare to your results.","06/May/16 11:15;Dnj;Hi Bryan,

It failed here. Still running. What happened in yours?","06/May/16 14:35;bryanpendleton;With your change applied, I ran 'ant junit-core' and saw some test diffs:

junit-core:
    [junit] Running org.apache.derbyTesting.junit.EnvTest
    [junit] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.296 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.derbynet._Suite
    [junit] Tests run: 330, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 102.169 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.tools._Suite
    [junit] Tests run: 127, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 92.221 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.demo._Suite
    [junit] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.37 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.lang._Suite
    [junit] Tests run: 3225, Failures: 0, Errors: 14, Skipped: 0, Time elapsed: 2,976.494 sec
    [junit] Test org.apache.derbyTesting.functionTests.tests.lang._Suite FAILED
    [junit] Running org.apache.derbyTesting.functionTests.tests.jdbcapi._Suite
    [junit] Tests run: 7881, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 1,526.023 sec
    [junit] Test org.apache.derbyTesting.functionTests.tests.jdbcapi._Suite FAILED
    [junit] Running org.apache.derbyTesting.functionTests.tests.store._Suite
    [junit] Tests run: 346, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,026.574 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.engine._Suite
    [junit] Tests run: 39, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 96.724 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationSuite
    [junit] Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 635.49 sec
    [junit] Running org.apache.derbyTesting.unitTests.junit._Suite
    [junit] Tests run: 166, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.923 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.upgradeTests._Suite
    [junit] Tests run: 6439, Failures: 14, Errors: 138, Skipped: 0, Time elapsed: 2,170.677 sec
    [junit] Test org.apache.derbyTesting.functionTests.tests.upgradeTests._Suite FAILED
    [junit] Running org.apache.derbyTesting.functionTests.suites.EncryptionSuite
    [junit] Tests run: 203, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 65.194 sec

But when I looked at the actual test failures, it turned out that it was
just the same test, jdbcapi/DatabaseMetaDataTest.java, which was
failing -- the test suites run that test multiple times, in multiple different
configurations.

DatabaseMetaDataTest calls getBestRowIdentifier with an invalid
scope, intentionally, in order to test it, but was not expecting an exception.

I changed the test to catch the expected exception for the invalid scope
(see the testChange.diff patch), and now the tests are running clean.

Can you try applying the testChange.diff patch to your system and
see if the test results are improved for you?","07/May/16 13:34;Dnj;Hi Bryan,

Are they running clean?
But I got something like this after adding the patch.

junit-core:
    [junit] Running org.apache.derbyTesting.junit.EnvTest
    [junit] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.987 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.derbynet._Suite
    [junit] Tests run: 339, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 863.906 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.tools._Suite
    [junit] Tests run: 127, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 200.987 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.demo._Suite
    [junit] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.368 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.lang._Suite
    [junit] Tests run: 3226, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11,455.151 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.jdbcapi._Suite
    [junit] Tests run: 7881, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5,330.829 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.store._Suite
    [junit] Tests run: 346, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3,127.492 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.engine._Suite
    [junit] Tests run: 39, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 197.515 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationSuite
    [junit] Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,306.592 sec
    [junit] Running org.apache.derbyTesting.unitTests.junit._Suite
    [junit] Tests run: 170, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 47.173 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.upgradeTests._Suite
    [junit] Tests run: 6439, Failures: 152, Errors: 35, Skipped: 0, Time elapsed: 8,246.18 sec
    [junit] Test org.apache.derbyTesting.functionTests.tests.upgradeTests._Suite FAILED
    [junit] Running org.apache.derbyTesting.functionTests.suites.EncryptionSuite
    [junit] Tests run: 203, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 199.743 sec
","07/May/16 14:48;bryanpendleton;Indeed, it looks like I am still seeing some problems in the upgradeTests, too.
But my results are not the same as yours. Can you look in your results?
You should have a directory named ""junit_YYYYMMDD_HHMM"" in your trunk
directory, and in there should be all the results of your testing. There
should be a file named TEST-org.apache.derbyTesting.functionTests.tests.upgradeTests._Suite.xml
and that file should have information about each test failure. In that
file, use your text editor to search for the strings 'error message=' and
'failure message=' to see information about each error and each test failure.

In my case, my results were:
    [junit] Tests run: 6439, Failures: 14, Errors: 0, Skipped: 0, Time elapsed: 2,491.535 sec
    [junit] Test org.apache.derbyTesting.functionTests.tests.upgradeTests._Suite FAILED

and in my XML file I see only one failure, but the same failure occurred
all 14 times:
  <testcase classname=""org.apache.derbyTesting.functionTests.tests.jdbcapi.DatabaseMetaDataTest"" name=""testGetTypeInfo"" time=""0.147"">
    <failure message=""Unexpected type DATE"" type=""junit.framework.AssertionFailedError"">junit.framework.AssertionFailedError: Unexpected type DATE
        at org.apache.derbyTesting.functionTests.tests.jdbcapi.DatabaseMetaDataTest.testGetTypeInfo(DatabaseMetaDataTest.java:2346)
        at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:120)
...

I will have to study this some more; I am not sure what is causing this
particular test to fail, and why it occurs only during the upgrade testing.

What do your errors and failures look like?+","07/May/16 18:30;bryanpendleton;Hi Danoja, it appears that my test failures are actually caused by DERBY-6856, and are not related to our work on this issue. If you are
getting ""Unexpected type DATE"" in your upgrade tests, it is because
of DERBY-6856. I will work on that problem separately.","09/May/16 01:29;Dnj;Hi Bryan,
 
I am getting many failures like following. I have not added all of them here.  Why do they differ from you?

<testcase classname=""org.apache.derbyTesting.functionTests.tests.upgradeTests.BasicSetup"" name=""testOldVersion"" time=""0.035"">
    <failure message=""Old minor (driver):  expected:&lt;1&gt; but was:&lt;13&gt;"" type=""junit.framework.AssertionFailedError"">junit.framework.AssertionFailedError: Old minor (driver):  expected:&lt;1&gt; but was:&lt;13&gt;


<testcase classname=""org.apache.derbyTesting.functionTests.tests.upgradeTests.Changes10_2"" name=""testDatabaseOwnerChange"" time=""0.042"">
    <failure message=""AUTHORIZATIONID not valid for SYSIBM expected:&lt;[DBA]&gt; but was:&lt;[APP]&gt;"" type=""junit.framework.ComparisonFailure"">junit.framework.ComparisonFailure: AUTHORIZATIONID not valid for SYSIBM expected:&lt;[DBA]&gt; but was:&lt;[APP]&gt;



  <testcase classname=""org.apache.derbyTesting.functionTests.tests.upgradeTests.Changes10_4"" name=""testErrorMessage"" time=""0.327"">
    <failure message=""expected error while creating unique constraint over nullable column"" type=""junit.framework.AssertionFailedError"">junit.framework.AssertionFailedError: expected error while creating unique constraint over nullable column","21/May/16 08:23;Dnj;Hi Bryan,
I ran the test again after updating the repositary. Then no failures found. 
Test result is like this.
junit-core:
    [junit] Running org.apache.derbyTesting.junit.EnvTest
    [junit] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.314 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.derbynet._Suite
    [junit] Tests run: 339, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 565.378 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.tools._Suite
    [junit] Tests run: 127, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 202.549 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.demo._Suite
    [junit] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.831 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.lang._Suite
    [junit] Tests run: 3226, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9,402.799 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.jdbcapi._Suite
    [junit] Tests run: 7881, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3,496.768 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.store._Suite
    [junit] Tests run: 346, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,802.678 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.engine._Suite
    [junit] Tests run: 39, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 202.396 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationSuite
    [junit] Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,391.063 sec
    [junit] Running org.apache.derbyTesting.unitTests.junit._Suite
    [junit] Tests run: 170, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 48.206 sec
    [junit] Running org.apache.derbyTesting.functionTests.tests.upgradeTests._Suite
    [junit] Tests run: 6439, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8,327.352 sec
    [junit] Running org.apache.derbyTesting.functionTests.suites.EncryptionSuite
    [junit] Tests run: 203, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 203.698 sec

Can we do the commits and change the status to resolved?","21/May/16 15:55;bryanpendleton;Hi Danoja, it is great to hear that the tests are working better now in your environment.

I think that, to move ahead with this change, we need to improve our patch slightly
so that we use a real Derby message for this error, rather than the hard-coded
""Invalid scope"" string in the experimental patch we've been testing.

This is a good chance to learn how we add a new error message to Derby.

So this means:

1) Modify java/shared/org/apache/derby/shared/common/reference/SQLState.java
    to define a new SQL state value for this error condition. I think you can just
    follow the pattern that is already in that file; try going to around line 950 in
   that file, pick a new unused SQLState code (e.g., 42XAT, perhaps?), and give
   it a friendly name, like LANG_INVALID_ROWID_SCOPE

 2) Modify java/engine/org/apache/derby/loc/messages.xml to add your message text.
    Just go to the corresponding location in that file (that is, where your SQLState
    lines up with the other SQLStates in its range), and add a XML text block that
    provides a message about the invalid scope value.

   I think that our error message should take the scope value as an argument, so
   something like: ""Invalid row identifier scope: {0}""

 3) Then, we want our test code in DatabaseMetaDataTest.java to do more than
   just catch-and-ignore the SQLException for invalid scope; we want it to check
   that it got the right message in this case, so we need the catch block to say
   assertSQLState( ...), like we do elsewhere in those tests (see around lines 3585, say)

After I add a new message, I always do a complete rebuild by doing

    ant clobber cleanjars
   ant all 

Once you think you've got your proposed patch all tidied up, and you've checked
that the tests are still passing, please post your proposed final patch for review.

Thanks!
","24/May/16 09:15;Dnj;Hi Bryan,
I did changes. all tests passed. diff file is attached.

Thanks","24/May/16 13:19;bryanpendleton;The patch looks great, thanks for adding the new message! I ran the new
code with a 'sqle.printStackTrace' in the exception block and verified that
the new message looks fine and has the expected argument value when
it is thrown. I think we're good to go with this change; I'll do a bit more
testing and commit the change later today.","24/May/16 13:55;bryanpendleton;It seems like we maybe don't need the ""getBestRowIdentifierEmpty"" query
in org.apache.derby.impl.jdbc.metadata.properties anymore? I think it was
only referenced by the call to getPreparedQuery() at line 2160 of
EmbedDatabaseMetaData.java, which now throws the exception instead.

I think that we could file a separate, new JIRA, which is linked to this bug,
and which states that we could remove that system query from metadata.properties.

Or, we could do that as part of this issue.

Which way would be better?","24/May/16 14:48;Dnj;
I think we should remove from line 887 to line 908 from org.apache.derby.impl.jdbc.metadata.properties. Is there any more to remove that query? If not I think we can do it as a part of this issue. 
","25/May/16 00:36;jira-bot;Commit 1745414 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1745414 ]

DERBY-3181: DatabaseMetaData.getBestRowIdentifier behavior with invalid scope

This patch was contributed by Danoja Dias (danojadias at gmail dot com)

The DatabaseMetaData.getBestRowIdentifier method takes a number of arguments,
including ""scope"", which is supposed to be one of the enumerated values:
bestRowTemporary, bestRowTransaction, and bestRowSession.

If an invalid scope argument was passed to this method, Derby was returning
a hard-coded ""empty"" row identifier, which was slightly different, in detail,
to the row identifier that is returned for a valid scope argument.

Since JDBC does not require that we return such a row identifier for an
invalid scope argument, it seems cleaner and more useful to throw an
exception with a message indicating that an invalid scope argument was
passed. The caller can then correct their application to make the call with
a valid scope argument, and will then receive a valid row identifier.","25/May/16 00:38;bryanpendleton;I think that *should* be all we need to do, but I am not certain.

Can you please try removing that section as an experiment, and maybe
do a clean build and some tests, to see if you encounter any problems?

In the meantime, I think that your existing patch is a clear
improvement over the current behavior, so I have committed it.

I suggest that we leave this issue open while you investigate
whether the query definition can be removed from the properties
file, or whether it is not as simple as that.","26/May/16 06:38;Dnj;metadata.properties file is changed and all tests passed.
I attached the diff file. ","01/Jun/16 16:59;jira-bot;Commit 1746487 from [~bryanpendleton] in branch 'code/trunk'
[ https://svn.apache.org/r1746487 ]

DERBY-3181: DatabaseMetaData.getBestRowIdentifier behavior with invalid scope

This patch was contributed by Danoja Dias (danojadias at gmail dot com)

This change is a follow-on to revision 1745414, and removes the
getBestRowIdentifierEmpty query definition from the queries in
metadata.properties.",01/Jun/16 17:07;bryanpendleton;We've completed the intended work for this issue; resolving.,,,,,,,,,,,,,,,,
