Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Outward issue link (Blocker),Outward issue link (Child-Issue),Outward issue link (Container),Outward issue link (Dependent),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Problem/Incident),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Regression),Outward issue link (Required),Outward issue link (Supercedes),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reviewer),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Since Version),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Schema version id mismatch while upgrading to 3.0.13,CASSANDRA-13559,13075498,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,29/May/17 00:59,12/Mar/19 14:20,13/Mar/19 22:34,31/May/17 16:19,3.0.14,3.11.0,,,,Legacy/Core,,,,,0,,,,"As the order of SchemaKeyspace is changed ([6991556 | https://github.com/apache/cassandra/commit/6991556e431a51575744248a4c484270c4f918c9], CASSANDRA-12213), the result of function [{{calculateSchemaDigest}}|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/schema/SchemaKeyspace.java#L311] is also changed for the same schema. Which causes schema mismatch while upgrading 3.0.x -> 3.0.13.
It could cause cassandra fail to start because Unknown CF exception. And streaming will fail:
{noformat}
ERROR [main] 2017-05-26 18:58:57,572 CassandraDaemon.java:709 - Exception encountered during startup
java.lang.IllegalArgumentException: Unknown CF 83c8eae0-3a65-11e7-9a27-e17fd11571e3
{noformat}
{noformat}
WARN  [MessagingService-Incoming-/IP] 2017-05-26 19:27:11,523 IncomingTcpConnection.java:101 - UnknownColumnFamilyException reading from socket; closing
org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find table for cfId 922b7940-3a65-11e7-adf3-a3ff55d9bcf1. If a table was just created, this is likely due to the schema not being fully propagated.  Please wait for schema agreement on table creation.
{noformat}

Restart the new node will cause:
{noformat}
Exception (java.lang.NoSuchFieldError) encountered during startup: ALL
java.lang.NoSuchFieldError: ALL
        at org.apache.cassandra.service.ClientState.<clinit>(ClientState.java:67)
        at org.apache.cassandra.cql3.QueryProcessor$InternalStateInstance.<init>(QueryProcessor.java:155)
        at org.apache.cassandra.cql3.QueryProcessor$InternalStateInstance.<clinit>(QueryProcessor.java:149)
        at org.apache.cassandra.cql3.QueryProcessor.internalQueryState(QueryProcessor.java:163)
        at org.apache.cassandra.cql3.QueryProcessor.prepareInternal(QueryProcessor.java:286)
        at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:294)
        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:900)
        at org.apache.cassandra.service.StartupChecks$9.execute(StartupChecks.java:354)
        at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:110)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:179)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697)
{noformat}

I would suggest to have the older list back for digest calculation and release 3.0.14.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-05-31 03:41:07.725,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue May 22 18:22:44 UTC 2018,,,,,,0|i3fks7:,9223372036854775807,3.0.13,,,,,,,Stefania,Stefania,,,,,,,,,,"29/May/17 01:35;jay.zhuang;Here is the patch: https://github.com/apache/cassandra/compare/cassandra-3.0...cooldoger:13559-3.0?expand=1

To remove the older list, maybe we should bump the message version.
","31/May/17 03:41;Stefania;I agree this is a problem, thanks for reporting it. Since we need both the new and old orders, I prefer to only change the order for flushing and restore the old order for everything else. I am going to prepare a patch shortly.","31/May/17 04:07;jjirsa;Any thoughts on an upgrade path from 3.0.13 - 3.0.14 Stefania?
","31/May/17 07:22;Stefania;Here are the patches for 3.0 and 3.11:

[patch for 3.0|https://github.com/stef1927/cassandra/tree/13559-3.0]
[patch for 3.11|https://github.com/stef1927/cassandra/tree/13559-3.11]

Since we don't need it for 4.0, I decided to keep the initial patch suggested by [~jay.zhuang], I merely added some comments and the entry into CHANGES.txt. I'm running the tests for 3.0 on our internal CI hosts. I will post a comment when the results are available.

Regarding the upgrade from 3.0.13 to 3.0.14, I don't think there is much we can do without a bump in the messaging version. I would tend to think we should release 3.0.14 with the patch asap. I also wonder why we did not notice this at all in our [upgrade tests|http://cassci.datastax.com/job/cassandra-3.0_dtest_upgrade/lastCompletedBuild/testReport/].

For 3.11 we definitely need the patch since we haven't released it yet; whilst for 4.0 we don't need it because the messaging version has been bumped and hence the schema version will not be compared.

Regarding the {{NoSuchFieldError}} in the ticket description, it doesn't make much sense to me and I cannot reproduce it. The node starts without problems, perhaps a build issue? ","31/May/17 09:41;Stefania;The test results look good, no failures for the unit tests and 5 known failures for the dtests.","31/May/17 10:08;snazy;+1, assuming tests look good.","31/May/17 10:41;ostefano;[~Stefania], could you please elaborate a bit further on the .13-.14 upgrade path?
I am running .13 in production and I am quite concerned what to expect during the next round of upgrades.","31/May/17 13:06;iamaleksey;+1 from me as well. Sorry for not spotting the issue in the original ticket, or even steering the implementation towards this issue.",31/May/17 13:27;jjirsa;Can someone explain the upgrade path from 3.0.13 to 3.0.14 please?,"31/May/17 13:35;iamaleksey;You face the storm; no way around it it seems.

Pragmatically however there will be a lot more people upgrading from versions that aren't 3.0.13 then from the single recently released 3.0.13, so it makes sense to optimise for the majority of the users.","31/May/17 13:40;jjirsa;There are three stack traces in the ticket that aren't just migration storm, and 2 of them are fail-to-startup errors. How does changing the schema version cause any of those?

I realize I can go run through minor version uploads and try to hit this myself, but I don't intuitively ""get"" why those stacks happen with ""just"" changing the schema version, but it seems likely they'll happen for anyone going 3.0.13 -> 3.0.14.
 

","31/May/17 13:44;iamaleksey;bq. How does changing the schema version cause any of those?

It can't and it doesn't. Those should be separate JIRA tickets, if valid at all.

This patch is only addressing the difference in digest calculation, which is objectively an issue and has been reproduced.","31/May/17 14:33;Stefania;bq. could you please elaborate a bit further on the .13-.14 upgrade path?

When an upgraded node joins the ring, all non-upgraded nodes will pull the schema from it, after 1 minute. They shouldn't pull any longer from this node unless a real schema change happens. Since the schema is identical, a schema pull results in all schema mutations being applied and the schema tables being flushed, nothing else since the delta would be nil. When another node is upgraded and joins, the process repeats itself. 

Furthermore, the upgraded nodes will see the schema of the non-upgraded nodes as different and pull from them. Eventually the process converges, when all nodes are upgraded the schema versions will be the same. 

Things visible to the operator should be the schema migrations in the logs. Other noticeable things will be the schema version logged, which will be different after the upgrade, and different schema versions appearing in {{nodetool describecluster|describering}}. Also, in the system local table. This means that if any problems are likely to occur, they will mostly likely be client side, i.e. clients will not see a schema agreement until the upgrade is completed. This might cause issues in some applications. 

To limit the flow of schema pulls. the schema should not be changed during the upgrade, if at all possible. I'll add a section to NEWS.txt. 

That's all I could find from code inspection. [~spodxx@gmail.com] rightly pointed out in the dev mailing list that the only urgent thing is to pull 3.0.13. We can take our time before releasing 3.0.14, at a minimum we should write an upgrade test.

The startup problem is not related to the digest mismatch: the {{ALL}} field exists, so I don't understand it at all. I also struggle to explain the unknown cf exceptions, they are probably unrelated but the upgrade test should shed some light.","31/May/17 14:43;jjirsa;If this is really just a single version change, this is a minor issue that doesn't justify pulling the package - CASSANDRA-13441 is present in .13 and earlier 3.0 versions will cause far more schema changes than this patch will for anyone upgrading from 2.1/2.2. This is worth fixing, but I don't see why it's worth pulling binaries.","31/May/17 14:49;iamaleksey;bq. CASSANDRA-13441 is present in .13 and earlier 3.0 versions will cause far more schema changes than this patch will for anyone upgrading from 2.1/2.2

Yeah, but for people upgrading from 3.0.x that aren't 3.0.13 the fix would still be nice, and there are more of those than people on 3.0.13 - I would assume.

Not advocating for pulling binaries. Neither is a an urgent 3.0.14 release necessary imo. But the fix should go in. ","31/May/17 15:22;spodxx@gmail.com;There are two different upgrade issues here. Let's not mix those up.


*3.0.x -> 3.0.13*

Based on the description by [~jay.zhuang] for this ticket, any node on  3.0.x that will be upgraded to the latest 3.0.13 bug fix release, will fail to start. This is the actual point why pulling the .13 release from the download page is probably a good idea, so people won't upgrade their cluster just to find themselves in a situation unable to bring any node up again.

*3.0.13 -> 3.0.14 (unrelease)*

Some users may already run 3.0.13 (e.g. by upgrading from 2.x or fresh installs). Maybe even on large production clusters. If we change the schema digest calculation in .14 again, we need to make sure that the users on .13 will be able to upgrade to .14 as well without seeing major issues, e.g. with schema migration storms. ","31/May/17 15:43;ostefano;bq. Some users may already run 3.0.13 (e.g. by upgrading from 2.x or fresh installs).

I am not quite sure the issue as described by Jay Zhuang was meant to be deterministic.
I have upgraded from 3.0.12 to 3.0.13 without issues but the schema change storm.","31/May/17 15:48;spodxx@gmail.com;Regarding the {{NoSuchFieldError}} startup error seems to be caused by some dirty class files. I can always reproduce this locally by doing something like this:

{noformat}
git checkout cassandra-3.0.12
ant clean jar
ccm create 3.0-3n --install-dir=/home/spod/git/cassandra-3.0 -n 3
ccm start
ccm node3 stop
git checkout cassandra-3.0.13
ant jar # NO CLEAN
ccm node3 start
{noformat}

If I do ""ant clean jar"" instead, the node is starting up fine. 

[~jay.zhuang], are you sure you've installed the vanilla Apacha tarbar cleanly on your side?","31/May/17 15:57;iamaleksey;bq. There are two different upgrade issues here. Let's not mix those up.

There is only one confirmed issue here. It's a shame the JIRA is mixed up. We accidentally changed the way the digest is calculated, slightly, in 3.0.13. The issue is increased migration traffic. Not a big deal, but we should roll it back, so that the majority (lagging behind latest, always) is not affected.

bq.  If we change the schema digest calculation in .14 again, we need to make sure that the users on .13 will be able to upgrade to .14 as well without seeing major issues, e.g. with schema migration storms.

Well, we can't, so you have to choose one:
1. prevent extra migration traffic for those migrating from [3.0.0, 3.0.12] to 3.0.14+
2. prevent extra migration traffic for those migrating from 3.0.13 to 3.0.14+

With 3.0.13 being super fresh, with relatively few people on it (who already went through some extra migrations and survived anyway), I would say (1) is more important than (2).","31/May/17 16:03;spodxx@gmail.com;That makes it much clearer, thanks for the wrap up! ","31/May/17 16:17;jay.zhuang;Thanks [~Stefania] for the quick fix.

For a normal upgrade, I don't see the ""fail to start"" issue. 

For call stack:
{noformat}
ERROR [main] 2017-05-26 18:58:57,572 CassandraDaemon.java:709 - Exception encountered during startup
java.lang.IllegalArgumentException: Unknown CF 83c8eae0-3a65-11e7-9a27-e17fd11571e3
{noformat}
It happens when the schema is changed while upgrading. For our case, a few nodes are not upgraded, so we have a cluster with mixed versions (3.0.x and 3.0.13) plus we add a table during that time which causes this issue.

For
{noformat}
Exception (java.lang.NoSuchFieldError) encountered during startup: ALL
java.lang.NoSuchFieldError: ALL
{noformat}
I don't think it's caused by this issue.

Schema version id mismatch also happens when upgrading from 2.2.x -> 3.0.x and maybe other major/minor version upgrade. I think it not worth pulling 3.0.13 binaries.","31/May/17 16:18;iamaleksey;Committed to 3.0 as [f96a5dc5840c7d1fea99aa450543af8e889c161e|https://github.com/apache/cassandra/commit/f96a5dc5840c7d1fea99aa450543af8e889c161e] and merged into 3.11, trunk omitted. Thanks.",31/May/17 16:20;iamaleksey;[~jay.zhuang] Feel free to continue in a new JIRA ticket to handle your remaining issues.,"01/Jun/17 03:22;Stefania;Thank you for committing the ticket [~iamaleksey] and to everybody for the clarifications.

I've created an upgrade test [here|https://github.com/riptano/cassandra-dtest/compare/master...stef1927:13559]. To run it export {{JAVA8_HOME}} and then choose one of the following test combinations:

{code}
upgrade_tests/regression_test.py:TestForRegressionsUpgrade_released_3_0_12_To_released_3_0_13.test_schema_agreement
upgrade_tests/regression_test.py:TestForRegressionsUpgrade_released_3_0_13_To_indev_3_0_x.test_schema_agreement
upgrade_tests/regression_test.py:TestForRegressionsUpgrade_released_3_0_12_To_indev_3_0_x.test_schema_agreement
{code}

The test verifies that the schemas match before and during upgrade, when creating a table before and during upgrade. 3.0.12 -> current will succeed, whilst 3.0.12 -> 3.0.13 and 3.0.13 -> current will fail. In the logs we can see the schema pulls but no other issues (the test checks for a schema match before creating the table during the upgrade). 

I'll leave the test with the custom versions available for a few days, if people won't to play with it, e.g. by testing with a larger cluster or by adding more functionality during the upgrade. Before creating a pull request, we need to uncomment  {{@since('3.0.14', max_version='3.99')}} and remove the lines in [{{OVERRIDE_MANIFEST}}|https://github.com/riptano/cassandra-dtest/compare/master...stef1927:13559#diff-7de72be82b9e05ef3451358461197944R121].
","02/Jun/17 11:03;spodxx@gmail.com;I've now created CASSANDRA-13569, which should help making any schema pulls created by mismatching schemas less stormy. Should be a good idea to get this in along with this patch in 3.0.14.",02/Jun/17 11:40;spodxx@gmail.com;Looks like hints will also not get dispatched in cases of mismatching schema IDs (see HintsDispatchTrigger). This should also be put in the yet to be updated NEWS.txt.,"02/Jun/17 11:44;iamaleksey;bq. Looks like hints will also not get dispatched in cases of mismatching schema IDs (see HintsDispatchTrigger). This should also be put in the yet to be updated NEWS.txt.

Only until the pair of nodes converges on the same schema - eventually. Just like with regular convergence. Not sure it's NEWSworthy.","05/Jun/17 01:31;Stefania;{{NEWS.txt}} updated (only for schema migrations and only in 3.0.x), see commit [6b36d9|https://github.com/apache/cassandra/commit/6b36d9f0506351f03555efaa3a0784d097913adf].

Pull request for upgrade test created [here|https://github.com/riptano/cassandra-dtest/pull/1477].","22/May/18 18:22;scubadrew;Hello everyone. I am seeing this error when restarting a 3.0.12 node.

ERROR [main] 2018-05-22 17:53:09,355 CassandraDaemon.java:710 - Exception encountered during startup
java.lang.IllegalArgumentException: Unknown CF 10c30740-1284-11e8-862f-5f9049872b18
 at org.apache.cassandra.db.Keyspace.getColumnFamilyStore(Keyspace.java:206) ~[apache-cassandra-3.0.12.jar:3.0.12]

I realize this thread is mostly concerned with upgrades. We are not trying to upgrade. Simply trying to stop and then restart the service on a single node in a long running cluster.

Any advice how we can recover this node?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent continuous schema exchange between 3.0 and 3.11 nodes,CASSANDRA-14109,13124420,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,snazy,snazy,snazy,12/Dec/17 17:28,12/Mar/19 14:20,13/Mar/19 22:34,13/Dec/17 13:20,3.11.2,,,,,Legacy/Coordination,Legacy/Distributed Metadata,,,,0,,,,"Continuous schema migrations can happen during an upgrade from 3.0.x to 3.x even with versions having the patches for CASSANDRA-13441 and CASSANDRA-13559.

The root cause is the {{cdc}} column, which is included in schema version calculation in {{RowIterators.digest()}} via {{SchemaKeyspace.calculateSchemaDigest()}}.

It is possible to make the schema-version calculation between 3.0 and 3.11 compatible. The idea here is: 3.11 accepts both 3.0 compatible and 3.11 ""native"" schema versions. As long as there is one 3.0 node in the cluster, 3.11 announces a 3.0 compatible schema version (without the {{cdc}} column). When there are no (more) 3.0 nodes in the cluster, announce the ""real"" 3.11 schema version (including the {{cdc}} column). ""Announce"" means announcing via Gossip and storing in {{system.local}}.

The change itself is against 3.11 only. A couple of log messages have been improved and some code regarding schema version checks has been moved into the {{Schema}} class. Those ""side changes"" are carried to trunk. Because of that, the 3.11 and trunk branches are different. The ""real"" change is in the 3.11 branch.

{{NEWS.txt}} for 3.11(only) contains upgrade notes.

||OSS 3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:schema-migration-upgrade-bug-3.11?expand=1]
||OSS trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:schema-migration-upgrade-bug-trunk?expand=1]
||OSS dtest|[branch|https://github.com/riptano/cassandra-dtest/compare/master...snazy:schema-migration-upgrade-bug?expand=1]

We've verified the functionality of the patch by usual CI tests and extensive tests using the new upgrade dtest.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-13 10:46:35.839,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 13 13:20:46 UTC 2017,,,,,,0|i3ntb3:,9223372036854775807,,,,,,,,adelapena,adelapena,,,,,,,,,,"13/Dec/17 10:46;adelapena;The patch looks good to me, +1. 

As a suggestion that can be addressed during commit, the new methods [{{EndpointState.getSchemaVersion()}}|https://github.com/snazy/cassandra/blob/a73fbda73f9074f189dd580512e09a4a78be9bb9/src/java/org/apache/cassandra/gms/EndpointState.java#L160] and [{{EndpointState.getReleaseVersion()}}|https://github.com/snazy/cassandra/blob/a73fbda73f9074f189dd580512e09a4a78be9bb9/src/java/org/apache/cassandra/gms/EndpointState.java#L168] could be annotated with {{@Nullable}}. ","13/Dec/17 13:20;snazy;Thanks a lot for the review!

Committed the ""upgrade fix"" as [e646e5032b68622f7ec1dd0c53137be08baabed9|https://github.com/apache/cassandra/commit/e646e5032b68622f7ec1dd0c53137be08baabed9] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11] and the code improvements from that as [7a40abb6a5108688fb1b10c375bb751cbb782ea4|https://github.com/apache/cassandra/commit/7a40abb6a5108688fb1b10c375bb751cbb782ea4] to [trunk|https://github.com/apache/cassandra/tree/trunk].
Dtest committed as e67ef2b80a45ae02cc15f4e1a6c57cc68c09b0f8.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Digest mismatch Exception if hints file has UnknownColumnFamily,CASSANDRA-13696,13087482,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,17/Jul/17 06:18,12/Mar/19 14:20,13/Mar/19 22:34,27/Jul/17 00:00,3.0.15,3.11.1,4.0,,,Legacy/Core,,,,,0,,,,"{noformat}
WARN  [HintsDispatcher:2] 2017-07-16 22:00:32,579 HintsReader.java:235 - Failed to read a hint for /127.0.0.2: a2b7daf1-a6a4-4dfc-89de-32d12d2d48b0 - table with id 3882bbb0-6a71-11e7-9bca-2759083e3964 is unknown in file a2b7daf1-a6a4-4dfc-89de-32d12d2d48b0-1500242103097-1.hints
ERROR [HintsDispatcher:2] 2017-07-16 22:00:32,580 HintsDispatchExecutor.java:234 - Failed to dispatch hints file a2b7daf1-a6a4-4dfc-89de-32d12d2d48b0-1500242103097-1.hints: file is corrupted ({})
org.apache.cassandra.io.FSReadError: java.io.IOException: Digest mismatch exception
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:199) ~[main/:na]
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:164) ~[main/:na]
    at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.sendHints(HintsDispatcher.java:157) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.sendHintsAndAwait(HintsDispatcher.java:139) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.dispatch(HintsDispatcher.java:123) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.dispatch(HintsDispatcher.java:95) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.deliver(HintsDispatchExecutor.java:268) [main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:251) [main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:229) [main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.run(HintsDispatchExecutor.java:208) [main/:na]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_111]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_111]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [main/:na]
    at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_111]
Caused by: java.io.IOException: Digest mismatch exception
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNextInternal(HintsReader.java:216) ~[main/:na]
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:190) ~[main/:na]
    ... 16 common frames omitted
{noformat}

It causes multiple cassandra nodes stop [by default|https://github.com/apache/cassandra/blob/cassandra-3.0/conf/cassandra.yaml#L188].

Here is the reproduce steps on a 3 nodes cluster, RF=3:
1. stop node1
2. send some data with quorum (or one), it will generate hints file on node2/node3
3. drop the table
4. start node1

node2/node3 will report ""corrupted hints file"" and stop. The impact is very bad for a large cluster, when it happens, almost all the nodes are down at the same time and we have to remove all the hints files (which contain the dropped table) to bring the node back.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-17 20:48:02.766,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 10 06:35:33 UTC 2018,,,,,,0|i3hkxb:,9223372036854775807,3.0.14,,,,,,,jjirsa,jjirsa,,,,,,,,,,"17/Jul/17 06:53;jay.zhuang;The problem is because {{[skipBytes()|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/io/util/RebufferingInputStream.java#L119]}} will leave some value in the buffer. Before returning, it should call {{input.checkCrc()}} to update the CRC.
The fix is to set {{hint=null}} and use the reminding code to do {{checkCRC()}}. Adding an uTest to reproduce the problem:
| Branch | uTest |
| [13696-3.0|https://github.com/cooldoger/cassandra/tree/13696-3.0] | [circleci#21|https://circleci.com/gh/cooldoger/cassandra/21]|
| [13696-3.11|https://github.com/cooldoger/cassandra/tree/13696-3.11] | [circleci#24|https://circleci.com/gh/cooldoger/cassandra/24]|
| [13696-trunk|https://github.com/cooldoger/cassandra/tree/13696-trunk] | [circleci#23|https://circleci.com/gh/cooldoger/cassandra/23]|

[~jjirsa] would you please review the patch?","17/Jul/17 20:48;jjirsa;Looks pretty serious, I'll try to get to it soon. Short term, your trunk patch doesn't actually compile ( {{CFMetaData}} is gone in trunk), can you fix that and  push that branch? ","18/Jul/17 06:00;jay.zhuang;Sure, updated the test for trunk:
| Branch | uTest |
| [13696-3.0|https://github.com/cooldoger/cassandra/tree/13696-3.0] | [circleci#21|https://circleci.com/gh/cooldoger/cassandra/21]|
| [13696-3.11|https://github.com/cooldoger/cassandra/tree/13696-3.11] | [circleci#24|https://circleci.com/gh/cooldoger/cassandra/24]|
| [13696-trunk|https://github.com/cooldoger/cassandra/tree/13696-trunk] | [circleci#25|https://circleci.com/gh/cooldoger/cassandra/25]|

Yes, I think it's a serious problem that all nodes go down at the same time. And to recover, the user has to delete all hints file (Not sure if there's an easy way to identify the ""corrupted"" hints files, system.log only shows the first one. So for us, we just delete all the hints files.)","19/Jul/17 05:23;jjirsa;Hey [~jay.zhuang] - took a look and I don't see any issues. The new tests look good. trunk circleCI run failed due to circleci, but run #26 looks green.

I'd like to glance at it again tomorrow when I'm more awake, but as of right now, it seems very reasonable and correct to me.

","19/Jul/17 06:31;jay.zhuang;Thanks [~jjirsa].
I did more investigation today. Seems it's more serious than I thought. Even there's no down node, ""drop table"" + write traffic, will trigger the problem.
Here is reproduce steps:
1. Create a 3 nodes cluster:
  {{$ ccm create test13696 -v 3.0.14 && ccm populate -n 3 && ccm start}}
2. Send some traffics with cassandra-stress (blogpost.yaml is only in trunk, if you use another yaml file, change the RF=3)
  {{$ tools/bin/cassandra-stress user profile=test/resources/blogpost.yaml cl=QUORUM truncate=never ops\(insert=1\) duration=30m -rate threads=2 -mode native cql3 -node 127.0.0.1}}
3. While the traffic is running, drop table
  {{$ cqlsh -e ""drop table  stresscql.blogposts""}}
*All 3 nodes go down because of ""Digest mismatch Exception"".*

The CRC calculation problem has been there for a long time, but only got exposed after CASSANDRA-13004 because of the MessagingService version bump. In the normal case when the versions are the same, HintsDispatcher uses {{[page.buffersIterator()|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsDispatcher.java#L138]}} instead of {{[page.hintsIterator()|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsDispatcher.java#L139]}}. {{buffersIterator()}} doesn't need to decode hints, so it won't have the problem.

I think the messagingVersion for the hints file should be updated: https://github.com/apache/cassandra/compare/cassandra-3.0...cooldoger:13696.2-3.0?expand=1 so it could dispatch hints in an optimized way. Not sure if we need to check/bump other {{MessagingService.VERSION_30}}s in the 3.0 branch.
cc [~ifesdjeen]","19/Jul/17 07:19;jjirsa;cc [~iamaleksey] as well.
","19/Jul/17 11:29;ifesdjeen;Might be we want to use either 30 or 3014 depending on which one is active?

Question: does this happen in mixed version cluster or all the nodes actually have the same protocol version?","19/Jul/17 17:48;jay.zhuang;{quote}
Question: does this happen in mixed version cluster or all the nodes actually have the same protocol version?
{quote}
All the nodes are on the same messagingVersion {{[VERSION_3014|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/net/MessagingService.java#L95]}}
It can be reproduced with a new 3.0.14 cluster.","20/Jul/17 12:20;ifesdjeen;I agree we should also return a correct version from the hints service (as [~jay.zhuang] already mentioned), like [here|https://github.com/apache/cassandra/compare/cassandra-3.0...cooldoger:13696.2-3.0?expand=1] same as we do in commit log descriptor.

This also would make the issue for same-version go away, and since it would make the service to pick a different code path I'd say it's also necessary to include it. 

WRT to the patch itself, might be it's better to just call {{resetCrc}} explicitly and still return null like I did [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13696-3.0#diff-cf15f9cac67d8b2f3e581129d617df16R242]? {{hint}} is a local variable, and setting it and carrying on makes the logic a bit harder to understand. For example, for me it was non-obvious that this boolean method would also do some buffer rewinding / state resetting under the hood. ","20/Jul/17 15:38;jjirsa;Wanted to chat with Aleksey offline about the version implications, so withholding comment on that for a bit, but:


{quote}
might be it's better to just call resetCrc explicitly and still return null like I did here? hint is a local variable, and setting it and carrying on makes the logic a bit harder to understand. For example, for me it was non-obvious that this boolean method would also do some buffer rewinding / state resetting under the hood.
{quote}

I think the current behavior is actually the right thing to do. Simply resetting the CRC state isn't enough, we need to check to see if the CRC matches, because we want to invoke the disk failure policy if we're reading corrupt data, and frankly, a corruption source (bad disk / RAM / etc) that flips bits could cause us to see an invalid CFID, and skipping the corruption test at that point would be the wrong thing to do. A few more comment lines are probably worthwhile, though, since it seems like an easy 'fix' to revert in the future because it's nonobvious. 

","20/Jul/17 16:14;ifesdjeen;bq. Simply resetting the CRC state isn't enough,

True. Re-read the issue description/first comment, now it's quite obvious. Thanks for explanation. 

Adding a comment would be great though!","20/Jul/17 18:35;jay.zhuang;Updated based on the reviews:
| Branch | uTest |
| [13696-3.0|https://github.com/cooldoger/cassandra/tree/13696-3.0] | [circleci#31|https://circleci.com/gh/cooldoger/cassandra/31]|
| [13696-3.11|https://github.com/cooldoger/cassandra/tree/13696-3.11] | [circleci#32|https://circleci.com/gh/cooldoger/cassandra/32]|
| [13696-trunk|https://github.com/cooldoger/cassandra/tree/13696-trunk] | [circleci#30|https://circleci.com/gh/cooldoger/cassandra/30]|

Also, {{resetCrc()}} could also cause CRC mismatch for the next hint read in the same file.

Another question is why dropTable + write traffic will cause hints file, do you think it's an issue? I create a separate ticket to track that: CASSANDRA-13712",24/Jul/17 12:24;iamaleksey;+1 from me as well.,"24/Jul/17 12:33;iamaleksey;{quote}
The CRC calculation problem has been there for a long time, but only got exposed after CASSANDRA-13004 because of the MessagingService version bump. In the normal case when the versions are the same, HintsDispatcher uses page.buffersIterator() instead of page.hintsIterator(). buffersIterator() doesn't need to decode hints, so it won't have the problem.
I think the messagingVersion for the hints file should be updated: https://github.com/apache/cassandra/compare/cassandra-3.0...cooldoger:13696.2-3.0?expand=1 so it could dispatch hints in an optimized way. Not sure if we need to check/bump other {{MessagingService.VERSION_30}} s in the 3.0 branch.
{quote}

And your analysis is correct, too. Thanks for catching and fixing it.","24/Jul/17 13:52;jjirsa;[~jay.zhuang] - could you rebuild 3.11 on circleci? test failed (exceeded memory limit), I'm fairly confident it's benign, but we should have a green test before committing (I'm confident it's benign because the test actually passed on 3.11 before the messaging fix, but we should prove there's not a regression there).
","24/Jul/17 17:47;jay.zhuang;Sure, rebuilt the CI, and it's passed: https://circleci.com/gh/cooldoger/cassandra/43","24/Jul/17 18:03;jjirsa;lgtm will commit when I have time.
","27/Jul/17 00:00;jjirsa;Committed to 3.0 as {{f919cf4a478cdbcb7864e8b47814a40bfcb343a7}} and merged to 3.11 and trunk. Thanks [~jay.zhuang], good find!
","09/Apr/18 18:05;vinegh;[~jay.zhuang]: I am not sure if this the same issue. I appiled commit you have here on 3.11.1 and we still see the following issue on a 14 node cluster

Any ideas on what might be wrong?

ERROR [HintsDispatcher:1] 2018-04-06 16:26:44,423 CassandraDaemon.java:228 - Exception in thread Thread[HintsDispatcher:1,1,main]
org.apache.cassandra.io.FSReadError: java.io.IOException: Digest mismatch exception
at org.apache.cassandra.hints.HintsReader$BuffersIterator.computeNext(HintsReader.java:298) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsReader$BuffersIterator.computeNext(HintsReader.java:263) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatcher.sendHints(HintsDispatcher.java:169) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatcher.sendHintsAndAwait(HintsDispatcher.java:128) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatcher.dispatch(HintsDispatcher.java:113) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatcher.dispatch(HintsDispatcher.java:94) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.deliver(HintsDispatchExecutor.java:278) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:260) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:238) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.run(HintsDispatchExecutor.java:217) ~[apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_141]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_141]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_141]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_141]
at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.1.jar:3.11.1-SNAPSHOT]
at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_141]
Caused by: java.io.IOException: Digest mismatch exception

To overcome this we do a truncatehints and everything works after that. Issue happens once a while we have seen it twice so far on 14 node cluster and 7 node cluster

 ","09/Apr/18 23:54;jay.zhuang;Hi [~vinegh], this should be a different issue, as [{{HintsDispatcher.java:128}}|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/hints/HintsDispatcher.java#L128] sends hints with {{buffer}}s, this patch is only to fix the digest mismatch for [{{HintsDispatcher.java:129}}|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/hints/HintsDispatcher.java#L129], which sends hints one by one.

Do you see the digest mismatch issue on one node or multiple nodes? Do you have schema change during that? Maybe you should file a separate ticket for that.","10/Apr/18 06:35;vinegh;[~jay.zhuang]: Yes we see it on multiple hosts. Yes, schema is being propagated to other nodes. We create schema on one node and bring up rest of nodes so it should take a while to propagate. I will file a separate ticket too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Max ttl of 20 years will overflow localDeletionTime,CASSANDRA-14092,13122467,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,03/Dec/17 22:01,12/Mar/19 14:20,13/Mar/19 22:34,11/Feb/18 13:38,2.1.20,2.2.12,3.0.16,3.11.2,,Legacy/Core,,,,,2,,,,"CASSANDRA-4771 added a max value of 20 years for ttl to protect against [year 2038 overflow bug|https://en.wikipedia.org/wiki/Year_2038_problem] for {{localDeletionTime}}.

It turns out that next year the {{localDeletionTime}} will start overflowing with the maximum ttl of 20 years ({{System.currentTimeMillis() + ttl(20 years) > Integer.MAX_VALUE}}), so we should remove this limitation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14227,,,,,,CASSANDRA-14228,CASSANDRA-4771,,,,,11/Feb/18 13:27;pauloricardomg;2.1-14092-dtest.png;https://issues.apache.org/jira/secure/attachment/12910103/2.1-14092-dtest.png,11/Feb/18 13:27;pauloricardomg;2.1-14092-testall.png;https://issues.apache.org/jira/secure/attachment/12910102/2.1-14092-testall.png,11/Feb/18 13:27;pauloricardomg;2.2-14092-dtest.png;https://issues.apache.org/jira/secure/attachment/12910101/2.2-14092-dtest.png,11/Feb/18 13:27;pauloricardomg;2.2-14092-testall.png;https://issues.apache.org/jira/secure/attachment/12910100/2.2-14092-testall.png,11/Feb/18 13:27;pauloricardomg;3.0-14092-dtest.png;https://issues.apache.org/jira/secure/attachment/12910099/3.0-14092-dtest.png,11/Feb/18 13:27;pauloricardomg;3.0-14092-testall.png;https://issues.apache.org/jira/secure/attachment/12910098/3.0-14092-testall.png,11/Feb/18 13:27;pauloricardomg;3.11-14092-dtest.png;https://issues.apache.org/jira/secure/attachment/12910097/3.11-14092-dtest.png,11/Feb/18 13:27;pauloricardomg;3.11-14092-testall.png;https://issues.apache.org/jira/secure/attachment/12910096/3.11-14092-testall.png,11/Feb/18 13:27;pauloricardomg;trunk-14092-dtest.png;https://issues.apache.org/jira/secure/attachment/12910095/trunk-14092-dtest.png,11/Feb/18 13:27;pauloricardomg;trunk-14092-testall.png;https://issues.apache.org/jira/secure/attachment/12910094/trunk-14092-testall.png,,,10.0,,,,,,,,,,,,,,,,,,,2018-01-24 17:15:29.398,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 11 13:37:48 UTC 2018,,,,,,0|i3nhbz:,9223372036854775807,3.0.15,,,,,,,beobal,beobal,,,,,,,,,,"03/Dec/17 22:04;pauloricardomg;I think that CASSANDRA-8099 already removed this limitation from the storage engine by encoding {{localDeletionTime}} as a varint, so it should be relatively easy to update the code to represent {{localDeletionTime}} as a long instead.",24/Jan/18 17:15;christianmovi;Reproduced in 3.0.15.,"24/Jan/18 19:46;rbfblk;Copying/paraphrasing my recent comment from CASSANDRA-4771 here:

It looks like local deletion time is encoded per cell as an offset in seconds from the minimum local deletion time in the SSTable. If I'm reading it right the offset is serialized as a variable length integer, so reading it into a long instead of an int would give a very large range of offsets that would be more than sufficient. However it looks to me like the minimum local deletion time at the SSTable metadata level is saved as a fixed-size 4-bytes and de-serialized into a (signed) int. Without changing the serialization format we could change the interpretation of that metadata field to assume it is an unsigned integer (and de-serialize it also into a long variable instead of an int) which should give us another 68 years. A better fix would probably be to also change the serialization format of minLocalDeletionTime in the metadata to an 8 byte integer or a variable size integer so it could hold values in the far far future. Changing the SSTable format might be a 4.0+ thing though. Either way the in-memory representation of local delete time would become a long not an int.

I haven't yet looked at what if anything this means for the inter-node wire format between coordinator and replica, that might be a complicating factor if this is a breaking change to the format.","26/Jan/18 02:55;pauloricardomg;The patch below [reduces the maximum allowed TTL|https://github.com/apache/cassandra/commit/6e66a4730a6f606fe1bb609ba4d8ec650877605e] from 20 years to 15 years to prevent integer overflow on the {{localDeletionTime}} field what causes inserts with TTL close to the current maximum of 20 years to fail with AssertionError on 2.1 and expire as soon as they are inserted on 3.0, as discussed on the [mailing list thread|https://www.mail-archive.com/dev@cassandra.apache.org/msg11888.html].This is just an immediate measure to prevent data loss on 3.0+ while we look for a more permanent solution to raise this limitation.

In addition to lowering the max default TTL, this patch also [adds ability to scrub to detect and fix rows/cels|https://github.com/apache/cassandra/commit/d9bbb7992696a02ccc490229280894f0437ae0b7] with overflowed {{localDeletionTime}} which were not yet turned into tombstones by compaction and detect rows which were already turned into tombstones to allow operators to identify which rows were affected. Furthermore, I [modified the compaction purger|https://github.com/apache/cassandra/commit/ac04a4199947d41962c34bce07d9aad4ebf87e47] to not purge rows with overflowed localDeletionTime to allow them to be fixed via scrub.

This is ready for a initial round of review, but I will still add some tests for scrub and a more detailed NEWS.txt entry.
||2.1||3.0||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092]|",26/Jan/18 16:02;pauloricardomg;Canceling the patch while I work on an alternative version after discussion on the ML.,"26/Jan/18 22:59;pauloricardomg;This patch below takes a simpler and more transparent approach. The idea is to keep the maximum allowed TTL of 20 years, but cap the maximum expiration time to 2038-01-19T03:14:06+00:00.

When this capping is done, the following log is print in the client and system.log:
{noformat}
WARN  [SharedPool-Worker-2] 2018-01-26 19:15:36,877 NoSpamLogger.java:94 - TTL of 630720000 seconds exceeds maximum supported expiration date of 2038-01-19T03:14:06+00:00. Rows with expiration date exceeding the maximum supported date will expire in the limit date. In order to avoid this use a lower TTL or upgrade to a version where this limitation is fixed. See CASSANDRA-14092 for more details.
{noformat}
In addition to this, the patch converts any negative localExpirationTime that may have been written to 2038-01-19T03:14:06+00:00 - fixing any wrong entries that may still be present but were not yet purged.

Since we store the TTL as a separate field, once we fix this limitation we can recompute the correct expiration time with the timestamp/1000+ttl time during upgradesstables.

I added tests to check that data inserted with a TTL exceeding the maximum allowed expiration time is present and a warning is logged. I also added tests to check that SSTables written with the negative localExpirationTime are correctly interpreted.

||2.1||2.2||3.0||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092-v2]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-14092-v2]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092-v2]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14092]|


Submitted CI, will attach results when ready.","29/Jan/18 14:06;dimitarndimitrov;Some review comments on the dtest and trunk changes:
 * On the dtest change:
 ## Shouldn't the dtest docstring [here|https://github.com/apache/cassandra-dtest/commit/83c73ef0a3cbe50232d3a9eea4fd26c877ea58db#diff-a8f4dac4af77196a8c7881abd067a5b9R345] say something related to the TTL problem?
 ## The start time [here|https://github.com/apache/cassandra-dtest/commit/83c73ef0a3cbe50232d3a9eea4fd26c877ea58db#diff-a8f4dac4af77196a8c7881abd067a5b9R348] seems redundant
 ## It may be good to extract the max TTL value in a variable - we may decide to keep a version of this test after we patch by just reducing that value, but before we fix it nicely
 * On the trunk change:
 ## Maybe it's my English, but [this wording|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2#diff-5414c0e96996be355c3aff1184ec859aR48] sounds a bit confusing to me, using ""maximum supported date"" and ""limit date"" for the same thing. Thoughts? If you're also hesitant, what do you think about ""Rows that should expire after that date would still expire on that date.""?
 ## You can quickly mention the relevant JIRA ticket [here|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2#diff-b7ca4b9c415e93b6cbfb31daf90cc598R185]
 ## Qualify the static access to {{Cell.sanitizeLocalDeletionTime}} [here|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2#diff-b7ca4b9c415e93b6cbfb31daf90cc598R53]
 ## Could you please add some comments/Javadoc for [{{Cell.sanitizeLocalDeletionTime}}|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2#diff-3e9f1fc67f99d27e92a3eb32201d8ca6R311]? I would assume that {{NO_TTL}} and {{NO_DELETION_TIME}} are needed to determine whether the cell is an expiring one, an expired one, or a tombstone, but I'm not too sure
 ## There are missing spaces between the boolean arguments of the delegation call for some of the unit tests (e.g. [here|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v2#diff-0d8cf6ca6ed99c947903359c1beaf386R74])","29/Jan/18 16:46;VincentWhite;I haven't had a chance to completely catch up on the ML discussion but I thought I would post a proof of concept branch I had that promotes localDeletionTime to long on trunk. The code definitely isn't meant to be a final product but it might be a useful starting point regardless of whether we actually decide to go this route or not. It's fairly straight forward and unfortunately most of the code went towards undoing optimizations introduced to tombstone histograms in CASSANDRA-13444. I put together the majority of this last year so I may be forgetting some glaring issues but I believe it was basically complete minus a few unit tests to be cleaned up and tools updated.

One outstanding issue I do recall is related to EXPIRED_LIVENESS_TTL which is currently Integer.MAX_VALUE but sounds like that should be resolved/removed at some point by CASSANDRA-13826.
||trunk||
|[branch|https://github.com/vincewhite/cassandra/commit/364f9ac848ae54eae9a1360d72aad4ba0a2b63a8]|","30/Jan/18 02:22;jjirsa;Given the nature of this bug, can we get someone familiar with engine internals (read: an existing committer) to review it? Like maybe [~beobal] ? ","30/Jan/18 05:13;pauloricardomg;{quote}Some review comments on the dtest and trunk changes:
{quote}
Thanks for your feedback! I addressed your suggestions/nits and updated the warning message to:
{noformat}
Request on table ks.ttl_table with default ttl of 630720000 seconds exceeds maximum supported expiration date of 2038-01-19T03:14:06+00:00 and will have its expiration capped to that date. In order to avoid this use a lower TTL or upgrade to a version where this limitation is fixed. See CASSANDRA-14092 for more details.
{noformat}
In addition to the fixes above, [~jasonstack] pointed out offline that the {{default_time_to_live}} is not capped to 20 years on the 2.1 branch, so I added the cap [here|https://github.com/apache/cassandra/commit/4d592eb990c1f7a84ae79738ea3ffb22f67282d3]. This made me realize that the warning was not being print when the insert was using the table default TTL, so I fixed this [here|https://github.com/apache/cassandra/commit/109fa214fbedc3b803e47f8dc79b41132ef2a1eb] and added a dtest [here|https://github.com/apache/cassandra-dtest/commit/09f39d5910d4c0dd1b800698deec7f7a6c5c747a]. I also added some dtests to check that the warning is print when the access is done via thrift ([here|https://github.com/apache/cassandra-dtest/commit/748ab67a1ce3950640747d6b980ef57b7871e59b]).

I updated all branches above with the changes above + other minor fixes and test fixes. Will update with test results when CI is ready.
{quote}I thought I would post a proof of concept branch I had that promotes localDeletionTime to long on trunk.
{quote}
Thanks a lot for your effort! While this looks like a straightforward approach and I don't see any particular problems with it at first glance, I'd like to focus right now on shipping a temporary solution to fix the silent data loss problem on 3.0+ as soon as we can before working on a permanent solution with proper vetting, testing and impact analysis.","30/Jan/18 19:15;beobal;This is a nasty problem because all the potential solutions are bad and I should say that I've only gone in depth on the 3.0 patch as of yet, but I assume that the other versions are functionally equivalent.

I have serious concerns about the default action here being to fix up the data. IMO, the default action should be to reject client requests which attempt to set a TTL that's going to push expiration/deletion time past the threshold. As mentioned on the dev list there might be clients out there that can't easily tolerate that, so my suggestion would be that we enable the capping of the expiration date (at insert/update time only and with a client + log warning) by means of a -D flag. All of which is definitely annoying and ugly, but probably not too controversial.

However, there is another issue as the current patch can also lead to previously 'gone' data being resurrected without warning or notification. If an SSTable contains data with an overflowed expiration, from the client's perspective that data is not present. Applying the patch before the data is purged fixes up the expiration date, capping it at the limit date and so the previously gone data will once again be returned in query results. Whether we should allow this resurrection is IMO highly questionable, not knowing what decisions have been taken outside of the database based on that data not being present/visible. My preference would be for gone data to stay gone, with another -D flag to turn on post-insert capping of the expiration date.

Moreover, this resurrection is temporary and persists only until the SSTable is involved in a compaction. At that point, the expiration date causes a purge and the data disappears again. This is definitely not cool and if we do fix up the data, it has to stay fixed up.
","30/Jan/18 19:41;jjordan;{quote}I have serious concerns about the default action here being to fix up the data. IMO, the default action should be to reject client requests which attempt to set a TTL that's going to push expiration/deletion time past the threshold. As mentioned on the dev list there might be clients out there that can't easily tolerate that, so my suggestion would be that we enable the capping of the expiration date (at insert/update time only and with a client + log warning) by means of a -D flag. All of which is definitely annoying and ugly, but probably not too controversial.
{quote}
I think our default should be to cap with warnings.  What is a user going to do when they get the error?  They are going to just go change there code to pick a new date that is still real big, so I don't see a reason to fail things.
{quote}Whether we should allow this resurrection is IMO highly questionable, not knowing what decisions have been taken outside of the database based on that data not being present/visible.
{quote}
We need a way to let users recover the data that was silently dropped.  I could see an argument for a -D to let the data stay dropped, rather than recovering it, but I definitely think our default here should be to recover the lost data.
{quote}Moreover, this resurrection is temporary and persists only until the SSTable is involved in a compaction. At that point, the expiration date causes a purge and the data disappears again. This is definitely not cool and if we do fix up the data, it has to stay fixed up.
{quote}
Does this actually happen?  The expiration date will be fixed up when the cell is loaded, so on compaction it should be written back out with the new time?  If this is not what happens then it is an over sight in the patch and should be fixed.","30/Jan/18 20:02;pauloricardomg;bq. I have serious concerns about the default action here being to fix up the data. IMO, the default action should be to reject client requests which attempt to set a TTL that's going to push expiration/deletion time past the threshold. As mentioned on the dev list there might be clients out there that can't easily tolerate that, so my suggestion would be that we enable the capping of the expiration date (at insert/update time only and with a client + log warning) by means of a -D flag. All of which is definitely annoying and ugly, but probably not too controversial.

My reasoning for this is that once we fix the issue permanently, the idea is to restore/recompute the correct localExpirationTime via (timestamp/1000 + ttl). This is obviously not perfect as the timestamp could be provided by the client, so there could be some slight variance here, but if someone is setting a TTL 20 years in advance, I think it is able to tolerate a few seconds or even minutes of difference in the expiration time. I don't think the case where the client is using a different timestamp format plus overflowing TTL is realistic enough that it will create problems, but we can also protect against this and perhaps provide and option to opt out of fix by default behavior if necessary.","30/Jan/18 20:17;pauloricardomg;bq. Whether we should allow this resurrection is IMO highly questionable, not knowing what decisions have been taken outside of the database based on that data not being present/visible. My preference would be for gone data to stay gone, with another -D flag to turn on post-insert capping of the expiration date.

I don't expect us to take longer than a few weeks (at worst a couple of months) to come up with a permanent solution for this, our goal here is to prevent users from inadvertedly losing data in the time frame where the permanent fix is not available. People that detect that data have gone missing without issuing a deletion, will be aware of these issues and will likely take additional measures to remediate it - like reducing the TTL or update their application to correctly handle the data in the affected time period. I personally find it highly unlikely that this scenario will be a problem in practice,  as long we properly document and communicate the problem on NEWS.txt, users that were affected and care about their data will likely take additional measure to recover their state. ","30/Jan/18 20:18;beobal;bq. I think our default should be to cap with warnings.  What is a user going to do when they get the error?  They are going to just go change there code to pick a new date that is still real big, so I don't see a reason to fail things.

Yes, I imagine that's exactly what they will do, but the user will fully aware that it's happening, not having their data changed on them. We ought to be being conservative by default here.

bq. We need a way to let users recover the data that was silently dropped.  I could see an argument for a -D to let the data stay dropped, rather than recovering it, but I definitely think our default here should be to recover the lost data.

I don't agree. It's definitely bad that the data was silently dropped, but like I said we (the db) has no way of knowing what (application) decisions have been taken based on the visible state of the database. I think it's pretty clear that if the data appeared to be gone at any point (and we have no good reason to assume that the client has not observed such a state) that it must stay gone. 

bq. Does this actually happen?

Yes, of course I tested it.","30/Jan/18 20:23;pauloricardomg;bq.  Moreover, this resurrection is temporary and persists only until the SSTable is involved in a compaction. At that point, the expiration date causes a purge and the data disappears again. This is definitely not cool and if we do fix up the data, it has to stay fixed up.

This was definitely not supposed to happen, I'll take a look at it. Would you have an easy repro for this?

Thanks for taking the time to review this [~beobal] and for the comments [~jjordan]!","31/Jan/18 09:48;beobal;{quote}Would you have an easy repro for this?
{quote}
Sure:
 * On an unpatched version (I used 3.0), insert a row with a big enough TTL
 * Query & see that the data is not returned.
 * Apply the patch & restart
 * Query & see that the row now appears
 * Run nodetool compact on the table
 * Query & the row has gone again

If you do *only* these steps on a clean node, then the compaction will result in 0 SSTables for the table. A more normal workflow of inserting rows then flushing until an automatic compaction is triggered also has the same effect, but of course doesn't remove any data without the overflowed expiry.","01/Feb/18 02:28;KurtG;I don't think it's a good idea not to fix the data. That's incrementally going to break more and more applications as we get closer to 2038.  We should be fixing the data, making it very clear that we are in fact fixing the data (client and server warnings) and advertising heavily on the website/ML/NEWS.txt for people to get off affected versions.","01/Feb/18 02:58;jjirsa;I don’t care if there’s logic to resurrect data, but it must be off by default. People may have already adapted and reacted to the absence of that data, and we can’t assume it's always safe to resurrect
","01/Feb/18 06:45;pauloricardomg;After a second thought I agree that trying to automatically recover lost data is bad not only because users may have reacted to the absence of data, but also because due to the negative {{localDeletionTime}}, the lost data will likely be purged instantly during compaction, so not much data will be left to recover after a while and the auto-recovery behavior will be pretty much undefined. Furthermore, we will need to keep the recovery logic in the storage engine at least until 4.0 with limited benefit.

The only way a user that set TTL 20 years ahead can get away with reliably recovering its data is if it has {{incremental_backups}} enabled, and in this case, it can easily find out which SSTables have a negative {{minLocalDeletionTime}} and run scrub on them to fix the overflow, so I'm strongly leaning towards my previous approach of providing a way for scrub to fix negative timestamps, rather then embedding this recovery logic in the storage engine itself - which would be arguably useful in a handful of cases.

Regarding what to do with TTLs exceeding the maximum expiry date until we have a permanent solution to the problem, I think it's fair to expose the cap vs reject trade-off as flag to the operator, so I introduced a {{-Dcassandra.expiration_date_overflow_policy={REJECT, CAP_NOWARN, CAP_WARN}} option, which defaults to the conservative REJECT behavior and has the following meaning explained on NEWS.txt:
{noformat}
      - REJECT: this is the default policy and will reject any requests with expiration date timestamp after 2038-01-19T03:14:06+00:00.
      - CAP_NOWARN: any insert with TTL expiring after 2038-01-19T03:14:06+00:00 will expire on 2038-01-19T03:14:06+00:00.
      - CAP_WARN: same as previous, but will log a warning message when inserted data is capped.
{noformat}
Any feedback and suggestions of alternative more meaningful naming is welcome.

One downside of defaulting to REJECT I see is that applications will start to break as time progresses and TTLs start reaching the maximum date more often, but given that we plan to fix the underlying issue soon in upcoming versions and users have a way to change the policy I think it shouldn't be a big problem, but I don't feel strongly about it so if anyone does please chime in.

The patch below implements and tests the expiration_date_overflow_policy, but also adds a new flag {{cassandra.recover_overflowed_expiration_best_effort}} to perform the auto-recovery, which I intend to remove due to the reasons explained above and replace it with the [scrub-based recovery|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092] (unless someone objects).

I plan to cleanup these details later today, but if anyone can take a look and give early feedback that would be nice, as I'm hoping to get this in as soon as we can to start a vote.

The 2.1 and 3.0 are functionally equivalent, except that the 2.1 patch adds a missing 20-years cap for the default_time_to_live option, and that the 3.0 patch adds the to-be-removed {{cassandra.recover_overflowed_expiration_best_effort}} option. The versions for the other branches are pretty much derived from those so I will do that after the final review round.
||2.1||3.0||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092-v3]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092-v3]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14092]|","01/Feb/18 11:22;beobal;Sounds good to me. Reject and optionally cap at input time, plus a offline route to full recovery (if possible & desired) via scrub sounds like the best way forward IMO. I'll do a full review when you have the cleaned up patches ready, but the WIP branches generally LGTM at first glance. The wording of the NEWS.txt entry is good, I do wonder if we should maybe place it right at the top of the file rather than just in the 3.0.16 section for extra emphasis. Any thoughts on that? 

I also have one piece of feedback on the policies; I don't see any benefit in being able to turn off logging of capped expirations (especially since we're using NoSpamLogger) but I do I think the client warning is useful. So I would change the policies to:
{noformat}
- REJECT (default and as you've defined it)
- CAP (cap, log and issue client warning)
- CAP_NOWARN (cap and log)
{noformat}

I also noticed that the logging of a parse error/invalid value for the policy sysprop is at DEBUG in the current patches, but it might be sensible to draw a bit more attention to that if it happens.
","01/Feb/18 23:51;KurtG;To clarify, CAP and CAP_NOWARN will cap the expiry but we'll have NO INTENTION of ever fixing it in an upgrade? Or would they have to do a scrub to convert anything that got capped to its actual TTL?

I think it's worth pointing out that REJECT is a ticking time bomb. The main concern being people who are still running anything <4.0 when their TTL's breach 2038-01-19 (which could be literally at any time). If the default was CAP and warn, fixing after upgrade then at least we wouldn't be bound to break peoples application in the future, and we'd still have almost 20 years to get everyone off these versions without breaking their applications.","02/Feb/18 05:54;pauloricardomg;Thanks for the quick turnaround [~beobal]! See follow-up below:
{quote}The wording of the NEWS.txt entry is good, I do wonder if we should maybe place it right at the top of the file rather than just in the 3.0.16 section for extra emphasis. Any thoughts on that?
{quote}
Good idea, I did this and also updated the text to contemplate the possibility of data loss before this patch and how to fix it with scrub:
{noformat}
MAXIMUM TTL EXPIRATION DATE NOTICE
-----------------------------------

The maximum expiration timestamp that can be represented by the storage engine is 2038-01-19T03:14:06+00:00,
which means that inserts with TTL that expire after this date are not currently supported.

Prior to 3.0.16 in the 3.0.X series and 3.11.2 in the 3.11 series, there was no protection against INSERTS
with TTL expiring after the maximum supported date, causing the expiration time field to overflow and the
records to expire immediately. Expired records due to overflow may have been removed permanently after a
compaction. The 2.1.X and 2.2.X series are not subject to data loss due to this issue if assertions are enabled,
since an AssertionError is thrown during INSERT when the expiration time field overflows on these versions.

In practice this issue will affect only users that use very large TTLs, close to the maximum allowed value of
630720000 seconds (20 years), starting from 2018-01-19T03:14:06+00:00. As time progresses, the maximum supported
TTL will be gradually reduced as the the maximum expiration date approaches. For instance, a user on an affected
version on 2028-01-19T03:14:06 with a TTL of 10 years will be affected by this bug, so we urge users of very
large TTLs to upgrade to a version where this issue is addressed as soon as possible.

Potentially affected users should inspect their SSTables and search for negative min local deletion times to
detect this issue. SSTables in this state must be backed up immediately, as they are subject to data loss
during auto-compactions, and may be recovered by running the sstablescrub tool from versions 3.0.16+ and/or 3.11.2+.

The Cassandra project plans to fix this limitation in newer versions, but while the fix is not available, operators
can decide which policy to apply when dealing with inserts with TTL exceeding the maximum supported expiration date:
  - REJECT: this is the default policy and will reject any requests with expiration date timestamp after 2038-01-19T03:14:06+00:00.
  - CAP: any insert with TTL expiring after 2038-01-19T03:14:06+00:00 will expire on 2038-01-19T03:14:06+00:00 and the client will receive a warning.
  - CAP_NOWARN: same as previous, except that the client warning will not be emitted.

These policies may be specified via the -Dcassandra.expiration_date_overflow_policy=POLICY startup option which can be set in the jvm.options file.

See CASSANDRA-14092 for more details about this issue.
{noformat}
Please let me know what do you think of the updated text. We should also probably publish this text (or a subset of it) during the release announcement e-mail.

While writing the text above, I figured that there is also a remote possibility of data loss in 2.1/2.2 if assertions are disabled, but didn't backport the scrub recovery since it was not a straightforward backport and I didn't think it was worth the effort right now. We can always do that later if necessary, the most important thing right now is to ship the policies. To reflect this I updated the 4th paragraph on 2.1 and 2.2 to:
{noformat}
2.1.X / 2.2.X users in the conditions above should not be subject to data loss unless assertions are disabled, in which
case the suspect SSTables must be backed up immediately and manually recovered, as they are subject to data loss
during auto-compaction.
{noformat}
 
{quote}I also have one piece of feedback on the policies; I don't see any benefit in being able to turn off logging of capped expirations (especially since we're using NoSpamLogger) but I do I think the client warning is useful.
{quote}
I agree and updated the patch with this suggestion, but at the same time I think advanced operators may want to control the periodicity of the logging, so I created a property {{cassandra.expiration_overflow_warning_interval_minutes=5}} to control this.
  
{quote}I also noticed that the logging of a parse error/invalid value for the policy sysprop is at DEBUG in the current patches, but it might be sensible to draw a bit more attention to that if it happens.
{quote}
Agreed, changed the logging to WARN.

I finished the cleanup of the patch and already provided a version for all branches. The 2.1 and 2.2 versions are pretty much the same, as well as the 3.0/3.11/trunk, except for some minor conflicts. Please find below a short summary of the changes per branch:
 * 2.1:
 ** Add REJECT and CAP expiration date overflow policies and tests
 ** Cap max default TTL at 20 years and tests
 ** Add NEWS.txt entry
 * 2.2:
 ** Same as 2.1, few minor import conflicts
 * 3.0
 ** Add REJECT and CAP, CAP_NOWARN expiration date overflow policies and tests
 ** Add ability to scrub to fix negative localDeletionTime and tests with broken SSTables
 ** Add ability to sstablemetadata to show minLocalDeletionTime
 ** Add expiration date overflow policies to jvm.options file
 ** Add NEWS.txt entry
 * 3.11
 ** Same as 3.0, few minor conflicts during merge
 * master
 ** Same as 3.11, few minor conflicts during merge
 ** Removed ability of scrub to fix sstables with negative localdeletionTime and tests
 * dtest
 ** Test all policies on CQL for default and user supplied TTL
 ** Test cap policy on thrift for default and user supplied TTL
 ** Check that offline scrub recovers sstable with negative localDeletionTime

I submitted a preliminary round of CI with the non-cleaned up patch and the results looked good. I will submit again for all the branches below and post the results here when they are ready.
||2.1||2.2||3.0||3.11||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092-v5]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-14092-v5]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092-v5]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14092-v5]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v5]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14092-v5]|","02/Feb/18 06:07;pauloricardomg;{quote}To clarify, CAP and CAP_NOWARN will cap the expiry but we'll have NO INTENTION of ever fixing it in an upgrade?
{quote}
We're not promising to fix the correct TTL in an upgrade, but we could do it in some cases, but I prefer to leave this decision for later.
{quote}Or would they have to do a scrub to convert anything that got capped to its actual TTL?
{quote}
The scrub is just to fix SSTables of affected systems that overflowed from 2018-01-19T03:14:06+00:00 until the upgrade and were backed up. As I said before, we're not doing any promise yet regarding honoring the actual TTL when it's capped. But if we were to implement this would probably be done during upgradesstables and not scrub.
{quote}I think it's worth pointing out that REJECT is a ticking time bomb.
{quote}
I agree but I don't feel strongly about the default, because the policies will be clearly specified in big letters in the NEWS.txt which is the document which everyone should read before upgrading, so if you don't want applications to break in your organization just change your policy to CAP.

Do you mind proof-reading the NEWS.txt and check if something is not clear/can be improved? Thanks!","02/Feb/18 08:36;mshuler;{quote}We should also probably publish this text (or a subset of it) during the release announcement e-mail.
{quote}
I'll definitely include the text with the release, which sounds pretty complete to me. Thanks for writing it up.","05/Feb/18 02:09;KurtG;bq. Do you mind proof-reading the NEWS.txt and check if something is not clear/can be improved? Thanks!
Looks good, but a few things to reduce confusion:
# Can we stick a PLEASE READ/WARNING/ALERT or something in the title to make it clear that users actually have to read that section?
Can we be a bit more clear on the following points?
# Expired records due to overflow *will* be removed permanently after a compaction. (there's only a very small chance that it would survive a compaction, and that's only if they previously wrote the same record and it overflowed by more, which somehow made it into a separate SSTable and didn't get compacted with )
# Data that is expired due to overflow *will* not be queryable and that prior to patching they'll have no way of telling they inserted with an overflowed TTL.
# How to search for negative min local deletion times (sstablemetadata)
# Probably a good time to note that all users will need to upgrade to >4.0 by 2038 and that during upgrade their TTL's will be set to their expected value.
","05/Feb/18 20:04;beobal;I think we're almost there with this, I just have a few smallish comments on the v5 patches:
 * In the 3.0+ branches, {{ExpirationDateOverflowHandling::maybeApplyExpirationDateOverflowPolicy}} can use {{Cell.NO_TTL}} rather than 0 in the first check.
 * In 3.0+ you renamed the static policy field to have a shorter name, but missed that in the 2.1 & 2.2 branches.
 * In 2.1 I saw some (very) intermittent test failures in TTLTest. I instrumented {{checkTTLIsCapped}} to print out the (min | actual | max) TTLs to sysout and eventually managed to repro it. You can see from the output that in the first line, the min and actual are actually > the max, which caused the test to fail (this happened around 10% of the time).

{code:java}
testlist:
 [echo] running test bucket 0 tests
 [junit] WARNING: multiple versions of ant detected in path for junit 
 [junit] jar:file:/usr/local/Cellar/ant@1.9/1.9.8/libexec/lib/ant.jar!/org/apache/tools/ant/Project.class
 [junit] and jar:file:/Users/sam/git/cassandra/build/lib/jars/ant-1.9.4.jar!/org/apache/tools/ant/Project.class
 [junit] Testsuite: org.apache.cassandra.cql3.validation.operations.TTLTest
 [junit] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.024 sec
 [junit] 
 [junit] ------------- Standard Output ---------------
 [junit] WARN 18:01:55 JNA link failure, one or more native method will be unavailable.
 [junit] WARN 18:01:55 JNA link failure, one or more native method will be unavailable.
 [junit] WARN 18:01:55 Request on table cql_test_keyspace.table_1 with default ttl of 630720000 seconds exceeds maximum supported expiration date of 2038-01-19T03:14:06+00:00 and will have its expiration capped to that date. In order to avoid this use a lower TTL or upgrade to a version where this limitation is fixed. See CASSANDRA-14092 for more details.
 [junit] WARN 18:01:55 Request on table cql_test_keyspace.table_1 with default ttl of 630720000 seconds exceeds maximum supported expiration date of 2038-01-19T03:14:06+00:00 and will have its expiration capped to that date. In order to avoid this use a lower TTL or upgrade to a version where this limitation is fixed. See CASSANDRA-14092 for more details.
 [junit] 629629931 | 629629931 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] 629629930 | 629629930 | 629629930
 [junit] ------------- ---------------- ---------------
 [junit] Testcase: testCapExpirationDatePolicyDefaultTTL(org.apache.cassandra.cql3.validation.operations.TTLTest): FAILED
 [junit] null
 [junit] junit.framework.AssertionFailedError
 [junit] at org.apache.cassandra.cql3.validation.operations.TTLTest.checkTTLIsCapped(TTLTest.java:259)
 [junit] at org.apache.cassandra.cql3.validation.operations.TTLTest.testCapExpirationDatePolicyDefaultTTL(TTLTest.java:139)
 [junit] 
 [junit]
{code}
The last thing is about providing a route to fix up overflowed dates via scrub, I think we should definitely leave the remedial Scrubber code in trunk until we have a proper fix committed. This will be in for 4.0, and at that point we can hapilly remove it, but until then it feels wrong to not have it. I also think we should have the scrub fix in 2.1 & 2.2 as some users have not yet moved to 3.x and they should probably get the chance to repair (maybe) their data if they want/are able to.

 ","08/Feb/18 17:10;pauloricardomg;Thanks for the review, this took a bit longer than expected as I found an edge case which required a change from the previous approach as I describe later. First, see the follow-up from the previous round of review:
{quote}In the 3.0+ branches, ExpirationDateOverflowHandling::maybeApplyExpirationDateOverflowPolicy can use Cell.NO_TTL rather than 0 in the first check.
{quote}
Agreed, fixed [here|https://github.com/apache/cassandra/commit/4bcea858f62b619b83a5db83f0cd93e192fea80c].
{quote}In 3.0+ you renamed the static policy field to have a shorter name, but missed that in the 2.1 & 2.2 branches.
{quote}
Good catch, fixed [here|https://github.com/apache/cassandra/commit/df59f2de8042e7ea67e520d509d675da1d53bbce].
{quote}In 2.1 I saw some (very) intermittent test failures in TTLTest. I instrumented checkTTLIsCapped to print out the (min | actual | max) TTLs to sysout and eventually managed to repro it. You can see from the output that in the first line, the min and actual are actually > the max, which caused the test to fail (this happened around 10% of the time).
{quote}
Oh, that's funny! Perhaps it could be related to the platform, as I cannot reproduce this locally? In any case, I updated the check [here|https://github.com/apache/cassandra/commit/18be7f140c3c2159f69d50b1bfb068927c734a13] to be more deterministic.

I also noticed that we weren't applying the expiration overflow policy in the CQLv2 interface, so I updated it [here|https://github.com/apache/cassandra/commit/4230a5b4126e972827990dc33c1a9140af07afe1]. I find it hard someone is using this but I wouldn't be totally surprised.
{quote}I think we should have the scrub fix in 2.1 & 2.2 as some users have not yet moved to 3.x and they should probably get the chance to repair (maybe) their data if they want/are able to.
{quote}
Agreed, as noted in the NEWS.txt, 2.1/2.2 is only affected if users have assertions disabled, but given that we have this [comment|https://github.com/apache/cassandra/blob/cassandra-2.1/conf/cassandra-env.sh#L173] on 2.1 I wouldn't be surprised if some users with assertions disabled hit this.

While writing the 2.1 scrubber (which is slightly different from 3.0 due to CASSANDRA-8099), I found a nasty edge case with the recovery process. The problem is that, if the cell with negative/overflowed {{localExpirationTime}} [is converted to a tombstone during compaction|https://github.com/apache/cassandra/blob/30ed83d9266a03debad98ffac5610dcb3ae30934/src/java/org/apache/cassandra/db/rows/AbstractCell.java#L95], we can't convert the tombstone back into an expired cell because the TTL value is lost and we can't differentiate this from an ordinary tombstone. Furthermore, even if the original SSTable is scrubbed and the negative {{localExpirationTime}} is converted to {{MAX_DELETION_TIME}}, if the tombstone was already generated, it will shadow/delete the fixed expired cell, because [deleted cells have precedence over live cells when the timestamp is the same|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/db/Conflicts.java#L43].

I updated {{recover_negative_expiration_date_sstables_with_scrub}} to [add an SSTable with the expired cell converted to a tombstone|https://github.com/apache/cassandra-dtest/commit/63b0e7d51fbacf4fabbb860d81873605c3b66c92], and verified that an existing tombstone shadows recovered data in the current version.

In order to fix this, the idea is during scrub increment the timestamp of cells with negative {{localExpirationTime}} by one in addition to setting the {{localExpirationTime}} to {{MAX_DELETION_TIME}}, so the recovered cell will shadow/supersede the potential tombstone that was already written.

Since this will not recreate the row, but technically reinsert the row with a higher timestamp, we cannot do this automatically on scrub, since it may overwrite an existing row inserted just one millisecond after the original row (while very hard to happen in practice, this may depend on application logic). For this reason, the user needs to specify the {{--reinsert-overflowed-ttl}} option during scrub to perform the recovery.

This approach is implemented [here|https://github.com/apache/cassandra/commit/5f9f704449ed1ef579c61953b02a9ef32f13082b] on 3.0 and ported to all other branches in a separate commit.
{quote}The last thing is about providing a route to fix up overflowed dates via scrub, I think we should definitely leave the remedial Scrubber code in trunk until we have a proper fix committed.
{quote}
Since 4.0 was not released, and it will come out with this fix it's impossible that someone will hit this on 4.0, but I don't see any problem in keeping the scrub option for the lazy ones upgrading from 3.x without fixing their SSTables beforehand. :) For this reason I kept the 3.11 SSTables in the 4.X (d)tests.

To prevent scrub and the tests from throwing an {{AssertionError}} on 2.1 I removed [this assertion|https://github.com/apache/cassandra/commit/4501eee5c962547c059a4f624155c5e18d5369d3], since it's no longer necessary given we apply the overflow policy.

Finally, I [refactored|https://github.com/apache/cassandra/commit/c3e1fcc25b3db9db212dfc805a47430ff537b101] the NEWS.txt section a bit to take [~KurtG] comments into account and also to give more emphasis to the expiration overflow policies since this will be what most users need to care about, and added a recovery subsection with detailed recovery instructions.

I now wonder if this notice has gotten to big and is cluttering the NEWS.txt file too much - what may confuse users looking for upgrade instructions - and we should maybe create a new WARNING/IMPORTANT file and add a pointer to it in the NEWS.txt file instead?

To facilitate review, I squashed the previous commits and created a new branch with new commits only representing changes since last review (except for the 2.2 patch, which is basically the same as 2.1 except the additional commit simplifying TTLTest). The previous CI looked good, I will submit a new CI run and post the results here later.
||2.1||2.2||3.0||3.11||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092-v6]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-14092-v6]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092-v6]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14092-v6]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-v6]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14092]|","09/Feb/18 12:15;beobal;{quote}Since 4.0 was not released, and it will come out with this fix it's impossible that someone will hit this on 4.0, but I don't see any problem in keeping the scrub option for the lazy ones upgrading from 3.x without fixing their SSTables beforehand.
{quote}
yeah, this was the scenario I was thinking about.
{quote}I now wonder if this notice has gotten to big and is cluttering the NEWS.txt file too much - what may confuse users looking for upgrade instructions - and we should maybe create a new WARNING/IMPORTANT file and add a pointer to it in the NEWS.txt file instead?
{quote}
I tend to agree with you. Even if adding another file is less than ideal, it's probably better than adding a large, unchanging header as that's likely just going to lead to more users not properly reading it and missing critical info about other future upgrades. So I'd be +1 on adding a new file ({{CASSANDRA\-14092.txt}} maybe?) and just a brief explanation of the severity of the issue plus a *very* clear pointer to more detailed file at the top of {{NEWS.txt}}. I think you should keep the
{code:java}
WARNING: MAXIMUM TTL EXPIRATION DATE NOTICE
--------------------------------------------
(General upgrading instructions are available in the next section)

The maximum expiration timestamp that can be represented by the storage engine is 2038-01-19T03:14:06+00:00,
which means that inserts with TTL that expire after this date are not currently supported.
{code}

and add the pointer after that.

1 nit on the text in {{NEWS.txt}}, you have a double ""the"" in the sentence 
 {{""As time progresses, the maximum supported TTL will be gradually reduced as the the maximum expiration date approaches.""}}

Extracting the news text to a new file notwithstanding, I'm +1 on the latest patches (pending CI, of course). Thanks for all the hard work on this [~pauloricardomg]","11/Feb/18 13:37;pauloricardomg;{quote}I tend to agree with you. Even if adding another file is less than ideal, it's probably better than adding a large, unchanging header as that's likely just going to lead to more users not properly reading it and missing critical info about other future upgrades. So I'd be +1 on adding a new file (CASSANDRA-14092.txt maybe?) and just a brief explanation of the severity of the issue plus a very clear pointer to more detailed file at the top of NEWS.txt.
{quote}
Sounds good. Settled on this (not-so) summarized note for NEWS.txt (the full text goes on CASSANDRA-14092.txt):
{noformat}
PLEASE READ: MAXIMUM TTL EXPIRATION DATE NOTICE (CASSANDRA-14092)
------------------------------------------------------------------
(General upgrading instructions are available in the next section)

The maximum expiration timestamp that can be represented by the storage engine is
2038-01-19T03:14:06+00:00, which means that inserts with TTL thatl expire after
this date are not currently supported. By default, INSERTS with TTL exceeding the
maximum supported date are rejected, but it's possible to choose a different
 expiration overflow policy. See CASSANDRA]-14092.txt for more details.

Prior to 3.0.16 (3.0.X) and 3.11.2 (3.11.x) there was no protection against INSERTS
with TTL expiring after the maximum supported date, causing the expiration time
field to overflow and the records to expire immediately. Clusters in the 2.X and
lower series are not subject to this when assertions are enabled. Backed up SSTables
can be potentially recovered and recovery instructions can be found on the
CASSANDRA-14092.txt file.

If you use or plan to use very large TTLS (10 to 20 years), read CASSANDRA-14092.txt
for more information.
{noformat}

{quote}Extracting the news text to a new file notwithstanding, I'm +1 on the latest patches (pending CI, of course).
{quote}
Still had to fix a couple of test nits and resubmit tests after rebase to get a clean run (attached internal CI screenshots, verified other failures and they look unrelated).

I've added the new CASSANDRA-14092.txt file to the debian/redhat package manifests [here|https://github.com/apache/cassandra/commit/a25898253ea3b5bc7262ff2d17f8466d489eaf96] (thanks [~mshuler] for the help).

Committed as {{b2949439ec62077128103540e42570238520f4ee}} to cassandra-2.1 and merged up to cassandra-2.2, cassandra-3.0, cassandra-3.11 and master (phew). cassandra-dtest commit merged as {{781c4ecdf67bb63f508e8af70599b45b896c28ae}}.

I've kept the previous branches with separate commits, and created a new branch with the squashed + rebased version:
||2.1||2.2||3.0||3.11||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.1...pauloricardomg:2.1-14092-final]|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...pauloricardomg:2.2-14092-final]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14092-final]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14092-final]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14092-final]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14092-rebased]|

I will open follow-up tickets to fix this limitation permanently and add the expiration overflow notice to the docs.

Thanks for all who helped with feedback and Sam for kindly reviewing this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range deletes in a CAS batch are ignored,CASSANDRA-13655,13084146,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jjirsa,jjirsa,jjirsa,03/Jul/17 06:40,12/Mar/19 14:20,13/Mar/19 22:34,11/Sep/17 16:39,3.0.15,3.11.1,4.0,,,Legacy/CQL,,,,,0,LWT,,,Range deletes in a CAS batch are ignored ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-06 15:04:39.851,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 11 16:39:06 UTC 2017,,,,,,0|i3h0g7:,9223372036854775807,3.0.14,,,,,,,slebresne,slebresne,,,,,,,,,,"05/Jul/17 22:17;jjirsa;|| Branch || Unit Tests || Dtests ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/113/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/114/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/115/] |

[~slebresne] any chance you're interested in reviewing? 
","06/Jul/17 15:04;slebresne;I can indeed have a look, but it won't be before next week, so if there is any other taker in the meantime, I won't get mad.",12/Jul/17 03:02;jay.zhuang;[~jjirsa] It looks like a serious bug. Can I review? (and someone else could double review it),"12/Jul/17 04:24;jjirsa;[~jay.zhuang] - more eyes are never a bad thing!
","12/Jul/17 05:47;jay.zhuang;Tried the patch locally, looks good to me, a few minor comments:
1. Would it be better to combine {{SliceUpdate}} and {{RowUpdate}}?
2. How about having a function for these 3 checks (like {{ModificationStatement.hasSlices()}} or a better name): [BatchStatement.java:420 | https://github.com/jeffjirsa/cassandra/commit/b9a6be6f5fc867718907d1abae124137d4f1cb45#diff-bee3b2111122530d9e0c5190e6773f62R420] and here: [ModificationStatement.java:629| https://github.com/jeffjirsa/cassandra/blob/cassandra-3.0-13655/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java#L629]","22/Jul/17 01:04;jjirsa;For #1, it should be an easy change, but I'll check to see if it somehow complicates things. I sort of like it the way it is, in case they eventually diverge, but perhaps that's planning for a future that may never happen.

I'm not thrilled about #2, to be honest. I'd prefer to leave them as distinct checks. 
","18/Aug/17 13:22;slebresne;Sorry for the delay getting to this. It's definitively an oversight that range deletions are ignored for CAS. The patch lgtm, with just the following minor remarks:
* I'd rename {{CQL3CasRequest.SliceUpdate}} (and related {{addSliceUpdate()}} method) to {{RangeDeletion}} or something of the like. {{SliceUpdate}} is unnecessarily imprecise for what it is (I personally wouldn't merge it to {{RowUpdate}} however; having some private {{AbstractUpdate}} to combine the 3 common fields is an option but I'm honestly not sure it's worth the trouble).
* I agree with [~jay.zhuang]'s point #2 above, about moving the checks to a {{ModificationStatement.hasSlices()}} method. This is, after all, the conditions that are necessary to check before calling {{ModificationStatement.createSlices()}} so it would make it more explicit and we won't forget to change one of the 2 places if things evolve.
* It doesn't seem {{ModificationStatement.toSlices}} needs to be made package protected, it can stay private.
* There is a misplaced bracket [here|https://github.com/jeffjirsa/cassandra/commit/c2e3941352c1da31ba28f657974918ae46e81c97#diff-bee3b2111122530d9e0c5190e6773f62R462].","29/Aug/17 20:33;jjirsa;Thank you both, sorry for the delay. Have pushed up a commit to each branch to address your collective comments. CircleCI is queued up, dtests are running. 

|| Branch || Unit Tests || Dtests ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/226/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/227/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13655] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13655] | [asf jenkins|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/228/] |
","30/Aug/17 07:01;slebresne;If CI's happy, +1 from me, thanks.","30/Aug/17 18:09;jjirsa;Unit tests are happy. DTests have the typical flakey tests + some failures that look environmental in 3.0 and 3.11, but trunk didnt run at all. The cqlsh copy dtests failed on both 3.0 and 3.11, and they both show a single failure in the last dozen runs, so that seems odd and I'll check that on the next run.

URLs will be:

Edit: rebuilding again ( no tests ran )
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/276/testReport/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/277/testReport/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/278/testReport/


","11/Sep/17 16:39;jjirsa;Thank you both for your review (I've included both [~jay.zhuang] and [~slebresne] in the commit line, appreciate 2 sets of eyes).

Committed to 3.0 as {{433f24cb04dbcf74029a918ee73155f78d5f8111}} and merged up.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using min_compress_ratio <= 1 causes corruption,CASSANDRA-13703,13088783,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,blambov,blambov,blambov,20/Jul/17 15:21,12/Mar/19 14:20,13/Mar/19 22:34,15/Sep/17 08:38,4.x,,,,,,,,,,0,,,,"This is because chunks written uncompressed end up below the compressed size threshold. Demonstrated by applying the attached patch meant to improve the testing of the 10520 changes, and running {{CompressedSequentialWriterTest.testLZ4Writer}}.

The default {{min_compress_ratio: 0}} is not affected as it never writes uncompressed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10520,,,,,,,,,,,,,,20/Jul/17 15:21;blambov;patch;https://issues.apache.org/jira/secure/attachment/12878193/patch,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-09-05 13:54:21.006,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 15 08:38:02 UTC 2017,,,,,,0|i3hsyf:,9223372036854775807,,,,,,,,dimitarndimitrov,dimitarndimitrov,,,,,,,,,,"21/Jul/17 07:53;blambov;Patch [here|https://github.com/blambov/cassandra/tree/13703].

Changes the max size threshold to be non-inclusive so that {{min_compress_ratio: 1}} can work (this would be a breaking change, but CASSANRDA-10520 is not part of any release yet), and adds an error for values between 0 and 1 exclusive.

Unit tests are clean, dtests have two failures that appear to be flaky on trunk:
{{repair_tests.incremental_repair_test.TestIncRepair.multiple_repair_test}}
{{bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test}}","05/Sep/17 13:54;dimitarndimitrov;+1 - from my still largely layman perspective, the change looks good.

A couple of small nits:
* {{CompressedChunkReader.maybeCheckCrc()}} (renamed in this patch to {{shouldCheckCrc()}}) may be removed or at least its visibility could be reduced. It seems to be used solely in CompressedChunkReader.java, lines 158 and 204 in this patch.
* The no-argument / single-argument factory methods for Snappy and LZ4 compressions in CompressionParams.java seem to differ in the values that they default to for min compression ratio and max compressed length.
** For Snappy, if nothing is specified, or only chunk length is specified, a default min compression ratio of 1.1 is used, and therefore max compressed length ends up somewhere roughly around 90% of chunk length.
** For LZ4, if nothing is specified, or only chunk length is specified, a default max compressed length of chunk length is used, and therefore min compression ratio ends up at 1.0 (I'm not sure if a precision error is possible there).

Edit: Of course, take this review with the appropriate rock-sized grain of salt.","06/Sep/17 16:52;blambov;Rebased and updated the branch, reducing the visibility of {{shouldCheckCrc}} and adding a comment and {{VisibleForTesting}} annotations to make it clear why the shorthand methods use inconsistent parameters.","15/Sep/17 08:38;blambov;Tests were fine, committed to trunk as [17a358c2cc2c583c3e0fa046ca8dee6d743ad1c5|https://github.com/apache/cassandra/commit/17a358c2cc2c583c3e0fa046ca8dee6d743ad1c5].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra does not start on Windows due to 'JNA link failure',CASSANDRA-13333,13056345,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,blerer,blerer,blerer,15/Mar/17 16:31,12/Mar/19 14:20,13/Mar/19 22:34,28/Mar/17 15:19,,,,,,,,,,,0,,,,"Cassandra 3.0 HEAD does not start on Windows. The only error in the logs is: 
{{ERROR 16:30:10 JNA failing to initialize properly.}} ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-15 21:30:35.144,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 15:19:13 UTC 2017,,,,,,0|i3cba7:,9223372036854775807,,,,,,,,mkjellman,mkjellman,,,,,,,,,,"15/Mar/17 21:30;jasobrown;caused by CASSANDRA-13233. [~mkjellman], you don't happen have a Windows machine to investigate this, perchance? (pretty sure you do not)","15/Mar/17 22:43;blerer;[~jasobrown], [~mkjellman] if you want I can investigate it. It seems that I am one of the only Windows dev around here.  ","15/Mar/17 22:50;jasobrown;[~blerer] Sorry to give you more work, but the help is much appreciated!","16/Mar/17 20:59;mkjellman;[~blerer] I don't have a Windows machine to test this unfortunately... but, I do know what's going on, although I'm not sure about the correct fix.

The following code from {{StartupChecks}}:

{code}
public static final StartupCheck checkJnaInitialization = new StartupCheck()
    {
        public void execute() throws StartupException
        {
            // Fail-fast if JNA is not available or failing to initialize properly
            if (!CLibrary.jnaAvailable())
                throw new StartupException(StartupException.ERR_WRONG_MACHINE_STATE, ""JNA failing to initialize properly. "");
        }
    };
{code}

This is due to the fact that when I implemented the Windows {{CLibraryWrapper}} implementation {{CLibraryWindows}}, I had it return false for jnaAvailable(). Given that we only use JNA right now to access libc calls I wasn't aware that any of those would be implemented in JNA to do something in Windows.

If that's not correct, I'll throw a very quick patch together to load JNA in the Windows implementation, but I'll still need some help to know what wrapped methods actually work on Windows.

Should the startup check just be exempted for Windows?","17/Mar/17 09:26;blerer;I went to look into how the things were working before the CASSANDRA-13233 changes.
The code does a difference between JNA not being there and the fact that it could not link a library. In the first case {{jnaAvailable()}} must return {{false}} but in the second case it should return {{true}} (and an error message should have been logged). The new implementation always return {{false}} even if the library is actually there.
So, to keep the existing behaviour, we would need to check if JNA is here and throw the appropriate errors. The easier way would probably to also have the following code in {{CLibraryWindows}} but it feels a bit weird:
{code}
        try
        {
            Native.register(""c"");
        }
        catch (NoClassDefFoundError e)
        {
            logger.warn(""JNA not found. Native methods will be disabled."");
            jnaAvailable = false;
        }
        catch (UnsatisfiedLinkError e)
        {
            logger.warn(""JNA link failure, one or more native method will be unavailable."");
            logger.trace(""JNA link failure details: {}"", e.getMessage());
        }
        catch (NoSuchMethodError e)
        {
            logger.warn(""Obsolete version of JNA present; unable to register C library. Upgrade to JNA 3.2.7 or later"");
            jnaAvailable = false;
        }
{code} 

Any suggestions?","17/Mar/17 13:21;blerer;I did a bit of digging. JNA is actually used on {{Windows}} by {{org.apache.cassandra.utils.WindowsTimer}}. So it makes sense to block startup if JNA is not there. 
 ",17/Mar/17 13:33;JoshuaMcKenzie;It's not worth disabling startup due to an inability to access winmm.dll. I used that to access the multimedia timer to change the kernel's timer coalescing which is strictly a performance improvement and shouldn't block startup of a node.,"17/Mar/17 14:00;blerer;Then does it really make sense to keep the {{jnaAvailable()}} method? Right now, that check does not really bring much because if the library cannot be linked the server will start anyway.
Is a warning in the log not enough? I really wonder if anybody is really using the NativeMBean to check that jna and mlock are available.
[~jasobrown], [~mkjellman] what is your opinion on that?",17/Mar/17 14:17;blerer;[~dbrosius] you are the one that exposed the information through JMX. Is it something that you really need?,"17/Mar/17 14:24;blerer;Sorry, I missed the comment pointing at CASSANDRA-5508.

","17/Mar/17 14:38;blerer;Another approach would be to convert  {{jnaAvailable()}} into {{isAvailable()}}. This method will return {{true}} if the library has been successfully linked and {{false}} otherwise. The startup check will then check, if the operating system is not Windows, that the library has been successfully linked and prevent the system to start if it has not.",17/Mar/17 14:49;jeromatron;I think it's reasonable to not hold up startup if we can still issue a warning and still keep the state whether it was able to use JNA to lock memory.,"17/Mar/17 16:46;dbrosius;[~blerer] i added it for someone else, so no i don't personally need it","17/Mar/17 17:19;blerer;I have pushed an initial version of the patch [here|https://github.com/apache/cassandra/compare/trunk...blerer:13333-3.0].
The patch replace the {{jnaAvailable()}} method by the {{isAvailable()}} method which will return {{true}} only if the library has been sucessfully linked.
In the case of Windows {{isAvailable()}} will always return {{false}}.
The patch remove the startup check so the server will start even if JNA is not found (in which case a warning would have already been logged).
The {{NativeAccessMBean::isAvailable}} method will now only return {{true}} if the native library has been successfully linked.  

If nobody disagree with the approach, I will push it on CI next week. ","21/Mar/17 16:52;blerer;||[3.0|https://github.com/apache/cassandra/compare/trunk...blerer:13333-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.0-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.0-dtest/]|
||[3.11|https://github.com/apache/cassandra/compare/trunk...blerer:13333-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.11-dtest/]|
||[trunk|https://github.com/apache/cassandra/compare/trunk...blerer:13333-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-trunk-dtest/]|

[~jasobrown], [~mkjellman] could one of you review the patches. Only 3.0 and 3.11 differ a bit. ","21/Mar/17 21:13;mkjellman;[~blerer]

* Just chatted quickly with [~jasobrown] and [~jjirsa] and I think hiding the fact we're using JNA under the hood and going with {{isAvailable()}} vs. {{jnaAvailable()}] is a good change, however, I think that changes the scope a bit. In the case of {{CLibraryWindows}} I used the existing Sigar based logic to get the current PID of the running JVM as I wasn't sure if the *nix APIs would work in Windows land. Thinking about it, this means {{isAvailable()}} is a bit more nuanced. For instance, in the current {{CLibraryWindows}} case, yes, the {{callGetpid()}} method is ""available"" but all the other methods that we happen to have right now use JNA, which isn't ""available"" in the Windows case.
* Do you happen to know if the native {{getpid()}} JNA will work on Windows? If so we can switch that from Sigar -> JNA for Windows too just like we're already doing for the Linux/Darwin case. If so, it makes sense to attempt to load JNA and libc in the WIndows case too?
* I don't think we should remove the checkJnaInitialization {{StartupCheck}} at minimum in the Linux and Darwin cases. Given that we ship JNA in the release, either we can't link against the library it due to an issue or someone removed it. Given the performance implications I think we should leave the hard fail in place -- but skip it if the current OS type is  Windows.","22/Mar/17 20:36;blerer;I force pushed a new patch.
The new patch use the {{Kernel32}} library to support natively the {{callGetPid}} method and keep the startup check. As the Windows library is not the {{c}} one, the patch also rename {{CLibrary}} to {{NativeLibrary}} as the name was misleading.  

||[3.0|https://github.com/apache/cassandra/compare/trunk...blerer:13333-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.0-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.0-dtest/]|
||[3.11|https://github.com/apache/cassandra/compare/trunk...blerer:13333-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.11-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-3.11-dtest/]|
||[trunk|https://github.com/apache/cassandra/compare/trunk...blerer:13333-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-trunk-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13333-trunk-dtest/]|

","22/Mar/17 21:07;mkjellman;[~blerer] this looks great! Renaming {{CLibrary}} --> {{NativeLibrary}} helps make the intent much clearer. 

# Should the loading of {{Native.register(""winmm"")}} in {{WindowsTimer}} also be moved into NativeLibraryWindows?
# Looks like the trunk patch didn't get pushed up or potentially just a copy paste error? Currently it's just pointing at blerer/trunk.
# Thanks for putting the MSDN API URL in the method javadoc. :)
# In {{NativeLibraryWindows}} I think the following logger statements could be simplified:

{code}
catch (UnsatisfiedLinkError e)
{
    logger.warn(""JNA link failure, one or more native method will be unavailable."");
    logger.error(""JNA link failure details: {}"", e.getMessage());
}
{code}

Can be simplified to:
{code}
logger.error(""Failed to link against JNA. Native methods will be unavailable."", e);
{code}","23/Mar/17 09:25;blerer;[~mkjellman] Thanks for the reviews.

bq. 1. Should the loading of {{Native.register(""winmm"")}} in {{WindowsTimer}} also be moved into NativeLibraryWindows?
{{WindowsTimer}} is really specific to Windows and according to [~JoshuaMcKenzie] [comment|https://issues.apache.org/jira/browse/CASSANDRA-13333?focusedCommentId=15929978&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15929978] we should not prevent startup due to an inability to access the {{winmm.dll}} library. So, I would be in favor of keeping it separeted for now.

bq. 2. Looks like the trunk patch didn't get pushed up or potentially just a copy paste error? Currently it's just pointing at blerer/trunk.
Sorry for that. It is a copy paste mistake. I fixed it.

bq. 3.  Thanks for putting the MSDN API URL in the method javadoc.
I am pretty sure that otherwise, I will have to end up googling it in a month or two ;-) 

bq. 4. In NativeLibraryWindows I think the following logger statements could be simplified:
I have pushed a new commit to fix it in all the branches. ","25/Mar/17 08:23;ChandraM;Hey , 

Is this change committed just wondering when this change will be available in the remote. 

I tried a few moments back after i took the latest changes , and it fails to start - ERROR [main] 2017-03-25 13:35:21,001 CassandraDaemon.java:663 - JNA failing to initialize properly.

Thanks in adv. ",25/Mar/17 13:18;blerer;The ticket status is still {{Patch Available}} which means that the patch has not been accepted yet.,27/Mar/17 21:52;mkjellman;+1 [~blerer] LGTM!,"28/Mar/17 15:19;blerer;Thanks for the review.
Committed into 3.0 at 9b8692c6a4c75b7df29a58b5d3d1d1ee5cb0c3a4 and merged into 3.11 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra 3.11.1 Repair Causes Out of Memory,CASSANDRA-14096,13122803,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,molsson,serhatd,serhatd,05/Dec/17 08:46,12/Mar/19 14:07,13/Mar/19 22:34,14/Feb/19 21:29,3.0.x,3.11.x,,,,Consistency/Repair,,,,,0,,,,"Number of nodes: 9
System resources: 8 Core, 16GB RAM
Replication factor: 3
Number of vnodes: 256

We get out of memory errors while repairing (incremental or full) our keyspace. I had tried to increase node's memory from 16GB to 32GB but result did not change. Repairing tables one by one in our keyspace was not completed successfully for all tables too. 

Only subrange repair with cassandra-reaper worked for me.

Here is the output of heap utils before oom:

{code}

ERROR [MessagingService-Incoming-/192.168.199.121] 2017-12-05 11:38:08,121 JVMStabilityInspector.java:142 - JVM state determined to be unstable.  Exiting forcefully due to:
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.gms.GossipDigestSerializationHelper.deserialize(GossipDigestSyn.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.gms.GossipDigestSynSerializer.deserialize(GossipDigestSyn.java:95) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.gms.GossipDigestSynSerializer.deserialize(GossipDigestSyn.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:123) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:192) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:180) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:94) ~[apache-cassandra-3.11.1.jar:3.11.1]
{code}

{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:      31105265     1493052720  org.apache.cassandra.utils.MerkleTree$Inner
   2:      31134570      996306240  org.apache.cassandra.utils.MerkleTree$Leaf
   3:      31195121      748682904  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   4:      22885384      667447608  [B
   5:        214550       18357360  [C
   6:        364637       17502576  java.nio.HeapByteBuffer
   7:         46525        9566496  [J
   8:        111024        5306976  [Ljava.lang.Object;
   9:        132674        5306960  org.apache.cassandra.db.rows.BufferCell
  10:        210309        5047416  java.lang.String
  11:         59984        3838976  org.apache.cassandra.utils.btree.BTreeSearchIterator
  12:        101181        3237792  java.util.HashMap$Node
  13:         27158        2719216  [I
  14:         60181        2407240  java.util.TreeMap$Entry
  15:         65998        2111936  org.apache.cassandra.db.rows.BTreeRow
  16:         62387        2023784  [Ljava.nio.ByteBuffer;
  17:         19086        1750464  [Ljava.util.HashMap$Node;
  18:         63466        1523184  javax.management.ObjectName$Property
  19:         61553        1477272  org.apache.cassandra.db.BufferClustering
  20:         29274        1405152  org.apache.cassandra.utils.MerkleTree
  21:         34602        1384080  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$100/78247817
  22:         40972        1311104  java.util.concurrent.ConcurrentHashMap$Node
  23:         39172        1253504  java.util.RandomAccessSubList
  24:         51657        1239768  org.apache.cassandra.db.LivenessInfo
  25:         19013        1216832  java.nio.DirectByteBuffer
  26:         28178        1127120  org.apache.cassandra.db.PreHashedDecoratedKey
  27:         32407        1033120  [Ljavax.management.ObjectName$Property;
  28:         42090        1010160  java.util.EnumMap$EntryIterator$Entry
  29:         40878         981072  java.util.Arrays$ArrayList
  30:         19721         946608  java.util.HashMap
  31:          8359         932600  java.lang.Class
  32:         37277         894648  org.apache.cassandra.dht.Range
  33:         26897         860704  org.apache.cassandra.db.rows.EncodingStats
  34:         19958         798320  org.apache.cassandra.utils.MergeIterator$Candidate
  35:         31281         750744  java.util.ArrayList
  36:         23291         745312  org.apache.cassandra.utils.MerkleTree$TreeRange
  37:         21650         692800  java.util.AbstractList$ListItr
  38:         27675         664200  java.lang.Long
  39:         16204         648160  javax.management.ObjectName
  40:         36873         589968  org.apache.cassandra.utils.WrappedInt
  41:          4100         557600  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference
  42:         21651         519624  java.util.SubList$1
  43:         12275         491000  java.math.BigInteger
  44:          8657         484792  org.apache.cassandra.utils.memory.BufferPool$Chunk
  45:         14732         471424  java.util.ArrayList$Itr
  46:          5371         429680  java.lang.reflect.Constructor
  47:         12640         404480  com.codahale.metrics.LongAdder
  48:         16156         387744  com.sun.jmx.mbeanserver.NamedObject
  49:         16133         387192  com.sun.jmx.mbeanserver.StandardMBeanSupport
  50:          9536         381440  org.apache.cassandra.db.EmptyIterators$EmptyUnfilteredRowIterator
  51:          6035         337960  org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator
  52:          6031         337736  org.apache.cassandra.db.transform.UnfilteredRows
  53:          8298         331920  org.apache.cassandra.db.rows.BTreeRow$Builder
  54:          5182         331648  sun.security.provider.SHA2$SHA256
  55:         10356         331392  org.apache.cassandra.utils.btree.BTree$$Lambda$192/259279152
  56:          8145         325800  org.apache.cassandra.db.rows.SerializationHelper
  57:          8144         325760  org.apache.cassandra.io.sstable.SSTableIdentityIterator
  58:          8144         325760  org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator
  59:           176         319536  [Ljava.util.concurrent.ConcurrentHashMap$Node;
  60:          9716         310912  java.net.InetAddress$InetAddressHolder
  61:          7770         310800  com.github.benmanes.caffeine.cache.NodeFactory$SStMW
  62:         18470         295520  org.apache.cassandra.db.rows.CellPath$SingleItemCellPath
  63:          2505         276784  [S
  64:          5646         271008  com.codahale.metrics.EWMA
  65:         11258         270192  java.util.concurrent.ConcurrentLinkedDeque$Node
  66:          8248         263936  org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator$1
  67:         10618         254832  java.lang.Double
  68:          7921         253472  org.apache.cassandra.cache.ChunkCache$Buffer
  69:          7773         248736  org.apache.cassandra.cache.ChunkCache$Key
  70:         10296         247104  org.apache.cassandra.dht.Token$KeyBound
  71:          6096         243816  [Lorg.apache.cassandra.db.transform.Transformation;
  72:          6035         241400  org.apache.cassandra.db.rows.Row$Merger
  73:          6034         241360  org.apache.cassandra.db.rows.RangeTombstoneMarker$Merger
  74:          6034         241360  org.apache.cassandra.db.rows.Row$Merger$ColumnDataReducer
  75:          9969         239256  org.apache.cassandra.db.RowIndexEntry
  76:          9699         232776  java.net.Inet4Address
  77:          5750         230000  org.apache.cassandra.utils.concurrent.Ref$State
  78:         13690         219040  java.util.concurrent.atomic.AtomicInteger
  79:          9091         218184  org.apache.cassandra.gms.GossipDigest
  80:         12392         216040  [Ljava.lang.Class;
  81:          5289         211560  org.apache.cassandra.utils.MergeIterator$ManyToOne
  82:         13079         209264  java.lang.Object
  83:          5183         207320  org.apache.cassandra.repair.Validator$CountingDigest
  84:          8157         195768  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxGauge
  85:          6035         193120  org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator$MergeReducer
  86:          6023         192736  org.apache.cassandra.db.LivenessInfo$ExpiringLivenessInfo
  87:          5745         183840  com.google.common.collect.RegularImmutableList
  88:          6035         180640  [Lorg.apache.cassandra.db.rows.Row;
  89:          6034         180600  [Lorg.apache.cassandra.db.rows.RangeTombstoneMarker;
  90:          6033         180576  [Lorg.apache.cassandra.db.DeletionTime;
  91:          7464         179136  org.apache.cassandra.db.rows.BTreeRow$$Lambda$109/2102075500
  92:          5288         171488  [Lorg.apache.cassandra.utils.MergeIterator$Candidate;
  93:          5331         170592  com.google.common.collect.Iterators$11
  94:          5183         165856  java.security.MessageDigest$Delegate
  95:          5178         165696  com.google.common.collect.Iterators$7
  96:          5157         165024  org.apache.cassandra.utils.MerkleTree$RowHash
  97:           169         163280  [Lio.netty.util.Recycler$DefaultHandle;
  98:          2304         147456  io.netty.buffer.PoolSubpage
  99:          4608         147456  java.util.EnumMap$EntryIterator
 100:          6034         144816  org.apache.cassandra.db.rows.Row$Merger$CellReducer
 101:          1595         140360  java.lang.reflect.Method
 102:          2893         138864  java.util.TreeMap
 103:          5750         138000  org.apache.cassandra.utils.concurrent.Ref
 104:          8453         135248  org.apache.cassandra.db.rows.BTreeRow$Builder$CellResolver
 105:          5613         134712  java.util.concurrent.atomic.AtomicLong
 106:          5509         132216  org.apache.cassandra.utils.btree.BTree$FiltrationTracker
 107:          5179         124296  com.google.common.collect.Iterables$6
 108:          5179         124296  com.google.common.collect.Iterables$8
 109:          5179         124296  com.google.common.collect.Iterators$5
 110:          5179         124296  com.google.common.collect.Iterators$8
 111:          5177         124248  com.google.common.collect.Iterables$2
 112:          5159         123816  sun.security.jca.GetInstance$Instance
 113:          2577         123696  java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync
 114:          2399         115152  org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoir
 115:          4643         111432  org.apache.cassandra.db.DeletionTime
 116:          4490         107760  org.apache.cassandra.db.Columns
 117:          2673         106920  java.util.EnumMap
 118:          4202         100848  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxCounter
 119:          6095          97520  org.apache.cassandra.db.transform.BaseIterator$Stop
 120:          4041          96984  java.util.concurrent.ConcurrentLinkedDeque
 121:          4033          96792  org.apache.cassandra.utils.concurrent.Ref$GlobalState
 122:          1882          90336  com.codahale.metrics.Meter
 123:          5596          89536  java.util.concurrent.atomic.AtomicLongArray
 124:          1845          88560  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxTimer
 125:          5179          82864  com.google.common.collect.Iterables$3
 126:          2050          82000  org.apache.cassandra.utils.btree.BTree$Builder
 127:          1111          71104  java.nio.DirectByteBufferR
 128:          1713          68520  java.util.LinkedHashMap$Entry
 129:          2115          67680  io.netty.util.Recycler$DefaultHandle
 130:          1687          67480  java.lang.ref.SoftReference
 131:          1519          66968  [Ljava.lang.String;
 132:          2724          65376  org.apache.cassandra.db.PartitionColumns
 133:          1598          63920  org.apache.cassandra.io.util.MmappedRegions$State
 134:          2572          61728  java.util.concurrent.locks.ReentrantReadWriteLock
 135:          3736          59776  java.util.concurrent.atomic.AtomicBoolean
 136:           154          59136  io.netty.util.concurrent.FastThreadLocalThread
 137:          1835          58720  org.apache.cassandra.utils.MergeIterator$TrivialOneToOne
 138:          1794          57408  org.apache.cassandra.gms.EndpointState
 139:           896          57344  org.apache.cassandra.config.ColumnDefinition
 140:          1385          55400  sun.misc.Cleaner
 141:          2302          55248  org.apache.cassandra.db.commitlog.CommitLogPosition
 142:          1713          54816  java.io.FileDescriptor
 143:           802          51328  sun.nio.ch.FileChannelImpl
 144:          2137          51288  org.apache.cassandra.db.rows.Row$Deletion
 145:           400          51200  org.apache.cassandra.io.sstable.format.big.BigTableReader
 146:          1584          50688  java.lang.StackTraceElement
 147:          1583          50656  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node
 148:          1583          50656  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
 149:          1579          50528  java.lang.ref.WeakReference
 150:          1563          50016  org.apache.cassandra.io.util.Memory
 151:          1559          49888  java.util.concurrent.locks.ReentrantLock$NonfairSync
 152:            60          48760  [D
 153:           867          48552  java.lang.invoke.MemberName
 154:          1176          47040  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$LocalSessionWrapper
 155:          1176          47040  org.apache.cassandra.net.MessageIn
 156:          1938          46512  org.apache.cassandra.db.rows.ComplexColumnData
 157:          1157          46280  com.google.common.util.concurrent.AbstractFuture$Sync
 158:          1893          45432  java.util.concurrent.Executors$RunnableAdapter
 159:           400          44800  org.apache.cassandra.io.sstable.metadata.StatsMetadata
 160:           605          43560  java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask
 161:          2713          43408  com.codahale.metrics.Counter
 162:          1794          43056  org.apache.cassandra.gms.HeartBeatState
 163:          1033          41320  org.apache.cassandra.db.rows.BTreeRow$Builder$ComplexColumnDeletion
 164:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock
 165:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$Sync$ThreadLocalHoldCounter
 166:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock
 167:           616          39424  com.google.common.collect.MapMakerInternalMap$Segment
 168:          1611          38664  com.codahale.metrics.Histogram
 169:          1611          38664  com.codahale.metrics.Timer
 170:          2410          38560  java.util.concurrent.atomic.AtomicReference
 171:           601          38464  java.util.concurrent.ConcurrentHashMap
 172:          1601          38424  org.apache.cassandra.io.util.ChannelProxy
 173:          1587          38088  org.apache.cassandra.cache.KeyCacheKey
 174:          1583          37992  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
 175:           945          37800  org.apache.cassandra.metrics.LatencyMetrics
 176:          1557          37368  org.apache.cassandra.gms.VersionedValue
 177:          1157          37024  java.lang.ThreadLocal$ThreadLocalMap$Entry
 178:          1540          36960  java.util.concurrent.LinkedBlockingQueue$Node
 179:          1525          36600  org.apache.cassandra.repair.NodePair
 180:           151          36240  org.apache.cassandra.metrics.TableMetrics
 181:          1490          35760  java.util.concurrent.ConcurrentLinkedQueue$Node
 182:          2213          35408  java.util.TreeMap$KeySet
 183:           868          34720  java.util.HashMap$ValueIterator
 184:           863          34520  java.lang.invoke.MethodType
 185:           710          34080  org.apache.cassandra.metrics.RestorableMeter$RestorableEWMA
 186:           418          33696  [Ljava.lang.ThreadLocal$ThreadLocalMap$Entry;
 187:           809          32360  sun.nio.ch.FileChannelImpl$Unmapper
 188:          1344          32256  com.google.common.util.concurrent.ExecutionList
 189:          1342          32208  org.apache.cassandra.utils.Pair
 190:          2012          32192  java.lang.Integer
 191:           800          32000  org.apache.cassandra.io.util.FileHandle
 192:          1333          31992  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxHistogram
 193:          1324          31776  [Lorg.apache.cassandra.dht.Range;
 194:           948          30336  org.apache.cassandra.db.partitions.AbstractBTreePartition$Holder
 195:          1223          29352  java.lang.StringBuilder
 196:           898          28736  sun.security.util.DerInputBuffer
 197:           898          28736  sun.security.util.DerValue
 198:          1196          28704  javax.management.openmbean.CompositeDataSupport
 199:          1176          28224  org.apache.cassandra.concurrent.ExecutorLocals
 200:          1176          28224  org.apache.cassandra.net.MessageDeliveryTask
 201:           866          27712  java.lang.invoke.MethodType$ConcurrentWeakInternSet$WeakEntry
 202:          1143          27432  org.apache.cassandra.repair.SyncStat
 203:           685          27400  org.apache.cassandra.io.sstable.IndexInfo
 204:          1109          26616  org.apache.cassandra.utils.Interval
 205:           828          26496  org.apache.cassandra.utils.MergeIterator$OneToOne
 206:           816          26112  java.lang.ref.ReferenceQueue
 207:           800          25600  org.apache.cassandra.io.util.FileHandle$Cleanup
 208:           982          23568  java.util.Collections$UnmodifiableRandomAccessList
 209:           716          22912  org.apache.cassandra.db.context.CounterContext$ContextState
 210:           941          22584  org.apache.cassandra.utils.MerkleTrees
 211:           400          22400  org.apache.cassandra.io.compress.CompressionMetadata
 212:           400          22400  org.apache.cassandra.io.sstable.IndexSummary
 213:           400          22400  org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier
 214:           553          22120  org.apache.cassandra.db.SerializationHeader
 215:           389          21784  sun.nio.cs.UTF_8$Encoder
 216:           160          21760  io.netty.util.internal.InternalThreadLocalMap
 217:           898          21552  sun.security.util.DerInputStream
 218:           445          21360  org.apache.cassandra.repair.RepairJob
 219:           885          21240  [Lsun.security.x509.AVA;
 220:           885          21240  sun.security.x509.AVA
 221:           885          21240  sun.security.x509.RDN
 222:           878          21072  org.apache.cassandra.repair.TreeResponse
 223:           855          20520  java.util.concurrent.ConcurrentSkipListMap$Node
 224:           628          20096  java.util.Hashtable$Entry
 225:           349          20024  [Z
 226:           621          19872  java.io.File
 227:          1233          19728  java.util.TreeMap$Values
 228:          1212          19392  java.util.Optional
 229:           404          19392  org.apache.cassandra.io.sstable.Descriptor
 230:           604          19328  [Lcom.codahale.metrics.Histogram;
 231:           802          19248  sun.nio.ch.NativeThreadSet
 232:           801          19224  org.apache.cassandra.io.util.MmappedRegions
 233:           399          19152  org.apache.cassandra.io.sstable.format.big.BigFormat$BigVersion
 234:           798          19152  org.apache.cassandra.io.util.ChannelProxy$Cleanup
 235:           798          19152  org.apache.cassandra.utils.EstimatedHistogram
 236:           788          18912  org.apache.cassandra.metrics.ClearableHistogram
 237:           766          18384  com.google.common.collect.SingletonImmutableList
 238:           762          18288  org.apache.cassandra.gms.GossipDigestSyn
 239:           569          18208  java.nio.DirectByteBuffer$Deallocator
 240:           569          18208  org.apache.cassandra.db.filter.ColumnFilter
 241:           300          18000  [Ljava.lang.ref.SoftReference;
 242:           160          17920  org.apache.cassandra.config.CFMetaData
 243:           744          17856  java.util.concurrent.CopyOnWriteArrayList
 244:           442          17680  java.util.HashMap$EntryIterator
 245:           221          17680  org.apache.cassandra.io.sstable.format.big.BigTableScanner
 246:           225          17464  [Ljava.lang.StackTraceElement;
 247:          1084          17344  java.util.EnumMap$EntrySet
 248:           424          16960  org.apache.cassandra.utils.btree.NodeCursor
 249:            32          16896  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference;
 250:           300          16800  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
 251:             1          16400  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node;
 252:           512          16384  org.apache.cassandra.repair.RepairJobDesc
 253:           154          16016  com.google.common.collect.MapMakerInternalMap
 254:           500          16000  java.lang.invoke.DirectMethodHandle
 255:           400          16000  org.apache.cassandra.io.sstable.BloomFilterTracker
 256:           998          15968  org.antlr.runtime.BitSet
 257:           664          15936  com.google.common.collect.ImmutableMapEntry$TerminalEntry
 258:           398          15920  java.util.WeakHashMap$Entry
 259:           392          15680  java.lang.ref.Finalizer
 260:           325          15600  java.util.concurrent.ConcurrentSkipListMap
 261:           487          15584  org.apache.cassandra.schema.CompressionParams
 262:           485          15520  sun.security.util.ObjectIdentifier
 263:           483          15456  org.apache.cassandra.db.partitions.AtomicBTreePartition
 264:           161          15456  org.apache.cassandra.schema.TableParams
 265:           170          15440  [Ljava.util.WeakHashMap$Entry;
 266:           384          15360  io.netty.buffer.PoolChunkList
 267:           382          15280  org.apache.cassandra.repair.RemoteSyncTask
 268:           941          15056  org.apache.cassandra.utils.MerkleTrees$TokenRangeComparator
 269:           622          14928  java.util.Collections$1
 270:           622          14928  org.apache.cassandra.db.RowIndexEntry$Serializer
 271:           930          14880  java.util.concurrent.locks.ReentrantLock
 272:           464          14848  org.apache.cassandra.cql3.ColumnIdentifier
 273:           925          14800  java.util.HashSet
 274:           264          14784  java.util.LinkedHashMap
 275:           151          14496  org.apache.cassandra.db.ColumnFamilyStore
 276:           604          14496  org.apache.cassandra.metrics.TableMetrics$TableHistogram
 277:           301          14448  ch.qos.logback.classic.Logger
 278:           355          14200  org.apache.cassandra.metrics.RestorableMeter
 279:           442          14144  org.apache.cassandra.io.util.RandomAccessReader
 280:           430          14056  [Lcom.google.common.collect.ImmutableMapEntry;
 281:           433          13856  com.google.common.collect.MapMakerInternalMap$StrongEntry
 282:           433          13856  com.google.common.collect.MapMakerInternalMap$WeakValueReference
 283:           855          13680  java.nio.channels.spi.AbstractInterruptibleChannel$1
 284:            34          13600  org.apache.cassandra.net.IncomingTcpConnection
 285:           333          13320  com.google.common.collect.RegularImmutableSortedMap
 286:           818          13088  java.lang.ref.ReferenceQueue$Lock
 287:           201          12864  java.net.URL
 288:           803          12848  sun.nio.ch.FileDispatcherImpl
 289:           401          12832  org.apache.cassandra.utils.BloomFilter
 290:           200          12800  java.util.regex.Matcher
 291:           400          12800  org.apache.cassandra.cache.ChunkCache$CachingRebufferer
 292:           400          12800  org.apache.cassandra.io.util.CompressedChunkReader$Mmap
 293:           400          12800  org.apache.cassandra.io.util.MmapRebufferer
 294:           799          12784  org.apache.cassandra.io.util.MmappedRegions$Tidier
 295:           799          12784  org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy
 296:           399          12768  org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy
 297:           797          12752  java.util.Collections$SingletonSet
 298:           396          12672  java.util.UUID
 299:           784          12544  java.util.HashMap$KeySet
 300:           521          12504  java.util.concurrent.ConcurrentLinkedQueue
 301:           154          12320  org.apache.cassandra.db.rows.RowAndDeletionMergeIterator
 302:           170          12240  java.lang.reflect.Field
 303:           507          12168  org.apache.cassandra.db.BufferDecoratedKey
 304:           151          12080  org.apache.cassandra.db.Memtable
 305:           302          12080  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategyOptions
 306:           376          12032  java.lang.invoke.LambdaForm$Name
 307:           213          11928  sun.security.ssl.CipherSuite
 308:            27          11880  org.apache.cassandra.net.OutboundTcpConnection
 309:           738          11808  java.util.HashMap$Values
 310:           208          11648  java.lang.Package
 311:           242          11616  org.apache.cassandra.utils.IntervalTree$IntervalNode
 312:           128          11264  [Lio.netty.buffer.PoolSubpage;
 313:           699          11184  java.util.HashMap$EntrySet
 314:           155          11160  org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater
 315:           344          11008  java.util.concurrent.ConcurrentSkipListMap$HeadIndex
 316:           341          10912  sun.misc.FDBigInteger
 317:           227          10896  sun.security.x509.X500Name
 318:           453          10872  org.apache.cassandra.utils.DefaultValue
 319:           333          10656  com.google.common.collect.RegularImmutableSortedSet
 320:           265          10600  java.util.Formatter$FormatSpecifier
 321:           263          10520  [Ljava.util.Formatter$Flags;
 322:           433          10392  org.apache.cassandra.cql3.ColumnIdentifier$InternedKey
 323:            72          10368  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicLong
 324:           324          10368  sun.security.x509.AlgorithmId
 325:           320          10240  io.netty.util.internal.chmv8.LongAdderV8
 326:           633          10128  java.util.concurrent.atomic.AtomicReferenceArray
 327:           180          10080  java.lang.invoke.MethodTypeForm
 328:           156           9984  io.netty.util.Recycler$Stack
 329:           416           9984  java.lang.ThreadLocal$ThreadLocalMap
 330:           622           9952  org.apache.cassandra.dht.Range$1
 331:           154           9856  org.apache.cassandra.cql3.UpdateParameters
 332:           244           9760  java.util.HashMap$KeyIterator
 333:           304           9728  java.util.concurrent.locks.AbstractQueuedSynchronizer$Node
 334:           302           9664  org.apache.cassandra.metrics.TableMetrics$TableMetricNameFactory
 335:           302           9664  org.apache.cassandra.utils.memory.MemtableAllocator$SubAllocator
 336:           400           9600  [Lorg.apache.cassandra.io.util.Memory;
 337:           400           9600  org.apache.cassandra.utils.StreamingHistogram
 338:           399           9576  [Ljava.lang.AutoCloseable;
 339:            25           9400  java.lang.Thread
 340:           195           9360  org.apache.cassandra.net.MessageOut
 341:           292           9344  [Lcom.codahale.metrics.Timer;
 342:            16           9216  io.netty.util.internal.shaded.org.jctools.queues.MpscChunkedArrayQueue
 343:           381           9144  org.apache.cassandra.repair.RepairResult
 344:           362           8688  com.google.common.util.concurrent.ExecutionList$RunnableExecutorPair
 345:            68           8680  [Ljava.util.Hashtable$Entry;
 346:           271           8672  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxMeter
 347:           108           8640  sun.security.x509.X509CertImpl
 348:           269           8608  javax.management.MBeanAttributeInfo
 349:           215           8600  com.google.common.collect.RegularImmutableMap
 350:           215           8600  org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator
 351:           151           8456  org.apache.cassandra.db.compaction.CompactionStrategyManager
 352:           260           8320  javax.management.MBeanParameterInfo
 353:           142           7952  java.beans.MethodDescriptor
 354:           331           7944  java.util.Collections$SingletonList
 355:           494           7904  com.google.common.base.Present
 356:           164           7872  java.util.WeakHashMap
 357:           227           7768  [Lsun.security.x509.RDN;
 358:           483           7728  org.apache.cassandra.utils.CounterId
 359:           318           7632  java.util.Collections$SetFromMap
 360:           318           7632  java.util.Formatter$FixedString
 361:           156           7488  org.apache.cassandra.utils.concurrent.OpOrder$Group
 362:           187           7480  com.google.common.util.concurrent.ListenableFutureTask
 363:           308           7392  org.apache.cassandra.utils.btree.BTreeSet
 364:           306           7344  java.beans.MethodRef
 365:           304           7296  org.apache.cassandra.io.util.MmappedRegions$Region
 366:           302           7248  org.apache.cassandra.utils.TopKSampler
 367:           151           7248  org.apache.cassandra.utils.memory.SlabAllocator
 368:           148           7104  java.lang.invoke.LambdaForm
 369:           292           7008  org.apache.cassandra.metrics.TableMetrics$TableTimer
 370:           155           6904  [Ljava.lang.invoke.LambdaForm$Name;
 371:           121           6776  jdk.internal.org.objectweb.asm.Item
 372:           169           6760  java.security.AccessControlContext
 373:           280           6720  java.util.Date
 374:           168           6720  java.util.IdentityHashMap
 375:           209           6688  org.apache.cassandra.db.ClusteringComparator
 376:           278           6672  com.google.common.collect.ImmutableSortedAsList
 377:           278           6672  com.google.common.collect.RegularImmutableSortedMap$EntrySet
 378:           278           6672  com.google.common.collect.RegularImmutableSortedMap$EntrySet$1
 379:           404           6464  java.util.concurrent.CopyOnWriteArraySet
 380:           200           6400  java.util.Formatter
 381:           400           6400  org.apache.cassandra.io.sstable.format.SSTableReader$UniqueIdentifier
 382:           399           6384  org.apache.cassandra.utils.obs.OffHeapBitSet
 383:            23           6368  [[S
 384:           394           6304  org.apache.cassandra.db.commitlog.IntervalSet
 385:           262           6288  java.util.concurrent.CopyOnWriteArrayList$COWIterator
 386:           156           6240  org.apache.cassandra.cql3.QueryOptions$DefaultQueryOptions
 387:           111           6216  sun.security.util.MemoryCache$SoftCacheEntry
 388:           155           6200  javax.management.MBeanOperationInfo
 389:           155           6200  org.apache.cassandra.db.Mutation
 390:           155           6200  org.apache.cassandra.db.partitions.PartitionUpdate
 391:           155           6200  org.apache.cassandra.utils.memory.AbstractAllocator$CloningBTreeRowBuilder
 392:           193           6176  org.apache.cassandra.net.OutboundTcpConnection$QueuedMessage
 393:           200           6160  [Ljava.util.Formatter$FormatString;
 394:           154           6160  java.util.Collections$SingletonMap
 395:           154           6160  org.apache.cassandra.db.rows.BTreeRow$$Lambda$122/418553968
 396:           154           6160  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$125/1196438970
 397:           152           6080  org.apache.cassandra.db.lifecycle.View
 398:           253           6072  java.util.concurrent.ConcurrentSkipListMap$Index
 399:           189           6048  org.apache.cassandra.repair.ValidationTask
 400:           108           6048  sun.security.x509.X509CertInfo
 401:           251           6024  javax.management.ImmutableDescriptor
 402:            62           5952  java.util.jar.JarFile$JarFileEntry
 403:            82           5904  java.beans.PropertyDescriptor
 404:           244           5856  org.apache.cassandra.db.rows.ComplexColumnData$$Lambda$111/177399658
 405:           243           5832  org.apache.cassandra.cql3.functions.FunctionName
 406:            52           5824  sun.nio.ch.SocketChannelImpl
 407:            90           5760  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$99/328488350
 408:           240           5736  [Lorg.apache.cassandra.db.marshal.AbstractType;
 409:           179           5728  org.apache.cassandra.auth.DataResource
 410:            89           5696  org.apache.cassandra.utils.btree.NodeBuilder
 411:           355           5680  org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy$1
 412:           229           5496  org.apache.cassandra.db.MutableDeletionInfo
 413:           227           5448  java.security.Provider$ServiceKey
 414:           224           5376  com.google.common.collect.SingletonImmutableSet
 415:            74           5328  ch.qos.logback.classic.spi.LoggingEvent
 416:            95           5320  java.security.Provider$Service
 417:           165           5280  java.lang.invoke.BoundMethodHandle$Species_L
 418:           106           5272  [Ljavax.management.MBeanAttributeInfo;
 419:           109           5232  java.util.concurrent.ThreadPoolExecutor$Worker
 420:           325           5200  org.apache.cassandra.utils.concurrent.WaitQueue
 421:           108           5184  javax.management.MBeanInfo
 422:           210           5040  com.google.common.collect.RegularImmutableAsList
 423:           210           5040  com.google.common.collect.RegularImmutableMap$EntrySet
 424:           208           4992  java.util.concurrent.ConcurrentHashMap$KeySetView
 425:           155           4960  org.apache.cassandra.db.commitlog.CommitLogSegment$Allocation
 426:           154           4928  [Lcom.google.common.collect.MapMakerInternalMap$Segment;
 427:           308           4928  org.apache.cassandra.db.Columns$$Lambda$121/617875913
 428:           154           4928  org.apache.cassandra.db.rows.EncodingStats$Collector
 429:           154           4928  org.apache.cassandra.io.util.DataOutputBufferFixed
 430:           102           4896  java.util.TimSort
 431:           152           4864  org.apache.cassandra.db.lifecycle.Tracker
 432:           202           4848  org.apache.cassandra.db.lifecycle.SSTableIntervalTree
 433:           121           4840  java.io.ObjectStreamField
 434:           151           4832  org.apache.cassandra.db.compaction.CompactionLogger
 435:            99           4752  javax.management.Notification
 436:           198           4752  org.apache.cassandra.db.ClusteringBound
 437:           198           4752  org.apache.cassandra.db.rows.ComplexColumnData$Builder
 438:           180           4744  [Ljava.security.ProtectionDomain;
 439:            63           4536  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterator
 440:            40           4480  java.net.SocksSocketImpl
 441:           275           4400  java.util.Formatter$Flags
 442:           273           4368  java.lang.Byte
 443:            32           4352  io.netty.buffer.PoolArena$DirectArena
 444:            32           4352  io.netty.buffer.PoolArena$HeapArena
 445:           181           4344  java.lang.invoke.LambdaForm$NamedFunction
 446:             6           4320  [Ljdk.internal.org.objectweb.asm.Item;
 447:            90           4320  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$313/480779282
 448:           108           4320  org.apache.cassandra.db.CachedHashDecoratedKey
 449:           178           4272  org.apache.cassandra.gms.GossipDigestAck
 450:           177           4248  java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject
 451:           131           4192  com.sun.jmx.mbeanserver.ConvertingMethod
 452:           128           4096  java.lang.NoSuchMethodException
 453:           256           4096  java.lang.Short
 454:            70           3920  sun.misc.URLClassPath$JarLoader
 455:            60           3840  java.util.jar.JarFile
 456:            80           3840  java.util.logging.LogManager$LoggerWeakRef
 457:           160           3840  org.apache.cassandra.db.Serializers
 458:           160           3840  org.apache.cassandra.db.Serializers$NewFormatSerializer
 459:           160           3840  org.apache.cassandra.io.sstable.IndexInfo$Serializer
 460:           160           3840  org.apache.cassandra.schema.Indexes
 461:            53           3816  java.util.regex.Pattern
 462:            95           3800  sun.security.rsa.RSAPublicKeyImpl
 463:           158           3792  com.sun.jmx.mbeanserver.PerInterface$MethodAndSig
 464:            59           3776  java.text.DateFormatSymbols
 465:           155           3720  org.apache.cassandra.utils.memory.ContextAllocator
 466:           154           3696  [Lorg.apache.cassandra.db.Directories$DataDirectory;
 467:           154           3696  com.google.common.collect.Collections2$TransformedCollection
 468:           154           3696  org.apache.cassandra.cql3.statements.UpdatesCollector
 469:           154           3696  org.apache.cassandra.db.filter.ClusteringIndexNamesFilter
 470:           154           3696  org.apache.cassandra.db.rows.Rows$$Lambda$120/877468788
 471:           151           3624  [Ljava.io.File;
 472:           151           3624  org.apache.cassandra.db.Directories
 473:           151           3624  org.apache.cassandra.db.Memtable$ColumnsCollector
 474:           151           3624  org.apache.cassandra.index.SecondaryIndexManager
 475:           151           3624  org.apache.cassandra.metrics.TableMetrics$10
 476:           151           3624  org.apache.cassandra.metrics.TableMetrics$11
 477:           151           3624  org.apache.cassandra.metrics.TableMetrics$12
 478:           151           3624  org.apache.cassandra.metrics.TableMetrics$14
 479:           151           3624  org.apache.cassandra.metrics.TableMetrics$15
 480:           151           3624  org.apache.cassandra.metrics.TableMetrics$16
 481:           151           3624  org.apache.cassandra.metrics.TableMetrics$17
 482:           151           3624  org.apache.cassandra.metrics.TableMetrics$19
 483:           151           3624  org.apache.cassandra.metrics.TableMetrics$2
 484:           151           3624  org.apache.cassandra.metrics.TableMetrics$21
 485:           151           3624  org.apache.cassandra.metrics.TableMetrics$23
 486:           151           3624  org.apache.cassandra.metrics.TableMetrics$24
 487:           151           3624  org.apache.cassandra.metrics.TableMetrics$25
 488:           151           3624  org.apache.cassandra.metrics.TableMetrics$27
 489:           151           3624  org.apache.cassandra.metrics.TableMetrics$29
 490:           151           3624  org.apache.cassandra.metrics.TableMetrics$3
 491:           151           3624  org.apache.cassandra.metrics.TableMetrics$30
 492:           151           3624  org.apache.cassandra.metrics.TableMetrics$31
 493:           151           3624  org.apache.cassandra.metrics.TableMetrics$32
 494:           151           3624  org.apache.cassandra.metrics.TableMetrics$33
 495:           151           3624  org.apache.cassandra.metrics.TableMetrics$34
 496:           151           3624  org.apache.cassandra.metrics.TableMetrics$4
 497:           151           3624  org.apache.cassandra.metrics.TableMetrics$5
 498:           151           3624  org.apache.cassandra.metrics.TableMetrics$6
 499:           151           3624  org.apache.cassandra.metrics.TableMetrics$7
 500:           151           3624  org.apache.cassandra.metrics.TableMetrics$8
 501:           151           3624  org.apache.cassandra.metrics.TableMetrics$9
 502:           113           3616  [Lorg.apache.cassandra.utils.memory.BufferPool$Chunk;
 503:           113           3616  org.apache.cassandra.utils.memory.BufferPool$LocalPoolRef
 504:           225           3600  org.apache.cassandra.cql3.FieldIdentifier
 505:           149           3576  org.apache.cassandra.cql3.restrictions.RestrictionSet
 506:           221           3536  java.util.zip.CRC32
 507:            63           3528  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionController
 508:            63           3528  org.apache.cassandra.repair.Validator
 509:            12           3480  [Ljava.util.concurrent.RunnableScheduledFuture;
 510:           108           3456  java.util.Collections$SynchronizedMap
 511:           143           3432  com.google.common.util.concurrent.Futures$CombinedFuture$2
 512:           143           3432  java.util.LinkedList$Node
 513:           107           3424  java.io.IOException
 514:            37           3384  [Lorg.apache.cassandra.io.sstable.IndexInfo;
 515:            60           3360  org.cliffc.high_scale_lib.ConcurrentAutoTable$CAT
 516:           122           3344  [Ljavax.management.MBeanParameterInfo;
 517:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$31/1914108708
 518:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$32/1889757798
 519:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$33/1166106620
 520:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$34/221861886
 521:            41           3328  [Ljava.lang.invoke.MethodHandle;
 522:            32           3328  java.io.ObjectStreamClass
 523:           208           3328  org.apache.cassandra.utils.concurrent.Refs
 524:            69           3312  com.google.common.util.concurrent.Futures$CombinedFuture
 525:           103           3296  org.apache.cassandra.schema.CompactionParams
 526:           137           3288  java.util.ArrayDeque
 527:            24           3264  com.codahale.metrics.Striped64$Cell
 528:           203           3248  org.apache.cassandra.io.util.DataOutputBuffer$GrowingChannel
 529:           135           3240  com.sun.jmx.remote.internal.ArrayNotificationBuffer$NamedNotification
 530:           101           3232  java.util.Vector
 531:           101           3232  org.apache.cassandra.schema.SpeculativeRetryParam
 532:           132           3168  org.apache.cassandra.db.view.TableViews
 533:            79           3160  com.google.common.collect.SingletonImmutableBiMap
 534:            98           3136  org.xml.sax.helpers.LocatorImpl
 535:            98           3136  sun.security.x509.BasicConstraintsExtension
 536:            78           3120  java.security.ProtectionDomain
 537:           129           3096  com.google.common.collect.RegularImmutableMap$NonTerminalMapEntry
 538:            77           3080  sun.nio.cs.UTF_8$Decoder
 539:            64           3072  org.apache.cassandra.db.compaction.CompactionIterator$Purger
 540:            64           3072  org.apache.cassandra.db.transform.UnfilteredPartitions
 541:            96           3072  sun.security.x509.SubjectKeyIdentifierExtension
 542:            24           3032  [Ljava.beans.MethodDescriptor;
 543:            92           3024  [Ljavax.management.MBeanOperationInfo;
 544:            94           3008  java.util.AbstractList$Itr
 545:            91           2912  com.codahale.metrics.Timer$Context
 546:           121           2904  org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate
 547:            60           2880  java.util.zip.Inflater
 548:            45           2880  javax.management.openmbean.OpenMBeanAttributeInfoSupport
 549:           118           2832  java.util.regex.Pattern$1
 550:           118           2832  sun.reflect.generics.tree.SimpleClassTypeSignature
 551:            88           2816  sun.security.x509.KeyUsageExtension
 552:           175           2800  org.apache.cassandra.gms.GossipDigestAck2
 553:           113           2712  org.apache.cassandra.utils.memory.BufferPool$LocalPool
 554:            37           2664  java.util.logging.Logger
 555:           111           2664  sun.security.util.Cache$EqualByteArray
 556:            55           2640  java.util.Hashtable
 557:           163           2608  java.util.IdentityHashMap$KeySet
 558:           162           2592  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable
 559:           108           2592  org.apache.cassandra.dht.LocalPartitioner$LocalToken
 560:            18           2592  sun.reflect.MethodAccessorGenerator
 561:           108           2592  sun.security.util.BitArray
 562:           108           2592  sun.security.x509.CertificateValidity
 563:           138           2584  [Lcom.sun.jmx.mbeanserver.MXBeanMapping;
 564:           107           2568  java.net.InetSocketAddress$InetSocketAddressHolder
 565:            64           2560  com.google.common.collect.Multimaps$UnmodifiableMultimap
 566:            64           2560  java.util.ArrayList$SubList
 567:            64           2560  java.util.ArrayList$SubList$1
 568:            64           2560  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1
 569:           160           2560  org.apache.cassandra.schema.Triggers
 570:            64           2560  org.apache.cassandra.utils.OverlapIterator
 571:            53           2544  java.util.concurrent.LinkedBlockingQueue
 572:           155           2480  org.apache.cassandra.utils.btree.UpdateFunction$Simple
 573:           155           2480  org.apache.cassandra.utils.concurrent.OpOrder
 574:            44           2464  java.lang.Class$ReflectionData
 575:           154           2464  java.util.concurrent.ConcurrentSkipListSet
 576:           154           2464  org.apache.cassandra.db.partitions.PartitionUpdate$$Lambda$117/1004624941
 577:           154           2464  org.apache.cassandra.db.partitions.PartitionUpdate$$Lambda$119/1364111969
 578:           154           2464  org.apache.cassandra.utils.WrappedBoolean
 579:           102           2448  org.apache.cassandra.schema.CachingParams
 580:            76           2432  java.security.CodeSource
 581:           151           2416  org.apache.cassandra.db.Memtable$StatsCollector
 582:           151           2416  org.apache.cassandra.utils.memory.EnsureOnHeap$NoOp
 583:            75           2400  java.util.LinkedList
 584:            50           2400  org.apache.cassandra.cql3.restrictions.StatementRestrictions
 585:            99           2376  sun.security.x509.CertificateExtensions
 586:            74           2368  java.io.ObjectStreamClass$WeakClassKey
 587:            98           2352  java.lang.Class$AnnotationData
 588:           147           2352  java.util.concurrent.ConcurrentHashMap$ValuesView
 589:            98           2352  java.util.jar.Attributes$Name
 590:            73           2336  java.util.regex.Pattern$Curly
 591:            97           2328  com.google.common.collect.ImmutableMapKeySet
 592:            48           2304  com.google.common.collect.HashMultimap
 593:            96           2304  com.google.common.collect.ImmutableMapKeySet$1
 594:            16           2304  io.netty.channel.epoll.EpollEventLoop
 595:           144           2304  org.apache.cassandra.db.ColumnFamilyStore$3
 596:            96           2304  org.apache.cassandra.metrics.KeyspaceMetrics$17
 597:            72           2304  sun.reflect.ClassFileAssembler
 598:            70           2240  java.util.concurrent.ConcurrentHashMap$ReservationNode
 599:            70           2240  java.util.logging.LogManager$LogNode
 600:            70           2240  org.apache.cassandra.utils.MerkleTree$TreeRangeIterator
 601:            91           2200  [Lcom.github.benmanes.caffeine.cache.RemovalCause;
 602:            91           2184  com.github.benmanes.caffeine.SingleConsumerQueue$Node
 603:            39           2184  org.apache.cassandra.db.marshal.UserType
 604:            90           2160  [Lcom.github.benmanes.caffeine.cache.Node;
 605:           118           2160  [Lsun.reflect.generics.tree.TypeArgument;
 606:            90           2160  com.github.benmanes.caffeine.cache.BoundedLocalCache$AddTask
 607:            90           2160  java.lang.StringBuffer
 608:            67           2144  java.util.TreeMap$ValueIterator
 609:            89           2136  java.lang.RuntimePermission
 610:            89           2136  org.apache.cassandra.io.compress.CompressionMetadata$Chunk
 611:            53           2120  sun.security.ec.NamedCurve
 612:            66           2112  java.io.FilePermission
 613:            66           2112  java.util.zip.ZipCoder
 614:            52           2080  sun.nio.ch.SocketAdaptor
 615:            37           2072  javax.management.MBeanServerNotification
 616:            37           2072  org.apache.cassandra.db.RowIndexEntry$IndexedEntry
 617:            86           2064  javax.management.openmbean.TabularDataSupport
 618:           129           2064  sun.security.x509.KeyIdentifier
 619:            64           2048  com.google.common.util.concurrent.Futures$ChainingListenableFuture
 620:           128           2048  java.lang.Character
 621:            64           2048  org.apache.cassandra.db.partitions.PurgeFunction$$Lambda$104/2021147872
 622:            64           2048  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2
 623:            64           2048  sun.misc.FloatingDecimal$ASCIIToBinaryBuffer
 624:            84           2016  java.security.Provider$UString
 625:            18           2016  java.util.GregorianCalendar
 626:            62           1984  org.apache.cassandra.utils.MerkleTrees$TreeRangeIterator
 627:            27           1944  sun.reflect.DelegatingClassLoader
 628:           120           1920  com.codahale.metrics.Striped64$HashCode
 629:            80           1920  java.util.regex.Pattern$GroupTail
 630:            34           1904  org.apache.cassandra.cql3.statements.SelectStatement
 631:            79           1896  com.google.common.collect.ImmutableList$1
 632:            79           1896  java.util.regex.Pattern$GroupHead
 633:            59           1888  java.util.RegularEnumSet
 634:           118           1888  sun.reflect.generics.tree.ClassTypeSignature
 635:           118           1888  sun.security.x509.SerialNumber
 636:            13           1872  java.text.DecimalFormat
 637:            39           1872  sun.util.locale.LocaleObjectCache$CacheEntry
 638:            10           1832  [[B
 639:            57           1824  org.apache.cassandra.cql3.functions.CastFcts$JavaFunctionWrapper
 640:            75           1800  java.util.regex.Pattern$Single
 641:            56           1792  java.lang.Throwable
 642:             8           1792  jdk.internal.org.objectweb.asm.MethodWriter
 643:            74           1776  com.google.common.util.concurrent.Futures$6
 644:           111           1776  java.util.LinkedHashMap$LinkedValues
 645:            44           1760  java.io.ObjectStreamClass$FieldReflectorKey
 646:            36           1728  org.apache.cassandra.concurrent.SEPWorker
 647:            72           1728  sun.reflect.ByteVectorImpl
 648:           108           1728  sun.security.x509.CertificateAlgorithmId
 649:           108           1728  sun.security.x509.CertificateSerialNumber
 650:           108           1728  sun.security.x509.CertificateVersion
 651:           108           1728  sun.security.x509.CertificateX509Key
 652:            18           1728  sun.util.calendar.Gregorian$Date
 653:           107           1712  java.net.InetSocketAddress
 654:             4           1696  [Ljava.lang.Thread;
 655:            53           1696  java.security.spec.EllipticCurve
 656:            30           1688  [Ljava.lang.reflect.Method;
 657:             6           1680  java.util.concurrent.ConcurrentHashMap$CounterCell
 658:            52           1664  java.lang.invoke.DirectMethodHandle$Special
 659:            52           1664  sun.nio.ch.SocketAdaptor$SocketInputStream
 660:            68           1632  org.apache.cassandra.cql3.Constants$Marker
 661:            68           1632  sun.reflect.NativeConstructorAccessorImpl
 662:           101           1616  org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$5/673586830
 663:            40           1600  ch.qos.logback.core.joran.event.StartEvent
 664:            40           1600  com.sun.jmx.mbeanserver.PerInterface
 665:            40           1600  sun.management.DiagnosticCommandArgumentInfo
 666:            99           1584  org.apache.cassandra.db.marshal.AbstractType$$Lambda$4/495702238
 667:            49           1568  java.io.DataOutputStream
 668:            49           1568  java.nio.channels.Channels$1
 669:            65           1560  java.security.spec.ECPoint
 670:            39           1560  org.apache.cassandra.io.util.SafeMemory
 671:            65           1560  org.apache.cassandra.utils.btree.TreeBuilder
 672:            64           1536  org.apache.cassandra.db.compaction.CompactionIterator$GarbageSkipper
 673:            63           1512  com.google.common.util.concurrent.Futures$1
 674:            63           1512  org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$EQRestriction
 675:            63           1512  org.apache.cassandra.db.compaction.CompactionManager$13
 676:            47           1504  org.apache.cassandra.cql3.statements.ParsedStatement$Prepared
 677:            47           1504  org.apache.cassandra.io.util.DataOutputBuffer$1$1
 678:            93           1488  java.util.Collections$UnmodifiableSet
 679:            61           1464  java.util.regex.Pattern$Slice
 680:            60           1440  java.util.zip.ZStreamRef
 681:            51           1408  [Ljava.io.ObjectStreamField;
 682:            16           1392  [Ljava.lang.Byte;
 683:             1           1376  [Lsun.misc.FDBigInteger;
 684:            43           1376  java.util.regex.Pattern$Branch
 685:            43           1376  org.apache.cassandra.concurrent.NamedThreadFactory
 686:            34           1360  ch.qos.logback.core.status.InfoStatus
 687:            17           1360  java.net.URI
 688:            34           1360  org.apache.cassandra.cql3.selection.Selection$SimpleSelection
 689:            61           1352  [Ljava.lang.reflect.Type;
 690:            24           1344  java.util.ResourceBundle$CacheKey
 691:            24           1344  javax.management.openmbean.CompositeType
 692:            72           1336  [Ljavax.management.openmbean.CompositeData;
 693:            33           1320  sun.security.x509.AuthorityKeyIdentifierExtension
 694:            79           1312  [Ljava.security.Principal;
 695:            54           1296  ch.qos.logback.classic.spi.StackTraceElementProxy
 696:            23           1288  java.net.SocketPermission
 697:            39           1280  [Ljava.math.BigInteger;
 698:            40           1280  ch.qos.logback.core.joran.event.EndEvent
 699:            16           1280  com.google.common.cache.LocalCache$Segment
 700:            20           1280  org.apache.cassandra.db.RowIndexEntry$ShallowIndexedEntry
 701:            43           1272  [Ljava.util.regex.Pattern$Node;
 702:            53           1272  sun.nio.ch.Util$BufferCache
 703:            79           1264  java.security.ProtectionDomain$Key
 704:            39           1248  java.lang.Thread$WeakClassKey
 705:            38           1240  [Ljava.lang.reflect.Field;
 706:            14           1232  org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor
 707:            38           1216  java.security.Permissions
 708:            50           1200  org.apache.cassandra.cql3.restrictions.ClusteringColumnRestrictions
 709:            50           1200  org.apache.cassandra.cql3.restrictions.IndexRestrictions
 710:            25           1200  org.apache.cassandra.metrics.ClientRequestMetrics
 711:             2           1184  [Lcom.github.benmanes.caffeine.cache.NodeFactory;
 712:            37           1184  java.net.Socket
 713:            49           1176  org.apache.cassandra.cql3.restrictions.PartitionKeySingleRestrictionSet
 714:            21           1176  sun.util.calendar.ZoneInfo
 715:            52           1168  [Lorg.apache.cassandra.cql3.ColumnSpecification;
 716:            24           1152  java.beans.BeanDescriptor
 717:            24           1152  java.lang.management.MemoryUsage
 718:            72           1152  org.apache.cassandra.db.ColumnFamilyStore$1
 719:            36           1152  org.apache.cassandra.io.util.SafeMemory$MemoryTidy
 720:            24           1152  org.hyperic.sigar.FileSystem
 721:            36           1152  sun.reflect.generics.repository.ClassRepository
 722:            20           1120  javax.management.openmbean.ArrayType
 723:            35           1120  org.apache.cassandra.cql3.ResultSet$ResultMetadata
 724:            69           1104  com.google.common.util.concurrent.Futures$8
 725:            69           1104  com.google.common.util.concurrent.Futures$CombinedFuture$1
 726:            46           1104  org.apache.cassandra.metrics.DefaultNameFactory
 727:            69           1104  sun.reflect.DelegatingConstructorAccessorImpl
 728:             3           1080  [Ljava.lang.Integer;
 729:            27           1080  com.google.common.collect.HashBiMap$BiEntry
 730:            27           1080  org.apache.cassandra.utils.CoalescingStrategies$DisabledCoalescingStrategy
 731:            45           1080  sun.reflect.generics.factory.CoreReflectionFactory
 732:            24           1064  [Ljava.beans.PropertyDescriptor;
 733:             2           1056  [Ljava.lang.Long;
 734:             2           1056  [Ljava.lang.Short;
 735:            26           1040  java.math.BigDecimal
 736:            43           1032  io.netty.channel.ChannelOption
 737:            43           1032  java.io.ExpiringCache$Entry
 738:            64           1024  org.apache.cassandra.db.compaction.AbstractCompactionStrategy$ScannerList
 739:            64           1024  org.apache.cassandra.db.compaction.CompactionIterator$1
 740:            64           1024  org.apache.cassandra.repair.RepairJob$3
 741:            63           1008  org.apache.cassandra.repair.RepairJob$2
 742:            12            960  [Lcom.google.common.collect.HashBiMap$BiEntry;
 743:            24            960  java.beans.GenericBeanInfo
 744:            30            960  java.security.Provider$EngineDescription
 745:            40            960  java.util.regex.Pattern$BitClass
 746:            20            960  org.antlr.runtime.CommonToken
 747:            30            960  org.apache.cassandra.cql3.ColumnSpecification
 748:            40            960  org.apache.cassandra.cql3.statements.SelectStatement$Parameters
 749:            60            960  org.cliffc.high_scale_lib.Counter
 750:            20            960  org.cliffc.high_scale_lib.NonBlockingHashMap$CHM
 751:            40            960  org.codehaus.jackson.map.type.ClassKey
 752:            40            960  org.xml.sax.helpers.AttributesImpl
 753:            46            944  [Lsun.reflect.generics.tree.FormalTypeParameter;
 754:            39            936  java.util.regex.Pattern$5
 755:             8            928  [Lorg.apache.cassandra.db.ClusteringBound;
 756:            29            928  java.security.BasicPermissionCollection
 757:            29            928  org.apache.cassandra.io.util.DataInputPlus$DataInputStreamPlus
 758:            23            920  org.codehaus.jackson.map.type.SimpleType
 759:            19            912  sun.management.DiagnosticCommandInfo
 760:            28            896  java.io.DataInputStream
 761:            18            864  net.jpountz.lz4.LZ4BlockOutputStream
 762:            54            864  org.apache.cassandra.config.ColumnDefinition$$Lambda$26/843299092
 763:            54            864  org.apache.cassandra.config.ColumnDefinition$$Lambda$27/605982374
 764:            54            864  org.apache.cassandra.config.ColumnDefinition$1
 765:            18            864  org.apache.cassandra.utils.SlidingTimeRate
 766:            36            864  sun.reflect.Label$PatchInfo
 767:            27            864  sun.reflect.generics.reflectiveObjects.TypeVariableImpl
 768:            36            864  sun.reflect.generics.tree.ClassSignature
 769:            44            856  [Ljavax.management.MBeanConstructorInfo;
 770:            21            840  com.sun.jmx.mbeanserver.MXBeanSupport
 771:            35            840  net.jpountz.xxhash.StreamingXXHash32JNI
 772:            35            840  sun.reflect.generics.scope.ClassScope
 773:            21            840  sun.util.locale.BaseLocale$Key
 774:             2            832  [Lorg.antlr.runtime.BitSet;
 775:            13            832  com.google.common.util.concurrent.SmoothRateLimiter$SmoothBursty
 776:            13            832  java.text.DecimalFormatSymbols
 777:            38            824  [Lsun.reflect.generics.tree.FieldTypeSignature;
 778:            34            816  org.apache.cassandra.cql3.selection.SelectionColumnMapping
 779:             6            816  org.apache.cassandra.metrics.KeyspaceMetrics
 780:            25            800  java.util.PropertyPermission
 781:            20            800  org.cliffc.high_scale_lib.NonBlockingHashMap
 782:            14            784  java.util.HashMap$TreeNode
 783:            14            784  org.apache.cassandra.cql3.statements.UpdateStatement
 784:            32            768  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$IdentityMapping
 785:            32            768  io.netty.channel.unix.FileDescriptor
 786:            16            768  java.util.ResourceBundle$BundleReference
 787:            24            768  java.util.ResourceBundle$LoaderReference
 788:            16            768  net.jpountz.lz4.LZ4BlockInputStream
 789:            32            768  org.apache.cassandra.cql3.functions.CastFcts$CastAsTextFunction
 790:            32            768  sun.reflect.generics.reflectiveObjects.ParameterizedTypeImpl
 791:            24            768  sun.security.x509.OIDMap$OIDInfo
 792:            23            736  javax.management.MBeanConstructorInfo
 793:            23            736  sun.management.MappedMXBeanType$BasicMXBeanType
 794:            30            720  com.google.common.collect.ImmutableEntry
 795:            30            720  java.io.ObjectStreamClass$EntryFuture
 796:            15            720  java.lang.management.PlatformComponent
 797:             9            720  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor
 798:             9            720  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
 799:             1            720  org.apache.cassandra.config.Config
 800:            18            720  org.apache.cassandra.metrics.ThreadPoolMetrics
 801:            22            704  com.sun.jmx.mbeanserver.WeakIdentityHashMap$IdentityWeakReference
 802:            11            704  java.text.SimpleDateFormat
 803:            29            696  org.apache.cassandra.net.MessagingService$Verb
 804:            36            688  [Lsun.reflect.generics.tree.ClassTypeSignature;
 805:            43            688  java.util.regex.Pattern$BranchConn
 806:            17            680  sun.reflect.UnsafeQualifiedStaticLongFieldAccessorImpl
 807:            29            672  [Ljava.lang.reflect.TypeVariable;
 808:            28            672  ch.qos.logback.core.spi.ContextAwareBase
 809:            28            672  java.util.regex.Pattern$Ctype
 810:            28            672  java.util.regex.Pattern$Start
 811:             4            672  jdk.internal.org.objectweb.asm.ClassWriter
 812:            42            672  org.apache.cassandra.config.ColumnDefinition$Raw$Literal
 813:            42            672  org.apache.cassandra.io.sstable.format.big.BigTableScanner$EmptySSTableScanner
 814:            28            672  sun.nio.ch.SocketOptionRegistry$RegistryKey
 815:            12            672  sun.security.ssl.CipherSuite$BulkCipher
 816:            41            656  ch.qos.logback.core.joran.spi.ElementPath
 817:            27            648  java.io.FilePermissionCollection
 818:            27            648  org.apache.cassandra.cql3.selection.RawSelector
 819:            27            648  sun.reflect.generics.tree.FormalTypeParameter
 820:            16            640  io.netty.util.collection.IntObjectHashMap
 821:             8            640  java.util.concurrent.ThreadPoolExecutor
 822:            40            640  java.util.jar.Attributes
 823:             8            640  java.util.zip.ZipEntry
 824:            10            640  jdk.internal.org.objectweb.asm.Label
 825:            20            640  org.apache.cassandra.cql3.functions.BytesConversionFcts$2
 826:            20            640  org.apache.cassandra.db.compaction.OperationType
 827:             3            624  [Ljava.lang.invoke.LambdaForm;
 828:            13            624  java.nio.HeapCharBuffer
 829:            26            624  java.security.spec.ECFieldF2m
 830:            26            624  java.util.regex.Pattern$Ques
 831:            39            624  org.apache.cassandra.serializers.TupleSerializer
 832:            39            624  org.apache.cassandra.serializers.UserTypeSerializer
 833:            27            616  [Ljava.lang.reflect.Constructor;
 834:            19            608  java.io.FileInputStream
 835:            19            608  java.rmi.server.UID
 836:            19            608  java.util.Locale
 837:            19            608  org.apache.cassandra.schema.IndexMetadata
 838:            19            608  sun.management.DiagnosticCommandImpl$Wrapper
 839:            19            608  sun.util.locale.BaseLocale
 840:            15            600  java.lang.ClassNotFoundException
 841:            25            600  java.lang.invoke.Invokers
 842:            25            600  java.util.concurrent.locks.ReentrantReadWriteLock$Sync$HoldCounter
 843:            25            600  org.apache.cassandra.gms.ApplicationState
 844:            25            600  sun.reflect.NativeMethodAccessorImpl
 845:            25            600  sun.reflect.annotation.AnnotationInvocationHandler
 846:            18            576  ch.qos.logback.core.joran.event.BodyEvent
 847:            12            576  java.io.ObjectInputStream$FilterValues
 848:            24            576  jdk.internal.org.objectweb.asm.ByteVector
 849:            12            576  org.apache.cassandra.db.marshal.MapType
 850:             9            576  org.apache.cassandra.metrics.ConnectionMetrics
 851:            24            576  org.apache.cassandra.metrics.ThreadPoolMetricNameFactory
 852:            35            560  ch.qos.logback.core.joran.spi.ElementSelector
 853:            14            560  io.netty.util.Recycler$WeakOrderQueue
 854:            10            560  java.util.zip.ZipFile$ZipFileInflaterInputStream
 855:            10            560  java.util.zip.ZipFile$ZipFileInputStream
 856:            14            560  javax.management.openmbean.SimpleType
 857:            10            560  sun.invoke.util.Wrapper
 858:            23            552  [Ljava.net.InetAddress;
 859:             3            552  [Lorg.apache.cassandra.net.MessagingService$Verb;
 860:            23            552  ch.qos.logback.core.pattern.LiteralConverter
 861:            23            552  io.netty.util.internal.logging.Slf4JLogger
 862:            23            552  org.codehaus.jackson.map.SerializationConfig$Feature
 863:             2            544  [Ljava.lang.Character;
 864:            17            544  io.netty.util.concurrent.DefaultPromise
 865:            34            544  java.io.FilePermission$1
 866:            17            544  java.nio.channels.ClosedChannelException
 867:            17            544  java.util.concurrent.atomic.AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl
 868:            34            544  net.jpountz.xxhash.StreamingXXHash32$1
 869:            17            544  org.apache.cassandra.transport.Message$Type
 870:            17            544  sun.reflect.MethodAccessorGenerator$1
 871:            17            544  sun.security.x509.DistributionPoint
 872:            17            544  sun.security.x509.URIName
 873:            22            528  java.net.URLClassLoader$1
 874:            22            528  org.apache.cassandra.cql3.CQL3Type$Native
 875:            33            528  sun.reflect.DelegatingMethodAccessorImpl
 876:            13            520  com.google.common.base.Stopwatch
 877:            13            520  io.netty.channel.unix.Errors$NativeIoException
 878:            13            520  java.lang.invoke.MethodHandleImpl$IntrinsicMethodHandle
 879:            13            520  java.text.DigitList
 880:             4            512  com.google.common.cache.LocalCache
 881:            16            512  io.netty.channel.epoll.IovArray
 882:            16            512  java.lang.NoSuchFieldException
 883:            32            512  java.util.TreeSet
 884:            16            512  java.util.concurrent.Semaphore$NonfairSync
 885:            16            512  sun.security.ssl.CipherSuite$KeyExchange
 886:            21            504  java.util.Locale$LocaleKey
 887:             9            504  java.util.concurrent.ConcurrentHashMap$ValueIterator
 888:            21            504  org.apache.cassandra.cql3.functions.AggregateFcts$24
 889:             9            504  org.apache.cassandra.net.RateBasedBackPressureState
 890:            21            504  sun.security.x509.AVAKeyword
 891:            31            496  sun.security.x509.GeneralName
 892:            19            488  [Lsun.management.DiagnosticCommandArgumentInfo;
 893:            20            480  java.io.ObjectStreamClass$2
 894:            12            480  java.lang.UNIXProcess$ProcessPipeInputStream
 895:            20            480  org.apache.cassandra.cql3.functions.AggregateFcts$22
 896:            20            480  org.apache.cassandra.cql3.functions.AggregateFcts$23
 897:            20            480  org.apache.cassandra.cql3.functions.BytesConversionFcts$1
 898:            20            480  org.apache.cassandra.dht.LocalPartitioner
 899:            15            480  org.apache.cassandra.index.internal.composites.RegularColumnIndex
 900:             6            480  org.apache.cassandra.repair.RepairSession
 901:            20            480  org.yaml.snakeyaml.tokens.Token$ID
 902:             6            480  sun.net.www.protocol.jar.URLJarFile
 903:            30            480  sun.security.x509.GeneralNames
 904:             6            456  [Lsun.invoke.util.Wrapper;
 905:            19            456  ch.qos.logback.classic.spi.ClassPackagingData
 906:            19            456  java.lang.Class$1
 907:            19            456  java.util.regex.Pattern$Dollar
 908:             5            448  [[Ljava.lang.Object;
 909:             7            448  java.security.SecureRandom
 910:            28            448  java.util.LinkedHashSet
 911:             8            448  javax.management.openmbean.OpenMBeanParameterInfoSupport
 912:             8            448  jdk.internal.org.objectweb.asm.AnnotationWriter
 913:            14            448  jdk.internal.org.objectweb.asm.Type
 914:            14            448  sun.security.x509.CRLDistributionPointsExtension
 915:            11            440  java.lang.ClassLoader$NativeLibrary
 916:            11            440  sun.security.ec.ECPublicKeyImpl
 917:             9            432  com.sun.jna.Function
 918:            27            432  java.security.spec.ECFieldFp
 919:            18            432  java.text.DateFormat$Field
 920:            18            432  java.util.Collections$UnmodifiableCollection$1
 921:            18            432  org.apache.cassandra.exceptions.ExceptionCode
 922:            18            432  org.apache.cassandra.io.util.WrappedDataOutputStreamPlus
 923:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$1
 924:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$2
 925:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$3
 926:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$4
 927:             9            432  org.apache.cassandra.net.OutboundTcpConnectionPool
 928:            18            432  org.cliffc.high_scale_lib.NonBlockingHashMap$NBHMEntry
 929:            13            416  io.netty.util.Recycler$WeakOrderQueue$Link
 930:            13            416  java.lang.invoke.SimpleMethodHandle
 931:            13            416  java.security.AlgorithmParameters
 932:            13            416  java.util.Stack
 933:             4            416  sun.net.www.protocol.file.FileURLConnection
 934:            17            408  org.apache.cassandra.utils.IntegerInterval
 935:            17            408  org.codehaus.jackson.map.DeserializationConfig$Feature
 936:            10            400  java.io.ObjectStreamClass$FieldReflector
 937:            10            400  java.lang.invoke.DirectMethodHandle$Accessor
 938:            10            400  javax.crypto.CryptoPermission
 939:            10            400  sun.reflect.generics.repository.MethodRepository
 940:             7            392  java.util.Calendar$Builder
 941:             1            392  org.apache.cassandra.utils.memory.MemtableCleanerThread
 942:             8            384  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicLong;
 943:             1            384  ch.qos.logback.core.AsyncAppenderBase$Worker
 944:             4            384  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap
 945:            16            384  io.netty.channel.epoll.EpollEventArray
 946:            12            384  java.io.EOFException
 947:             1            384  java.lang.ref.Finalizer$FinalizerThread
 948:             8            384  java.net.SocketInputStream
 949:             8            384  java.net.SocketOutputStream
 950:            12            384  java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue
 951:            12            384  java.util.concurrent.atomic.AtomicLongFieldUpdater$CASUpdater
 952:             1            384  java.util.logging.LogManager$Cleaner
 953:            16            384  javax.management.StandardMBean
 954:            16            384  org.apache.cassandra.cql3.Attributes
 955:            16            384  org.apache.cassandra.cql3.Constants$Setter
 956:            16            384  org.apache.cassandra.cql3.Operations
 957:            12            384  org.apache.cassandra.cql3.SingleColumnRelation
 958:             8            384  org.apache.cassandra.hints.HintsStore
 959:            16            384  org.apache.cassandra.metrics.TableMetrics$35
 960:             1            384  org.apache.cassandra.net.MessagingService$SocketThread
 961:            16            384  org.apache.cassandra.schema.TableParams$Option
 962:             1            384  org.apache.cassandra.thrift.ThriftServer$ThriftServerThread
 963:            16            384  sun.misc.MetaIndex
 964:            16            384  sun.nio.ch.OptionKey
 965:             3            384  sun.nio.fs.UnixFileAttributes
 966:            12            384  sun.nio.fs.UnixPath
 967:             1            376  java.lang.ref.Reference$ReferenceHandler
 968:            16            368  [Ljava.security.cert.Certificate;
 969:            17            368  [Ljavax.management.MBeanNotificationInfo;
 970:            23            368  java.lang.ThreadLocal
 971:             3            360  [Lorg.apache.cassandra.gms.ApplicationState;
 972:            15            360  com.sun.jmx.remote.util.ClassLogger
 973:             9            360  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$Limit
 974:             9            360  java.io.BufferedInputStream
 975:            15            360  java.io.ObjectStreamClass$ClassDataSlot
 976:            15            360  java.net.InetAddress
 977:             9            360  org.apache.cassandra.db.marshal.SetType
 978:            15            360  org.apache.cassandra.utils.memory.SlabAllocator$Region
 979:            11            352  java.lang.ClassLoader$1
 980:            11            352  java.util.concurrent.SynchronousQueue
 981:            11            352  org.apache.cassandra.db.ConsistencyLevel
 982:             4            352  sun.rmi.transport.ConnectionInputStream
 983:             7            336  [Ljavax.management.openmbean.OpenType;
 984:             7            336  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$CompositeMapping
 985:            14            336  java.lang.invoke.LambdaFormEditor$Transform$Kind
 986:             6            336  java.nio.DirectLongBufferU
 987:            21            336  java.util.Collections$UnmodifiableCollection
 988:             7            336  java.util.Properties
 989:             6            336  org.apache.cassandra.concurrent.SEPExecutor
 990:             6            336  sun.management.MemoryPoolImpl
 991:             5            328  [Ljava.io.ObjectInputStream$HandleTable$HandleList;
 992:            16            328  [Ljava.lang.management.PlatformComponent;
 993:             4            320  [Lio.netty.buffer.PoolArena;
 994:            10            320  [Ljava.lang.invoke.LambdaForm$BasicType;
 995:            10            320  java.io.FileOutputStream
 996:             8            320  java.io.ObjectOutputStream$HandleTable
 997:            10            320  java.lang.OutOfMemoryError
 998:            10            320  java.lang.StringCoding$StringEncoder
 999:            10            320  java.lang.reflect.WeakCache$CacheValue
1000:            10            320  java.security.cert.PolicyQualifierInfo
1001:             8            320  org.apache.cassandra.db.marshal.ListType
1002:            20            320  org.apache.cassandra.dht.LocalPartitioner$1
1003:             8            320  org.apache.cassandra.gms.ArrayBackedBoundedStats
1004:             8            320  org.apache.cassandra.gms.ArrivalWindow
1005:            10            320  sun.reflect.generics.tree.MethodTypeSignature
1006:             8            320  sun.rmi.transport.tcp.TCPTransport$ConnectionHandler
1007:            10            320  sun.security.util.DisabledAlgorithmConstraints$KeySizeConstraint
1008:            13            312  [Ljava.net.InetSocketAddress;
1009:            13            312  com.sun.jna.Pointer
1010:            13            312  java.lang.management.ManagementPermission
1011:            19            304  sun.reflect.BootstrapConstructorAccessorImpl
1012:             1            296  com.github.benmanes.caffeine.SingleConsumerQueue
1013:             1            296  com.github.benmanes.caffeine.cache.BoundedBuffer$RingBuffer
1014:             4            288  [Lch.qos.logback.classic.spi.StackTraceElementProxy;
1015:            12            288  [Lcom.codahale.metrics.Striped64$Cell;
1016:            12            288  ch.qos.logback.core.joran.spi.HostClassAndPropertyDouble
1017:             1            288  com.github.benmanes.caffeine.cache.LocalCacheFactory$SSLiMW
1018:             6            288  com.google.common.collect.HashBiMap
1019:             9            288  com.google.common.collect.RegularImmutableSet
1020:             4            288  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8
1021:            12            288  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor$ListenerWrapper
1022:             6            288  java.io.BufferedReader
1023:            12            288  java.lang.ProcessEnvironment$Variable
1024:             9            288  java.lang.reflect.Proxy$Key1
1025:             9            288  java.util.concurrent.CountDownLatch$Sync
1026:             9            288  java.util.concurrent.SynchronousQueue$TransferStack$SNode
1027:             9            288  java.util.logging.Level
1028:            18            288  java.util.regex.Pattern$Begin
1029:            12            288  org.apache.cassandra.concurrent.Stage
1030:            18            288  org.apache.cassandra.io.util.DataOutputStreamPlus$2
1031:             4            288  org.apache.cassandra.locator.TokenMetadata
1032:             9            288  org.apache.commons.lang3.JavaVersion
1033:             6            288  sun.nio.cs.StreamDecoder
1034:            18            288  sun.reflect.Label
1035:             4            288  sun.rmi.transport.ConnectionOutputStream
1036:             9            288  sun.security.jca.ProviderConfig
1037:             7            280  java.net.SocketTimeoutException
1038:             7            280  org.apache.cassandra.streaming.messages.StreamMessage$Type
1039:             7            280  org.apache.thrift.transport.TTransportException
1040:             7            280  sun.misc.FloatingDecimal$BinaryToASCIIBuffer
1041:             7            280  sun.rmi.transport.tcp.TCPEndpoint
1042:             1            272  [Lorg.codehaus.jackson.sym.Name;
1043:            17            272  com.sun.proxy.$Proxy3
1044:            17            272  net.jpountz.lz4.LZ4HCJNICompressor
1045:            17            272  org.apache.cassandra.cql3.Constants$Value
1046:            17            272  sun.reflect.ClassDefiner$1
1047:            17            272  sun.security.x509.DNSName
1048:             3            264  [[D
1049:            11            264  com.google.common.collect.ImmutableMapValues
1050:            11            264  java.net.StandardSocketOptions$StdSocketOption
1051:            11            264  java.rmi.server.ObjID
1052:            11            264  java.util.regex.Pattern$SliceI
1053:            11            264  org.apache.cassandra.io.sstable.Component
1054:            11            264  org.apache.cassandra.io.sstable.Component$Type
1055:            11            264  org.apache.cassandra.metrics.DroppedMessageMetrics
1056:            11            264  org.apache.cassandra.metrics.TableMetrics$36
1057:            11            264  org.apache.cassandra.net.MessagingService$DroppedMessages
1058:            11            264  sun.rmi.transport.ObjectEndpoint
1059:            11            264  sun.security.util.DisabledAlgorithmConstraints$DisabledConstraint
1060:            10            256  [Ljava.io.ObjectStreamClass$ClassDataSlot;
1061:             8            256  com.google.common.cache.LocalCache$StrongEntry
1062:            16            256  io.netty.channel.epoll.EpollEventLoop$1
1063:            16            256  io.netty.channel.epoll.EpollEventLoop$2
1064:            16            256  io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator
1065:            16            256  io.netty.util.concurrent.SingleThreadEventExecutor$2
1066:            16            256  io.netty.util.concurrent.SingleThreadEventExecutor$DefaultThreadProperties
1067:             8            256  java.util.Collections$UnmodifiableMap
1068:            16            256  java.util.concurrent.Semaphore
1069:             8            256  javax.management.MBeanNotificationInfo
1070:             8            256  org.apache.cassandra.cql3.functions.CastFcts$JavaCounterFunctionWrapper
1071:             8            256  org.apache.cassandra.db.ClusteringPrefix$Kind
1072:             8            256  org.apache.cassandra.repair.messages.RepairMessage$Type
1073:             8            256  sun.management.NotificationEmitterSupport$ListenerInfo
1074:             8            256  sun.misc.ProxyGenerator$PrimitiveTypeInfo
1075:             8            256  sun.misc.URLClassPath$JarLoader$2
1076:             8            256  sun.security.x509.CertificatePoliciesExtension
1077:             6            240  [Ljava.lang.invoke.BoundMethodHandle$SpeciesData;
1078:            10            240  com.sun.org.apache.xerces.internal.impl.XMLScanner$NameType
1079:            10            240  java.io.BufferedOutputStream
1080:             6            240  java.lang.UNIXProcess
1081:            10            240  java.nio.file.StandardOpenOption
1082:            10            240  java.security.CryptoPrimitive
1083:             3            240  java.util.concurrent.ScheduledThreadPoolExecutor
1084:            15            240  java.util.regex.Pattern$Dot
1085:            10            240  org.apache.cassandra.auth.Permission
1086:             5            240  org.apache.cassandra.config.ViewDefinition
1087:             5            240  org.apache.cassandra.db.lifecycle.LogRecord
1088:             5            240  org.apache.cassandra.db.view.View
1089:             6            240  org.apache.cassandra.metrics.SEPMetrics
1090:             6            240  org.apache.cassandra.schema.KeyspaceMetadata
1091:            10            240  org.codehaus.jackson.JsonParser$Feature
1092:            10            240  org.yaml.snakeyaml.events.Event$ID
1093:            15            240  org.yaml.snakeyaml.nodes.Tag
1094:             6            240  sun.management.MemoryPoolImpl$CollectionSensor
1095:             6            240  sun.management.MemoryPoolImpl$PoolSensor
1096:             5            240  sun.misc.URLClassPath
1097:            10            240  sun.reflect.generics.scope.MethodScope
1098:            15            240  sun.reflect.generics.tree.TypeVariableSignature
1099:            10            240  sun.rmi.runtime.Log$LoggerLog
1100:            10            240  sun.security.x509.Extension
1101:             5            240  sun.util.locale.provider.LocaleResources$ResourceReference
1102:             8            232  [Ljava.lang.Boolean;
1103:             2            224  [Lorg.codehaus.jackson.map.SerializationConfig$Feature;
1104:             7            224  [Lsun.nio.fs.NativeBuffer;
1105:             7            224  com.google.common.util.concurrent.MoreExecutors$DirectExecutorService
1106:             4            224  java.io.ObjectInputStream$BlockDataInputStream
1107:            14            224  java.rmi.server.Operation
1108:             7            224  java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl
1109:             7            224  java.util.regex.Pattern$BnM
1110:             7            224  org.codehaus.jackson.JsonGenerator$Feature
1111:             4            224  org.codehaus.jackson.map.introspect.AnnotatedClass
1112:             4            224  org.codehaus.jackson.map.introspect.BasicBeanDescription
1113:             7            224  sun.nio.fs.NativeBuffer
1114:             7            224  sun.reflect.annotation.AnnotationType
1115:             4            224  sun.rmi.transport.Target
1116:             7            224  sun.security.x509.NetscapeCertTypeExtension
1117:             9            216  java.lang.ProcessEnvironment$Value
1118:             9            216  java.util.Collections$SynchronizedSet
1119:             9            216  java.util.logging.Level$KnownLevel
1120:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$1
1121:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$2
1122:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$3
1123:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$4
1124:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$5
1125:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$6
1126:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$7
1127:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$8
1128:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$9
1129:             3            216  sun.security.provider.NativePRNG$RandomIO
1130:             9            216  sun.util.logging.PlatformLogger$Level
1131:             7            208  [Ljava.lang.invoke.LambdaForm$NamedFunction;
1132:             2            208  [Lorg.apache.cassandra.cql3.CQL3Type$Native;
1133:            13            208  com.google.common.util.concurrent.RateLimiter$SleepingStopwatch$1
1134:             2            208  java.lang.invoke.InnerClassLambdaMetafactory
1135:            13            208  sun.nio.ch.SocketAdaptor$2
1136:             2            200  [Ljava.text.DateFormat$Field;
1137:             5            200  io.netty.channel.group.DefaultChannelGroup
1138:             5            200  java.lang.invoke.BoundMethodHandle$SpeciesData
1139:             5            200  java.lang.invoke.DirectMethodHandle$Constructor
1140:             5            200  java.util.stream.StreamOpFlag
1141:             5            200  org.apache.cassandra.cql3.statements.SelectStatement$RawStatement
1142:             5            200  org.apache.cassandra.db.view.ViewBuilder
1143:             5            200  sun.rmi.transport.WeakRef
1144:             6            192  [Ljava.rmi.server.Operation;
1145:             3            192  [Lorg.apache.cassandra.db.ConsistencyLevel;
1146:             4            192  [[Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference;
1147:             3            192  ch.qos.logback.classic.PatternLayout
1148:             6            192  ch.qos.logback.core.util.CachingDateFormatter
1149:             8            192  com.google.common.cache.LocalCache$AccessQueue$1
1150:             4            192  com.google.common.collect.TreeMultimap
1151:             6            192  java.lang.ProcessBuilder
1152:             6            192  java.lang.invoke.LambdaForm$BasicType
1153:             8            192  java.lang.invoke.MethodHandleImpl$Intrinsic
1154:             8            192  java.math.RoundingMode
1155:            12            192  java.util.concurrent.ConcurrentSkipListMap$EntrySet
1156:             4            192  java.util.concurrent.locks.ReentrantReadWriteLock$FairSync
1157:             8            192  java.util.regex.Pattern$7
1158:             8            192  javax.crypto.CryptoPermissionCollection
1159:             4            192  javax.management.openmbean.TabularType
1160:             3            192  jdk.internal.org.objectweb.asm.FieldWriter
1161:             4            192  jdk.internal.org.objectweb.asm.Frame
1162:             8            192  jdk.net.SocketFlow$Status
1163:             6            192  org.apache.cassandra.db.Keyspace
1164:             4            192  org.apache.cassandra.db.RangeTombstoneList
1165:             8            192  org.apache.cassandra.db.WriteType
1166:             8            192  org.apache.cassandra.serializers.MapSerializer
1167:             8            192  org.apache.cassandra.serializers.SetSerializer
1168:             8            192  org.apache.cassandra.serializers.UTF8Serializer$UTF8Validator$State
1169:             8            192  org.apache.cassandra.service.StorageService$Mode
1170:             3            192  org.apache.cassandra.utils.MerkleTree$TreeDifference
1171:             6            192  org.apache.commons.lang3.text.StrBuilder
1172:             8            192  org.yaml.snakeyaml.scanner.Constant
1173:            12            192  sun.nio.ch.SocketAdaptor$1
1174:             6            192  sun.rmi.runtime.NewThreadAction
1175:             2            192  sun.security.provider.Sun
1176:             6            192  sun.security.util.MemoryCache
1177:             8            192  sun.security.x509.PolicyInformation
1178:             2            176  [Lorg.apache.cassandra.transport.Message$Type;
1179:             2            176  [Lorg.codehaus.jackson.map.DeserializationConfig$Feature;
1180:            10            176  [Lsun.reflect.generics.tree.TypeSignature;
1181:            11            176  java.text.NumberFormat$Field
1182:            11            176  java.util.LinkedHashMap$LinkedEntrySet
1183:            11            176  java.util.concurrent.SynchronousQueue$TransferStack
1184:             2            176  javax.management.remote.rmi.NoCallStackClassLoader
1185:             2            176  org.apache.cassandra.db.commitlog.MemoryMappedSegment
1186:            11            176  sun.security.ec.ECParameters
1187:             1            168  [[Ljava.math.BigInteger;
1188:             7            168  ch.qos.logback.classic.Level
1189:             3            168  ch.qos.logback.classic.encoder.PatternLayoutEncoder
1190:             7            168  com.google.common.collect.ImmutableEnumSet
1191:             7            168  com.sun.management.VMOption$Origin
1192:             7            168  com.sun.org.apache.xerces.internal.util.FeatureState
1193:             7            168  java.lang.invoke.MethodHandles$Lookup
1194:             7            168  java.net.NetPermission
1195:             7            168  java.util.BitSet
1196:             3            168  javax.management.openmbean.OpenMBeanOperationInfoSupport
1197:             7            168  javax.security.auth.AuthPermission
1198:             7            168  org.apache.cassandra.cql3.Constants$Type
1199:             7            168  org.apache.cassandra.db.Directories$FileAction
1200:             7            168  org.apache.cassandra.utils.concurrent.SimpleCondition
1201:             7            168  org.apache.cassandra.utils.progress.ProgressEventType
1202:             7            168  org.codehaus.jackson.annotate.JsonMethod
1203:             7            168  sun.nio.fs.NativeBuffer$Deallocator
1204:             7            168  sun.rmi.server.LoaderHandler$LoaderKey
1205:             3            168  sun.rmi.transport.tcp.TCPChannel
1206:             3            168  sun.rmi.transport.tcp.TCPConnection
1207:             3            168  sun.security.provider.SHA
1208:             7            168  sun.security.x509.NetscapeCertTypeExtension$MapEntry
1209:             4            160  [F
1210:             2            160  ch.qos.logback.core.rolling.RollingFileAppender
1211:            10            160  io.netty.util.internal.ConcurrentSet
1212:             4            160  java.io.ObjectOutputStream$BlockDataOutputStream
1213:             5            160  java.io.SerializablePermission
1214:             5            160  java.lang.StringCoding$StringDecoder
1215:             5            160  javax.management.StandardEmitterMBean
1216:             5            160  org.apache.cassandra.db.marshal.CompositeType
1217:             5            160  org.apache.cassandra.repair.RepairRunnable$1
1218:             5            160  org.apache.cassandra.transport.ProtocolVersion
1219:             5            160  org.apache.cassandra.transport.messages.ResultMessage$Kind
1220:             5            160  org.apache.cassandra.utils.CassandraVersion
1221:             4            160  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotV
1222:             5            160  sun.rmi.transport.StreamRemoteCall
1223:             5            160  sun.security.ssl.CipherSuite$MacAlg
1224:            10            160  sun.security.x509.CertificatePolicyId
1225:             5            160  sun.util.locale.provider.LocaleProviderAdapter$Type
1226:             6            144  [Ljava.io.Closeable;
1227:             2            144  [Ljava.math.BigDecimal;
1228:             1            144  [Ljava.util.concurrent.ForkJoinTask$ExceptionNode;
1229:             1            144  [Lorg.codehaus.jackson.sym.CharsToNameCanonicalizer$Bucket;
1230:             3            144  ch.qos.logback.classic.pattern.DateConverter
1231:             3            144  ch.qos.logback.classic.pattern.ExtendedThrowableProxyConverter
1232:             3            144  ch.qos.logback.classic.spi.ThrowableProxy
1233:             6            144  com.google.common.collect.AbstractMultimap$EntrySet
1234:             6            144  com.sun.org.apache.xerces.internal.util.Status
1235:             6            144  java.io.InputStreamReader
1236:             3            144  java.lang.ThreadGroup
1237:             6            144  java.lang.UNIXProcess$$Lambda$15/1221027335
1238:             6            144  java.lang.UNIXProcess$ProcessPipeOutputStream
1239:             9            144  java.util.concurrent.CountDownLatch
1240:             6            144  java.util.regex.Pattern$CharProperty$1
1241:             2            144  org.antlr.runtime.RecognizerSharedState
1242:             6            144  org.apache.cassandra.cql3.CFName
1243:             6            144  org.apache.cassandra.cql3.WhereClause
1244:             6            144  org.apache.cassandra.db.filter.DataLimits$Kind
1245:             6            144  org.apache.cassandra.db.view.ViewManager
1246:             3            144  org.apache.cassandra.locator.SimpleStrategy
1247:             3            144  org.apache.cassandra.metrics.CacheMetrics
1248:             6            144  org.apache.cassandra.metrics.SEPMetrics$1
1249:             6            144  org.apache.cassandra.metrics.SEPMetrics$2
1250:             6            144  org.apache.cassandra.metrics.SEPMetrics$3
1251:             6            144  org.apache.cassandra.metrics.SEPMetrics$4
1252:             6            144  org.apache.cassandra.schema.KeyspaceParams
1253:             6            144  org.apache.cassandra.schema.ReplicationParams
1254:             6            144  org.apache.cassandra.service.ActiveRepairService$1
1255:             6            144  org.apache.cassandra.service.ActiveRepairService$2
1256:             6            144  org.apache.cassandra.streaming.StreamSession$State
1257:             6            144  org.codehaus.jackson.annotate.JsonAutoDetect$Visibility
1258:             6            144  org.github.jamm.MemoryMeter$Guess
1259:             6            144  sun.misc.PerfCounter
1260:             6            144  sun.security.ssl.ProtocolVersion
1261:             6            144  sun.security.util.DisabledAlgorithmConstraints$Constraint$Operator
1262:             4            128  [Lcom.google.common.cache.LocalCache$Segment;
1263:             4            128  [Lcom.google.common.collect.MapMakerInternalMap$EntryFactory;
1264:             2            128  [Lorg.apache.cassandra.concurrent.Stage;
1265:             2            128  [Lorg.apache.cassandra.io.sstable.Component$Type;
1266:             2            128  ch.qos.logback.core.rolling.FixedWindowRollingPolicy
1267:             4            128  ch.qos.logback.core.rolling.helper.FileNamePattern
1268:             8            128  com.google.common.cache.LocalCache$AccessQueue
1269:             8            128  com.google.common.cache.LocalCache$StrongValueReference
1270:             4            128  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$ArrayMapping
1271:             2            128  java.io.ExpiringCache$1
1272:             4            128  java.io.ObjectInputStream$HandleTable
1273:             4            128  java.io.ObjectInputStream$PeekInputStream
1274:             4            128  java.lang.UNIXProcess$Platform
1275:             2            128  java.lang.invoke.InvokerBytecodeGenerator
1276:             4            128  java.util.Random
1277:             4            128  java.util.concurrent.ExecutionException
1278:             4            128  net.jpountz.util.Native$OS
1279:             4            128  org.apache.cassandra.cql3.functions.CastFcts$CassandraFunctionWrapper
1280:             4            128  org.apache.cassandra.db.marshal.ReversedType
1281:             1            128  org.apache.cassandra.io.compress.CompressedSequentialWriter
1282:             1            128  org.apache.cassandra.io.sstable.format.big.BigTableWriter
1283:             4            128  org.apache.cassandra.io.util.SequentialWriterOption
1284:             4            128  org.apache.cassandra.locator.PendingRangeMaps
1285:             2            128  org.apache.cassandra.metrics.CASClientRequestMetrics
1286:             8            128  org.apache.cassandra.serializers.MapSerializer$$Lambda$24/2072313080
1287:             8            128  sun.net.www.ParseUtil
1288:             4            128  sun.rmi.transport.LiveRef
1289:             8            128  sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$$Lambda$292/1509453068
1290:             4            128  sun.security.ssl.CipherSuite$PRF
1291:             4            128  sun.security.x509.ExtendedKeyUsageExtension
1292:             3            120  [Lorg.codehaus.jackson.annotate.JsonMethod;
1293:             1            120  [[Ljava.lang.String;
1294:             5            120  ch.qos.logback.core.pattern.parser.TokenStream$TokenizerState
1295:             5            120  ch.qos.logback.core.subst.Token$Type
1296:             5            120  ch.qos.logback.core.util.AggregationType
1297:             3            120  com.google.common.collect.AbstractMapBasedMultimap$AsMap
1298:             5            120  com.sun.org.apache.xerces.internal.util.PropertyState
1299:             5            120  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$State
1300:             5            120  com.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$State
1301:             3            120  java.lang.invoke.BoundMethodHandle$Species_LL
1302:             3            120  java.lang.invoke.MethodHandleImpl$AsVarargsCollector
1303:             5            120  java.util.stream.StreamOpFlag$Type
1304:             3            120  org.apache.cassandra.cache.AutoSavingCache
1305:             5            120  org.apache.cassandra.config.Config$DiskFailurePolicy
1306:             5            120  org.apache.cassandra.cql3.VariableSpecifications
1307:             5            120  org.apache.cassandra.cql3.statements.IndexTarget$Type
1308:             5            120  org.apache.cassandra.db.lifecycle.LogRecord$Status
1309:             5            120  org.apache.cassandra.db.lifecycle.LogRecord$Type
1310:             3            120  org.apache.cassandra.db.lifecycle.LogTransaction$SSTableTidier
1311:             3            120  org.apache.cassandra.index.internal.composites.ClusteringColumnIndex
1312:             5            120  org.apache.cassandra.schema.CompactionParams$Option
1313:             1            120  org.apache.cassandra.service.StorageService
1314:             5            120  org.apache.cassandra.utils.NativeLibrary$OSType
1315:             5            120  org.yaml.snakeyaml.DumperOptions$ScalarStyle
1316:             5            120  sun.misc.FloatingDecimal$PreparedASCIIToBinaryBuffer
1317:             5            120  sun.security.jca.ServiceId
1318:             5            120  sun.security.util.DisabledAlgorithmConstraints
1319:             2            112  [Ljava.lang.invoke.MethodType;
1320:             2            112  [Ljava.security.CryptoPrimitive;
1321:             2            112  [Ljava.util.List;
1322:             2            112  [Lorg.apache.cassandra.auth.Permission;
1323:             2            112  [Lorg.apache.cassandra.db.PartitionPosition;
1324:             3            112  [Lorg.apache.cassandra.transport.ProtocolVersion;
1325:             7            112  com.google.common.util.concurrent.MoreExecutors$ListeningDecorator
1326:             2            112  com.sun.management.GcInfo
1327:             2            112  io.netty.buffer.PooledByteBufAllocator
1328:             7            112  java.util.concurrent.ConcurrentHashMap$EntrySetView
1329:             2            112  org.apache.cassandra.cql3.statements.DeleteStatement
1330:             2            112  org.apache.cassandra.db.compaction.LeveledCompactionStrategy
1331:             2            112  org.apache.cassandra.repair.LocalSyncTask
1332:             7            112  org.apache.cassandra.serializers.ListSerializer
1333:             2            112  org.apache.cassandra.utils.memory.MemtablePool$SubPool
1334:             7            112  sun.security.provider.NativePRNG
1335:             1            104  com.codahale.metrics.ThreadLocalRandom
1336:             1            104  io.netty.channel.epoll.EpollServerSocketChannel
1337:             1            104  org.apache.cassandra.db.ColumnIndex
1338:             1            104  sun.rmi.server.LoaderHandler$Loader
1339:             2             96  [Lcom.google.common.cache.LocalCache$EntryFactory;
1340:             6             96  [Ljava.io.ObjectStreamClass$MemberSignature;
1341:             2             96  [Ljava.util.concurrent.TimeUnit;
1342:             1             96  [Lorg.apache.cassandra.db.compaction.OperationType;
1343:             2             96  [Lorg.apache.cassandra.repair.messages.RepairMessage$Type;
1344:             1             96  [Lorg.yaml.snakeyaml.tokens.Token$ID;
1345:             1             96  [[J
1346:             1             96  ch.qos.logback.classic.LoggerContext
1347:             3             96  ch.qos.logback.classic.pattern.FileOfCallerConverter
1348:             3             96  ch.qos.logback.classic.pattern.LevelConverter
1349:             3             96  ch.qos.logback.classic.pattern.LineOfCallerConverter
1350:             3             96  ch.qos.logback.classic.pattern.LineSeparatorConverter
1351:             3             96  ch.qos.logback.classic.pattern.MessageConverter
1352:             3             96  ch.qos.logback.classic.pattern.ThreadConverter
1353:             3             96  ch.qos.logback.core.joran.action.AppenderRefAction
1354:             4             96  ch.qos.logback.core.pattern.parser.Token
1355:             2             96  ch.qos.logback.core.recovery.ResilientFileOutputStream
1356:             2             96  ch.qos.logback.core.rolling.helper.DateTokenConverter
1357:             4             96  ch.qos.logback.core.subst.Token
1358:             2             96  ch.qos.logback.core.util.InvocationGate
1359:             4             96  com.google.common.cache.LocalCache$WriteQueue$1
1360:             4             96  com.google.common.collect.AbstractIterator$State
1361:             4             96  com.google.common.collect.Iterators$12
1362:             4             96  com.googlecode.concurrentlinkedhashmap.LinkedDeque
1363:             3             96  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$EnumMapping
1364:             2             96  com.sun.jmx.mbeanserver.MBeanIntrospector$MBeanInfoMap
1365:             2             96  com.sun.jmx.mbeanserver.MBeanIntrospector$PerInterfaceMap
1366:             1             96  com.sun.net.ssl.internal.ssl.Provider
1367:             3             96  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$NameMap
1368:             3             96  io.netty.buffer.EmptyByteBuf
1369:             3             96  java.io.ByteArrayInputStream
1370:             6             96  java.io.FileInputStream$1
1371:             4             96  java.io.ObjectOutputStream$ReplaceTable
1372:             6             96  java.lang.UNIXProcess$$Lambda$16/1801942731
1373:             6             96  java.net.Socket$2
1374:             6             96  java.net.Socket$3
1375:             4             96  java.net.URLClassLoader$2
1376:             4             96  java.nio.file.FileVisitResult
1377:             4             96  java.text.Normalizer$Form
1378:             6             96  java.util.LinkedHashMap$LinkedKeySet
1379:             2             96  java.util.concurrent.ArrayBlockingQueue
1380:             3             96  java.util.concurrent.ConcurrentHashMap$ForwardingNode
1381:             3             96  java.util.concurrent.locks.ReentrantLock$FairSync
1382:             4             96  java.util.stream.StreamShape
1383:             4             96  javax.management.NotificationBroadcasterSupport$ListenerInfo
1384:             4             96  org.apache.cassandra.auth.IRoleManager$Option
1385:             4             96  org.apache.cassandra.config.CFMetaData$Flag
1386:             4             96  org.apache.cassandra.config.ColumnDefinition$Kind
1387:             4             96  org.apache.cassandra.config.Config$CommitFailurePolicy
1388:             4             96  org.apache.cassandra.config.Config$DiskAccessMode
1389:             4             96  org.apache.cassandra.config.Config$MemtableAllocationType
1390:             4             96  org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions$InternodeEncryption
1391:             1             96  org.apache.cassandra.cql3.Cql_Parser
1392:             4             96  org.apache.cassandra.db.SystemKeyspace$BootstrapState
1393:             2             96  org.apache.cassandra.db.compaction.LeveledManifest
1394:             4             96  org.apache.cassandra.db.context.CounterContext$Relationship
1395:             4             96  org.apache.cassandra.db.lifecycle.LogTransaction$Obsoletion
1396:             4             96  org.apache.cassandra.dht.Bounds
1397:             4             96  org.apache.cassandra.hints.HintsDispatcher$Callback$Outcome
1398:             4             96  org.apache.cassandra.io.sstable.SSTableRewriter$InvalidateKeys
1399:             4             96  org.apache.cassandra.io.sstable.format.SSTableReader$OpenReason
1400:             4             96  org.apache.cassandra.io.sstable.format.SSTableReadsListener$SkippingReason
1401:             4             96  org.apache.cassandra.io.sstable.metadata.MetadataType
1402:             2             96  org.apache.cassandra.io.util.FileHandle$Builder
1403:             2             96  org.apache.cassandra.locator.LocalStrategy
1404:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$1
1405:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$10
1406:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$11
1407:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$12
1408:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$13
1409:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$14
1410:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$15
1411:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$16
1412:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$2
1413:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$3
1414:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$4
1415:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$5
1416:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$6
1417:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$7
1418:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$8
1419:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$9
1420:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$KeyspaceMetricNameFactory
1421:             6             96  org.apache.cassandra.schema.Functions
1422:             4             96  org.apache.cassandra.schema.SpeculativeRetryParam$Kind
1423:             6             96  org.apache.cassandra.schema.Tables
1424:             6             96  org.apache.cassandra.schema.Views
1425:             4             96  org.apache.cassandra.transport.Event$Type
1426:             1             96  org.apache.cassandra.triggers.CustomClassLoader
1427:             4             96  org.apache.cassandra.utils.AbstractIterator$State
1428:             4             96  org.apache.cassandra.utils.AsymmetricOrdering$Op
1429:             3             96  org.apache.cassandra.utils.NoSpamLogger
1430:             4             96  org.apache.cassandra.utils.SortedBiMultiValMap
1431:             4             96  org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional$State
1432:             2             96  org.codehaus.jackson.map.MapperConfig$Base
1433:             4             96  org.yaml.snakeyaml.nodes.NodeId
1434:             2             96  sun.management.GarbageCollectorImpl
1435:             2             96  sun.management.GcInfoBuilder
1436:             4             96  sun.misc.FormattedFloatingDecimal$Form
1437:             1             96  sun.misc.Launcher$AppClassLoader
1438:             4             96  sun.net.www.MessageHeader
1439:             1             96  sun.nio.ch.ServerSocketChannelImpl
1440:             2             96  sun.nio.cs.StreamEncoder
1441:             6             96  sun.rmi.transport.Transport$$Lambda$295/399097450
1442:             3             96  sun.rmi.transport.Transport$1
1443:             1             96  sun.security.ec.SunEC
1444:             1             96  sun.security.jca.ProviderList$1
1445:             1             96  sun.security.rsa.SunRsaSign
1446:             3             96  sun.security.ssl.ProtocolList
1447:             4             88  [Ljava.util.Map$Entry;
1448:             1             88  [Lnet.jpountz.lz4.LZ4Compressor;
1449:             1             88  [Lorg.apache.cassandra.exceptions.ExceptionCode;
1450:             1             88  [Lsun.security.util.ObjectIdentifier;
1451:             1             88  [[Ljava.lang.Byte;
1452:             1             88  java.util.jar.JarVerifier
1453:             1             88  org.apache.cassandra.concurrent.JMXConfigurableThreadPoolExecutor
1454:             1             88  org.apache.cassandra.db.compaction.CompactionManager$CacheCleanupExecutor
1455:             1             88  org.apache.cassandra.db.compaction.CompactionManager$CompactionExecutor
1456:             1             88  org.apache.cassandra.db.compaction.CompactionManager$ValidationExecutor
1457:             1             88  org.apache.cassandra.gms.Gossiper
1458:             1             88  org.apache.cassandra.io.sstable.IndexSummaryBuilder
1459:             1             88  org.apache.cassandra.io.sstable.metadata.MetadataCollector
1460:             1             88  sun.misc.Launcher$ExtClassLoader
1461:             1             80  [Lio.netty.util.concurrent.SingleThreadEventExecutor;
1462:             2             80  [Ljava.lang.management.MemoryUsage;
1463:             2             80  [Ljava.util.stream.StreamOpFlag$Type;
1464:             5             80  [Lorg.apache.cassandra.config.ColumnDefinition;
1465:             2             80  [Lorg.apache.cassandra.config.Config$DiskFailurePolicy;
1466:             1             80  [Lorg.apache.cassandra.cql3.Operator;
1467:             1             80  [Lorg.apache.cassandra.schema.TableParams$Option;
1468:             2             80  [Lorg.apache.cassandra.transport.messages.ResultMessage$Kind;
1469:             2             80  [Lorg.codehaus.jackson.annotate.JsonAutoDetect$Visibility;
1470:             1             80  [Lsun.security.ssl.CipherSuite$KeyExchange;
1471:             1             80  ch.qos.logback.classic.AsyncAppender
1472:             2             80  ch.qos.logback.classic.filter.ThresholdFilter
1473:             1             80  ch.qos.logback.classic.turbo.ReconfigureOnChangeFilter
1474:             2             80  ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy
1475:             2             80  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$TabularMapping
1476:             1             80  com.sun.jmx.remote.util.ClassLoaderWithRepository
1477:             5             80  com.sun.proxy.$Proxy1
1478:             5             80  io.netty.channel.group.DefaultChannelGroup$1
1479:             2             80  io.netty.channel.unix.Errors$NativeConnectException
1480:             2             80  io.netty.util.Signal
1481:             2             80  java.io.ExpiringCache
1482:             2             80  java.util.Locale$Category
1483:             5             80  java.util.logging.SimpleFormatter
1484:             2             80  java.util.regex.Pattern$Loop
1485:             5             80  javax.security.auth.x500.X500Principal
1486:             1             80  org.apache.cassandra.concurrent.StageManager$TracingExecutor
1487:             1             80  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$SMAwareReconfigureOnChangeFilter
1488:             1             80  org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter
1489:             1             80  org.apache.cassandra.io.sstable.SSTableRewriter
1490:             5             80  org.apache.cassandra.repair.RepairSession$1
1491:             2             80  org.codehaus.jackson.sym.CharsToNameCanonicalizer
1492:             2             80  sun.management.MemoryManagerImpl
1493:             2             80  sun.reflect.UnsafeQualifiedStaticObjectFieldAccessorImpl
1494:             1             80  sun.reflect.misc.MethodUtil
1495:             2             80  sun.rmi.server.LoaderHandler$LoaderEntry
1496:             2             80  sun.rmi.server.UnicastServerRef
1497:             2             80  sun.rmi.server.UnicastServerRef2
1498:             2             80  sun.security.provider.DSAPublicKeyImpl
1499:             5             80  sun.security.util.DisabledAlgorithmConstraints$Constraints
1500:             2             80  sun.util.logging.resources.logging
1501:             1             72  [Ljava.lang.invoke.LambdaFormEditor$Transform$Kind;
1502:             4             72  [Ljava.nio.file.LinkOption;
1503:             3             72  [Ljava.util.concurrent.ConcurrentHashMap$CounterCell;
1504:             1             72  [Ljavax.management.openmbean.SimpleType;
1505:             2             72  [Lsun.security.jca.ProviderConfig;
1506:             1             72  ch.qos.logback.core.ConsoleAppender
1507:             3             72  ch.qos.logback.core.joran.action.NOPAction
1508:             3             72  ch.qos.logback.core.joran.action.PropertyAction
1509:             3             72  ch.qos.logback.core.pattern.FormatInfo
1510:             3             72  ch.qos.logback.core.rolling.helper.CompressionMode
1511:             3             72  ch.qos.logback.core.spi.FilterReply
1512:             3             72  ch.qos.logback.core.subst.Tokenizer$TokenizerState
1513:             3             72  com.github.benmanes.caffeine.cache.AccessOrderDeque
1514:             3             72  com.github.benmanes.caffeine.cache.Caffeine$Strength
1515:             3             72  com.google.common.base.CharMatcher$13
1516:             3             72  com.google.common.base.CharMatcher$RangesMatcher
1517:             3             72  com.google.common.collect.AbstractMapBasedMultimap$KeySet
1518:             1             72  io.netty.channel.DefaultChannelHandlerContext
1519:             1             72  io.netty.channel.DefaultChannelPipeline$HeadContext
1520:             1             72  io.netty.channel.DefaultChannelPipeline$TailContext
1521:             1             72  io.netty.channel.epoll.EpollServerSocketChannelConfig
1522:             3             72  java.io.ObjectStreamClass$ExceptionInfo
1523:             3             72  java.lang.UNIXProcess$LaunchMechanism
1524:             3             72  java.lang.annotation.RetentionPolicy
1525:             3             72  java.nio.file.FileTreeWalker$EventType
1526:             3             72  java.rmi.dgc.VMID
1527:             3             72  java.security.SecurityPermission
1528:             3             72  java.util.Base64$Encoder
1529:             1             72  java.util.ResourceBundle$RBClassLoader
1530:             3             72  java.util.concurrent.atomic.AtomicMarkableReference$Pair
1531:             3             72  java.util.jar.Manifest
1532:             1             72  java.util.logging.LogManager$RootLogger
1533:             1             72  java.util.logging.LogRecord
1534:             3             72  java.util.stream.Collector$Characteristics
1535:             3             72  java.util.stream.MatchOps$MatchKind
1536:             3             72  javax.crypto.CryptoPermissions
1537:             1             72  javax.management.remote.rmi.RMIConnectionImpl$CombinedClassLoader
1538:             1             72  javax.management.remote.rmi.RMIConnectionImpl$CombinedClassLoader$ClassLoaderWrapper
1539:             3             72  javax.security.auth.Subject$SecureSet
1540:             3             72  org.apache.cassandra.auth.DataResource$Level
1541:             3             72  org.apache.cassandra.config.ColumnDefinition$ClusteringOrder
1542:             3             72  org.apache.cassandra.config.Config$InternodeCompression
1543:             3             72  org.apache.cassandra.config.Config$UserFunctionTimeoutPolicy
1544:             3             72  org.apache.cassandra.config.ReadRepairDecision
1545:             3             72  org.apache.cassandra.cql3.AssignmentTestable$TestResult
1546:             1             72  org.apache.cassandra.cql3.Cql_Lexer
1547:             3             72  org.apache.cassandra.cql3.ResultSet$Flag
1548:             3             72  org.apache.cassandra.db.Conflicts$Resolution
1549:             3             72  org.apache.cassandra.db.Directories$FileType
1550:             3             72  org.apache.cassandra.db.commitlog.CommitLogSegment$CDCState
1551:             1             72  org.apache.cassandra.db.compaction.CompactionIterator
1552:             3             72  org.apache.cassandra.db.lifecycle.SSTableSet
1553:             3             72  org.apache.cassandra.db.marshal.AbstractType$ComparisonType
1554:             3             72  org.apache.cassandra.db.monitoring.MonitoringState
1555:             3             72  org.apache.cassandra.db.rows.SerializationHelper$Flag
1556:             1             72  org.apache.cassandra.io.util.SequentialWriter
1557:             3             72  org.apache.cassandra.locator.TokenMetadata$Topology
1558:             3             72  org.apache.cassandra.metrics.CacheMetrics$1
1559:             3             72  org.apache.cassandra.metrics.CacheMetrics$6
1560:             3             72  org.apache.cassandra.metrics.CacheMetrics$7
1561:             3             72  org.apache.cassandra.metrics.StreamingMetrics
1562:             3             72  org.apache.cassandra.repair.RepairParallelism
1563:             3             72  org.apache.cassandra.repair.SystemDistributedKeyspace$RepairState
1564:             3             72  org.apache.cassandra.repair.messages.ValidationComplete
1565:             3             72  org.apache.cassandra.schema.CompactionParams$TombstoneOption
1566:             3             72  org.apache.cassandra.schema.IndexMetadata$Kind
1567:             3             72  org.apache.cassandra.service.CacheService$CacheType
1568:             3             72  org.apache.cassandra.streaming.StreamEvent$Type
1569:             3             72  org.apache.cassandra.transport.Server$LatestEvent
1570:             3             72  org.apache.cassandra.utils.BiMultiValMap
1571:             3             72  org.apache.cassandra.utils.NoSpamLogger$Level
1572:             3             72  org.apache.cassandra.utils.memory.MemtableAllocator$LifeCycle
1573:             1             72  org.apache.commons.lang3.builder.ToStringStyle$DefaultToStringStyle
1574:             1             72  org.apache.commons.lang3.builder.ToStringStyle$MultiLineToStringStyle
1575:             1             72  org.apache.commons.lang3.builder.ToStringStyle$NoFieldNameToStringStyle
1576:             1             72  org.apache.commons.lang3.builder.ToStringStyle$ShortPrefixToStringStyle
1577:             1             72  org.apache.commons.lang3.builder.ToStringStyle$SimpleToStringStyle
1578:             1             72  org.apache.thrift.server.TThreadPoolServer$Args
1579:             3             72  org.yaml.snakeyaml.DumperOptions$FlowStyle
1580:             3             72  org.yaml.snakeyaml.DumperOptions$LineBreak
1581:             3             72  org.yaml.snakeyaml.introspector.BeanAccess
1582:             3             72  sun.misc.FloatingDecimal$ExceptionalBinaryToASCIIBuffer
1583:             3             72  sun.misc.ObjectInputFilter$Status
1584:             3             72  sun.misc.Signal
1585:             3             72  sun.nio.fs.UnixFileAttributeViews$Basic
1586:             3             72  sun.rmi.transport.SequenceEntry
1587:             3             72  sun.security.provider.NativePRNG$Variant
1588:             3             72  sun.security.ssl.CipherSuite$CipherType
1589:             3             72  sun.security.ssl.CipherSuiteList
1590:             1             72  sun.util.locale.provider.JRELocaleProviderAdapter
1591:             3             72  sun.util.resources.ParallelListResourceBundle$KeySet
1592:             2             64  [Ljava.lang.UNIXProcess$LaunchMechanism;
1593:             2             64  [Ljava.lang.annotation.RetentionPolicy;
1594:             3             64  [Ljava.security.CodeSigner;
1595:             3             64  [Ljava.security.cert.X509Certificate;
1596:             2             64  [Ljava.util.stream.Collector$Characteristics;
1597:             2             64  [Lorg.apache.cassandra.config.CFMetaData$Flag;
1598:             2             64  [Lorg.apache.cassandra.config.ColumnDefinition$ClusteringOrder;
1599:             2             64  [Lorg.apache.cassandra.config.ColumnDefinition$Kind;
1600:             2             64  [Lorg.apache.cassandra.config.Config$CommitFailurePolicy;
1601:             2             64  [Lorg.apache.cassandra.config.Config$InternodeCompression;
1602:             2             64  [Lorg.apache.cassandra.config.Config$MemtableAllocationType;
1603:             2             64  [Lorg.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions$InternodeEncryption;
1604:             2             64  [Lorg.apache.cassandra.cql3.ResultSet$Flag;
1605:             2             64  [Lorg.apache.cassandra.db.SystemKeyspace$BootstrapState;
1606:             2             64  [Lorg.apache.cassandra.io.sstable.metadata.MetadataType;
1607:             2             64  [Lorg.apache.cassandra.schema.CompactionParams$TombstoneOption;
1608:             2             64  [Lorg.apache.cassandra.schema.IndexMetadata$Kind;
1609:             2             64  [Lorg.apache.cassandra.transport.Event$Type;
1610:             2             64  [Lorg.yaml.snakeyaml.nodes.NodeId;
1611:             2             64  ch.qos.logback.classic.joran.action.LevelAction
1612:             2             64  ch.qos.logback.core.joran.spi.ConsoleTarget
1613:             2             64  ch.qos.logback.core.rolling.helper.Compressor
1614:             2             64  ch.qos.logback.core.rolling.helper.IntegerTokenConverter
1615:             4             64  ch.qos.logback.core.spi.FilterAttachableImpl
1616:             1             64  com.clearspring.analytics.stream.cardinality.HyperLogLogPlus
1617:             2             64  com.github.benmanes.caffeine.cache.References$WeakKeyReference
1618:             1             64  com.github.benmanes.caffeine.cache.stats.CacheStats
1619:             1             64  com.google.common.cache.CacheStats
1620:             4             64  com.google.common.cache.LocalCache$WriteQueue
1621:             2             64  com.google.common.util.concurrent.Striped$LargeLazyStriped
1622:             4             64  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$BoundedEntryWeigher
1623:             2             64  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$CollectionMapping
1624:             1             64  com.sun.jmx.remote.internal.ArrayNotificationBuffer
1625:             2             64  com.sun.management.GarbageCollectionNotificationInfo
1626:             2             64  com.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$Property
1627:             1             64  io.netty.channel.ChannelOutboundBuffer
1628:             4             64  io.netty.util.concurrent.FastThreadLocal
1629:             4             64  java.io.ObjectInputStream$ValidationList
1630:             2             64  java.io.PrintStream
1631:             2             64  java.lang.ClassValue$Entry
1632:             2             64  java.lang.NoSuchMethodError
1633:             2             64  java.lang.VirtualMachineError
1634:             2             64  java.lang.ref.ReferenceQueue$Null
1635:             2             64  java.net.Inet6Address
1636:             2             64  java.net.Inet6Address$Inet6AddressHolder
1637:             2             64  java.util.ResourceBundle$Control$1
1638:             2             64  java.util.concurrent.ConcurrentLinkedQueue$Itr
1639:             2             64  java.util.jar.Manifest$FastInputStream
1640:             1             64  javax.management.remote.rmi.RMIConnectionImpl
1641:             1             64  javax.management.remote.rmi.RMIConnectorServer
1642:             4             64  javax.security.auth.login.AppConfigurationEntry$LoginModuleControlFlag
1643:             4             64  org.apache.cassandra.concurrent.SEPWorker$Work
1644:             2             64  org.apache.cassandra.cql3.functions.TokenFct
1645:             2             64  org.apache.cassandra.db.commitlog.CommitLogDescriptor
1646:             2             64  org.apache.cassandra.db.lifecycle.LogFile
1647:             2             64  org.apache.cassandra.db.lifecycle.LogTransaction
1648:             2             64  org.apache.cassandra.io.sstable.format.SSTableFormat$Type
1649:             2             64  org.apache.cassandra.io.sstable.metadata.MetadataCollector$MinMaxIntTracker
1650:             2             64  org.apache.cassandra.io.util.SafeMemoryWriter
1651:             1             64  org.apache.cassandra.locator.DynamicEndpointSnitch
1652:             1             64  org.apache.cassandra.metrics.ViewWriteMetrics
1653:             1             64  org.apache.cassandra.net.MessagingService
1654:             2             64  org.apache.cassandra.service.ClientState
1655:             2             64  org.apache.cassandra.service.GCInspector$GCState
1656:             1             64  org.apache.cassandra.service.GCInspector$State
1657:             1             64  org.apache.cassandra.thrift.CustomTThreadPoolServer
1658:             1             64  org.apache.cassandra.utils.SigarLibrary
1659:             4             64  org.apache.cassandra.utils.SortedBiMultiValMap$1
1660:             4             64  org.codehaus.jackson.map.introspect.AnnotationMap
1661:             4             64  sun.net.www.protocol.jar.Handler
1662:             4             64  sun.rmi.server.MarshalOutputStream$1
1663:             2             64  sun.rmi.transport.DGCImpl$LeaseInfo
1664:             2             64  sun.rmi.transport.tcp.TCPTransport
1665:             2             64  sun.security.ssl.EphemeralKeyManager$EphemeralKeyPair
1666:             2             64  sun.security.ssl.SSLSessionContextImpl
1667:             2             64  sun.security.x509.PrivateKeyUsageExtension
1668:             2             64  sun.security.x509.SubjectAlternativeNameExtension
1669:             2             64  sun.util.locale.provider.LocaleServiceProviderPool
1670:             1             56  [Lcom.sun.org.apache.xerces.internal.impl.XMLScanner$NameType;
1671:             1             56  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$Limit;
1672:             1             56  [Ljava.lang.Runnable;
1673:             1             56  [Ljava.nio.file.StandardOpenOption;
1674:             2             56  [Ljdk.internal.org.objectweb.asm.Type;
1675:             1             56  [Lorg.apache.commons.lang3.JavaVersion;
1676:             1             56  [Lorg.codehaus.jackson.JsonParser$Feature;
1677:             1             56  [Lorg.yaml.snakeyaml.events.Event$ID;
1678:             1             56  [Lsun.util.logging.PlatformLogger$Level;
1679:             1             56  [[I
1680:             1             56  com.sun.jmx.remote.internal.ServerNotifForwarder
1681:             1             56  io.netty.util.concurrent.ScheduledFutureTask
1682:             1             56  java.lang.invoke.LambdaFormEditor$Transform
1683:             1             56  java.util.concurrent.ConcurrentHashMap$KeyIterator
1684:             1             56  java.util.logging.ConsoleHandler
1685:             1             56  java.util.logging.LogManager
1686:             1             56  javax.management.remote.JMXConnectionNotification
1687:             1             56  javax.management.remote.rmi.RMIJRMPServerImpl
1688:             1             56  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache
1689:             1             56  org.apache.cassandra.config.EncryptionOptions$ClientEncryptionOptions
1690:             1             56  org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions
1691:             1             56  org.apache.cassandra.cql3.CqlLexer$DFA1
1692:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA14
1693:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA22
1694:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA24
1695:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA28
1696:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA30
1697:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA37
1698:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA44
1699:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA9
1700:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA1
1701:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA15
1702:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA153
1703:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA154
1704:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA172
1705:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA174
1706:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA176
1707:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA178
1708:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA181
1709:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA189
1710:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA194
1711:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA195
1712:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA204
1713:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA44
1714:             1             56  org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard
1715:             1             56  org.apache.cassandra.db.commitlog.PeriodicCommitLogService
1716:             1             56  org.apache.cassandra.db.compaction.CompactionController
1717:             1             56  org.apache.cassandra.db.lifecycle.LifecycleTransaction
1718:             1             56  org.apache.cassandra.io.compress.CompressionMetadata$Writer
1719:             1             56  org.apache.cassandra.metrics.CacheMissMetrics
1720:             1             56  org.codehaus.jackson.map.ObjectMapper
1721:             1             56  org.codehaus.jackson.map.ser.StdSerializerProvider
1722:             1             56  org.codehaus.jackson.sym.BytesToNameCanonicalizer
1723:             1             56  org.hyperic.sigar.SigarLoader
1724:             1             56  sun.rmi.runtime.Log$InternalStreamHandler
1725:             1             48  [Lcom.sun.beans.util.Cache$CacheEntry;
1726:             1             48  [Lcom.sun.management.VMOption$Origin;
1727:             1             48  [Ljava.beans.WeakIdentityMap$Entry;
1728:             3             48  [Ljava.lang.annotation.Annotation;
1729:             1             48  [Ljava.lang.invoke.MethodHandleImpl$Intrinsic;
1730:             1             48  [Ljava.math.RoundingMode;
1731:             2             48  [Ljava.nio.file.FileVisitOption;
1732:             1             48  [Ljdk.net.SocketFlow$Status;
1733:             2             48  [Lorg.apache.cassandra.config.Config$CommitLogSync;
1734:             1             48  [Lorg.apache.cassandra.cql3.Constants$Type;
1735:             1             48  [Lorg.apache.cassandra.db.ClusteringPrefix$Kind;
1736:             1             48  [Lorg.apache.cassandra.db.Directories$FileAction;
1737:             1             48  [Lorg.apache.cassandra.db.WriteType;
1738:             2             48  [Lorg.apache.cassandra.exceptions.RequestFailureReason;
1739:             2             48  [Lorg.apache.cassandra.net.RateBasedBackPressure$Flow;
1740:             1             48  [Lorg.apache.cassandra.serializers.UTF8Serializer$UTF8Validator$State;
1741:             1             48  [Lorg.apache.cassandra.service.StorageService$Mode;
1742:             1             48  [Lorg.apache.cassandra.streaming.messages.StreamMessage$Type;
1743:             1             48  [Lorg.apache.cassandra.utils.progress.ProgressEventType;
1744:             1             48  [Lorg.codehaus.jackson.JsonGenerator$Feature;
1745:             1             48  [Lsun.security.x509.NetscapeCertTypeExtension$MapEntry;
1746:             1             48  ch.qos.logback.classic.jmx.JMXConfigurator
1747:             3             48  ch.qos.logback.classic.pattern.EnsureExceptionHandling
1748:             3             48  ch.qos.logback.classic.spi.PackagingDataCalculator
1749:             1             48  ch.qos.logback.core.joran.action.DefinePropertyAction
1750:             1             48  ch.qos.logback.core.joran.spi.InterpretationContext
1751:             1             48  ch.qos.logback.core.joran.spi.Interpreter
1752:             2             48  ch.qos.logback.core.rolling.helper.RenameUtil
1753:             3             48  ch.qos.logback.core.spi.LogbackLock
1754:             2             48  ch.qos.logback.core.subst.Node$Type
1755:             2             48  ch.qos.logback.core.util.FileSize
1756:             2             48  com.clearspring.analytics.stream.cardinality.HyperLogLogPlus$Format
1757:             3             48  com.google.common.cache.LocalCache$LocalLoadingCache
1758:             1             48  com.google.common.collect.EmptyImmutableListMultimap
1759:             2             48  com.google.common.collect.HashBiMap$Inverse
1760:             1             48  com.google.common.collect.ImmutableListMultimap
1761:             2             48  com.google.common.collect.ImmutableMultimap$Values
1762:             2             48  com.sun.jmx.mbeanserver.ClassLoaderRepositorySupport$LoaderEntry
1763:             1             48  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$Mappings
1764:             2             48  com.sun.jmx.mbeanserver.WeakIdentityHashMap
1765:             2             48  com.sun.jmx.remote.internal.ServerNotifForwarder$IdAndFilter
1766:             1             48  com.sun.jna.NativeLibrary
1767:             3             48  com.sun.org.apache.xerces.internal.impl.dv.dtd.ListDatatypeValidator
1768:             2             48  io.netty.buffer.PooledByteBufAllocator$PoolThreadLocalCache
1769:             2             48  io.netty.channel.VoidChannelPromise
1770:             2             48  io.netty.util.Recycler$2
1771:             2             48  io.netty.util.UniqueName
1772:             1             48  io.netty.util.concurrent.GlobalEventExecutor
1773:             3             48  io.netty.util.internal.TypeParameterMatcher$ReflectiveMatcher
1774:             2             48  java.io.ByteArrayOutputStream
1775:             2             48  java.io.File$PathStatus
1776:             3             48  java.io.FileOutputStream$1
1777:             2             48  java.io.OutputStreamWriter
1778:             2             48  java.io.SerialCallbackContext
1779:             3             48  java.lang.Boolean
1780:             3             48  java.lang.Float
1781:             3             48  java.lang.InheritableThreadLocal
1782:             1             48  java.lang.invoke.BoundMethodHandle$Species_L4
1783:             2             48  java.lang.invoke.ConstantCallSite
1784:             2             48  java.lang.invoke.InfoFromMemberName
1785:             2             48  java.lang.invoke.InnerClassLambdaMetafactory$ForwardingMethodGenerator
1786:             2             48  java.lang.management.MemoryType
1787:             2             48  java.lang.reflect.ReflectPermission
1788:             2             48  java.net.InetAddress$Cache
1789:             2             48  java.net.InetAddress$Cache$Type
1790:             2             48  java.net.InetAddress$CacheEntry
1791:             1             48  java.net.NetworkInterface
1792:             2             48  java.net.ServerSocket
1793:             2             48  java.net.SocketPermissionCollection
1794:             2             48  java.net.StandardProtocolFamily
1795:             3             48  java.nio.channels.FileChannel$MapMode
1796:             2             48  java.nio.charset.CoderResult
1797:             3             48  java.nio.charset.CodingErrorAction
1798:             2             48  java.rmi.dgc.Lease
1799:             2             48  java.security.AllPermissionCollection
1800:             3             48  java.text.AttributedCharacterIterator$Attribute
1801:             3             48  java.util.Base64$Decoder
1802:             2             48  java.util.PropertyPermissionCollection
1803:             3             48  java.util.TreeMap$EntrySet
1804:             2             48  java.util.concurrent.Executors$DefaultThreadFactory
1805:             3             48  java.util.concurrent.atomic.AtomicMarkableReference
1806:             2             48  java.util.logging.Logger$LoggerBundle
1807:             1             48  java.util.regex.Pattern$GroupCurly
1808:             2             48  java.util.regex.Pattern$Prolog
1809:             2             48  javax.management.MBeanServerInvocationHandler
1810:             1             48  javax.management.remote.rmi.RMIConnectionImpl$RMIServerCommunicatorAdmin
1811:             1             48  javax.security.auth.SubjectDomainCombiner$WeakKeyValueMap
1812:             1             48  org.antlr.runtime.ANTLRStringStream
1813:             2             48  org.apache.cassandra.cache.AutoSavingCache$2
1814:             2             48  org.apache.cassandra.config.Config$CommitLogSync
1815:             2             48  org.apache.cassandra.config.Config$DiskOptimizationStrategy
1816:             2             48  org.apache.cassandra.config.ParameterizedClass
1817:             2             48  org.apache.cassandra.cql3.Sets$Marker
1818:             2             48  org.apache.cassandra.cql3.Sets$Setter
1819:             2             48  org.apache.cassandra.cql3.functions.FunctionCall
1820:             2             48  org.apache.cassandra.cql3.statements.Bound
1821:             2             48  org.apache.cassandra.db.Directories$OnTxnErr
1822:             2             48  org.apache.cassandra.db.Memtable$LastCommitLogPosition
1823:             2             48  org.apache.cassandra.db.ReadCommand$Kind
1824:             2             48  org.apache.cassandra.db.aggregation.AggregationSpecification$Kind
1825:             1             48  org.apache.cassandra.db.commitlog.CommitLogArchiver
1826:             1             48  org.apache.cassandra.db.compaction.CompactionInfo
1827:             2             48  org.apache.cassandra.db.filter.ClusteringIndexFilter$Kind
1828:             2             48  org.apache.cassandra.db.lifecycle.LifecycleTransaction$State
1829:             2             48  org.apache.cassandra.db.lifecycle.LogReplica
1830:             2             48  org.apache.cassandra.db.rows.Unfiltered$Kind
1831:             2             48  org.apache.cassandra.exceptions.RequestFailureReason
1832:             1             48  org.apache.cassandra.gms.FailureDetector
1833:             2             48  org.apache.cassandra.hints.HintsDispatcher$Action
1834:             1             48  org.apache.cassandra.hints.HintsService
1835:             2             48  org.apache.cassandra.io.sstable.format.SSTableReadsListener$SelectionReason
1836:             1             48  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter
1837:             1             48  org.apache.cassandra.io.sstable.metadata.MetadataCollector$MinMaxLongTracker
1838:             2             48  org.apache.cassandra.io.util.NIODataInputStream
1839:             1             48  org.apache.cassandra.locator.NetworkTopologyStrategy
1840:             3             48  org.apache.cassandra.metrics.CacheMetrics$2
1841:             3             48  org.apache.cassandra.metrics.CacheMetrics$3
1842:             3             48  org.apache.cassandra.metrics.CacheMetrics$4
1843:             3             48  org.apache.cassandra.metrics.CacheMetrics$5
1844:             2             48  org.apache.cassandra.metrics.TableMetrics$Sampler
1845:             1             48  org.apache.cassandra.net.MessagingService$2
1846:             1             48  org.apache.cassandra.net.RateBasedBackPressure
1847:             2             48  org.apache.cassandra.net.RateBasedBackPressure$Flow
1848:             1             48  org.apache.cassandra.repair.messages.RepairOption
1849:             2             48  org.apache.cassandra.schema.CachingParams$Option
1850:             2             48  org.apache.cassandra.schema.KeyspaceParams$Option
1851:             1             48  org.apache.cassandra.service.ActiveRepairService$ParentRepairSession
1852:             2             48  org.apache.cassandra.streaming.ProgressInfo$Direction
1853:             2             48  org.apache.cassandra.transport.Event$StatusChange$Status
1854:             2             48  org.apache.cassandra.transport.Message$Direction
1855:             2             48  org.apache.cassandra.utils.ChecksumType$3
1856:             2             48  org.apache.cassandra.utils.Throwables$FileOpType
1857:             2             48  org.apache.cassandra.utils.btree.BTree$Dir
1858:             2             48  org.apache.cassandra.utils.concurrent.WaitQueue$RegisteredSignal
1859:             2             48  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotE
1860:             2             48  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotK
1861:             1             48  org.codehaus.jackson.JsonFactory
1862:             1             48  org.codehaus.jackson.map.DeserializationConfig
1863:             1             48  org.codehaus.jackson.map.SerializationConfig
1864:             2             48  org.codehaus.jackson.map.deser.std.CalendarDeserializer
1865:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$BooleanDeserializer
1866:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$ByteDeserializer
1867:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$CharacterDeserializer
1868:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$DoubleDeserializer
1869:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$FloatDeserializer
1870:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$IntegerDeserializer
1871:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$LongDeserializer
1872:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$ShortDeserializer
1873:             2             48  org.codehaus.jackson.map.ser.StdSerializers$BooleanSerializer
1874:             1             48  org.hyperic.sigar.FileSystemMap
1875:             1             48  org.hyperic.sigar.Sigar
1876:             2             48  sun.management.ManagementFactoryHelper$1
1877:             2             48  sun.misc.NativeSignalHandler
1878:             2             48  sun.misc.URLClassPath$FileLoader
1879:             3             48  sun.nio.fs.UnixFileAttributes$UnixAsBasicFileAttributes
1880:             2             48  sun.rmi.server.UnicastServerRef$1
1881:             3             48  sun.rmi.server.WeakClassHashMap$ValueCell
1882:             2             48  sun.security.jca.ProviderList
1883:             2             48  sun.security.jca.ProviderList$3
1884:             2             48  sun.security.provider.DSAParameters
1885:             2             48  sun.security.ssl.SSLAlgorithmConstraints
1886:             3             48  sun.security.util.AlgorithmDecomposer
1887:             2             48  sun.security.util.DisabledAlgorithmConstraints$UsageConstraint
1888:             2             48  sun.security.util.DisabledAlgorithmConstraints$jdkCAConstraint
1889:             3             48  sun.security.x509.RFC822Name
1890:             3             48  sun.text.normalizer.NormalizerBase$QuickCheckResult
1891:             1             48  sun.text.resources.FormatData
1892:             1             48  sun.text.resources.en.FormatData_en
1893:             1             48  sun.text.resources.en.FormatData_en_US
1894:             1             40  [Lch.qos.logback.core.pattern.parser.TokenStream$TokenizerState;
1895:             1             40  [Lch.qos.logback.core.subst.Token$Type;
1896:             1             40  [Lch.qos.logback.core.util.AggregationType;
1897:             1             40  [Lcom.google.common.collect.SortedLists$KeyPresentBehavior;
1898:             2             40  [Lcom.sun.jmx.mbeanserver.ClassLoaderRepositorySupport$LoaderEntry;
1899:             1             40  [Lcom.sun.org.apache.xerces.internal.util.Status;
1900:             1             40  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$State;
1901:             1             40  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$State;
1902:             1             40  [Ljava.lang.management.MemoryPoolMXBean;
1903:             2             40  [Ljava.util.logging.Handler;
1904:             1             40  [Ljava.util.stream.StreamOpFlag;
1905:             1             40  [Lorg.apache.cassandra.cql3.statements.IndexTarget$Type;
1906:             1             40  [Lorg.apache.cassandra.db.filter.DataLimits$Kind;
1907:             1             40  [Lorg.apache.cassandra.db.lifecycle.LogRecord$Type;
1908:             1             40  [Lorg.apache.cassandra.schema.CompactionParams$Option;
1909:             1             40  [Lorg.apache.cassandra.streaming.StreamSession$State;
1910:             1             40  [Lorg.apache.cassandra.utils.NativeLibrary$OSType;
1911:             1             40  [Lorg.github.jamm.MemoryMeter$Guess;
1912:             1             40  [Lorg.yaml.snakeyaml.DumperOptions$ScalarStyle;
1913:             1             40  [Lsun.security.jca.ServiceId;
1914:             1             40  [Lsun.security.util.DisabledAlgorithmConstraints$Constraint$Operator;
1915:             1             40  [Lsun.util.locale.provider.LocaleProviderAdapter$Type;
1916:             1             40  [[Ljava.lang.invoke.LambdaForm$Name;
1917:             1             40  ch.qos.logback.core.BasicStatusManager
1918:             1             40  ch.qos.logback.core.joran.spi.ConfigurationWatchList
1919:             1             40  com.google.common.collect.AbstractMapBasedMultimap$2
1920:             1             40  com.google.common.collect.AbstractMapBasedMultimap$WrappedSet
1921:             1             40  com.google.common.collect.EmptyImmutableSortedMap
1922:             1             40  com.sun.beans.finder.MethodFinder$1
1923:             1             40  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor
1924:             1             40  com.sun.jmx.mbeanserver.JmxMBeanServer
1925:             1             40  com.sun.jmx.mbeanserver.MBeanServerDelegateImpl
1926:             1             40  io.netty.channel.AbstractChannel$CloseFuture
1927:             1             40  io.netty.channel.DefaultChannelPipeline
1928:             1             40  io.netty.channel.epoll.AbstractEpollServerChannel$EpollServerSocketUnsafe
1929:             1             40  java.beans.WeakIdentityMap$Entry
1930:             1             40  java.lang.reflect.Proxy$Key2
1931:             1             40  java.rmi.NoSuchObjectException
1932:             1             40  java.util.ResourceBundle$1
1933:             1             40  javax.crypto.CryptoAllPermission
1934:             1             40  net.jpountz.lz4.LZ4Factory
1935:             1             40  org.antlr.runtime.CommonTokenStream
1936:             1             40  org.apache.cassandra.concurrent.SharedExecutorPool
1937:             1             40  org.apache.cassandra.config.TransparentDataEncryptionOptions
1938:             1             40  org.apache.cassandra.cql3.CqlLexer
1939:             1             40  org.apache.cassandra.db.commitlog.CommitLog
1940:             1             40  org.apache.cassandra.db.compaction.CompactionTask
1941:             1             40  org.apache.cassandra.exceptions.RepairException
1942:             1             40  org.apache.cassandra.io.sstable.format.big.BigTableWriter$TransactionalProxy
1943:             1             40  org.apache.cassandra.locator.GossipingPropertyFileSnitch
1944:             1             40  org.apache.cassandra.net.MessagingService$1
1945:             1             40  org.apache.cassandra.net.MessagingService$3
1946:             1             40  org.apache.cassandra.streaming.management.StreamEventJMXNotifier
1947:             1             40  org.apache.cassandra.transport.Server
1948:             1             40  org.apache.cassandra.utils.NoSpamLogger$NoSpamLogStatement
1949:             1             40  org.apache.cassandra.utils.memory.SlabPool
1950:             1             40  org.codehaus.jackson.map.util.StdDateFormat
1951:             1             40  sun.management.DiagnosticCommandImpl
1952:             1             40  sun.management.MappedMXBeanType$CompositeDataMXBeanType
1953:             1             40  sun.management.MappedMXBeanType$MapMXBeanType
1954:             1             40  sun.nio.cs.StandardCharsets$Aliases
1955:             1             40  sun.nio.cs.StandardCharsets$Cache
1956:             1             40  sun.nio.cs.StandardCharsets$Classes
1957:             1             40  sun.security.ssl.SSLContextImpl$DefaultSSLContext
1958:             1             32  [Lch.qos.logback.core.rolling.helper.CompressionMode;
1959:             1             32  [Lch.qos.logback.core.spi.FilterReply;
1960:             1             32  [Lch.qos.logback.core.subst.Tokenizer$TokenizerState;
1961:             1             32  [Lcom.github.benmanes.caffeine.cache.Caffeine$Strength;
1962:             1             32  [Lcom.google.common.base.Predicates$ObjectPredicate;
1963:             1             32  [Lcom.google.common.cache.LocalCache$Strength;
1964:             1             32  [Lcom.google.common.collect.AbstractIterator$State;
1965:             1             32  [Lcom.google.common.collect.MapMakerInternalMap$Strength;
1966:             1             32  [Lcom.google.common.collect.SortedLists$KeyAbsentBehavior;
1967:             1             32  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus;
1968:             1             32  [Lcom.sun.beans.util.Cache$Kind;
1969:             1             32  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$NameMap;
1970:             2             32  [Ljava.lang.Enum;
1971:             1             32  [Ljava.lang.OutOfMemoryError;
1972:             1             32  [Ljava.lang.ThreadGroup;
1973:             1             32  [Ljava.lang.UNIXProcess$Platform;
1974:             1             32  [Ljava.lang.management.MemoryManagerMXBean;
1975:             1             32  [Ljava.nio.file.FileTreeWalker$EventType;
1976:             1             32  [Ljava.nio.file.FileVisitResult;
1977:             1             32  [Ljava.text.Normalizer$Form;
1978:             1             32  [Ljava.util.concurrent.atomic.AtomicReference;
1979:             1             32  [Ljava.util.stream.MatchOps$MatchKind;
1980:             1             32  [Ljava.util.stream.StreamShape;
1981:             1             32  [Lnet.jpountz.util.Native$OS;
1982:             1             32  [Lorg.apache.cassandra.auth.DataResource$Level;
1983:             1             32  [Lorg.apache.cassandra.auth.IRoleManager$Option;
1984:             1             32  [Lorg.apache.cassandra.config.Config$DiskAccessMode;
1985:             1             32  [Lorg.apache.cassandra.config.Config$UserFunctionTimeoutPolicy;
1986:             1             32  [Lorg.apache.cassandra.config.ReadRepairDecision;
1987:             1             32  [Lorg.apache.cassandra.cql3.AssignmentTestable$TestResult;
1988:             1             32  [Lorg.apache.cassandra.cql3.statements.StatementType;
1989:             1             32  [Lorg.apache.cassandra.db.Conflicts$Resolution;
1990:             1             32  [Lorg.apache.cassandra.db.Directories$FileType;
1991:             1             32  [Lorg.apache.cassandra.db.commitlog.CommitLogSegment$CDCState;
1992:             1             32  [Lorg.apache.cassandra.db.context.CounterContext$Relationship;
1993:             1             32  [Lorg.apache.cassandra.db.lifecycle.SSTableSet;
1994:             1             32  [Lorg.apache.cassandra.db.marshal.AbstractType$ComparisonType;
1995:             1             32  [Lorg.apache.cassandra.db.marshal.CollectionType$Kind;
1996:             1             32  [Lorg.apache.cassandra.db.monitoring.MonitoringState;
1997:             1             32  [Lorg.apache.cassandra.db.rows.SerializationHelper$Flag;
1998:             1             32  [Lorg.apache.cassandra.hints.HintsDispatcher$Callback$Outcome;
1999:             1             32  [Lorg.apache.cassandra.io.sstable.format.SSTableReader$OpenReason;
2000:             1             32  [Lorg.apache.cassandra.io.sstable.format.SSTableReadsListener$SkippingReason;
2001:             1             32  [Lorg.apache.cassandra.repair.RepairParallelism;
2002:             1             32  [Lorg.apache.cassandra.repair.SystemDistributedKeyspace$RepairState;
2003:             1             32  [Lorg.apache.cassandra.schema.SpeculativeRetryParam$Kind;
2004:             1             32  [Lorg.apache.cassandra.service.CacheService$CacheType;
2005:             1             32  [Lorg.apache.cassandra.streaming.StreamEvent$Type;
2006:             1             32  [Lorg.apache.cassandra.utils.AbstractIterator$State;
2007:             1             32  [Lorg.apache.cassandra.utils.AsymmetricOrdering$Op;
2008:             1             32  [Lorg.apache.cassandra.utils.NoSpamLogger$Level;
2009:             1             32  [Lorg.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional$State;
2010:             1             32  [Lorg.apache.cassandra.utils.memory.MemtableAllocator$LifeCycle;
2011:             2             32  [Lorg.codehaus.jackson.type.JavaType;
2012:             1             32  [Lorg.yaml.snakeyaml.DumperOptions$FlowStyle;
2013:             1             32  [Lorg.yaml.snakeyaml.DumperOptions$LineBreak;
2014:             1             32  [Lorg.yaml.snakeyaml.introspector.BeanAccess;
2015:             1             32  [Lsun.misc.FormattedFloatingDecimal$Form;
2016:             1             32  [Lsun.misc.ObjectInputFilter$Status;
2017:             1             32  [Lsun.security.provider.NativePRNG$Variant;
2018:             1             32  [Lsun.security.ssl.CipherSuite$CipherType;
2019:             1             32  [Lsun.security.ssl.CipherSuite$PRF;
2020:             1             32  [[Lcom.google.common.collect.MapMakerInternalMap$EntryFactory;
2021:             1             32  ch.qos.logback.classic.joran.JoranConfigurator
2022:             1             32  ch.qos.logback.classic.joran.action.ConfigurationAction
2023:             1             32  ch.qos.logback.classic.joran.action.EvaluatorAction
2024:             1             32  ch.qos.logback.classic.joran.action.LoggerAction
2025:             1             32  ch.qos.logback.classic.joran.action.LoggerContextListenerAction
2026:             1             32  ch.qos.logback.classic.joran.action.ReceiverAction
2027:             1             32  ch.qos.logback.classic.joran.action.RootLoggerAction
2028:             1             32  ch.qos.logback.classic.sift.SiftAction
2029:             1             32  ch.qos.logback.classic.spi.LoggerContextVO
2030:             1             32  ch.qos.logback.core.helpers.CyclicBuffer
2031:             1             32  ch.qos.logback.core.joran.action.AppenderAction
2032:             1             32  ch.qos.logback.core.joran.action.ConversionRuleAction
2033:             1             32  ch.qos.logback.core.joran.action.IncludeAction
2034:             1             32  ch.qos.logback.core.joran.action.NestedBasicPropertyIA
2035:             1             32  ch.qos.logback.core.joran.action.NestedComplexPropertyIA
2036:             1             32  ch.qos.logback.core.joran.action.NewRuleAction
2037:             1             32  ch.qos.logback.core.joran.action.ParamAction
2038:             1             32  ch.qos.logback.core.joran.action.ShutdownHookAction
2039:             1             32  ch.qos.logback.core.joran.action.StatusListenerAction
2040:             1             32  ch.qos.logback.core.joran.action.TimestampAction
2041:             1             32  ch.qos.logback.core.joran.conditional.ElseAction
2042:             1             32  ch.qos.logback.core.joran.conditional.IfAction
2043:             1             32  ch.qos.logback.core.joran.conditional.ThenAction
2044:             1             32  ch.qos.logback.core.joran.spi.SimpleRuleStore
2045:             2             32  ch.qos.logback.core.spi.AppenderAttachableImpl
2046:             1             32  com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache
2047:             1             32  com.github.benmanes.caffeine.cache.FrequencySketch
2048:             2             32  com.google.common.base.Joiner
2049:             2             32  com.google.common.base.Predicates$InPredicate
2050:             1             32  com.google.common.collect.AbstractMapBasedMultimap$NavigableKeySet
2051:             1             32  com.google.common.collect.EmptyImmutableBiMap
2052:             2             32  com.google.common.util.concurrent.Striped$2
2053:             2             32  com.sun.beans.WeakCache
2054:             1             32  com.sun.beans.finder.BeanInfoFinder
2055:             1             32  com.sun.jmx.mbeanserver.Repository
2056:             1             32  com.sun.jmx.remote.internal.ArrayQueue
2057:             1             32  com.sun.jmx.remote.security.JMXSubjectDomainCombiner
2058:             1             32  com.sun.org.apache.xerces.internal.impl.XMLEntityScanner$1
2059:             2             32  com.sun.org.apache.xerces.internal.impl.dv.dtd.ENTITYDatatypeValidator
2060:             2             32  com.sun.proxy.$Proxy5
2061:             1             32  io.netty.bootstrap.ServerBootstrap$ServerBootstrapAcceptor
2062:             1             32  io.netty.channel.epoll.EpollEventLoopGroup
2063:             2             32  io.netty.channel.group.ChannelMatchers$ClassMatcher
2064:             1             32  io.netty.util.concurrent.DefaultThreadFactory
2065:             2             32  io.netty.util.internal.logging.Slf4JLoggerFactory
2066:             1             32  java.beans.ThreadGroupContext
2067:             1             32  java.beans.ThreadGroupContext$1
2068:             2             32  java.io.ObjectStreamClass$1
2069:             2             32  java.io.ObjectStreamClass$3
2070:             2             32  java.io.ObjectStreamClass$4
2071:             2             32  java.io.ObjectStreamClass$5
2072:             1             32  java.io.UnixFileSystem
2073:             1             32  java.lang.ArithmeticException
2074:             1             32  java.lang.ArrayIndexOutOfBoundsException
2075:             1             32  java.lang.ClassCastException
2076:             1             32  java.lang.Exception
2077:             1             32  java.lang.NullPointerException
2078:             2             32  java.lang.Shutdown$Lock
2079:             1             32  java.lang.UnsupportedOperationException
2080:             1             32  java.lang.reflect.WeakCache
2081:             1             32  java.lang.reflect.WeakCache$CacheKey
2082:             2             32  java.nio.ByteOrder
2083:             1             32  java.nio.channels.NotYetConnectedException
2084:             1             32  java.text.DontCareFieldPosition
2085:             2             32  java.util.Hashtable$EntrySet
2086:             1             32  java.util.PriorityQueue
2087:             1             32  java.util.TreeMap$EntryIterator
2088:             1             32  java.util.TreeMap$KeyIterator
2089:             1             32  java.util.concurrent.CancellationException
2090:             1             32  java.util.concurrent.ConcurrentSkipListMap$KeyIterator
2091:             2             32  java.util.concurrent.ConcurrentSkipListMap$KeySet
2092:             1             32  java.util.concurrent.FutureTask
2093:             1             32  java.util.concurrent.ThreadLocalRandom
2094:             2             32  java.util.logging.ErrorManager
2095:             1             32  java.util.logging.LogManager$SystemLoggerContext
2096:             1             32  java.util.regex.Pattern$3
2097:             1             32  javax.crypto.spec.RC5ParameterSpec
2098:             2             32  javax.management.NotificationFilterSupport
2099:             1             32  javax.management.remote.JMXServiceURL
2100:             1             32  javax.security.auth.Subject
2101:             1             32  net.jpountz.xxhash.XXHashFactory
2102:             1             32  org.apache.cassandra.auth.CassandraRoleManager
2103:             1             32  org.apache.cassandra.batchlog.BatchlogManager
2104:             2             32  org.apache.cassandra.cache.ConcurrentLinkedHashCache
2105:             2             32  org.apache.cassandra.cache.ConcurrentLinkedHashCache$1
2106:             1             32  org.apache.cassandra.config.Schema
2107:             1             32  org.apache.cassandra.cql3.QueryOptions$SpecificOptions
2108:             1             32  org.apache.cassandra.cql3.functions.TimeFcts$4
2109:             1             32  org.apache.cassandra.cql3.functions.TimeFcts$5
2110:             2             32  org.apache.cassandra.db.RangeSliceVerbHandler
2111:             1             32  org.apache.cassandra.db.commitlog.SimpleCachedBufferPool
2112:             1             32  org.apache.cassandra.db.compaction.CompactionManager
2113:             2             32  org.apache.cassandra.db.lifecycle.LogReplicaSet
2114:             2             32  org.apache.cassandra.db.lifecycle.LogTransaction$TransactionTidier
2115:             1             32  org.apache.cassandra.db.marshal.AsciiType
2116:             1             32  org.apache.cassandra.db.marshal.PartitionerDefinedOrder
2117:             2             32  org.apache.cassandra.db.rows.CellPath$EmptyCellPath
2118:             2             32  org.apache.cassandra.dht.AbstractBounds$AbstractBoundsSerializer
2119:             1             32  org.apache.cassandra.hints.HintsBuffer
2120:             1             32  org.apache.cassandra.hints.HintsBufferPool
2121:             1             32  org.apache.cassandra.hints.HintsDispatchExecutor
2122:             1             32  org.apache.cassandra.hints.HintsDispatchTrigger
2123:             1             32  org.apache.cassandra.index.internal.composites.CollectionKeyIndex
2124:             1             32  org.apache.cassandra.io.compress.CompressedSequentialWriter$TransactionalProxy
2125:             1             32  org.apache.cassandra.io.compress.LZ4Compressor
2126:             1             32  org.apache.cassandra.io.sstable.IndexSummaryManager
2127:             1             32  org.apache.cassandra.metrics.CQLMetrics
2128:             2             32  org.apache.cassandra.metrics.ClientMetrics$$Lambda$278/1979648826
2129:             1             32  org.apache.cassandra.metrics.CommitLogMetrics
2130:             1             32  org.apache.cassandra.metrics.CompactionMetrics
2131:             2             32  org.apache.cassandra.metrics.TableMetrics$AllTableMetricNameFactory
2132:             2             32  org.apache.cassandra.net.ResponseVerbHandler
2133:             1             32  org.apache.cassandra.repair.RepairRunnable
2134:             2             32  org.apache.cassandra.schema.Types
2135:             1             32  org.apache.cassandra.security.EncryptionContext
2136:             1             32  org.apache.cassandra.service.ActiveRepairService
2137:             1             32  org.apache.cassandra.service.CassandraDaemon
2138:             1             32  org.apache.cassandra.service.NativeTransportService
2139:             1             32  org.apache.cassandra.thrift.TCustomServerSocket
2140:             1             32  org.apache.cassandra.thrift.ThriftServer
2141:             1             32  org.apache.cassandra.utils.ExpiringMap
2142:             2             32  org.apache.cassandra.utils.IntegerInterval$Set
2143:             1             32  org.apache.cassandra.utils.ResourceWatcher$WatchedResource
2144:             1             32  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder
2145:             1             32  org.apache.cassandra.utils.btree.BTree$1
2146:             1             32  org.apache.cassandra.utils.btree.TreeBuilder$1
2147:             1             32  org.apache.cassandra.utils.concurrent.WaitQueue$TimedSignal
2148:             1             32  org.apache.cassandra.utils.memory.BufferPool$GlobalPool
2149:             1             32  org.apache.thrift.protocol.TBinaryProtocol$Factory
2150:             2             32  org.cliffc.high_scale_lib.NonBlockingHashMap$2
2151:             2             32  org.cliffc.high_scale_lib.NonBlockingHashMap$3
2152:             1             32  org.codehaus.jackson.map.deser.BeanDeserializerFactory$ConfigImpl
2153:             1             32  org.codehaus.jackson.map.deser.StdDeserializerProvider
2154:             1             32  org.codehaus.jackson.map.introspect.VisibilityChecker$Std
2155:             2             32  org.codehaus.jackson.map.ser.StdSerializers$NumberSerializer
2156:             2             32  org.codehaus.jackson.map.ser.std.StdKeySerializer
2157:             1             32  org.codehaus.jackson.map.type.TypeFactory
2158:             2             32  org.codehaus.jackson.map.util.RootNameLookup
2159:             1             32  org.github.jamm.MemoryMeter
2160:             1             32  sun.instrument.InstrumentationImpl
2161:             1             32  sun.management.GcInfoCompositeData
2162:             1             32  sun.management.MappedMXBeanType$InProgress
2163:             1             32  sun.nio.ch.ServerSocketAdaptor
2164:             2             32  sun.nio.ch.SocketDispatcher
2165:             1             32  sun.nio.cs.StandardCharsets
2166:             1             32  sun.nio.fs.LinuxFileSystem
2167:             1             32  sun.reflect.UnsafeIntegerFieldAccessorImpl
2168:             1             32  sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl
2169:             2             32  sun.rmi.server.UnicastRef
2170:             2             32  sun.rmi.server.UnicastRef2
2171:             2             32  sun.rmi.transport.DGCImpl$1
2172:             1             32  sun.rmi.transport.proxy.RMIMasterSocketFactory
2173:             1             32  sun.rmi.transport.tcp.TCPTransport$AcceptLoop
2174:             1             32  sun.security.provider.SecureRandom
2175:             2             32  sun.security.ssl.SSLAlgorithmDecomposer
2176:             1             32  sun.security.ssl.X509TrustManagerImpl
2177:             1             32  sun.security.validator.SimpleValidator
2178:             1             32  sun.security.x509.AuthorityInfoAccessExtension
2179:             1             32  sun.security.x509.IssuerAlternativeNameExtension
2180:             1             32  sun.security.x509.PolicyMappingsExtension
2181:             1             32  sun.util.locale.provider.LocaleResources
2182:             1             24  [Lch.qos.logback.core.joran.spi.ConsoleTarget;
2183:             1             24  [Lch.qos.logback.core.subst.Node$Type;
2184:             1             24  [Lcom.clearspring.analytics.stream.cardinality.HyperLogLogPlus$Format;
2185:             1             24  [Lcom.github.benmanes.caffeine.cache.Buffer;
2186:             1             24  [Lcom.github.benmanes.caffeine.cache.DisabledTicker;
2187:             1             24  [Lcom.github.benmanes.caffeine.cache.DisabledWriter;
2188:             1             24  [Lcom.github.benmanes.caffeine.cache.SingletonWeigher;
2189:             1             24  [Lcom.github.benmanes.caffeine.cache.stats.DisabledStatsCounter;
2190:             1             24  [Lcom.google.common.base.Functions$IdentityFunction;
2191:             1             24  [Lcom.google.common.cache.CacheBuilder$NullListener;
2192:             1             24  [Lcom.google.common.cache.CacheBuilder$OneWeigher;
2193:             1             24  [Lcom.google.common.collect.GenericMapMaker$NullListener;
2194:             1             24  [Lcom.google.common.collect.Maps$EntryFunction;
2195:             1             24  [Lcom.google.common.util.concurrent.MoreExecutors$DirectExecutor;
2196:             1             24  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingListener;
2197:             1             24  [Lcom.googlecode.concurrentlinkedhashmap.Weighers$SingletonEntryWeigher;
2198:             1             24  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$Property;
2199:             1             24  [Ljava.io.File$PathStatus;
2200:             1             24  [Ljava.lang.ClassValue$Entry;
2201:             1             24  [Ljava.lang.management.MemoryType;
2202:             1             24  [Ljava.net.InetAddress$Cache$Type;
2203:             1             24  [Ljava.net.InterfaceAddress;
2204:             1             24  [Ljava.net.StandardProtocolFamily;
2205:             1             24  [Ljava.rmi.server.ObjID;
2206:             1             24  [Ljava.util.Comparators$NaturalOrderComparator;
2207:             1             24  [Ljava.util.Locale$Category;
2208:             1             24  [Ljava.util.concurrent.ExecutorService;
2209:             1             24  [Ljava.util.concurrent.ThreadPoolExecutor;
2210:             1             24  [Ljavax.net.ssl.KeyManager;
2211:             1             24  [Ljavax.net.ssl.TrustManager;
2212:             1             24  [Lorg.apache.cassandra.concurrent.ExecutorLocal;
2213:             1             24  [Lorg.apache.cassandra.config.Config$DiskOptimizationStrategy;
2214:             1             24  [Lorg.apache.cassandra.config.Config$RequestSchedulerId;
2215:             1             24  [Lorg.apache.cassandra.cql3.QueryProcessor$InternalStateInstance;
2216:             1             24  [Lorg.apache.cassandra.cql3.Term;
2217:             1             24  [Lorg.apache.cassandra.cql3.statements.Bound;
2218:             1             24  [Lorg.apache.cassandra.db.Directories$OnTxnErr;
2219:             1             24  [Lorg.apache.cassandra.db.ReadCommand$Kind;
2220:             1             24  [Lorg.apache.cassandra.db.aggregation.AggregationSpecification$Kind;
2221:             1             24  [Lorg.apache.cassandra.db.filter.ClusteringIndexFilter$Kind;
2222:             1             24  [Lorg.apache.cassandra.db.rows.Unfiltered$Kind;
2223:             1             24  [Lorg.apache.cassandra.hints.HintsDispatcher$Action;
2224:             1             24  [Lorg.apache.cassandra.io.compress.BufferType;
2225:             1             24  [Lorg.apache.cassandra.io.sstable.format.SSTableFormat$Type;
2226:             1             24  [Lorg.apache.cassandra.io.sstable.format.SSTableReadsListener$SelectionReason;
2227:             1             24  [Lorg.apache.cassandra.metrics.TableMetrics$Sampler;
2228:             1             24  [Lorg.apache.cassandra.schema.CachingParams$Option;
2229:             1             24  [Lorg.apache.cassandra.schema.KeyspaceParams$Option;
2230:             1             24  [Lorg.apache.cassandra.streaming.ProgressInfo$Direction;
2231:             1             24  [Lorg.apache.cassandra.transport.Event$StatusChange$Status;
2232:             1             24  [Lorg.apache.cassandra.transport.Message$Direction;
2233:             1             24  [Lorg.apache.cassandra.utils.ChecksumType;
2234:             1             24  [Lorg.apache.cassandra.utils.Throwables$FileOpType;
2235:             1             24  [Lorg.apache.cassandra.utils.btree.BTree$Dir;
2236:             1             24  [Lsun.launcher.LauncherHelper;
2237:             1             24  [Lsun.security.ssl.EphemeralKeyManager$EphemeralKeyPair;
2238:             1             24  ch.qos.logback.classic.joran.action.ConsolePluginAction
2239:             1             24  ch.qos.logback.classic.joran.action.ContextNameAction
2240:             1             24  ch.qos.logback.classic.joran.action.InsertFromJNDIAction
2241:             1             24  ch.qos.logback.classic.joran.action.JMXConfiguratorAction
2242:             1             24  ch.qos.logback.classic.spi.TurboFilterList
2243:             1             24  ch.qos.logback.classic.util.ContextSelectorStaticBinder
2244:             1             24  ch.qos.logback.classic.util.LogbackMDCAdapter
2245:             1             24  ch.qos.logback.core.joran.action.ContextPropertyAction
2246:             1             24  ch.qos.logback.core.joran.spi.CAI_WithLocatorSupport
2247:             1             24  ch.qos.logback.core.joran.spi.EventPlayer
2248:             1             24  com.clearspring.analytics.stream.cardinality.RegisterSet
2249:             1             24  com.codahale.metrics.MetricRegistry
2250:             1             24  com.github.benmanes.caffeine.cache.BoundedBuffer
2251:             1             24  com.github.benmanes.caffeine.cache.BoundedLocalCache$PerformCleanupTask
2252:             1             24  com.github.benmanes.caffeine.cache.DisabledTicker
2253:             1             24  com.github.benmanes.caffeine.cache.DisabledWriter
2254:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$1
2255:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$10
2256:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$100
2257:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$101
2258:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$102
2259:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$103
2260:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$104
2261:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$105
2262:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$106
2263:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$107
2264:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$108
2265:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$109
2266:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$11
2267:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$110
2268:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$111
2269:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$112
2270:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$113
2271:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$114
2272:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$115
2273:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$116
2274:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$117
2275:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$118
2276:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$119
2277:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$12
2278:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$120
2279:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$121
2280:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$122
2281:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$123
2282:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$124
2283:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$125
2284:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$126
2285:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$127
2286:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$128
2287:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$129
2288:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$13
2289:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$130
2290:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$131
2291:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$132
2292:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$133
2293:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$134
2294:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$135
2295:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$136
2296:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$137
2297:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$138
2298:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$139
2299:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$14
2300:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$140
2301:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$141
2302:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$142
2303:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$143
2304:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$144
2305:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$15
2306:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$16
2307:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$17
2308:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$18
2309:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$19
2310:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$2
2311:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$20
2312:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$21
2313:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$22
2314:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$23
2315:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$24
2316:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$25
2317:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$26
2318:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$27
2319:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$28
2320:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$29
2321:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$3
2322:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$30
2323:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$31
2324:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$32
2325:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$33
2326:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$34
2327:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$35
2328:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$36
2329:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$37
2330:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$38
2331:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$39
2332:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$4
2333:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$40
2334:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$41
2335:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$42
2336:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$43
2337:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$44
2338:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$45
2339:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$46
2340:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$47
2341:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$48
2342:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$49
2343:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$5
2344:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$50
2345:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$51
2346:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$52
2347:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$53
2348:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$54
2349:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$55
2350:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$56
2351:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$57
2352:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$58
2353:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$59
2354:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$6
2355:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$60
2356:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$61
2357:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$62
2358:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$63
2359:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$64
2360:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$65
2361:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$66
2362:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$67
2363:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$68
2364:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$69
2365:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$7
2366:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$70
2367:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$71
2368:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$72
2369:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$73
2370:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$74
2371:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$75
2372:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$76
2373:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$77
2374:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$78
2375:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$79
2376:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$8
2377:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$80
2378:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$81
2379:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$82
2380:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$83
2381:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$84
2382:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$85
2383:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$86
2384:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$87
2385:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$88
2386:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$89
2387:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$9
2388:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$90
2389:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$91
2390:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$92
2391:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$93
2392:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$94
2393:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$95
2394:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$96
2395:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$97
2396:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$98
2397:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$99
2398:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$1
2399:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$2
2400:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$3
2401:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$4
2402:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$5
2403:             1             24  com.github.benmanes.caffeine.cache.SingletonWeigher
2404:             1             24  com.github.benmanes.caffeine.cache.stats.DisabledStatsCounter
2405:             1             24  com.google.common.base.CharMatcher$Or
2406:             1             24  com.google.common.base.Functions$IdentityFunction
2407:             1             24  com.google.common.base.Joiner$1
2408:             1             24  com.google.common.base.Joiner$MapJoiner
2409:             1             24  com.google.common.base.Predicates$ObjectPredicate$1
2410:             1             24  com.google.common.base.Predicates$ObjectPredicate$2
2411:             1             24  com.google.common.base.Predicates$ObjectPredicate$3
2412:             1             24  com.google.common.base.Predicates$ObjectPredicate$4
2413:             1             24  com.google.common.cache.CacheBuilder$NullListener
2414:             1             24  com.google.common.cache.CacheBuilder$OneWeigher
2415:             1             24  com.google.common.cache.LocalCache$EntryFactory$1
2416:             1             24  com.google.common.cache.LocalCache$EntryFactory$2
2417:             1             24  com.google.common.cache.LocalCache$EntryFactory$3
2418:             1             24  com.google.common.cache.LocalCache$EntryFactory$4
2419:             1             24  com.google.common.cache.LocalCache$EntryFactory$5
2420:             1             24  com.google.common.cache.LocalCache$EntryFactory$6
2421:             1             24  com.google.common.cache.LocalCache$EntryFactory$7
2422:             1             24  com.google.common.cache.LocalCache$EntryFactory$8
2423:             1             24  com.google.common.cache.LocalCache$Strength$1
2424:             1             24  com.google.common.cache.LocalCache$Strength$2
2425:             1             24  com.google.common.cache.LocalCache$Strength$3
2426:             1             24  com.google.common.collect.ByFunctionOrdering
2427:             1             24  com.google.common.collect.ConcurrentHashMultiset
2428:             1             24  com.google.common.collect.EmptyImmutableSortedSet
2429:             1             24  com.google.common.collect.GenericMapMaker$NullListener
2430:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$1
2431:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$2
2432:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$3
2433:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$4
2434:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$5
2435:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$6
2436:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$7
2437:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$8
2438:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$1
2439:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$2
2440:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$3
2441:             1             24  com.google.common.collect.Maps$EntryFunction$1
2442:             1             24  com.google.common.collect.Maps$EntryFunction$2
2443:             1             24  com.google.common.collect.Sets$3
2444:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$1
2445:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$2
2446:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$3
2447:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$1
2448:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$2
2449:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$3
2450:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$4
2451:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$5
2452:             1             24  com.google.common.util.concurrent.Futures$1$1
2453:             1             24  com.google.common.util.concurrent.Futures$ChainingListenableFuture$1
2454:             1             24  com.google.common.util.concurrent.MoreExecutors$DirectExecutor
2455:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$KeySetView
2456:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingListener
2457:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$1
2458:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$2
2459:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$3
2460:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$KeySet
2461:             1             24  com.googlecode.concurrentlinkedhashmap.Weighers$SingletonEntryWeigher
2462:             1             24  com.sun.beans.util.Cache$Kind$1
2463:             1             24  com.sun.beans.util.Cache$Kind$2
2464:             1             24  com.sun.beans.util.Cache$Kind$3
2465:             1             24  com.sun.jmx.mbeanserver.ClassLoaderRepositorySupport
2466:             1             24  com.sun.jmx.mbeanserver.MXBeanLookup
2467:             1             24  com.sun.jmx.remote.internal.ArrayNotificationBuffer$ShareBuffer
2468:             1             24  com.sun.jna.Structure$3
2469:             1             24  com.sun.org.apache.xerces.internal.impl.Constants$ArrayEnumeration
2470:             1             24  io.netty.buffer.UnpooledByteBufAllocator
2471:             1             24  io.netty.channel.AdaptiveRecvByteBufAllocator
2472:             1             24  io.netty.channel.SucceededChannelFuture
2473:             1             24  io.netty.channel.unix.Socket
2474:             1             24  io.netty.util.concurrent.FailedFuture
2475:             1             24  java.lang.ClassValue$Version
2476:             1             24  java.lang.Package$1
2477:             1             24  java.lang.ProcessEnvironment$StringEnvironment
2478:             1             24  java.lang.invoke.MethodHandleImpl$4
2479:             1             24  java.lang.invoke.MethodType$ConcurrentWeakInternSet
2480:             1             24  java.math.MutableBigInteger
2481:             1             24  java.net.Inet4AddressImpl
2482:             1             24  java.net.InterfaceAddress
2483:             1             24  java.nio.file.FileVisitOption
2484:             1             24  java.nio.file.LinkOption
2485:             1             24  java.security.CodeSigner
2486:             1             24  java.security.Policy$PolicyInfo
2487:             1             24  java.security.Policy$UnsupportedEmptyCollection
2488:             1             24  java.util.Collections$EmptyMap
2489:             1             24  java.util.Collections$UnmodifiableList
2490:             1             24  java.util.Comparators$NaturalOrderComparator
2491:             1             24  java.util.Currency
2492:             1             24  java.util.Locale$Cache
2493:             1             24  java.util.OptionalLong
2494:             1             24  java.util.ResourceBundle$Control$CandidateListCache
2495:             1             24  java.util.Vector$1
2496:             1             24  java.util.concurrent.Executors$DelegatedScheduledExecutorService
2497:             1             24  java.util.concurrent.TimeUnit$1
2498:             1             24  java.util.concurrent.TimeUnit$2
2499:             1             24  java.util.concurrent.TimeUnit$3
2500:             1             24  java.util.concurrent.TimeUnit$4
2501:             1             24  java.util.concurrent.TimeUnit$5
2502:             1             24  java.util.concurrent.TimeUnit$6
2503:             1             24  java.util.concurrent.TimeUnit$7
2504:             1             24  java.util.logging.LogManager$5
2505:             1             24  java.util.logging.LogManager$LoggerContext
2506:             1             24  java.util.logging.LoggingPermission
2507:             1             24  java.util.regex.Pattern$SingleI
2508:             1             24  javax.crypto.spec.RC2ParameterSpec
2509:             1             24  javax.management.NotificationBroadcasterSupport
2510:             1             24  javax.net.ssl.SSLContext
2511:             1             24  org.antlr.runtime.CharStreamState
2512:             1             24  org.apache.cassandra.auth.CassandraAuthorizer
2513:             1             24  org.apache.cassandra.auth.CassandraRoleManager$Role
2514:             1             24  org.apache.cassandra.auth.PasswordAuthenticator
2515:             1             24  org.apache.cassandra.cache.ChunkCache
2516:             1             24  org.apache.cassandra.config.Config$1
2517:             1             24  org.apache.cassandra.config.Config$RequestSchedulerId
2518:             1             24  org.apache.cassandra.config.RequestSchedulerOptions
2519:             1             24  org.apache.cassandra.cql3.Attributes$Raw
2520:             1             24  org.apache.cassandra.cql3.ColumnConditions
2521:             1             24  org.apache.cassandra.cql3.CqlParser
2522:             1             24  org.apache.cassandra.cql3.ErrorCollector
2523:             1             24  org.apache.cassandra.cql3.Lists$Marker
2524:             1             24  org.apache.cassandra.cql3.Maps$DiscarderByKey
2525:             1             24  org.apache.cassandra.cql3.Maps$Marker
2526:             1             24  org.apache.cassandra.cql3.Maps$Setter
2527:             1             24  org.apache.cassandra.cql3.Operator$1
2528:             1             24  org.apache.cassandra.cql3.Operator$10
2529:             1             24  org.apache.cassandra.cql3.Operator$11
2530:             1             24  org.apache.cassandra.cql3.Operator$12
2531:             1             24  org.apache.cassandra.cql3.Operator$13
2532:             1             24  org.apache.cassandra.cql3.Operator$14
2533:             1             24  org.apache.cassandra.cql3.Operator$15
2534:             1             24  org.apache.cassandra.cql3.Operator$2
2535:             1             24  org.apache.cassandra.cql3.Operator$3
2536:             1             24  org.apache.cassandra.cql3.Operator$4
2537:             1             24  org.apache.cassandra.cql3.Operator$5
2538:             1             24  org.apache.cassandra.cql3.Operator$6
2539:             1             24  org.apache.cassandra.cql3.Operator$7
2540:             1             24  org.apache.cassandra.cql3.Operator$8
2541:             1             24  org.apache.cassandra.cql3.Operator$9
2542:             1             24  org.apache.cassandra.cql3.QueryProcessor$InternalStateInstance
2543:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$1
2544:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$10
2545:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$11
2546:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$12
2547:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$13
2548:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$14
2549:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$15
2550:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$16
2551:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$17
2552:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$18
2553:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$19
2554:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$2
2555:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$20
2556:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$21
2557:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$3
2558:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$4
2559:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$5
2560:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$6
2561:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$7
2562:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$8
2563:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$9
2564:             1             24  org.apache.cassandra.cql3.functions.BytesConversionFcts$3
2565:             1             24  org.apache.cassandra.cql3.functions.BytesConversionFcts$4
2566:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$1
2567:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$10
2568:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$11
2569:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$12
2570:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$2
2571:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$3
2572:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$6
2573:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$7
2574:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$8
2575:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$9
2576:             1             24  org.apache.cassandra.cql3.functions.UuidFcts$1
2577:             1             24  org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$InRestrictionWithMarker
2578:             1             24  org.apache.cassandra.cql3.restrictions.TermSlice
2579:             1             24  org.apache.cassandra.cql3.restrictions.TokenRestriction$SliceRestriction
2580:             1             24  org.apache.cassandra.cql3.statements.StatementType$1
2581:             1             24  org.apache.cassandra.cql3.statements.StatementType$2
2582:             1             24  org.apache.cassandra.cql3.statements.StatementType$3
2583:             1             24  org.apache.cassandra.cql3.statements.StatementType$4
2584:             1             24  org.apache.cassandra.db.BlacklistedDirectories
2585:             1             24  org.apache.cassandra.db.Clustering$1
2586:             1             24  org.apache.cassandra.db.Clustering$2
2587:             1             24  org.apache.cassandra.db.Slice$1
2588:             1             24  org.apache.cassandra.db.commitlog.CommitLog$Configuration
2589:             1             24  org.apache.cassandra.db.compaction.CompactionLogger$CompactionLogSerializer
2590:             1             24  org.apache.cassandra.db.filter.DataLimits$1
2591:             1             24  org.apache.cassandra.db.filter.DataLimits$CQLLimits
2592:             1             24  org.apache.cassandra.db.marshal.AsciiType$1
2593:             1             24  org.apache.cassandra.db.marshal.BooleanType
2594:             1             24  org.apache.cassandra.db.marshal.ByteType
2595:             1             24  org.apache.cassandra.db.marshal.BytesType
2596:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$1
2597:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$2
2598:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$3
2599:             1             24  org.apache.cassandra.db.marshal.CounterColumnType
2600:             1             24  org.apache.cassandra.db.marshal.DecimalType
2601:             1             24  org.apache.cassandra.db.marshal.DoubleType
2602:             1             24  org.apache.cassandra.db.marshal.DurationType
2603:             1             24  org.apache.cassandra.db.marshal.EmptyType
2604:             1             24  org.apache.cassandra.db.marshal.FloatType
2605:             1             24  org.apache.cassandra.db.marshal.InetAddressType
2606:             1             24  org.apache.cassandra.db.marshal.Int32Type
2607:             1             24  org.apache.cassandra.db.marshal.IntegerType
2608:             1             24  org.apache.cassandra.db.marshal.LongType
2609:             1             24  org.apache.cassandra.db.marshal.ShortType
2610:             1             24  org.apache.cassandra.db.marshal.SimpleDateType
2611:             1             24  org.apache.cassandra.db.marshal.TimeType
2612:             1             24  org.apache.cassandra.db.marshal.TimeUUIDType
2613:             1             24  org.apache.cassandra.db.marshal.TimestampType
2614:             1             24  org.apache.cassandra.db.marshal.TypeParser
2615:             1             24  org.apache.cassandra.db.marshal.UTF8Type
2616:             1             24  org.apache.cassandra.db.marshal.UUIDType
2617:             1             24  org.apache.cassandra.db.transform.Stack
2618:             1             24  org.apache.cassandra.dht.Murmur3Partitioner
2619:             1             24  org.apache.cassandra.dht.Murmur3Partitioner$1
2620:             1             24  org.apache.cassandra.hints.HintsCatalog
2621:             1             24  org.apache.cassandra.hints.HintsWriteExecutor
2622:             1             24  org.apache.cassandra.io.compress.BufferType$1
2623:             1             24  org.apache.cassandra.io.compress.BufferType$2
2624:             1             24  org.apache.cassandra.io.util.ChecksumWriter
2625:             1             24  org.apache.cassandra.io.util.SequentialWriter$TransactionalProxy
2626:             1             24  org.apache.cassandra.io.util.SsdDiskOptimizationStrategy
2627:             1             24  org.apache.cassandra.locator.ReconnectableSnitchHelper
2628:             1             24  org.apache.cassandra.metrics.AuthMetrics
2629:             1             24  org.apache.cassandra.metrics.BufferPoolMetrics
2630:             1             24  org.apache.cassandra.metrics.CassandraMetricsRegistry
2631:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$1
2632:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$2
2633:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$3
2634:             1             24  org.apache.cassandra.metrics.CompactionMetrics$3
2635:             1             24  org.apache.cassandra.metrics.HintedHandoffMetrics
2636:             1             24  org.apache.cassandra.metrics.MessagingMetrics
2637:             1             24  org.apache.cassandra.net.MessagingService$Verb$1
2638:             1             24  org.apache.cassandra.net.MessagingService$Verb$10
2639:             1             24  org.apache.cassandra.net.MessagingService$Verb$11
2640:             1             24  org.apache.cassandra.net.MessagingService$Verb$12
2641:             1             24  org.apache.cassandra.net.MessagingService$Verb$13
2642:             1             24  org.apache.cassandra.net.MessagingService$Verb$2
2643:             1             24  org.apache.cassandra.net.MessagingService$Verb$3
2644:             1             24  org.apache.cassandra.net.MessagingService$Verb$4
2645:             1             24  org.apache.cassandra.net.MessagingService$Verb$5
2646:             1             24  org.apache.cassandra.net.MessagingService$Verb$6
2647:             1             24  org.apache.cassandra.net.MessagingService$Verb$7
2648:             1             24  org.apache.cassandra.net.MessagingService$Verb$8
2649:             1             24  org.apache.cassandra.net.MessagingService$Verb$9
2650:             1             24  org.apache.cassandra.service.CacheService
2651:             1             24  org.apache.cassandra.service.GCInspector
2652:             1             24  org.apache.cassandra.service.PendingRangeCalculatorService
2653:             1             24  org.apache.cassandra.service.QueryState
2654:             1             24  org.apache.cassandra.service.StartupChecks
2655:             1             24  org.apache.cassandra.service.StartupChecks$8
2656:             1             24  org.apache.cassandra.streaming.StreamManager
2657:             1             24  org.apache.cassandra.thrift.Cassandra$Processor
2658:             1             24  org.apache.cassandra.tracing.TracingImpl
2659:             1             24  org.apache.cassandra.transport.ConnectionLimitHandler
2660:             1             24  org.apache.cassandra.transport.Frame$Compressor
2661:             1             24  org.apache.cassandra.transport.Frame$Decompressor
2662:             1             24  org.apache.cassandra.transport.Frame$Encoder
2663:             1             24  org.apache.cassandra.transport.Message$Dispatcher
2664:             1             24  org.apache.cassandra.transport.Message$ProtocolDecoder
2665:             1             24  org.apache.cassandra.transport.Message$ProtocolEncoder
2666:             1             24  org.apache.cassandra.transport.RequestThreadPoolExecutor
2667:             1             24  org.apache.cassandra.transport.Server$ConnectionTracker
2668:             1             24  org.apache.cassandra.transport.Server$EventNotifier
2669:             1             24  org.apache.cassandra.transport.Server$Initializer
2670:             1             24  org.apache.cassandra.triggers.TriggerExecutor
2671:             1             24  org.apache.cassandra.utils.ChecksumType$1
2672:             1             24  org.apache.cassandra.utils.ChecksumType$2
2673:             1             24  org.apache.cassandra.utils.ConcurrentBiMap
2674:             1             24  org.apache.cassandra.utils.ExpiringMap$1
2675:             1             24  org.apache.cassandra.utils.HistogramBuilder
2676:             1             24  org.apache.cassandra.utils.IntervalTree
2677:             1             24  org.apache.cassandra.utils.JMXServerUtils$Registry
2678:             1             24  org.apache.cassandra.utils.concurrent.OpOrder$Barrier
2679:             1             24  org.apache.cassandra.utils.memory.BufferPool$Debug
2680:             1             24  org.apache.cassandra.utils.progress.jmx.JMXProgressSupport
2681:             1             24  org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport
2682:             1             24  org.codehaus.jackson.map.deser.BeanDeserializerFactory
2683:             1             24  org.codehaus.jackson.map.ser.BeanSerializerFactory
2684:             1             24  org.codehaus.jackson.map.ser.BeanSerializerFactory$ConfigImpl
2685:             1             24  org.codehaus.jackson.map.ser.impl.FailingSerializer
2686:             1             24  org.codehaus.jackson.map.ser.impl.SerializerCache
2687:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$BooleanArraySerializer
2688:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$DoubleArraySerializer
2689:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$FloatArraySerializer
2690:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$IntArraySerializer
2691:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$LongArraySerializer
2692:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$ShortArraySerializer
2693:             1             24  org.slf4j.helpers.FormattingTuple
2694:             1             24  org.slf4j.impl.StaticLoggerBinder
2695:             1             24  org.yaml.snakeyaml.external.com.google.gdata.util.common.base.PercentEscaper
2696:             1             24  sun.instrument.TransformerManager
2697:             1             24  sun.launcher.LauncherHelper
2698:             1             24  sun.management.CompilationImpl
2699:             1             24  sun.management.GarbageCollectionNotifInfoCompositeData
2700:             1             24  sun.management.MemoryImpl
2701:             1             24  sun.management.OperatingSystemImpl
2702:             1             24  sun.management.RuntimeImpl
2703:             1             24  sun.management.ThreadImpl
2704:             1             24  sun.management.VMManagementImpl
2705:             1             24  sun.misc.JarIndex
2706:             1             24  sun.net.ProgressMonitor
2707:             1             24  sun.net.sdp.SdpProvider
2708:             1             24  sun.net.www.protocol.http.Handler
2709:             1             24  sun.nio.cs.ISO_8859_1
2710:             1             24  sun.nio.cs.US_ASCII
2711:             1             24  sun.nio.cs.UTF_16
2712:             1             24  sun.nio.cs.UTF_16BE
2713:             1             24  sun.nio.cs.UTF_16LE
2714:             1             24  sun.nio.cs.UTF_8
2715:             1             24  sun.rmi.runtime.RuntimeUtil$1
2716:             1             24  sun.rmi.server.LoaderHandler$1
2717:             1             24  sun.rmi.transport.DGCImpl
2718:             1             24  sun.rmi.transport.Target$$Lambda$338/684260999
2719:             1             24  sun.security.provider.certpath.X509CertPath
2720:             1             24  sun.security.ssl.SunX509KeyManagerImpl
2721:             1             24  sun.security.validator.EndEntityChecker
2722:             1             24  sun.security.x509.AccessDescription
2723:             1             24  sun.security.x509.CertificatePolicyMap
2724:             1             24  sun.util.locale.BaseLocale$Cache
2725:             1             24  sun.util.locale.provider.CalendarDataProviderImpl
2726:             1             24  sun.util.locale.provider.CalendarProviderImpl
2727:             1             24  sun.util.locale.provider.CurrencyNameProviderImpl
2728:             1             24  sun.util.locale.provider.DateFormatSymbolsProviderImpl
2729:             1             24  sun.util.locale.provider.DecimalFormatSymbolsProviderImpl
2730:             1             24  sun.util.locale.provider.NumberFormatProviderImpl
2731:             1             24  sun.util.logging.PlatformLogger
2732:             1             24  sun.util.logging.PlatformLogger$JavaLoggerProxy
2733:             1             24  sun.util.resources.LocaleData$1
2734:             1             16  [Lch.qos.logback.classic.spi.ThrowableProxy;
2735:             1             16  [Ljava.beans.EventSetDescriptor;
2736:             1             16  [Ljava.lang.Double;
2737:             1             16  [Ljava.lang.Float;
2738:             1             16  [Ljava.lang.Throwable;
2739:             1             16  [Ljava.net.NetworkInterface;
2740:             1             16  [Ljava.net.URL;
2741:             1             16  [Ljava.nio.file.attribute.FileAttribute;
2742:             1             16  [Ljava.security.Provider;
2743:             1             16  [Ljava.text.FieldPosition;
2744:             1             16  [Ljavax.security.cert.X509Certificate;
2745:             1             16  [Lnet.jpountz.lz4.LZ4JNI;
2746:             1             16  [Lnet.jpountz.lz4.LZ4Utils;
2747:             1             16  [Lnet.jpountz.util.ByteBufferUtils;
2748:             1             16  [Lnet.jpountz.util.Native;
2749:             1             16  [Lnet.jpountz.util.SafeUtils;
2750:             1             16  [Lnet.jpountz.xxhash.XXHashJNI;
2751:             1             16  [Lorg.apache.cassandra.db.rows.Cell;
2752:             1             16  [Lorg.apache.cassandra.db.transform.Stack$MoreContentsHolder;
2753:             1             16  [Lorg.codehaus.jackson.map.AbstractTypeResolver;
2754:             1             16  [Lorg.codehaus.jackson.map.Deserializers;
2755:             1             16  [Lorg.codehaus.jackson.map.KeyDeserializers;
2756:             1             16  [Lorg.codehaus.jackson.map.Serializers;
2757:             1             16  [Lorg.codehaus.jackson.map.deser.BeanDeserializerModifier;
2758:             1             16  [Lorg.codehaus.jackson.map.deser.ValueInstantiators;
2759:             1             16  [Lorg.codehaus.jackson.map.introspect.AnnotationMap;
2760:             1             16  [Lorg.codehaus.jackson.map.ser.BeanSerializerModifier;
2761:             1             16  [Lsun.instrument.TransformerManager$TransformerInfo;
2762:             1             16  ch.qos.logback.classic.selector.DefaultContextSelector
2763:             1             16  ch.qos.logback.core.joran.spi.ConsoleTarget$1
2764:             1             16  ch.qos.logback.core.joran.spi.ConsoleTarget$2
2765:             1             16  ch.qos.logback.core.joran.spi.DefaultNestedComponentRegistry
2766:             1             16  ch.qos.logback.core.joran.util.ConfigurationWatchListUtil
2767:             1             16  com.codahale.metrics.Clock$UserTimeClock
2768:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$1
2769:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$2
2770:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$3
2771:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$4
2772:             1             16  com.codahale.metrics.Striped64$ThreadHashCode
2773:             1             16  com.codahale.metrics.ThreadLocalRandom$1
2774:             1             16  com.github.benmanes.caffeine.SingleConsumerQueue$$Lambda$80/692511295
2775:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$79/608770405
2776:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache$$Lambda$81/1858886571
2777:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$EntrySetView
2778:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$KeySetView
2779:             1             16  com.github.benmanes.caffeine.cache.BoundedWeigher
2780:             1             16  com.github.benmanes.caffeine.cache.Caffeine$$Lambda$77/2064869182
2781:             1             16  com.google.common.base.Absent
2782:             1             16  com.google.common.base.CharMatcher$1
2783:             1             16  com.google.common.base.CharMatcher$15
2784:             1             16  com.google.common.base.CharMatcher$2
2785:             1             16  com.google.common.base.CharMatcher$3
2786:             1             16  com.google.common.base.CharMatcher$4
2787:             1             16  com.google.common.base.CharMatcher$5
2788:             1             16  com.google.common.base.CharMatcher$6
2789:             1             16  com.google.common.base.CharMatcher$7
2790:             1             16  com.google.common.base.CharMatcher$8
2791:             1             16  com.google.common.base.Equivalence$Equals
2792:             1             16  com.google.common.base.Equivalence$Identity
2793:             1             16  com.google.common.base.Predicates$NotPredicate
2794:             1             16  com.google.common.base.Predicates$OrPredicate
2795:             1             16  com.google.common.base.Suppliers$SupplierOfInstance
2796:             1             16  com.google.common.base.Ticker$1
2797:             1             16  com.google.common.cache.CacheBuilder$1
2798:             1             16  com.google.common.cache.CacheBuilder$2
2799:             1             16  com.google.common.cache.CacheBuilder$3
2800:             1             16  com.google.common.cache.LocalCache$1
2801:             1             16  com.google.common.cache.LocalCache$2
2802:             1             16  com.google.common.cache.LocalCache$LocalManualCache
2803:             1             16  com.google.common.collect.ComparatorOrdering
2804:             1             16  com.google.common.collect.EmptyImmutableSet
2805:             1             16  com.google.common.collect.Iterators$1
2806:             1             16  com.google.common.collect.Iterators$2
2807:             1             16  com.google.common.collect.MapMakerInternalMap$1
2808:             1             16  com.google.common.collect.MapMakerInternalMap$2
2809:             1             16  com.google.common.collect.Multisets$5
2810:             1             16  com.google.common.collect.NaturalOrdering
2811:             1             16  com.google.common.collect.ReverseOrdering
2812:             1             16  com.google.common.io.ByteStreams$1
2813:             1             16  com.google.common.util.concurrent.Futures$4
2814:             1             16  com.google.common.util.concurrent.Futures$7
2815:             1             16  com.google.common.util.concurrent.Runnables$1
2816:             1             16  com.google.common.util.concurrent.Striped$5
2817:             1             16  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingQueue
2818:             1             16  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor$ResourceContext$1
2819:             1             16  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory
2820:             1             16  com.sun.jmx.mbeanserver.DescriptorCache
2821:             1             16  com.sun.jmx.mbeanserver.MBeanAnalyzer$MethodOrder
2822:             1             16  com.sun.jmx.mbeanserver.MBeanInstantiator
2823:             1             16  com.sun.jmx.mbeanserver.MXBeanIntrospector
2824:             1             16  com.sun.jmx.mbeanserver.SecureClassLoaderRepository
2825:             1             16  com.sun.jmx.mbeanserver.StandardMBeanIntrospector
2826:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$5
2827:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$BroadcasterQuery
2828:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$BufferListener
2829:             1             16  com.sun.jmx.remote.internal.ServerCommunicatorAdmin$Timeout
2830:             1             16  com.sun.jmx.remote.internal.ServerNotifForwarder$NotifForwarderBufferFilter
2831:             1             16  com.sun.jmx.remote.protocol.iiop.IIOPProxyImpl
2832:             1             16  com.sun.jmx.remote.security.SubjectDelegator
2833:             1             16  com.sun.jna.Native$1
2834:             1             16  com.sun.jna.Native$2
2835:             1             16  com.sun.jna.Native$7
2836:             1             16  com.sun.jna.Structure$1
2837:             1             16  com.sun.jna.Structure$2
2838:             1             16  com.sun.jna.VarArgsChecker$RealVarArgsChecker
2839:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.IDDatatypeValidator
2840:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.IDREFDatatypeValidator
2841:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.NMTOKENDatatypeValidator
2842:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.NOTATIONDatatypeValidator
2843:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.StringDatatypeValidator
2844:             1             16  com.sun.org.apache.xerces.internal.utils.SecuritySupport
2845:             1             16  com.sun.proxy.$Proxy2
2846:             1             16  com.sun.proxy.$Proxy4
2847:             1             16  com.sun.proxy.$Proxy7
2848:             1             16  io.netty.buffer.ByteBufUtil$1
2849:             1             16  io.netty.buffer.ByteBufUtil$2
2850:             1             16  io.netty.channel.ChannelFutureListener$1
2851:             1             16  io.netty.channel.ChannelFutureListener$2
2852:             1             16  io.netty.channel.ChannelFutureListener$3
2853:             1             16  io.netty.channel.ChannelMetadata
2854:             1             16  io.netty.channel.ChannelOutboundBuffer$1
2855:             1             16  io.netty.channel.DefaultChannelPipeline$1
2856:             1             16  io.netty.channel.DefaultMessageSizeEstimator
2857:             1             16  io.netty.channel.DefaultMessageSizeEstimator$HandleImpl
2858:             1             16  io.netty.channel.DefaultSelectStrategy
2859:             1             16  io.netty.channel.DefaultSelectStrategyFactory
2860:             1             16  io.netty.channel.group.ChannelMatchers$1
2861:             1             16  io.netty.channel.group.ChannelMatchers$InvertMatcher
2862:             1             16  io.netty.util.Recycler$1
2863:             1             16  io.netty.util.Recycler$3
2864:             1             16  io.netty.util.concurrent.DefaultPromise$CauseHolder
2865:             1             16  io.netty.util.concurrent.GlobalEventExecutor$1
2866:             1             16  io.netty.util.concurrent.GlobalEventExecutor$TaskRunner
2867:             1             16  io.netty.util.concurrent.MultithreadEventExecutorGroup$1
2868:             1             16  io.netty.util.concurrent.MultithreadEventExecutorGroup$PowerOfTwoEventExecutorChooser
2869:             1             16  io.netty.util.concurrent.RejectedExecutionHandlers$1
2870:             1             16  io.netty.util.concurrent.SingleThreadEventExecutor$1
2871:             1             16  io.netty.util.internal.NoOpTypeParameterMatcher
2872:             1             16  java.io.DeleteOnExitHook$1
2873:             1             16  java.io.FileDescriptor$1
2874:             1             16  java.io.ObjectInputStream$$Lambda$293/697818519
2875:             1             16  java.io.ObjectInputStream$1
2876:             1             16  java.lang.ApplicationShutdownHooks$1
2877:             1             16  java.lang.CharacterDataLatin1
2878:             1             16  java.lang.ClassValue$Identity
2879:             1             16  java.lang.ProcessBuilder$NullInputStream
2880:             1             16  java.lang.ProcessBuilder$NullOutputStream
2881:             1             16  java.lang.Runtime
2882:             1             16  java.lang.String$CaseInsensitiveComparator
2883:             1             16  java.lang.System$2
2884:             1             16  java.lang.Terminator$1
2885:             1             16  java.lang.UNIXProcess$$Lambda$13/1784131088
2886:             1             16  java.lang.UNIXProcess$$Lambda$14/2143582219
2887:             1             16  java.lang.UNIXProcess$Platform$$Lambda$10/616881582
2888:             1             16  java.lang.invoke.MemberName$Factory
2889:             1             16  java.lang.invoke.MethodHandleImpl$2
2890:             1             16  java.lang.invoke.MethodHandleImpl$3
2891:             1             16  java.lang.management.PlatformComponent$1
2892:             1             16  java.lang.management.PlatformComponent$10
2893:             1             16  java.lang.management.PlatformComponent$11
2894:             1             16  java.lang.management.PlatformComponent$12
2895:             1             16  java.lang.management.PlatformComponent$13
2896:             1             16  java.lang.management.PlatformComponent$14
2897:             1             16  java.lang.management.PlatformComponent$15
2898:             1             16  java.lang.management.PlatformComponent$2
2899:             1             16  java.lang.management.PlatformComponent$3
2900:             1             16  java.lang.management.PlatformComponent$4
2901:             1             16  java.lang.management.PlatformComponent$5
2902:             1             16  java.lang.management.PlatformComponent$6
2903:             1             16  java.lang.management.PlatformComponent$7
2904:             1             16  java.lang.management.PlatformComponent$8
2905:             1             16  java.lang.management.PlatformComponent$9
2906:             1             16  java.lang.ref.Reference$1
2907:             1             16  java.lang.ref.Reference$Lock
2908:             1             16  java.lang.reflect.Proxy$KeyFactory
2909:             1             16  java.lang.reflect.Proxy$ProxyClassFactory
2910:             1             16  java.lang.reflect.ReflectAccess
2911:             1             16  java.math.BigDecimal$1
2912:             1             16  java.net.InetAddress$2
2913:             1             16  java.net.URLClassLoader$7
2914:             1             16  java.nio.Bits$1
2915:             1             16  java.nio.Bits$1$1
2916:             1             16  java.nio.charset.CoderResult$1
2917:             1             16  java.nio.charset.CoderResult$2
2918:             1             16  java.nio.file.Files$AcceptAllFilter
2919:             1             16  java.rmi.server.RMIClassLoader$2
2920:             1             16  java.security.AllPermission
2921:             1             16  java.security.ProtectionDomain$2
2922:             1             16  java.security.ProtectionDomain$JavaSecurityAccessImpl
2923:             1             16  java.text.DontCareFieldPosition$1
2924:             1             16  java.util.Collections$EmptyEnumeration
2925:             1             16  java.util.Collections$EmptyIterator
2926:             1             16  java.util.Collections$EmptyList
2927:             1             16  java.util.Collections$EmptySet
2928:             1             16  java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet
2929:             1             16  java.util.Currency$CurrencyNameGetter
2930:             1             16  java.util.EnumMap$1
2931:             1             16  java.util.ResourceBundle$Control
2932:             1             16  java.util.Spliterators$EmptySpliterator$OfDouble
2933:             1             16  java.util.Spliterators$EmptySpliterator$OfInt
2934:             1             16  java.util.Spliterators$EmptySpliterator$OfLong
2935:             1             16  java.util.Spliterators$EmptySpliterator$OfRef
2936:             1             16  java.util.TreeMap$EntrySpliterator$$Lambda$68/1819038759
2937:             1             16  java.util.WeakHashMap$KeySet
2938:             1             16  java.util.concurrent.Executors$FinalizableDelegatedExecutorService
2939:             1             16  java.util.concurrent.ThreadPoolExecutor$AbortPolicy
2940:             1             16  java.util.jar.JarVerifier$3
2941:             1             16  java.util.jar.JavaUtilJarAccessImpl
2942:             1             16  java.util.logging.LoggingProxyImpl
2943:             1             16  java.util.regex.Pattern$4
2944:             1             16  java.util.regex.Pattern$LastNode
2945:             1             16  java.util.regex.Pattern$Node
2946:             1             16  java.util.stream.Collectors$$Lambda$178/1708585783
2947:             1             16  java.util.stream.Collectors$$Lambda$179/2048467502
2948:             1             16  java.util.stream.Collectors$$Lambda$180/1269763229
2949:             1             16  java.util.stream.Collectors$$Lambda$221/1489469437
2950:             1             16  java.util.stream.Collectors$$Lambda$222/431613642
2951:             1             16  java.util.stream.Collectors$$Lambda$223/1098744211
2952:             1             16  java.util.stream.Collectors$$Lambda$247/1746129463
2953:             1             16  java.util.stream.Collectors$$Lambda$60/1724814719
2954:             1             16  java.util.stream.Collectors$$Lambda$61/1718322084
2955:             1             16  java.util.stream.Collectors$$Lambda$62/24039137
2956:             1             16  java.util.stream.Collectors$$Lambda$63/992086987
2957:             1             16  java.util.stream.LongPipeline$$Lambda$189/1888591113
2958:             1             16  java.util.stream.LongPipeline$$Lambda$325/1014276638
2959:             1             16  java.util.zip.ZipFile$1
2960:             1             16  javax.crypto.JceSecurityManager
2961:             1             16  javax.management.JMX
2962:             1             16  javax.management.MBeanServerBuilder
2963:             1             16  javax.management.NotificationBroadcasterSupport$1
2964:             1             16  javax.management.remote.JMXPrincipal
2965:             1             16  javax.management.remote.rmi.RMIConnectionImpl_Stub
2966:             1             16  javax.management.remote.rmi.RMIServerImpl_Stub
2967:             1             16  javax.xml.parsers.SecuritySupport
2968:             1             16  net.jpountz.lz4.LZ4JNICompressor
2969:             1             16  net.jpountz.lz4.LZ4JNIFastDecompressor
2970:             1             16  net.jpountz.lz4.LZ4JNISafeDecompressor
2971:             1             16  net.jpountz.xxhash.StreamingXXHash32JNI$Factory
2972:             1             16  net.jpountz.xxhash.StreamingXXHash64JNI$Factory
2973:             1             16  net.jpountz.xxhash.XXHash32JNI
2974:             1             16  net.jpountz.xxhash.XXHash64JNI
2975:             1             16  org.apache.cassandra.auth.AllowAllAuthenticator$Negotiator
2976:             1             16  org.apache.cassandra.auth.AllowAllInternodeAuthenticator
2977:             1             16  org.apache.cassandra.auth.AuthCache$1
2978:             1             16  org.apache.cassandra.auth.AuthMigrationListener
2979:             1             16  org.apache.cassandra.auth.CassandraRoleManager$$Lambda$264/195066780
2980:             1             16  org.apache.cassandra.auth.CassandraRoleManager$1
2981:             1             16  org.apache.cassandra.auth.CassandraRoleManager$2
2982:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$265/385180766
2983:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$266/694021194
2984:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$267/767298601
2985:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$268/274090580
2986:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$269/1588510401
2987:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$270/331234425
2988:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$271/996989596
2989:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$272/1507030140
2990:             1             16  org.apache.cassandra.batchlog.Batch$Serializer
2991:             1             16  org.apache.cassandra.batchlog.BatchRemoveVerbHandler
2992:             1             16  org.apache.cassandra.batchlog.BatchStoreVerbHandler
2993:             1             16  org.apache.cassandra.batchlog.BatchlogManager$$Lambda$258/2042553130
2994:             1             16  org.apache.cassandra.batchlog.BatchlogManager$$Lambda$290/1638031626
2995:             1             16  org.apache.cassandra.cache.AutoSavingCache$1
2996:             1             16  org.apache.cassandra.cache.ChunkCache$$Lambda$78/420307438
2997:             1             16  org.apache.cassandra.cache.NopCacheProvider$NopCache
2998:             1             16  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$1
2999:             1             16  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1
3000:             1             16  org.apache.cassandra.concurrent.StageManager$1
3001:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$213/1328645530
3002:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$214/2107098463
3003:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$232/1529326426
3004:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$233/570714518
3005:             1             16  org.apache.cassandra.config.CFMetaData$Builder$$Lambda$30/671596011
3006:             1             16  org.apache.cassandra.config.CFMetaData$Serializer
3007:             1             16  org.apache.cassandra.config.ColumnDefinition$$Lambda$25/207471778
3008:             1             16  org.apache.cassandra.config.DatabaseDescriptor$1
3009:             1             16  org.apache.cassandra.config.Schema$$Lambda$262/956354740
3010:             1             16  org.apache.cassandra.config.Schema$$Lambda$263/2080528880
3011:             1             16  org.apache.cassandra.cql3.ColumnConditions$$Lambda$116/841977955
3012:             1             16  org.apache.cassandra.cql3.Constants$1
3013:             1             16  org.apache.cassandra.cql3.Constants$NullLiteral
3014:             1             16  org.apache.cassandra.cql3.Constants$UnsetLiteral
3015:             1             16  org.apache.cassandra.cql3.Cql_Parser$1
3016:             1             16  org.apache.cassandra.cql3.IfExistsCondition
3017:             1             16  org.apache.cassandra.cql3.IfNotExistsCondition
3018:             1             16  org.apache.cassandra.cql3.QueryOptions$Codec
3019:             1             16  org.apache.cassandra.cql3.QueryProcessor
3020:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$17/951221468
3021:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$18/1046545660
3022:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$19/1545827753
3023:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$20/1611832218
3024:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$21/2027317551
3025:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$22/273077527
3026:             1             16  org.apache.cassandra.cql3.QueryProcessor$MigrationSubscriber
3027:             1             16  org.apache.cassandra.cql3.ResultSet$Codec
3028:             1             16  org.apache.cassandra.cql3.ResultSet$ResultMetadata$Codec
3029:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$41/1614133563
3030:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$42/839771540
3031:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$43/1751403001
3032:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$44/1756819670
3033:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$45/178604517
3034:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$46/1543518287
3035:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$47/464872674
3036:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$48/1659286984
3037:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$49/1793899405
3038:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager
3039:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$1
3040:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$2
3041:             1             16  org.apache.cassandra.cql3.restrictions.RestrictionSet$1
3042:             1             16  org.apache.cassandra.cql3.selection.Selection$1
3043:             1             16  org.apache.cassandra.cql3.statements.CreateTableStatement$$Lambda$23/1470868839
3044:             1             16  org.apache.cassandra.db.CBuilder$1
3045:             1             16  org.apache.cassandra.db.Clustering$Serializer
3046:             1             16  org.apache.cassandra.db.ClusteringBoundOrBoundary$Serializer
3047:             1             16  org.apache.cassandra.db.ClusteringPrefix$Serializer
3048:             1             16  org.apache.cassandra.db.ColumnFamilyStore$$Lambda$190/1269783694
3049:             1             16  org.apache.cassandra.db.ColumnFamilyStore$2
3050:             1             16  org.apache.cassandra.db.ColumnFamilyStore$FlushLargestColumnFamily
3051:             1             16  org.apache.cassandra.db.Columns$$Lambda$205/2092785251
3052:             1             16  org.apache.cassandra.db.Columns$Serializer
3053:             1             16  org.apache.cassandra.db.CounterMutation$CounterMutationSerializer
3054:             1             16  org.apache.cassandra.db.CounterMutationVerbHandler
3055:             1             16  org.apache.cassandra.db.DataRange$Serializer
3056:             1             16  org.apache.cassandra.db.DecoratedKey$1
3057:             1             16  org.apache.cassandra.db.DefinitionsUpdateVerbHandler
3058:             1             16  org.apache.cassandra.db.DeletionPurger$$Lambda$105/2116697030
3059:             1             16  org.apache.cassandra.db.DeletionTime$Serializer
3060:             1             16  org.apache.cassandra.db.Directories$3
3061:             1             16  org.apache.cassandra.db.Directories$DataDirectory
3062:             1             16  org.apache.cassandra.db.EmptyIterators$EmptyPartitionIterator
3063:             1             16  org.apache.cassandra.db.HintedHandOffManager
3064:             1             16  org.apache.cassandra.db.Keyspace$1
3065:             1             16  org.apache.cassandra.db.MigrationRequestVerbHandler
3066:             1             16  org.apache.cassandra.db.Mutation$MutationSerializer
3067:             1             16  org.apache.cassandra.db.MutationVerbHandler
3068:             1             16  org.apache.cassandra.db.PartitionPosition$RowPositionSerializer
3069:             1             16  org.apache.cassandra.db.PartitionRangeReadCommand$Deserializer
3070:             1             16  org.apache.cassandra.db.ReadCommand$1
3071:             1             16  org.apache.cassandra.db.ReadCommand$1WithoutPurgeableTombstones$$Lambda$110/208106294
3072:             1             16  org.apache.cassandra.db.ReadCommand$2
3073:             1             16  org.apache.cassandra.db.ReadCommand$3
3074:             1             16  org.apache.cassandra.db.ReadCommand$LegacyPagedRangeCommandSerializer
3075:             1             16  org.apache.cassandra.db.ReadCommand$LegacyRangeSliceCommandSerializer
3076:             1             16  org.apache.cassandra.db.ReadCommand$LegacyReadCommandSerializer
3077:             1             16  org.apache.cassandra.db.ReadCommand$Serializer
3078:             1             16  org.apache.cassandra.db.ReadCommandVerbHandler
3079:             1             16  org.apache.cassandra.db.ReadQuery$1
3080:             1             16  org.apache.cassandra.db.ReadRepairVerbHandler
3081:             1             16  org.apache.cassandra.db.ReadResponse$1
3082:             1             16  org.apache.cassandra.db.ReadResponse$LegacyRangeSliceReplySerializer
3083:             1             16  org.apache.cassandra.db.ReadResponse$Serializer
3084:             1             16  org.apache.cassandra.db.SchemaCheckVerbHandler
3085:             1             16  org.apache.cassandra.db.SerializationHeader$Serializer
3086:             1             16  org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer
3087:             1             16  org.apache.cassandra.db.SinglePartitionReadCommand$Group$$Lambda$106/1952605049
3088:             1             16  org.apache.cassandra.db.SizeEstimatesRecorder
3089:             1             16  org.apache.cassandra.db.Slice$Serializer
3090:             1             16  org.apache.cassandra.db.Slices$SelectAllSlices
3091:             1             16  org.apache.cassandra.db.Slices$SelectAllSlices$1
3092:             1             16  org.apache.cassandra.db.Slices$SelectNoSlices
3093:             1             16  org.apache.cassandra.db.Slices$SelectNoSlices$1
3094:             1             16  org.apache.cassandra.db.Slices$Serializer
3095:             1             16  org.apache.cassandra.db.SnapshotCommandSerializer
3096:             1             16  org.apache.cassandra.db.StorageHook$1
3097:             1             16  org.apache.cassandra.db.SystemKeyspace$$Lambda$186/1473888912
3098:             1             16  org.apache.cassandra.db.TruncateResponse$TruncateResponseSerializer
3099:             1             16  org.apache.cassandra.db.TruncateVerbHandler
3100:             1             16  org.apache.cassandra.db.TruncationSerializer
3101:             1             16  org.apache.cassandra.db.WriteResponse
3102:             1             16  org.apache.cassandra.db.WriteResponse$Serializer
3103:             1             16  org.apache.cassandra.db.aggregation.AggregationSpecification$1
3104:             1             16  org.apache.cassandra.db.aggregation.AggregationSpecification$Serializer
3105:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$$Lambda$72/500233312
3106:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$1
3107:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogService$1
3108:             1             16  org.apache.cassandra.db.commitlog.CommitLog$$Lambda$227/2024217158
3109:             1             16  org.apache.cassandra.db.commitlog.CommitLogPosition$1
3110:             1             16  org.apache.cassandra.db.commitlog.CommitLogPosition$CommitLogPositionSerializer
3111:             1             16  org.apache.cassandra.db.commitlog.CommitLogReplayer$$Lambda$228/1186545861
3112:             1             16  org.apache.cassandra.db.commitlog.CommitLogReplayer$MutationInitiator
3113:             1             16  org.apache.cassandra.db.commitlog.CommitLogSegment$$Lambda$175/1833918497
3114:             1             16  org.apache.cassandra.db.commitlog.IntervalSet$1
3115:             1             16  org.apache.cassandra.db.commitlog.SimpleCachedBufferPool$1
3116:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$184/889018651
3117:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$185/638825183
3118:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$242/1509719872
3119:             1             16  org.apache.cassandra.db.compaction.CompactionManager$1
3120:             1             16  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionController$$Lambda$307/363853319
3121:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$133/1728760599
3122:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$134/703363283
3123:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$172/1546684896
3124:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$85/654029265
3125:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$86/2030162789
3126:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$87/1306548322
3127:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$88/973942848
3128:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$89/558033602
3129:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$90/1361733480
3130:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$91/999951331
3131:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$92/1918201666
3132:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$93/1181004273
3133:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$95/1423931162
3134:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$96/1090942546
3135:             1             16  org.apache.cassandra.db.compaction.LeveledManifest$1
3136:             1             16  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy$1
3137:             1             16  org.apache.cassandra.db.context.CounterContext
3138:             1             16  org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer
3139:             1             16  org.apache.cassandra.db.filter.ClusteringIndexNamesFilter$NamesDeserializer
3140:             1             16  org.apache.cassandra.db.filter.ClusteringIndexSliceFilter$SliceDeserializer
3141:             1             16  org.apache.cassandra.db.filter.ColumnFilter$Serializer
3142:             1             16  org.apache.cassandra.db.filter.DataLimits$Serializer
3143:             1             16  org.apache.cassandra.db.filter.RowFilter$CQLFilter
3144:             1             16  org.apache.cassandra.db.filter.RowFilter$Serializer
3145:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$58/435914790
3146:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$59/1273958371
3147:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$64/731243659
3148:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$66/1037955032
3149:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$70/331596257
3150:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$165/1814072734
3151:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$203/2022031193
3152:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$204/1336053009
3153:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$140/1142908098
3154:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$141/423008343
3155:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$142/88843440
3156:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$177/1035048662
3157:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$162/1676168006
3158:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$166/1882192501
3159:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$168/700891016
3160:             1             16  org.apache.cassandra.db.lifecycle.LogTransaction$LogFilesByName$$Lambda$52/894421232
3161:             1             16  org.apache.cassandra.db.lifecycle.LogTransaction$LogFilesByName$$Lambda$54/276869158
3162:             1             16  org.apache.cassandra.db.lifecycle.Tracker$$Lambda$170/1786214274
3163:             1             16  org.apache.cassandra.db.marshal.CollectionType$CollectionPathSerializer
3164:             1             16  org.apache.cassandra.db.monitoring.ApproximateTime$$Lambda$108/2001863314
3165:             1             16  org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer
3166:             1             16  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$$Lambda$107/2345640
3167:             1             16  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer
3168:             1             16  org.apache.cassandra.db.rows.AbstractTypeVersionComparator
3169:             1             16  org.apache.cassandra.db.rows.BTreeRow$$Lambda$118/474868079
3170:             1             16  org.apache.cassandra.db.rows.BTreeRow$$Lambda$123/164389557
3171:             1             16  org.apache.cassandra.db.rows.Cell$$Lambda$101/1913147328
3172:             1             16  org.apache.cassandra.db.rows.Cell$Serializer
3173:             1             16  org.apache.cassandra.db.rows.ColumnData$$Lambda$28/494077446
3174:             1             16  org.apache.cassandra.db.rows.EncodingStats$Serializer
3175:             1             16  org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer
3176:             1             16  org.apache.cassandra.db.rows.UnfilteredSerializer
3177:             1             16  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$194/5263871
3178:             1             16  org.apache.cassandra.db.view.View$$Lambda$219/1557380482
3179:             1             16  org.apache.cassandra.dht.BootStrapper$StringSerializer
3180:             1             16  org.apache.cassandra.dht.Murmur3Partitioner$2
3181:             1             16  org.apache.cassandra.dht.StreamStateStore
3182:             1             16  org.apache.cassandra.dht.Token$TokenSerializer
3183:             1             16  org.apache.cassandra.gms.EchoMessage
3184:             1             16  org.apache.cassandra.gms.EchoMessage$EchoMessageSerializer
3185:             1             16  org.apache.cassandra.gms.EndpointStateSerializer
3186:             1             16  org.apache.cassandra.gms.GossipDigestAck2Serializer
3187:             1             16  org.apache.cassandra.gms.GossipDigestAck2VerbHandler
3188:             1             16  org.apache.cassandra.gms.GossipDigestAckSerializer
3189:             1             16  org.apache.cassandra.gms.GossipDigestAckVerbHandler
3190:             1             16  org.apache.cassandra.gms.GossipDigestSerializer
3191:             1             16  org.apache.cassandra.gms.GossipDigestSynSerializer
3192:             1             16  org.apache.cassandra.gms.GossipDigestSynVerbHandler
3193:             1             16  org.apache.cassandra.gms.GossipShutdownVerbHandler
3194:             1             16  org.apache.cassandra.gms.Gossiper$1
3195:             1             16  org.apache.cassandra.gms.Gossiper$GossipTask
3196:             1             16  org.apache.cassandra.gms.HeartBeatStateSerializer
3197:             1             16  org.apache.cassandra.gms.VersionedValue$VersionedValueFactory
3198:             1             16  org.apache.cassandra.gms.VersionedValue$VersionedValueSerializer
3199:             1             16  org.apache.cassandra.hints.EncodedHintMessage$Serializer
3200:             1             16  org.apache.cassandra.hints.Hint$Serializer
3201:             1             16  org.apache.cassandra.hints.HintMessage$Serializer
3202:             1             16  org.apache.cassandra.hints.HintResponse
3203:             1             16  org.apache.cassandra.hints.HintResponse$Serializer
3204:             1             16  org.apache.cassandra.hints.HintVerbHandler
3205:             1             16  org.apache.cassandra.hints.HintsBuffer$$Lambda$327/1070755303
3206:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$244/955891688
3207:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$245/1579667951
3208:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$246/2099786968
3209:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$282/2033605821
3210:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$283/1986677941
3211:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$284/355640298
3212:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$250/1791992279
3213:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$251/1557383930
3214:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$252/763495689
3215:             1             16  org.apache.cassandra.hints.HintsStore$$Lambda$318/991892116
3216:             1             16  org.apache.cassandra.hints.HintsStore$$Lambda$322/1059094831
3217:             1             16  org.apache.cassandra.hints.HintsWriteExecutor$FsyncWritersTask$$Lambda$289/2053564305
3218:             1             16  org.apache.cassandra.index.Index$CollatedViewIndexBuildingSupport
3219:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$152/111521464
3220:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$153/118079547
3221:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$182/992085984
3222:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$188/887656608
3223:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$312/1070341018
3224:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$1
3225:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$2
3226:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$3
3227:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$4
3228:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$5
3229:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$6
3230:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$7
3231:             1             16  org.apache.cassandra.index.transactions.UpdateTransaction$1
3232:             1             16  org.apache.cassandra.io.compress.CompressionMetadata$ChunkSerializer
3233:             1             16  org.apache.cassandra.io.compress.SnappyCompressor
3234:             1             16  org.apache.cassandra.io.sstable.Descriptor$$Lambda$71/999647352
3235:             1             16  org.apache.cassandra.io.sstable.IndexSummary$IndexSummarySerializer
3236:             1             16  org.apache.cassandra.io.sstable.IndexSummaryManager$1
3237:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$73/1687768728
3238:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$74/15478307
3239:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$75/1394837936
3240:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$1
3241:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$Equals
3242:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$GreaterThan
3243:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$GreaterThanOrEqualTo
3244:             1             16  org.apache.cassandra.io.sstable.format.SSTableReadsListener$1
3245:             1             16  org.apache.cassandra.io.sstable.format.SSTableWriter$$Lambda$160/1520196427
3246:             1             16  org.apache.cassandra.io.sstable.format.SSTableWriter$$Lambda$311/1357900831
3247:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat
3248:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat$ReaderFactory
3249:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory
3250:             1             16  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter$$Lambda$150/504911193
3251:             1             16  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter$$Lambda$151/451889382
3252:             1             16  org.apache.cassandra.io.sstable.metadata.CompactionMetadata$CompactionMetadataSerializer
3253:             1             16  org.apache.cassandra.io.sstable.metadata.StatsMetadata$StatsMetadataSerializer
3254:             1             16  org.apache.cassandra.io.sstable.metadata.ValidationMetadata$ValidationMetadataSerializer
3255:             1             16  org.apache.cassandra.io.util.DataOutputBuffer$1
3256:             1             16  org.apache.cassandra.io.util.DataOutputStreamPlus$1
3257:             1             16  org.apache.cassandra.io.util.FileHandle$$Lambda$158/795408782
3258:             1             16  org.apache.cassandra.io.util.MmappedRegions$State$$Lambda$197/1396226930
3259:             1             16  org.apache.cassandra.io.util.Rebufferer$1
3260:             1             16  org.apache.cassandra.locator.DynamicEndpointSnitch$1
3261:             1             16  org.apache.cassandra.locator.DynamicEndpointSnitch$2
3262:             1             16  org.apache.cassandra.locator.EndpointSnitchInfo
3263:             1             16  org.apache.cassandra.locator.PendingRangeMaps$1
3264:             1             16  org.apache.cassandra.locator.PendingRangeMaps$2
3265:             1             16  org.apache.cassandra.locator.PendingRangeMaps$3
3266:             1             16  org.apache.cassandra.locator.PendingRangeMaps$4
3267:             1             16  org.apache.cassandra.locator.PropertyFileSnitch
3268:             1             16  org.apache.cassandra.locator.PropertyFileSnitch$1
3269:             1             16  org.apache.cassandra.locator.SimpleSeedProvider
3270:             1             16  org.apache.cassandra.locator.TokenMetadata$1
3271:             1             16  org.apache.cassandra.metrics.BufferPoolMetrics$1
3272:             1             16  org.apache.cassandra.metrics.CQLMetrics$1
3273:             1             16  org.apache.cassandra.metrics.CQLMetrics$2
3274:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$82/1609657810
3275:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$83/2101898459
3276:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$84/342161168
3277:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$1
3278:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$2
3279:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$3
3280:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$4
3281:             1             16  org.apache.cassandra.metrics.ClientMetrics
3282:             1             16  org.apache.cassandra.metrics.CompactionMetrics$1
3283:             1             16  org.apache.cassandra.metrics.CompactionMetrics$2
3284:             1             16  org.apache.cassandra.metrics.HintedHandoffMetrics$1
3285:             1             16  org.apache.cassandra.metrics.HintedHandoffMetrics$2
3286:             1             16  org.apache.cassandra.metrics.TableMetrics$1
3287:             1             16  org.apache.cassandra.metrics.TableMetrics$13
3288:             1             16  org.apache.cassandra.metrics.TableMetrics$18
3289:             1             16  org.apache.cassandra.metrics.TableMetrics$20
3290:             1             16  org.apache.cassandra.metrics.TableMetrics$22
3291:             1             16  org.apache.cassandra.metrics.TableMetrics$26
3292:             1             16  org.apache.cassandra.metrics.TableMetrics$28
3293:             1             16  org.apache.cassandra.metrics.ViewWriteMetrics$1
3294:             1             16  org.apache.cassandra.net.IAsyncCallback$1
3295:             1             16  org.apache.cassandra.net.MessagingService$4
3296:             1             16  org.apache.cassandra.net.MessagingService$5
3297:             1             16  org.apache.cassandra.net.MessagingService$CallbackDeterminedSerializer
3298:             1             16  org.apache.cassandra.notifications.SSTableDeletingNotification
3299:             1             16  org.apache.cassandra.repair.NodePair$NodePairSerializer
3300:             1             16  org.apache.cassandra.repair.RepairJobDesc$RepairJobDescSerializer
3301:             1             16  org.apache.cassandra.repair.RepairMessageVerbHandler
3302:             1             16  org.apache.cassandra.repair.messages.AnticompactionRequest$AnticompactionRequestSerializer
3303:             1             16  org.apache.cassandra.repair.messages.CleanupMessage$CleanupMessageSerializer
3304:             1             16  org.apache.cassandra.repair.messages.PrepareMessage$PrepareMessageSerializer
3305:             1             16  org.apache.cassandra.repair.messages.RepairMessage$RepairMessageSerializer
3306:             1             16  org.apache.cassandra.repair.messages.SnapshotMessage$SnapshotMessageSerializer
3307:             1             16  org.apache.cassandra.repair.messages.SyncComplete$SyncCompleteSerializer
3308:             1             16  org.apache.cassandra.repair.messages.SyncRequest$SyncRequestSerializer
3309:             1             16  org.apache.cassandra.repair.messages.ValidationComplete$ValidationCompleteSerializer
3310:             1             16  org.apache.cassandra.repair.messages.ValidationRequest$ValidationRequestSerializer
3311:             1             16  org.apache.cassandra.scheduler.NoScheduler
3312:             1             16  org.apache.cassandra.schema.CQLTypeParser$$Lambda$207/2843617
3313:             1             16  org.apache.cassandra.schema.CompressionParams$Serializer
3314:             1             16  org.apache.cassandra.schema.Functions$$Lambda$236/1017996482
3315:             1             16  org.apache.cassandra.schema.Functions$$Lambda$237/2135117754
3316:             1             16  org.apache.cassandra.schema.Functions$$Lambda$239/854637578
3317:             1             16  org.apache.cassandra.schema.Functions$$Lambda$240/305461269
3318:             1             16  org.apache.cassandra.schema.Functions$Builder$$Lambda$36/146874094
3319:             1             16  org.apache.cassandra.schema.IndexMetadata$Serializer
3320:             1             16  org.apache.cassandra.schema.LegacySchemaMigrator$$Lambda$132/399524457
3321:             1             16  org.apache.cassandra.schema.SchemaKeyspace$$Lambda$216/2137640552
3322:             1             16  org.apache.cassandra.schema.Types$RawBuilder$$Lambda$206/1399449613
3323:             1             16  org.apache.cassandra.schema.Types$RawBuilder$RawUDT$$Lambda$210/2069170964
3324:             1             16  org.apache.cassandra.schema.Views$$Lambda$50/1348115836
3325:             1             16  org.apache.cassandra.serializers.BooleanSerializer
3326:             1             16  org.apache.cassandra.serializers.ByteSerializer
3327:             1             16  org.apache.cassandra.serializers.BytesSerializer
3328:             1             16  org.apache.cassandra.serializers.DecimalSerializer
3329:             1             16  org.apache.cassandra.serializers.DoubleSerializer
3330:             1             16  org.apache.cassandra.serializers.InetAddressSerializer
3331:             1             16  org.apache.cassandra.serializers.Int32Serializer
3332:             1             16  org.apache.cassandra.serializers.LongSerializer
3333:             1             16  org.apache.cassandra.serializers.TimeUUIDSerializer
3334:             1             16  org.apache.cassandra.serializers.TimestampSerializer
3335:             1             16  org.apache.cassandra.serializers.TimestampSerializer$1
3336:             1             16  org.apache.cassandra.serializers.TimestampSerializer$2
3337:             1             16  org.apache.cassandra.serializers.TimestampSerializer$3
3338:             1             16  org.apache.cassandra.serializers.UTF8Serializer
3339:             1             16  org.apache.cassandra.serializers.UUIDSerializer
3340:             1             16  org.apache.cassandra.service.CacheService$CounterCacheSerializer
3341:             1             16  org.apache.cassandra.service.CacheService$KeyCacheSerializer
3342:             1             16  org.apache.cassandra.service.CacheService$RowCacheSerializer
3343:             1             16  org.apache.cassandra.service.CassandraDaemon$$Lambda$273/1244026033
3344:             1             16  org.apache.cassandra.service.CassandraDaemon$1
3345:             1             16  org.apache.cassandra.service.CassandraDaemon$2
3346:             1             16  org.apache.cassandra.service.CassandraDaemon$NativeAccess
3347:             1             16  org.apache.cassandra.service.ClientState$$Lambda$97/466481125
3348:             1             16  org.apache.cassandra.service.ClientWarn
3349:             1             16  org.apache.cassandra.service.DefaultFSErrorHandler
3350:             1             16  org.apache.cassandra.service.EchoVerbHandler
3351:             1             16  org.apache.cassandra.service.LoadBroadcaster
3352:             1             16  org.apache.cassandra.service.LoadBroadcaster$1
3353:             1             16  org.apache.cassandra.service.MigrationManager
3354:             1             16  org.apache.cassandra.service.MigrationManager$MigrationsSerializer
3355:             1             16  org.apache.cassandra.service.NativeTransportService$$Lambda$277/794251840
3356:             1             16  org.apache.cassandra.service.NativeTransportService$$Lambda$279/1246696592
3357:             1             16  org.apache.cassandra.service.PendingRangeCalculatorService$1
3358:             1             16  org.apache.cassandra.service.SnapshotVerbHandler
3359:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$1/1204167249
3360:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$114/1819989346
3361:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$2/1615780336
3362:             1             16  org.apache.cassandra.service.StartupChecks$1
3363:             1             16  org.apache.cassandra.service.StartupChecks$10
3364:             1             16  org.apache.cassandra.service.StartupChecks$11
3365:             1             16  org.apache.cassandra.service.StartupChecks$12
3366:             1             16  org.apache.cassandra.service.StartupChecks$2
3367:             1             16  org.apache.cassandra.service.StartupChecks$3
3368:             1             16  org.apache.cassandra.service.StartupChecks$4
3369:             1             16  org.apache.cassandra.service.StartupChecks$5
3370:             1             16  org.apache.cassandra.service.StartupChecks$6
3371:             1             16  org.apache.cassandra.service.StartupChecks$7
3372:             1             16  org.apache.cassandra.service.StartupChecks$9
3373:             1             16  org.apache.cassandra.service.StorageProxy
3374:             1             16  org.apache.cassandra.service.StorageProxy$1
3375:             1             16  org.apache.cassandra.service.StorageProxy$2
3376:             1             16  org.apache.cassandra.service.StorageProxy$3
3377:             1             16  org.apache.cassandra.service.StorageProxy$4
3378:             1             16  org.apache.cassandra.service.StorageService$$Lambda$259/1361973748
3379:             1             16  org.apache.cassandra.service.StorageService$1
3380:             1             16  org.apache.cassandra.service.paxos.Commit$CommitSerializer
3381:             1             16  org.apache.cassandra.service.paxos.CommitVerbHandler
3382:             1             16  org.apache.cassandra.service.paxos.PrepareResponse$PrepareResponseSerializer
3383:             1             16  org.apache.cassandra.service.paxos.PrepareVerbHandler
3384:             1             16  org.apache.cassandra.service.paxos.ProposeVerbHandler
3385:             1             16  org.apache.cassandra.streaming.ReplicationFinishedVerbHandler
3386:             1             16  org.apache.cassandra.streaming.StreamHook$1
3387:             1             16  org.apache.cassandra.streaming.StreamRequest$StreamRequestSerializer
3388:             1             16  org.apache.cassandra.streaming.StreamSummary$StreamSummarySerializer
3389:             1             16  org.apache.cassandra.streaming.compress.CompressionInfo$CompressionInfoSerializer
3390:             1             16  org.apache.cassandra.streaming.messages.CompleteMessage$1
3391:             1             16  org.apache.cassandra.streaming.messages.FileMessageHeader$FileMessageHeaderSerializer
3392:             1             16  org.apache.cassandra.streaming.messages.IncomingFileMessage$1
3393:             1             16  org.apache.cassandra.streaming.messages.KeepAliveMessage$1
3394:             1             16  org.apache.cassandra.streaming.messages.OutgoingFileMessage$1
3395:             1             16  org.apache.cassandra.streaming.messages.PrepareMessage$1
3396:             1             16  org.apache.cassandra.streaming.messages.ReceivedMessage$1
3397:             1             16  org.apache.cassandra.streaming.messages.RetryMessage$1
3398:             1             16  org.apache.cassandra.streaming.messages.SessionFailedMessage$1
3399:             1             16  org.apache.cassandra.streaming.messages.StreamInitMessage$StreamInitMessageSerializer
3400:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$add
3401:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$atomic_batch_mutate
3402:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate
3403:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$cas
3404:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_cluster_name
3405:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspace
3406:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspaces
3407:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_local_ring
3408:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_partitioner
3409:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_ring
3410:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_schema_versions
3411:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_snitch
3412:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_splits
3413:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_splits_ex
3414:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_token_map
3415:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_version
3416:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query
3417:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query
3418:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query
3419:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql_query
3420:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get
3421:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_count
3422:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_indexed_slices
3423:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_multi_slice
3424:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_paged_slice
3425:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices
3426:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_slice
3427:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$insert
3428:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$login
3429:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$multiget_count
3430:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$multiget_slice
3431:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql3_query
3432:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query
3433:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$remove
3434:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$remove_counter
3435:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$set_cql_version
3436:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$set_keyspace
3437:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family
3438:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_add_keyspace
3439:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family
3440:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace
3441:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family
3442:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_update_keyspace
3443:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$trace_next_query
3444:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$truncate
3445:             1             16  org.apache.cassandra.thrift.CassandraServer
3446:             1             16  org.apache.cassandra.thrift.CassandraServer$1
3447:             1             16  org.apache.cassandra.transport.CBUtil$1
3448:             1             16  org.apache.cassandra.transport.Message$ExceptionHandler
3449:             1             16  org.apache.cassandra.transport.Server$1
3450:             1             16  org.apache.cassandra.transport.messages.AuthChallenge$1
3451:             1             16  org.apache.cassandra.transport.messages.AuthResponse$1
3452:             1             16  org.apache.cassandra.transport.messages.AuthSuccess$1
3453:             1             16  org.apache.cassandra.transport.messages.AuthenticateMessage$1
3454:             1             16  org.apache.cassandra.transport.messages.BatchMessage$1
3455:             1             16  org.apache.cassandra.transport.messages.CredentialsMessage$1
3456:             1             16  org.apache.cassandra.transport.messages.ErrorMessage$1
3457:             1             16  org.apache.cassandra.transport.messages.EventMessage$1
3458:             1             16  org.apache.cassandra.transport.messages.ExecuteMessage$1
3459:             1             16  org.apache.cassandra.transport.messages.OptionsMessage$1
3460:             1             16  org.apache.cassandra.transport.messages.PrepareMessage$1
3461:             1             16  org.apache.cassandra.transport.messages.QueryMessage$1
3462:             1             16  org.apache.cassandra.transport.messages.ReadyMessage$1
3463:             1             16  org.apache.cassandra.transport.messages.RegisterMessage$1
3464:             1             16  org.apache.cassandra.transport.messages.ResultMessage$1
3465:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Prepared$1
3466:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Rows$1
3467:             1             16  org.apache.cassandra.transport.messages.ResultMessage$SchemaChange$1
3468:             1             16  org.apache.cassandra.transport.messages.ResultMessage$SetKeyspace$1
3469:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Void$1
3470:             1             16  org.apache.cassandra.transport.messages.StartupMessage$1
3471:             1             16  org.apache.cassandra.transport.messages.SupportedMessage$1
3472:             1             16  org.apache.cassandra.utils.AlwaysPresentFilter
3473:             1             16  org.apache.cassandra.utils.AsymmetricOrdering$Reversed
3474:             1             16  org.apache.cassandra.utils.BloomFilter$1
3475:             1             16  org.apache.cassandra.utils.BooleanSerializer
3476:             1             16  org.apache.cassandra.utils.Clock
3477:             1             16  org.apache.cassandra.utils.CoalescingStrategies$1
3478:             1             16  org.apache.cassandra.utils.CoalescingStrategies$2
3479:             1             16  org.apache.cassandra.utils.EstimatedHistogram$EstimatedHistogramSerializer
3480:             1             16  org.apache.cassandra.utils.FBUtilities$1
3481:             1             16  org.apache.cassandra.utils.FastByteOperations$UnsafeOperations
3482:             1             16  org.apache.cassandra.utils.Interval$1
3483:             1             16  org.apache.cassandra.utils.Interval$2
3484:             1             16  org.apache.cassandra.utils.JMXServerUtils$Exporter
3485:             1             16  org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper
3486:             1             16  org.apache.cassandra.utils.JVMStabilityInspector$Killer
3487:             1             16  org.apache.cassandra.utils.MerkleTree$Hashable$HashableSerializer
3488:             1             16  org.apache.cassandra.utils.MerkleTree$Inner$InnerSerializer
3489:             1             16  org.apache.cassandra.utils.MerkleTree$Leaf$LeafSerializer
3490:             1             16  org.apache.cassandra.utils.MerkleTree$MerkleTreeSerializer
3491:             1             16  org.apache.cassandra.utils.MerkleTrees$MerkleTreesSerializer
3492:             1             16  org.apache.cassandra.utils.NanoTimeToCurrentTimeMillis$$Lambda$255/703776031
3493:             1             16  org.apache.cassandra.utils.NativeLibraryLinux
3494:             1             16  org.apache.cassandra.utils.NoSpamLogger$1
3495:             1             16  org.apache.cassandra.utils.StreamingHistogram$$Lambda$76/244613162
3496:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder$$Lambda$136/1321552491
3497:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder$$Lambda$137/732447846
3498:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramSerializer
3499:             1             16  org.apache.cassandra.utils.SystemTimeSource
3500:             1             16  org.apache.cassandra.utils.UUIDGen
3501:             1             16  org.apache.cassandra.utils.UUIDSerializer
3502:             1             16  org.apache.cassandra.utils.btree.BTree$$Lambda$193/1448037571
3503:             1             16  org.apache.cassandra.utils.btree.UpdateFunction$$Lambda$29/24650043
3504:             1             16  org.apache.cassandra.utils.concurrent.Ref$ReferenceReaper
3505:             1             16  org.apache.cassandra.utils.memory.BufferPool$1
3506:             1             16  org.apache.cassandra.utils.memory.BufferPool$2
3507:             1             16  org.apache.cassandra.utils.memory.HeapAllocator
3508:             1             16  org.apache.cassandra.utils.vint.VIntCoding$1
3509:             1             16  org.apache.thrift.TProcessorFactory
3510:             1             16  org.apache.thrift.transport.TFramedTransport$Factory
3511:             1             16  org.cliffc.high_scale_lib.NonBlockingHashMap$Prime
3512:             1             16  org.cliffc.high_scale_lib.NonBlockingHashSet
3513:             1             16  org.codehaus.jackson.map.deser.std.AtomicBooleanDeserializer
3514:             1             16  org.codehaus.jackson.map.deser.std.ClassDeserializer
3515:             1             16  org.codehaus.jackson.map.deser.std.DateDeserializer
3516:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$CurrencyDeserializer
3517:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$InetAddressDeserializer
3518:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$LocaleDeserializer
3519:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$PatternDeserializer
3520:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$TimeZoneDeserializer
3521:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$URIDeserializer
3522:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$URLDeserializer
3523:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$UUIDDeserializer
3524:             1             16  org.codehaus.jackson.map.deser.std.JavaTypeDeserializer
3525:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers
3526:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$BooleanDeser
3527:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$ByteDeser
3528:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$CharDeser
3529:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$DoubleDeser
3530:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$FloatDeser
3531:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$IntDeser
3532:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$LongDeser
3533:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$ShortDeser
3534:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$StringDeser
3535:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$BigDecimalDeserializer
3536:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$BigIntegerDeserializer
3537:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$NumberDeserializer
3538:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$SqlDateDeserializer
3539:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$StackTraceElementDeserializer
3540:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$BoolKD
3541:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$ByteKD
3542:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$CharKD
3543:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$DoubleKD
3544:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$FloatKD
3545:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$IntKD
3546:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$LongKD
3547:             1             16  org.codehaus.jackson.map.deser.std.StringDeserializer
3548:             1             16  org.codehaus.jackson.map.deser.std.TimestampDeserializer
3549:             1             16  org.codehaus.jackson.map.deser.std.TokenBufferDeserializer
3550:             1             16  org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer
3551:             1             16  org.codehaus.jackson.map.ext.OptionalHandlerFactory
3552:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector
3553:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$GetterMethodFilter
3554:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$MinimalMethodFilter
3555:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$SetterAndGetterMethodFilter
3556:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$SetterMethodFilter
3557:             1             16  org.codehaus.jackson.map.introspect.JacksonAnnotationIntrospector
3558:             1             16  org.codehaus.jackson.map.ser.StdSerializers$DoubleSerializer
3559:             1             16  org.codehaus.jackson.map.ser.StdSerializers$FloatSerializer
3560:             1             16  org.codehaus.jackson.map.ser.StdSerializers$IntLikeSerializer
3561:             1             16  org.codehaus.jackson.map.ser.StdSerializers$IntegerSerializer
3562:             1             16  org.codehaus.jackson.map.ser.StdSerializers$LongSerializer
3563:             1             16  org.codehaus.jackson.map.ser.StdSerializers$SqlDateSerializer
3564:             1             16  org.codehaus.jackson.map.ser.StdSerializers$SqlTimeSerializer
3565:             1             16  org.codehaus.jackson.map.ser.impl.UnknownSerializer
3566:             1             16  org.codehaus.jackson.map.ser.std.CalendarSerializer
3567:             1             16  org.codehaus.jackson.map.ser.std.DateSerializer
3568:             1             16  org.codehaus.jackson.map.ser.std.NullSerializer
3569:             1             16  org.codehaus.jackson.map.ser.std.StdArraySerializers$ByteArraySerializer
3570:             1             16  org.codehaus.jackson.map.ser.std.StdArraySerializers$CharArraySerializer
3571:             1             16  org.codehaus.jackson.map.ser.std.StringSerializer
3572:             1             16  org.codehaus.jackson.map.ser.std.ToStringSerializer
3573:             1             16  org.codehaus.jackson.map.type.TypeParser
3574:             1             16  org.codehaus.jackson.node.JsonNodeFactory
3575:             1             16  org.github.jamm.MemoryLayoutSpecification$2
3576:             1             16  org.github.jamm.MemoryMeter$1
3577:             1             16  org.github.jamm.NoopMemoryMeterListener
3578:             1             16  org.github.jamm.NoopMemoryMeterListener$1
3579:             1             16  org.slf4j.helpers.NOPLoggerFactory
3580:             1             16  org.slf4j.helpers.SubstituteLoggerFactory
3581:             1             16  org.slf4j.impl.StaticMDCBinder
3582:             1             16  org.xerial.snappy.SnappyNative
3583:             1             16  org.yaml.snakeyaml.constructor.SafeConstructor$ConstructUndefined
3584:             1             16  org.yaml.snakeyaml.external.com.google.gdata.util.common.base.UnicodeEscaper$2
3585:             1             16  sun.management.ClassLoadingImpl
3586:             1             16  sun.management.HotSpotDiagnostic
3587:             1             16  sun.management.ManagementFactoryHelper$PlatformLoggingImpl
3588:             1             16  sun.misc.ASCIICaseInsensitiveComparator
3589:             1             16  sun.misc.FloatingDecimal$1
3590:             1             16  sun.misc.FormattedFloatingDecimal$1
3591:             1             16  sun.misc.Launcher
3592:             1             16  sun.misc.Launcher$Factory
3593:             1             16  sun.misc.ObjectInputFilter$Config$$Lambda$294/1344368391
3594:             1             16  sun.misc.Perf
3595:             1             16  sun.misc.Unsafe
3596:             1             16  sun.net.DefaultProgressMeteringPolicy
3597:             1             16  sun.net.ExtendedOptionsImpl$$Lambda$253/1943122657
3598:             1             16  sun.net.www.protocol.file.Handler
3599:             1             16  sun.net.www.protocol.jar.JarFileFactory
3600:             1             16  sun.nio.ch.EPollSelectorProvider
3601:             1             16  sun.nio.ch.ExtendedSocketOption$1
3602:             1             16  sun.nio.ch.FileChannelImpl$1
3603:             1             16  sun.nio.ch.Net$1
3604:             1             16  sun.nio.ch.Util$1
3605:             1             16  sun.nio.fs.LinuxFileSystemProvider
3606:             1             16  sun.reflect.GeneratedConstructorAccessor12
3607:             1             16  sun.reflect.GeneratedConstructorAccessor18
3608:             1             16  sun.reflect.GeneratedMethodAccessor10
3609:             1             16  sun.reflect.GeneratedMethodAccessor11
3610:             1             16  sun.reflect.GeneratedMethodAccessor12
3611:             1             16  sun.reflect.GeneratedMethodAccessor13
3612:             1             16  sun.reflect.GeneratedMethodAccessor14
3613:             1             16  sun.reflect.GeneratedMethodAccessor15
3614:             1             16  sun.reflect.GeneratedMethodAccessor6
3615:             1             16  sun.reflect.GeneratedMethodAccessor7
3616:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor36
3617:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor37
3618:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor38
3619:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor39
3620:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor40
3621:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor41
3622:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor42
3623:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor43
3624:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor44
3625:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor45
3626:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor46
3627:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor47
3628:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor49
3629:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor50
3630:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor51
3631:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor52
3632:             1             16  sun.reflect.ReflectionFactory
3633:             1             16  sun.reflect.generics.tree.BooleanSignature
3634:             1             16  sun.reflect.generics.tree.BottomSignature
3635:             1             16  sun.reflect.generics.tree.VoidDescriptor
3636:             1             16  sun.rmi.registry.RegistryImpl$$Lambda$8/817299424
3637:             1             16  sun.rmi.registry.RegistryImpl$$Lambda$9/2031951755
3638:             1             16  sun.rmi.registry.RegistryImpl_Skel
3639:             1             16  sun.rmi.registry.RegistryImpl_Stub
3640:             1             16  sun.rmi.runtime.Log$LoggerLogFactory
3641:             1             16  sun.rmi.runtime.RuntimeUtil
3642:             1             16  sun.rmi.server.LoaderHandler$2
3643:             1             16  sun.rmi.server.UnicastServerRef$HashToMethod_Maps
3644:             1             16  sun.rmi.transport.DGCImpl$$Lambda$6/516537656
3645:             1             16  sun.rmi.transport.DGCImpl$2$$Lambda$7/1023268896
3646:             1             16  sun.rmi.transport.DGCImpl_Skel
3647:             1             16  sun.rmi.transport.DGCImpl_Stub
3648:             1             16  sun.rmi.transport.Target$$Lambda$339/2000963151
3649:             1             16  sun.rmi.transport.proxy.RMIDirectSocketFactory
3650:             1             16  sun.rmi.transport.tcp.TCPTransport$1
3651:             1             16  sun.security.rsa.RSAKeyFactory
3652:             1             16  sun.security.ssl.EphemeralKeyManager
3653:             1             16  sun.security.util.ByteArrayLexOrder
3654:             1             16  sun.security.util.ByteArrayTagOrder
3655:             1             16  sun.text.normalizer.NormalizerBase$Mode
3656:             1             16  sun.text.normalizer.NormalizerBase$NFCMode
3657:             1             16  sun.text.normalizer.NormalizerBase$NFDMode
3658:             1             16  sun.text.normalizer.NormalizerBase$NFKCMode
3659:             1             16  sun.text.normalizer.NormalizerBase$NFKDMode
3660:             1             16  sun.util.calendar.Gregorian
3661:             1             16  sun.util.locale.provider.AuxLocaleProviderAdapter$NullProvider
3662:             1             16  sun.util.locale.provider.CalendarDataUtility$CalendarWeekParameterGetter
3663:             1             16  sun.util.locale.provider.SPILocaleProviderAdapter
3664:             1             16  sun.util.resources.LocaleData
3665:             1             16  sun.util.resources.LocaleData$LocaleDataResourceBundleControl
Total     119374210     4034601936
{code}

","Operation System: Debian Jessie
Java: Oracle JDK 1.8.0_151
Cassandra: 3.11.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Jul/18 14:07;molsson;CASSANDRA-14096-3.0.patch;https://issues.apache.org/jira/secure/attachment/12930399/CASSANDRA-14096-3.0.patch,15/Dec/18 20:08;jolynch;Merkle_On_Heap_Sizes.png;https://issues.apache.org/jira/secure/attachment/12951908/Merkle_On_Heap_Sizes.png,15/Dec/18 02:43;jolynch;Repair_Retaining_Merkel_Trees.png;https://issues.apache.org/jira/secure/attachment/12951877/Repair_Retaining_Merkel_Trees.png,15/Dec/18 02:43;jolynch;Trees_Retained_SyncingTasks.png;https://issues.apache.org/jira/secure/attachment/12951875/Trees_Retained_SyncingTasks.png,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2017-12-05 09:06:53.712,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 14 21:29:23 UTC 2019,,,,,,0|i3njdb:,9223372036854775807,,,,,,,,bdeggleston,bdeggleston,,,,,,,,,,"05/Dec/17 09:06;spodxx@gmail.com;Can you try to repair tables one by one and see if any particular table and schema is causing the issue?
Did you upgrade from an older Cassandra version before?","05/Dec/17 10:30;serhatd;[~spodxx@gmail.com] 
I tried one by one, it seems big ones (from point of my view) causing the issue.
I upgraded from DDC 3.5.","05/Dec/17 17:56;molsson;Seeing as it was merkle trees using the memory I remembered CASSANDRA-11390 and I believe that [this|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1421] line could be the cause of this:

{code}
int maxDepth = rangeOwningRatio > 0 ? (int) Math.floor(20 - Math.log(1 / rangeOwningRatio) / Math.log(2)) : 0;
{code}

I believe the '-' sign in the middle should actually be a '+' since log(1 / rangeOwningRatio) will produce a negative result. As it is, this could create merkle trees with a depth larger than 20 instead of lower which was the intended behavior and that could explain the higher memory usage. Do you have the possibility to patch this and verify if this indeed was the cause?","06/Dec/17 07:26;serhatd;OK, I will patch and test.","11/Dec/17 14:29;krummas;yeah looks like I missed changing that to a + during the process of CASSANDRA-11390, want to create a patch [~molsson]?

edit: never mind, since we do {{Math.log(1 / rangeOwningRatio)}} it should still be positive right?","11/Dec/17 14:39;serhatd;Changed the ""-"" sign to ""+"", still repairing a node causes high memory usage ","12/Dec/17 17:32;molsson;[~krummas] Yes, that should be the case, I was a bit hasty in my judgement of the log-function unfortunately.

As far as I can tell there has been one other ticket changing merkle tree size calculations, CASSANDRA-12580. But it does not seem like it should affect the total maximum size of the trees?

Assuming that the tree size limitations apply as before CASSANDRA-5220 (max 2^20 for a single repair session) and with a replication factor of 3 the total amount of merkle tree nodes in memory should not be more than (2^20) * 3, if I'm not mistaken. The assumption builds on that the three merkle trees accumulate in the repair coordinator. In the heap dump both the number of inner and leaf nodes are roughly 10 times that number. It's also roughly the same number of LongTokens in memory which seems odd.

[~serhatd] How many partitions are there in the table if you run nodetool tablestats?","08/Jan/18 21:38;syegournov;I have similar issue on 3.11.1. IMHO [https://issues.apache.org/jira/browse/CASSANDRA-11390] works well - I verified the depth of trees created with respect to rangeOwningRatio  (I added some extra logging to log each instance of MerkleTree being created):

{code:java}

DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,574 CompactionManager.java:1427 - Created single merkle tree with size 65536 and maxDepth determined to be 16, numPartitions 587271, allPartitions 8774183
DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,574 CompactionManager.java:1427 - Created single merkle tree with size 16384 and maxDepth determined to be 14, numPartitions 219402, allPartitions 8774183
DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,574 CompactionManager.java:1427 - Created single merkle tree with size 8192 and maxDepth determined to be 13, numPartitions 122506, allPartitions 8774183
DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,575 CompactionManager.java:1427 - Created single merkle tree with size 32768 and maxDepth determined to be 15, numPartitions 476682, allPartitions 8774183
DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,575 CompactionManager.java:1427 - Created single merkle tree with size 65536 and maxDepth determined to be 16, numPartitions 648328, allPartitions 8774183
DEBUG [ValidationExecutor:256] 2018-01-08 15:56:07,575 CompactionManager.java:1427 - Created single merkle tree with size 32768 and maxDepth determined to be 15, numPartitions 435338, allPartitions 8774183
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,619 CompactionManager.java:1427 - Created single merkle tree with size 16384 and maxDepth determined to be 14, numPartitions 70665, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,619 CompactionManager.java:1427 - Created single merkle tree with size 65536 and maxDepth determined to be 16, numPartitions 280970, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,620 CompactionManager.java:1427 - Created single merkle tree with size 65536 and maxDepth determined to be 16, numPartitions 301579, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,620 CompactionManager.java:1427 - Created single merkle tree with size 32768 and maxDepth determined to be 15, numPartitions 153994, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,620 CompactionManager.java:1427 - Created single merkle tree with size 131072 and maxDepth determined to be 17, numPartitions 690696, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,620 CompactionManager.java:1427 - Created single merkle tree with size 131072 and maxDepth determined to be 17, numPartitions 836873, allPartitions 3773014
DEBUG [ValidationExecutor:257] 2018-01-08 15:56:07,621 CompactionManager.java:1427 - Created single merkle tree with size 262144 and maxDepth determined to be 18, numPartitions 1437704, allPartitions 3773014
{code}

Here is tablestats (RF=3 in each of the two datacenters, 256 vnodes each node) and first live histogram from repair coordinator (running full repair). The first histogram was taken on coordinator when not a single Merkle Tree was yet received (i.e. no ""Received merkle tree for %s from %s"" logged yet).

{code:java}

Read Count: 509888
	Read Latency: 0.18279529229948538 ms.
	Write Count: 748705
	Write Latency: 0.018887297400177642 ms.
	Pending Flushes: 0
		Table: document_signature
		SSTable count: 12
		Space used (live): 71321036931
		Space used (total): 71321036931
		Space used by snapshots (total): 0
		Off heap memory used (total): 860944484
		SSTable Compression Ratio: 0.7950665041038063
		Number of partitions (estimate): 418442461
		Memtable cell count: 8847
		Memtable data size: 537349
		Memtable off heap memory used: 634552
		Memtable switch count: 1
		Local read count: 3542
		Local read latency: 0.426 ms
		Local write count: 16916
		Local write latency: 0.020 ms
		Pending flushes: 0
		Percent repaired: 97.79
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used: 586304912
		Bloom filter off heap memory used: 586304816
		Index summary off heap memory used: 268176412
		Compression metadata off heap memory used: 5828704
		Compacted partition minimum bytes: 87
		Compacted partition maximum bytes: 124
		Compacted partition mean bytes: 123
		Average live cells per slice (last five minutes): 1.0
		Maximum live cells per slice (last five minutes): 1
		Average tombstones per slice (last five minutes): 1.0
		Maximum tombstones per slice (last five minutes): 1
		Dropped Mutations: 0

{code}
 
{code:java}
  #      Instances          Bytes  Type
   1:      45135165     2166487920  org.apache.cassandra.utils.MerkleTree$Inner
   2:      45215104     1808604160  java.math.BigInteger
   3:      45218792     1447964176  [I
   4:      45136673     1444373536  org.apache.cassandra.utils.MerkleTree$Leaf
   5:      45213909      723422544  org.apache.cassandra.dht.RandomPartitioner$BigIntegerToken
   6:       3293590      171240328  [B
   7:        239596       15581200  [Ljava.lang.Object;
   8:        201671       12906944  java.nio.DirectByteBuffer
   9:        105623        9402016  [C
  10:        136213        4358816  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node
  11:        136213        4358816  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
  12:         12312        3890512  [J
  13:         88490        3539600  org.apache.cassandra.db.rows.BufferCell
  14:        137639        3303336  org.apache.cassandra.db.RowIndexEntry
  15:        136278        3270672  org.apache.cassandra.cache.KeyCacheKey
  16:        136213        3269112  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
  17:        104492        2507808  java.lang.String
  18:         77919        2493408  org.apache.cassandra.db.rows.BTreeRow
...
 83:          1508          72384  org.apache.cassandra.utils.MerkleTree

{code}

To me it looks like validation tasks on the coordinating node itself already take quite some by memory for MerkleTrees - and then when MerkleTrees start to accumulate from other nodes things get worse and worse till JVM crash:

{code:java}
   #      Instances          Bytes  Type
   1:      50976925     2446892400  org.apache.cassandra.utils.MerkleTree$Inner
   2:      51092256     2043690240  java.math.BigInteger
   3:      51096217     1636064888  [I
   4:      50978516     1631312512  org.apache.cassandra.utils.MerkleTree$Leaf
   5:      51091061      817456976  org.apache.cassandra.dht.RandomPartitioner$BigIntegerToken
   6:      12850540      598727368  [B
   7:        346308       18478816  [Ljava.lang.Object;
   8:        274678       17579392  java.nio.DirectByteBuffer
   9:        106347        9386112  [C
  10:        123749        4949960  org.apache.cassandra.db.rows.BufferCell
  11:        153443        4910176  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node
  12:        153443        4910176  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
  13:         12693        4000864  [J
  14:        154841        3716184  org.apache.cassandra.db.RowIndexEntry
  15:        153508        3684192  org.apache.cassandra.cache.KeyCacheKey
  16:        153443        3682632  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
  17:        113042        3617344  org.apache.cassandra.db.rows.BTreeRow
  18:        106816        3418112  org.apache.cassandra.db.rows.EncodingStats

{code}

And another one, few seconds before JVM became unresponsive:

{code:java}
   #      Instances          Bytes  Type
   1:      86668240     4160075520  org.apache.cassandra.utils.MerkleTree$Inner
   2:      86866263     3474650520  java.math.BigInteger
   3:      86870454     2780862192  [I
   4:      86670565     2773458080  org.apache.cassandra.utils.MerkleTree$Leaf
   5:      56604877     2664969056  [B
   6:      86865068     1389841088  org.apache.cassandra.dht.RandomPartitioner$BigIntegerToken
   7:        443148       28361472  java.nio.DirectByteBuffer
   8:        592525       24834288  [Ljava.lang.Object;
   9:        107122        9437936  [C
  10:        205317        8212680  org.apache.cassandra.db.rows.BufferCell
  11:        197149        6308768  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node
  12:        197149        6308768  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
  13:        194391        6220512  org.apache.cassandra.db.rows.BTreeRow
  14:        188155        6020960  org.apache.cassandra.db.rows.EncodingStats
  15:        187964        6014848  org.apache.cassandra.db.partitions.AbstractBTreePartition$Holder
  16:        187960        6014720  org.apache.cassandra.db.partitions.AtomicBTreePartition
  17:        187518        6000576  org.apache.cassandra.db.LivenessInfo$ExpiringLivenessInfo
  18:        198500        4764000  org.apache.cassandra.db.RowIndexEntry
  19:        197215        4733160  org.apache.cassandra.cache.KeyCacheKey
  20:        197149        4731576  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
  21:        194451        4666824  org.apache.cassandra.db.BufferClustering
  22:        189176        4540224  java.util.concurrent.ConcurrentSkipListMap$Node
  23:        188127        4515048  org.apache.cassandra.db.PartitionColumns
  24:        188017        4512408  org.apache.cassandra.db.Columns
...
  83:          2325         111600  org.apache.cassandra.utils.MerkleTree
{code}

The only way I was able to run repairs was incremental repair with in sequential mode (obviously I had to mark all sstables as repaired and maintain high repaired %). But this does not work all the time because there is another bug with a race condition causing assertion error (that's a separate bug so I won't cover the details here)

IMHO the main problem is that all repair sessions run in parallel - and with high RF and multiple DCs it's very easy to fill up JVM even with the way depth of the MerkleTree is calculated using ratio of owned partitions for the range. Coordinating node in turn suffers even more - because it has MerkleTrees allocated as a part of doValidationCompaction() and also accumulates MerkleTrees from the cluster in ValidationTask as part of its coordinating activity.","08/Jan/18 21:55;jjirsa;Number of elements in a binary tree of depth h should be {{2^(h+1) - 1}}, so if we build trees with h=20, then we're looking at about half a 2M {{Inner}} nodes at 48 bytes per for each table (for each replica). Your histogram shows ~86M, which would be ~43 tables, which seems like it'd be on the order of ""expected"" in terms of schemas - 4G of Inner nodes for that does indeed seem painful.

I suspect we may be able to do better here (not sure yet, havent really thought about it), but from a practical standpoint, subrange repair may be a bit easier on your heap, too.","25/Jan/18 08:04;syegournov;[~jjirsa] Yes, I know that depth of trees is on the expected level - just wanted to make a point that the change to dynamically calculate the depth of tree with rangeOwningRatio in mind seems to work quite well.

Indeed from practical standpoint I cannot see how full/incremental repairs can be used at all in production because even if the column family repair does not put such a big pressure on GC - it still hurts higher percentile read latencies badly. Not to mention that during long repairs there is high risk of a repair failure due to gossip detecting node down - and restarting from scratch is a pain. IMHO subrange repair with persisted repair state (e.g. with Cassandra Reaper) is the only real life option.

Because basically both full/incremental repairs can hurt cluster badly in a matter of minutes - not to mention incremental repair having its own bugs. Considering the impact of full/incremental repairs maybe it's worth to warn a user that things might go really bad.","05/Jul/18 14:06;molsson;*TL;DR*
I believe the original JIRA has two problems, CASSANDRA-14332 as well as my findings below.

We store MerkleTrees in the repair session until a full keyspace has been repaired. So it seems this problem gets worse the more tables you have in a single keyspace.
*TL;DR*


When I noticed CASSANDRA-14332 I thought it might be related to this ticket as simultaneous validation compactions would result in multiple MerkleTrees. Even for repairing a single table we could perform multiple validation compactions if we have both vnodes and *nodes > replication factor*.

So I decided to set up a three-node cluster with RF=3 and check the difference with and without CASSANDRA-14332. In my setup I had ten tables in a single keyspace with 20 million partitions in each of the tables to make sure the repairs were long running as well as using the max MerkleTree size. All tests were performed on version 3.0.15, but I believe they would apply for more recent versions as well (3.0.x, 3.11.x and maybe even 4.x).

Without CASSANDRA-14332 you could see multiple validation compactions starting at the same time (as expected) and the top memory consumers were:
{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:      22412689     1075809072  org.apache.cassandra.utils.MerkleTree$Inner
   2:      21801471     1060797400  [B
   3:      22438801      718041632  org.apache.cassandra.utils.MerkleTree$Leaf
   4:      22466069      539185656  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   5:        105245        8449496  [C
   6:         37102        4554912  [Ljava.lang.Object;
   7:        110463        4418520  java.util.TreeMap$Entry
{code}
~3GB MerkleTrees/byte arrays/long tokens.

With CASSANDRA-14332 the validation compactions started one by one (as expected) and the heap started out with:
{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:        660043       47091600  [B
   2:        747730       35891040  org.apache.cassandra.utils.MerkleTree$Inner
   3:        748498       23951936  org.apache.cassandra.utils.MerkleTree$Leaf
   4:        750376       18009024  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   5:        103072        8418992  [C
   6:         21823        3787144  [Ljava.lang.Object;
   7:          4496        3528432  [J
{code}
But after a while you could see the following build-up:
{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:      24055994     1171217840  [B
   2:      12702738      609731424  org.apache.cassandra.utils.MerkleTree$Inner
   3:      12717330      406954560  org.apache.cassandra.utils.MerkleTree$Leaf
   4:      12733033      305592792  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   5:        102947        8382984  [C
   6:         23384        3826696  [Ljava.lang.Object;
   7:          4793        3552624  [J
{code}
~2GB MerkleTrees/byte arrays/long tokens.

After creating a heap dump I could see that the MerkleTrees were being kept in the map [*RepairSession#syncingTasks*|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/repair/RepairSession.java#L99]. This map isn't cleared until all tables in the keyspace has been repaired so it seems like this problem gets worse the more tables you have in a single keyspace.

+Background+
After validations complete the trees are sent to the repair coordinator which collects and compares them to see what needs to be streamed. The actual work of checking the MerkleTree difference and synchronizing the data is handed off to a separate task (LocalSyncTask or RemoteSyncTask). In the case of RemoteSyncTasks they are stored in *RepairSession#syncingTasks* and waits for the remote replica to notify it that it has synchronized the data with the other replica. The problem is that the SyncTasks keeps the MerkleTrees and since they are stored in the map they can't be GC:d until they are removed from the map. This doesn't happen until the full session is completed.

+Potential solutions+
*1. Calculate MerkleTrees difference before handing it off to RemoteSyncTask*
Avoid storing the MerkleTrees in the SyncTasks by calculating the difference before creating the task.
This could still cause some build-up of memory (as we still store them all in the map) but can probably be neglected.

This approach could potentially affect the performance of the repairs as the calculations will be performed in a single thread. A similar approach seems to be taken in 4.0 though for the [optimized stream path|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/repair/RepairJob.java#L205].

*2. Remove MerkleTrees as sync requests completes* (patch attached)
When we get a response from the remote replica that the sync was completed (*RepairSession#syncComplete*) we can remove the RemoteSyncTask from the map instead of [retrieving it|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/repair/RepairSession.java#L190].
But then we still have the scenario when there are no differences in the MerkleTrees as that would not send a sync request and in turn not trigger the removal.
In that case we could make sure to remove the left-overs when all nodes have been synchronized for the table.

I believe this is a rather non-invasive approach although it feels kind of hackish.

With the patch applied the maximum memory usage I observed was:
{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:       2238987      125061864  [B
   2:       2241647      107599056  org.apache.cassandra.utils.MerkleTree$Inner
   3:       2243951       71806432  org.apache.cassandra.utils.MerkleTree$Leaf
   4:       2247405       53937720  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   5:        102724        8344368  [C
   6:         22298        3855408  [Ljava.lang.Object;
   7:          4700        3558992  [J
{code}
This was from when the repair coordinator had not finished the validation compaction for a table while the two replicas had.

After the coordinator was finished and started on the next table it went down to the expected single MerkleTree:
{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:        745967       35806416  org.apache.cassandra.utils.MerkleTree$Inner
   2:        746735       23895520  org.apache.cassandra.utils.MerkleTree$Leaf
   3:         28892       19281632  [B
   4:        748653       17967672  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   5:        102882        8371456  [C
   6:         22386        3858952  [Ljava.lang.Object;
   7:          4712        3559952  [J
{code}

Feel free to try out the patch in combination with CASSANDRA-14332. It should apply cleanly to 3.11.x but trunk has changed somewhat so I wouldn't feel confident in supplying a patch without more verification.


Sorry for the long comment but I won't be able to stay updated on this JIRA ticket in the coming weeks so I tried to get as much information as possible in.",24/Aug/18 03:09;madega;Checking if there is a patch fix/release for this?  It appears the issue is also on 3.11.2,"24/Aug/18 08:08;molsson;[~madega], do you also get OOM while running repair on 3.11.2?

If that's the case, how is your schema? Do you have a large number of tables? That could be related to either CASSANDRA-14332 which was fixed in 3.11.3 or to the issue I mentioned above. A patch for the above issue is available on this ticket if you want to try it out, but it's probably not that useful without CASSANDRA-14332 as well.","05/Nov/18 18:49;jjirsa;Removing ready-to-commit. Anonymous visitors, please don't change the status. ","13/Dec/18 23:27;bdeggleston;I think it would be better to just go with option #1 and compute the differences as soon as possible. I wouldn’t consider changing the SyncTask ctor args from {{TreeResponse r1, TreeResponse r2}} to {{InetAddress ep1, InetAddress ep2, List<Range<Token>> differences}} as an invasive change. It also means we don’t keep the trees in memory any longer than they’re useful, and not holding onto them during streaming is a plus.
","13/Dec/18 23:32;bdeggleston;removing the [4.0-feature-freeze-review-requested|https://issues.apache.org/jira/issues/?jql=labels+%3D+4.0-feature-freeze-review-requested] tag, since CASSANDRA-3200 fixes this issue in trunk","15/Dec/18 02:43;jolynch;[~bdeggleston] we just got hit by this in a 3.0.17 single token per node production cluster while running parallel subrange repair on a single table at a time. I think it's because when CASSANDRA-5263 expanded Merkle max tree depth to 20 ([source|https://github.com/apache/cassandra/blob/1816520d6c59cece5ef8346c95e4f12e7c285751/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1220]) it looks like we can allocate close to ~190 MiB _per tree_ since Cassandra 2.1 (the estimation logic changed a bunch but the max of 20 is still there). With a RF of e.g. 3*3 = 9 we allocate gigabytes of memory per {{RepairSession}} and then with this bug we hold it on heap for the duration of the repair. Then when we do parallel subranges to speed up full repair, e.g. 5 of them, we can easily run out of memory. I agree with the rest of this report that we shouldn't be holding them on heap for the duration of the repair, but I also think we shouldn't allocating that much heap in the first place.

According to my heap dumps of the node each Merkle tree has {{2^20}} leaves (and ~{{2^20}} inner nodes) which are retaining 191 MiB of heap memory resident per tree, and we have RF of them (so in our case 3 * 3 = 9). This leads to 191 MiB * 9 = 1.7 GiB per repair session of memory that is held on the heap and due to the bug talked about in this ticket we hold that for the entire repair session. If we then do 4 or 5 parallel subranges we quickly allocate 8+ GiB of memory and can OOM the node. What do you think about the following changes I could make:
 # Per your recommendation above, don't hold onto the trees for the entire repair
 # Provide a yaml configuration option called {{repair_session_heap_space_in_mb}} which defaults to something like 1% of the heap (so 80MB in the 8GB use case). Then we use this information and the RF information of the keyspace to decide the maximum depth of a single merkle tree instead of arbitrarily picking {{2^15}} or {{2^20}}.

I think we can work backwards to acceptable max tree size with something like the following formula (someone please check my math):
{noformat}
sizeof(leaf) = sizeof(hash) + rowsInRange + sizeOfRange + object_overhead
             = 48           + 8           + 8           + 16 
             = 80 bytes
sizeof(inner) = sizeof(leaf) + 2 * sizeof(pointer) + sizeof(token)
              = 80           + 8                   + 24           
              = 112 bytes

In a merkle tree of depth (n)

#leafs = 2^n
#inner = 2^n - 1 ~= 2^n

So for a tree of depth n:
sizeof(tree_n) ~= #leafs  * sizeof(leaf) + #inner  * sizeof(leaf)
sizeof(tree_n) ~= 2^n     * 80           + 2^n     * 112
sizeof(tree_n) ~= 2^n     * (192 bytes)

So to solve for n given sizeof(tree_n):

n = floor(log_2(sizeof(tree_n) / 192)){noformat}
I've attached screenshots showing evidence of the [^Merkle_On_Heap_Sizes.png] that I based my calculations on. The heap dump was taken on a 3.0.17 node that was coordinating 5 subrange repairs on a large table. As we can see when we drill into the heap dump we are not retaining 48 bytes per leaf node, we are retaining about ~80 bytes per leaf node and then ~120 bytes per inner node (object overhead is ... tricky to calculate) and we have like {{2^n}} of both.","16/Dec/18 01:38;bdeggleston;I’m a little worried about limiting tree depth like that. As I understand it (and I’m not super familiar with this area of repair), merkle tree depth is correlated to data density, so limiting the depth is going to inflate the streaming done when there are differences, causing other problems.

It seems like the simplest solution might be to just repair smaller subranges if you’re ooming on huge merkle trees during repair? That should limit tree size, right? There is an anti-compaction cost associated with that, but that seems more predictable/controllable than the other options.

I realize that’s not a super great solution, but I’d like to avoid making any significant behavioral changes here for 3.x and 4.0. That said, I think repairing dense nodes and large partitions is an area that should get some attention for 4.next. WDYT?","16/Dec/18 02:06;jolynch;{quote}I’m a little worried about limiting tree depth like that. As I understand it (and I’m not super familiar with this area of repair), merkle tree depth is correlated to data density, so limiting the depth is going to inflate the streaming done when there are differences, causing other problems.
{quote}
I think that the choice to increase the max resolution to {{2^20}} from {{2^15}} was very dangerous given the flawed estimation math in that ticket and from the production data we can see a small subrange repair can allocate gigabytes of memory. For example, in the case above we were doing splits of 1048576 partition keys in the first place and still ended up with trees that had 1048576 leaf nodes, I don't recall ever seeing that in our 2.1 clusters where we run the same adaptive subrange algorithm. It's possible that the range estimation logic in 2.1 was less likely to create large trees as the 3.0 partition estimation does, but I also think that the most reasonable thing we can do is have the highest resolution tree we can have for a given amount of memory, which is what my proposal would do. If you're particularly worried I could default it to something like 25% of the heap to preserve the current behavior of allocating close to 2 GiB for an RF=3*3 cluster?  
{quote}It seems like the simplest solution might be to just repair smaller subranges if you’re ooming on huge merkle trees during repair? That should limit tree size, right? There is an anti-compaction cost associated with that, but that seems more predictable/controllable than the other options.
{quote}
I completely agree but the above OOM happened when we were repairing _very_ small ranges (1048576 partitions per split). I would be hesitant to go much smaller as small ranges create too many small sstables that create other issues. I do agree more smaller ranges are better, but we are already repairing tiny subranges as it is and this bug is such a big stability regression in 3.0 that it would block us from moving many clusters until it is fixed. Not being able to do parallel subrange means that we can't repair the large clusters fast enough.
{quote}I realize that’s not a super great solution, but I’d like to avoid making any significant behavioral changes here for 3.x and 4.0. That said, I think repairing dense nodes and large partitions is an area that should get some attention for 4.next. WDYT?
{quote}
Our repair algorithm already does the proposed solution (small subranges, on single token nodes, repairing a single table at a time), and even with that we still cause OOMs on any reasonably sized dataset. I think introducing a (potentially optional) cap on heap allocation by a single {{RepairSession}} is the least invasive way to ensure that users of the database can run full/subrange repair on 3.0 without fear of OOMing. Of course this limit would be in addition to your suggestion to not hold all the trees on memory during streaming as well.","16/Dec/18 02:30;jolynch;Ah, so it looks like a bug was fixed in [CASSANDRA-11390|https://github.com/apache/cassandra/commit/1d1bfae580d44d3b8a4678c5af5767ff17102128] where in 2.1 we calculated the size of the tree from the partition count based on the [natural log|https://github.com/apache/cassandra/blob/34a1d5da58fb8edcad39633084541bb4162f5ede/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1059] instead of in the [log base 2|https://github.com/apache/cassandra/blob/1816520d6c59cece5ef8346c95e4f12e7c285751/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1220] as we do in 3.0. So in 2.1 if we did a spit range of 1048576 partitions we'd end up with a tree that was limited to size {{floor(log(1048576)) = 13}} and in 3.0 we end up with a tree that is limited to {{floor(log_2(1048576)) = 20}}. So even though 2.1 added the possibility for the trees to expand to {{2^20}}, the use of natural log protected us from OOMing when we did split ranges since a tree with depth 13 allocates 1.5 MiB vs a tree with depth 20 which allocates 192 MiB, 128x more memory to repair the same tiny range of data in 3.0+ as 2.1.

I still think that limiting the amount of memory that can be used by a single {{RepairSession}} is the most reasonable thing we can do in the 3.0/3.11 releases to make them stable again. I think that's more understandable to the user than arbitrarily picking {{2^15}} or {{2^20}} as our maximum tree size, just say ""we will get the maximum resolution we can given the memory you gave us"".","16/Dec/18 03:02;jjirsa;“Repair smaller sub ranges to work around the fact that the default behavior is untenable” is a bad place for us to put users

","16/Dec/18 05:01;bdeggleston;bq. “Repair smaller sub ranges to work around the fact that the default behavior is untenable” is a bad place for us to put users

You’re not wrong. Though the users this puts in a bad place seem to 1) be few and far between, and 2) seasoned C* operators. Inconveniencing them seems less bad than changing the default behavior of repair for everyone else (no offense). That said, I didn’t realize this was happening on small subranges. [~jolynch], [~serhatd], is this something you're seeing a lot of?

I think my main concern is that adding logic to fit merkle trees into a max memory size seems overly fancy, especially for 3.x. Would adding a configurable max merkle tree depth be good enough?","16/Dec/18 05:44;jolynch;{quote}I think my main concern is that adding logic to fit merkle trees into a max memory size seems overly fancy, especially for 3.x. Would adding a configurable max merkle tree depth be good enough?
{quote}
Sure that could fix it too, but I think users may not understand the implication of having a tree of depth 15 vs one of depth 20 (from what I can tell even experienced developers/operators don't understand the implications of having such a large tree resident in memory). If my math above is correct, wouldn't it be as easy as a function that we could thoroughly test, e.g.:
{noformat}
static int estimatedMaxSizeForBytes(long numBytes)
{
    long adjustedBytes = Math.max(1, numBytes / 192);
    return Math.max(1, (int) Math.floor(Math.log(adjustedBytes) / Math.log(2)
}
{noformat}
Then when we allocate the trees we just take the memory budget given by the user, divide by the RF of the keyspace, and plug it into this formula. Then the advice we give to users who want very precise trees is ""give it more memory"", which is a tradeoff I think users can actually make easily.
{quote}That said, I didn’t realize this was happening on small subranges. Joseph Lynch, Serhat Rıfat Demircan, is this something you're seeing a lot of?
{quote}
At least for us we only recently started moving clusters to 3.0 from 2.1 in production, and we are seeing OOMs on _all_ of the large dataset clusters that have to do parallel subranges to meet gc_grace. As a result we have had to disable parallel subrange and extend gc_grace as a workaround due to these very large Merkle trees (note these workloads worked fine on 2.1). Full range or single worker subrange isn't OOMing but still allocates 2GiB of heap and it is much slower naturally than our parallel subrange. To be frank, many of our large clusters simply can't repair on 3.0 fast enough whereas on 2.1 we could repair about 4-8x faster due to being able to do ranges in parallel. Since we have single tokens we can re-factor our algorithm to drift across the ring instead of doing one range faster, but that won't really work for vnode clusters and tbh is way more complex then just fixing this bug.

We were just going to roll back the 3.0 sizing change internally, but I was hoping that I could do slightly more work and fix it for the community as well with an option users can easily configure. I'm happy to do that if you have time to review.","17/Dec/18 03:41;bdeggleston;Specifying max depth is less elegant than specifying a max tree size, but I have a lot fewer concerns about it being implemented correctly, and the risk involved in putting it into 3.x. [~krummas], what do you think?
","17/Dec/18 10:20;krummas;[~bdeggleston] agreed, but taking RF in to account also makes sense;

How about something like this for 3.0+3.11? And then do what [~jolynch] suggests in 4.0
1. make max depth configurable, default at 20
2. add a flag to factor in RF when calculating the depth, default off","17/Dec/18 19:26;bdeggleston;Seems reasonable. [~molsson], we just expanded the scope and complexity of this ticket, do you still want to do it? If not, I (or [~jolynch] if he's interested) can take it.","19/Dec/18 07:24;jolynch;[~krummas] [~bdeggleston]

I took a whack at Marcus' suggestion to make max depth configurable (also a hot property) for 3.0+3.11 and then have a patch for trunk which limits the total memory used taking into account RF and the partitioner etc... Let me know what you guys think. I'll give it the 3.0 patch a spin in our clusters with max depth set to 16 to see if it alleviates the OOM issues.

 
||3.0||3.11||trunk||
|[patch|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0]|[patch|https://github.com/apache/cassandra/compare/cassandra-3.11...jolynch:CASSANDRA-14096-3.11]|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk]|

I'll run the single failed 3.11 unit test locally, and the trunk unit test that's failing is CASSANDRA-14922.

I think that the tree size estimation logic is not [too complicated|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk#diff-e657fd15ed537a2bf54a672b6b84afecR1182]. Most of the patch is tests tbh including testing the range splitting adjustment introduced in CASSANDRA-11390 ([1|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk#diff-3f1f2846fe6fbee924c09edaf34753deR238], [2|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk#diff-ec7f7df91e07f7bac10771e53e47d5f9R572]).

 ","20/Dec/18 03:17;jolynch;I added a commit to the 3.0 and 3.11 branches which I think is the minimally invasive change I can make that will make sure that the {{SyncTasks}} do not hold the merkle trees on heap longer than they need to. It's different than the way that trunk does it, but if I understand the trunk patch properly the tree differences will now be single threaded, which isn't that big a deal in a single token case (computing differences even with RF=15 would be a few seconds even on the largest trees) but in the vnode case it may hold the trees on heap for longer than we'd like (as it's no longer RF merkle trees, it's RF * number of shared ranges merkle trees). I'm tinkering with the patch to see if I can more or less backport the trunk strategy but still keep the difference calculations in parallel.

With regards to the trunk patch, right now the max repair session size defaults to 1/8 of the heap, but this is actually quite a lot of memory given that each repair job could allocate up to this amount which would mean 1/2 the heap could be used if someone used 4 job threads I believe. [~krummas] made a nice suggestion offline to somehow communicate the budget from the coordinator to the nodes running the validation, so I'm seeing how hard that would be. If that's too tricky we could probably just lower the memory limit to 1/16 the heap and be happy knowing that even if you repair with 4 jobs you'll only allocate ~1/8 of the heap max.","20/Dec/18 14:30;molsson;[~bdeggleston] I was going to say that I think I'll have limited time for it in the coming weeks for it so it's good that [~jolynch] has already uploaded patches. I can try and take some time to do some review/testing on it.

A few initial comments on the patches:
 * I like the 3.0/3.11 approach of keeping the list and clearing it. It seems like the solution that would change behavior the least (except for the upside of not keeping the trees of course).
 * The flag for factoring in RF does not seem to be in the 3.x branches (should it?).
 * [In 4.0|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk#diff-b6397657542c94b41574354981a11843R69] the definition of _repair_session_max_tree_depth_ has effectively changed. Before the total size was divided over different trees based on *rangeOwningRatio*. Now each tree can be up to _repair_session_max_tree_depth_. Since we have the memory cap as well this is probably not a huge problem but was this an intended change?

For _repair_session_space_in_mb_: With *1/8* and a heap of 8GB and rf = 3 the cap should effectively be defined by the max tree depth. The limit would kick in for heaps below ~4.8GB. With *1/16* it would kick in on heaps below ~9GB, so 8GB heaps will have a minor change in behavior for rf = 3. I'm not sure which heap size/rf is the most common but it would probably change the default tree depth on clusters with *rf > 3*.
{code}
# rf = 3 and max tree depth of 20, 1/8 of heap for merkle trees
3 * 200MB = ~600MB # Coordinator tree sizes, holds true for both cases
600MB * 8 = ~4.8GB # Total heap size when the 1/8 memory limit would cap at 600MB

# rf = 3 and max tree depth of 20, 1/16 of heap for merkle trees
600MB * 16 = ~9.6GB # Total heap size when the 1/16 memory limit would cap at 600MB
{code}

While I'm on the side of wanting to lower the default tree size caps (and effectively change the default behavior of repair) I get the feeling that the risks with over streaming is causing people to be cautious here. (Almost) regardless of what we choose as default here it might change behavior for someone, so unless I misunderstood the concern (i.e. if the concern is only for 3.x) we might have to make a decision:
# Set the default to total heap space (should be capped at the default max tree size, as it is today)
# Set the default to x% of available heap (would change default behavior)
# Other suggestions

I would argue that the default tree depth should be a bit more on the safe side for new users. But then there is of course the other side with over streaming, so what is safe and not is a hard question to get an answer to. In general I get the feeling that the changes done to the merkle trees (cap at 20, correctly calculate the necessary depth) have made repair more unstable performance wise, which would suggest that lowering the default cap would be safer. But I might of course be missing the over streaming issues. So I'm not going to push too much here to get lower defaults but I think that it might be good to take that into consideration at least for 4.0+, especially if we leave it configurable for the experienced users (with NEWS/upgrade entries).","20/Dec/18 21:26;jolynch;Thanks [~molsson] for the feedback! Let's see what Blake and the other Marcus think about the list+clear vs the trunk approach. I do think that tree calculations shouldn't be too slow, but if we're wrong 4.0 seems like the safer place to introduce that change.
{quote}The flag for factoring in RF does not seem to be in the 3.x branches (should it?).
{quote}
Hm, I'm not sure how to incorporate the RF information without some form of the memory estimation as well. The current 3.0/3.11 algorithm of subtracting {{log_2(1 / ratio)}} reduces the tree size but not by a particularly predictable quantity. 
{quote}Before the total size was divided over different trees based on rangeOwningRatio. Now each tree can be up to repair_session_max_tree_depth
{quote}
For the trunk patch I split the memory cap up [proportional to each range|https://github.com/apache/cassandra/commit/3f721b7c931c0e9e9b118c491930aa390df56afe#diff-b6397657542c94b41574354981a11843R66] using the {{rangeOwningRatio}}. So if you have 330MB to work with (after taking into account RF of 3 on a 8GB heap), and you have 4 ranges with (0.25, 0.5, 0.25, 0.25) of the partitions, then the ranges get (82MB, 165MB, 82MB, 82MB) of space each yielding trees of depth (18, 19, 18, 18) compared to the status quo where we would get {{20-log_2(1 / ratio)}} which yields (18, 19, 18, 18).

I agree with your analysis concerning 1/8 and rf=3, I chose 1/8 so that the trees would be roughly the same depth as they are today with 8GB heaps for RF=3. The main goal is to keep that memory usage fixed as RF grows to 6, 9 or 15.

I don't understand your analysis for 1/16 heap though. The memory budget is reduced by [RF|https://github.com/apache/cassandra/commit/3f721b7c931c0e9e9b118c491930aa390df56afe#diff-b6397657542c94b41574354981a11843R57] in the trunk path, so with 1/16 of 8GiB and RF=3 a given validation compaction would not exceed {{8GiB / (48) = 170 MiB}} which yields a tree of depth 19 so the coordinator would hold {{3 * 170 = ~512MiB}} on heap which is slightly smaller than status quo. The main difference would be when you have RF=6 or 9 where the status quo allocates 1.1GiB and 1.7GiB respectively and a 1/16 default size would allocate a fixed ~512MiB.
{quote}I would argue that the default tree depth should be a bit more on the safe side for new users. But then there is of course the other side with over streaming, so what is safe and not is a hard question to get an answer to. In general I get the feeling that the changes done to the merkle trees (cap at 20, correctly calculate the necessary depth) have made repair more unstable performance wise, which would suggest that lowering the default cap would be safer. But I might of course be missing the over streaming issues.
{quote}
I personally feel strongly that the change in behavior to allow trees to go to depth 2^20 represents a regression in database stability for common configurations (RF=6 and 9 in particular) and we should be revert the default max depth to something closer to 18 in the 3.0+3.11 branch and with the configuration option (which is hot swappable) if someone really wants it to be 20 they can do that. I believe the correct solution to overstreaming has been in the past, and continues to be, to do subrange repair which allows us to spend the memory in a controlled fashion over time rather than having the larger range repairs use a lot of memory all at once. This is also, practically speaking, how most teams I know of are successfully repairing large datasets.
{quote}So I'm not going to push too much here to get lower defaults but I think that it might be good to take that into consideration at least for 4.0+, especially if we leave it configurable for the experienced users (with NEWS/upgrade entries).
{quote}
If we do the memory budget in 4.0 I'm less concerned about the max depth being 20, since the memory budget will protect high RF users. I do think though that at the point someone is hitting depths of 20 and encounter overstreaming they _really_ need to start doing subrange.",21/Dec/18 03:05;jolynch;Just put up an alternative 3.0 [patch|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0-trunk-backport] which just backports the strategy used in trunk and still keeps the computation parallel by [transforming|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0-trunk-backport#diff-1ec1e6dd403ead5e9e44a57a0702f79eR106] all of the {{TreeResponse}} s into an intermediate {{TreeDifference}} which then get's transformed into {{SyncStats}}. This option is closer to what trunk does but also keeps the computation of trees parallel. I can of course just rip out the parallel part and just compute the differences on the single thread as well and it'll probably be fine.,"21/Dec/18 12:49;molsson;No problem, thanks for driving this issue further.

bq. Hm, I'm not sure how to incorporate the RF information without some form of the memory estimation as well. The current 3.0/3.11 algorithm of subtracting log_2(1 / ratio) reduces the tree size but not by a particularly predictable quantity.

I think it could be done by creating an adjusted range owning ratio.
{code:java}
double adjustedRangeOwningRatio = rangeOwningRatio / rf;
int maxDepth = adjustedRangeOwningRatio > 0 ? (int) Math.floor(DatabaseDescriptor.getRepairSessionMaxTreeDepth() - Math.log(1 / adjustedRangeOwningRatio) / Math.log(2)) : 0;
{code}
IIRC the max depth prior to this makes sure that locally allocated trees don't contain more nodes than a single tree of size 20. If we add the division by the replication factor here it should make each replica create merkle trees that when combined would not exceed one tree of size 20. So if we have a range which consists of 30% of the data and a replication factor of 3 it would say that locally we should not create a tree which exceeds 10% of the max tree. Even if we have a variance in data density per range between nodes it should even out between the different ranges as one node should not create merkle trees that combined exceeds 33% of the max tree. Basically I think this would extend the current limitation that is local node only to the whole cluster so that we don't allocate more than ~190MB for the coordinator.

bq. For the trunk patch I split the memory cap up proportional to each range using the rangeOwningRatio. So if you have 330MB to work with (after taking into account RF of 3 on a 8GB heap), and you have 4 ranges with (0.25, 0.5, 0.25, 0.25) of the partitions, then the ranges get (82MB, 165MB, 82MB, 82MB) of space each yielding trees of depth (18, 19, 18, 18) compared to the status quo where we would get 20-log_2(1 / ratio) which yields (18, 19, 18, 18).

That is true for the default settings but if memory limits are increased you could get a result of (20, 20, 20, 20) as the calculation for  _20-log_2(1 / ratio)_ [was removed in the patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk#diff-b6397657542c94b41574354981a11843R69]. Now the max tree depth is used as a static max depth for a single tree rather than a limit for all trees together.

bq. I don't understand your analysis for 1/16 heap though. The memory budget is reduced by RF in the trunk path, so with 1/16 of 8GiB and RF=3 a given validation compaction would not exceed 8GiB / (48) = 170 MiB which yields a tree of depth 19 so the coordinator would hold 3 * 170 = ~512MiB on heap which is slightly smaller than status quo. The main difference would be when you have RF=6 or 9 where the status quo allocates 1.1GiB and 1.7GiB respectively and a 1/16 default size would allocate a fixed ~512MiB.

I agree that it would change mostly for rf=6/9 and my main point was that the patch would change default repair behavior. I got the feeling that there was some caution regarding this from others in the community so I wanted to point out where we could expect the default behavior to change.","04/Jan/19 23:08;jolynch;{quote}I think it could be done by creating an adjusted range owning ratio.
{noformat}
double adjustedRangeOwningRatio = rangeOwningRatio / rf;
int maxDepth = adjustedRangeOwningRatio > 0 ? (int) Math.floor(DatabaseDescriptor.getRepairSessionMaxTreeDepth() - Math.log(1 / adjustedRangeOwningRatio) / Math.log(2)) : 0;
{noformat}
{quote}
I think I see what you're saying, and I think that's a pretty safe optional change (we'd default to off) for the 3.x series. Ok I added another commit on the [3.0|https://github.com/apache/cassandra/commit/20a9b32f620168657dcfa2e98b92db20e90deb52] and [3.11|https://github.com/apache/cassandra/commit/09a164b60cfb41cd3ae8c3d68fbe8169069a4cb8] branch which does this.
{quote}That is true for the default settings but if memory limits are increased you could get a result of (20, 20, 20, 20) as the calculation for 20-log_2(1 / ratio) was removed in the patch. Now the max tree depth is used as a static max depth for a single tree rather than a limit for all trees together.
{quote}
I'm curious if we should even have the maximum depth option anymore in trunk once we have size limit as the memory usage is really the only thing that matters. If a user gives us 2GB of memory, we should (imo) calculate the most precise trees we can with 2GB. In the trunk patch, if my understanding is correct, the {{repair_session_space_in_mb}} limit is the limit for all trees together across all replicas and all ranges participating in the repair session.","08/Jan/19 00:24;jolynch;I believe the remaining decision points are as follows:

*3.0/3.11*:
 # Do we want to use the list+clear [approach|https://github.com/apache/cassandra/commit/342921c54e463180171a4d64944acd54b5d97727] or the trunk approach for clearing merkel tree differences. If we go with the trunk approach should we refactor to keep the calculation [parallel|https://github.com/apache/cassandra/commit/7c61e843a32c5e9b94ab9269432bd3b1f84cddd9]?
 # Do we introduce {{repair_session_account_for_rf}} option that defaults to false?
 # Should we reduce the default max from 20 to something like 18?

I think we should opt for the choices in 3.x that provide maximal stability (so list+clear, introduce the option and default to false, reduce to 18 as a compromise between the old 2^15 and new 2^20 depths). I think the reduction in max depth is a move towards stability because it seems to me from this ticket and discussion on irc [[1|https://wilderness.apache.org/channels/?f=cassandra-dev/2018-12-17], [2|https://wilderness.apache.org/channels/?f=cassandra-dev/2018-12-18]] that as most users are upgrading to 3.0 they are having issues running repair and so I feel it is prudent to roll back slightly to the older, safer, defaults.

*trunk*:
 # What should the default memory budget be. Currently it is 1/8 of the heap in the patch but 1/16 might be more reasonable.
 # Should trunk only have the {{repair_session_space_in_mb}} or also support overriding the maximum tree depth to 20.

I think we should default to a memory budget of 1/16 of the heap by default so that we have a _safe_ default that doesn't cause OOM when a user runs with the maximum {{[job_threads|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/tools/nodetool/Repair.java#L86]}} of 4 (which would allocate 1/8 of the heap). We would very minorly reduce resolution for the most common setups (8GB heaps with RF=3 would get 2^19 instead of 2^20). For users that want higher resolution they can use subrange repair (correct but harder) or allocate more memory (incorrect but easier). Personally I think just having the memory option should be sufficient but having the max depth option present (commented out) and explaining clearly that if you want to give the trees more memory you may need to raise that max depth as well isn't that bad (and perhaps prevents really bad mistakes like allocating GiB of memory by accident).

[~molsson], [~krummas], [~bdeggleston]. What do you think? If there is rough consensus I'd like to commit to those choices and squash down the patches to move this forward.","08/Jan/19 22:07;bdeggleston;My 2 cents:

*3.0/3.11*
{quote}Do we want to use the list+clear approach or the trunk approach for clearing merkel tree differences. If we go with the trunk approach should we refactor to keep the calculation parallel?
{quote}
I’d prefer the trunk approach. Releasing the memory in the task adds mutable state to the task, and doing the calculations in the RepairJob didn’t seem terrible based on our offline back of the envelope calculations.
{quote}Do we introduce repair_session_account_for_rf option that defaults to false?
{quote}
I’m -0 on accounting for rf. I don’t think it adds a lot of value without a memory based calculation, but I don’t have a strong opinion here.
{quote}Should we reduce the default max from 20 to something like 18?
{quote}
I think so, yes.

*trunk*
{quote}What should the default memory budget be. Currently it is 1/8 of the heap in the patch but 1/16 might be more reasonable.
{quote}
Maybe 1/16, but not to exceed something like 80-100MB? This would prevent huge trees on nodes with large heaps, without consuming too much memory on nodes with small heaps
{quote}Should trunk only have the repair_session_space_in_mb or also support overriding the maximum tree depth to 20.
{quote}
If the max_depth option is in 3.x, we need to put it in trunk as well.","17/Jan/19 16:32;molsson;*3.0/3.11*
1. I would prefer the trunk approach as well (non-parallel) as long as there is no dramatic performance regression.
2. I think this feature could be useful to avoid having to change the max tree depth when adding data centers (and extend current replication to it). But if we set the default values reasonably low it shouldn't be necessary. +0
3. +1 from me.

*trunk*
1. 1/16 sounds reasonable to me.
2. I think we should keep the option and align it with the default we choose for 3.x.","22/Jan/19 10:38;jolynch;Alright, it sounds like we have rough consensus that we should backport the trunk approach to handling the trees staying in memory, reduce the maximum depth to 18 by default, and we will not include the account for rf logic in the 3.x patchset (trunk does). For trunk it sounds like we keep the max depth option (default of 18) and default the max size to 1/16 the heap, whichever is smaller.

I've put up patches that I believe implement this consensus as well as cleaning up the syncing tasks at the end per the suggestion far up in the comments by Marcus Olsson so that we don't accumulate RF of them per table (even now that they're just holding the differences we probably shouldn't be holding onto them). I've tried to add some basic unit tests of the RepairJob but it is somewhat hard to do without more refactoring of the RepairJob code since it's all so tightly coupled; I tried to walk the right line of refactoring for testability (sort of like Alex did in trunk) but not introduce too many changes ... but I can either remove the changes to make the diffs easier to check or I can try refactoring more to make unit testing more possible.

Unit tests are running at:
||3.0||3.11||trunk||
|[patch|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0-final]|[patch|https://github.com/apache/cassandra/compare/cassandra-3.11...jolynch:CASSANDRA-14096-3.11-final]|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-final.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-final]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-final.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-final]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk]|

[~bdeggleston] let me know what you think, if you would like changes please let me know. If you are happy I'll squash them down to one commit for merging/testing. Once we're ready to squash I can give the 3.0 change a test in my repro environment to see if it alleviates our sub-range stability issues.

 

 ","29/Jan/19 19:22;bdeggleston;First round of feedback. Most of the 3.x stuff applies to the trunk patch as well.

3.x patch:

cassandra.yaml
* The comment here is pretty verbose. Something more concise like ""Limits the max merkle tree depth to avoid consuming too much memory during repair. The default is 18, set as low as 15 if you're running out of memory during repairs"" would be more easily digestable.

DatabaseDescriptor
* validate that the max depth is something sensible (like at least > 10), and log a warning if it's too high (>20?).
* regarding ConfigurationException message. “must be > 0” is more easily understood than “must be strictly positive”

CompactionManager
* should add some additional min/maxing to prevent negative/zero max depths in extreme cases if the max depth is set really low

RepairJob
* since the trees don’t escape {{createSyncTasks}}, I don’t think we need to call {{removeSyncingTask}} here (or have a comment in {{onFailure}})

RepairSession
* class internals are overexposed for testing. Instead of making members package private, you could support your tests with some accessor methods {{getNumSyncTasks}} / overridable protected methods {{createExecutor()}}, etc.

trunk:

cassandra.yaml
* Let’s remove {{repair_session_max_tree_depth}} as a commented out param in cassandra.yaml. Sorry my previous comment on this was vague. We do need to support yaml files with this param in them for back compat, but we should treat it as a deprecated config value. We should log a warning mentioning it’s deprecated on startup if the yaml file has it. Having 2 config params that achieve the same thing in different ways is confusing. This will also simplify the comment blurb.

DatabaseDescriptor
* should log warning if max tree depth is configured.

StorageService.
* I think the new jmx endpoints would make more sense be better on ActiveRepairServiceMBean?
","30/Jan/19 20:40;jolynch;Thanks for the feedback! I'm working on implementing this but just a quick question:

{quote}
RepairJob
 * since the trees don’t escape {{createSyncTasks}}, I don’t think we need to call {{removeSyncingTask}} here (or have a comment in {{onFailure}})
{quote}
I agree that we don't need to remove them since the remote sync tasks are much smaller now (as they just hold the tree differences), and the ones that get removed here are only the empty ones that didn't have any differences, but isn't it good practice to remove them once they're done either way? I'm thinking it might be a problem for keyspaces with many tables that are all fully consistent as we will have RF sync tasks held in that map per table. I suppose it's reasonable to just leave the empty ones since they're so small but I figured it would be better to be thorough. Trunk doesn't have this problem because we don't generate any sync task at all if there are no differences in trunk (I am hesitant to backport that change since I'm not sure of the full context of that change).","30/Jan/19 23:51;bdeggleston;It’s just best to minimize changes when dealing with bug fix branches. This is a small change in behavior with a negligible benefit. Would this change actually cause any real problems? Probably not, but it takes longer to verify that than it does to just not change it.","31/Jan/19 07:35;jolynch;Ok, that makes sense. I've pushed a commit to each branch that I believe takes into account your feedback:

||3.0||3.11||trunk||
|[26e5b2e6|https://github.com/apache/cassandra/commit/26e5b2e68810e724c9f14321fd0f6ca43b66d08e]|[e139f536|https://github.com/apache/cassandra/commit/e139f5366f62ba17ac00794258270a82dc395345]|[a1ab1f6a|https://github.com/apache/cassandra/commit/a1ab1f6a28c2e6ea98cc2c43a8cd5fa42edf2c06]|

If the branches look good let me know and I will squash them down and add the NEWS/CHANGES entries.","31/Jan/19 19:29;bdeggleston;ok nearly there. Could you make the following changes and rebase each branch? I'll squash on commit.

all branches:
* make jmx configurable Config members volatile
* jmx methods should throw exceptions if they get invalid input (not just log warnings)
 
3.x:
* in RepairSession: syncingTasks.remove should be syncingTasks.get
* there are extra parentheses in the ConfigurationException thrown in DatabaseDescriptor

trunk:
* max depth code shouldn’t be removed (the only change should be the yaml changes, the Config changes, and warning that it’s deprecated on startup)","01/Feb/19 08:20;jolynch;Awesome, I've made the changes and pushed to all three branches. I can't edit this JIRA title so I didn't add the CHANGES entries (my understanding is that the Jira title, CHANGE entry, and commit title should all match) but I was thinking something like ""Reduce memory held by merkle trees during repair to prevent OOM (CASSANDRA-14096)"" for the CHANGES entry?

If you want to do the squash I've left the original branches as is but rebased on top of latest:
||3.0||3.11||trunk||
|[patch|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0-final]|[patch|https://github.com/apache/cassandra/compare/cassandra-3.11...jolynch:CASSANDRA-14096-3.11-final]|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-final.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-final]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-final.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-final]|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk]|

I've also created rebase/squashed commits for running the dtests you could cherry-pick if you prefer:
||3.0||3.11||trunk||
|[e633d6bc|https://github.com/apache/cassandra/commit/e633d6bc9dca621ea7ac7a8a5d7675c93baee66d]|[9c227ac2|https://github.com/apache/cassandra/commit/9c227ac251536339c080897aabbdcd7c650ae526]|[bd58a4c0|https://github.com/apache/cassandra/commit/bd58a4c062d0adb97bb3afac75687959bf103d4d]|
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...jolynch:CASSANDRA-14096-3.0-squash]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...jolynch:CASSANDRA-14096-3.11-squash]|[branch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14096-trunk-squash]|
|dtests: [!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-squash.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.0-squash]|dtests: [!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-squash.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-3.11-squash]|dtest: [!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk-squash.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk-squash]|

*Test Failures:*

trunk unit tests:
 * {{PagingTest}}: Unrelated, I posted a patch that fixes this on CASSANDRA-14956
 * {{PendingAntiCompactionBytemanTest#testExceptionAnticompaction}}: Pretty sure this is a flake, and the dtest+unit test run is fully green.
 * {{DistributedReadWritePathTest#readRepairTest}}: I believe this is CASSANDRA-14922, debugging that separately

3.x dtests:
 * {{thrift_hsha_test.TestThriftHSHA#test_closing_connections}}: Doesn't appear related
 * {{materialized_views_test.TestMaterializedViews#test_interrupt_build_process}}: Doesn't appear related
  ",14/Feb/19 21:29;bdeggleston;commited to 3.0 as [b30c8c98a594a5682f6ea1f0b5511463b700b6e8|https://github.com/apache/cassandra/commit/b30c8c98a594a5682f6ea1f0b5511463b700b6e8] and merged up to trunk. Thanks [~jolynch],,,,,,,,,,,,,,,,,,,,,,,,,,
Adding a field to an UDT can corrupte the tables using it,CASSANDRA-13776,13095594,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,blerer,blerer,blerer,18/Aug/17 09:34,12/Mar/19 14:07,13/Mar/19 22:34,24/Aug/17 16:33,3.0.15,3.11.1,4.0,,,,,,,,0,,,,"Adding a field to an UDT which is used as a {{Set}} element or as a {{Map}} element can corrupt the table.
The problem can be reproduced using the following test case:
{code}
    @Test
    public void testReadAfterAlteringUserTypeNestedWithinSet() throws Throwable
    {
        String ut1 = createType(""CREATE TYPE %s (a int)"");
        String columnType = KEYSPACE + ""."" + ut1;

        try
        {
            createTable(""CREATE TABLE %s (x int PRIMARY KEY, y set<frozen<"" + columnType + "">>)"");
            disableCompaction();

            execute(""INSERT INTO %s (x, y) VALUES(1, ?)"", set(userType(1), userType(2)));
            assertRows(execute(""SELECT * FROM %s""), row(1, set(userType(1), userType(2))));
            flush();

            assertRows(execute(""SELECT * FROM %s WHERE x = 1""),
                       row(1, set(userType(1), userType(2))));

            execute(""ALTER TYPE "" + KEYSPACE + ""."" + ut1 + "" ADD b int"");
            execute(""UPDATE %s SET y = y + ? WHERE x = 1"",
                    set(userType(1, 1), userType(1, 2), userType(2, 1)));

            flush();
            assertRows(execute(""SELECT * FROM %s WHERE x = 1""),
                           row(1, set(userType(1),
                                      userType(1, 1),
                                      userType(1, 2),
                                      userType(2),
                                      userType(2, 1))));

            compact();

            assertRows(execute(""SELECT * FROM %s WHERE x = 1""),
                       row(1, set(userType(1),
                                  userType(1, 1),
                                  userType(1, 2),
                                  userType(2),
                                  userType(2, 1))));
        }
        finally
        {
            enableCompaction();
        }
    }
{code} 

There are in fact 2 problems:
# When the {{sets}} from the 2 versions are merged the {{ColumnDefinition}} being picked up can be the older one. In which case when the tuples are sorted it my lead to an {{IndexOutOfBoundsException}}.
# During compaction, the old column definition can be the one being kept for the SSTable metadata. If it is the case the SSTable will not be readable any more and will be marked as {{corrupted}}.

If one of the tables using the type has a Materialized View attached to it, the MV updates can also fail with {{IndexOutOfBoundsException}}.

This problem can be reproduced using the following test:
{code}
    @Test
    public void testAlteringUserTypeNestedWithinSetWithView() throws Throwable
    {
        String columnType = typeWithKs(createType(""CREATE TYPE %s (a int)""));

        createTable(""CREATE TABLE %s (pk int, c int, v int, s set<frozen<"" + columnType + "">>, PRIMARY KEY (pk, c))"");
        execute(""CREATE MATERIALIZED VIEW "" + keyspace() + "".view1 AS SELECT c, pk, v FROM %s WHERE pk IS NOT NULL AND c IS NOT NULL AND v IS NOT NULL PRIMARY KEY (c, pk)"");

        execute(""INSERT INTO %s (pk, c, v, s) VALUES(?, ?, ?, ?)"", 1, 1, 1, set(userType(1), userType(2)));
        flush();

        execute(""ALTER TYPE "" + columnType + "" ADD b int"");
        execute(""UPDATE %s SET s = s + ?, v = ? WHERE pk = ? AND c = ?"",
                set(userType(1, 1), userType(1, 2), userType(2, 1)), 2, 1, 1);


        assertRows(execute(""SELECT * FROM %s WHERE pk = ? AND c = ?"", 1, 1),
                       row(1, 1, 2, set(userType(1),
                                        userType(1, 1),
                                        userType(1, 2),
                                        userType(2),
                                        userType(2, 1))));
    }
{code}      ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14010,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-23 14:01:45.297,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 16:33:30 UTC 2017,,,,,,0|i3iy6f:,9223372036854775807,,,,,,,,snazy,snazy,,,,,,,,,,"22/Aug/17 13:32;blerer;I pushed the patches for [3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...blerer:13776-3.0], [3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:13776-3.11] and [trunk|https://github.com/apache/cassandra/compare/trunk...blerer:13776-trunk].
I ran the tests on our internal CI and the failing tests look unrelated to the patches.","23/Aug/17 14:01;snazy;Nice work, Benjamin!
The new unit tests cover the issue.

Just a few things:
* We should order on an array (or ArrayList) in {{SerializationHeader.orderByDescendingGeneration()} and use ""static"" comparator instances
* Checks on component count are mussing for tuples and composites in {{AbstractTypeVersionComparator}}
* Test should cover all type ""types"" in {{AbstractTypeVersionComparatorTest}}

Pushed some commits [here|https://github.com/snazy/cassandra/commits/13776-3.0-review].

+1 with the above changes. CI (internal one) looks good.
",24/Aug/17 16:33;blerer;Committed into 3.0 at cf0b6d107bade419dada49a5da40d2579c80ade8 and merged into 3.11 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Heartbeats can cause gossip information to go permanently missing on certain nodes,CASSANDRA-13700,13088570,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jkni,jkni,jkni,19/Jul/17 21:50,12/Mar/19 14:07,13/Mar/19 22:34,24/Jul/17 20:29,2.1.19,2.2.11,3.0.15,3.11.1,4.0,Legacy/Distributed Metadata,,,,,1,,,,"In {{Gossiper.getStateForVersionBiggerThan}}, we add the {{HeartBeatState}} from the corresponding {{EndpointState}} to the {{EndpointState}} to send. When we're getting state for ourselves, this means that we add a reference to the local {{HeartBeatState}}. Then, once we've built a message (in either the Syn or Ack handler), we send it through the {{MessagingService}}. In the case that the {{MessagingService}} is sufficiently slow, the {{GossipTask}} may run before serialization of the Syn or Ack. This means that when the {{GossipTask}} acquires the gossip {{taskLock}}, it may increment the {{HeartBeatState}} version of the local node as stored in the endpoint state map. Then, when we finally serialize the Syn or Ack, we'll follow the reference to the {{HeartBeatState}} and serialize it with a higher version than we saw when constructing the Ack or Ack2.

Consider the case where we see {{HeartBeatState}} with version 4 when constructing an Ack and send it through the {{MessagingService}}. Then, we add some piece of state with version 5 to our local {{EndpointState}}. If {{GossipTask}} runs and increases the {{HeartBeatState}} version to 6 before the {{MessageOut}} containing the Ack is serialized, the node receiving the Ack will believe it is current to version 6, despite the fact that it has never received a message containing the {{ApplicationState}} tagged with version 5.

I've reproduced in this in several versions; so far, I believe this is possible in all versions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-20 14:14:10.701,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 29 15:54:34 UTC 2017,,,,,,0|i3hrn3:,9223372036854775807,3.0.14,,,,,,,jasobrown,jasobrown,,,,,,,,,,"20/Jul/17 14:14;jasobrown;[~jkni] Fantastic debugging here, Joel. We have seen this problem, as well, with missing STATUS and TOKENS entries.

I followed this through, and I believe you are correct. Just to point out (because I had to dig and reason through it), the key problem is (as Joel points out) the shared mutable state of {{HeartBeatState}}. In {{Gossiper.getStateForVersionBiggerThan}}, when the local node is building up the {{Map<ApplicationState, VersionedValue> states}} about itself, if any states are added after the function returns *and* the heartbeat is incremented before serialization, the peer will get the updated heartbeat value but not the updated states (as we the set of states for the local node that we're sending over was already constructed a priori the serialization).

Off the top of my head, I think there are at least two possible ways to fix this:

- clone the {{HeartBeatState}} when constructing the {{EndpointState}} to return from {{Gossiper.getStateForVersionBiggerThan}}. That way it's not referencing mutable heartbeat state.
- execute the {{GossipTask}} on the same thread the we receive the gossip syn/ack/ack2 messages (on the {{Stage.GOSSIP}} thread). That way we force (almost) all references to gossip's stated mutable state into one thread.

The first option is simpler, smaller in scope, and certainly safer.
The second option is has performance implications, especially if the {{GossipTask}} takes a while to execute, then we could start backing up the tasks on the stage. This option, though, has the ""possibility"" of eliminating more of the state race bugs that we seems to continually uncover as time goes on. (Side note: there are still some updates to local Gossip state from the main thread (via {{StorageService}}) at startup, and the response to the {{EchoMessage}} is on the wrong thread, as well.)

Joel, can you share the method of how you are able to reproduce this?","20/Jul/17 14:17;jasobrown;We also might need to make {{HeartBeatState.version}} volatile, but I'm still thinking about it (just adding it here for discussion)","20/Jul/17 16:25;jkni;Thanks, Jason! In this case, I agree the first option is safer for this issue. Something like the second likely makes sense eventually, at least as part of a larger audit of correctness issues in gossip. I believe your volatile suggestion is correct.

I don't have a lot of helpful information to reproduce this; it reproduces in larger clusters, particularly with higher latency levels. We can see the effects locally with a few well-timed sleeps in MessagingService, but that isn't terribly representative.

Branches pushed here:
||branch||
|[13700-2.1|https://github.com/jkni/cassandra/tree/13700-2.1]||
|[13700-2.2|https://github.com/jkni/cassandra/tree/13700-2.2]||
|[13700-3.0|https://github.com/jkni/cassandra/tree/13700-3.0]||
|[13700-3.11|https://github.com/jkni/cassandra/tree/13700-3.11]||
|[13700-trunk|https://github.com/jkni/cassandra/tree/13700-trunk]||

There's a somewhat conceptually similar issue when we bump the gossip generation in the middle of constructing a reply - I believe that's the cause in [CASSANDRA-11825], which presents similar problems. I'm choosing to address them separately because they're indeed distinct problems and 11825 requires an additional trigger (enabling and disabling gossip during runtime).",20/Jul/17 17:58;jasobrown;+1,"20/Jul/17 17:59;jasobrown;re: CASSANDRA-11825. Yes, shared mutable state strikes again, and thanks for addressing them separately ;)","24/Jul/17 20:29;jkni;Thanks! Tests looked good on all branches.

Committed to 2.1 as {{2290c0d4b0c20ce3407ae2c542e580c75a5ab337}} and merged forward through 2.2, 3.0, 3.11, and trunk.","27/Jul/17 18:36;jasobrown;[~jkni] In our internal review of this change, we discovered that that patch can be made incrementally safer. In {{Gossiper#getStateForVersionBiggerThan()}}, we [get the generation|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/gms/Gossiper.java#L899], but it's possible the {{epState.getHeartBeatState()}} could have been swapped out before the next line executes (where we get the version).

Are you ok if I make the following small change to {{Gossiper#getStateForVersionBiggerThan()}}:
{code}
-            int localHbGeneration = epState.getHeartBeatState().getGeneration();
-            int localHbVersion = epState.getHeartBeatState().getHeartBeatVersion();
+            HeartBeatState heartBeatState = epState.getHeartBeatState();
+            int localHbGeneration = heartBeatState.getGeneration();
+            int localHbVersion = heartBeatState.getHeartBeatVersion();
{code}

Basically, just grab a reference the the same `HeartBeatState` that we'll use for both the generation and version. Of course, this does not protect against that `HeartBeatState` instance being mutated, but at least we can be smarter about what we reference from the `epState`.","01/Aug/17 14:32;jkni;I'm not sure of any places in the current codebase where the distinction matters in practice, but the change is cleaner and makes the code more tolerant of changes elsewhere, so +1.",29/Aug/17 15:54;jasobrown;pushed the minor fix as sha {{6b927783ba0777d3dd7c2c3311b246a8dbce5b59}} to 2.1+.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
INSERT statement fails when Tuple type is used as clustering column with default DESC order,CASSANDRA-13717,13089042,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,skonto,akichidis,akichidis,21/Jul/17 13:28,12/Mar/19 14:07,13/Mar/19 22:34,12/Sep/17 21:19,3.0.15,3.11.1,4.0,,,Legacy/Core,Legacy/CQL,,,,0,,,,"When a column family is created and a Tuple is used on clustering column with default clustering order DESC, then the INSERT statement fails. 

For example, the following table will make the INSERT statement fail with error message ""Invalid tuple type literal for tdemo of type frozen<tuple<timestamp, text>>"" , although the INSERT statement is correct (works as expected when the default order is ASC)

{noformat}
create table test_table (
	id int,
	tdemo tuple<timestamp, varchar>,
	primary key (id, tdemo)
) with clustering order by (tdemo desc);
{noformat}
",Cassandra 3.11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Jul/17 13:27;akichidis;example_queries.cql;https://issues.apache.org/jira/secure/attachment/12878350/example_queries.cql,10/Aug/17 01:51;jjirsa;fix_13717;https://issues.apache.org/jira/secure/attachment/12881118/fix_13717,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-07-21 21:09:32.476,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 12 21:19:27 UTC 2017,,,,,,0|i3hujr:,9223372036854775807,3.11.0,,,,,,,jasobrown,jasobrown,,,,,,,,,,"21/Jul/17 13:36;akichidis;After some quick debugging I saw that this error is thrown because of the tuple validation on Tuples.java:

{noformat}
        if (!(receiver.type instanceof TupleType))
            throw invalidRequest(""Invalid tuple type literal for %s of type %s"", receiver.name, receiver.type.asCQL3Type());
{noformat}

When the default ordering on tuple is DESC, the *receiver.type* is instance of type *ReversedType* and not *TupleType* . However, I don't have the expertise to know the cause of this.
","21/Jul/17 21:09;jjirsa;There are a bunch of cases in {{Tuples.java}} where we check {{instanceof TupleType}} that are probably wrong in the case that it's Reversed. I suspect we should be checking if the {{baseType}} (which is already public) of the {{ReversedType}} is a Tuple.
","10/Aug/17 01:44;skonto;I have created a patch and verified Jeff's suggestion:
[patch|https://drive.google.com/open?id=0B0SeiqgJaLZvclhmY0N4dEJtUGs]

{noformat}
cqlsh> create keyspace test with replication = {'class':'SimpleStrategy','replication_factor': 1};cqlsh> create table test.test_table ( id int, tdemo tuple<timestamp, varchar>, primary key (id, tdemo) ) with clustering order by (tdemo desc);
cqlsh> insert into test.test_table (id, tdemo) values (1, ('2017-02-03 03:05+0000','Europe'));
cqlsh> select * from test.test_table;
 id | tdemo
----+-----------------------------------------------
  1 | ('2017-02-03 03:05:00.000000+0000', 'Europe')

(1 rows)

{noformat}

What are the next steps for the review (I am new here)?
","10/Aug/17 01:52;jjirsa;Welcome [~skonto] ! Next step would be to assign yourself (I've done that for you), and then hit 'submit patch' to mark the issue as patch available (I've done that for you again).

We typically ask either the contributor (you) or the reviewer (someone who will volunteer, hopefully soon) to push the patch to a github branch and kick off CI (we have it setup to use circleci for unit tests, and a committer can kick off dtests). We typically ask that your patch includes a test case that fails before your patch and succeeds after it's applied, so while this is not a review, I will say that any reviewer should ask you to do that. 

","10/Aug/17 02:37;skonto;Thnx [~jjirsa] ! Here is my branch: https://github.com/skonto/cassandra/tree/cassandra-13717. As for the test case I read the contribution wiki etc... still a bit confused where should I add it? In dtests or just part of the patch?
","10/Aug/17 02:43;jjirsa;I typically prefer keeping them in unit tests (junit tests in the same repo, check out the test/ directory). There should be a section for cql3 tests, and almost certainly a TupleTest within it that you can add a function or two to.
","10/Aug/17 04:41;jjirsa;[~skonto] - do you know which versions need to be fixed? 3.0? 3.11? trunk?

I've kicked off some test builds [here (unit tests)|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13717] and [here (dtest)|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/173] - we'll want to do that for each branch that needs this fix (and of course, we'll want to add tests to this fix as well).
","10/Aug/17 11:13;skonto;[~jjirsa] I fixed it for trunk (version 4). I could backport it to 3.11 (version reported) as soon as it is verified that this fix is ok.
Good to know about the test procedure, thanx a lot! I will check the unit tests.","10/Aug/17 12:26;skonto;[~jjirsa] I added a test there in TupleTypeTest, updated the branch. How can I update the patch? 
Should I cancel it and add a new one?",20/Aug/17 20:53;skonto;[~jjirsa] Any update or something I should do?,"11/Sep/17 22:21;jasobrown;This bug exists in 3.0, as well - perhaps earlier, but even if it does this doesn't meet the bar of being 'critical' to patch 2.x for.","11/Sep/17 23:03;jasobrown;backported the patch to 3.0 and 3.11, and running tests now:

||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13717-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/13717-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13717-trunk]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/298/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/299/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/300/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13717-3.0]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13717-3.11]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13717-trunk]|
","12/Sep/17 21:19;jasobrown;committed as sha {{a08a816a6a3497046ba75a38d76d5095347dfe95}}.

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception in CompactionExecutor leading to tmplink files not being removed,CASSANDRA-13545,13073913,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,,derokhin,derokhin,22/May/17 15:54,12/Mar/19 14:07,13/Mar/19 22:34,24/Jan/19 18:44,,,,,,Local/Compaction,,,,,0,,,,"We are facing an issue where compactions fail on a few nodes with the following message
{code}
ERROR [CompactionExecutor:1248] 2017-05-22 15:32:55,390 CassandraDaemon.java:185 - Exception in thread Thread[CompactionExecutor:1248,1,main]
java.lang.AssertionError: null
	at org.apache.cassandra.io.sstable.IndexSummary.<init>(IndexSummary.java:86) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.io.sstable.IndexSummaryBuilder.build(IndexSummaryBuilder.java:235) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:316) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:170) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:115) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.append(DefaultCompactionWriter.java:64) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:184) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:256) ~[apache-cassandra-2.2.5.jar:2.2.5]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{code}
Also, the number of tmplink files in /var/lib/cassandra/data/<keyspace name>/blocks/tmplink* is growing constantly until node runs out of space. Restarting cassandra removes all tmplink files, but the issue still continues.
We are using Cassandra 2.2.5 on Debian 8 with Oracle Java 8
{code}
root@cassandra-p10:/var/lib/cassandra/data/mugenstorage/blocks-33167ef0447a11e68f3e5b42fc45b62f# dpkg -l | grep -E ""java|cassandra""
ii  cassandra                      2.2.5                        all          distributed storage system for structured data
ii  cassandra-tools                2.2.5                        all          distributed storage system for structured data
ii  java-common                    0.52                         all          Base of all Java packages
ii  javascript-common              11                           all          Base support for JavaScript library packages
ii  oracle-java8-installer         8u121-1~webupd8~0            all          Oracle Java(TM) Development Kit (JDK) 8
ii  oracle-java8-set-default       8u121-1~webupd8~0            all          Set Oracle JDK 8 as default Java
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12743,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 05 17:17:22 UTC 2017,,,,,,0|i3fazz:,9223372036854775807,,,,,,,,,,,,2.2.5,,,,,,,"05/Jun/17 17:17;derokhin;One of our engineers has been able to find at least one issue which leads to this condition. His findings are below.
---

With a consistent reproduction outside of the production cluster, I downloaded the cassandra source code, setup a remote debugger (eclipse) and connected it to the cassandra process running on my node.
 
At this point I was able to setup breakpoints and examine a live system, starting at the last frame in the traceback (org.apache.cassandra.io.sstable.IndexSummary.<init>(IndexSummary.java:86)). Stepping through the code duing a live compaction, I was able to determine that the issue is indeed a bug in Cassandra that occurs when it is trying to run a compaction job with a very large number of partitions.
 
The SafeMemoryWriter class is used to build the index summary for the new sstable.
{code:java}
public class SafeMemoryWriter extends DataOutputBuffer
{
    private SafeMemory memory;
 
    @SuppressWarnings(""resource"")
    public SafeMemoryWriter(long initialCapacity)
    {
        this(new SafeMemory(initialCapacity));
    }
 
    private SafeMemoryWriter(SafeMemory memory)
    {
        super(tailBuffer(memory).order(ByteOrder.BIG_ENDIAN));
        this.memory = memory;
    }
 
    public SafeMemory currentBuffer()
    {
        return memory;
    }
 
    @Override
    protected void reallocate(long count)
    {
        long newCapacity = calculateNewSize(count);
        if (newCapacity != capacity())
        {
            long position = length();
            ByteOrder order = buffer.order();
 
            SafeMemory oldBuffer = memory;
            memory = this.memory.copy(newCapacity);
            buffer = tailBuffer(memory);
 
            int newPosition = (int) (position - tailOffset(memory));
            buffer.position(newPosition);
            buffer.order(order);
 
            oldBuffer.free();
        }
    }
 
    public void setCapacity(long newCapacity)
    {
        reallocate(newCapacity);
    }
 
    public void close()
    {
        memory.close();
    }
 
    public Throwable close(Throwable accumulate)
    {
        return memory.close(accumulate);
    }
 
    public long length()
    {
        return tailOffset(memory) +  buffer.position();
    }
 
    public long capacity()
    {
        return memory.size();
    }
 
    @Override
    public SafeMemoryWriter order(ByteOrder order)
    {
        super.order(order);
        return this;
    }
 
    @Override
    public long validateReallocation(long newSize)
    {
        return newSize;
    }
 
    private static long tailOffset(Memory memory)
    {
        return Math.max(0, memory.size - Integer.MAX_VALUE);
    }
 
    private static ByteBuffer tailBuffer(Memory memory)
    {
        return memory.asByteBuffer(tailOffset(memory), (int) Math.min(memory.size, Integer.MAX_VALUE));
    }
}
{code}
The appears like it is intended to work with buffers larger than Integer.MAX_VALUE, however if the initial size of the buffer is larger than that the initial value of length() will be incorrect (it won’t be zero) and writing via the DataOutputBuffer will write in the wrong location (it won’t start at offset 0).
 
 
{code:java}
    public IndexSummaryBuilder(long expectedKeys, int minIndexInterval, int samplingLevel)
    {
        this.samplingLevel = samplingLevel;
        this.startPoints = Downsampling.getStartPoints(BASE_SAMPLING_LEVEL, samplingLevel);
 
        long maxExpectedEntries = expectedKeys / minIndexInterval;
        if (maxExpectedEntries > Integer.MAX_VALUE)
        {
            // that's a _lot_ of keys, and a very low min index interval
            int effectiveMinInterval = (int) Math.ceil((double) Integer.MAX_VALUE / expectedKeys);
            maxExpectedEntries = expectedKeys / effectiveMinInterval;
            assert maxExpectedEntries <= Integer.MAX_VALUE : maxExpectedEntries;
            logger.warn(""min_index_interval of {} is too low for {} expected keys; using interval of {} instead"",
                        minIndexInterval, expectedKeys, effectiveMinInterval);
            this.minIndexInterval = effectiveMinInterval;
        }
        else
        {
            this.minIndexInterval = minIndexInterval;
        }
 
        // for initializing data structures, adjust our estimates based on the sampling level
        maxExpectedEntries = Math.max(1, (maxExpectedEntries * samplingLevel) / BASE_SAMPLING_LEVEL);
        offsets = new SafeMemoryWriter(4 * maxExpectedEntries).order(ByteOrder.nativeOrder());
        entries = new SafeMemoryWriter(40 * maxExpectedEntries).order(ByteOrder.nativeOrder());
 
        // the summary will always contain the first index entry (downsampling will never remove it)
        nextSamplePosition = 0;
        indexIntervalMatches++;
    }
{code}
The bug occurs when the entries table in the index summary for the new sstable is larger than Integer.MAX_VALUE bytes (2 GiB). This happens when expectedKeys > Integer.MAX_VALUE / 40 * minIndexInterval . Our partitions for the blocks table have a mean size of 179 bytes, so we would expect to see issues on this table for compactions over about 1.12 TiB.
 
The default value of minIndexInterval is 128, however it is adjustable per table and can be used to avoid this condition. It should be set to a power of 2. I ran this cql on my test node:
{code:sql}
ALTER TABLE tablename.blocks WITH min_index_interval = 512 ;
{code}
Since this change, I haven’t seen the assertion. The compaction has proceeded much farther than before, but it has not completed yet since it is so large.
{noformat}
$ nodetool compactionstats -H
pending tasks: 1
                                     id   compaction type       keyspace    table   completed     total    unit   progress
   9965f4b0-4749-11e7-b21c-91cb0a91f895        Compaction   tablename   blocks   629.51 GB   1.34 TB   bytes     45.71%
Active compaction remaining time :        n/a
{noformat}
I would expect that making this change would fix the issue for all future compactions on all nodes.
 
The index summary is used to reduce disk io to the sstable index. A larger index interval would result in a less efficient index summary and more io to the sstable index. However the min is just the minimum value, the actual value is controlled automatically by Cassandra. On p10, it is 2048 for the larger blocks sstables, so I would not expect a performance impact.

Compaction failed with new error
{code}
ERROR [CompactionExecutor:6] 2017-06-04 10:15:26,115 CassandraDaemon.java:185 - Exception in thread Thread[CompactionExecutor:6,1,RMI Runtime]
java.lang.AssertionError: Illegal bounds [-2147483648..-2147483640); size: 3355443200
        at org.apache.cassandra.io.util.Memory.checkBounds(Memory.java:339) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.SafeMemory.checkBounds(SafeMemory.java:104) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.Memory.getLong(Memory.java:260) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:224) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.CompressedSegmentedFile.createMappedSegments(CompressedSegmentedFile.java:80) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.<init>(CompressedPoolingSegmentedFile.java:38) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Builder.complete(CompressedPoolingSegmentedFile.java:101) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.SegmentedFile$Builder.complete(SegmentedFile.java:188) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.util.SegmentedFile$Builder.complete(SegmentedFile.java:179) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinal(BigTableWriter.java:345) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinalEarly(BigTableWriter.java:333) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.SSTableRewriter.switchWriter(SSTableRewriter.java:297) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.SSTableRewriter.doPrepare(SSTableRewriter.java:345) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:169) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.doPrepare(CompactionAwareWriter.java:79) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:169) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish(Transactional.java:179) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.finish(CompactionAwareWriter.java:89) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:196) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:256) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when starting cassandra: Unable to make UUID from 'aa' (SASI index),CASSANDRA-13669,13084807,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jasonstack,loucash,loucash,05/Jul/17 11:43,12/Mar/19 14:07,13/Mar/19 22:34,25/Jun/18 11:14,3.11.3,4.0,,,,Feature/SASI,,,,,0,sasi,,,"Recently I experienced a problem that prevents me to restart cassandra.
I narrowed it down to SASI Index when added on uuid field.



Steps to reproduce:
1. start cassandra (./bin/cassandra -f)
2. create keyspace, table, index and add data:

{noformat}
CREATE KEYSPACE testkeyspace
WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} 
           AND durable_writes = true;

use testkeyspace ;

CREATE TABLE testtable (
   col1 uuid,
   col2 uuid,
   ts timeuuid,
   col3 uuid,
   PRIMARY KEY((col1, col2), ts) ) with clustering order by (ts desc);

CREATE CUSTOM INDEX col3_testtable_idx ON testtable(col3)
USING 'org.apache.cassandra.index.sasi.SASIIndex'
WITH OPTIONS = {'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer', 'mode': 'PREFIX'};

INSERT INTO testtable(col1, col2, ts, col3)
VALUES(898e0014-6161-11e7-b9b7-238ea83bd70b,
               898e0014-6161-11e7-b9b7-238ea83bd70b,
               now(), 898e0014-6161-11e7-b9b7-238ea83bd70b);
{noformat}

3. restart cassandra

It crashes with an error (sorry it's huge):
{noformat}
DEBUG 09:09:20 Writing Memtable-testtable@1005362073(0.075KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit), flushed range = (min(-9223372036854775808), max(9223372036854775807)]
ERROR 09:09:20 Exception in thread Thread[PerDiskMemtableFlushWriter_0:1,5,main]
org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.Collections$SingletonSet.forEach(Collections.java:4767) ~[na:1.8.0_131]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_131]
	at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
Exception (java.lang.RuntimeException) encountered during startup: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51)
ERROR 09:09:20 Exception encountered during startup
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.blockForWrites(CommitLogReplayer.java:168) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.commitlog.CommitLog.recoverFiles(CommitLog.java:188) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:167) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:323) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:601) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:730) [apache-cassandra-3.9.jar:3.9]
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384) ~[apache-cassandra-3.9.jar:3.9]
	... 6 common frames omitted
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1122) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1084) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384) ~[apache-cassandra-3.9.jar:3.9]
	... 5 common frames omitted
Caused by: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.Collections$SingletonSet.forEach(Collections.java:4767) ~[na:1.8.0_131]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_131]
	at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	... 3 common frames omitted
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391)
at org.apache.cassandra.db.commitlog.CommitLogReplayer.blockForWrites(CommitLogReplayer.java:168)
at org.apache.cassandra.db.commitlog.CommitLog.recoverFiles(CommitLog.java:188)
at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:167)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:323)
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:601)
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:730)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:192)
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384)
... 6 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51)
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391)
at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1122)
at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1084)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:192)
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384)
... 5 more
Caused by: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118)
at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132)
at java.util.Collections$SingletonSet.forEach(Collections.java:4767)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119)
at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233)
at java.lang.Iterable.forEach(Iterable.java:75)
at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233)
at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107)
at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169)
at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48)
at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
... 3 more
{noformat}

When I do ""nodetool flush"" I also get:
{noformat}
$  ./bin/nodetool flush
objc[35941]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/bin/java (0x1052a34c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10536b4e0). One of the two will be used. Which one is undefined.
error: Unable to make UUID from 'aa'
-- StackTrace --
org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118)
at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132)
at java.util.Collections$SingletonSet.forEach(Collections.java:4767)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119)
at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233)
at java.lang.Iterable.forEach(Iterable.java:75)
at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233)
at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107)
at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169)
at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48)
at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:748)
{noformat}

Any ideas how to solve it?
I can keep col3 as text, I figured it out, but I already have bunch of data on production and I basically can't do anything with any of nodes, because I won't be able to start them again.

Thanks,
Lukasz","Tested on:
* macOS Sierra 10.12.5
* Ubuntu 14.04.5 LTS",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-19 16:37:48.528,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 25 11:14:05 UTC 2018,,,,,,0|i3h4in:,9223372036854775807,3.11.0,3.11.1,4.0,,,,,adelapena,adelapena,,,3.9,,,,,,,05/Jul/17 15:12;loucash;Is there any other recovery other than deleting commit log? When commit log is deleted I loose (almost) all the data from a table.,05/Jul/17 17:58;loucash;One way to avoid this error is to delete faulty index before stopping a node.,"19/Sep/17 16:37;mshuler;Reproduced on 3.11.0 release sha, cassandra-3.11 branch HEAD (commit 594f1c1d), and trunk branch HEAD (commit eb76692).",14/May/18 13:35;jasonstack;The problem is that we cannot use Analyzer on UUID type..,"15/May/18 03:21;jasonstack;|branch|utest|
|[trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13669-trunk]|[utest|https://circleci.com/gh/jasonstack/cassandra/736?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link]|
|[3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13669-3.11]|[utest|https://circleci.com/gh/jasonstack/cassandra/735?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link]|

Changes:
 # Validate target type for SASI analyzer.
 # SASI Analyzer only supports Text/Ascii/Varchar

[~adelapena] do you mind having a look?","15/May/18 12:13;adelapena;bq. [~adelapena] do you mind having a look?

Sure, I'll take a look ASAP.","16/May/18 13:50;adelapena;Overall the patch looks good to me. Just a few nits:
 * I have let some minor typo fixes [here|https://github.com/adelapena/cassandra/commit/338b4abea0cd87a11102814936867ff04463d16b].
 * I think that the error message might show the CQL type, instead of the internal one. Done [here|https://github.com/adelapena/cassandra/commit/8b78595dd2eb9c5a75182ad34c93e698bd1f5acb].
* Not a big deal, but during the review I have modified the test to cover all the analyzers and data types, [here|https://github.com/adelapena/cassandra/commit/f5330feeeac1ef87a8dba2d627f241a44bdf7373]. Please take a look and merge it if you find it useful.","17/May/18 07:46;jasonstack;Thanks for the review, I have included your branch. Resubmitted CI.","18/May/18 00:59;jasonstack;unit tests passed, dtest failed to run due to recent changes on dtest.",25/Jun/18 11:14;adelapena;Committed to 3.11 as [ea62d8862c311e3d9b64d622bea0a68d3825aa7d|https://github.com/apache/cassandra/commit/ea62d8862c311e3d9b64d622bea0a68d3825aa7d] and merged to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDK 8u161 breaks JMX integration,CASSANDRA-14173,13131823,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,beobal,beobal,beobal,18/Jan/18 09:14,12/Mar/19 14:07,13/Mar/19 22:34,02/Feb/18 13:00,3.11.2,4.0,,,,,,,,,11,,,,"{\{org.apache.cassandra.utils.JMXServerUtils}} which is used to programatically configure the JMX server and RMI registry (CASSANDRA-2967, CASSANDRA-10091) depends on some JDK internal classes/interfaces. A change to one of these, introduced in Oracle JDK 1.8.0_162 is incompatible, which means we cannot build using that JDK version. Upgrading the JVM on a node running 3.6+ will result in Cassandra being unable to start.
{noformat}
ERROR [main] 2018-01-18 07:33:18,804 CassandraDaemon.java:706 - Exception encountered during startup
java.lang.AbstractMethodError: org.apache.cassandra.utils.JMXServerUtils$Exporter.exportObject(Ljava/rmi/Remote;ILjava/rmi/server/RMIClientSocketFactory;Ljava/rmi/server/RMIServerSocketFactory;Lsun/misc/ObjectInputFilter;)Ljava/rmi/Remote;
        at javax.management.remote.rmi.RMIJRMPServerImpl.export(RMIJRMPServerImpl.java:150) ~[na:1.8.0_162]
        at javax.management.remote.rmi.RMIJRMPServerImpl.export(RMIJRMPServerImpl.java:135) ~[na:1.8.0_162]
        at javax.management.remote.rmi.RMIConnectorServer.start(RMIConnectorServer.java:405) ~[na:1.8.0_162]
        at org.apache.cassandra.utils.JMXServerUtils.createJMXServer(JMXServerUtils.java:104) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.maybeInitJmx(CassandraDaemon.java:143) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:188) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:600) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:689) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]{noformat}

This is also a problem for CASSANDRA-9608, as the internals are completely re-organised in JDK9, so a more stable solution that can be applied to both JDK8 & JDK9 is required.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-18 09:22:08.43,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 06 10:21:27 UTC 2018,,,,,,0|i3p1y7:,9223372036854775807,3.10,3.11.0,3.11.1,3.6,3.7,3.8,3.9,,,,,,,,,,,,18/Jan/18 09:22;spodxx@gmail.com;This seems to be caused by [JDK-8159377|http://www.oracle.com/technetwork/java/javase/8u161-relnotes-4021379.html#JDK-8159377] ([cfdf57b094ce|http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/rev/cfdf57b094ce]) in 8u161.,"20/Jan/18 18:07;eribeiro;Hi guys,

I have uploaded a patch file based on cassandra-3.x branch. Please, see if it helps to solve the issue. 

Best regards,

Ed","20/Jan/18 18:28;mshuler;Build fails on JDK 1.8u152 - this patch would appear to *require* >= 1.8u161?
{noformat}
(trunk *)mshuler@hana:~/git/cassandra$ git diff
diff --git a/src/java/org/apache/cassandra/utils/JMXServerUtils.java b/src/java/org/apache/cassandra/utils/JMXServerUtils.java
index e78ed01746..072f237049 100644
--- a/src/java/org/apache/cassandra/utils/JMXServerUtils.java
+++ b/src/java/org/apache/cassandra/utils/JMXServerUtils.java
@@ -46,6 +46,7 @@ import org.slf4j.LoggerFactory;
import com.sun.jmx.remote.internal.RMIExporter;
import com.sun.jmx.remote.security.JMXPluggableAuthenticator;
import org.apache.cassandra.auth.jmx.AuthenticationProxy;
+import sun.misc.ObjectInputFilter;
import sun.rmi.registry.RegistryImpl;
import sun.rmi.server.UnicastServerRef2;

@@ -308,10 +309,10 @@ public class JMXServerUtils
// to our custom Registry too.
private Remote connectorServer;

- public Remote exportObject(Remote obj, int port, RMIClientSocketFactory csf, RMIServerSocketFactory ssf)
+ public Remote exportObject(Remote obj, int port, RMIClientSocketFactory csf, RMIServerSocketFactory ssf, ObjectInputFilter filter)
throws RemoteException
{
- Remote remote = new UnicastServerRef2(port, csf, ssf).exportObject(obj, null, true);
+ Remote remote = new UnicastServerRef2(port, csf, ssf, filter).exportObject(obj, null, true);
// Keep a reference to the first object exported, the JMXConnectorServer
if (connectorServer == null)
connectorServer = remote;
(trunk *)mshuler@hana:~/git/cassandra$ java -version
java version ""1.8.0_152""
Java(TM) SE Runtime Environment (build 1.8.0_152-b16)
Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode)
(trunk *)mshuler@hana:~/git/cassandra$ ant
Buildfile: /home/mshuler/git/cassandra/build.xml
Trying to override old definition of task junit

init:
[mkdir] Created dir: /home/mshuler/git/cassandra/build/classes/main
[mkdir] Created dir: /home/mshuler/git/cassandra/build/test/lib
[mkdir] Created dir: /home/mshuler/git/cassandra/build/test/classes
[mkdir] Created dir: /home/mshuler/git/cassandra/build/test/stress-classes
[mkdir] Created dir: /home/mshuler/git/cassandra/src/gen-java
[mkdir] Created dir: /home/mshuler/git/cassandra/build/lib
[mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco
[mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco/partials

maven-ant-tasks-localrepo:
[copy] Copying 1 file to /home/mshuler/git/cassandra/build

maven-ant-tasks-download:

maven-ant-tasks-init:

maven-declare-dependencies:

maven-ant-tasks-retrieve-build:
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies.xml
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies-sources.xml
[copy] Copying 61 files to /home/mshuler/git/cassandra/build/lib/jars
[copy] Copying 15 files to /home/mshuler/git/cassandra/build/lib/sources
[copy] Copying 9 files to /home/mshuler/git/cassandra/build/lib/jars
[unzip] Expanding: /home/mshuler/git/cassandra/build/lib/jars/org.jacoco.agent-0.7.5.201505241946.jar into /home/mshuler/git/cassandra/build/lib/jars

check-gen-cql3-grammar:

gen-cql3-grammar:
[echo] Building Grammar /home/mshuler/git/cassandra/src/antlr/Cql.g ...

generate-cql-html:

generate-jflex-java:
[jflex] Generated: StandardTokenizerImpl.java

build-project:
[echo] apache-cassandra: /home/mshuler/git/cassandra/build.xml
[javac] Compiling 1554 source files to /home/mshuler/git/cassandra/build/classes/main
[javac] Note: Processing compiler hints annotations
[javac] Note: Processing compiler hints annotations
[javac] Note: Writing compiler command file at META-INF/hotspot_compiler
[javac] Note: Done processing compiler hints annotations
[javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/JMXServerUtils.java:305: error: Exporter is not abstract and does not override abstract method exportObject(Remote,int,RMIClientSocketFactory,RMIServerSocketFactory) in RMIExporter
[javac] private static class Exporter implements RMIExporter
[javac] ^
[javac] Note: Some input files use or override a deprecated API.
[javac] Note: Recompile with -Xlint:deprecation for details.
[javac] Note: Some input files use unchecked or unsafe operations.
[javac] Note: Recompile with -Xlint:unchecked for details.
[javac] 1 error

BUILD FAILED
/home/mshuler/git/cassandra/build.xml:762: Compile failed; see the compiler error output for details.

Total time: 58 seconds{noformat}","21/Jan/18 03:17;eribeiro;Yes [~mshuler], the first cut of this patch *required* >= 1.8u161 :( 

OTOH, I have put together another version (see further attachments above) that compiles on both 152 and 161. *_But_* *_I still need to verify if it doesn't break Cassandra_ _at runtime, though!_*

(heh, changing jdk back and forth messed up my IDE so it dragged me down)

AFAIK, 161 build has both the legacy and the new constructor (the one with _ObjectInputFilter_), so it should work as intended, but I am not sure about 152 yet. _Please_, let me know what you think, thanks.

*update:* I have uploaded a couple of patches following the pattern cassandra-14173.<versions>.patch that apply to a single branch or a range of branches. 

 ",22/Jan/18 07:13;fattahsafa;161 has the same issue. I had to downgrade to 152 to get it working again.,"26/Jan/18 18:55;beobal;In the linked branch I've removed the dependencies on internal classes completely, so this works both with u161 and earlier JDKs. The branch is against 3.11 and applies trivially to trunk.

I've run the relevant dtests locally & they seem fine as well as manually testing with jconsole & nodetool using:
 * local / remote only connections
 * SSL both with & without client auth
 * internal C* authentication & authorization
 * default JMX authn/z

||branch||testall||dtest||
|[14173-3.11|https://github.com/beobal/cassandra/tree/14173-3.11]|[testall|https://circleci.com/gh/beobal/cassandra/tree/14173-3.11]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/476/]|

 ","26/Jan/18 19:09;eribeiro;Woot! Great job, [~beobal]! :) I have removed my hacky-patches from upload area not to pollute this issue.","30/Jan/18 11:22;jasobrown;+1. I think we should commit this to 3.0, as well. wdyt?",30/Jan/18 11:35;beobal;It only affects versions above 3.6 ,"30/Jan/18 18:49;jasobrown;bq. It only affects versions above 3.6

wfm - go ahead and commit to 3.11 and trunk.","31/Jan/18 10:59;beobal;thanks, the patch actually caused a new (harmless) compiler warning, so the CI failed. I've added an annotation to suppress & will commit once the followup CI run completes.","02/Feb/18 00:13;yogeshkumar.more@gmail.com;Hi Sam,

Thanks for the fix.

May i know when it is planned for release?

We are under some pressure to upgrade java version to u162. 

Thanks,

Yogesh.

 ","02/Feb/18 08:56;beobal;We currently have just 2 issues blocking the 3.11.2 release; this and CASSANDRA-14092. Both should be committed soon, at which point a release vote will be called. tl;dr I would probably expect a release next week.","02/Feb/18 11:52;beobal;I ran into a few issues with CI partly due to the recent dtest changes from CASSANDRA-14134. The master branch of dtests is currently incompatible with non-trunk C* builds (though that's being worked on in CASSANDRA-14206) and running the pre-14134 dtests via builds.apache.org is proving problematic, especially so because of the multi-hour runtime. I've managed to get a run of the latest dtests patched with [~aweisberg]'s 14206 branch through CircleCI here: https://circleci.com/workflow-run/48e1f33f-05a6-4f63-b988-01cc09f03f5e

There are only a handful of failures, none of which are JMX related so I'm going to commit this now. I appreciate this is not ideal, but I think it's the most pragmatic option right now.",02/Feb/18 13:00;beobal;I should add that the CI for trunk version looks reasonable with only pre-existing failures: https://circleci.com/workflow-run/b656ab4a-2249-4b49-8cd2-25edd8f2955a,02/Feb/18 13:00;beobal;Committed to 3.11 & trunk in 28ee665b3c0c9238b61a871064f024d54cddcc79,06/Feb/18 10:21;tsteinmaurer;[~beobal]: locally built Cassandra 3.11 from source including the fix and deployed in our loadtest environment. Starts up fine now with 8u162. Thanks a lot!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary indexes are always rebuilt at startup,CASSANDRA-13725,13089524,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,sbtourist,sbtourist,sbtourist,24/Jul/17 13:06,12/Mar/19 14:07,13/Mar/19 22:34,26/Jul/17 16:41,4.0,,,,,Feature/2i Index,,,,,0,,,,"Following CASSANDRA-10130, a bug has been introduced that causes a 2i to be rebuilt at startup, even if such index is already built.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-24 13:12:20.614,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 26 16:17:36 UTC 2017,,,,,,0|i3hxif:,9223372036854775807,,,,,,,,adelapena,adelapena,,,,,,,,,,"24/Jul/17 13:12;githubbot;GitHub user sbtourist opened a pull request:

    https://github.com/apache/cassandra/pull/135

    CASSANDRA-13725

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sbtourist/cassandra CASSANDRA-13725

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/135.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #135
    
----
commit 915382930a45244d439fd8407407322f6b5fa330
Author: Sergio Bossa <sergio.bossa@gmail.com>
Date:   2017-07-24T13:09:15Z

    Indexes created during column family initialization should not be marked as ""not built"", to avoid rebuilding them needlessly.

----
","24/Jul/17 13:49;sbtourist;This is caused by calling {{SIM#markIndexesBuilding}} when creating the index during column family initialization, which marks the index as ""not built"" and causes the index initialization task to rebuild it.

Given there's no need to mark the index when a new column family is created (as the index will be ""not built"" by definition and there can't be any concurrent indexing), we can just pass a boolean up to {{createIndex()}} to distinguish between index creation at different times, i.e. when a column family is [created|https://github.com/sbtourist/cassandra/blob/CASSANDRA-13725/src/java/org/apache/cassandra/db/Keyspace.java#L394] or [reloaded|https://github.com/sbtourist/cassandra/blob/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L129].

Such solution is implemented in the following patch, with a new dtest verifying it:
|[trunk|https://github.com/apache/cassandra/pull/135]|[dtest|https://github.com/apache/cassandra-dtest/pull/2]|

Test runs are in progress on our internal CI and I will report results as soon as they're ready.
","26/Jul/17 12:31;adelapena;Both the patch and the dtest look good to me, and the CI results seem ok, +1.",26/Jul/17 16:07;adelapena;Committed as [6e19e81db8e4c43bf5ef33308de1ae79916bb61c|https://github.com/apache/cassandra/commit/6e19e81db8e4c43bf5ef33308de1ae79916bb61c].,26/Jul/17 16:17;adelapena;Dtests committed as [b724df80d3bbb55b6b41845633e3a9034116f3be|https://github.com/apache/cassandra-dtest/commit/b724df80d3bbb55b6b41845633e3a9034116f3be].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bugs handling range tombstones in the sstable iterators,CASSANDRA-13340,13056710,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,16/Mar/17 16:01,12/Mar/19 14:07,13/Mar/19 22:34,23/Mar/17 16:20,3.0.13,3.11.0,,,,,,,,,0,,,,"There is 2 bugs in the way sstable iterators handle range tombstones:
# empty range tombstones can be returned due to a strict comparison that shouldn't be.
# the sstable reversed iterator can actually return completely bogus results when range tombstones are spanning multiple index blocks.

The 2 bugs are admittedly separate but as they both impact the same area of code and are both range tombstones related, I suggest just fixing both here (unless something really really mind).

Marking the ticket critical mostly for the 2nd bug: it can truly make use return bad results on reverse queries.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-16 23:06:52.516,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 23 16:20:07 UTC 2017,,,,,,0|i3cdjb:,9223372036854775807,,,,,,,,blambov,blambov,,,,,,,,,,"16/Mar/17 16:18;slebresne;Attaching fix for both issue with unit test below. I tried to explain the problems it solves and how it solves it in the patches.
| [13340-3.0|https://github.com/pcmanus/cassandra/commits/13340-3.0] | [utests|http://cassci.datastax.com/job/pcmanus-13340-3.0-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13340-3.0-dtest] |
| [13340-3.11|https://github.com/pcmanus/cassandra/commits/13340-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13340-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13340-3.11-dtest] |
","16/Mar/17 23:06;mkjellman;did you mean to commit this line in one of the unit tests commented out?

https://github.com/pcmanus/cassandra/commit/66100ffecba6ff55027f6e85b29efaf98700edaa#diff-70d3a7f61389330811d6eb2f7d2d1b76R1391","17/Mar/17 08:17;slebresne;Forgot to remove it, sorry, I can do so on commit though if that's fine (it's not needed for the test to reproduce the failure and I prefer unit test to be as focused as possible, but I indeed forgot to remove it entirely). Thanks for the notice.","23/Mar/17 11:19;blambov;I think this is correct but I find the terminology confusing. Previous block [sometimes|https://github.com/pcmanus/cassandra/commit/75a0c57b3c130343dd8068e32ef096e3057191e9#diff-68d265d33b5303cd50645cb4a7eba569R309] means the next we'll iterate to (previous on disk), [other times|https://github.com/pcmanus/cassandra/commit/75a0c57b3c130343dd8068e32ef096e3057191e9#diff-68d265d33b5303cd50645cb4a7eba569R327] the previous we iterated. I'd prefer a consistent meaning for these; it looks like the new code is at odds with the previous convention on these, so the meanings of {{hasPrevious/NextBlock}} need reversing. {{skipLast/First}} have the same problem, I'd at least add {{IteratedItem}} to their names to add a little clarity.

[{{canIncludeSliceStart/End}}|https://github.com/pcmanus/cassandra/commit/75a0c57b3c130343dd8068e32ef096e3057191e9#diff-68d265d33b5303cd50645cb4a7eba569R334] appear to have exactly the opposite meaning of {{hasPrevious/NextBlock}}. I can see {{loadFromDisk}} needs separate booleans for the difference in the non-indexed case, but {{readCurrentBlock}} can do without the latter, can't it?","23/Mar/17 15:14;slebresne;bq. Previous block sometimes means the next we'll iterate to (previous on disk), other times the previous we iterated.

You're right, that's confusing. That said, I tried switching the newly introduced {{hasPrevious/NextBlock}} but at least to me that felt pretty confusing, so I decide to switch the existing usage. Basically feels more logical to me, though that's possibly somewhat personal. In any case it's consistent now, previous/next refer to the previous/next block we'll iterate.

bq. {{skipLast/First}} have the same problem

If you mean that first/last can be a tad confused when we're reading a block in one sense but iterating on its items afterward in the other sense, then I agree, but I didn't felt inverting those was really improving things. I did added {{IteratedItem}} and completed the comments so it's hopefully more clear.

bq. but {{readCurrentBlock}} can do without the latter, can't it?

Absolutely, removed the redundant argument, thanks.","23/Mar/17 15:38;blambov;LGTM

Nit: there are a few more inverted meanings: [in these two comments|https://github.com/pcmanus/cassandra/blob/94c0a9cca6b072e5f35c666c56e7ad1eb0577e7c/src/java/org/apache/cassandra/db/columniterator/SSTableReversedIterator.java#L189] as well as [this {{lastOfPrevious}}|https://github.com/pcmanus/cassandra/blob/94c0a9cca6b072e5f35c666c56e7ad1eb0577e7c/src/java/org/apache/cassandra/db/columniterator/SSTableReversedIterator.java#L352].","23/Mar/17 16:20;slebresne;Committed (with nits fixed), thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple test failures with NPE in org.apache.cassandra.streaming.StreamSession,CASSANDRA-13416,13061785,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,mshuler,mshuler,mshuler,05/Apr/17 15:52,12/Mar/19 14:07,13/Mar/19 22:34,05/Apr/17 15:59,,,,,,,,,,,0,test-failure,,,"example failures:

http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.dht/StreamStateStoreTest/testUpdateAndQueryAvailableRanges
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.dht/StreamStateStoreTest/testUpdateAndQueryAvailableRanges_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/LocalSyncTaskTest/testDifference
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/LocalSyncTaskTest/testDifference_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/incrementalStreamPlan
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/fullStreamPlan
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/incrementalStreamPlan_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.repair/StreamingRepairTaskTest/fullStreamPlan_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testScheduleTimeout
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testFailSessionDuringTransferShouldNotReleaseReferences
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testScheduleTimeout_compression
http://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.streaming/StreamTransferTaskTest/testFailSessionDuringTransferShouldNotReleaseReferences_compression

one example stack trace:
{code}
Stacktrace

java.lang.NullPointerException
	at org.apache.cassandra.streaming.StreamSession.isKeepAliveSupported(StreamSession.java:244)
	at org.apache.cassandra.streaming.StreamSession.<init>(StreamSession.java:196)
	at org.apache.cassandra.streaming.StreamCoordinator$HostStreamingData.getOrCreateNextSession(StreamCoordinator.java:293)
	at org.apache.cassandra.streaming.StreamCoordinator.getOrCreateNextSession(StreamCoordinator.java:158)
	at org.apache.cassandra.streaming.StreamPlan.requestRanges(StreamPlan.java:94)
	at org.apache.cassandra.repair.LocalSyncTask.startSync(LocalSyncTask.java:85)
	at org.apache.cassandra.repair.SyncTask.run(SyncTask.java:75)
	at org.apache.cassandra.repair.LocalSyncTaskTest.testDifference(LocalSyncTaskTest.java:117)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,05/Apr/17 15:52;mshuler;jenkins-trunk_testall-1491_logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12862097/jenkins-trunk_testall-1491_logs.tar.gz,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 05 15:59:29 UTC 2017,,,,,,0|i3d8un:,9223372036854775807,,,,,,,,,,,,,,,,,,,05/Apr/17 15:59;mshuler;It looks like this was fixed in https://github.com/apache/cassandra/commit/633babf0f02fac56cad7bff03a4ff415feb38f39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReverseIndexedReader may drop rows during 2.1 to 3.0 upgrade,CASSANDRA-13525,13071133,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,beobal,beobal,beobal,11/May/17 11:27,12/Mar/19 14:07,13/Mar/19 22:34,24/May/17 18:15,3.0.14,3.11.0,,,,Legacy/Local Write-Read Paths,,,,,0,,,,"During an upgrade from 2.1 (or 2.2) to 3.0 (or 3.x) queries which perform reverse iteration may silently drop rows from their results. This can happen before sstableupgrade is run and when the sstables are indexed.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-05-16 03:21:16.855,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed May 24 18:15:09 UTC 2017,,,,,,0|i3etvr:,9223372036854775807,3.0.13,3.10,,,,,,,,,,,,,,,,,"11/May/17 11:52;beobal;
The problem occurs when a row spans an block in the row index. When deciding whether to continue reading from disk or move to the next index block, {{UnfilteredDeserializer.OldFormatDeserializer}} accounts for the fact that {{hasNext()}} has bumped the file pointer past the end of the last consumed {{Unfiltered}}. But, it doesn't account for reading the legacy atoms from disk to feed the unfiltered iterator. The legacy atom iterator actually reads the first atom of the {{Unfiltered}} *after* the next one, so that needs to be included when calculating the {{lastConsumedPosition}}, which in turn determines whether the current index block has been exhausted. This only affects the reverse iterator as it's mitigation for rows which cross index block boundaries is more complex than the forward iterator. 

For 3.0, I've pushed [a branch|https://github.com/beobal/cassandra/tree/13525-3.0] with a fix and unit test. Also, it adds a unit test for CASSANDRA-13236 (there's a [dtest PR|https://github.com/riptano/cassandra-dtest/pull/1469] open already), this issue was discovered during investigation into that one but I struggled to figure out a decent unit test for it at the time. 

The 3.11 branch should be pretty much identical, but merging 3.0 -> 3.11 is broken at the moment, (looks like [263740daa4|https://github.com/apache/cassandra/commit/263740daa4c8162a157aa6fbb97793f158d142d1] needs to be merged with {{-s ours}}, but I'll double check). When that's fixed I'll push a 3.11 branch.
","11/May/17 14:27;beobal;Fixed the 3.0 -> 3.11 merge issue, so both branches are up now, I'll update with CI results shortly.
Trunk obviously isn't affected through removing support for pre-3.0 sstables.

||branch||circle-ci||
|[13525-3.0|https://github.com/beobal/cassandra/tree/13525-3.0]|
|[13525-3.11|https://github.com/beobal/cassandra/tree/13525-3.11]|

","16/May/17 03:21;jjirsa;Code looks good to me.

Looks like I found your circleCI run for [3.0|https://circleci.com/gh/beobal/cassandra/10#tests/containers/2] :

{code}
Your build ran 4768 tests with 1 failure
testAccessAndSchema-compression - org.apache.cassandra.cql3.ViewSchemaTest
{code}

For [3.11|https://circleci.com/gh/beobal/cassandra/11#tests/containers/2]:

{code}
Your build ran 5959 tests with 2 failures
testNoArgs-compression - org.apache.cassandra.tools.CompactionStressTest
morejava.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.cassandra.stress.CompactionStress
testWriteAndCompact-compression - org.apache.cassandra.tools.CompactionStressTest
morejava.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.cassandra.stress.CompactionStress
{code}

Neither of those seem related to this patch, so I'm good with that.

I've kicked off build #49 for 3.0 ( https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/ ) and #50 for 3.11 , though it'll be quite some time before it's complete (maybe many days).



","17/May/17 17:38;jjirsa;The dtests are awful - [3.0 has 971 failures|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/49/] and [3.11 has 667 failures|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/50/]

Most look related to ip/port bind failures, but they started with this patch, which is odd. 

","17/May/17 17:43;beobal;There are similar bind related results for trunk though, which obv. doesn't have this patch:

https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-dtest/142/#showFailuresLink","17/May/17 19:45;jjirsa;Re-queued as #55 and #56
","24/May/17 17:40;jjirsa;lgtm, commit away!","24/May/17 18:15;beobal;Thanks, committed to 3.0 in {{62092e45c8bbb75ac9f680188b3746913602507b}} and merged to 3.11, and then to trunk with {{-s ours}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ArithmeticException: / by zero when index is created on table with clustering columns only,CASSANDRA-13400,13061084,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,adelapena,adelapena,adelapena,03/Apr/17 10:24,12/Mar/19 14:19,13/Mar/19 22:34,04/Apr/17 13:04,3.0.13,,,,,Legacy/Local Write-Read Paths,,,,,0,2i,secondary_index,,"If we create an index over the clustering key of a table without regular columns:
{code}
CREATE KEYSPACE k WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
CREATE TABLE k.t (pk text, ck text, PRIMARY KEY (pk, ck));
INSERT INTO k.t (pk, ck) VALUES ( 'pk','ck');
CREATE INDEX idx ON k.t(ck);
{code}
Then the following error index creation erros is logged:
{code}
INFO  10:19:34 Submitting index build of idx for data in BigTableReader(path='/Users/adelapena/datastax/cassandra/data/data/k/t-ed3d6f90185611e7949f55d18a2e5858/mc-1-big-Data.db')
ERROR 10:19:34 Exception in thread Thread[SecondaryIndexManagement:2,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:402) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex.buildBlocking(CassandraIndex.java:723) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex.lambda$getBuildIndexTask$5(CassandraIndex.java:693) ~[main/:na]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_112]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [main/:na]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_112]
Caused by: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_112]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_112]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:398) ~[main/:na]
	... 7 common frames omitted
Caused by: java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.index.SecondaryIndexManager.calculateIndexingPageSize(SecondaryIndexManager.java:629) ~[main/:na]
	at org.apache.cassandra.index.SecondaryIndexBuilder.build(SecondaryIndexBuilder.java:62) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionManager$11.run(CompactionManager.java:1347) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_112]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_112]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_112]
	... 5 common frames omitted
{code}
Any further queries using the index will fail:
{code}
SELECT * FROM k.t where ck = 'ck';
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-04 12:18:59.272,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 04 13:04:22 UTC 2017,,,,,,0|i3d4iv:,9223372036854775807,3.0.11,,,,,,,blerer,blerer,,,,,,,,,,"03/Apr/17 11:09;adelapena;The problem is due to a typo at {{SecondaryIndexManager.calculateIndexingPageSize}}.

The bug only affects 3.0. It doesn't affect to 3.11 nor trunk because it was solved with [this direct commit|https://github.com/adelapena/cassandra/commit/eaced9a541d09d55973b6f88d720e16ac948a559].

This patch fixes the typo and adds a simple test:
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13400-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13400-3.0-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13400-3.0-dtest/]|","04/Apr/17 12:18;blerer;+1
Thanks for the patch.",04/Apr/17 13:04;blerer;Committed into 3.0 at 828ca7cc925de90c3883e935c66f7beec6fa9113 and merged into 3.11 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Dropping column results in ""corrupt"" SSTable",CASSANDRA-13337,13056599,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jborgstrom,jborgstrom,16/Mar/17 09:43,12/Mar/19 14:19,13/Mar/19 22:34,27/Mar/17 10:05,3.0.13,3.11.0,,,,Local/Compaction,,,,,0,,,,"It seems like dropping a column can make SSTables containing rows with writes to only the dropped column will become uncompactable.

Also Cassandra <= 3.9 and <= 3.0.11 will even refuse to start with the same stack trace

{code}
cqlsh -e ""create keyspace test with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }""
cqlsh -e ""create table test.test(pk text primary key, x text, y text)""

cqlsh -e ""update test.test set x='1' where pk='1'""
nodetool flush

cqlsh -e ""update test.test set x='1', y='1' where pk='1'""
nodetool flush
cqlsh -e ""alter table test.test drop x""

nodetool compact test test
error: Corrupt empty row found in unfiltered partition
-- StackTrace --
java.io.IOException: Corrupt empty row found in unfiltered partition
	at org.apache.cassandra.db.rows.UnfilteredSerializer.deserialize(UnfilteredSerializer.java:382)
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator.computeNext(SSTableSimpleIterator.java:87)
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator.computeNext(SSTableSimpleIterator.java:65)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.doCompute(SSTableIdentityIterator.java:123)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.computeNext(SSTableIdentityIterator.java:100)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.computeNext(SSTableIdentityIterator.java:30)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:95)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:32)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:369)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:189)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:158)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:509)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:369)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129)
	at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:58)
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:67)
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:26)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96)
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:227)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:190)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$8.runMayThrow(CompactionManager.java:610)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
	at java.lang.Thread.run(Thread.java:745)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-20 15:25:19.689,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 27 10:05:14 UTC 2017,,,,,,0|i3ccun:,9223372036854775807,3.0.12,3.10,,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,"20/Mar/17 15:25;slebresne;This is definitively wrong, attaching patch for fix (including an unit test to reproduce).
| [13337-3.0|https://github.com/pcmanus/cassandra/commits/13337-3.0] | [utests|http://cassci.datastax.com/job/pcmanus-13337-3.0-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13337-3.0-dtest] |
| [13337-3.11|https://github.com/pcmanus/cassandra/commits/13337-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13337-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13337-3.11-dtest] |

I'll note that the patch basically disable the error message seen here, instead simply ignoring empty rows from disk since they can happen. I suppose it would be possible to do a more involved checking to make sure we didn't wrote something actually empty, but I'm not sure at all it's worth the trouble (not the cost of that check on a pretty hot path) especially given that expecting non-empty rows was wrong no only due to dropping, but also because we can actually skip columns due to the {{ColumnFilter}} within {{SerializationHelper}}. I believe the latter would only potentially impact thrift queries, which may be why nobody as yet reported that problem, but it's still wrong.
","20/Mar/17 16:58;ifesdjeen;There's another way to reproduce the same issue with slightly different steps:

{code}
CREATE KEYSPACE IF NOT EXISTS ""test"" WITH REPLICATION = {'class' : 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': 1 };

CREATE TABLE IF NOT EXISTS ""test"".""reproduce"" (pk1 int, ck1 int, v1 int, v2 int, v3 int, v4 int, v5 int, PRIMARY KEY(pk1, ck1));

UPDATE ""test"".""reproduce"" SET v2 = 1, v3 = 3, v4 = 4 WHERE pk1 = 1 AND ck1 = 0;

ALTER TABLE ""test"".""reproduce"" DROP v2;
ALTER TABLE ""test"".""reproduce"" DROP v3;
ALTER TABLE ""test"".""reproduce"" DROP v4;

SELECT * FROM test.reproduce;
{code}

But essentially the problem is that we do return empty rows from local storage. For example, when {{UPDATE}} was used to set only a subset of rows, then the rows that were used in {{UPDATE}} get dropped. When trying to query, we end up with an empty row. This wouldn't happen with {{INSERT}} since for {{INSERT}} we have liveness set.

I just see a single small problem: 

{code}
        createTable(""CREATE TABLE %s(k int PRIMARY KEY, x int, y int)"");
        execute(""UPDATE %s SET x = 1 WHERE k = 0"");
        flush(doFlush); // (1)
        execute(""ALTER TABLE %s DROP x"");
{code}

If we do flush at point {{1}}, we will end up with a single row {{row(1, null)}}. However, if we do not do flush and query directly from memtable, we end up with an empty result.","21/Mar/17 11:04;slebresne;bq. I just see a single small problem

You're right, that is wrong. And that's because while compaction uses {{UnfilteredSerializer.deserialize()}} which the first patch changes, {{SSTableIterator/SSTableReversedIterator}} which are used by queries do not, it uses {{UnfilteredDeserializer}} (which is more lazy). Pushed an additional patch that fix that part (unfortunately, due to how {{UnfilteredDesializer}} works, we can't easily handle this within {{UnfilteredDeserializer}} itself; still not terribly hard to handle properly).","24/Mar/17 15:44;ifesdjeen;Sorry for the delay, I have somehow missed the notification. 
The new patch looks great, thanks for taking care of it!

+1 ","27/Mar/17 10:05;slebresne;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Legacy deserializer can create empty range tombstones,CASSANDRA-13341,13056717,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,16/Mar/17 16:24,12/Mar/19 14:19,13/Mar/19 22:34,29/Mar/17 11:25,3.0.13,3.11.0,,,,,,,,,0,,,,"Range tombstones in the 2.x file format is a bit far-westy so you can actually get sequences of range tombstones like {{\[1, 4\]@3 \[1, 10\]@5}}. But the current legacy deserializer doesn't handle this correctly. On the first range, it will generate a {{INCL_START(1)@3}} open marker, but upon seeing the next tombstone it will decide to close the previously opened range and re-open with deletion time 5, so will generate {{EXCL_END_INCL_START(1)@3-5}}. That result in the first range being empty, which break future assertions in the code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-21 10:48:57.876,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 29 11:25:16 UTC 2017,,,,,,0|i3cdkv:,9223372036854775807,,,,,,,,blambov,blambov,,,,,,,,,,"16/Mar/17 16:49;slebresne;Attaching fix and a unit test to demonstrate the problem. The basic idea is that the legacy deserializer has to wait until he has seen all range tombstones with the same start before generating an open marker.

| [13341-3.0|https://github.com/pcmanus/cassandra/commits/13341-3.0] | [utests|http://cassci.datastax.com/job/pcmanus-13341-3.0-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13341-3.0-dtest] |
| [13341-3.11|https://github.com/pcmanus/cassandra/commits/13341-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13341-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13341-3.11-dtest] |

[~blambov]: setting you as reviewer because while it's a different problem than in CASSANDRA-13237, it touches basically the same code. If you don't have time, feel free to de-assign yourself though.
","21/Mar/17 10:48;blambov;Looks good with a couple of documentation nits:
- The comment to [{{popMarker}}|https://github.com/pcmanus/cassandra/commit/a0db00a532ed52b173fe24db1ce641ac9848ce2f#diff-19cb83af11bbbc4d803ca33c8cdea57aR675] needs to mention that this is called multiple times; it currently leaves the false impression we would get the outstanding open marker as the last item in the iteration.
- [{{openNew}} comment text|https://github.com/pcmanus/cassandra/commit/a0db00a532ed52b173fe24db1ce641ac9848ce2f#diff-19cb83af11bbbc4d803ca33c8cdea57aR690] looks wrong and at odds with the code.
- ""popping"" not [""poping""|https://github.com/pcmanus/cassandra/commit/a0db00a532ed52b173fe24db1ce641ac9848ce2f#diff-19cb83af11bbbc4d803ca33c8cdea57aR640]. The whole diagnostic printout seems costly and should probably disappear.
","22/Mar/17 15:19;slebresne;Thanks, pushed a new commit with fixes for those. Re-triggered CI to be extra sure even though it's mostly updates to comments. ","29/Mar/17 11:25;slebresne;Tests are good, committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LCS estimated compaction tasks does not take number of files into account,CASSANDRA-13354,13057554,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,Jan Karlsson,Jan Karlsson,Jan Karlsson,20/Mar/17 15:12,12/Mar/19 14:19,13/Mar/19 22:34,07/Apr/17 10:57,4.0,,,,,Local/Compaction,,,,,1,,,,"In LCS, the way we estimate number of compaction tasks remaining for L0 is by taking the size of a SSTable and multiply it by four. This would give 4*160mb with default settings. This calculation is used to determine whether repaired or repaired data is being compacted.

Now this works well until you take repair into account. Repair streams over many many sstables which could be smaller than the configured SSTable size depending on your use case. In our case we are talking about many thousands of tiny SSTables. As number of files increases one can run into any number of problems, including GC issues, too many open files or plain increase in read latency.

With the current algorithm we will choose repaired or unrepaired depending on whichever side has more data in it. Even if the repaired files outnumber the unrepaired files by a large margin.

Similarily, our algorithm that selects compaction candidates takes up to 32 SSTables at a time in L0, however our estimated task calculation does not take this number into account. These two mechanisms should be aligned with each other.

I propose that we take the number of files in L0 into account when estimating remaining tasks. 
",Cassandra 2.2.9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Mar/17 15:43;Jan Karlsson;13354-trunk.txt;https://issues.apache.org/jira/secure/attachment/12859578/13354-trunk.txt,23/Mar/17 19:41;Jan Karlsson;patchedTest.png;https://issues.apache.org/jira/secure/attachment/12860216/patchedTest.png,23/Mar/17 19:42;Jan Karlsson;unpatchedTest.png;https://issues.apache.org/jira/secure/attachment/12860217/unpatchedTest.png,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-04-06 20:29:31.601,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,,,Fri Apr 07 10:57:10 UTC 2017,,,,,,0|i3cir3:,9223372036854775807,2.2.9,,,,,,,krummas,krummas,,,,,,,,,,"20/Mar/17 15:16;Jan Karlsson;Added patch on 4.0 to fix this. Applies cleanly to other versions as well(tested 2.2.9).
I have tested this in a cluster and will upload some graphs as well.
Comments and suggestions welcome!","23/Mar/17 19:47;Jan Karlsson;I did some tests simulating traffic on a 4 node cluster. 2 of the nodes were running with my patch while the other two ran without it.
Steps to reproduce:
Traffic on
Turn one of the nodes off
Wait 7 minutes
Truncate hints on all other nodes
Turn node on
Run repair on the node

As you can see the unpatched version kept increasing as non-repaired data from ongoing traffic was prioritized. If I had more discrepancies in my data set, this would just increase to the configured FD limit or until you die from heap pressure.

Repair is completed at 8:11pm but those small repaired files are not compacted as it picks unrepaired new sstables over the small repaired sstables. However, it did show a downwards trend as compaction was slightly faster than insertion and would probably eventually end with the repaired files compacted.

During the unpatched test, it only showed 2 pending compactions with 22k~ file descriptors open/10k~ sstables. At 8:33pm I disabled the traffic completely to hurry this along.
SSTables in each level: [10347/4, 5, 0, 0, 0, 0, 0, 0, 0]",06/Apr/17 20:29;JoshuaMcKenzie;[~krummas]: have bandwidth for review on this one?,"07/Apr/17 07:03;krummas;patch lgtm, I just pushed a tiny nit here: https://github.com/krummas/cassandra/commits/13354

I'm running dtests, not unlikely that some test relies on the old calculations, will commit if tests look good and you agree with my small change [~Jan Karlsson]",07/Apr/17 07:19;Jan Karlsson;yes small change lgtm,"07/Apr/17 10:57;krummas;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cqlsh COPY fails importing Map<String,List<String>>, ParseError unhashable type list",CASSANDRA-13364,13058179,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,Stefania,nicolaen,nicolaen,22/Mar/17 10:54,12/Mar/19 14:19,13/Mar/19 22:34,07/Apr/17 01:09,2.1.18,2.2.10,3.0.13,3.11.0,,Legacy/Tools,,,,,0,cqlsh,,,"When importing data with the _COPY_ command into a column family that has a _map<text, frozen<list<text>>>_ field, I get a _unhashable type: 'list'_ error. Here is how to reproduce:

{code}
CREATE TABLE table1 (
    col1 int PRIMARY KEY,
    col2map map<text, frozen<list<text>>>
);

insert into table1 (col1, col2map) values (1, {'key': ['value1']});

cqlsh:ks> copy table1 to 'table1.csv';


table1.csv file content:
1,{'key': ['value1']}


cqlsh:ks> copy table1 from 'table1.csv';
...
Failed to import 1 rows: ParseError - Failed to parse {'key': ['value1']} : unhashable type: 'list',  given up without retries
Failed to process 1 rows; failed rows written to kv_table1.err
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s
1 rows imported from 1 files in 0.420 seconds (0 skipped).
{code}

But it works fine for Map<String, Set<String>>.

{code}
CREATE TABLE table2 (
    col1 int PRIMARY KEY,
    col2map map<text, frozen<set<text>>>
);

insert into table2 (col1, col2map) values (1, {'key': {'value1'}});

cqlsh:ks> copy table2 to 'table2.csv';


table2.csv file content:
1,{'key': {'value1'}}


cqlsh:ks> copy table2 from 'table2.csv';
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s
1 rows imported from 1 files in 0.417 seconds (0 skipped).
{code}

The exception seems to arrive in _convert_map_ function in _ImportConversion_ class inside _copyutil.py_.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-29 03:34:57.189,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 07 01:09:32 UTC 2017,,,,,,0|i3cmlr:,9223372036854775807,,,,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,"29/Mar/17 03:34;Stefania;The problem is that we cannot parse maps into Python dictionaries because otherwise we couldn't parse dictionaries with another dictionary as key. So we use sets of tuples for dictionaries. This means that even values must be hashable and lists aren't. A trivial fix is to however parse lists as tuples, rather than lists. The driver should accept tuples for list cql types.

This is a regression for 2.1 and so the fix should be applied to 2.1 too.

||2.1||2.2||3.0||3.11||trunk||
|[patch|https://github.com/stef1927/cassandra/tree/13364-cqlsh-2.1]|[patch|https://github.com/stef1927/cassandra/tree/13364-cqlsh-2.2]|[patch|https://github.com/stef1927/cassandra/tree/13364-cqlsh-3.0]|[patch|https://github.com/stef1927/cassandra/tree/13364-cqlsh-3.11]|[patch|https://github.com/stef1927/cassandra/tree/13364-cqlsh]|
|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13364-cqlsh-2.1-cqlsh-tests/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13364-cqlsh-2.2-cqlsh-tests/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13364-cqlsh-3.0-cqlsh-tests/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13364-cqlsh-3.11-cqlsh-tests/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13364-cqlsh-cqlsh-tests/]|

The patch applies cleanly to all branches.

Test is [here|https://github.com/riptano/cassandra-dtest/pull/1456].","29/Mar/17 08:03;nicolaen;I see. Thanks for the fix 👍 I've just checked that the tuple type in python preserves the order, and it does.","06/Apr/17 12:41;ifesdjeen;+1, the patch looks good! 

The test failures on 2.1 and trunk ({{describe_mv}} and {{test_bulk_round_trip}}) seem to be unrelated...","07/Apr/17 01:09;Stefania;Thank you [~ifesdjeen] and [~nicolaen].

Committed to 2.1 as 010b5f3a567663a5ceb823932a0b430848d331e3 and merged upwards.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix racy read command serialization,CASSANDRA-13363,13058159,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,arokhin,arokhin,22/Mar/17 09:22,12/Mar/19 14:19,13/Mar/19 22:34,30/Aug/17 16:39,3.0.15,3.11.1,,,,,,,,,0,pull-request-available,,,"Constantly see this error in the log without any additional information or a stack trace.

{code}
Exception in thread Thread[MessagingService-Incoming-/10.0.1.26,5,main]
{code}

{code}
java.lang.ArrayIndexOutOfBoundsException: null
{code}

Logger: org.apache.cassandra.service.CassandraDaemon
Thrdead: MessagingService-Incoming-/10.0.1.12
Method: uncaughtException
File: CassandraDaemon.java
Line: 229","CentOS 6, Cassandra 3.10","Github user johnyannj closed the pull request at:

    https://github.com/apache/cassandra/pull/137
;01/Oct/18 12:53;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-03 13:18:00.526,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 30 16:39:24 UTC 2017,,,,,,0|i3cmhb:,9223372036854775807,,,,,,,,beobal,beobal,,,,,,,,,,03/Apr/17 13:18;slebresne;I really don't think we can do anything about it unless you provide a full stack trace.,"03/Apr/17 13:22;ifesdjeen;You can turn off {{OmitStackTraceInFastThrow}} if it's on, my guess is that the stack trace might be emitted because the amount of thrown exceptions has reached a threshold and was optimised away.","10/Apr/17 10:48;arokhin;[~ifesdjeen] Thank you, turned it off. [~slebresne] I'll let you know when/if the issue is reproduced. ","04/May/17 17:30;arokhin;[~slebresne] Looks like we met the exception again. With a stack trace this time. Is that helpful?

{noformat}
ERROR 17:22:43 Exception in thread Thread[MessagingService-Incoming-/10.0.1.12,5,main]
java.lang.ArrayIndexOutOfBoundsException: 71
    at org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer.deserialize(AbstractClusteringIndexFilter.java:74) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer.deserialize(SinglePartitionReadCommand.java:1021) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:638) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:568) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.13.jar:3.0.13]
    at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.13.jar:3.0.13]
{noformat}
","04/May/17 17:33;arokhin;Quick update - Cassandra version now is 3.0.13

And worth to mention that 3 nodes Cassandra cluster becomes completely unreposnsive after that exception. 

Exception itself occures on all 3 nodes.","18/Jul/17 11:23;shashikantkulkarni;Hello,
I am also facing the similar issue so adding my comment here. I have Apache Cassandra v3.9, with 3 node in cluster. Replication factor 2. Same datacenter. CentOS 6.8, Java 8

{noformat}
ERROR [MessagingService-Incoming-/10.0.0.111] 2017-07-06 14:26:02,506 CassandraDaemon.java:226 - Exception in thread Thread[MessagingService-Incoming-/10.0.0.111,5,main]
java.lang.IndexOutOfBoundsException: index (2) must be less than size (2)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:310) ~[guava-18.0.jar:na]
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:292) ~[guava-18.0.jar:na]
	at com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:65) ~[guava-18.0.jar:na]
	at org.apache.cassandra.db.ClusteringPrefix$Serializer.deserializeValuesWithoutSize(ClusteringPrefix.java:359) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.ClusteringBoundOrBoundary$Serializer.deserializeValues(ClusteringBoundOrBoundary.java:179) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.ClusteringBoundOrBoundary$Serializer.deserialize(ClusteringBoundOrBoundary.java:161) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.Slice$Serializer.deserialize(Slice.java:322) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.Slices$Serializer.deserialize(Slices.java:336) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.filter.ClusteringIndexSliceFilter$SliceDeserializer.deserialize(ClusteringIndexSliceFilter.java:174) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer.deserialize(AbstractClusteringIndexFilter.java:77) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer.deserialize(SinglePartitionReadCommand.java:1041) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:696) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:626) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:114) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:190) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.9.0.jar:3.9.0]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.9.0.jar:3.9.0]
{noformat}

After this exception the cluster becomes unresponsive and start throwing errors if we try to query for web application.

Thanks","05/Aug/17 10:18;zhaoyan;Hello,
I am also facing the similar issue so adding my comment here.

cassandra 3.0.14  jdk8

{code:java}
ERROR [MessagingService-Incoming-/10.xx.0.205] 2017-08-05 18:12:38,319 CassandraDaemon.java:207 - Exception in thread Thread[MessagingService-Incoming-/10.xx.0.205,5,main]
java.lang.ArrayIndexOutOfBoundsException: 36
        at org.apache.cassandra.db.Slice$Bound$Serializer.deserialize(Slice.java:542) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.Slice$Serializer.deserialize(Slice.java:335) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.Slices$Serializer.deserialize(Slices.java:346) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.filter.ClusteringIndexSliceFilter$SliceDeserializer.deserialize(ClusteringIndexSliceFilter.java:176) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer.deserialize(AbstractClusteringIndexFilter.java:77) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer.deserialize(SinglePartitionReadCommand.java:1031) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:638) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:568) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.14.jar:3.0.14]
ERROR [MessagingService-Incoming-/10.xx.0.205] 2017-08-05 18:12:39,994 CassandraDaemon.java:207 - Exception in thread Thread[MessagingService-Incoming-/10.xx.0.205,5,main]
java.lang.ArrayIndexOutOfBoundsException: 103
        at org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer.deserialize(AbstractClusteringIndexFilter.java:74) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer.deserialize(SinglePartitionReadCommand.java:1031) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:638) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:568) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.14.jar:3.0.14]
{code}
","05/Aug/17 18:00;jjirsa;Relevant code is:

{code}
        public ClusteringIndexFilter deserialize(DataInputPlus in, int version, CFMetaData metadata) throws IOException
        {
            Kind kind = Kind.values()[in.readUnsignedByte()]; // We read a byte, and then try to find the kind, and we're way outside of the array
            boolean reversed = in.readBoolean();

            return kind.deserializer.deserialize(in, version, metadata, reversed);
        }
{code}

And

{code}
            public Slice.Bound deserialize(DataInputPlus in, int version, List<AbstractType<?>> types) throws IOException
            {
                Kind kind = Kind.values()[in.readByte()]; // same thing, read a byte, way outside of the array
                return deserializeValues(in, kind, version, types);
            }
{code}
","05/Aug/17 21:08;jjirsa;{quote}
And worth to mention that 3 nodes Cassandra cluster becomes completely unreposnsive after that exception.
Exception itself occures on all 3 nodes.
{quote}

Does a cluster restart help, or does it stay broken?

Did you recently do any changes? Changing schemas?  Do you see any signs of physical hardware errors (interface counters showing errors)?","06/Aug/17 08:22;zhaoyan;please  see  CASSANDRA-12435   same question.

when I query with index  ( use expr)  by java driver,  It will appear。

as I test,  3.0.10-3.0.14  all has this problem。","09/Aug/17 02:37;zhaoyan;I apply one patch here:

https://github.com/apache/cassandra/pull/137","09/Aug/17 03:59;githubbot;GitHub user johnyannj opened a pull request:

    https://github.com/apache/cassandra/pull/137

    CASSANDRA-13363/CASSANDRA-12435 fix index flag before not be consistent with follow data content

    fix CASSANDRA-13363/CASSANDRA-12435
    
    when the command will be executed by local node and target node.
    the command.index will be reload by local thread.
    
    when code reach indexFlag(command.index.isPresent())  is false
    but reach if (command.index.isPresent()) , it is changed to true, because the ""command.index"" has been loaded by local execute thread.
    
    the command.index can be reload by target node, so it's serialize is optional。 but the flag must be consistent with follow data content.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/johnyannj/cassandra johnyannj-patch-fix-13363

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/137.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #137
    
----
commit 5050ff51b1f62fe7a678be01f79ff962a0fd6caa
Author: zhaoyan <zhaoyan@zhaoyanblog.com>
Date:   2017-08-09T02:35:46Z

    Update ReadCommand.java
    
    fix CASSANDRA-13363/CASSANDRA-12435
    
    when the command will be executed by local node and target node.
    the command.index will be reload by local thread.
    
    when code reach indexFlag(command.index.isPresent())  is false
    but reach if (command.index.isPresent()) , it is changed to true, because the ""command.index"" has been loaded by local execute thread.
    
    the command.index can be reload by target node, so it's serialize is optional。 but the flag must be consistent with follow data content.

----
","09/Aug/17 04:24;jjirsa;Marking patch available
","14/Aug/17 15:29;iamaleksey;The problem is real, and the patch does work, but I'm afraid it doesn't quite solve the issue completely.

There is also an issue involving {{serializedSize()}}. {{index}} field might be empty at the time when we calculate the size of the message, and switch to non-empty afterwards. It's not currently an issue since {{READ_COMMAND}} is not using a {{CallbackDeterminedSerializer}} but it's still a bug.

The proper fix would be to make sure the field never changes and is only set once at construction time. While at it, might also want to refactor it to not be {{Optional}}. {{Optional}} is reserved for return types, not object fields and method arguments.

Give me a couple hours to try work it out properly?","16/Aug/17 09:14;zhaoyan;hi @Aleksey Yeschenko

thank you for your review.
the serializedSize()  is real a hidden trouble。

I do agree with you that：
""The proper fix would be to make sure the field never changes and is only set once at construction time.""

the index is designed as lazy load , load once , and not set at construction time,    so  I think its load may be very hard.

What about removing index from the readcommand‘s  serialize?
","16/Aug/17 10:05;beobal;[~zhaoyan] 

bq.the index is designed as lazy load , load once , and not set at construction time, so I think its load may be very hard

This was true at one point, but in CASSANDRA-10215 that changed so that we could avoid performing the lookup to determine which index to use multiple times during the query execution. That patch was didn't really go far enough though (which is my bad), as [~iamaleksey] points out there are several shortcomings with it; it shouldn't be using {{Optional}}, the field should be final and so forth. 

However, there is also another issue at play here. Queries with secondary indexes were historically always executed as partition range queries, as their results can span multiple partitions. Even when a partition key restriction is present, we convert the query to a range read command and the index field *is* always set at construction time (in {{SelectStatement::getRangeCommand}}), so it is never lazily loaded. There is a related bug though, CASSANDRA-11617, which causes queries with a partition key restriction && a custom index expression to be executed as single partition read command. When these are created (in {{SelectStatement::getSliceCommands}}), we *don't* ensure that the index is loaded at construction time (because we weren't expecting any query with an index to generate such a command). So in this case, the lazy loading happens when the command is executed locally, leading to the race causing the serialization issues you're seeing. 

bq.What about removing index from the readcommand‘s serialize?

This would partially undo CASSANDRA-10215 and require each replica to figure out which index to use, which is potentially expensive (relatively) and unnecesary.
 
So to cut a long story short, [~iamaleksey]'s diagnosis and solution are correct, making sure that where an index is appropriate it gets set on the read command during construction is the right  way to fix this. The more thorough refactoring, which should have been done in CASSANDRA-10215, is also a good idea.  ","18/Aug/17 01:12;zhaoyan;Hi @Sam Tunnicliffe 

Thank you for your patient explanation.

I has one more question： 

""This would partially undo CASSANDRA-10215 and require each replica to figure out which index to use, ""

Does it take a lot of time to do ""the lookup to determine which index to use ""?

1=>send the command to other replica, must after the local replica figure out which index to use .
2=>send to all replica, all replica figure it out itself.

which looks better?","18/Aug/17 10:03;beobal;{quote} Does it take a lot of time to do ""the lookup to determine which index to use ""?

1=>send the command to other replica, must after the local replica figure out which index to use .
2=>send to all replica, all replica figure it out itself.

which looks better?
{quote}

Through profiling, we found that a non-trivial amount of time was spent in {{SecondaryIndexManager::getBestIndexFor}}. Part of the problem was that once found, there was no index field on the command to set, so originally this lookup was being done 3 or 4 times per-query, on every replica. 
Once that was fixed, it made sense to just do the index lookup once when the command is constructed, so it could be included in the serialization & so save the replicas from having to do the lookup. So the lookup goes from being performed (4 x num_replicas) times to once per query, which seems like a good idea generally. 

",18/Aug/17 14:52;iamaleksey;Doing it at most once on each replica is probably not a big deal at all. But I'd rather not change current behaviour (except fix this issue). Which would also mean we'd be able to implement CASSANDRA-10214 at any point without a protocol change.,"21/Aug/17 12:10;iamaleksey;[~zhaoyan] Thanks again for the initial analysis. And that patch would 100% prevent the AIOOBE, just not fix the underlying issue entirely.

A more comprehensive refactoring to fix the race for good:
||3.0||3.11||4.0||
|[branch|https://github.com/iamaleksey/cassandra/tree/13363-3.0]|[branch|https://github.com/iamaleksey/cassandra/tree/13363-3.11]|[branch|https://github.com/iamaleksey/cassandra/tree/13363-4.0]|
|[utest|https://circleci.com/gh/iamaleksey/cassandra/2]|[utest|https://circleci.com/gh/iamaleksey/cassandra/3]|[utest|https://circleci.com/gh/iamaleksey/cassandra/4]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/211/testReport/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/212/testReport/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/213/testReport/]|
[~beobal] can you please review?

EDIT: Added test results","29/Aug/17 15:12;beobal;LGTM except now that {{findIndex}} is baked into {{SinglePartitionReadCommand}}, we perform a lookup through {{SIM::getBestIndexFor}} when creating the commands to actually read from index tables in {{CompositesSearcher}} & {{KeysSearcher}}. Also, because those commands are created using the base table's CFM, we actually end up finding an index (the one we're in the process of searching). In practice, I don't suppose this is much of a problem as the execution of those commands is done directly through {{queryMemtableAndDisk}}, so we don't attempt to erroneously use the index. However, it is confusing and more seriously, it breaks {{secondary_index_test:TestSecondaryIndexes.test_only_coordinator_chooses_index_for_query}}.

Aside from that, I just have a couple of tiny nits, feel free to ignore either/both:

{{getBestIndexFor(ReadCommand)}} is now only used by tests which could easily be tweaked to use the version which takes a {{RowFilter}}. OFC, that trivial method doesn't really muck up the the {{SIM}} API, so nbd if it stays, but it isn't really adding anything either so ¯\_(ツ)_/¯

In some methods of {{*ReadCommand}}, the long lists of args are formatted 1 per line, and in others are in a single line. e.g. {{SPRC::create}} vs {{SPRC::copy}} etc. All of these are already touched by this patch, so they may as well be consistently formatted.
","29/Aug/17 17:10;iamaleksey;Thanks for the review. Pushed a commit addressing the issues to the same branch (and rebased on top of most recent 3.0 while at it). Tests scheduled/running: [utest|https://circleci.com/gh/iamaleksey/cassandra/17], [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/223/].","29/Aug/17 17:50;beobal;LGTM and the failing dtest is passing for me locally now, so +1 assuming CI is still happy.","30/Aug/17 16:39;iamaleksey;The only unit test that failed was {{ViewFilteringTest}}, due to a timeout. Reran locally, it passed. No new dtests seems to be affected (11 failures total).

Committed as [7f297bcf8aced983cbc9c4103d0ebefc1789f0dd|https://github.com/apache/cassandra/commit/7f297bcf8aced983cbc9c4103d0ebefc1789f0dd] to 3.0 and merged up.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactionStrategyManager should take write not read lock when handling remove notifications,CASSANDRA-13422,13062252,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,06/Apr/17 21:34,12/Mar/19 14:19,13/Mar/19 22:34,10/Apr/17 23:50,3.11.0,4.0,,,,Legacy/Local Write-Read Paths,,,,,0,,,,"{{getNextBackgroundTask}} in various compaction strategies (definitely {{LCS}}) rely on checking the result of {{DataTracker.getCompacting()}} to avoid accessing data and metadata related to tables that have already head their resources released.

There is a race where this check is unreliable and will claim a table that has its resources already released is not compacting resulting in use after free.

[{{LeveledCompactionStrategy.findDroppableSSTable}}|https://github.com/apache/cassandra/blob/c794d2bed7ca1d10e13c4da08a3d45f5c755c1d8/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java#L504] for instance has this three part logical && condition where the first check is against the compacting set before calling {{worthDroppingTombstones}} which fails if the table has been released.

The order of events is basically that CompactionStrategyManager acquires the read lock in getNextBackgroundTask(), then proceeds eventually to findDroppableSSTable and acquires a set of SSTables from the manifest. While the manifest is thread safe it's not accessed atomically WRT to other operations. Once it has acquired the set of tables it acquires (not atomically) the set of compacting SSTables and iterates checking the former against the latter.

Meanwhile other compaction threads are marking tables obsolete or compacted and releasing their references. Doing this removes them from {{DataTracker}} and publishes a notification to the strategies, but this notification only requires the read lock. After the compaction thread has published the notifications it eventually marks the table as not compacting in {{DataTracker}} or removes it entirely.

The race is then that the compaction thread generating a new background task acquires the sstables from the manifest on the stack. Any table in that set that was compacting at that time must remain compacting so that it can be skipped. Another compaction thread finishes a compaction and is able to remove the table from the manifest and then remove it from the compacting set. The thread generating the background task then acquires the list of compacting tables which doesn't include the table it is supposed to skip.

The simple fix appears to be to require threads to acquire the write lock in order to publish notifications of tables being removed from compaction strategies. While holding the write lock it won't be possible for someone to see a view of tables in the manifest where tables that are compacting aren't compacting in the view.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-07 12:59:21.959,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 10 23:50:52 UTC 2017,,,,,,0|i3dbqf:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"06/Apr/17 22:02;aweisberg;||Code|utests|dtests||
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...aweisberg:cassandra-13422-3.11?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/10]|[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/6/#showFailuresLink]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13422-trunk?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/12]|[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/5/]|",07/Apr/17 12:59;krummas;+1,10/Apr/17 23:50;aweisberg;Committed as [c97514243e8c58bdda0ebf75212a8a217f3d017e|https://github.com/apache/cassandra/commit/f6f50129d72b149a62f7e26e081e4d43097f9236],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure at RemoveTest.testBadHostId,CASSANDRA-13407,13061357,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,04/Apr/17 07:27,12/Mar/19 14:19,13/Mar/19 22:34,20/Apr/17 09:54,,,,,,,,,,,0,,,,"Example trace:

{code}
java.lang.NullPointerException
	at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:881)
	at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:876)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:2201)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1855)
	at org.apache.cassandra.Util.createInitialRing(Util.java:216)
	at org.apache.cassandra.service.RemoveTest.setup(RemoveTest.java:89)
{code} 

[failure example|https://cassci.datastax.com/job/trunk_testall/1491/testReport/org.apache.cassandra.service/RemoveTest/testBadHostId/]
[history|https://cassci.datastax.com/job/trunk_testall/lastCompletedBuild/testReport/org.apache.cassandra.service/RemoveTest/testBadHostId/history/]
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-06 17:01:05.426,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 20 09:53:45 UTC 2017,,,,,,0|i3d67j:,9223372036854775807,,,,,,,,jkni,jkni,,,,,,,,,,"06/Apr/17 13:21;ifesdjeen;It seems that the problem was in the fact that we are staring {{Gossiper}}:

|[patch|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13407-trunk]|[utest|https://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13407-trunk-testall/]|

(utests were running ~25 times without this failure occuring, multiplexing a single test didn't help reproducing it with a patch either).","06/Apr/17 17:01;jkni;For posterity, this is the race possible when the Gossiper is started, as far as I can tell.

In setup, we initialize a fake ring using Util.createInitialRing. This will intialize the nodes in an unsafe manner and then inject the token states. If a status check runs before the tokens state is set, the previously decommissioned node will look like a fat client, since it won't have tokens and will not have a DEAD_STATE. Since we aren't gossiping, we won't have heard from it in greater than fatClientTimeout, so we'll remove it. If this races with the ss.onChange in createInitialRing, we can remove the endpointstate while processing it, which will cause a NPE as above.

We also need to remove SchemaLoader.loadSchema() as you did in the patch - this is because it starts the Gossiper as well. This is fine; we don't appear to need it.

The patch looks good - the race exists in theory on 2.1/2.2, but it appears to only manifest on 3.0+. I don't think it is worth committing to 2.1 for that reason - let's do 2.2+ forward and run the test at least once on each branch before committing.

","07/Apr/17 08:23;ifesdjeen;Looks like I was able to gather a bit more information on the issue. To confirm what you're saying. It is possible to reproduce locally by tweaking timeouts (particularly making the gossip interval shorter, to emulate the slow VM). 

{code}
INFO  [GossipTasks:1] 2017-04-03 23:05:53,433 Gossiper.java:810 - FatClient /127.0.0.4 has been silent for 1000ms, removing from gossip
DEBUG [GossipTasks:1] 2017-04-03 23:05:53,436 Gossiper.java:432 - removing endpoint /127.0.0.4
DEBUG [GossipTasks:1] 2017-04-03 23:05:53,436 Gossiper.java:407 - evicting /127.0.0.4 from gossip
{code}

After that we can get an NPE either in {{Gossiper#getHostId}} or {{StorageService#isStatus}}. 

The patch for 2.0 and 3.0 is slightly different, as if we do not initialise schema, we'll get the following error: 

{code}
    [junit] junit.framework.AssertionFailedError: []
    [junit]     at org.apache.cassandra.db.lifecycle.Tracker.getMemtableFor(Tracker.java:312)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1185)
    [junit]     at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:573)
    [junit]     at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:421)
    [junit]     at org.apache.cassandra.db.Mutation.apply(Mutation.java:210)
    [junit]     at org.apache.cassandra.db.Mutation.apply(Mutation.java:215)
    [junit]     at org.apache.cassandra.db.Mutation.apply(Mutation.java:224)
    [junit]     at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternalWithoutCondition(ModificationStatement.java:566)
    [junit]     at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:556)
    [junit]     at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:295)
    [junit]     at org.apache.cassandra.db.SystemKeyspace.updatePeerInfo(SystemKeyspace.java:712)
    [junit]     at org.apache.cassandra.service.StorageService.updatePeerInfo(StorageService.java:1801)
    [junit]     at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:2014)
    [junit]     at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1669)
    [junit]     at org.apache.cassandra.Util.createInitialRing(Util.java:213)
    [junit]     at org.apache.cassandra.service.RemoveTest.setup(RemoveTest.java:77)
{code}

|[2.2|https://github.com/apache/cassandra/compare/2.2...ifesdjeen:13407-2.2]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13407-2.2-testall/]|
|[3.0|https://github.com/apache/cassandra/compare/3.0...ifesdjeen:13407-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13407-3.0-testall/]|
|[3.11|https://github.com/apache/cassandra/compare/3.11...ifesdjeen:13407-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13407-3.11-testall/]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13407-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13407-trunk-testall/]|","10/Apr/17 17:27;jkni;Patches look good - good catch on 2.2/3.0 difference.

+1.","20/Apr/17 09:53;ifesdjeen;Committed to 2.2 as [31590f5da10de8bbcf36d19617ced02b37be2a57|https://github.com/apache/cassandra/commit/31590f5da10de8bbcf36d19617ced02b37be2a57] and merged up to [3.0|https://github.com/apache/cassandra/commit/b063b38f33474cd0687167599a3a8cec7ed82631], [3.11|https://github.com/apache/cassandra/commit/638df6f971b806460113031524e14b21ae3e20f8] and [trunk|https://github.com/apache/cassandra/commit/90e50789680c8d386a57884a0d58ee62e5f4e73d].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible NPE on upgrade to 3.0/3.X in case of IO errors,CASSANDRA-13389,13059899,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,Stefania,Stefania,Stefania,29/Mar/17 01:53,12/Mar/19 14:19,13/Mar/19 22:34,30/Mar/17 01:16,3.0.13,3.11.0,,,,Local/Startup and Shutdown,,,,,0,,,,"There is a NPE on upgrade to 3.0/3.X if a data directory contains directories that generate IO errors, for example if the cassandra process does not have permission to read them.

Here is the exception:

{code}
ERROR [main] 2017-03-06 16:41:30,678  CassandraDaemon.java:710 - Exception encountered during startup
java.lang.NullPointerException: null
	at org.apache.cassandra.io.util.FileUtils.delete(FileUtils.java:372) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
	at org.apache.cassandra.db.SystemKeyspace.migrateDataDirs(SystemKeyspace.java:1359) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:190) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
{code}

This is caused by {{File.listFiles()}}, which returns null in case of an IO error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-29 11:38:38.793,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 30 01:16:05 UTC 2017,,,,,,0|i3cx7j:,9223372036854775807,,,,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,"29/Mar/17 02:41;Stefania;The patch replaces {{File.listFiles()}} with directory streams. The 3.0 patch applies cleanly to 3.11. We don't require any patch for trunk since the upgrade code is no longer applicable for 4.0. CI pending.

||3.0||3.11||
|[patch|https://github.com/stef1927/cassandra/tree/13389-3.0]|[patch|https://github.com/stef1927/cassandra/tree/13389-3.11]|
|[testall|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13389-3.0-testall/]|[testall|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13389-3.11-testall/]|
|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13389-3.0-dtest/]|[dtest|http://cassci.datastax.com/view/Dev/view/stef1927/job/stef1927-13389-3.11-dtest/]|
","29/Mar/17 11:38;ifesdjeen;LGTM, +1

Thank you for the patch!
For history purposes, I'll note that this patch isn't applicable to trunk as it's related to legacy table migration.","30/Mar/17 01:16;Stefania;Thanks for the review. Committed to 3.0 as 849f8cd6162c4850d64581a2c4a542c677e43e0a and merged into 3.11, then merged into trunk with {{-s ours}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expired rows without regular column data can crash upgradesstables,CASSANDRA-13395,13060674,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,blerer,blerer,blerer,31/Mar/17 13:30,12/Mar/19 14:19,13/Mar/19 22:34,04/Apr/17 10:28,3.0.13,3.11.0,,,,Legacy/Local Write-Read Paths,,,,,0,,,,"In {{2.x}} if an expired row is compacted its row marker will be converted into a {{DeletedCell}}. In {{3.0}}, when the row is read by {{LegacyLayout}} it will be converted in a row without {{PrimaryKeyLivenessInfo}}. If the row does not contains any data for the regular columns, or if the table simply has no regular columns it will then be considered as {{empty}}. Which will crash {{upgradesstables}} with the following error:
{code}
java.lang.AssertionError
        at org.apache.cassandra.db.rows.Rows.collectStats(Rows.java:70)
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter$StatsCollector.applyToRow(BigTableWriter.java:207)
        at org.apache.cassandra.db.transform.BaseRows.applyOne(BaseRows.java:116)
        at org.apache.cassandra.db.transform.BaseRows.add(BaseRows.java:107)
        at org.apache.cassandra.db.transform.UnfilteredRows.add(UnfilteredRows.java:41)
        at org.apache.cassandra.db.transform.Transformation.add(Transformation.java:156)
        at org.apache.cassandra.db.transform.Transformation.apply(Transformation.java:122)
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:147)
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:125)
        at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:57)
        at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:109)
        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:195)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
        at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:416)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:308)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$0(NamedThreadFactory.java:79)
        at java.lang.Thread.run(Thread.java:745)
{code}
This problem is cause",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-03 08:43:46.351,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 04 10:28:05 UTC 2017,,,,,,0|i3d1zr:,9223372036854775807,3.0.12,,,,,,,slebresne,slebresne,,,,,,,,,,"31/Mar/17 13:56;blerer;I discussed offline with [~slebresne] and the internal iterators do not accept empty rows for performance reasons.
As we know that except for indexes the deleted cells are caused by the compaction of expired row marker we can avoid the empty row problem by treating those rows as the expired ones. The only information missing being the original TTL we can simply replace that one by a fake one.

I pushed an initial version of the patch [here|https://github.com/apache/cassandra/compare/trunk...blerer:13395-3.0].  ","31/Mar/17 14:08;blerer;Waiting for the upgrade tests.
","03/Apr/17 08:31;blerer;The CI results for the upgrade tests: |[3.0|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13395-upgrade-3.0-upgrade/]|[3.11|http://cassci.datastax.com/view/Dev/view/blerer/job/blerer-13395-upgrade-3.11-upgrade/]|. 
I checked the failing tests and they are the same as the ones on 3.0 and 3.11 HEADs.",03/Apr/17 08:43;slebresne;+1,04/Apr/17 10:28;blerer;Committed into 3.0 at 462b9cf63bf986671f8a080ef1802f0c27e7c772 and merged into 3.X.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run more test targets on CircleCI,CASSANDRA-13413,13061719,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,05/Apr/17 12:33,12/Mar/19 14:19,13/Mar/19 22:34,07/Apr/17 06:15,2.1.18,2.2.10,3.0.13,3.11.0,4.0,,,,,,0,,,,"Currently we only run {{ant test}} on circleci, we should use all the (free) containers we have and run more targets in parallel.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13414,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-05 12:47:37.539,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 17 18:54:50 UTC 2017,,,,,,0|i3d8fz:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"05/Apr/17 12:36;krummas;https://github.com/krummas/cassandra/commits/marcuse/testcircle

It runs {{ant eclipse-warnings; ant test}} if we only have one container configured, if we have 4 it also runs {{ant long-test}}, {{ant test-compression}} and {{ant stress-test}} on the different containers

https://circleci.com/gh/krummas/cassandra/5 (not finished when posting this, so might be broken)","05/Apr/17 12:47;jasobrown;this is awesome! +1.

Maybe add 'microbench' target somewhere? Not sure how long it takes to execute, though...","05/Apr/17 13:03;krummas;bq. Maybe add 'microbench' target somewhere?
would be nice, but not sure the place is in CI - we would need jmh to output some sort of junit xml test report, and fail builds based on the benchmark times I guess? And not sure how predictable the performance is in CircleCI if it actually gives us something (other than we make sure that the benchmarks can actually build/execute)","05/Apr/17 13:13;jasobrown;bq. not sure how predictable the performance is

Yeah, you are correct about that, and that's the best reason for not including it. I'm still +1 on the current patch on your branch","05/Apr/17 13:16;krummas;my test failed quite badly: https://circleci.com/gh/krummas/cassandra/5

rerunning","05/Apr/17 15:59;jjirsa;Build #6 looks better, but it's somehow not gathering the junit result properly 

{quote}
Your build ran 2980 tests in junit with 0 failures
Slowest test: org.apache.cassandra.cql3.PagingQueryTest pagingOnRegularColumn (took 105.24 seconds).
{quote}

{quote}
    [junit] Testsuite: org.apache.cassandra.net.MessagingServiceTest-compression
    [junit] Testsuite: org.apache.cassandra.net.MessagingServiceTest-compression Tests run: 12, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.835 sec
    [junit] 
    [junit] Testcase: testDroppedMessages(org.apache.cassandra.net.MessagingServiceTest)-compression:	FAILED
    [junit] expected:<...dropped latency: 227[8] ms> but was:<...dropped latency: 227[7] ms>
    [junit] junit.framework.AssertionFailedError: expected:<...dropped latency: 227[8] ms> but was:<...dropped latency: 227[7] ms>
    [junit] 	at org.apache.cassandra.net.MessagingServiceTest.testDroppedMessages(MessagingServiceTest.java:114)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.net.MessagingServiceTest FAILED
{quote}

Passed on 3 of the 4 containers/suites, though, so it's pretty close. {{test-compression}} puts results into {{build/test/output/compression/}} so you'll need to copy those into the results directory in the post step.","05/Apr/17 17:06;mshuler;I was working on multiple runners and fixing a broken test in CASSANDRA-13414. It looks like we OOM the container with more than one runner, so since y'all are here, can you include this one-liner to fix o.a.c.utils.BitSetTest.compareBitSets?
{code}
diff --git a/circle.yml b/circle.yml
index f0df31265a..66e752f367 100644
--- a/circle.yml
+++ b/circle.yml
@@ -8,6 +8,7 @@ test:
     - sudo ifconfig lo:2 127.0.0.3 up
     - sudo ifconfig lo:3 127.0.0.4 up
     - sudo ifconfig lo:4 127.0.0.5 up
+    - sudo apt-get update; sudo apt-get install wamerican
     - ant build
 
   override:
{code}

Since you're removing the pre: section, I'm not sure how to include that in the parallel bits, yet. Thanks!","05/Apr/17 17:08;krummas;sure, will include!","05/Apr/17 17:12;krummas;running a new test with wamerican installed and it also collects the compression xmls, thanks guys: https://circleci.com/gh/krummas/cassandra/8","06/Apr/17 17:14;krummas;ok, finally got it working: https://circleci.com/gh/krummas/cassandra/16","06/Apr/17 17:35;mshuler;{{ant test-cdc}} writes its result xml files to {{build/test/output/cdc}} and `ant test-all` writes to multiple for each dep target. Jenkins allows ant-style globbing, so we use {{**/TEST-*.xml}} everywhere to just grab everything. I'd bet something like a blanket {{find ./build/test/output -type f -name *.xml}} and cp to $CIRCLE_TEST_REPORTS/junit/ - sorry, I think that will overwrite same-named files, so maybe {{find ./build/test/output -type d}} for finding all the directories under there and then doing the recursive copy of each?

Sorry I don't have the bandwidth to test all the possibles, but just spitballing something to cover all the possible targets a little better than a single conditional.","06/Apr/17 17:38;krummas;My patch essentially runs test-all:
{{<target name=""test-all"" depends=""eclipse-warnings,test,long-test,test-compression,stress-test"" ...}}
and this patch runs those different targets in the 4 different containers.

Had no idea test-cdc existed, is that being run on cassci?","06/Apr/17 17:42;mshuler;test-cdc is running on the ASF Jenkins branches that contain it, so cassandra-3.11 and trunk - https://builds.apache.org/view/A-D/view/Cassandra/","06/Apr/17 17:48;krummas;updated patch to copy all xml files in output directory, running now","06/Apr/17 17:49;krummas;and it wont overwrite the files, the commands are being run on each of the containers","06/Apr/17 19:15;jjirsa;lgtm

",06/Apr/17 19:29;mshuler;ship it! :),"06/Apr/17 21:55;aweisberg;These seems less useful for me in day to day testing where I would rather optimize for response time vs. daily test runs that are more thorough.

If I need to build several branches at once I would rather use my containers for that or better yet for running the same branch faster.

For me this would be a step backwards.",07/Apr/17 04:47;krummas;[~aweisberg] just leave the parallelism at 1 and it should works just like before,"07/Apr/17 06:15;krummas;committed, thanks",07/Apr/17 16:10;aweisberg;Where do I set the parallelism to 1? This is still going to take a 1.5 hour test run and turn it into 4 hours of testing that needs to run through my pool of 4 containers.,07/Apr/17 16:11;aweisberg;Found out where you adjust parallelism https://circleci.com/gh/[username]/cassandra/edit#parallel-builds,"07/Apr/17 16:20;krummas;bq. This is still going to take a 1.5 hour test run and turn it into 4 hours of testing
what do you mean?

with parallelism=1 circleci will only run 'case 0' here: https://github.com/apache/cassandra/blob/trunk/circle.yml#L10",07/Apr/17 16:24;aweisberg;Ah great. I thought it was going to run them serially.,"17/Aug/17 18:54;eduard.tudenhoefner;It looks like the *stress-test* target only got introduced with CASSANDRA-11638 (3.10) and so *stress-test* isn't valid in all previous versions. [~krummas] I created CASSANDRA-13775, could you review it please?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Schema version changes for each upgraded node in a rolling upgrade, causing migration storms",CASSANDRA-13441,13063579,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,12/Apr/17 18:23,12/Mar/19 14:19,13/Mar/19 22:34,24/Apr/17 16:13,3.0.14,3.11.0,4.0,,,Legacy/Distributed Metadata,,,,,0,,,,"In versions < 3.0, during a rolling upgrade (say 2.0 -> 2.1), the first node to upgrade to 2.1 would add the new tables, setting the new 2.1 version ID, and subsequently upgraded hosts would settle on that version.

When a 3.0 node upgrades and writes its own new-in-3.0 system tables, it'll write the same tables that exist in the schema with brand new timestamps. As written, this will cause all nodes in the cluster to change schema (to the version with the newest timestamp). On a sufficiently large cluster with a non-trivial schema, this could cause (literally) millions of migration tasks to needlessly bounce across the cluster.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13812,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-12 22:09:08.112,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 18 14:11:54 UTC 2017,,,,,,0|i3djwf:,9223372036854775807,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"12/Apr/17 22:09;michaelsembwever;I suspect we'll see a number of people doing 2.1.x and 2.2.x upgrades to 3.11.x (especially the bigger clusters after a few patch releases on 3.11), long before we see many upgrading to 4.0.x.

Why not slate this for 3.11.x ?","12/Apr/17 22:55;jjirsa;The risk is that 3.11 is wire-compatible with 3.0, which means a 3.0 -> 3.11 upgrade would be ugly with this patch (constantly bouncing schemas back and forth between the 3.0 and 3.11 versions).
","14/Apr/17 17:22;jjirsa;Patch for trunk at: https://github.com/jeffjirsa/cassandra/tree/cassandra-13441
CircleCI running unit tests: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13441
Dtest will run from here: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/13/ (may be quite some time until dtests run, maybe available by Monday)

","18/Apr/17 02:19;michaelsembwever;{quote}Patch for trunk at: https://github.com/jeffjirsa/cassandra/tree/cassandra-13441{quote}

Did you push the branch [~jjirsa] ? (i can't see any such branch in your fork)","18/Apr/17 02:59;jjirsa;Aleksey mentioned he had some concerns with the approach in IRC and, more importantly, suggested a cleaner way to do it (rather than change the hashing, write new system tables with fixed time stamps). 

Using this method, we're able to actually push this to 3.0, 3.11, and trunk without causing the ping-pong effect the original patch caused, and it's much cleaner (doesn't go all the way down into the storage engine). 

New patches here:

|| Branch || Testall || Dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/commits/cassandra-3.0-13441] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13441] | [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/16/] |
| [3.11|https://github.com/jeffjirsa/cassandra/commits/cassandra-3.11-13441] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13441] | [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/17/] |
| [trunk|https://github.com/jeffjirsa/cassandra/commits/cassandra-13441] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13441] | [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/18/] |


(Unit and dtests are queued now but will likely take some time to complete)","18/Apr/17 14:30;iamaleksey;The patches look good to me. +1

P.S. Thought initially that you overlooked specifying a 0 timestamp for keyspace metadata itself (replication params and durable_writes), but apparently there, in {{MigrationManager.maybeAddKeyspace()}} we already do correctly set 0 timestamp. Makes me wonder how we forgot to do the same for tables back then.","24/Apr/17 16:13;jjirsa;Thanks for the review Aleksey.

Committed as {{032a8cc1e9424c718a78e1463530d62e2e310d4a}} ","11/May/17 15:12;juliuszaromskis;Hi, any workaround for this issue? I've hit this after upgrading from 3.0.9 to 3.0.13 and doing sstableupgrade. Noticed weird disk write patterns and started seeing migration tasks bouncing around. I've only managed to update first of the 3 nodes. Migration tasks have stopped after I've rebooted first node.

{noformat}
Cluster Information:
        Name: cloud.zaromskis.lt cluster
        Snitch: org.apache.cassandra.locator.DynamicEndpointSnitch
        Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
        Schema versions:
                600b7268-d42a-3b72-8706-093b6c8cfaff: [10.240.0.6]
                77a40699-8e9e-35aa-834e-68c32e40a45a: [10.240.0.7, 10.240.0.8]
{noformat}

{noformat}
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address     Load       Tokens       Owns (effective)  Host ID                               Rack
UN  10.240.0.6  284.95 GB  256          63.4%             d0d83d9d-0dec-45cd-9ca9-93515fa131f3  rack1
UN  10.240.0.7  288.53 GB  256          64.1%             6d9709a0-0e10-46a1-9afa-d106b74ca9e0  rack1
UN  10.240.0.8  326.31 GB  256          72.5%             5c969700-8bd9-49a4-9772-1284439f8364  rack1
{noformat}

The migrations are in fact executed, as I can see that on the disk, new files are created every second in system keyspace. Why doesn't cluster settle on the same schema version then?

The schema version of first node would not propagate to other nodes. I'm afraid further upgrades might create new schema versions? I can't afford to lose any data. Any advise?

","11/May/17 16:02;jjirsa;Hi [~juliuszaromskis] - if you're upgrading from 3.0.9 to 3.0.13, it's unlikely that this is your issue (this would mostly impact people going from 2.1 -> 3.0, or 2.2 -> 3.0. Unless you're very confident that the schema version on {{10.240.0.6}} is different and more desirable than that on the other two nodes, the most likely solution is to issue a {{nodetool resetlocalschema}} on 10.240.0.6, allowing it to re-pull its schema from .7 and .8.

","11/May/17 16:35;juliuszaromskis;Here's a apiece of debug log, maybe this will help to identify if it's the same issue or not. 

{noformat}
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:05,896 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22120-big-Data.db (0.098KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8644066)
DEBUG [InternalResponseStage:589] 2017-05-11 16:04:05,941 MigrationManager.java:556 - Gossiping my schema version 77a40699-8e9e-35aa-834e-68c32e40a45a
DEBUG [InternalResponseStage:588] 2017-05-11 16:04:05,944 ColumnFamilyStore.java:850 - Enqueuing flush of keyspaces: 2079 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:05,944 Memtable.java:368 - Writing Memtable-keyspaces@1326973542(0.582KiB serialized bytes, 4 ops, 0%/0% of on/off-heap limit)
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:05,951 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22200-big-Data.db (0.489KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8685297)
DEBUG [InternalResponseStage:588] 2017-05-11 16:04:05,971 ColumnFamilyStore.java:850 - Enqueuing flush of tables: 65895 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:05,972 Memtable.java:368 - Writing Memtable-tables@512792876(20.714KiB serialized bytes, 31 ops, 0%/0% of on/off-heap limit)
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:05,980 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/mc-22197-big-Data.db (13.019KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8700606)
DEBUG [InternalResponseStage:588] 2017-05-11 16:04:06,005 ColumnFamilyStore.java:850 - Enqueuing flush of columns: 204664 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:06,006 Memtable.java:368 - Writing Memtable-columns@643217662(43.015KiB serialized bytes, 286 ops, 0%/0% of on/off-heap limit)
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:06,018 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22195-big-Data.db (20.975KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8707212)
DEBUG [CompactionExecutor:47263] 2017-05-11 16:04:06,055 CompactionTask.java:146 - Compacting (75968370-3663-11e7-ab7b-d7e32ecfc62d) [/mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22193-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22194-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22192-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22195-big-Data.db:level=0, ]
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in dropped_columns
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in triggers
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in views
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in types
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in functions
DEBUG [MemtablePostFlush:95264] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:903 - forceFlush requested but everything is clean in aggregates
DEBUG [InternalResponseStage:588] 2017-05-11 16:04:06,057 ColumnFamilyStore.java:850 - Enqueuing flush of indexes: 598 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:06,057 Memtable.java:368 - Writing Memtable-indexes@1653592334(0.127KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:06,068 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22121-big-Data.db (0.098KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8742156)
DEBUG [CompactionExecutor:47266] 2017-05-11 16:04:06,090 CompactionTask.java:146 - Compacting (759bdaa0-3663-11e7-ab7b-d7e32ecfc62d) [/mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22120-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22121-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22119-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22118-big-Data.db:level=0, ]
DEBUG [InternalResponseStage:588] 2017-05-11 16:04:06,112 MigrationManager.java:556 - Gossiping my schema version 77a40699-8e9e-35aa-834e-68c32e40a45a
DEBUG [CompactionExecutor:47263] 2017-05-11 16:04:06,120 CompactionTask.java:218 - Compacted (75968370-3663-11e7-ab7b-d7e32ecfc62d) 4 sstables to [/mnt/storage/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-22196-big,] to level=0.  29,348 bytes to 10,277 (~35% of original) in 64ms = 0.153139MB/s.  0 total partitions merged to 6.  Partition merge counts were {1:2, 4:4, }
DEBUG [CompactionExecutor:47266] 2017-05-11 16:04:06,136 CompactionTask.java:218 - Compacted (759bdaa0-3663-11e7-ab7b-d7e32ecfc62d) 4 sstables to [/mnt/storage/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/mc-22122-big,] to level=0.  392 bytes to 98 (~25% of original) in 45ms = 0.002077MB/s.  0 total partitions merged to 1.  Partition merge counts were {4:1, }
DEBUG [NonPeriodicTasks:1] 2017-05-11 16:04:06,782 MigrationManager.java:124 - submitting migration task for /10.240.0.6
DEBUG [NonPeriodicTasks:1] 2017-05-11 16:04:06,782 MigrationManager.java:124 - submitting migration task for /10.240.0.7
DEBUG [InternalResponseStage:586] 2017-05-11 16:04:06,792 ColumnFamilyStore.java:850 - Enqueuing flush of keyspaces: 2079 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:06,793 Memtable.java:368 - Writing Memtable-keyspaces@847538504(0.582KiB serialized bytes, 4 ops, 0%/0% of on/off-heap limit)
DEBUG [MemtableFlushWriter:107524] 2017-05-11 16:04:06,799 Memtable.java:401 - Completed flushing /mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22201-big-Data.db (0.489KiB) for commitlog position ReplayPosition(segmentId=1483955071260, position=8939092)
DEBUG [CompactionExecutor:47266] 2017-05-11 16:04:06,816 CompactionTask.java:146 - Compacting (760aa200-3663-11e7-ab7b-d7e32ecfc62d) [/mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22201-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22198-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22199-big-Data.db:level=0, /mnt/storage/cassandra/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/mc-22200-big-Data.db:level=0, ]
DEBUG [InternalResponseStage:586] 2017-05-11 16:04:06,816 ColumnFamilyStore.java:850 - Enqueuing flush of tables: 65895 (0%) on-heap, 0 (0%) off-heap
DEBUG [MemtableFlushWriter:107525] 2017-05-11 16:04:06,817 Memtable.java:368 - Writing Memtable-tables@1477384931(20.714KiB serialized bytes, 31 ops, 0%/0% of on/off-heap limit)
{noformat}

This has continued for several hours until 've decided to continue with the migration to 3.0.13, here's the result:

{noformat}
Cluster Information:
        Name: cloud.zaromskis.lt cluster
        Snitch: org.apache.cassandra.locator.DynamicEndpointSnitch
        Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
        Schema versions:
                600b7268-d42a-3b72-8706-093b6c8cfaff: [10.240.0.6, 10.240.0.7, 10.240.0.8]
{noformat}

Looks like cluster is consistent now","07/Jul/17 06:07;xuzhongxing;I still see this when a new node is joining the cluster. The cassandra version are all 3.11.0

ERROR [main] 2017-07-07 13:56:29,873 MigrationManager.java:172 - Migration task failed to complete
","18/Sep/17 14:11;juliuszaromskis;This has indeed resolved an issue I was having. Using 3.0.14 now.

Note: if anyone else has problems adding new nodes or datacenter to the cluster, this resolves an issue with schema version mismatch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ViewBuilder can miss data due to sstable generation filter,CASSANDRA-13405,13061195,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,03/Apr/17 17:54,12/Mar/19 14:19,13/Mar/19 22:34,04/Apr/17 16:49,3.0.13,3.11.0,,,,Feature/Materialized Views,Legacy/Local Write-Read Paths,,,,0,materializedviews,,,"The view builder for one MV is restarted when other MVs are added on the same keyspace.  There is an issue if compactions are running between these restarts that can cause the view builder to skip data, since the builder tracks the max sstable generation to filter by when it starts back up.

I don't see a need for this generation tracking across restarts, it only needs to be tracked during a builders life (to avoid adding in newly compacted data).  

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-03 20:24:04.359,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 04 16:49:35 UTC 2017,,,,,,0|i3d57j:,9223372036854775807,,,,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,"03/Apr/17 18:24;tjake;Fix with test to repro.
Basically ignore the generation value...

I've also added some better debug logging to help operators and us see what's happening.

[3.0|https://github.com/tjake/cassandra/tree/13405-3.0]
[testall|http://cassci.datastax.com/view/Dev/view/tjake/job/tjake-13405-3.0-testall/]
[dtest|http://cassci.datastax.com/view/Dev/view/tjake/job/tjake-13405-3.0-dtest/]


[3.11|https://github.com/tjake/cassandra/tree/13405-3.11]
[testall|http://cassci.datastax.com/view/Dev/view/tjake/job/tjake-13405-3.11-testall/]
[dtest|http://cassci.datastax.com/view/Dev/view/tjake/job/tjake-13405-3.11-dtest/]","03/Apr/17 20:24;pauloricardomg;bq. The view builder for one MV is restarted when other MVs are added on the same keyspace.

Just curious, is there any reason for this? I don't see a reason for interrupting current view builds when a new view is added so perhaps we should improve this? Either here or in another ticket..

bq. I don't see a need for this generation tracking across restarts, it only needs to be tracked during a builders life (to avoid adding in newly compacted data). 

good catch, I think the original idea was to support resume after crash but this is not safe since compacted sstables can be garbage collected in-between view build restart - we could try to handle this by keeping the references between rebuild, but resuming by token range is probably good enough already and much simpler.

The patch looks good and new logs will be very helpful, commented a few minor nits directly on github code. +1 after nits are addressed and 3.11 dtest results look good. Thanks!",04/Apr/17 13:59;tjake;Nits addressed running CI again,04/Apr/17 16:49;tjake;Committed {{2f1ab4a4248ac24c890e195cd5714ca54510c19a}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible AssertionError in UnfilteredRowIteratorWithLowerBound,CASSANDRA-13366,13058245,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,22/Mar/17 14:34,12/Mar/19 14:19,13/Mar/19 22:34,23/Mar/17 09:30,3.11.0,,,,,,,,,,0,,,,"In the code introduced by CASSANDRA-8180, we build a lower bound for a partition (sometimes) based on the min clustering values of the stats file. We can't do that if the sstable has and range tombston marker and the code does check that this is the case, but unfortunately the check is done using the stats {{minLocalDeletionTime}} but that value isn't populated properly in pre-3.0. This means that if you upgrade from 2.1/2.2 to 3.4+, you may end up getting an exception like
{noformat}
WARN  [ReadStage-2] 2017-03-20 13:29:39,165  AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
java.lang.AssertionError: Lower bound [INCL_START_BOUND(Foo, -9223372036854775808, -9223372036854775808) ]is bigger than first returned value [Marker INCL_START_BOUND(Foo)@1490013810540999] for sstable /var/lib/cassandra/data/system/size_estimates-618f817b005f3678b8a453f3930b8e86/system-size_estimates-ka-1-Data.db
    at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.computeNext(UnfilteredRowIteratorWithLowerBound.java:122)
{noformat}
and this until the sstable is upgraded.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-23 03:23:17.466,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 27 15:34:45 UTC 2017,,,,,,0|i3cn0f:,9223372036854775807,,,,,,,,Stefania,Stefania,,,3.4,,,,,,,"22/Mar/17 14:49;slebresne;Attaching patch below:
| [13366-3.11|https://github.com/pcmanus/cassandra/commits/13366-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13366-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13366-3.11-dtest] |

Mostly, this just make sure we don't use pre-3.0 sstables for building the lower bound since it's unsafe. There was also a corner case with {{null}} in clusterings (which we only allow for compact tables for backward compatiblity and should be pretty rare) that wasn't handled so the patch adds that. And I added a bunch of comments as I felt this could be useful to future readers.

I'd like to try to write an upgrade dtest for this, but haven't taken the time yet. I'll update when that's the case, but the problem is simple enough that this probably shouldn't block review in the meantime ([~Stefania] assigning you since you wrote CASSANDRA-8180, but feel free to unassign if you don't have time).
","23/Mar/17 03:23;Stefania;Thanks for fixing this [~slebresne], it LGTM and the comments you've added are extremely useful.

CI results also look good.

Two typos, [here|https://github.com/pcmanus/cassandra/commit/f7fa6e97581e8e7eab739c584878bb1ea564f18a#commitcomment-21451015] and [here|https://github.com/pcmanus/cassandra/commit/f7fa6e97581e8e7eab739c584878bb1ea564f18a#commitcomment-21450989]. 

I also assume that [{{mayOverlapWith(}}|https://github.com/pcmanus/cassandra/commit/f7fa6e97581e8e7eab739c584878bb1ea564f18a#diff-894e091348f28001de5b7fe88e65733fL2016] was removed despite being public, because it is unreliable in the presence of range tombstones and compact tables, so I think it's justifiable.
","23/Mar/17 09:33;slebresne;Committed thanks (I'm still keeping the ""write a dtest"" on my TODO list, but it may took me a few days to get to it and I don't see the point delaying the commit given this is pretty simple one).

bq. because it is unreliable in the presence of range tombstones and compact tables

Correct, it was unused and unsafe, so felt safer to just get rid of it.","27/Mar/17 15:34;philipthompson;If the test is going to take more than a few days after the ticket, could we get a separate jira ticket, maybe a subtask for that? I would hate to see it be forgotten and then no test added.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnIdentifier object size wrong when tables are not flushed,CASSANDRA-13533,13072508,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,eduard.tudenhoefner,eduard.tudenhoefner,eduard.tudenhoefner,16/May/17 16:07,12/Mar/19 14:19,13/Mar/19 22:34,26/May/17 19:24,3.0.14,3.11.0,4.0,,,Legacy/Core,,,,,0,,,,"It turns out that the object size of {{ColumnIdentifier}} is wrong when *cassandra.test.flush_local_schema_changes: false*. This looks like stuff is being wrongly reused when no flush is happening.

We only noticed this because we were using the prepared stmt cache and noticed that prepared statements would account for *1-6mb* when *cassandra.test.flush_local_schema_changes: false*. With *cassandra.test.flush_local_schema_changes: true* (which is the default) those would be around *5000 bytes*.

Attached is a test that reproduces the problem and also a fix.

Also after talking to [~jkni] / [~blerer] we shouldn't probably take {{ColumnDefinition}} into account when measuring object sizes with {{MemoryMeter}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/May/17 23:46;eduard.tudenhoefner;columnidentifier.png;https://issues.apache.org/jira/secure/attachment/12868425/columnidentifier.png,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-05-16 22:48:38.384,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri May 26 19:24:34 UTC 2017,,,,,,0|i3f2d3:,9223372036854775807,3.0.13,,,,,,,jkni,jkni,,,3.0.0,,,,,,,"16/May/17 16:19;eduard.tudenhoefner;Branch: https://github.com/nastra/cassandra/tree/CASSANDRA-13533-30
Tests: https://circleci.com/gh/nastra/cassandra/tree/CASSANDRA-13533-30","16/May/17 22:48;jjirsa;{quote}
This looks like stuff is being wrongly reused when no flush is happening
{quote}

Obviously that flag is only for unit tests, but can you describe the symptom that led you to finding/fixing this?  



","16/May/17 23:59;eduard.tudenhoefner;[~jjirsa] we were using that flag in some of our integration tests where we found out that, _very rarely_, simple things (usually taking < 1 second) would take a long amount of time (> 30 seconds). 

We boiled it then down to the point where we saw that this time was 99% spent in the prepared stmt cache for adding/evicting elements. And in our case, eviction was happening because a single prepared stmt would account *1mb - 6mb* (prep stmt cache size was *8mb* for us). After looking closer why prep stmts would be so large, we eventually saw that the binary representation of {{ColumnIdentifier#text}} was be wrong and not what we would have expected (see attached screenshot).

We also only detected all of this because on *cassandra-3.0* we don't use *.omitSharedBufferOverhead()* with {{MemoryMeter}} object measurements ( [QueryProcessor.java#L67|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/cql3/QueryProcessor.java#L67]). 
On *cassandra-3.11* things got refactored over time and the prep stmt cache was using {{MemoryMeter}} stuff from {{ObjectSizes}}, where *.omitSharedBufferOverhead()* is being applied and the issue wouldn't happen ([ObjectSizes.java#L35|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/utils/ObjectSizes.java#L35]).

","24/May/17 20:49;jkni;The patch looks good. I merged this forward through 3.11 and trunk - testall and dtests look good for all branches relative to upstream.

I'm not sure about including the test as written; it passes on 3.11 and trunk even before the fix because of the default of offheap memtables introduced in [CASSANDRA-9472]. It seems to me that we might as well reduce this test to just checking that interning a ColumnIdentifier uses a minimal bytebuffer and add it to {{ColumnIdentifierTest}}.","24/May/17 21:52;jkni;As a concrete example, something like the following would suffice to show the interning issue on all active branches.

{code}
    @Test
    public void testInterningUsesMinimalByteBuffer()
    {
        byte[] bytes = new byte[2];
        bytes[0] = 0x63;
        ByteBuffer byteBuffer = ByteBuffer.wrap(bytes);
        byteBuffer.limit(1);

        ColumnIdentifier c1 = ColumnIdentifier.getInterned(byteBuffer, UTF8Type.instance);

        Assert.assertEquals(2, byteBuffer.capacity());
        Assert.assertEquals(1, c1.bytes.capacity());
    }
{code}

What do you think, [~eduard.tudenhoefner]?",24/May/17 23:02;eduard.tudenhoefner;that works for me [~jkni]. The test was only there to show that stuff was broken and anything we can do to shorten the circumstances for reproduction is a +1 from my side.,"26/May/17 19:24;jkni;+1 - thanks for the patch. I ran tests for all relevant branches; no unit tests failed on 3.0/3.11/trunk, and dtests looked the same as the present state of those branches.

Committed to 3.0 as {{8ffdd26cbee33c5dc1205c0f7292628e1a2c69e3}} and merged forward.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in snapshot_test.TestSnapshot.test_snapshot_and_restore_dropping_a_column,CASSANDRA-13483,13067693,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,mshuler,mshuler,28/Apr/17 13:45,12/Mar/19 14:19,13/Mar/19 22:34,03/May/17 10:36,,,,,,Legacy/Testing,,,,,0,dtest,test-failure,,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_offheap_dtest/488/testReport/snapshot_test/TestSnapshot/test_snapshot_and_restore_dropping_a_column

{code}
Error Message

Subprocess ['nodetool', '-h', 'localhost', '-p', '7100', ['refresh', 'ks', 'cf']] exited with non-zero status; exit status: 1; 
stdout: nodetool: Unknown keyspace/cf pair (ks.cf)
See 'nodetool help' or 'nodetool help <command>'.
{code}
{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/snapshot_test.py"", line 145, in test_snapshot_and_restore_dropping_a_column
    node1.nodetool('refresh ks cf')
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 789, in nodetool
    return handle_external_tool_process(p, ['nodetool', '-h', 'localhost', '-p', str(self.jmx_port), cmd.split()])
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 2002, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13495,,,,,,,,,28/Apr/17 13:45;mshuler;node1.log;https://issues.apache.org/jira/secure/attachment/12865549/node1.log,28/Apr/17 13:45;mshuler;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12865550/node1_debug.log,28/Apr/17 13:45;mshuler;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12865548/node1_gc.log,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-04-30 16:30:57.863,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue May 02 00:06:44 UTC 2017,,,,,,0|i3e8of:,9223372036854775807,2.2.x,,,,,,,philipthompson,philipthompson,,,,,,,,,,"30/Apr/17 16:30;jasonstack;This test is added in [CASSANDRA-13276|https://issues.apache.org/jira/browse/CASSANDRA-13276] in 3.11 depending on schema.cql feature added in [CASSANDRA-7190|https://issues.apache.org/jira/browse/CASSANDRA-7190] in 3.10.

'@since('3.11)' should fix the failed test. 

Here is the [dtest|https://github.com/riptano/cassandra-dtest/pull/1466]

[~ifesdjeen] could you review it? thanks",02/May/17 00:06;jasonstack;dtest merged,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cqlsh throws and error when querying a duration data type,CASSANDRA-13549,13074352,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,akhilm,akhilm,akhilm,23/May/17 23:00,12/Mar/19 14:19,13/Mar/19 22:34,29/May/17 15:46,3.11.0,4.0,,,,Legacy/CQL,,,,,0,,,,"h3. Overview

Querying duration related data from the cqlsh prompt results in an error.

Consider the following create table and insert statement.
{code:title=Table and insert statement with duration data type|borderStyle=solid}
CREATE TABLE duration_test (
  primary_key text,
  col20 duration,
  PRIMARY KEY (primary_key)
);
INSERT INTO duration_test (primary_key, col20) VALUES ('primary_key_example', 1y5mo89h4m48s);
{code}

On executing a select query on col20 in cqlsh I get an error ""Failed to format value '""\x00\xfe\x02GS\xfc\xa5\xc0\x00' : 'ascii' codec can't decode byte 0xfe in position 2: ordinal not in range(128)""
{code:title=Duration Query|borderStyle=solid}
Select  col20 from duration_test;
{code}

h3. Investigation

On investigating this further I found that the current python Cassandra driver used found in lib/cassandra-driver-internal-only-3.7.0.post0-2481531.zip does not seem to support duration data type. This was added in Jan this year https://github.com/datastax/python-driver/pull/689.

So I downloaded the latest driver release https://github.com/datastax/python-driver/releases/tag/3.9.0. I embedded the latest driver into cassandra-driver-internal-only-3.7.0.post0-2481531.zip. This fixed the driver related issue but there was still a formatting issue. 

I then went on to modify the format_value_duration methos in the pylib/cqlshlib/formatting.py. Diff posted below

{code}
 @formatter_for('Duration')
 def format_value_duration(val, colormap, **_):
-    buf = six.iterbytes(val)
-    months = decode_vint(buf)
-    days = decode_vint(buf)
-    nanoseconds = decode_vint(buf)
-    return format_python_formatted_type(duration_as_str(months, days, nanoseconds), colormap, 'duration')
+    return format_python_formatted_type(duration_as_str(val.months, val.days, val.nanoseconds), colormap, 'duration')
{code}

This resulted in fixing the issue and duration types are now correctly displayed.

Happy to fix the issue if I can get some guidance on:
# If this is a valid issue. Tried searching JIRA but did not find anything reported. 
# If my assumptions are correct i.e. this is actually a bug
# how to package the new driver into the source code. 







",Cassandra 3.10 dev environment running on a MacOS Sierra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-05-24 07:47:08.618,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon May 29 15:46:08 UTC 2017,,,,,,0|i3fdpj:,9223372036854775807,3.10,,,,,,,blerer,blerer,,,,,,,,,,"24/May/17 07:47;blerer;I need to have a look into it because, it should have worked with the {{3.10}} internal driver even if it was not supporting natively durations. ","24/May/17 13:39;blerer;I am not fully sure of what changed at the driver level since CASSANDRA-11873 was committed.

With the current internal driver ({{cassandra-driver-internal-only-3.7.0.post0-2481531.zip}}) the CQL type being returned is a custom type.
Which makes sense as it is the type returned by Cassandra for {{Duration}} when the protocol version is {{4}}.
Due to that the proper formatter is not found correctly and the {{ascii}} one is used.
With the latest driver. Even if C* return a custom type the driver map it to a {{duration}} and return a {{Duration}} object that cannot be processed by the current formatter.

The proper fix is in my opinion to provide 2 formatters in the 3.11 version one for the custom type and one for the duration type.

I have pushed a patch for it [here|https://github.com/apache/cassandra/compare/trunk...blerer:13549-3.11].

[~aholmber] does my approach make sense to you?


","24/May/17 17:57;aholmber;Historically we would only have one formatter because it is assumed that the combination of bundled driver, and protocol version in use is fixed for a given version of cqlsh. I could see making the case for two if you don't want to upgrade now, but want to be defensive about changes in driver version, or possibly to maintain parity in `cqlshlib.formatting` across branches. I don't know of anything that would preclude updating the bundled driver at this point.

I'm still a little perplexed by what changed if there was no driver upgrade, but I haven't gone looking.

To answer the earlier question:
bq. how to package the new driver into the source code.
See https://wiki.apache.org/cassandra/HowToContribute#Bundled_Drivers","24/May/17 20:11;blerer;bq. I'm still a little perplexed by what changed if there was no driver upgrade, but I haven't gone looking.

We updated the driver since  CASSANDRA-11873 but I am not convinced that the problem was coming from there. I suspect that it was caused by some change I did but I could not find which change was the guilty one :-(

My concern was that the {{duration}} is not required for the protocol {{V4}} but only for {{V5}}. Nevertheless, updating the driver is what make the most sense.

 [~akhilm] The fix that you suggested is the good one :-) If you want to finish the work that you started feel free to asign the ticket to yourself and put me as reviewer.","25/May/17 10:53;akhilm;[~blerer] Thanks for the feedback. Will complete the fix. I do not have permissions to assign the ticket to myself. Can you please assign the ticket to me or give me permission to assign the ticket to myself. Thanks

","26/May/17 01:18;akhilm;[~blerer] I just noticed that version 3.10.0 of the python driver was released yesterday (https://github.com/datastax/python-driver/releases/tag/3.10.0). Tested my changes against 3.10.0 and everything worked as expected.

Do you want me to stick with the 3.9.0 driver or switch to the 3.10.0 driver as it is the latest?

Thanks for your input. 

","26/May/17 02:23;akhilm;I have committed the changes to the following two branches.

[13549-trunk|https://github.com/amehra/cassandra/tree/13549-trunk]
[13549-3.11|https://github.com/amehra/cassandra/tree/13549-3.11]

Both branches have 3.10.0 driver in them and the required changes in formatting.py. If you want me to drop it back to 3.9.0 python driver please let me know. 

Is there any way I can run the dtests on these two branches on http://cassci.datastax.com/ ?

Thanks ","29/May/17 08:23;blerer;Thanks for the patches :-)

bq. If you want me to drop it back to 3.9.0 python driver please let me know. 

According to the driver [CHANGELOGS|https://github.com/datastax/python-driver/blob/master/CHANGELOG.rst] the support for the {{Duration}} type has been added to the {{3.10}} version so we should use that one. It also not make a lot of sense to build a pre-release of {{3.11}}.

bq. Is there any way I can run the dtests on these two branches on http://cassci.datastax.com/ ?

{{cassci}} will soon be shutdown so we should not run build there anymore. I will run the tests for the 2 branches on our internal CI.

  ","29/May/17 15:20;blerer;I ran the test for 3.11 and trunk. There are few failling CQLSH tests but they were failing before the patch so I am +1 for the patch.

Thanks for the work.",29/May/17 15:46;blerer;Committed into 3.11 at 5a860a70f28b7756e0073d2d5d239b5e748a0b73 and merged into trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE on non-existing row read when row cache is enabled,CASSANDRA-13482,13067657,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,28/Apr/17 10:52,12/Mar/19 14:19,13/Mar/19 22:34,12/Jul/17 13:55,3.0.15,3.11.1,4.0,,,,,,,,1,,,,"The problem is reproducible on 3.0 with:

{code}
-# row_cache_class_name: org.apache.cassandra.cache.OHCProvider
+row_cache_class_name: org.apache.cassandra.cache.OHCProvider

-row_cache_size_in_mb: 0
+row_cache_size_in_mb: 100
{code}

Table setup:

{code}
CREATE TABLE cache_tables (pk int, v1 int, v2 int, v3 int, primary key (pk, v1)) WITH CACHING = { 'keys': 'ALL', 'rows_per_partition': '1' } ;
{code}

No data is required, only a head query (or any pk/ck query but with full partitions cached). 

{code}
select * from cross_page_queries where pk = 10000 ;
{code}

{code}
java.lang.AssertionError: null
        at org.apache.cassandra.db.rows.UnfilteredRowIterators.concat(UnfilteredRowIterators.java:193) ~[main/:na]
        at org.apache.cassandra.db.SinglePartitionReadCommand.getThroughCache(SinglePartitionReadCommand.java:461) ~[main/:na]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:358) ~[main/:na]
        at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:395) ~[main/:na]
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1794) ~[main/:na]
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2472) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[main/:na]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-05-18 16:00:30.418,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 12 13:55:44 UTC 2017,,,,,,0|i3e8gf:,9223372036854775807,3.10,,,,,,,slebresne,slebresne,,,,,,,,,,"18/May/17 16:00;batenev;I have same error on cassandra 3.10:

query fail with row_cache on, and fine with cache off.
consistency level doesn't matter - one/quorum/all -> same result.
switch cache off and when on and/or restart cassandra doesn't fix problem.


java.lang.AssertionError: null
        at org.apache.cassandra.db.rows.UnfilteredRowIterators.concat(UnfilteredRowIterators.java:210) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.db.SinglePartitionReadCommand.getThroughCache(SinglePartitionReadCommand.java:474) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:374) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:407) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:48) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66) ~[apache-cassandra-3.10.jar:3.10]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.10.jar:3.10]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]


---

cassandra@cqlsh:sb> select * from session where agent_id = 846bed6c-978c-4cdb-958a-6a6155e9cdb5;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 2 failures"" info={'failures': 2, 'received_responses': 0, 'required_responses': 2, 'consistency': 'QUORUM'}

cassandra@cqlsh:sb> ALTER TABLE  session WITH caching = {'keys': 'ALL'};
cassandra@cqlsh:sb> select * from session where agent_id = 846bed6c-978c-4cdb-958a-6a6155e9cdb5;

 agent_id | session_id | all_isps | all_locations | all_logins | last_ip | last_login | last_page | last_time | legacy_id
----------+------------+----------+---------------+------------+---------+------------+-----------+-----------+-----------

(0 rows)

cassandra@cqlsh:sb> ALTER TABLE  session WITH compaction = {'class': 'LeveledCompactionStrategy'} and caching = {'keys': 'ALL','rows_per_partition': 1};
cassandra@cqlsh:sb> select * from session where agent_id = 846bed6c-978c-4cdb-958a-6a6155e9cdb5;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 2 failures"" info={'failures': 2, 'received_responses': 0, 'required_responses': 2, 'consistency': 'QUORUM'}

---

if i do select with another agent_id - all work fine! only one key fail...","09/Jun/17 09:23;ifesdjeen;I've composed a patch to mitigate the problem. 

In order to fix it, we have to allow for concatenating the iterators with different amounts of columns, although make sure that cases with wrapped iterators, limits, stopping and empty iterators all have some predictable behaviour. 

While working on this problem I have also discovered the slight inconsistency in the way concatenation and {{MoreRows}} is working right now: {{DataLimits}} filter [here|https://github.com/apache/cassandra/blob/a87b15d1d6c42f4247c84b460ed39899d8813a6f/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java#L423] will call {{stopInPartition}}, which would effectively ""stop"" the original {{iter}} iterator, and make it return {{hasNext() => false}}, even though only one row was consumed. 

Right now, it works only because internally {{concat}} would take {{input}} from the {{BaseIterator}} and discard this {{isStopped}} in [tryGetMoreContent|https://github.com/apache/cassandra/blob/81f6c784ce967fadb6ed7f58de1328e713eaf53c/src/java/org/apache/cassandra/db/transform/BaseIterator.java#L124]. In the context of this patch this would mean that we'd get only cached results and avoid reading mem/sstable. 

In other words, current behaviour can be described as:

{code}
    iter1 = /* some iterator yielding: 1, 2, 3 */;
    iter2 = /* some iterator yielding: 3, 4, 5, 6, 7, 8, 9 */;
    concatenated = concat(iter1, DataLimits.cqlLimits(3).filter(iter2));
{code}

would result in {{concatenated}} yielding 1 through 9, which is incorrect.

The patch implements the following changes:

  * {{concat}} [now allows|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13482-trunk#diff-57d0dfa95504bfd17d30539b3b338c0cL205] different amount of columns from iterators, but returned columns will be a union of the two iterators
  * {{BaseIterator}} [would now|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13482-trunk#diff-d14a4b314544d3720010343e330e7e3cR125] take the {{stop}} from the child iterator, which means that the iterator was stopped, even if it might have contents, it will not yield additional data. Unfortunately, since the iterator itself had a stopping transformation, and it's own {{stop}} reference is ""leaked"" during the creation of the transformation, we have to save (and check) both the original stop and the child one. Here, the naming might be a bit off.
  * {{cacheIterator}} [is now|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13482-trunk#diff-2e17efa5977a71330df6651d3bec0d12R424] using a custom wrapped iterator that will not call {{stop}} on the wrapping iterator, but previous problem with {{Unfiltered}} is now fixed

|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...ifesdjeen:13482-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-3.0-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-3.0-dtest/]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...ifesdjeen:13482-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-3.11-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-3.11-dtest/]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13482-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-trunk-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13482-trunk-dtest/]|","12/Jul/17 07:39;slebresne;lgtm, +1.","12/Jul/17 13:55;ifesdjeen;Thank you for the review,

Committed to 3.0 with [7251c9559805d83423ca5ddbe4f955ce668c3d9a|https://github.com/apache/cassandra/commit/7251c9559805d83423ca5ddbe4f955ce668c3d9a] and merged up to [3.11|https://github.com/apache/cassandra/commit/29db2511621e420b8d64c867a16e317589397d36] and [trunk|https://github.com/apache/cassandra/commit/f48a319ac884ef8d6eb54db3176ea2acf627bb89].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in rebuild_test.TestRebuild.disallow_rebuild_from_nonreplica_test,CASSANDRA-13583,13078100,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,michael.hamm,michael.hamm,07/Jun/17 20:06,12/Mar/19 14:19,13/Mar/19 22:34,17/Jul/17 16:59,4.0,,,,,,,,,,0,dtest,test-failure,,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/524/testReport/rebuild_test/TestRebuild/disallow_rebuild_from_nonreplica_test

{noformat}
Error Message

ToolError not raised
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: Python driver version in use: 3.10
dtest: DEBUG: cluster ccm directory: /tmp/dtest-0tUjhX
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 DC1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 DC1> discovered
--------------------- >> end captured logging << ---------------------
{noformat}


{noformat}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/tools/decorators.py"", line 48, in wrappedtestrebuild
    f(obj)
  File ""/home/automaton/cassandra-dtest/rebuild_test.py"", line 357, in disallow_rebuild_from_nonreplica_test
    node1.nodetool('rebuild -ks ks1 -ts (%s,%s] -s %s' % (node3_token, node1_token, node3_address))
  File ""/usr/lib/python2.7/unittest/case.py"", line 116, in __exit__
    ""{0} not raised"".format(exc_name))
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Jun/17 20:06;michael.hamm;node1.log;https://issues.apache.org/jira/secure/attachment/12871912/node1.log,07/Jun/17 20:06;michael.hamm;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12871914/node1_debug.log,07/Jun/17 20:06;michael.hamm;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12871909/node1_gc.log,07/Jun/17 20:06;michael.hamm;node2.log;https://issues.apache.org/jira/secure/attachment/12871911/node2.log,07/Jun/17 20:06;michael.hamm;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12871913/node2_debug.log,07/Jun/17 20:06;michael.hamm;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12871907/node2_gc.log,07/Jun/17 20:06;michael.hamm;node3.log;https://issues.apache.org/jira/secure/attachment/12871910/node3.log,07/Jun/17 20:06;michael.hamm;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12871915/node3_debug.log,07/Jun/17 20:06;michael.hamm;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12871908/node3_gc.log,,,,9.0,,,,,,,,,,,,,,,,,,,2017-06-19 18:40:24.948,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 17 16:59:23 UTC 2017,,,,,,0|i3g01z:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,19/Jun/17 18:40;jkni;This is consistently failing when run without vnodes after the commit of [CASSANDRA-4650].,"20/Jun/17 15:29;krummas;https://github.com/krummas/cassandra/commits/marcuse/13583

problem was that we ignored the sourceFound variable if localhost was in the map, but we need to still check that since localhost is filtered away when doing rebuild","05/Jul/17 21:46;aweisberg;So I don't quite have the context to understand this. For bootstrap we filter out localhost using an ISourceFilter for obvious reasons. But for rebuild we don't want to generate a source not found error if the localhost is the only source, but neither do we want to stream from it? I don't understand why we don't need a source to stream from for rebuild.","05/Jul/17 21:57;aweisberg;OK, I think I had it backwards. In the test case there is no source including localhost so we want the error and we previously didn't get it. But we also don't want to stream from localhost ever even if it counts as a source for some non-rebuild and non-bootstrap purpose.

Do I have it right?","06/Jul/17 06:33;krummas;bq. Do I have it right?
yes

running dtests here: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/118/",06/Jul/17 17:53;aweisberg;The dtest failures look unrelated so  1.,"17/Jul/17 16:59;krummas;committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnMetadata.cellValueType() doesn't return correct type for non-frozen collection,CASSANDRA-13573,13077216,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,ostefano,ostefano,05/Jun/17 10:38,12/Mar/19 14:19,13/Mar/19 22:34,08/Aug/17 14:58,3.0.x,3.11.x,4.x,,,Feature/Materialized Views,Legacy/Core,Legacy/CQL,Legacy/Tools,,0,,,,"Schema and data""
{noformat}
CREATE TABLE ks.cf (
    hash blob,
    report_id timeuuid,
    subject_ids frozen<set<int>>,
    PRIMARY KEY (hash, report_id)
) WITH CLUSTERING ORDER BY (report_id DESC);

INSERT INTO ks.cf (hash, report_id, subject_ids) VALUES (0x1213, now(), {1,2,4,5});
{noformat}

sstabledump output is:

{noformat}
sstabledump mc-1-big-Data.db 
[
  {
    ""partition"" : {
      ""key"" : [ ""1213"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 16,
        ""clustering"" : [ ""ec01eed0-49d9-11e7-b39a-97a96f529c02"" ],
        ""liveness_info"" : { ""tstamp"" : ""2017-06-05T10:29:57.434856Z"" },
        ""cells"" : [
          { ""name"" : ""subject_ids"", ""value"" : """" }
        ]
      }
    ]
  }
]
{noformat}

While the values are really there:

{noformat}
cqlsh:ks> select * from cf ;

 hash   | report_id                            | subject_ids
--------+--------------------------------------+-------------
 0x1213 | 02bafff0-49d9-11e7-b39a-97a96f529c02 |   {1, 2, 4}
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12628,CASSANDRA-12594,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-06 08:19:41.101,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 08 15:03:45 UTC 2017,,,,,,0|i3fvdr:,9223372036854775807,,,,,,,,adelapena,adelapena,,,3.0.9,,,,,,,"06/Jul/17 08:19;jasonstack;There are some issues in sstabledump

1. as you reported,  frozen collections

2. non-frozen UDT


{quote}
Exception in thread ""main"" java.lang.ClassCastException: org.apache.cassandra.db.marshal.UserType cannot be cast to org.apache.cassandra.db.marshal.CollectionType
	at org.apache.cassandra.tools.JsonTransformer.serializeCell(JsonTransformer.java:413)
	at org.apache.cassandra.tools.JsonTransformer.serializeColumnData(JsonTransformer.java:396)
	at org.apache.cassandra.tools.JsonTransformer.serializeRow(JsonTransformer.java:276)
	at org.apache.cassandra.tools.JsonTransformer.serializePartition(JsonTransformer.java:210)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
	at org.apache.cassandra.tools.JsonTransformer.toJson(JsonTransformer.java:100)
	at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:236)
{quote}","07/Jul/17 08:30;jasonstack;I am thinking to change all {{type.getString()}} in {{JsonTransformer}} to {{type.toJSONString}} when [13592|https://issues.apache.org/jira/browse/CASSANDRA-13592] merged.

Current {{collectionType.getString()}} only generates entire data as byte string. Imo,{{getString}} is not designed for generate json readable values.","07/Jul/17 14:10;cnlwsu;The json transformer uses jackson's JsonGenerator so you do not need things designed to be in json output. Switching to {{toJSONString}} would end up double escaped as the jackson generator will try to remove the json encoding from the JSONString output.

If I remember correctly pretty much all UDTs were not supported initially because the information to deserialize the UDT requires the system schema tables to be read, which may not be available unless running on the same system with exact configuration and requires client initialization (which is dangerous and we dont want to do). If we have adequate info to deserialize the UDTs from the stats metadata it probably just needs a different check to handle it.","09/Jul/17 11:23;jasonstack;I think we could use {{`writeRawValue(String)`}}  to avoid double escape.  Now that [13592|https://issues.apache.org/jira/browse/CASSANDRA-13592] is merged, we should have correct json representations for all types.. better than using {{type.toString}} which, imo, serves a different purposes.

bq.  all UDTs were not supported initially because the information to deserialize the UDT requires the system schema tables to be read (which is dangerous and we dont want to do)

If there is dependency to local schema tables, I agree not to support UDT and print raw-bytes instead.

But AFAIK after 8099, there should not be a dependency on schema tables while deserializing UDT.","09/Jul/17 15:36;jasonstack;Some issues are related to {{ColumnMetadata.cellValueType()}} which currently :  {{a}}. if CollectionType, returns value's type; {{b}} otherwise,  its own type.

It doesn't handle properly:  {{1}}. frozen collection type,  {{2}}. non-frozen udt which requires cellPath to retrieve value's type..  

There are 3 kind of usage for {{ColumnMetadata.cellValueType()}}:

1. to check if column is counter type, it's safe

2. used in MV to check if cell value (base's non key column in view's primary key) is changed. 
    in the existing implementation, it will get {{cellValueType}} of a {{frozen collection}} to decode {{frozen collection bytes}}
    it can easily result in runtime error. eg. base has non-key column {{frozen<list<tuple<text,text>>}}  as view's primary key. so  {{tuple<text, text> type}} is used to decode {{frozen<list>}}
 
 {{non-frozen-udt}} cannot be used as view's primary key. the issue here is only with {{frozen-collection}}.
 
3. in {{sasi}}.  haven't check how it is affected.  will check it later","10/Jul/17 02:59;jasonstack;first draft of the patch. if it looks good, I will prepare fixes for 2.2/3.0/3.11 as well
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13573] | [unit|https://circleci.com/gh/jasonstack/cassandra/130] | [dtest|https://github.com/jasonstack/cassandra-dtest-riptano/commits/CASSANDRA-13573] |

unit test: passed.
dtest: {{cqlsh_tests.cqlsh_tests.TestCqlsh.test_describe}} & {{bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test}} both are broken for some time

changes:
1. use {{type.toJSONString()}} with {{json.writeRawValue()}} instead of {{type.getString()}} to generate readable content 
2. {{column.cellValueType}} now :  {{a}}. if non-frozen collection, return value type, {{b}}. otherwise, return column type.
","11/Jul/17 02:12;jasonstack;About the impact on SASI,  now sasi doesn't support {{complex}} type (aka, type with cellPath, non-frozen collection or udt).  

By fixing {{column.cellValueTytpe}}, the {{columnIndex.isLiteral()}} is now properly returning {{false}} if indexed column is {{frozen-collection}}.","01/Aug/17 10:02;adelapena;The patch looks good to me, excellent job.

The assertion at {{ColumnMetadata#cellValueType()}} to check that the precondition mentioned in the comment is satisfied makes sense to me.

There are some minor nits about code style:
* There are some missed space-after-comma at {{ViewTest#testFrozenCollectionsWithComplicatedInnerType()}}. Also, it would be nice for the sake of uniformity to align the create table and create view statement as they are in similar tests in the same file.
* It would be good to add a {{@jira_ticket CASSANDRA-13573}} tag in the dtest docstring.
* This is just an idea, but I think that the dtest {{CREATE TABLE}} and insertion could be more readable using a cell per line, something like:
{code}
session.execute('CREATE TABLE ks.cf ('
                'key int PRIMARY KEY,'
                'list_f frozen<list<int>>,'
                'set_f frozen<set<int>>, '
                'map_f frozen<map<int,int>>,'
                'tuple_f frozen<tuple<int,int>>, '
                'user_type_f frozen<user_type>, '
                'list_v list<int>,'
                'set_v set<int>,'
                'map_v map<int,int>,'
                'tuple_v tuple<int,int>,'
                'user_type_v simple_type)')
...
session.execute(statement, [1,
                            [1, 2, 3],  # list_f
                            {1, 2, 3},  # set_f
                            {1: 1, 2: 2, 3: 3},  # map_f
                            (9, 9),  # map_f
                            FrozenUserType('SG', 100100, {'321', '123'}),  # user_type_f
                            [1, 2],  # list_v
                            {1, 2},  # set_v
                            {1: 1, 2: 2},  # map_v
                            (8, 8),  # tuple_v
                            NonFrozenUserType('SG', 100100)  # user_type_v
                            ])
{code}
What do you think?",01/Aug/17 13:42;cnlwsu;Can we make sure to test this with CASSANDRA-13683 ? The sstabledump tool was changed to be tool initiated for last few versions which can lead to UDTs mistakenly accessed from the C* system tables which are not available always. We do not want to add it as a requirement to have to run sstabledump on a C* node.,"02/Aug/17 05:13;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13573-trunk] | [unit|https://circleci.com/gh/jasonstack/cassandra/356 ] |irrelevant:
materialized_views_test.TestMatyerializedViews.view_tombstones_test 
bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test
cql_tests.cqlsh_tests.TestCqlsh.test_describe
 |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13573-3.11] | [unit|https://circleci.com/gh/jasonstack/cassandra/360] | passed |
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13573-3.0] | [unit|https://circleci.com/gh/jasonstack/cassandra/350]  | authe_test.TestAuth.system_auth_ks_is_alterable_test irrelevant | 
| [dtest|https://github.com/jasonstack/cassandra-dtest-riptano/commits/CASSANDRA-13573] |

Addressed comments in dtest and src. 

I tested with ""clientInitialization()"" with data/schema folder removed. UDT works.   ","03/Aug/17 10:08;adelapena;The changes look good to me. It seems that the CI tests that are finished are ok; it can be committed if the remaining tests pass.

One tiny detail that I forgot and can be fixed during commit, the comment ""Test that sstabledump against non-frozen udt"" in the dtest should be ""Test sstabledump against non-frozen udt"", without the ""that"".

Thanks!",04/Aug/17 19:22;cnlwsu;lgtm +1,"08/Aug/17 14:57;adelapena;Committed to 3.0 as [3960260472fcd4e0243f62cc813992f1365197c6|https://github.com/apache/cassandra/commit/3960260472fcd4e0243f62cc813992f1365197c6] and merged into 3.11 and trunk.

Dtest committed to master as [959208749d70e5808aec144e87b73e90d56a7f91|https://github.com/apache/cassandra-dtest/commit/959208749d70e5808aec144e87b73e90d56a7f91]",08/Aug/17 15:03;jasonstack;Thank you both for your review,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testall failure in org.apache.cassandra.index.internal.CassandraIndexTest.indexOnRegularColumn,CASSANDRA-13427,13062450,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,07/Apr/17 14:48,12/Mar/19 14:19,13/Mar/19 22:34,20/Apr/17 09:34,,,,,,,,,,,0,,,,"Because of the name clash, there's a following failure happening (extremely infrequently, it's worth noting, seen it only once, no further traces / instances found):

{code}
Error setting schema for test (query was: CREATE INDEX v_index ON cql_test_keyspace.table_22(v))
{code}

Stacktrace:

{code}
java.lang.RuntimeException: Error setting schema for test (query was: CREATE INDEX v_index ON cql_test_keyspace.table_22(v))
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-10 11:11:00.659,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 20 09:33:27 UTC 2017,,,,,,0|i3dcy7:,9223372036854775807,,,,,,,,adelapena,adelapena,,,,,,,,,,"07/Apr/17 15:23;ifesdjeen;Prepared a simple patch: 

|[3.0|https://github.com/apache/cassandra/compare/3.0...ifesdjeen:13427-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13427-3.0-testall/]|
|[3.11|https://github.com/apache/cassandra/compare/3.11...ifesdjeen:13427-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13427-3.11-testall/]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13427-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13427-trunk-testall/]|","10/Apr/17 11:11;adelapena;I think that you have forgotten to increase the variable [{{CassandraIndexTest.indexCounter}}|https://github.com/ifesdjeen/cassandra/blob/13427-3.0/test/unit/org/apache/cassandra/index/internal/CassandraIndexTest.java#L529] used to generate the unique names for indexes. The tests don't fail because the generated index name also contains the table name, which contains an {{AtomicInteger}} sequence number with similar purpose. Alternatively, [CASSANDRA-13385|https://issues.apache.org/jira/browse/CASSANDRA-13385] could be useful to get the generated index names. A part from this, the patch looks good to me.","11/Apr/17 07:32;ifesdjeen;[~adelapena] great find! Sorry I missed it. Fixed, rebased and re-triggered CI.","11/Apr/17 11:48;adelapena;Thanks [~ifesdjeen], now it's perfect, +1","20/Apr/17 09:33;ifesdjeen;Committed to 3.0 as [e5c2a1839f2cdf16771dcba726f862e61fda8d4f|https://github.com/apache/cassandra/commit/e5c2a1839f2cdf16771dcba726f862e61fda8d4f] and merged up to [3.11|https://github.com/apache/cassandra/commit/fc834186ff0faa5ff78512637badc59990e51173] and [trunk|https://github.com/apache/cassandra/commit/37f5005a15c9addc3b99c5a35cb72f9fc9c2c912],",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove legacy auth tables support,CASSANDRA-13371,13058575,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,spodxx@gmail.com,spodxx@gmail.com,spodxx@gmail.com,23/Mar/17 12:48,12/Mar/19 14:19,13/Mar/19 22:34,03/Aug/17 10:59,3.0.15,3.11.1,4.0,,,Feature/Authorization,,,,,0,security,,,"Starting with Cassandra 3.0, we include support for converting pre CASSANDRA-7653 user tables, until they will be dropped by the operator. Converting e.g. permissions happens by simply copying all of them from {{permissions}} -> {{role_permissions}}, until the {{permissions}} table has been dropped.

Upgrading to 4.0 will only be possible from 3.0 upwards, so I think it's safe to assume that the new permissions table has already been populated, whether the old table was dropped or not. Therefor I'd suggest to just get rid of the legacy support.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-25 14:58:34.461,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 03 10:59:14 UTC 2017,,,,,,0|i3cp1r:,9223372036854775807,,,,,,,,snazy,snazy,,,,,,,,,,"04/Jul/17 09:21;spodxx@gmail.com;* [branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13371]
* [test-all|https://circleci.com/gh/spodkowinski/cassandra/75]
* [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/111/]","25/Jul/17 14:58;snazy;Code looks good to me (pretty much identical to [this branch|https://github.com/apache/cassandra/compare/trunk...snazy:13729-remove-legacy-auth-trunk], CASSANDRA-13729).

Except, it's missing a startup check and a unit test for that.
The startup check is necessary as people may still be effectively using the old auth tables.","27/Jul/17 07:53;spodxx@gmail.com;I've rebased the patch and added the startup check, test and NEWS.txt of yours. 

* [branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13371]
* [test-all|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-13371]
* [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/150/]

Btw, there's also CASSANDRA-13662 if you're interested in further cleanup for trunk.","28/Jul/17 12:14;snazy;+1

Just two nits, feel free to do those on commit:
* {{CassandraRoleManager.getRoleFromTable}} is only used from {{getRole}} - code can be moved into {{getRole}}
* {{import org.apache.cassandra.schema.Schema;}} is no longer needed in PasswordAuthenticator
","28/Jul/17 19:10;jjordan;We should probably also add a WARN on 3.0/3.11 if the legacy tables are being used, especially if we are going to not allow upgrading if you still have them around.",29/Jul/17 08:26;snazy;[~spodxx@gmail.com] can you provide a patch for what [~jjordan] proposed?,"31/Jul/17 12:43;spodxx@gmail.com;How about using the startup check for that and make it log to warn in 3.0/3.11 and start throwing a StartupException in 4.0?

",31/Jul/17 12:45;snazy;Yea - just log a warning in 3.0 & 3.x.,"31/Jul/17 13:53;spodxx@gmail.com;Had to change the API for the startup check a bit and had to update the corresponding test for backporting...

||trunk||3.11||3.0||
|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13371-trunk]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13371-3.11]|[branch|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13371-3.0]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/158]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/157]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/156]|
|[testall|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-13371-trunk]|[testall|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-13371-3.11]|[testall|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-13371-3.0]|

","31/Jul/17 14:18;snazy;One thing in the 3.0 + 3.11 branches: Better don't reference the classes {{PasswordAuthenticator}}, {{CassandraRoleManager}}, {{CassandraAuthorizer}} but use the table names as strings for the {{LEGACY_AUTH_TABLES}} constant. Referencing the classes may unintentionally initialize stuff.
Beside that: +1 - feel free to  fix that on commit.",03/Aug/17 10:59;spodxx@gmail.com;Committed as d74ed4b78886c to 3.0. Merged to 3.11 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index queries are rejected on COMPACT tables,CASSANDRA-13627,13081418,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,blerer,blerer,blerer,21/Jun/17 13:04,12/Mar/19 14:19,13/Mar/19 22:34,26/Jun/17 11:34,3.0.15,3.11.1,4.0,,,Legacy/CQL,,,,,0,,,,"Since {{3.0}}, {{compact}} tables are using under the hood {{static}} columns. Due to that {{SELECT}} queries using secondary indexes get rejected with the following error:
{{Queries using 2ndary indexes don't support selecting only static columns}}.

This problem can be reproduced using the following unit test:
{code}    @Test
    public void testIndicesOnCompactTable() throws Throwable
    {
        createTable(""CREATE TABLE %s (pk int PRIMARY KEY, v int) WITH COMPACT STORAGE"");
        createIndex(""CREATE INDEX ON %s(v)"");

        execute(""INSERT INTO %S (pk, v) VALUES (?, ?)"", 1, 1);
        execute(""INSERT INTO %S (pk, v) VALUES (?, ?)"", 2, 1);
        execute(""INSERT INTO %S (pk, v) VALUES (?, ?)"", 3, 3);

        assertRows(execute(""SELECT pk, v FROM %s WHERE v = 1""),
                   row(1, 1),
                   row(2, 1));

    }{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-06-23 16:54:08.51,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 26 11:34:10 UTC 2017,,,,,,0|i3gjnr:,9223372036854775807,,,,,,,,adelapena,adelapena,,,3.0.0,,,,,,,"23/Jun/17 08:44;blerer;I pushed some patches for [3.0|https://github.com/apache/cassandra/compare/trunk...blerer:13627-3.0] and [trunk|https://github.com/apache/cassandra/compare/trunk...blerer:13627-trunk]. I ran the test on our internal CI for {{3.0}}, {{3.11}} and {{trunk}}. There are no unit test failures and the DTests failure are unrelated to the patches.

The patches make make sure that {{Selection::containsStaticColumns}} and {{SelectStatement::selectOnlyStaticColumns}} (in trunk) return the correct value for static compact tables.    ","23/Jun/17 16:54;adelapena;The patch looks good to me, +1.

Just a couple of trivial comments that you could perfectly ignore or fix during commit:
 * It seems that [here|https://github.com/blerer/cassandra/blob/26e7ca7b1b0b0c493508cc6565e16d185323763d/test/unit/org/apache/cassandra/cql3/validation/entities/SecondaryIndexTest.java#L1306-L1308] there are double spaces which are probably accidental. 
 * The insert statements in the newly created {{SecondaryIndexTest#testIndicesOnCompactTable}} use an uppercase ""%S"" format specifier, which will produce the uppercased name of the table. This is not a problem at all because the unquoted table name is case insensitive, but I'm mentioning it just in case the choice of the uppercased format specifier were accidental.",26/Jun/17 11:33;blerer;Thanks for the review. I fixed the mentioned problems.,26/Jun/17 11:34;blerer;Committed into 3.0 at 57c590f6f71907dda6f3d88a16883b5dbcf259ee and merged into 3.11 and trunk. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bind parameters of collection types are not properly validated,CASSANDRA-13646,13083466,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,blerer,blerer,blerer,29/Jun/17 13:53,12/Mar/19 14:19,13/Mar/19 22:34,07/Jul/17 16:18,2.2.11,3.0.15,3.11.1,4.0,,,,,,,0,,,,"It looks like C* is not validating properly the bind parameters for collection types. If an element of the collection is invalid the value will not be rejected and might cause an Exception later on.
The problem can be reproduced with the following test:
{code}
    @Test
    public void testInvalidQueries() throws Throwable
    {
        createTable(""CREATE TABLE %s (k int PRIMARY KEY, s frozen<set<tuple<int, text, double>>>)"");
        execute(""INSERT INTO %s (k, s) VALUES (0, ?)"", set(tuple(1,""1"",1.0,1), tuple(2,""2"",2.0,2)));
    }
{code}

The invalid Tuple will cause an ""IndexOutOfBoundsException: Index: 3, Size: 3""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-03 15:29:52.429,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 07 11:51:37 UTC 2017,,,,,,0|i3gw9r:,9223372036854775807,,,,,,,,adelapena,adelapena,,,2.2.0,,,,,,,"03/Jul/17 11:21;blerer;When collections are validated the collection elements are validated using the {{Serializer::validate}} method. As Tuples and UDFs use a {{BytesSerializer}} the validate method does not check if the bytes are valid for the expected Tuple or UDT.
The patches add some specific serializers for {{Tuples}} or {{UDTs}} that override the {{Serializer::validate}} method.
||[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...blerer:13646-2.2]||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...blerer:13646-3.0]||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:13646-3.11]||[trunk|https://github.com/apache/cassandra/compare/cassandra-trunk...blerer:trunk]||
I ran the patches on our internal CI and the failing tests seems unrelated to the patches.
",03/Jul/17 15:06;blerer;[~adelapena] Could you review?,"03/Jul/17 15:29;adelapena;[~blerer], sure. Hopefully I'll have it ready in a few days.","05/Jul/17 15:40;adelapena;Excellent patch, +1","07/Jul/17 11:51;blerer;Thanks for the review :-).

Committed into 2.2 at b77e11cfd51ddb0f3ac07530399abe999df0573e and merged into 3.0, 3.11 and trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade metrics to 3.1.5,CASSANDRA-13648,13083632,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,29/Jun/17 23:34,12/Mar/19 14:19,13/Mar/19 22:34,27/Jul/17 23:52,4.0,,,,,Dependencies,,,,,0,,,,"GH PR #123 indicates that metrics 3.1.5 will fix a reconnect bug:

https://github.com/apache/cassandra/pull/123
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Jul/17 08:08;spodxx@gmail.com;metrics-core-3.1.5.jar.asc;https://issues.apache.org/jira/secure/attachment/12878110/metrics-core-3.1.5.jar.asc,20/Jul/17 08:08;spodxx@gmail.com;metrics-jvm-3.1.5.jar.asc;https://issues.apache.org/jira/secure/attachment/12878111/metrics-jvm-3.1.5.jar.asc,20/Jul/17 08:08;spodxx@gmail.com;metrics-logback-3.1.5.jar.asc;https://issues.apache.org/jira/secure/attachment/12878112/metrics-logback-3.1.5.jar.asc,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-06-29 23:36:30.54,,,no_permission,,,https://github.com/apache/cassandra/pull/123,https://github.com/apache/cassandra/pull/123,,,,,,,,,9223372036854775807,,,Thu Jul 27 23:52:26 UTC 2017,,,,,,0|i3gxa7:,9223372036854775807,,,,,,,,spodxx@gmail.com,spodxx@gmail.com,,,,,,,,,,"29/Jun/17 23:36;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra/pull/123
  
    Created upstream issue https://issues.apache.org/jira/browse/CASSANDRA-13648
","29/Jun/17 23:45;jjirsa;Patch on the attached GH PR

Tests queued but not yet complete: 
Circle CI: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13648 
DTests: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/109/","18/Jul/17 10:55;spodxx@gmail.com;Patch is missing the actual jar files in lib. The build.xml dependencies are just for creating the project pom, if I remember correctly.
","18/Jul/17 17:29;jjirsa;Thanks [~spodxx@gmail.com] - just rebased and force pushed with new metrics-core, metrics-jvm, and metrics-logback jars (3.1.5)


","18/Jul/17 22:18;jjirsa;New dtest run started @ https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/134/
CircleCI run green @ https://circleci.com/gh/jeffjirsa/cassandra/250
","20/Jul/17 08:01;spodxx@gmail.com;Looks like dtest needs a rebuild. 

I also think you missed to move the corresponding lib/license files.

Did you omit the CHANGES.txt update intentionally? 
","22/Jul/17 00:19;jjirsa;CHANGES was intentionally omitted because it always conflicts. The licenses weren't intentional. Changed that, kicked off new run here.

Also changed you to reviewer since you did most of the real work. 

Dtest @ https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/146/
",24/Jul/17 14:57;spodxx@gmail.com;Patch looks good.,27/Jul/17 23:52;jjirsa;Committed as {{1e7c4b9c0584b5f63d121a5c37e0fc1e352e6108}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement short read protection on partition boundaries,CASSANDRA-13595,13079423,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,adelapena,adelapena,13/Jun/17 09:20,12/Mar/19 14:19,13/Mar/19 22:34,30/Sep/17 11:07,3.0.15,3.11.1,,,,Legacy/Coordination,,,,,0,Correctness,,,"It seems that short read protection doesn't work when the short read is done at the end of a partition in a range query. The final assertion of this dtest fails:
{code}
def short_read_partitions_delete_test(self):
        cluster = self.cluster
        cluster.set_configuration_options(values={'hinted_handoff_enabled': False})
        cluster.set_batch_commitlog(enabled=True)
        cluster.populate(2).start(wait_other_notice=True)
        node1, node2 = self.cluster.nodelist()

        session = self.patient_cql_connection(node1)
        create_ks(session, 'ks', 2)
        session.execute(""CREATE TABLE t (k int, c int, PRIMARY KEY(k, c)) WITH read_repair_chance = 0.0"")

        # we write 1 and 2 in a partition: all nodes get it.
        session.execute(SimpleStatement(""INSERT INTO t (k, c) VALUES (1, 1)"", consistency_level=ConsistencyLevel.ALL))
        session.execute(SimpleStatement(""INSERT INTO t (k, c) VALUES (2, 1)"", consistency_level=ConsistencyLevel.ALL))

        # we delete partition 1: only node 1 gets it.
        node2.flush()
        node2.stop(wait_other_notice=True)
        session = self.patient_cql_connection(node1, 'ks', consistency_level=ConsistencyLevel.ONE)
        session.execute(SimpleStatement(""DELETE FROM t WHERE k = 1""))
        node2.start(wait_other_notice=True)

        # we delete partition 2: only node 2 gets it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        session = self.patient_cql_connection(node2, 'ks', consistency_level=ConsistencyLevel.ONE)
        session.execute(SimpleStatement(""DELETE FROM t WHERE k = 2""))
        node1.start(wait_other_notice=True)

        # read from both nodes
        session = self.patient_cql_connection(node1, 'ks', consistency_level=ConsistencyLevel.ALL)
        assert_none(session, ""SELECT * FROM t LIMIT 1"")
{code}
However, the dtest passes if we remove the {{LIMIT 1}}.

Short read protection [uses a {{SinglePartitionReadCommand}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/DataResolver.java#L484], maybe it should use a {{PartitionRangeReadCommand}} instead?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-12 09:22:56.604,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 30 11:07:37 UTC 2017,,,,,,0|i3g7d3:,9223372036854775807,2.1.x,2.2.x,3.0.x,3.11.x,4.x,,,beobal,beobal,,,,,,,,,,"12/Sep/17 09:22;jasonstack;The cause is no short-read-protection generated for node2 with key=2, because no UnfilteredRowIterator with key=2 for node2...

{code}
For initial read:
Node1 returns:  
           PartitionIterator {
                UnfilteredIterator( k=1@tombstone, back by short-read-protection)  
                UnfilteredIterator( k=2, back by short-read-protection)  
          }
Node2 returns: 
           PartitionIterator {
                UnfilteredIterator( k=1, back by short-read-protection) 
          }
{code}

","13/Sep/17 10:30;jasonstack;It seems that current ShortReadProtection only supports the same partition.

The idea is to extend current ShortReadProtection function with across-partition support. If {{current response (UnfilteredPartitionIterator)}} reached the end and its last key is ""behind"" (token smaller than) other responses' current key, we will do a {{PartitionRange retry}} from the last key to make sure no response is falling short of unfetched partitions. ","13/Sep/17 11:16;iamaleksey;bq. The idea is to extend current ShortReadProtection function with across-partition support.

Correct. Short read protection hasn't been implemented properly for range reads, which causes correctness issues in particular with paging.

I'm currently addressing the few outstanding issues with single partition short reads on 3.0 and above (CASSANDRA-13794, CASSANDRA-12872). This would be an extension of that work, I guess - or at least there is strong overlap. Feel free to give it a go though - however it may or may not have to be altered afterwards to harmonise both implementations.","13/Sep/17 15:34;jasonstack;[~iamaleksey] Thanks for the heads up.

| Source |
| [3.0|https://github.com/jasonstack/cassandra/commits/13595-3.0] |
| [dtest|https://github.com/riptano/cassandra-dtest/commits/13595] |

Here is draft of making {{ShortReadProtection}} extend {{MorePartitions}}.

Changes:
   1. If there is no more rows ahead, no short-read-protection(both partition and row) is triggered
   2. Short-read-partition-protection only fetches range from last-partition-key to max token of current command range
   3. Extend UnfilteredPartitionItr with short-read-partition-protection first, then apply short-read-partition

If you have better solution or integration, feel free to mark it as duplicated. ",13/Sep/17 16:11;jjirsa;Quick note that the dtest repo has moved to [apache/cassandra-dtest|https://github.com/apache/cassandra-dtest] - so we should move [e706952e|https://github.com/riptano/cassandra-dtest/commit/e706952e8d3bb18af1f96cb0cfc283e53260513e] over to the new repo,"13/Sep/17 16:27;iamaleksey;[~jasonstack] Yours is not a duplicate, I'd just recommend holding it off a bit until the fixes for CASSANDRA-13794 and CASSANDRA-12872 get in. The code you've submitted doesn't seem very far off of what it should be, either. I will probably review/collaborate after I'm done with the other two tickets.

But this is 3.0. Have you looked into 2.1? I'm not sure how I feel about committing it to 2.1. On one hand, it's a huge correctness issues. Does it count as critical?","14/Sep/17 01:23;jasonstack;[~jjirsa] thanks, I will move to new repo.

[~iamaleksey] I haven't looked at 2.2 yet, just started off with 3.0, will do that after you finalized 13794, 12872. 

bq. Does it count as critical?

As a user, yes..","20/Sep/17 17:23;iamaleksey;[~jasonstack] My current thinking is that as bad as this is, fixing in 3.0+ is fine. You need to upgrade to get the fix though.

Can you rebase based on the latest 3.0, though - with CASSANDRA-13794 committed? Then we can take it from there.","22/Sep/17 16:23;iamaleksey;Ok first cut, but there are some major issues with it.

1. You can't rely on the class of the command to determine if it's a single partition or a range read command. We do use range read commands for indexed queries even when the partition key is set. See CASSANDRA-11617 and CASSANDRA-11872 for some more context.

2. You cannot use {{command.limits().forShortReadRetry()}} method for the new limits here. It wasn't written with ranged reads in mind, and does among other things throw away the per partition limit - not what you want to happen.

3. The command created doesn't get passed original {{command.isForThrift()}} and hardcodes it as {{false}}. This was an issue with row-level SRP as well, but I fixed it yesterday. Should be using {{PartitionRangeReadCommand.withUpdatedLimitsAndDataRange()}} instead. As a bonus, it preserves the correct {{indexMetadata}} so you don't have to do an extra lookup.

4. I don't think that new range calculation is correct, and accounts for collisions of multiple partitions keys mapping to tokens.

5. {{shouldDoPartitionShortReadProtection()}} can be written a lot simpler, and some is redundant. {{if (mergedResultCounter.counted() >= command.limits().count())}} can't ever be true (but also is equivalent to {{if (mergedResultCounter.isDone())}}). The only meaningful thing you can do here is
{code}
            // if the returned partition doesn't have enough partitions/rows to satisfy even the original limit, don't ask for more
            if (!singleResultCounter.isDone())
                return null;
{code}
, to be honest.

6. I'm not a huge fan of the way {{expectedRows}} is shared between two protections, and am not sure it's correct.

7. The metric for SRP requests isn't being incremented.

I have a version of this that fixes most of these. Needs some more work and manual testing, and eventually approval. It's a bit urgent for me, so do you mind if I take it over from this point? Thanks.",23/Sep/17 08:16;jasonstack;Thanks for the feedback. Feel free to take over.,"26/Sep/17 10:23;iamaleksey;[3.0 branch|https://github.com/iamaleksey/cassandra/commits/13595-3.0], [a new dtest|https://github.com/iamaleksey/cassandra-dtest/commits/13595], [CircleCI run|https://circleci.com/gh/iamaleksey/cassandra/41], [dtest run|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/335/testReport/].

utests have he usual circle issues with {{RemoveTest}} and {{ViewComplexTest}}, dtests have mostly the git clone issue again with some of the tests. Everything relevant seems to be passing.","29/Sep/17 14:44;beobal;The patch looks good to me, there's just one thing I'm not entirely clear about. In CASSANDRA-13794 we added an (arbitrary) lower bound of 8 to limit of the single partition read in SRRP. This removes that, but it I'm not quite sure why.
","29/Sep/17 17:37;iamaleksey;[~beobal] Sorry, it was indeed a little unclear. {{forShortReadRetry()}} was modified to retain the per partition limit, so asking for more rows then the limit no longer worked, and made the code a bit confusing.

It also just happens that that arbitrary minimum was bothering me anyway. We should probably not go beyond the limit set by the user - unless we know for sure that it is safe. But until we are smarter about this, and start taking row sizes into account, I feel uneasy about enforcing arbitrary minimums.

Pushed another commit on top that addresses two issues:
1. An NPE when trying to perform SRP on an empty iterator ({{lastPartitionKey}} would be null)
2. Lack of a reliable stop condition causing looping

With the change all relevant dtests are passing locally.","29/Sep/17 18:14;beobal;Thanks, that explanation is helpful.
Looks good to me, +1 assuming CI looks rosy.","29/Sep/17 19:21;iamaleksey;Thanks. Squashed the last two commits together, will commit once CI is happy.","30/Sep/17 11:07;iamaleksey;Committed to 3.0 as [68a67469f8b25534d086b29b8fe0fa4ec3f9d1ec|https://github.com/apache/cassandra/commit/68a67469f8b25534d086b29b8fe0fa4ec3f9d1ec] and merged with 3.11 and trunk. [Utests|https://circleci.com/gh/iamaleksey/cassandra/49] only had the usual {{ViewComplexTest}} and {{CommitLogSegmentManagerTest}} failures. [Dtest run|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/lastCompletedBuild/testReport/] was bad as usual but with no failures related to the read path.

Dtest committed as [b76a06672ca418a2a7e90278886252deccdc9edd|https://github.com/apache/cassandra-dtest/commit/b76a06672ca418a2a7e90278886252deccdc9edd].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use an ExecutorService for repair commands instead of new Thread(..).start(),CASSANDRA-13594,13079386,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,13/Jun/17 06:23,12/Mar/19 14:19,13/Mar/19 22:34,14/Aug/17 12:16,4.0,,,,,,,,,,0,,,,"Currently when starting a new repair, we create a new Thread and start it immediately

It would be nice to be able to 1) limit the number of threads and 2) reject starting new repair commands if we are already running too many.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Aug/17 12:32;krummas;13594.png;https://issues.apache.org/jira/secure/attachment/12881450/13594.png,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-07-05 22:05:07.464,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 15 17:07:45 UTC 2017,,,,,,0|i3g74v:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,"13/Jun/17 06:23;krummas;https://github.com/krummas/cassandra/commits/marcuse/limit_repair_command_threads

introduces 2 new config variables:
* {{repair_command_pool_size}} - how many repair commands can we run at the same time
* {{repair_command_pool_full_strategy}} what do we do when the pool is full ({{queue}} or {{reject}})","05/Jul/17 22:05;aweisberg;I think it's pretty unlikely there is a test dependency on repair command concurrency, but but maybe run the dtests just to be safe?

The code itself looks good.",06/Jul/17 06:34;krummas;https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/119/,"06/Jul/17 18:01;aweisberg;Maybe a legit failure? 
{{thread_count_repair_test (repair_tests.repair_test.TestRepair) ... Build timed out (after 20 minutes). Marking the build as aborted.}}","07/Jul/17 09:05;krummas;Yes, it is

Since we are reusing threads in the ExecutorService we need to clear out the tracing state between runs, I have pushed a commit that does that and restarted the dtests.","20/Jul/17 17:23;aweisberg;The dtest you started has almost certainly been aged out. https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/145/

I think we need to set up archiving of information from Apache Jenkins since they are throwing away everything after a mere 10 builds.","08/Aug/17 12:02;krummas;Still trying to get dtests to run successfully, they seem quite broken right now",08/Aug/17 15:07;aweisberg;I've noticed that short_read test seems to time out quite a bit. I'll run it locally a bit and see if it reproduces. I think I did before with no luck. This issue might only appear in Apache Jenkins.,11/Aug/17 12:34;krummas;https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/170/ ([^13594.png]),11/Aug/17 17:59;aweisberg;Unrelated but I did manage to reproduce short_read_test failing after letting it run a lot of times.,"14/Aug/17 12:16;krummas;Committed, thanks
",15/Aug/17 16:27;jkni;This causes a test failure in {{DatabaseDescriptorRefTest}} because of the new Config class - I've pushed a fix [here|https://github.com/jkni/cassandra/commit/ec3e7a84e5bae4b6968ee39a39f331fe0f5dd036].,"15/Aug/17 16:33;krummas;+1, sorry about that, again...",15/Aug/17 17:07;jkni;No problem - fix committed as {{256a74faa31fcf25bdae753c563fa2c69f7f355c}}. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update of column with TTL results in secondary index not returning row,CASSANDRA-13412,13061716,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,adelapena,ebautistabar,ebautistabar,05/Apr/17 12:17,12/Mar/19 14:19,13/Mar/19 22:34,10/May/17 11:04,2.1.18,2.2.10,,,,Feature/2i Index,,,,,0,,,,"Cassandra versions: 2.2.3, 3.0.11
1 datacenter, keyspace has RF 3. Default consistency level.

Steps:
1. I create these table and index.
{code}
CREATE TABLE my_table (
    a text,
    b text,
    c text,
    d set<int>,
    e float,
    f text,
    g int,
    h double,
    j set<int>,
    k float,
    m set<text>,
    PRIMARY KEY (a, b, c)
) WITH read_repair_chance = 0.0
   AND dclocal_read_repair_chance = 0.1
   AND gc_grace_seconds = 864000
   AND bloom_filter_fp_chance = 0.01
   AND caching = { 'keys' : 'ALL', 'rows_per_partition' : 'NONE' }
   AND comment = ''
   AND compaction = { 'class' : 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy' }
   AND compression = { 'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor' }
   AND default_time_to_live = 0
   AND speculative_retry = '99.0PERCENTILE'
   AND min_index_interval = 128
   AND max_index_interval = 2048;
CREATE INDEX my_index ON my_table (c);
{code}
2. I have 9951 INSERT statements in a file and I run the following command to execute them. The INSERT statements have no TTL and no consistency level is specified.
{code}
cqlsh <ip> <port> -u <user> -f <file>
{code}
3. I update a column filtering by the whole primary key, and setting a TTL. For example:
{code}
UPDATE my_table USING TTL 30 SET h = 10 WHERE a = 'test_a' AND b = 'test_b' AND c = 'test_c';
{code}
4. After the time specified in the TTL I run the following queries:
{code}
SELECT * FROM my_table WHERE a = 'test_a' AND b = 'test_b' AND c = 'test_c';
SELECT * FROM my_table WHERE c = 'test_c';
{code}
The first one returns the correct row with an empty h column (as it has expired). However, the second query (which uses the secondary index on column c) returns nothing.

I've done the query through my app which uses the Java driver v3.0.4 and reads with CL local_one, from the cql shell and from DBeaver 3.8.5. All display the same behaviour. The queries are performed minutes after the writes and the servers don't have a high load, so I think it's unlikely to be a consistency issue.

I've tried to reproduce the issue in ccm and cqlsh by creating a new keyspace and table, and inserting just 1 row, and the bug doesn't manifest. This leads me to think that it's an issue only present with not trivially small amounts of data, or maybe present only after Cassandra compacts or performs whatever maintenance it needs to do.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-12 12:33:28.954,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 11:02:39 UTC 2017,,,,,,0|i3d8fb:,9223372036854775807,2.1.17,2.2.9,,,,,,ifesdjeen,ifesdjeen,,,2.2.3,,,,,,,"12/Apr/17 12:33;adelapena;[~ebautistabar], I can reproduce the problem in 2.1 and 2.2 this way:
{code}
CREATE KEYSPACE k WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};

CREATE TABLE k.t (
  a text,
  b text,
  c text,
  d set<int>,
  e float,
  f text,
  g int,
  h double,
  j set<int>,
  k float,
  m set<text>,
  PRIMARY KEY (a, b, c)
);
CREATE INDEX ON k.t(c);

INSERT INTO k.t (a, b, c, h) VALUES ('test_a', 'test_b', 'test_c', 1);
UPDATE k.t USING TTL 10 SET h = 10 WHERE a = 'test_a' AND b = 'test_b' AND c = 'test_c';
-- Wait 10 seconds

SELECT * FROM k.t WHERE a = 'test_a' AND b = 'test_b' AND c = 'test_c'; -- 1 row
SELECT * FROM k.t WHERE c = 'test_c'; -- 0 rows
{code}
Or, in a simpler way:
{code}
CREATE KEYSPACE k WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

CREATE TABLE k.t (
    pk int,
    ck int,
    a int,
    b int,
    PRIMARY KEY (pk, ck)
);
CREATE INDEX ON k.t(ck);

INSERT INTO k.t (pk, ck, a, b) VALUES (1, 2, 3, 4);
UPDATE k.t USING TTL 10 SET b = 10 WHERE pk = 1 AND ck = 2;
-- Wait 10 seconds

SELECT * FROM k.t WHERE pk = 1 AND ck = 2; -- 1 row
SELECT * FROM k.t WHERE ck = 2; -- 0 rows
{code}
However, I can't reproduce the problem in 3.0.11. Could you please provide a sequence of insertions producing the failure in 3.0.11?","13/Apr/17 08:39;adelapena;The problem is also affecting to indexes on partition key components:
{code}
CREATE TABLE k.t (
    pk1 int,
    pk2 int,
    a int,
    b int,
    PRIMARY KEY ((pk1, pk2))
);
CREATE INDEX ON k.t(pk1);

INSERT INTO k.t (pk1, pk2, a, b) VALUES (1, 2, 3, 4);
UPDATE k.t USING TTL 10 SET b = 10 WHERE pk1 = 1 AND pk2 = 2;
-- Wait 10 seconds

SELECT * FROM k.t WHERE pk1 = 1 AND pk2 = 2; -- 1 row
SELECT * FROM k.t WHERE pk1 = 1; -- 0 rows
{code}

Index entries inherit the TTL of the indexed cell, and this is right with regular columns. However, indexes of primary key columns index every column, meaning that their index entries will always get the TTL of the last updated row column. 

The proposed solution is to not generate expiring cells when the indexed column is part of the primary key. Here is the patch for 2.1 and 2.2:

||[2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...adelapena:13412-2.1]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.1-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.1-dtest/]|
||[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...adelapena:13412-2.2]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.2-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.2-dtest/]|
","17/Apr/17 09:00;ebautistabar;[~adelapena], thanks a lot for taking care of this. I'll try to reproduce it again on 3.0.11 and write back with more information.

When you reproduced it in 2.2.x, what did you use? CCM, a real cluster or both?","17/Apr/17 11:12;adelapena;You are very welcome. At first, I've reproduced the problem in a three nodes ccm cluster. Then, since the cause of the problem seems to be local, I've reproduced it in a single instance and I've added the two unit tests that can be found in the suggested patch.","20/Apr/17 08:20;ifesdjeen;+1, the patch looks good.

Minor remark: we might want to add a test for a regular column, too (e.g. that the new value isn't queryable) and possibly add same tests to 3.0+, as the behaviour is important (although it does work on the later branches).","20/Apr/17 15:49;adelapena;Totally agree. I have added a new test for an index on a regular column. Here is the new patch, where the unaffected 3.0+ branches have only the unit tests:

||[2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...adelapena:13412-2.1]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.1-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.1-dtest/]|
||[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...adelapena:13412-2.2]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.2-testall/]|[dtests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-2.2-dtest/]|
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13412-3.0]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-3.0-testall/]| |
||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:13412-3.11]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-3.11-testall/]| |
||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:13412-trunk]|[utests|http://cassci.datastax.com/view/Dev/view/adelapena/job/adelapena-13412-trunk-testall/]| |","26/Apr/17 13:57;ebautistabar;I've tried to identify a precise sequence of steps to reproduce this in 3.0.x but have had no luck so far. You can close this issue if you want. If I manage to reproduce it in 3.0.x consistently I will open another issue (or re-open this one, whatever you prefer).

Thanks!","09/May/17 13:41;ifesdjeen;Sorry for the delay! Thank you for the patch,

+1 from my side, LGTM.","10/May/17 11:02;adelapena;Committed as [b0db519b79701cecac92a7a2c93101cf17fb928d|https://github.com/apache/cassandra/commit/b0db519b79701cecac92a7a2c93101cf17fb928d], thanks for reviewing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anticompaction race can leak sstables/txn,CASSANDRA-13688,13086717,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,12/Jul/17 21:52,12/Mar/19 14:19,13/Mar/19 22:34,11/Aug/17 17:31,4.0,,,,,,,,,,0,,,,"At the top of {{CompactionManager#performAntiCompaction}}, the parent repair session is loaded, if the session can't be found, a RuntimeException is thrown. This can happen if a participant is evicted after the IR prepare message is received, but before the anticompaction starts. This exception is thrown outside of the try/finally block that guards the sstable and lifecycle transaction, causing them to leak, and preventing the sstables from ever being removed from View.compacting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-13 16:49:21.208,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 11 17:31:19 UTC 2017,,,,,,0|i3hg8n:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,"12/Jul/17 21:58;bdeggleston;Branch below. This also fixes 2 edge cases I found while diagnosing this, adds some validation to CompactionTask, and tests them

1. If a promotion/demotion compaction fails, none of the sstables that have had their repairedAt/pendingRepair value changed prior to the failure were being moved to the correct strategy.
2. Removal of the compaction strategy for a given repair session doesn't check that the strategy is empty before removing it. If there are sstables still in the strategy, they will be left in compaction limbo until the node is bounced (or the compaction strategy is reloaded)
3. CompactionTask wasn't validating that repaired/unrepaired/pending repair sstables weren't being compacted together.

[trunk|https://github.com/bdeggleston/cassandra/tree/13688]
[utests|https://circleci.com/gh/bdeggleston/cassandra/71]","13/Jul/17 16:49;aweisberg;So there is the issue here https://github.com/apache/cassandra/compare/trunk...bdeggleston:13688?expand=1#diff-d4e3b82e9bebfd2cb466b4a30af07fa4R610 we talked about where it's using the wrong result value to decide whether to clean up.

Maybe run the dtests, but otherwise it looks good.","13/Jul/17 19:26;bdeggleston;Pushed up fix for that [here|https://github.com/bdeggleston/cassandra/commit/994f2afe3f42811b1ebf64e3f6de9c14a3b7a28d].

Just for posterity, the problem being fixed here is that {{submitIfRunning}} will return a cancelled future if the executor is shutdown. However, we weren't checking the returned future, we were checking the submitted task, which shouldn't ever be cancelled.

[new utests| https://circleci.com/gh/bdeggleston/cassandra/73]
[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/125/]","17/Jul/17 21:00;aweisberg;First dtest run timed out. dtests ran here https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/127/
Does that materialized view test look like anything to you? I don't recall seeing it fail recently. Don't see it failing in the last 30 builds on trunk. I kicked off the dtests again.",20/Jul/17 17:08;aweisberg;Will we ever get a clean run of this? https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/142/,11/Aug/17 17:31;bdeggleston;Finally got a good dtest run. Committed as {{e9cc805db1133982c022657f8cab86cd24b3686f}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstabledump should not use tool initialization,CASSANDRA-13683,13085989,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,cnlwsu,cnlwsu,cnlwsu,10/Jul/17 15:41,12/Mar/19 14:19,13/Mar/19 22:34,08/Aug/17 06:25,4.0,,,,,,,,,,0,,,,"The tool initialization is not necessary for the sstabledump utility and by using it we possibly introduce a unnecessary requirement of being on the same C* instance (since we may introduce changes that require it, UDT changes can lead to dangerous assumptions) that has the schema. This also does things like introduce possibly devastating issues from having the same commitlog/sstable locations.

A good example is updating the sstable activity table, and having that flushed to commitlogs as an application outside the running C* service. This may not be on same user (ie root) which than when a postmemtable flusher after restart attempts to delete it will throw an exception and potentially kill that thread pool. One that happens commitlogs will cease to get recycled and will just burn disk space until we run out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-10 15:47:45.944,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 08 06:25:09 UTC 2017,,,,,,0|i3hbrr:,9223372036854775807,,,,,,,,jjordan,jjordan,,,,,,,,,,"10/Jul/17 15:47;githubbot;GitHub user clohfink opened a pull request:

    https://github.com/apache/cassandra/pull/130

    Switch to client init for sstabledump for CASSANDRA-13683

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/clohfink/cassandra 13683

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/130.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #130
    
----
commit b763eb2ac60bf35a3676c248e40c02a5737d094a
Author: Chris Lohfink <clohfink@apple.com>
Date:   2017-07-10T15:46:34Z

    Switch to client init for sstabledump for CASSANDRA-13683

----
","28/Jul/17 21:47;jjordan;+1.  Looks good to me, we have definitely seen issues with tools doing things they probably shouldn't have by initializing too much.","08/Aug/17 06:25;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/130
","08/Aug/17 06:25;jjirsa;Thanks to both of you. Committed as {{9e3483f844d9db6fe2a6210550622fc2cd8aef72}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock in AbstractCommitLogSegmentManager,CASSANDRA-13652,13083994,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,Fuud,Fuud,01/Jul/17 12:52,12/Mar/19 14:19,13/Mar/19 22:34,17/Jul/17 20:56,3.11.1,4.0,,,,Legacy/Core,,,,,0,,,,"AbstractCommitLogManager uses LockSupport.(un)park incorreclty. It invokes unpark without checking if manager thread was parked in approriate place. 
For example, logging frameworks uses queues and queues uses ReadWriteLock's that uses LockSupport. Therefore AbstractCommitLogManager.wakeManager can wake thread inside Lock and manager thread will sleep forever at park() method (because unpark permit was already consumed inside lock).

For examle stack traces:
{code}
""MigrationStage:1"" id=412 state=WAITING
    at sun.misc.Unsafe.park(Native Method)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
    at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.awaitAvailableSegment(AbstractCommitLogSegmentManager.java:263)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.advanceAllocatingFrom(AbstractCommitLogSegmentManager.java:237)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.forceRecycleAll(AbstractCommitLogSegmentManager.java:279)
    at org.apache.cassandra.db.commitlog.CommitLog.forceRecycleAllSegments(CommitLog.java:210)
    at org.apache.cassandra.config.Schema.dropView(Schema.java:708)
    at org.apache.cassandra.schema.SchemaKeyspace.lambda$updateKeyspace$23(SchemaKeyspace.java:1361)
    at org.apache.cassandra.schema.SchemaKeyspace$$Lambda$382/1123232162.accept(Unknown Source)
    at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608)
    at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1080)
    at org.apache.cassandra.schema.SchemaKeyspace.updateKeyspace(SchemaKeyspace.java:1361)
    at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1332)
    at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1282)
      - locked java.lang.Class@cc38904
    at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:51)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$LocalSessionWrapper.run(DebuggableThreadPoolExecutor.java:322)
    at com.ringcentral.concurrent.executors.MonitoredRunnable.run(MonitoredRunnable.java:36)
    at MON_R_MigrationStage.run(NamedRunnableFactory.java:67)
    at com.ringcentral.concurrent.executors.MonitoredThreadPoolExecutor$MdcAwareRunnable.run(MonitoredThreadPoolExecutor.java:114)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$61/1733339045.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:745)

""COMMIT-LOG-ALLOCATOR:1"" id=80 state=WAITING
    at sun.misc.Unsafe.park(Native Method)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$1.runMayThrow(AbstractCommitLogSegmentManager.java:128)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$61/1733339045.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:745)
{code}

Solution is to use Semaphore instead of low-level LockSupport.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-01 12:58:59.927,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 11 04:03:39 UTC 2017,,,,,,0|i3gzif:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,"01/Jul/17 12:58;githubbot;GitHub user Fuud opened a pull request:

    https://github.com/apache/cassandra/pull/127

    CASSANDRA-13652: Deadlock in AbstractCompactionManager

    Change incorrect usage of LockSupport.park/unpark to Semaphore.
    Old implementation can deadlock because permit from unpark invocation can be consumed by park inside logging framework and manager thread will be parked forever.
    
    https://issues.apache.org/jira/browse/CASSANDRA-13652

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Fuud/cassandra commitlog_deadlock

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/127.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #127
    
----
commit 175be27297e9933906a9261cd8d0af3a772bff24
Author: Fedor Bobin <fuudtorrentsru@gmail.com>
Date:   2017-07-01T12:53:22Z

    CASSANDRA-13652: Deadlock in AbstractCompactionManager

----
","07/Jul/17 05:44;githubbot;GitHub user Fuud opened a pull request:

    https://github.com/apache/cassandra/pull/129

    CASSANDRA-13652: Deadlock in AbstractCompactionManager

    PR with same result as #127.
    Instead of small fix in #127 this PR contains refactoring to make AbstractCommitLogSegmentManager code more clear. 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Fuud/cassandra commitlog_deadlock_v2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/129.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #129
    
----
commit e1a695874dc24e532ae21ef627e852bf999a75f3
Author: Fedor Bobin <fuudtorrentsru@gmail.com>
Date:   2017-07-07T05:37:22Z

    CASSANDRA-13652: Deadlock in AbstractCompactionManager

----
","10/Jul/17 16:33;jjirsa;Anyone familiar with this code ( git history suggests maybe [~aweisberg] or [~blambov] ) interested in reviewing?



","10/Jul/17 17:42;aweisberg;I thought that LockSupport was kind of low level when I first saw that, but I didn't want to bike shed. TIL. It was using a semaphore before, and I don't remember why we switched. I approve of the change I just want to make sure [~blambov] didn't have a specific reason for making the switch. ","11/Jul/17 08:36;Fuud;Just to keep all things together, copy from mailing list
http://www.mail-archive.com/dev@cassandra.apache.org/msg11313.html

-------------------------------------------------------------

Hello,

I found possible deadlock in AbstractCommitLogSegmentManager. The root cause is incorrect use of LockSupport.park/unpark pair. Unpark should be invoked only if caller is sure that thread was parked in appropriate place. Otherwice permission given by calling unpark can be consumed by other structures (for example - inside ReadWriteLock).

Jira: https://issues.apache.org/jira/browse/CASSANDRA-13652

I suggest simplest solution: change LockSupport to Semaphore.
PR: https://github.com/apache/cassandra/pull/127

Also I suggest another solution with SynchronousQueue-like structure to move available segment from Manager Thread to consumers. With theese changes code became more clear and 
straightforward.

PR https://github.com/apache/cassandra/pull/129

We can not use j.u.c.SynchronousQueue because we need to support shutdown and there is only way to terminate SynchronousQueue.put is to call Thread.interrupt(). But C* uses nio and it does not expect ClosedByInterruptException during IO operations. Thus we can not interrupt Manager Thread. 
I implemented o.a.c.u.c.Transferer that supports shutdown and restart (needed for tests).
https://github.com/Fuud/cassandra/blob/e1a695874dc24e532ae21ef627e852bf999a75f3/src/java/org/apache/cassandra/utils/concurrent/Transferer.java

Also I modified o.a.c.d.c.SimpleCachedBufferPool to support waiting for free space.

Please feel free to ask any questions.

Thank you.

Feodor Bobin
fuudtorrentsru@gmail.com","11/Jul/17 18:29;blambov;The {{park}} call at [line 130|https://github.com/apache/cassandra/pull/127/files#diff-85e13493c70723764c539dd222455979L130] is indeed suspect, as it does not check there's no action to perform before parking. I would solve the problem by dropping that call, which would make the usage of park/unpark conform to the specifications.","11/Jul/17 18:53;aweisberg;Ah you are right it's the lack of the condition check that causes the problem. I think LockSupport park/unpark is fine it's just a thread specific semaphore bounded to a single permit.

It's technically ok if other usages of park are woken because spurious wakeups are part of the specification so other usages should handle it.",11/Jul/17 19:02;aweisberg;Although TBH thinking on it why tempt fate with LockSupport.unpark without checking the thread is actually blocked on what we think it is? Let's go with the semaphore and drop the wait at line 130.,"12/Jul/17 08:00;Fuud;[~aweisberg]
>>It's technically ok if other usages of park are woken because spurious wakeups are part of the specification so other usages should handle it.

Yes. But such other useages will eat permit and Manager Thread will be blocked in our code.","12/Jul/17 09:29;blambov;bq. Yes. But such other useages will eat permit and Manager Thread will be blocked in our code.

This is only relevant if the eating happens before checking we need to park and actually calling {{LockSupport.park()}}. This is indeed possible for the line 130 call, which is why it should be removed and the control allowed to continue to the checked park on line 146.","12/Jul/17 09:34;aweisberg;Most usages of LockSupport don't introduce (as many) spurious unparks because they check to see of the thread is blocked on the specific condition not just any condition. I want to use a Semaphore so we don't introduce spurious unparks and trigger bugs in other usages of LockSupport.park. I don't see a huge advantage to working with LockSupport directly.

Yes either way blocking without checking the condition at line 130 has to go.","12/Jul/17 11:42;blambov;The reason to prefer {{park}} for me is understandability of the code. This is a loop that does some work and pauses when there's no need to do any, a perfect candidate for park/unpark.

{{Semaphore}}, although applicable, implies something else. Our {{WaitQueue}} is a better alternative for this kind of application.","12/Jul/17 19:12;aweisberg;WaitQueue seems even more obtuse? It's just a condition variable we are looking for so why not ReentrantLock and Condition or synchronized and wait/notify?

I mean I am fine with it. People working on Cassandra need to learn how WaitQueue works at some point. It just seems like a performance optimization to avoid locking.","13/Jul/17 18:39;aweisberg;OK, how about this?

[3.11 code|https://github.com/apache/cassandra/compare/trunk...aweisberg:13652-3.11?expand=1], [utests|https://circleci.com/gh/aweisberg/cassandra/335], [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/124/]

[~Fuud] are you ok with me rewriting your patch this way?",14/Jul/17 08:48;Fuud;Yes. Seems good.,17/Jul/17 08:17;blambov;This looks good to me.,17/Jul/17 20:56;aweisberg;Committed as [eb717f154bd24453273d7175006fdef75e5724c2|https://github.com/apache/cassandra/commit/eb717f154bd24453273d7175006fdef75e5724c2]. Thanks.,"30/Aug/17 05:42;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra/pull/127
  
    Hi @Fuud - eb717f154bd24453273d7175006fdef75e5724c2 has been merged and CASSANDRA-13652 is closed. Is this PR still needed? If not, can you close it (the committers are unable to close it on your behalf)? 
","31/Aug/17 04:04;githubbot;Github user Fuud commented on the issue:

    https://github.com/apache/cassandra/pull/127
  
    @jeffjirsa Sorry, my fault. 
","31/Aug/17 04:04;githubbot;Github user Fuud closed the pull request at:

    https://github.com/apache/cassandra/pull/127
","11/Sep/17 04:03;githubbot;Github user Fuud commented on the issue:

    https://github.com/apache/cassandra/pull/129
  
    eb717f1 has been merged and CASSANDRA-13652 is closed. 
","11/Sep/17 04:03;githubbot;Github user Fuud closed the pull request at:

    https://github.com/apache/cassandra/pull/129
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incremental repair failure recovery throwing IllegalArgumentException,CASSANDRA-13658,13084348,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,03/Jul/17 21:29,12/Mar/19 14:19,13/Mar/19 22:34,06/Jul/17 00:04,4.0,,,,,,,,,,0,,,,"{code}
java.lang.RuntimeException: java.lang.IllegalArgumentException: Invalid state transition FINALIZED -> FINALIZED
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:201)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Invalid state transition FINALIZED -> FINALIZED
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:145)
	at org.apache.cassandra.repair.consistent.LocalSessions.setStateAndSave(LocalSessions.java:452)
	at org.apache.cassandra.repair.consistent.LocalSessions.handleStatusResponse(LocalSessions.java:679)
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:188)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-04 06:40:05.579,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 00:04:53 UTC 2017,,,,,,0|i3h1p3:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"03/Jul/17 21:52;bdeggleston;https://github.com/bdeggleston/cassandra/tree/13658

Added a check for noop state transitions",04/Jul/17 06:40;krummas;+1,06/Jul/17 00:00;bdeggleston;utest run: https://circleci.com/gh/bdeggleston/cassandra/59,06/Jul/17 00:04;bdeggleston;committed as {{5ccbebaf85b61673bb8c34b1f435d730183587ee}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncaught exceptions in Netty pipeline,CASSANDRA-13649,13083695,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,norman,spodxx@gmail.com,spodxx@gmail.com,30/Jun/17 07:56,12/Mar/19 14:19,13/Mar/19 22:34,21/Aug/17 22:45,2.2.11,3.0.15,3.11.1,4.0,,Legacy/Streaming and Messaging,Legacy/Testing,,,,0,patch,,,"I've noticed some netty related errors in trunk in [some of the dtest results|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/106/#showFailuresLink]. Just want to make sure that we don't have to change anything related to the exception handling in our pipeline and that this isn't a netty issue. Actually if this causes flakiness but is otherwise harmless, we should do something about it, even if it's just on the dtest side.


{noformat}
WARN  [epollEventLoopGroup-2-9] 2017-06-28 17:23:49,699 Slf4JLogger.java:151 - An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.
io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed: Connection reset by peer
	at io.netty.channel.unix.FileDescriptor.readAddress(...)(Unknown Source) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
{noformat}

And again in another test:
{noformat}
WARN  [epollEventLoopGroup-2-8] 2017-06-29 02:27:31,300 Slf4JLogger.java:151 - An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.
io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed: Connection reset by peer
	at io.netty.channel.unix.FileDescriptor.readAddress(...)(Unknown Source) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
{noformat}

Edit:
The {{io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed}} error also causes tests to fail for 3.0 and 3.11. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Aug/17 18:24;norman;0001-CASSANDRA-13649-Ensure-all-exceptions-are-correctly-.patch;https://issues.apache.org/jira/secure/attachment/12882634/0001-CASSANDRA-13649-Ensure-all-exceptions-are-correctly-.patch,30/Jun/17 07:48;spodxx@gmail.com;test_stdout.txt;https://issues.apache.org/jira/secure/attachment/12875197/test_stdout.txt,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-06-30 19:34:13.57,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 10 08:45:01 UTC 2018,,,,,,0|i3gxo7:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"30/Jun/17 19:34;jasobrown;It's a netty common practice to include an exception handler at the end of a netty pipeline to handle cases like this. However, I'm reticent to add yet another handler to the pipeline as some of my testing for CASSANDRA-8457 (admittedly, very early-stage testing) showed that we spend extra time in the pipeline just by all the mechanics around invoking another handler (checking the promise, state of the channel, and so on).

That being said, I can probably find some time to reinvestigate as part of finalizing all the netty-related things for 4.0. [~spodxx@gmail.com] feel free to assign to me if you like, but I probably can't get to it for about a month.","03/Jul/17 08:58;slebresne;I'll note that I've seen those as well and I'm not 100% sure this isn't a ""bug"" in Netty's epoll implementation in that I don't think we get this if the NIO transport is used. That is, it looks like both the Epoll and NIO Netty implementation don't behave the same way with respect to this. I'll also not we do catch exceptions in the pipeline in {{Message.Dispatcher.exceptionCaught}}, without re-throwing them, and I'm not sure why that wouldn't catch this (in fact, I believe this does catch the exception with the NIO event loop, and that's why I'm suggesting the epoll one may not be doing it's job properly). Haven't investigated much though tbh, so take this with a grain of salt.",03/Jul/17 13:15;spodxx@gmail.com;Could it be that the read call executed by epoll native happens before handlers have been added by {{Server.Initializer}}?,"04/Aug/17 08:48;spodxx@gmail.com;Looking at this issue again got me a bit suspicious on how our Initializer is [handled by ServerBootstrap|https://github.com/netty/netty/blob/3cc405296310643bccddc8c81998c97f25b3201c/transport/src/main/java/io/netty/bootstrap/ServerBootstrap.java#L169]. It seems that instances added by {{ServerBootstrap.childHandler()}} will not be added immediately to the pipeline (as opposed to {{ServerBootstrap.handler()}}, see code comment for further explanation. This behaviour was changed in netty [4638df2062|https://github.com/netty/netty/commit/4638df20628a8987c8709f0f8e5f3679a914ce1a] ([5566|https://github.com/netty/netty/issues/5566]) and included by upgrading netty in CASSANDRA-13114. Did anyone already noticed this issue before February?

I'm not really sure there's a lot we can do on our side. Maybe add our Initilizer using {{handler()}} instead of {{childHandler()}}? But still, if there's really a race then {{ServerBootstrapAcceptor}} won't get added and the pipeline will not work anyways. [~norman], do you have any suggestions?
",04/Aug/17 17:42;norman;[~spodxx@gmail.com] sorry I am a bit busy atm but will check over the next days and come back to you.,09/Aug/17 11:42;norman;[~spodxx@gmail.com] yes how handler(...) and childHandler(...) are now handled is more consistent. Can you give me a link to the code where you setup your handlers that this exceptions produce ?,"09/Aug/17 11:58;spodxx@gmail.com;The Initializer implementing ChannelInitializer<Channel> is added by calling ServerBootstrap.childHandler() here:
https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/Server.java#L153
","09/Aug/17 12:15;norman;Is it possible that you have an eventExecutor set here?:

https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/Server.java#L329

And if so can you show me the implementation of it ?","09/Aug/17 12:27;spodxx@gmail.com;It should always be set when not run in a testing context, as instanciated in 
https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/NativeTransportService.java#L65
and implemented in
https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/RequestThreadPoolExecutor.java
",14/Aug/17 12:36;norman;And this only happens with the native epoll transport but not with the nio transport ?,"18/Aug/17 08:43;norman;Actually I think it is because of how you do the event dispatching in Cassandra... working on a patch, stay tuned.",18/Aug/17 18:24;norman;To ensure we handle all exceptions in the netty pipeline that are produced by either the Channel itself or the ChannelHandlers in the pipeline we need to ensure the handlers that does so not uses the RequestThreadPoolExecutor as it not enforces strict ordering of the events per Channel.,18/Aug/17 18:25;norman;This patch should fix the problem and should go in all active Cassandra trees.,"18/Aug/17 18:35;jasobrown;I'm +1 on the patch, and I'll setup a branch to run the utests/dtests (for sanity sake).

[~spodxx@gmail.com] and [~iamaleksey]: if dtests/utests are happy, I'm thinking 3.0 and up can take this patch. Should we consider 2.2, as well? This is a bug, but not a critical one, so perhaps we should skip 2.2.","21/Aug/17 07:54;spodxx@gmail.com;+1 for 2.2 upwards. If the fix seems to be too risky for 2.2, we should probably not include it in 3.0 as well. On the opposite, if it's not, then let's not diverge our code base without good reason.","21/Aug/17 08:59;norman;Its up to you guys which versions the patch should be applied to, just wanted to mention this fix is very low-risk in terms of Netty itself as it just move logic to an extra ChannelHandler. Thats all. ",21/Aug/17 12:31;iamaleksey;3.0 still accepts non-critical bug fixes. Are we treating 2.2 like 2.1 or like 3.0 at this point? I honestly don't remember anymore.,"21/Aug/17 22:44;jasobrown;I took the editorial liberty of committing to 2.2 and up. sha is {{e1aa7d32c95ff3f06de97803a186ff432237ecab}}

Thanks for the the patch, [~norman]!","29/Aug/17 13:18;ajithshetty28;Jason/Norman, we are using cassandra ReleaseVersion: 3.0.11.
We see the same issue.

INFO  [SharedPool-Worker-1] 2017-08-27 05:35:17,241 Message.java:615 - Unexpected exception during request; channel = [id: 0xcdc01633, L:/XXXXXX:9042 ! R:/XXXXXXXX:58310]
io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed: Connection reset by peer
        at io.netty.channel.unix.FileDescriptor.readAddress(...)(Unknown Source) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]

And from the application side we see the below error:

2017-08-27 05:34:05,121 ERROR [mainLog] (nioEventLoopGroup-22-47) [/XXXXXXXX:9042] Timed out waiting for server response; nested exception is com.datastax.driver.core.exceptions.OperationTimedOutException: [/XXXXXXX:9042] Timed out waiting for server response: org.springframework.cassandra.support.exception.CassandraUncategorizedException: [/XXXXXXX:9042] Timed out waiting for server response; nested exception is com.datastax.driver.core.exceptions.OperationTimedOutException: [/XXXXXXX:9042] Timed out waiting for server response
        at org.springframework.cassandra.support.CassandraExceptionTranslator.translateExceptionIfPossible(CassandraExceptionTranslator.java:129) [spring-cql-1.5.0.RELEASE.jar:]
        at org.springframework.cassandra.core.CqlTemplate.potentiallyConvertRuntimeException(CqlTemplate.java:946) [spring-cql-1.5.0.RELEASE.jar:]

Has it been fixed int the 3.0.11 version??

Thanks,
Ajith Shetty","26/Sep/17 10:42;jasobrown;[~ajithshetty28] Please look at the ""Fix Version"" field above: this was patched in 3.0.15","11/Oct/17 17:18;varung;[~norman] I rolled out this patch to one of our production cluster, I still see syscall:read exception in the logs.

INFO  [epollEventLoopGroup-11-9] 2017-10-11 17:01:10,074 Message.java:641 - Unexpected exception during request; channel = [id: 0x72599f85, L:/X.X.X.X:XXXX - R:/X.X.X.X:XXXX]
io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed: Connection reset by peer","16/Oct/17 11:35;multani;I also noticed the frequency of this error message increased a lot after upgrading to 3.11.1:

{code:java}
INFO  [epollEventLoopGroup-2-3] 2017-10-16 10:00:37,592 Message.java:623 - Unexpected exception during request; channel = [id: 0xb253764e, L:/10.40.3.15:9042 - R:/10.30.0.10:58996]
io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed: Connection reset by peer
        at io.netty.channel.unix.FileDescriptor.readAddress(...)(Unknown Source) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}
","10/Oct/18 08:45;Narkomax;We have ReleaseVersion: 3.11.1 and there is a lot of errors. Pls help.

INFO [epollEventLoopGroup-2-4] 2018-10-09 19:30:50,113 Message.java:623 - Unexpected exception during request; channel = [id: 0x295ce677, L:/192.168.xx.xxx:9042 - R:/192.168.xx.xxx:58644]
io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed: Connection reset by peer
 at io.netty.channel.unix.FileDescriptor.readAddress(...)(Unknown Source) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]

 

Also sometimes i have error ""cluster initialization was aborted after timing out""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix incorrect [2.1 <— 3.0] serialization of counter cells with pre-2.1 local shards,CASSANDRA-13691,13087066,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,14/Jul/17 02:04,12/Mar/19 14:19,13/Mar/19 22:34,01/Aug/17 15:00,3.0.15,3.11.1,,,,Legacy/Coordination,,,,,0,counters,upgrade,,"We stopped generating local shards in C* 2.1, after CASSANDRA-6504 (Counters 2.0). But it’s still possible to have counter cell values
around, remaining from 2.0 times, on 2.1, 3.0, 3.11, and even trunk nodes, if they’ve never been overwritten.

In 2.1, we used two classes for two kinds of counter columns:
{{CounterCell}} class to store counters - internally as collections of {{CounterContext}} blobs, encoding collections of (host id, count, clock) tuples
{{CounterUpdateCell}} class to represent unapplied increments - essentially a single long value; this class was never written to commit log, memtables, or sstables, and was only used inside {{Mutation}} object graph - in memory, and marshalled over network in cases when counter write coordinator and counter write leader were different nodes
3.0 got rid of {{CounterCell}} and {{CounterUpdateCell}}, among other {{Cell}} classes. In order to represent these unapplied increments - equivalents of 2.1 {{CounterUpdateCell}} - in 3.0 we encode them as regular counter columns, with a ‘special’ {{CounterContext}} value. I.e. a counter context with a single local shard. We do that so that we can reuse local shard reconcile logic (summing up) to seamlessly support counters with same names collapsing to single increments in batches. See {{UpdateParameters.addCounter()}} method comments [here|https://github.com/apache/cassandra/blob/cassandra-3.0.14/src/java/org/apache/cassandra/cql3/UpdateParameters.java#L157-L171] for details. It also assumes that nothing else can generate a counter with local shards.

It works fine in pure 3.0 clusters, and in mixed 2.1/3.0 clusters, assuming that there are no counters with legacy local shards remaining from 2.0 era. It breaks down badly if there are.

{{LegacyLayout.serializeAsLegacyPartition()}} and consequently {{LegacyCell.isCounterUpdate()}} - classes responsible for serializing and deserialising in 2.1 format for compatibility - use the following logic to tell if a cell of {{COUNTER}} kind is a regular final counter or an unapplied increment:

{code}
private boolean isCounterUpdate()
{
    // See UpdateParameters.addCounter() for more details on this
    return isCounter() && CounterContext.instance().isLocal(value);
}
{code}

{{CounterContext.isLocal()}} method here looks at the first shard of the collection of tuples and returns true if it’s a local one.

This method would correctly identify a cell generated by {{UpdateParameters.addCounter()}} as a counter update and serialize it correctly as a 2.1 {{CounterUpdateCell}}. However, it would also incorrectly flag any regular counter cell that just so happens to have a local shard as the first tuple of the counter context as a counter update. If a 2.1 node as a coordinator of a read requests fetches such a value from a 3.0 node, during a rolling upgrade, instead of the expected {{CounterCell}} object it will receive a {{CounterUpdateCell}}, breaking all the things. In the best case scenario it will cause an assert in {{AbstractCell.reconcileCounter()}} to be raised.

To fix the problem we must find an unambiguous way, without false positives or false negatives, to represent and identify unapplied counter updates on 3.0 side. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14958,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-01 14:20:08.625,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 01 14:59:28 UTC 2017,,,,,,0|i3hidj:,9223372036854775807,,,,,,,,slebresne,slebresne,,,3.0.0,,,,,,,"15/Jul/17 00:06;iamaleksey;Ended up using a special clock id for counter update contexts, so that we can change for that clock id instead of merely looking at the shard type, to unambiguously tell regular counter values from counter updates in 3.0.

Branches with the fix: [3.0|https://github.com/iamaleksey/cassandra/tree/13691-3.0], [3.11|https://github.com/iamaleksey/cassandra/tree/13691-3.11], [4.0|https://github.com/iamaleksey/cassandra/tree/13691-4.0].

The commits include a small unit test to confirm that regular counter contexts with old local shards in first spot are no longer recognized as counter updates. Unfortunately it's a lot more painful given our existing framework to create upgrade tests that would span the whole range from 2.0 to 3.0, due to differences in supported driver protocol version.

Steps for manual reproduction and fix verification:

1. generate some 2.0 counter columns with local shards

{code}
ccm create test -n 3 -s -v 2.0.17

ccm node1 cqlsh
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};
cqlsh> CREATE TABLE test.test (id int PRIMARY KEY, v1 counter, v2 counter, v3 counter);

python
>>> from cassandra.cluster import Cluster
>>> cluster = Cluster(['127.0.0.1', '127.0.0.2', '127.0.0.3'])
>>> session = cluster.connect()
>>> query = ""UPDATE test.test SET v1 = v1 + 1, v2 = v2 + 1, v3 = v3 + 1 where id = ?""
>>> prepared = session.prepare(query)
>>> for i in range(0, 1000):
...     session.execute(prepared, [i])
...

ccm flush
{code}

2. upgrade cluster to 2.1

{code}
ccm stop
ccm setdir -v 2.1.17
ccm start

ccm node1 nodetool upgradesstables
ccm node2 nodetool upgradesstables
ccm node3 nodetool upgradesstables
{code}

3. upgrade node3 to 3.0

{code}
ccm node3 stop
ccm node3 setdir -v 3.0.14
ccm node3 start
{code}

4. with a 2.1 coordinator, try to read the table with CL.ALL

{code}
ccm node1 cqlsh
cqlsh> CONSISTENCY ALL;
cqlsh> SELECT COUNT(*) FROM test.test;
ServerError: <ErrorMessage code=0000 [Server error] message=""java.lang.AssertionError: Wrong class type: class org.apache.cassandra.db.BufferCounterUpdateCell"">
{code}

5. upgrade node3 to patched 3.0

{code}
ccm node3 stop
ccm node3 setdir --install-dir=/Users/aleksey/Code/cassandra
ccm node3 start
{code}

6. with a 2.1 coordinator, try again to read the table with CL.ALL

{code}
ccm node1 cqlsh
cqlsh> CONSISTENCY ALL;
cqlsh> SELECT COUNT(*) FROM test.test;

 count
-------
  1000

(1 rows)
{code}

I'll try to automate it as an upgrade test, too, but for now the manual steps and the unit test will have to do.",26/Jul/17 16:36;iamaleksey;Dtest [here|https://github.com/iamaleksey/cassandra-dtest/commits/13691] - requires latest ccm with [issue 463|https://github.com/pcmanus/ccm/issues/463] addressed.,"01/Aug/17 14:20;slebresne;Had a look, and this lgtm. Don't mean to stole Sam's thunder, but +1 as far as I'm concerned.","01/Aug/17 14:59;iamaleksey;Thanks.

Committed to 3.0 as [ba71289778369e71d9abbdb93cb6b91ba67f9c85|https://github.com/apache/cassandra/commit/ba71289778369e71d9abbdb93cb6b91ba67f9c85] and merged into 3.11 and 4.0. dtest committed as [55c4ca8bd450b81da6eed5055981b629b55dea15|https://github.com/apache/cassandra-dtest/commit/55c4ca8bd450b81da6eed5055981b629b55dea15].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly timed kill -9 can put incremental repair sessions in an illegal state,CASSANDRA-13660,13084357,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,03/Jul/17 22:07,12/Mar/19 14:19,13/Mar/19 22:34,06/Jul/17 17:20,4.0,,,,,,,,,,0,,,,"If a node is killed after it has sent a finalize promise message to an incremental repair coordinator, but before that section of commit log has been synced to disk, it can startup with the incremental repair session in a previous state, leading the following exception:

{code}
java.lang.RuntimeException: java.lang.IllegalArgumentException: Invalid state transition PREPARED -> FINALIZED
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:201) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_112]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_112]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.lang.IllegalArgumentException: Invalid state transition PREPARED -> FINALIZED
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:145) ~[guava-18.0.jar:?]
	at org.apache.cassandra.repair.consistent.LocalSessions.setStateAndSave(LocalSessions.java:452) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	at org.apache.cassandra.repair.consistent.LocalSessions.handleStatusResponse(LocalSessions.java:679) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:188) ~[cie-cassandra-3.0.13.5.jar:3.0.13.5]
	... 7 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-04 06:42:26.859,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:20:19 UTC 2017,,,,,,0|i3h1r3:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"03/Jul/17 22:18;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13660]
[utests|https://circleci.com/gh/bdeggleston/cassandra/61]

This prevents a scenario where sessions can revert to an earlier state on startup. Here, as part of the finalize propose sequence, we flush the repairs table after setting the local state to FINALIZE_PROMISED, but before responding to the coordinator. If we fail after, we'll be in a state we can get to FINALIZED to. If we fail before the sync, we won't have responded to the coordinator, so the session will end up failing anyway.",04/Jul/17 06:42;krummas;+1,06/Jul/17 17:20;bdeggleston;committed as {{7df240e74f0bda9a15eff3c9de02eb0cd8771b20}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PendingRepairManager race can cause NPE during validation,CASSANDRA-13659,13084355,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,03/Jul/17 21:59,12/Mar/19 14:19,13/Mar/19 22:34,06/Jul/17 00:07,4.0,,,,,,,,,,0,,,,"{{getScanners}} assumes that a compaction strategy exists for the given repair session, which may not always be the case",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-04 06:40:47.238,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 00:07:21 UTC 2017,,,,,,0|i3h1qn:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"03/Jul/17 22:02;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13659]
[utests|https://circleci.com/gh/bdeggleston/cassandra/60]

patch calls {{getOrCreate}} instead of {{get}}",04/Jul/17 06:40;krummas;+1,06/Jul/17 00:07;bdeggleston;committed as {{6a7fad6011dcc586344334c95aa9601477b9c5a3}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RangeFetchMapCalculator should not try to optimise 'trivial' ranges,CASSANDRA-13664,13084534,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,04/Jul/17 12:28,12/Mar/19 14:19,13/Mar/19 22:34,14/Aug/17 12:35,4.0,,,,,,,,,,0,,,,"RangeFetchMapCalculator (CASSANDRA-4650) tries to make the number of streams out of each node as even as possible.

In a typical multi-dc ring the nodes in the dcs are setup using token + 1, creating many tiny ranges. If we only try to optimise over the number of streams, it is likely that the amount of data streamed out of each node is unbalanced.

We should ignore those trivial ranges and only optimise the big ones, then share the tiny ones over the nodes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Aug/17 12:23;krummas;Screen Shot 2017-08-14 at 14.22.23.png;https://issues.apache.org/jira/secure/attachment/12881726/Screen+Shot+2017-08-14+at+14.22.23.png,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-07-05 22:13:25.084,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 14 12:35:34 UTC 2017,,,,,,0|i3h2u7:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,"04/Jul/17 12:30;krummas;https://github.com/krummas/cassandra/commits/marcuse/opt_large

simply filters out ranges considered trivial (less than 1000 tokens for RP and M3P, no trivial ranges for BOP etc), then re-adds them once the optimisation is done.",05/Jul/17 22:13;aweisberg;I still need to look at this in more detail but isn't the issue here that the streams aren't weighted and the decision made by total weight? RFMC is still pretty new to me so I need to look at exactly what is being done there.,"06/Jul/17 06:47;krummas;bq. isn't the issue here that the streams aren't weighted
Yes, that would be a nicer solution. It wasn't obvious to me how to do maximum bipartite matching with weighted edges though so I went with the easy solution (I guess having an edge for each token would be a way, but that would be quite silly). Also have to say I didn't spend very much time trying to figure it out, so if you have an idea, please let me know.",06/Jul/17 16:43;aweisberg;Changes makes sense to me. Can you run the dtests just to sanity check?,19/Jul/17 02:50;krummas;https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/139/,"11/Aug/17 01:09;jjirsa;This is marked ready to commit - is it good to go? That dtest run has already expired.
","11/Aug/17 12:26;krummas;rerunning tests
https://circleci.com/gh/krummas/cassandra/68
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/176

I'll try to grab screen shots once they are finished",14/Aug/17 12:24;krummas;[^Screen Shot 2017-08-14 at 14.22.23.png] - looks like only flaky failures,14/Aug/17 12:35;krummas;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node start can fail if the base table of a materialized view is not found,CASSANDRA-13737,13091644,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,adelapena,adelapena,adelapena,01/Aug/17 18:37,12/Mar/19 14:19,13/Mar/19 22:34,08/Aug/17 09:32,3.0.15,3.11.1,4.0,,,Feature/Materialized Views,Legacy/Distributed Metadata,,,,0,,,,"Node start can fail if the base table of a materialized view is not found, which is something that can happen under certain circumstances. There is a dtest reproducing the problem:
{code}
cluster = self.cluster
cluster.populate(3)
cluster.start()
node1, node2, node3 = self.cluster.nodelist()
session = self.patient_cql_connection(node1, consistency_level=ConsistencyLevel.QUORUM)
create_ks(session, 'ks', 3)

session.execute('CREATE TABLE users (username varchar PRIMARY KEY, state varchar)')

node3.stop(wait_other_notice=True)

# create a materialized view only in nodes 1 and 2
session.execute(('CREATE MATERIALIZED VIEW users_by_state AS '
                 'SELECT * FROM users WHERE state IS NOT NULL AND username IS NOT NULL '
                 'PRIMARY KEY (state, username)'))

node1.stop(wait_other_notice=True)
node2.stop(wait_other_notice=True)

# drop the base table only in node 3
node3.start(wait_for_binary_proto=True)
session = self.patient_cql_connection(node3, consistency_level=ConsistencyLevel.QUORUM)
session.execute('DROP TABLE ks.users')

cluster.stop()
cluster.start()  # Fails
{code}
This is the error during node start:
{code}
java.lang.IllegalArgumentException: Unknown CF 958ebc30-76e4-11e7-869a-9d8367a71c76
	at org.apache.cassandra.db.Keyspace.getColumnFamilyStore(Keyspace.java:215) ~[main/:na]
	at org.apache.cassandra.db.view.ViewManager.addView(ViewManager.java:143) ~[main/:na]
	at org.apache.cassandra.db.view.ViewManager.reload(ViewManager.java:113) ~[main/:na]
	at org.apache.cassandra.schema.Schema.alterKeyspace(Schema.java:618) ~[main/:na]
	at org.apache.cassandra.schema.Schema.lambda$merge$18(Schema.java:591) ~[main/:na]
	at java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet.lambda$entryConsumer$0(Collections.java:1575) ~[na:1.8.0_131]
	at java.util.HashMap$EntrySet.forEach(HashMap.java:1043) ~[na:1.8.0_131]
	at java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet.forEach(Collections.java:1580) ~[na:1.8.0_131]
	at org.apache.cassandra.schema.Schema.merge(Schema.java:591) ~[main/:na]
	at org.apache.cassandra.schema.Schema.mergeAndAnnounceVersion(Schema.java:564) ~[main/:na]
	at org.apache.cassandra.schema.MigrationTask$1.response(MigrationTask.java:89) ~[main/:na]
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:53) ~[main/:na]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [main/:na]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-01 21:47:44.551,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 08 09:31:44 UTC 2017,,,,,,0|i3ia5r:,9223372036854775807,,,,,,,,tjake,tjake,,,,,,,,,,"01/Aug/17 21:47;szhou;We had the same issue on 3.0.14 couple of days ago. Looks like somehow the MV data was corrupted and restart of any data would be stuck. Even ""drop MV"" from cqlsh doesn't work (on a different node, before restart) because the base table doesn't exist.","02/Aug/17 09:58;adelapena;Here is a very simple patch that simply ignores (and warns about) the internal addition of materialized views whose base table is unknown:
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13737-3.0]||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:13737-3.11]||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:13737-trunk]||[dtest|https://github.com/apache/cassandra-dtest/compare/master...adelapena:13737]
",07/Aug/17 19:02;tjake;+1 assuming clean CI,"07/Aug/17 19:44;githubbot;GitHub user adelapena opened a pull request:

    https://github.com/apache/cassandra-dtest/pull/4

    Add dtest for CASSANDRA-13737

    Add test verifying that a schema propagation adding a view over a non existing table doesn't prevent a node from start

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/adelapena/cassandra-dtest 13737

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/4.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4
    
----
commit 250239bf8f3da3e72f3507626a62c21ee51837e9
Author: Andrés de la Peña <a.penya.garcia@gmail.com>
Date:   2017-08-02T08:57:49Z

    Add test verifying that a schema propagation adding a view over a non existing table doesn't prevent a node from start

----
","07/Aug/17 19:45;adelapena;Committed to 3.0 as [918667929f87a2e8e74913fe6d6e5dd137fe4e4f|https://github.com/apache/cassandra/commit/918667929f87a2e8e74913fe6d6e5dd137fe4e4f] and merged to 3.11 and trunk.

Created [this PR|https://github.com/apache/cassandra-dtest/pull/4] for the dtest.
","08/Aug/17 09:31;githubbot;Github user adelapena commented on the issue:

    https://github.com/apache/cassandra-dtest/pull/4
  
    Committed as [959208749d70e5808aec144e87b73e90d56a7f91](https://github.com/apache/cassandra-dtest/commit/959208749d70e5808aec144e87b73e90d56a7f91)
","08/Aug/17 09:31;githubbot;Github user adelapena closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/4
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incremental repair sessions shouldn't be deleted if they still have sstables,CASSANDRA-13758,13094203,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,11/Aug/17 21:13,12/Mar/19 14:19,13/Mar/19 22:34,18/Aug/17 17:49,4.0,,,,,,,,,,0,incremental_repair,,,"The incremental session cleanup doesn't verify that there are no remaining sstables marked as part of the repair before deleting it. Deleting a successful repair session which still has outstanding sstables will cause those sstables to be demoted to unrepaired, creating an inconsistency.

This typically wouldn't be an issue, since we'd expect the sstables to long since have been promoted / demoted. However, I've seen a few ref leak issues which can cause sstables to get stuck. Those have been fixed, but we should still protect against that edge case to prevent inconsistencies caused by future (or currently unknown) bugs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-17 15:33:41.186,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 18 17:49:07 UTC 2017,,,,,,0|i3ipnb:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"11/Aug/17 21:27;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13758]

[utest|https://circleci.com/gh/bdeggleston/cassandra/87]",17/Aug/17 15:33;krummas;+1 - but it might make sense to log something if a session is kept because it contains data? Feel free to add that on commit if you agree,18/Aug/17 17:49;bdeggleston;added warning and committed as {{e1a1b80d424e31eeb5805c710ce010953160e3a4}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingHistogram is not thread safe,CASSANDRA-13756,13093973,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jjirsa,xiaxiangzhou,xiaxiangzhou,11/Aug/17 01:45,12/Mar/19 14:19,13/Mar/19 22:34,05/Sep/17 16:58,3.0.15,3.11.1,,,,,,,,,0,,,,"When we test C*3 in shadow cluster, we notice after a period of time, several data node suddenly run into 100% cpu and stop process query anymore.

After investigation, we found that threads are stuck on the sum() in streaminghistogram class. Those are jmx threads that working on expose getTombStoneRatio metrics (since jmx is kicked off every 3 seconds, there is a chance that multiple jmx thread is access streaminghistogram at the same time).  

After further investigation, we find that the optimization in CASSANDRA-13038 led to a spool flush every time when we call sum(). Since TreeMap is not thread safe, threads will be stuck when multiple threads visit sum() at the same time.

There are two approaches to solve this issue. 

The first one is to add a lock to the flush in sum() which will introduce some extra overhead to streaminghistogram.

The second one is to avoid streaminghistogram to be access by multiple threads. For our specific case, is to remove the metrics we added.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13038,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-17 23:00:15.033,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 05 16:58:15 UTC 2017,,,,,,0|i3io8v:,9223372036854775807,3.0.13,,,,,,,jasobrown,jasobrown,,,,,,,,,,"17/Aug/17 23:00;jjirsa;Shouldn't need a version for trunk, but [~jasobrown] if you can check me there to be sure that'd be nice (I think in the faster rewrite for trunk, we now [build|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/streamhist/StreamingTombstoneHistogramBuilder.java#L182-L186] a snapshot that is no longer modified on read).
 
|| branch || utest || dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13756] | [3.0 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13756] | [3.0 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/221/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13756] | [3.11 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13756] | [3.11 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/222/] |
","17/Aug/17 23:33;jasobrown;+1 on the changes, and please commit if/when the tests pass.

wrt trunk, yes, I think it should be safe due to the snapshots we create and access from {{StatsMetadata}}.","18/Aug/17 00:33;dikanggu;[~jjirsa], thanks for fixing it!","18/Aug/17 04:55;jjirsa;utests are clean, waiting on dtests. Apparently ~8 of the jenkins slaves are offline, so little bit delayed.",21/Aug/17 18:46;hkroger;Linking to SSTable Corruption ticket which this same bug seems to cause.,21/Aug/17 19:34;hkroger;[~jjirsa] Serialization probably needs to be synchronized somehow as well. Seems like sstable statistics can be corrupted at the moment because of this issue and although I am not sure it looks like that this patch probably doesn't address.,22/Aug/17 10:45;hkroger;Created a branch in https://issues.apache.org/jira/browse/CASSANDRA-13752 for serialization fix.,28/Aug/17 12:58;hkroger;Cassandra cleanup is probably also affected by this.,"29/Aug/17 18:56;jjirsa;Pushed new branches up based on what we learned from 13752:


|| branch || utest || dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13756] | [3.0 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13756] | [3.0 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/224/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13756] | [3.11 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13756] | [3.11 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/225/] |


",29/Aug/17 19:18;jasobrown;I'm +1 on the snapshot version of the code. You may want to wait a day to see if [~krummas] has any additional feedback.,"30/Aug/17 07:27;krummas;We should probably remove {{synchronized}} from {{flushHistogram}} and {{equals}} in the builder or make it actually thread safe and add it to the {{update}} methods as well (but the builder should never be updated by several threads so removing it should be ok)

Other than that, +1","05/Sep/17 16:58;jjirsa;Thanks all. Marcus, I've addressed your comments on commit. Committed as {{ef5ac1a4abe4fb5f407c0a24f4bc808932c5d7a2}} to 3.0 and 3.11, and {{merge -s ours}} to trunk (no changes but CHANGES.txt) with Jason and Marcus both listed as reviewers. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BTree.Builder memory leak,CASSANDRA-13754,13093846,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,snazy,urandom,urandom,10/Aug/17 16:15,12/Mar/19 14:19,13/Mar/19 22:34,13/Sep/17 17:05,3.11.1,,,,,Legacy/Core,,,,,1,,,,"After a chronic bout of {{OutOfMemoryError}} in our development environment, a heap analysis is showing that more than 10G of our 12G heaps are consumed by the {{threadLocals}} members (instances of {{java.lang.ThreadLocalMap}}) of various {{io.netty.util.concurrent.FastThreadLocalThread}} instances.  Reverting [cecbe17|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=cecbe17e3eafc052acc13950494f7dddf026aa54] fixes the issue.","Cassandra 3.11.0, Netty 4.0.44.Final, OpenJDK 8u141-b15",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Sep/17 22:14;urandom;Screenshot from 2017-09-11 16-54-43.png;https://issues.apache.org/jira/secure/attachment/12886512/Screenshot+from+2017-09-11+16-54-43.png,13/Sep/17 15:47;urandom;Screenshot from 2017-09-13 10-39-58.png;https://issues.apache.org/jira/secure/attachment/12886890/Screenshot+from+2017-09-13+10-39-58.png,01/Oct/17 11:47;tsteinmaurer;cassandra_3.11.1_Recycler_memleak.png;https://issues.apache.org/jira/secure/attachment/12889883/cassandra_3.11.1_Recycler_memleak.png,02/Oct/17 20:26;tsteinmaurer;cassandra_3.11.1_snapshot_heaputilization.png;https://issues.apache.org/jira/secure/attachment/12890028/cassandra_3.11.1_snapshot_heaputilization.png,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2017-08-10 18:08:49.389,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 03 08:09:14 UTC 2017,,,,,,0|i3ingv:,9223372036854775807,,,,,,,,jjordan,jjordan,,,,,,,,,,"10/Aug/17 18:08;jjirsa;What version are you on [~urandom] (or really, which version of netty is in the classpath) ? 
","10/Aug/17 21:43;urandom;Cassandra 3.11.0, Netty 4.0.44.Final","31/Aug/17 07:38;markusdlugi;[~jjirsa], we are experiencing the same issue on Cassandra 3.11.0 and Netty 4.0.44. We have some custom triggers which create additional rows when {{INSERT}} s on specific CFs are executed. When inserting a lot of data, the Cassandra would constantly crash with {{OutOfMemoryError}} s. We analyzed a heap dump and came to the same conclusion, which is that the instances of {{FastThreadLocalThread}} were responsible for this behaviour. As a quick workaround, we added a call to {{FastThreadLocal.removeAll()}} to our triggers, which alleviates the problem for us.

It seems that there already was an issue once with {{FastThreadLocal}} s, as described in CASSANDRA-13033, but apparently the change by [~snazy] in CASSANDRA-13034 resurfaced the issue. For a proper fix, I think all {{FastThreadLocalThread}} instances should call {{FastThreadLocal.removeAll()}} when they are done with their work. A quick glance at the code indicates that this affects the classes {{CompressedInputStream}} , {{StreamingInboundHandler}} , {{NamedThreadFactory}} and {{SEPWorker}}.","31/Aug/17 10:03;snazy;Thanks for the investigation, [~markusdlugi], I'm going to check what's happening. All {{Runnables}} should have been decorated with a {{try-finally}} to call {{FastThreadLocal.removeAll()}} to prevent exactly that.","31/Aug/17 10:55;markusdlugi;I see, you are talking about {{NamedTheadFactory.threadLocalDeallocator()}}, right? That should help for any threads created there, but as stated above, {{FastThreadLocalThread}} is also used in some other classes. Heap-wise, the most problematic one for us was the usage in {{SEPWorker}} I think, and this seems to have only been introduced with commit [1e92ce43a5a730f81d3f6cfd72e7f4b126db788a|https://github.com/apache/cassandra/commit/1e92ce43a5a730f81d3f6cfd72e7f4b126db788a] by [~tjake]. Maybe something similar can be done there and in all other classes making use of {{FastThreadLocalThread}}.","31/Aug/17 11:33;snazy;Yea, that {{SEPWorker}} plus a couple more places (see [here|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:13754-ftl-leak-3.11?expand=1] and [here|https://github.com/apache/cassandra/compare/trunk...snazy:13754-ftl-leak-trunk?expand=1]). It's actually difficult to tell whether there was really something broken in all these places in the patch, but those changes don't (should not) harm. But if someone introduces a {{FastThreadLocal}} there in the future, it should have no negative impact.
Our CI's currently checking the patches.
[~markusdlugi], do you have any chance to verify the patches?","31/Aug/17 13:53;markusdlugi;[~snazy], I checked out the 13754-ftl-leak-3.11 branch and ran our load test with your patches. Unfortunately, the memory leak is still there, Cassandra first starts GCing like crazy and then throws an {{OutOfMemoryError}} after a while. So it seems like the call to {{FastThreadLocal.removeAll()}} is not properly executed.

Actually, while writing this it just struck me why that is the case. The thread created in {{SEPWorker}} does not stop after it finishes, but is instead returned to the pool and picks up new tasks. So it will never leave the {{run()}} method, and therefore also never execute {{FastThreadLocal.removeAll()}}. So I think we will have to include that call in the {{SEPWorker}} itself.","31/Aug/17 14:05;snazy;[~markusdlugi] do you have a heap dump for me or references to what's actually in these {{FastThreadLocal}} s or which FTLs actually cause this? I suspect these are FTLs created in non-static fields, which probably need a different handling.

Calling {{FastThreadLocal.removeAll()}} after every iteration in {{SEPWorker}} is not a good solution, because that would effectively kill the benefit of all static FTLs.","31/Aug/17 16:24;snazy;Well, yea. Looking at the heap dump, that [~markusdlugi] provided, is looks like the node is ""just"" overloaded with too many and maybe too big writes in combination with a small heap. There are lots of {{BTree$Builder}} instances with live references in their {{Object[] values}} array to {{HeapByteBuffer}} instances, each holding a 1MB {{byte[]}}.
{{BTree$Builder}} instances reset the {{Object[] values}} when finished - i.e. those builders are actively doing something (writes are happening at that time).
TL;DR I don't think this is actually related to the issue that [~urandom] describes.

[~urandom], can you explain what actually what these {{ThreadLocal}} instances referenced?","01/Sep/17 07:41;markusdlugi;[~snazy], I don't think the node is overloaded. I originally thought so as well, so I made a little experiment where I included a cap in our load test limiting the {{INSERT}} s per minute from ~25,000 to ~10,000. As a consequence, the node survived a little longer, but in the end it still died with an {{OutOfMemoryError}} after more data had been inserted. So it's not that there are too many active writes, it's just that the node fails after a certain amount of total writes, which indicates to me that a memory leak is indeed happening.

I also had another look into the heap dump I sent you, and you are correct that the heap is mostly filled with {{BTree$Builder}} instances that still have stuff in their {{values}} array. However, if you look closer, you will notice that for each of these instances, the {{values}} array always contains {{null}} for the first couple of entries, and only after those there is still actual content. For some reason, the actual content always starts at index 28, whereas indices 0 - 27 are {{null}} - not sure if this is a coincidence? But you can also see that for all the {{BTree$Builder}} objects, the {{count}} attribute is 0, which also indicates to me that {{BTree$Builder.cleanup()}} has already run and those are not active writes. This theory is supported by the fact that my little workaround of manually calling {{FastThreadLocal.removeAll()}} actually works, because this means that no other objects except the {{FastThreadLocal}} s still have references to the builders.

Therefore, I think we have two issues here:

# {{SEPWorker}} is never cleaning the {{FastThreadLocal}} s, therefore accumulating references to otherwise dead objects - maybe we can include something to at least remove non-static entries regularly?
# {{BTree$Builder}} seems to have an issue properly cleaning up after building, so the objects referenced by the {{FastThreadLocal}} s of the {{SEPWorker}} threads are very large and thus ultimately lead to the {{OutOfMemoryError}} s","01/Sep/17 10:21;snazy;Your observation regarding {{BTree.Builder.values[]}} seems correct.
However, {{SEPWorker}} must *not* remove the thread locals - it's the intention of these thread-locals to be kept for reuse.","01/Sep/17 12:08;markusdlugi;Your latest patch which resets the entire {{BTree$Builder.values}} array seemed to do the trick, entire load test is now running smoothly. No more crazy GCing and most importantly no {{OutOfMemoryError}} s. Thanks a lot for the fast support and help!","01/Sep/17 14:19;snazy;Given that the FTL changes apparently do not have any influence to the OOM issuse, but look serious enough to fix, I've split them out into CASSANDRA-13838.

Patch for this ticket is reduced to the BTree change.
CI looks good.",01/Sep/17 14:21;jjordan;+1 for just https://github.com/apache/cassandra/commit/2cafd0b6b4bbc5a6ec5726d47d0093bdac3af19c to fix this and splitting out the other changes to a new ticket.,01/Sep/17 14:22;jjordan;and +1 for the patch.,"01/Sep/17 17:49;snazy;Committed as [bed7fa5ef8492d1ff3852cf299622a5ad4e0b621|https://github.com/apache/cassandra/commit/bed7fa5ef8492d1ff3852cf299622a5ad4e0b621] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11] and merged to trunk.
","11/Sep/17 22:13;urandom;I apologize for chiming in here so late, but I'm not sure this addresses what I was seeing.  In my dumps, all of the heap is tied up in the {{ThreadLocalMap}} s of instances of {{FastThreadLocalThread}} for _Native-Transport-Requests_, _RequestResponseStage_, _ReadStage_, etc; I think what I was seeing is different than [~markusdlugi].

See the attached screenshot of the dominator tree view in MemoryAnalyzer.

I can make a dump available, but be warned, it is 12G in size.","12/Sep/17 06:29;markusdlugi;[~urandom], I think it might still be the same issue. The threads you mentioned are all created by the {{SEPWorker}} as well, as you can also see in your screenshot where your {{FastThreadLocalThread}} has a reference to an instance of that class. Now I'm not sure whether the actual content of your {{ThreadLocalMap}} s is the same as in my heap dump - in my case, the maps mostly held instances of {{BTree$Builder}} , which then had references to many {{byte[]}} arrays. Maybe you can check if this is the case for you as well?

Other than that, you could also try and see if the patches created by [~snazy] alleviate your issue.","13/Sep/17 15:47;urandom;
{quote}
I think it might still be the same issue. The threads you mentioned are all created by the SEPWorker as well, as you can also see in your screenshot where your FastThreadLocalThread has a reference to an instance of that class. Now I'm not sure whether the actual content of your ThreadLocalMap s is the same as in my heap dump - in my case, the maps mostly held instances of BTree$Builder , which then had references to many byte[] arrays. Maybe you can check if this is the case for you as well?
{quote}

There are no instances of {{BTree}} here, (see new screenshot attached).

{quote}
Other than that, you could also try and see if the patches created by Robert Stupp alleviate your issue.
{quote}

Do you mean [bed7fa5|https://github.com/apache/cassandra/commit/bed7fa5ef8492d1ff3852cf299622a5ad4e0b621]?  I haven't applied that, no, but it doesn't look like I'm leaking anything {{BTree}} so I don't think that would help.","13/Sep/17 17:05;snazy;[~urandom], it's not about the {{BTree.Builder}} instances, it's about what's kept referenced by those - and that is the bunch of {{HeapByteBuffer}} instances, as reported by [~markusdlugi] and fixed by the patch for this ticket. The screenshot you attached, matches the fixed issue. Therefore, I recommend to try the patch or a build from the recent 3.11/trunk branches and test again. Going go resolve this issue. If it's really something else that's causing the issue, I'd prefer to open another ticket, because there is already something committed for this ticket that addresses a very particular issue.","01/Oct/17 11:48;tsteinmaurer;We have deployed Cassandra 3.11.1 (snapshot build from September 25, 2017) into our 9-node loadtest cluster. We still see increasing heap utilization over time with ~ 140 Recycler$Stack instances consuming ~1,8 GB heap. See attached screen (Eclipse memory analyzer screen): cassandra_3.11.1_Recycler_memleak.png","02/Oct/17 20:27;tsteinmaurer;72hrs heap utilization increase with 3.11.1 snapshot build from Sept. 25, 2017 + cluster rolling restart marker => cassandra_3.11.1_snapshot_heaputilization.png","02/Oct/17 21:44;snazy;I'll note that the fixed issue and what you're describing are probably two different things.
It might also be that the combination of recycling the btree-builders _and_ many cells in a partition. This is technically different from what's been fixed.
It would help a lot, if someone can come up with steps (ideally some code) to reproduce the issue as mat screenshots show that something happened but not why.",03/Oct/17 08:09;tsteinmaurer;[~snazy]: Created CASSANDRA-13929 and discussed a potential fix.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid writetime for null columns in cqlsh,CASSANDRA-13711,13088800,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,20/Jul/17 15:45,12/Mar/19 14:19,13/Mar/19 22:34,31/Jul/17 22:32,3.0.15,3.11.1,4.0,,,Legacy/CQL,,,,,0,,,,"From the user list:

https://lists.apache.org/thread.html/448731c029eee72e499fc6acd44d257d1671193f850a68521c2c6681@%3Cuser.cassandra.apache.org%3E

{code}
(oss-ccm) MacBook-Pro:~ jjirsa$ ccm create test -n 1 -s -v 3.0.10
Current cluster is now: test
(oss-ccm) MacBook-Pro:~ jjirsa$ ccm node1 cqlsh
Connected to test at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.0.10 | CQL spec 3.4.0 | Native protocol v4]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class':'SimpleStrategy', 'replication_factor': 1};
cqlsh> CREATE TABLE test.t ( a text primary key, b text );
cqlsh> insert into test.t(a) values('z');
cqlsh> insert into test.t(a) values('w');
cqlsh> insert into test.t(a) values('e');
cqlsh> insert into test.t(a) values('r');
cqlsh> insert into test.t(a) values('t');
cqlsh> select a,b, writetime (b) from test.t;

a | b | writetime(b)
---+------+--------------
z | null | null
e | null | null
r | null | null
w | null | null
t | null | null

(5 rows)
cqlsh>
cqlsh> insert into test.t(a,b) values('t','x');
cqlsh> insert into test.t(a) values('b');
cqlsh> select a,b, writetime (b) from test.t;

 a | b    | writetime(b)
---+------+------------------
 z | null |             null
 e | null |             null
 r | null |             null
 w | null |             null
 t |    x | 1500565131354883
 b | null | 1500565131354883

(6 rows)
{code}

Data on disk:

{code}
MacBook-Pro:~ jjirsa$ ~/.ccm/repository/3.0.14/tools/bin/sstabledump /Users/jjirsa/.ccm/test/node1/data0/test/t-bed196006d0511e7904be9daad294861/mc-1-big-Data.db
[
  {
    ""partition"" : {
      ""key"" : [ ""z"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 20,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:41:54.818118Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""e"" ],
      ""position"" : 21
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 44,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:42:04.288547Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""r"" ],
      ""position"" : 45
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 68,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:42:08.991417Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""w"" ],
      ""position"" : 69
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 92,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:41:59.005382Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""t"" ],
      ""position"" : 93
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 120,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T15:38:51.354883Z"" },
        ""cells"" : [
          { ""name"" : ""b"", ""value"" : ""x"" }
        ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""b"" ],
      ""position"" : 121
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 146,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T15:39:03.631297Z"" },
        ""cells"" : [ ]
      }
    ]
  }
]MacBook-Pro:~ jjirsa$
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-28 12:09:47.228,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 29 19:05:33 UTC 2017,,,,,,0|i3ht27:,9223372036854775807,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"27/Jul/17 01:22;jjirsa;FWIW, this is not a cqlsh bug, repro's on python drivers, and does not repro on 2.1

","27/Jul/17 19:04;jjirsa;|| Branch || Circle || Dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/commits/cassandra-3.0-13711] | [3.0 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13711 ] | [3.0 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/153/] |
| [3.11|https://github.com/jeffjirsa/cassandra/commits/cassandra-3.11-13711] | [3.11 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13711 ] | [3.11 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/154/] |
| [trunk|https://github.com/jeffjirsa/cassandra/commits/cassandra-13711] | [trunk circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13711 ] | [trunk dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/152/] |
","28/Jul/17 06:40;jjirsa;[~blerer] you were in this code recently, interested in reviewing? If not, [~beobal] or [~iamaleksey] ? 
",28/Jul/17 12:09;iamaleksey;+1,"31/Jul/17 22:32;jjirsa;Committed to 3.0 as {{9dc896f4ea51276de4ea76ffca3fd719e0c8b8a1}} and merged 3.11 and 4.0
","29/Aug/17 09:53;slebresne;The unit tests committed with that ticket are pretty fragile because they insert with a TTL of 100 and expect a following read to get the same TTL of 100, but a read give you the remaining TTL (and this is returning with a 1 second precision), so on a slow run or through some timing issue you can easily get a failure with:
{noformat}
junit.framework.AssertionFailedError: Invalid value for row 0 column 3 (ttl(i) of type int), expected <100> but got <99>
	at org.apache.cassandra.cql3.CQLTester.assertRows(CQLTester.java:1255)
	at org.apache.cassandra.cql3.validation.operations.SelectTest.testMixedTTLOnColumnsWide(SelectTest.java:4806)
{noformat}
[~jjirsa]: Would you mind having a look at making those tests less flaky? (personally don't mind if you'd rather ninja-fix).","29/Aug/17 19:05;jjirsa;I saw him open it as CASSANDRA-13764 - let me fix it there just so we don't re-open this


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't let user drop (or generally break) tables in system_distributed,CASSANDRA-13813,13097898,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,slebresne,slebresne,28/Aug/17 07:25,12/Mar/19 14:19,13/Mar/19 22:34,17/Oct/17 12:44,3.0.16,3.11.2,,,,Legacy/Distributed Metadata,,,,,0,,,,"There is not currently no particular restrictions on schema modifications to tables of the {{system_distributed}} keyspace. This does mean you can drop those tables, or even alter them in wrong ways like dropping or renaming columns. All of which is guaranteed to break stuffs (that is, repair if you mess up with on of it's table, or MVs if you mess up with {{view_build_status}}).

I'm pretty sure this was never intended and is an oversight of the condition on {{ALTERABLE_SYSTEM_KEYSPACES}} in [ClientState|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/ClientState.java#L397]. That condition is such that any keyspace not listed in {{ALTERABLE_SYSTEM_KEYSPACES}} (which happens to be the case for {{system_distributed}}) has no specific restrictions whatsoever, while given the naming it's fair to assume the intention that exactly the opposite.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13954,CASSANDRA-13812,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-29 10:53:06.985,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 17 12:44:11 UTC 2017,,,,,,0|i3jc8v:,9223372036854775807,,,,,,,,slebresne,slebresne,,,,,,,,,,"29/Aug/17 10:53;iamaleksey;The reason it isn't listed is because we want to allow changing the replication strategy/factor on those keyspaces themselves.

It's just that it's been implemented sub-diligently. So we want reject any changes to the tables (including dropping them), reject dropping of those keyspaces, but allow altering keyspace params. Also reject: creating of new tables in those keyspaces.","29/Aug/17 12:31;slebresne;bq. The reason it isn't listed is because we want to allow changing the replication strategy/factor on those keyspaces themselves.

The name of the set makes it clear what the _intention_ was. What I'm saying is that the code doesn't implement that intention. Currently, you can alter the replication strategy/factor of _all_ system distributed keyspace, even {{system_distributed}}, and I'm reasonably convinced that wasn't the intention. This certainly isn't hard to fix though.

Now with that said, I  happen to not particularly see a terribly good reason to limit which of the system distributed keyspace can have their strategy/factor changed (and again, despite the original code intention, we currently _don't_ have such limit), so instead of simply making the check on {{ALTERABLE_SYSTEM_KEYSPACES}} work as originally intended (and thus starting to forbid changing the RF on {{system_distributed}}), I'd advocate for just removing {{ALTERABLE_SYSTEM_KEYSPACES}} and deal with all those keyspace consistently. Just my 2 cents though.","29/Aug/17 12:50;iamaleksey;Not having {{system_distributed}} in {{ALTERABLE_SYSTEM_KEYSPACES}} is an oversight from the patch that added {{system_distributed}}.

Had it been added correctly, CASSANDRA-13812 would not be a problem - {{ClientState.preventSystemKSSchemaModification()}} would've forbidden those drops.

The constant is there to enumerate the keyspace whose tables cannot be modified, and that's the purpose it serves. It might not have the best possible name, though, or be ideally documented.

I'd start with adding {{system_distributed}} to {{ALTERABLE_SYSTEM_KEYSPACES}} - that would immediately fix CASSANDRA-13812 and this ticket. Then rename {{ALTERABLE_SYSTEM_KEYSPACES}} to something more descriptive (even {{PARTIALLY_ALTERABLE_SYSTEM_KEYSPACES}} would be a start. And document the intent.

EDIT: As Sylvain reminded me offline, we don't really need {{ALTERABLE_SYSTEM_KEYSPACES}} at all. The intended set of keyspaces in there is duplicated in {{Schema.REPLICATED_SYSTEM_KEYSPACE_NAMES}}, which is what we should be using instead - directly or via {{SchemaConstants.isReplicatedSystemKeyspace()}}.","05/Oct/17 17:41;iamaleksey;Fixes for [3.0|https://github.com/iamaleksey/cassandra/tree/13813-3.0], [3.11|https://github.com/iamaleksey/cassandra/tree/13813-3.11], [4.0|https://github.com/iamaleksey/cassandra/tree/13813-4.0] and a new [dtest|https://github.com/iamaleksey/cassandra-dtest/tree/13813].

Utests and dtests queued up.

[~slebresne] Any chance you could review? I've implemented what we talked about on IRC - and renamed some confusingly named things while at it.","06/Oct/17 10:46;slebresne;So the thing is that due to how the existing code is work, we currently allow anything on the tables of the {{system_distributed}} tables. And while I doubt there is any debate that we shouldn't let use drop those tables, or even drop/rename columns of those tables, some user may have been relying on altering these table _properties_. CASSANDRA-12701 particularly comes to mind here: it's kind of a bug that the repair tables in there don't have TTLs and so user may have legitimately work around it by adding the table TTL themselves. The attached patch will remove the ability for this work-around (to be extra precise, user that have already changed the properties would obviously be fine, it's new user wanting to do it that wouldn't be able to), and this feels uncomfortable to do in minor releases at least.

I guess, if we make sure to commit CASSANDRA-12701 before this, we at least remove one imo legitimate reason to modify those tables params and that would be better. Even without that though, modifying some system tables params isn't an entirely crazy idea (we can debate whether that's truly useful, but it's not crazy) and this remove that ability. Don't get me wrong btw, I know that we currently only allow this for {{system_distributed}} tables and so it's inconsistent with other system keyspace in the first place, and I'm not definitively saying we should allow modifying params on all system tables. I'm just raising the point that the patch removes something that could be considered as useful by some, and doing so in a minor concerns me a little bit.","06/Oct/17 13:08;iamaleksey;[~slebresne] I don't believe it's reasonable to keep bugs in the codebase just because they mask out other bugs. Leaving this ability is dangerous, as it breaks the assumptions we make about replicated system keyspaces, and the way we alter them. Those who make these changes manually - because we weren't careful enough and haven't shielded them from being able to - might have to pay the price later, when their cluster are unable to pick up our altered defaults, because of our bumped hardcoded timestamps being smaller than the new ones set via {{ALTER TABLE}}.

FWIW, those who are really keen to work around this issue, might as well alter {{system_schema.tables}} manually, and get the desired result. With just as much risk.

Either way, again, I don't believe that keeping the bug in just because it masks the bug in CASSANDRA-12701 is reasonable. So if you are only slightly concerned and don't mind really strongly, I'd prefer to go ahead and fix this issue here and now, before more people shoot themselves in the feet.

","06/Oct/17 14:04;slebresne;bq. I don't believe it's reasonable to keep bugs in the codebase just because they mask out other bugs.

Just to make sure my position is clear, I'm not suggesting we leave this ticket unsolved. What I'm suggesting is that it would be possible to forbid truly harmful changes (droping tables and adding/removing/renaming columns; that part is the real bug to me) while still allowing the altering of table parameters (which actually don't seem that problematic to me on principle). Sure, it requires adding some special casing in {{AlterTableStatement}}, but it's not very complex either.

bq. when their cluster are unable to pick up our altered defaults, because of our bumped hardcoded timestamps being smaller than the new ones set via ALTER TABLE

I'm not sure I understand the problem. If user have manually and knowingly updated some table params, my guess is that they expect (even rely on) future changes to defaults to not override their changes. Isn't the whole point why we've picked 0 for our hardcoded timestamp in fact?

But if there is genunine reason I'm missing that make it dangerous for user to alter those parameters, that's certainly important to  understand.  I'm pretty sure some user _have_ done it (at least to work around CASSANDRA-12701, possibly for other reasons), and if their change is a time-bomb, we kind of owe them to disarm that bomb.

But anyway, to sum it up, the patch lgtm from a technical level, and I'm definitively on board with forbidding clearly harmful changes. The only specific part I'm unsure is the ""altering table parameters"" part. In light of my current understanding, it doesn't look obviously harmful (I can even see it being desirable if I'm being honest), so I'm uncomfortable removing the ability in a minor release.

Happy to gather a few other opinions though and if the concensus is that the patch is fine the way it currently is, no problem, not trying to veto this in any way.","09/Oct/17 10:14;iamaleksey;bq. I'm not sure I understand the problem. If user have manually and knowingly updated some table params, my guess is that they expect (even rely on) future changes to defaults to not override their changes. Isn't the whole point why we've picked 0 for our hardcoded timestamp in fact?

Right. But the way ALTER works, we serialise the whole table, including all params and all columns, with the new timestamp in {{system_schema.*}} tables. Which makes it impossible for us to change the defaults later, even those that the user didn't modify on purpose. And this isn't something we can change very easily in a minor I'm afraid.

This is why we don't allow altering anything beyond keyspace params, and why this issue is, as it stands, a serious bug, and was never intended to be allowed.","09/Oct/17 10:38;iamaleksey;FWIW, I'll be the first to admit that the current situation is not ideal. It wasn't me who came up with it, but I share part of the blame - replicated system keyspaces are a bit of a mess, and this has already caused us some issues, and hassle with {{system_auth}}, and it won't be the last.

We can't even fix CASSANDRA-12701 properly in a minor without causing migration mismatch fun. So all things considered, my personal preference would be to shield existing users from causing further issues for themselves by accidentally or intentionally modifying those tables. At least until we have a good answer to these related issues, which I don't :(","09/Oct/17 13:04;slebresne;bq.  But the way ALTER works, we serialise the whole table, including all params and all columns

Good point, I hadn't though about that part. Sad!

So I guess I would agree in principle about shielding user against clearly dysfunctional behaviors. The problem is that in practice I know for a fact that CASSANDRA-12701 has been an issue for some users, where the tables had been growing way too much, to the point that being able to work-around that by setting a TTL manually probably override concerns about hypothetical future changes to defaults not being picked up.

Or to put it another way, none of this is ideal, but I wonder is ""repair history tables regularly grows out of control regularly"" isn't a bigger problem in practice than ""future defaults changes to system tables may not be picked up"". Anyway, again, not opposed on the current patch personally, but uneased by it, so wouldn't mind a few additional opinion to see if it's just me being difficult (which is possible).","09/Oct/17 13:09;iamaleksey;bq. so wouldn't mind a few additional opinion to see if it's just me being difficult (which is possible).

Oh, you've never been difficult. Neither have I. FWIW I don't feel very strongly about this going to 3.0.x vs. this going to 4.0 only. Worst case I'll just fix this for us internally.

Seeing that neither of us feels really strongly about this, I don't mind getting some opinions from others, either. I'll throw a signal on IRC and hopefully someone will reply. Either way it's not urgent.","09/Oct/17 13:53;jjordan;I am a little concerned about this change not letting anything be updated, but I do understand the reasons, and I can't really see a way around them. Given that an experience person can still get around this restriction by doing inserts into the schema tables, that is probably enough if there are any future bugs to be worked around.  Un-experienced users should not be changing these values by themselves.","10/Oct/17 01:23;KurtG;I think if we can't provide a data model for our tables that works for all scenarios then we need to allow operators to make changes. I've had quite a few occasions where modifying ""system"" tables was necessary, and I'm sure more tables will be introduced that don't work in all scenarios in the future. 

While there is the workaround of just inserting into the system_schema tables that is fraught with peril, and far more likely for them to do something that breaks things. I can't see someone saying ""woops I accidentally DROPped/ALTERed a random column in system_distributed.view_build_status"", but I can definitely see someone trying to insert into system_schema.tables and making mistakes. As soon as we make them replicated we hand over some responsibility to the operator to manage them (not that the non-replicated keyspaces have a history of being perfect though), and I'd expect to be able to change table properties that potentially affect the cluster.

Cassandra already requires you to know what your doing as an operator, this really doesn't increase that expectation. There are a million other bad choices you could make when managing a cluster that would be far more catastrophic (and far more likely). I would like to move away from that, but a lot of that sort of thing requires major changes to fix. As in this case it seems we'll need the capability limitation framework or other major changes to make a reasonable compromise. ","10/Oct/17 09:58;slebresne;bq. an experience person can still get around this restriction by doing inserts into the schema tables

I also just realized that doing so actually avoids the issue we currently have with {{ALTER}} that it rewrites all columns, so it makes it a somewhat better work-around (of course, still a work-around and that don't dispense us for fixing all this more cleanly). My only bother is that while I haven't actually tried it recently, last time I did try updating the schema tables manually, it was annoying because the changes were not automatically picked-up and in fact tended to be overridden, so I had to force a reload in weird ways (altering some other unrelated table in the keyspace, which here would actually be an issue). So it would be nice if we added a JMX call to force reload schema tables from disk to make this easier (should be easy). If we do, I'm warming up to the idea of considering this is the only really safe work-around until we find a better way to deal with all thise.

bq. if we can't provide a data model for our tables that works for all scenarios then we need to allow operators to make changes.

I'm not sure what you are trying to mean by that. If it's a reference to CASSANDRA-12701, then what makes that change problematic is the very same reason why leaving {{ALTER}} working here is problematic. So feel free to suggest a concrete solution to those problems if you have one, but otherwise, I'm not sure how this statement helps make a decision on this issue.

bq. I've had quite a few occasions where modifying ""system"" tables was necessary, and I'm sure more tables will be introduced that don't work in all scenarios in the future.

First, it would be nice if you could be a bit more concrete on those time where it was ""necessary"": which tables, what modification and why what it necessary? We're trying to find the best course of action for a very concrete problem and we are all experienced C* developers: let's be specific.

Second, I'm not sure how to re-conciliate that sentence as a whole to the concrete problem at end. Let's remind that we do _already_ refuse {{ALTER}} on most system tables, and this ticket is not about discussing whether we should allow {{ALTER}} on system tables _in general_. If you want to discuss that, I'm good with it (outside of the fact that we will have to solve the technical gotcha mentioned above) and your arguments seem to really be applied to such change, but please open a separate ticket and let's not divert that one.","10/Oct/17 10:24;iamaleksey;bq. My only bother is that while I haven't actually tried it recently, last time I did try updating the schema tables manually, it was annoying because the changes were not automatically picked-up and in fact tended to be overridden, so I had to force a reload in weird ways (altering some other unrelated table in the keyspace, which here would actually be an issue). So it would be nice if we added a JMX call to force reload schema tables from disk to make this easier (should be easy). If we do, I'm warming up to the idea of considering this is the only really safe work-around until we find a better way to deal with all thise.

Yep. You either do an unrelated {{ALTER}} (usually {{WITH comment = ...}}) or bounce the node. Also wouldn't mind at all adding the new JMX call, as a companion to {{resetlocalschema}}.","10/Oct/17 10:30;iamaleksey;[~slebresne] I can/will extend the patch with a new {{reloadlocalschema}} JMX call and a nodetool cmd when/if you warm up to it sufficiently (:

EDIT: Actually, never mind. I'll do it either way, in a separate JIRA, for cleanliness sake, and it's independently useful anyway. Will poke you on it once ready.",13/Oct/17 14:33;iamaleksey;Created CASSANDRA-13954 for jmx/nodetool work.,"16/Oct/17 02:17;KurtG;bq. I'm not sure what you are trying to mean by that.
Not sure how to make this clearer. If we can't provide a one-size-fits-all solution for our system distributed tables, which we can't, we should allow for them to be modified by operators. In CASSANDRA-12701 we can't set a TTL because we can't pick a TTL that's appropriate for all use cases (or compaction strategy for that matter). Users have to pick their own, thus we should allow them to change these properties. So far it only may be that TTL is the one that users need to change, but I'm sure there are potentially others, and I'm sure there will be more in the future.

bq. First, it would be nice if you could be a bit more concrete on those time where it was ""necessary""
Sure, I've had to change TTL as per CASSANDRA-12701 multiple times. I've never bothered changing compaction strategy but obviously that would be beneficial.
I've also had to set up jobs to routinely truncate system_traces. This may sound absurd, but in environments where you don't necessarily have control over clients, it is applicable. Being able to TTL on this keyspace would improve things here.
I've also had to drop tables in system_distributed on occasion when for some reason or another they got corrupt/different ID's across the cluster. This has also occured for system_traces, but obviously had to delete manually from system_schema.
Obviously also the usual system_auth changes, however so far that only relates to modifying RF, but I'll note that we do require dropping tables in that keyspace for conversion to role based access.

bq.  this ticket is not about discussing whether we should allow ALTER on system tables in general.
Yep, I was never talking about system tables in general, just the distributed ones. I thought this would be obvious considering that's what the ticket was about.

I think it's worth pointing out the following as well, because it's been raised that having alterable system_distributed tables is a major defect.
This issue (and related ones) were raised by developers as ""major"" issues.
There appears to be no evidence so far that this issue has actually affected users.
We're rushing to fix it because for some reason, all of a sudden, we've determined that users breaking their own cluster by doing funny things is a huge problem. This is odd to me, considering users routinely break their clusters by running repairs, but I've never heard of anyone breaking their cluster because they altered a system ""distributed"" keyspace.
There appears to be no evidence that you can really do catastrophic damage by altering these tables. If we fixed the fact that they don't get recreated automatically anymore I'm pretty sure any issues created by doing odd changes on the tables (which is very unlikely) would easily be resolved by dropping the tables and restarting a node. On that note, not sure if repair actually will break if the history tables are broken, but as far as MV are concerned, they keep working even if you truncate/drop the tables.
The suggested fix still leaves a workaround to make the changes we'd be blocking here, and this is on purpose. This kind of implies that we're perfectly OK with users making these changes. I mean, if this is an actual issue surely we should also create a ticket ""Don't let user insert garbage into system_schema"".

I'm really not convinced that this is such a major high priority issue that we can't just put it off until 4 and fix it with something more reasonable like the capability limitation framework, while maintaining the current behaviour. 
","17/Oct/17 08:19;slebresne;As suggested earlier, I'm personally +1 on the patch here now that we have CASSANDRA-13954.",17/Oct/17 12:44;iamaleksey;Thanks. Committed to 3.0 as [9e37967e17a1b223df35c1c7cec4dc4adf0b2d91|https://github.com/apache/cassandra/commit/9e37967e17a1b223df35c1c7cec4dc4adf0b2d91] and merged up with 3.11 and trunk. Dtest committed as [957ae2bc455c56835632193e1aa251495c3724f8|https://github.com/apache/cassandra-dtest/commit/957ae2bc455c56835632193e1aa251495c3724f8].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed compaction is not captured,CASSANDRA-13833,13098935,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,31/Aug/17 17:39,12/Mar/19 14:19,13/Mar/19 22:34,04/Sep/17 13:05,2.2.11,3.0.15,3.11.1,4.0,,Local/Compaction,,,,,0,,,,"Follow up for CASSANDRA-13785, when the compaction failed, it fails silently. No error message is logged and exceptions metric is not updated. Basically, it's unable to get the exception: [CompactionManager.java:1491|https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1491]

Here is the call stack:
{noformat}
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:195)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:264)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at java.lang.Thread.run(Thread.java:745)
{noformat}
There're 2 {{FutureTask}} in the call stack, for example {{FutureTask1(FutureTask2))}}, If the call thrown an exception, {{FutureTask2}} sets the status, save the exception and return. But FutureTask1 doesn't get any exception, then set the status to normal. So we're unable to get the exception in:
[CompactionManager.java:1491|https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1491]

2.1.x is working fine, here is the call stack:
{noformat}
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:177) ~[main/:na]
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:73) ~[main/:na]
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:264) ~[main/:na]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_141]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_141]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_141]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_141]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_141]
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-01 07:18:39.03,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 04 13:05:54 UTC 2017,,,,,,0|i3jilb:,9223372036854775807,2.2.x,3.0.x,3.11.x,4.x,,,,krummas,krummas,,,,,,,,,,"31/Aug/17 20:21;jay.zhuang;Attached the patch to change {{submit()}} to {{execute()}}, as we already created the {{FutureTask}} for {{Runnable}}, {{submit()}} will create one more {{FutureTask}}, which causes nested {{FutureTask}}:
{code}
    public Future<?> submit(Runnable task) {
        if (task == null) throw new NullPointerException();
        RunnableFuture<Void> ftask = newTaskFor(task, null);
        execute(ftask);
        return ftask;
    }
{code}

| branch | utest |
| [13833-2.2|https://github.com/cooldoger/cassandra/tree/13833-2.2] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13833-2.2.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13833-2.2] |
| [13833-3.0|https://github.com/cooldoger/cassandra/tree/13833-3.0] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13833-3.0.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13833-3.0] |
| [13833-3.11|https://github.com/cooldoger/cassandra/tree/13833-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13833-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13833-3.11] |
| [13833-trunk|https://github.com/cooldoger/cassandra/tree/13833-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13833-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13833-trunk] |","31/Aug/17 20:41;jay.zhuang;[~aweisberg] would you please review? You made the similar change ({{submit() -> execute()}}) in 3.11 and trunk: CASSANDRA-12358

cc [~Stefania]","01/Sep/17 07:18;krummas;nice catch, code LGTM, just a small test fix for 3.11 and trunk: https://github.com/krummas/cassandra/commit/bb9c9e0b685d3b4e76a7b082b46b01a7ed6c8af5

running dtests:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/261/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/262/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/263/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/264/","04/Sep/17 12:27;krummas;I reran the failures locally and the ones that looked suspicious all pass (except for trunk which looks completely broken now)

I'll get it committed
","04/Sep/17 13:05;krummas;committed as {{e80ede6d393460f22ee}}, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CircleCI fix - only collect the xml file from containers where it exists,CASSANDRA-13807,13097635,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,25/Aug/17 15:21,12/Mar/19 14:19,13/Mar/19 22:34,30/Aug/17 11:41,2.1.19,2.2.11,3.0.15,3.11.1,4.0,,,,,,0,,,,"Followup from CASSANDRA-13775 - my fix with {{ant eclipse-warnings}} obviously does not work since it doesn't generate any xml files

Push a new fix here: https://github.com/krummas/cassandra/commits/marcuse/fix_circle_3.0 which only collects the xml file from the first 3 containers
Test running here: https://circleci.com/gh/krummas/cassandra/86",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-28 15:20:51.68,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 30 11:41:40 UTC 2017,,,,,,0|i3jamn:,9223372036854775807,,,,,,,,eduard.tudenhoefner,eduard.tudenhoefner,,,,,,,,,,28/Aug/17 06:50;krummas;[~eduard.tudenhoefner] could you have a look?,"28/Aug/17 15:20;eduard.tudenhoefner;[~krummas] changes lgtm, so +1 on merging",30/Aug/17 11:41;krummas;I added the same case statement to all branches - just made sure to include the nodes that generate an xml file,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix short read protection logic for querying more rows,CASSANDRA-13794,13097299,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,benedict,iamaleksey,24/Aug/17 12:36,12/Mar/19 14:19,13/Mar/19 22:34,20/Sep/17 16:59,3.0.15,3.11.1,,,,Legacy/Coordination,,,,,0,Correctness,,,"Discovered by [~benedict] while reviewing CASSANDRA-13747:

{quote}
While reviewing I got a little suspicious of the modified line {{DataResolver}} :479, as it seemed that n and x were the wrong way around... and, reading the comment of intent directly above, and reproducing the calculation, they are indeed.

This is probably a significant enough bug that it warrants its own ticket for record keeping, though I'm fairly agnostic on that decision.

I'm a little concerned about our current short read behaviour, as right now it seems we should be requesting exactly one row, for any size of under-read, which could mean extremely poor performance in case of large under-reads.

I would suggest that the outer unconditional {{Math.max}} is a bad idea, has been (poorly) insulating us from this error, and that we should first be asserting that the calculation yields a value >= 0 before setting to 1.
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-04 19:28:08.742,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 20 16:59:24 UTC 2017,,,,,,0|i3j8kn:,9223372036854775807,,,,,,,,benedict,benedict,,,,,,,,,,"04/Sep/17 19:28;iamaleksey;Work in progress branch [here|https://github.com/iamaleksey/cassandra/tree/13794-3.0]. Currently missing (new) tests, but I want to get the underlying logic reviewed and approved, first. Would add coverage before committing it.

A short summary of the issue: the code right now has two variables swapped, which ultimately results in us always fetching 1 extra row per short read protection requests, in a blocking manner, making it very inefficient. But upon closer look, there are some other inefficiencies here that can and should be addressed:

1. One of our stop conditions is {{lastCount == counter.counted()}}. It's supposed to abort a short read if our previous attempt to fetch more rows yielded 0 extra rows. It's not incorrect, but is only a special case of the more general scenario: our previous attempt to fetch more extra rows yielded fewer results than we requested for. That would mean there is no more rows to fetch at that replica, and allows us to abort earlier and more frequently.

2. Another of our stop conditions is {{!counter.isDoneForPartition()}}. Once again, it isn't incorrect, but it can be extended further. Due to the way {{isDoneForPartition()}} is defined ({{isDone() || rowInCurrentPartition >= perPartitionLimit}}) and because of that counter being counting-only, it is possible for us to have fetched enough rows total for other partitions short read retries previously to hit the global limit of rows in the counter. That would make {{isDone()}} return {{true}} always, and have {{isDoneForPartition()}} return false positives even if the partition currently processed only has a partition level deletion and/or tombstones. That can affect queries that set per partition limit explicitly or when running {{SELECT DISTINCT}} queries. Spotted that during CASSANDRA-13747 fixing.

3. Once we've swapped {{x}} and {{n}} in {{moreContents()}} to fix the logic error, we'd still have some issues. In degenerate cases where we have some nodes missing a fresh partition deletion, for example, the formula would fetch *a lot* of rows {{n * (n - 1)}}, with {{n}} growing exponentially with every attempt.

Upon closer inspection, the formula doesn't make 100% sense. It claims that we miss {{n - x}} rows - where {{n = counter.countedInCurrentPartition()}} and {{x = postReconciliationCounter.countedInCurrentPartition()}}, but the number we really miss is {{limit - postReconciliationCounter.counted()}} or {{perPartitionLimit - postReconciliationCounter.countedInCurrentPartition()}}. They might be the same on our first short read protection iteration, but will be diverging further and further with each request. In addition to that, it seems to assume a uniform distribution of tombstones (in the end result) rows in the source partition, which can't be true for most workloads.

I couldn't come up with some ideal heuristic that covers all workloads, so I stuck to something safe that respects client paging limits but still attempts to minimise the # of requests we make by fetching (in most cases) more rows than is minimally necessary. I'm not completely sure about it, but I welcome any ideas on how to make it better. Either way, anything we do should be significantly more efficient than what we have now.

I've also made some renames, refactorings, and moved a few things around to better understand the code myself, and make it clearer for future contributors - including future me. The most significant noticeable change is application of the per-response counter shift to {{mergeWithShortReadProtection()}} method, instead of overloading {{ShortReadRowProtection}} with responsibilities - I also like it to be next to the global counter creation, so you can see the contrast in arguments.","04/Sep/17 19:29;iamaleksey;Marking the ticket as {{Patch Available}}, despite its lack of (new) tests, so that it can be reviewed first. Tests will be committed with the rest of the code.","14/Sep/17 19:13;benedict;This patch is great (excepting a couple of extraneous edits).  Love the comments.

+1

I would suggest filing two follow-up tickets to address some short comings with this code path, but they're edge-case, and not straight-forward enough, to not block this merge.

# For some extreme users, 16 rows could be a huge amount of data.  There should probably be some modulation of this lower bound based on known data sizes in the table, or the like.
# Conversely, in an overloaded cluster on which users are commonly performing fairly large-limit reads (say, 1k+ moderate sized rows) of larger partitions, we could find ourselves doubling the amount of work the cluster needs to do; during overload we can expect dropped writes, and a single missing row in any read would trigger a same-sized read.  This could iteratively compound the overload. 
 The best solutions to this problem are probably non-trivial, though a simplish approach might be to use exponential growth, bounded on both sides by a minimum and maximum (perhaps similarly determined from the known data size distribution) - with the query limit being used as the first value if it is small enough.

I _would_ say that (2) is no worse than the status-quo, given the per-request overheads are probably greater than the per-datum overheads in a typical cluster, but CASSANDRA-12872 suggests we haven't been incurring the full overheads of SRP, so we cannot claim that.  I still think it is reasonable to address this in a follow-up ticket, however.","20/Sep/17 16:59;iamaleksey;Committed to 3.0 as [f93e6e3401c343dec74687d8b079b5697813ab28|https://github.com/apache/cassandra/commit/f93e6e3401c343dec74687d8b079b5697813ab28] and merged with 3.11 and trunk.

Circle run for 3.0 [here|https://circleci.com/gh/iamaleksey/cassandra/39] has two completely unrelated {{CommitLogSegmentManagerTest}} failures, and [dtest run|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/314/testReport/] here is mostly failures to git clone.

The passing tests include the 3 new dtests added since this JIRA was created. My initial plan was to cover it with proper unit tests, too - similar to read repair tests we have - but doing it properly has proven to be too time consuming. In addition to the tests we have, I did a lot of manual testing (which uncovered a couple more issues - not affecting my branch). But more unit test coverage will be added later - we've budgeted a significant chunk of time on {{DataResolver}} testing alone.

Follow up JIRAs I'll file soonish. Thanks for the review! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RepairJob blocks on syncTasks,CASSANDRA-13797,13097450,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,24/Aug/17 22:15,12/Mar/19 14:19,13/Mar/19 22:34,14/Mar/18 12:56,3.0.15,3.11.1,4.0,,,Consistency/Repair,,,,,0,,,,"The thread running {{RepairJob}} blocks while it waits for the validations it starts to complete ([see here|https://github.com/bdeggleston/cassandra/blob/9fdec0a82851f5c35cd21d02e8c4da8fc685edb2/src/java/org/apache/cassandra/repair/RepairJob.java#L185]). However, the downstream callbacks (ie: the post-repair cleanup stuff) aren't waiting for {{RepairJob#run}} to return, they're waiting for a result to be set on RepairJob the future, which happens after the sync tasks have completed. This post repair cleanup stuff also immediately shuts down the executor {{RepairJob#run}} is running in. So in noop repair sessions, where there's nothing to stream, I'm seeing the callbacks sometimes fire before {{RepairJob#run}} wakes up, and causing an {{InterruptedException}} is thrown.

I'm pretty sure this can just be removed, but I'd like a second opinion. This appears to just be a holdover from before repair coordination became async. I thought it might be doing some throttling by blocking, but each repair session gets it's own executor, and validation is  throttled by the fixed size executors doing the actual work of validation, so I don't think we need to keep this around.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14332,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-25 14:27:17.596,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 14 16:35:19 UTC 2018,,,,,,0|i3j9hr:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"24/Aug/17 22:23;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13797]
[utest|https://circleci.com/gh/bdeggleston/cassandra/104]
[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/218/]","25/Aug/17 14:27;krummas;+1 (if the tests pass, I restarted the dtests)","29/Aug/17 21:54;bdeggleston;let's see if this one finishes: [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/231]

Edit: also aborted",11/Sep/17 22:54;bdeggleston;rebased on latest trunk: [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/295/],"12/Sep/17 15:10;bdeggleston;Adding 3.0 branch & tests since it's also affected.

[3.0|https://github.com/bdeggleston/cassandra/tree/13797-3.0]
[utest|https://circleci.com/gh/bdeggleston/cassandra/117]
[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/302/]",13/Sep/17 12:56;jjordan;If adding 3.0 you also need 3.11,"13/Sep/17 22:06;bdeggleston;Got clean utest runs on 3.0, 3.11, and trunk. 3.0 and trunk dtest failures were either also failing in their respective parent branches, or not reproducible locally. Committed to 3.0 as {{e7299c08f940057e8fd4dfa3f24dcc6e0cb5f78d}} and merged up","02/Oct/17 16:35;jjirsa;Please keep an eye on fixvers.
","01/Mar/18 23:17;VincentWhite;Now that we don't wait for the validations of each repair job to finish before moving onto the next one, I don't see anything to stop the repair coordinator from spinning through all the token ranges and effectively triggering all the validations tasks at once, which could be a significant amount of validation compactions on each node depending on your topology and common ranges for that keyspace. I'm also not sure of the overhead of creating all the futures/listeners on the coordinator at once in this case. 

In 3 the validation executor thread pool has no size limit so a new validation is started as soon as a validation request is received. I admit I haven't caught up on the changes to repair in trunk, and while the validation executor pool size is configurable in trunk, its default is still Integer.MAX_VALUE.

I understand this same affect (hundreds of concurrent validations) can still happen if you trigger a repair across a keyspace with a large number of column families but with this change there is no way of avoiding it without using subrange repairs on a single column family (if you have a topology/replication that cant be merged into a small number of common ranges) .
","02/Mar/18 01:04;bdeggleston;[~VincentWhite] are you seeing this happen? It's been a little while since I've thought about this, but I do remember worrying about something like and eventually convincing myself that this runaway validation case you're describing is prevented by something else.","02/Mar/18 03:43;VincentWhite;After upgrading an ~18 node, vnode multi-DC cluster from 3.11.0 to 3.11.1 it started seeing some nodes running hundreds of concurrent validation compactions, rolling back it went back to 1 concurrent validation per CF. I haven't had a chance to reproduce it at that scale but my locale testing show that if I have enough data, or just add a sleep(9999999) to validation compactions to simulate long validations, they continue to accumulate over a few seconds until the repair session has looped through all the common ranges. 

",14/Mar/18 00:22;KurtG;This issue is actually pretty serious for anyone running vnodes and a mid sized cluster. This isn't the first time we've had unbounded validation compactions kicked off simultaneously and it's caused a lot of problems at Instaclustr in the past. We should really fix this by 3.11.3 because it easily causes massive latency spikes whenever a repair kicks off due to validations taking up all the CPU. I'd like a simple revert but that doesn't fix the issue in the description. Don't think this warrants a new ticket so I think reopening this one is in order. [~bdeggleston] [~krummas] WDYT?,"14/Mar/18 12:55;jjordan;[~KurtG] when a ticket has already gone out in a release we prefer to open a brand new ticket rather than re-open an existing one, as fix versions get confusing if you re-open and already released ticket.","14/Mar/18 16:35;bdeggleston;Agreed this should be a new ticket. Maybe the right fix is to backport CASSANDRA-13521 with concurrent validations set to something reasonable? I think the incorrect assumption of this ticket that's causing problems was that the validation executor was bounded, which obviously isn't the case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow filtering on non-primary-key base column for MV,CASSANDRA-13798,13097486,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,25/Aug/17 03:02,12/Mar/19 14:19,13/Mar/19 22:34,31/Aug/17 10:43,3.11.1,4.0,,,,Feature/Materialized Views,,,,,0,,,,"We should probably consider disallow filtering conditions on non-primary-key base column for Materialized View which is introduced in CASSANDRA-10368.

The main problem is that the liveness of view row is now depending on multiple base columns (multiple filtered non-pk base column + base column used in view pk) and this semantic could not be properly support without drastic storage format changes. (SEE CASSANDRA-11500, [background|https://issues.apache.org/jira/browse/CASSANDRA-11500?focusedCommentId=16119823&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16119823])

We should step back and re-consider the non-primary-key filtering feature together with supporting multiple non-PK cols in MV clustering key in CASSANDRA-10226.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13956,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-25 12:40:13.314,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 31 12:11:26 UTC 2017,,,,,,0|i3j9pr:,9223372036854775807,,,,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,"25/Aug/17 11:08;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13798-trunk] |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13798-3.11] |

The patch is to revert supporting non-pk base column filtering on MV on 3.11 & trunk.  The feature was added in 3.10 and the consequence may have been overlooked. It's better to revert it in 3.x/trunk, before more users start using it.

At the same time, I will work on a proper design (eg. multi-timestamp approach discussed in CASSANDRA-11500 and CASSANDRA-10226) for non-pk base column filtering and multiple non-pk base column in view pk.

[~pauloricardomg] [~brstgt] [~KurtG] what do you think?","25/Aug/17 12:40;JoshuaMcKenzie;bq. The feature was added in 3.10
bq. revert it in 3.x/trunk, before more users start using it.
What, if anything, do we want to do for the theoretical user on 3.10/3.11 that's created an MV w/this schema?","25/Aug/17 14:03;brstgt;Even if the current implementation has known issues, you cannot kill that (or any other) feature just like that. As [~JoshuaMcKenzie] mentioned, how do you treat existing installations + schemas?
If I was affected (I really have to check this) this would either force me to change my schema or to be blocked on updates. Both is not viable if the current solution works for my needs. For example I am not really affected if I have an insert-only payload or if my data does not expire.

What you of course can do: 
Spit our a warning in the logs on bootstrap if the schema is affected and on schema changes that are affected and refer to a JIRA. So one can decide to stay with it or to migrate the schema / model to be not affected any more.

My 2 cents.","25/Aug/17 15:29;pauloricardomg;bq.  If I was affected (I really have to check this) this would either force me to change my schema or to be blocked on updates. Both is not viable if the current solution works for my needs. For example I am not really affected if I have an insert-only payload or if my data does not expire.

It seems from a quick peek at the patch that this just forbids creation of new tables with non-PK conditions, but does not break existing tables. So this just changes the UX to prevent news users from shooting themselves in the foot, but keeps the underlying machinery to support it (which will probably be reused when the feature is fully supported). Perhaps we can even have a hidden system flag to allow table creation if you ""know what you are doing""© and you can guarantee there are no column updates. In any case, we should explain the reasoning behind the revert and maybe expose the system flag for append-only workloads on NEWS.txt.","25/Aug/17 15:48;jasonstack;This ticket is to block creating new table with non-pk base column filtering.

All internal filter related logic are still there for supporting filter conditions on base pk. (existed even before CASSANDRA-10368)

Existing users of 3.10/3.11 with non-pk base column should still be able to successfully upgrade and use the broken non-pk base column filtering.( probably, we should give them an option to avoid filtering as well..). ","28/Aug/17 02:01;KurtG;I think something should be done on 3.11 and blocking the behaviour with a workaround flag as [~pauloricardomg] suggested is probably the best way to go about it. MV's are already causing a lot of issues for people doing dev on 3.11, and I know we have users with this case, that will undoubtedly run into issues sooner or later.

However I'm not convinced about also committing this to 4.0. If we're not committing to fixing it in 4.0 then MV's will be pretty useless until 5.0 if it does take massive storage changes, which could be a long way away. Which will be problematic for people who have already gone down this path, and we'll be missing a much wanted feature for quite a long time. IMO we made the mistake of pushing it out too fast, we should fix it asap.","28/Aug/17 07:38;pauloricardomg;bq. However I'm not convinced about also committing this to 4.0. If we're not committing to fixing it in 4.0 then MV's will be pretty useless until 5.0 if it does take massive storage changes, which could be a long way away. 

The storage changes required to support this are probably a new SSTable format + changes in the binary protocol, so while it requires a major version, it doesn't need to be 5.0, but 4.1 if we are not able to ship this by 4.0.

bq.  Which will be problematic for people who have already gone down this path, and we'll be missing a much wanted feature for quite a long time. IMO we made the mistake of pushing it out too fast, we should fix it asap.

We should definitely fix this ASAP, but adding this to trunk doesn't prevent that in any way, we're just adding this limitation while the problem is not fixed. As a follow-up to this, we should create another ticket to implement this feature properly. Whether that ticket should block the 4.0 release it's up for the community to decide, but on the meantime the code will be correct in not allowing people to use broken functionality.","29/Aug/17 01:48;KurtG;Right OK. If we can do it in 4.1 that's fine. It seems we don't actually have a difference between major and minor then? My (incorrect) assumption was that massive storage changes constituted a major version change, and 4.x would be intended to be minor.


","29/Aug/17 03:52;jasonstack;Updated NEWS.txt and introduced a flag: {{cassandra.mv.filtering.nonkey.columns=false}}.

If users ""know what they are doing"" ,eg. append-only, they could turn on the flag to create MV with non-pk column filtering.
","29/Aug/17 08:07;pauloricardomg;bq. Right OK. If we can do it in 4.1 that's fine. It seems we don't actually have a difference between major and minor then? My (incorrect) assumption was that massive storage changes constituted a major version change, and 4.x would be intended to be minor.

Hmm it depends whether the tick tock model is happening or not after 4.0, I'm not sure what the consensus was with that. But I meant that sstable version changes are normally during majors, and assumed 4.1 would be a major in the non-tick model while 4.0.1 would be a minor (following the previous model). In any case, this is just to leave things at a consistent state in case the community decides to release 4.0 without waiting for this feature to be fully enabled, but doesn't prevent us to removing this limitation if the feature is correctly implemented by 4.0 release timeframe. We will create another ticket to properly enable this feature following this.","31/Aug/17 10:42;pauloricardomg;bq. Updated NEWS.txt and introduced a flag: cassandra.mv.filtering.nonkey.columns=false.

Thanks, the patch LGTM. I just updated the flag to {{cassandra.mv.allow_filtering_nonkey_columns_unsafe}} (added unsafe to make sure it's not safe unless you know what you are doing, and replaced dots for underscores, since dot is usually used to separate the flag component but not the flag name), hope that's OK.

Committed to cassandra-3.11 and merged to trunk as {{425880ffb2e6bd5aeaa509fdbfa553db58b74c43}}. Thank you!

Created CASSANDRA-13832 to re-enable and remove the flag after we have CASSANDRA-13826.",31/Aug/17 12:11;jasonstack;Thanks for reviewing and feedback,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load is over calculated after each IndexSummaryRedistribution,CASSANDRA-13738,13092049,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,03/Aug/17 01:23,12/Mar/19 14:19,13/Mar/19 22:34,05/Sep/17 08:50,2.2.11,3.0.15,3.11.1,4.0,,Legacy/Core,,,,,0,,,,"For example, here is one of our cluster with about 500GB per node, but {{nodetool status}} shows far more load than it actually is and keeps increasing, restarting the process will reset the load, but keeps increasing afterwards:
{noformat}
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack
UN  IP1*****       13.52 TB   256          100.0%            c4c31e0a-3f01-49f7-8a22-33043737975d  rac1
UN  IP2*****       14.25 TB   256          100.0%            efec4980-ec9e-4424-8a21-ce7ddaf80aa0  rac1
UN  IP3*****       13.52 TB   256          100.0%            7dbcfdfc-9c07-4b1a-a4b9-970b715ebed8  rac1
UN  IP4*****       22.13 TB   256          100.0%            8879e6c4-93e3-4cc5-b957-f999c6b9b563  rac1
UN  IP5*****       18.02 TB   256          100.0%            4a1eaf22-4a83-4736-9e1c-12f898d685fa  rac1
UN  IP6*****       11.68 TB   256          100.0%            d633c591-28af-42cc-bc5e-47d1c8bcf50f  rac1
{noformat}

!sizeIssue.png|test!

The root cause is if the SSTable index summary is redistributed (typically executes hourly), the updated SSTable size is added again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Aug/17 01:28;jay.zhuang;sizeIssue.png;https://issues.apache.org/jira/secure/attachment/12880147/sizeIssue.png,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-08-03 01:53:18.436,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,,,Tue Sep 05 08:47:53 UTC 2017,,,,,,0|i3icnj:,9223372036854775807,2.2.x,3.0.x,3.11.0,4.x,,,,krummas,krummas,,,,,,,,,,"03/Aug/17 01:53;jjirsa;Are you aware yet if this is a new regression? If so, when was it introduced?
","03/Aug/17 03:38;KurtG;This has been around for a long time - haven't had the opportunity to find out what the exact cause was but this makes sense. I've definitely seen it in 3.7. Pretty sure I've also seen it in 3.0 and 2.1 as well. I don't think it happens in all versions, or at least for some reason it doesn't happen on all clusters.","03/Aug/17 05:56;jay.zhuang;I think the problem has been there for awhile. It only happens when [the IndexSummary is rebuilt|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/IndexSummaryRedistribution.java#L129], which is triggered by [large read traffic load changing|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/IndexSummaryRedistribution.java#L202]. I'm able to reproduce it with an unittest. Here is the patch, please review:
| branch | utest |
| [13738-2.2|https://github.com/cooldoger/cassandra/tree/13738-2.2] | [circleci#56|https://circleci.com/gh/cooldoger/cassandra/56] |
| [13738-3.0|https://github.com/cooldoger/cassandra/tree/13738-3.0] | [circleci#54|https://circleci.com/gh/cooldoger/cassandra/54] |
| [13738-3.11|https://github.com/cooldoger/cassandra/tree/13738-3.11] | [circleci#51|https://circleci.com/gh/cooldoger/cassandra/51] |
| [13738-trunk|https://github.com/cooldoger/cassandra/tree/trunk] | [circleci#57|https://circleci.com/gh/cooldoger/cassandra/57] |

Seems branch {{2.1}} don't have this issue.","04/Aug/17 00:31;jay.zhuang;I deployed the change to the cluster which has the problem. Confirmed the issue has been fixed. It has been running for more than 6 hours, so far looks fine.
[~jjirsa] do you mind reviewing the patch?","04/Aug/17 01:40;jjirsa;I'll take review but I'm at least two weeks away from getting to it

If anyone else beats me to it I won't mind
","06/Aug/17 18:57;jay.zhuang;Updated unittest to make it stable:
| branch | utest |
| [13738-2.2|https://github.com/cooldoger/cassandra/tree/13738-2.2] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-2.2.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-2.2] |
| [13738-3.0|https://github.com/cooldoger/cassandra/tree/13738-3.0] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.0.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.0] |
| [13738-3.11|https://github.com/cooldoger/cassandra/tree/13738-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.11] |
| [13738-trunk|https://github.com/cooldoger/cassandra/tree/13738-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-trunk] |
","27/Aug/17 20:26;jjirsa;[~iamaleksey] - are you willing to take review on this?

",29/Aug/17 12:16;iamaleksey;[~jjirsa] Not my strongest area of the codebase. Maybe [~krummas] has some spare cycles?,29/Aug/17 12:18;krummas;sure,"30/Aug/17 08:03;krummas;lgtm

running dtests just to be sure:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/237/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/238/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/239/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/240/","30/Aug/17 15:29;iamaleksey;[~jay.zhuang] You aren't running all the unit tests, FYI - because there is no way to get a green run currently. You have parallelism set to 1 instead of 4, which skips long-test, test-compression, and stress-test. Should set it to 4 and rerun.","30/Aug/17 17:12;jay.zhuang;[~iamaleksey] Thanks for the reminder, updated setting and rerunning the tests.","01/Sep/17 03:36;jay.zhuang;Yeah, all the builds are failing after the parallelism is set to 4 :(, rebased the code and updated the unittest:
| branch | utest |
| [13738-2.2|https://github.com/cooldoger/cassandra/tree/13738-2.2] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-2.2.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-2.2] |
| [13738-3.0|https://github.com/cooldoger/cassandra/tree/13738-3.0] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.0.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.0] |
| [13738-3.11|https://github.com/cooldoger/cassandra/tree/13738-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-3.11] |
| [13738-trunk|https://github.com/cooldoger/cassandra/tree/13738-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13738-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13738-trunk] |
","01/Sep/17 16:16;jay.zhuang;2.2 branch uTest fail for {{ant eclipse-warnings}}, but I'm unable to reproduce it locally:
{noformat}
eclipse-warnings:
    [mkdir] Created dir: /home/ubuntu/cassandra/build/ecj
     [echo] Running Eclipse Code Analysis.  Output logged to /home/ubuntu/cassandra/build/ecj/eclipse_compiler_checks.txt
     [java] incorrect classpath: /home/ubuntu/cassandra/build/cobertura/classes
     [java] ----------
     [java] 1. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/db/compaction/CompactionManager.java (at line 853)
     [java] 	ISSTableScanner scanner = cleanupStrategy.getScanner(sstable, getRateLimiter());
     [java] 	                ^^^^^^^
     [java] Resource 'scanner' should be managed by try-with-resource
     [java] ----------
     [java] ----------
     [java] 2. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java (at line 257)
     [java] 	scanners.add(new LeveledScanner(intersecting, range));
     [java] 	             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     [java] Potential resource leak: '<unassigned Closeable value>' may not be closed
     [java] ----------
     [java] ----------
     [java] 3. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/tools/SSTableExport.java (at line 315)
     [java] 	ISSTableScanner scanner = reader.getScanner();
     [java] 	                ^^^^^^^
     [java] Resource 'scanner' should be managed by try-with-resource
     [java] ----------
     [java] 3 problems (3 errors)
{noformat}
And for the other test failures, I don't think they're introduced by this patch.","05/Sep/17 08:47;krummas;failing dtests pass locally

committed as {{4e834c53ca57910e8c4}}, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix AssertionError in short read protection,CASSANDRA-13747,13092855,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,07/Aug/17 11:15,12/Mar/19 14:19,13/Mar/19 22:34,29/Aug/17 11:37,3.0.15,3.11.1,,,,Legacy/Coordination,,,,,0,Correctness,,,"{{ShortReadRowProtection.moreContents()}} expects that by the time we get to that method, the global post-reconciliation counter was already applied to the current partition. However, sometimes it won’t get applied, and the global counter continues counting with {{rowInCurrentPartition}} value not reset from previous partition, which in the most obvious case would trigger the assertion we are observing - {{assert !postReconciliationCounter.isDoneForPartition();}}. In other cases it’s possible because of this lack of reset to query a node for too few extra rows, causing unnecessary SRP data requests.

Why is the counter not always applied to the current partition?

The merged {{PartitionIterator}} returned from {{DataResolver.resolve()}} has two transformations applied to it, in the following order:
{{Filter}} - to purge non-live data from partitions, and to discard empty partitions altogether (except for Thrift)
{{Counter}}, to count and stop iteration

Problem is, {{Filter}} ’s {{applyToPartition()}} code that discards empty partitions ({{closeIfEmpty()}} method) would sometimes consume the iterator, triggering short read protection *before* {{Counter}} ’s {{applyToPartition()}} gets called and resets its {{rowInCurrentPartition}} sub-counter.

We should not be consuming iterators until all transformations are applied to them. For transformations it means that they cannot consume iterators unless they are the last transformation on the stack.

The linked branch fixes the problem by splitting {{Filter}} into two transformations. The original - {{Filter}} - that does filtering within partitions - and a separate {{EmptyPartitionsDiscarder}}, that discards empty partitions from {{PartitionIterators}}. Thus {{DataResolve.resolve()}}, when constructing its {{PartitionIterator}}, now does merge first, then applies {{Filter}}, then {{Counter}}, and only then, as its last (third) transformation - the {{EmptyPartitionsDiscarder}}. Being the last one applied, it’s legal for it to consume the iterator, and triggering {{moreContents()}} is now no longer a problem.

Fixes: [3.0|https://github.com/iamaleksey/cassandra/commits/13747-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/13747-3.11], [4.0|https://github.com/iamaleksey/cassandra/commits/13747-4.0]. dtest [here|https://github.com/iamaleksey/cassandra-dtest/commits/13747].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13794,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-17 08:21:18.289,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 29 11:37:22 UTC 2017,,,,,,0|i3ihjj:,9223372036854775807,,,,,,,,benedict,benedict,,,,,,,,,,"07/Aug/17 23:42;iamaleksey;Stack trace for the triggered assertion:

{code}
java.lang.AssertionError
	at org.apache.cassandra.service.DataResolver$ShortReadProtection$ShortReadRowProtection.moreContents(DataResolver.java:449)
	at org.apache.cassandra.service.DataResolver$ShortReadProtection$ShortReadRowProtection.moreContents(DataResolver.java:412)
	at org.apache.cassandra.db.transform.BaseIterator.tryGetMoreContents(BaseIterator.java:111)
	at org.apache.cassandra.db.transform.BaseIterator.hasMoreContents(BaseIterator.java:101)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:155)
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:369)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:189)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:158)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:509)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:369)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129)
	at org.apache.cassandra.db.transform.FilteredRows.isEmpty(FilteredRows.java:50)
	at org.apache.cassandra.db.transform.Filter.closeIfEmpty(Filter.java:73)
	at org.apache.cassandra.db.transform.Filter.applyToPartition(Filter.java:43)
	at org.apache.cassandra.db.transform.Filter.applyToPartition(Filter.java:26)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96)
	at org.apache.cassandra.service.StorageProxy$SingleRangeResponse.computeNext(StorageProxy.java:2200)
	at org.apache.cassandra.service.StorageProxy$SingleRangeResponse.computeNext(StorageProxy.java:2172)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:92)
	at org.apache.cassandra.service.StorageProxy$RangeCommandIterator.computeNext(StorageProxy.java:2243)
	at org.apache.cassandra.service.StorageProxy$RangeCommandIterator.computeNext(StorageProxy.java:2210)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:92)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:707)
{code}","17/Aug/17 08:21;benedict;The patch looks good, but while reviewing I got a little suspicious of the modified line {{DataResolver:479}}, as it seemed that {{n}} and {{x}} were the wrong way around... and, reading the comment of intent directly above, and reproducing the calculation, they are indeed.

-Assuming, now correctly defined, that {{n <= x}}, this also obviates the need for the {{Math.max(x, 1)}} you have introduced.  This must be true, given that we can only have a short-read triggered in the case that we have yielded too few rows, so we must have fewer than we requested (even if other rows we didn't know about were introduced by other peers).-

This is _probably_ a significant enough bug that it warrants its own ticket for record keeping, though I'm fairly agnostic on that decision.  

I'm a little concerned about our current short read behaviour, as right now it seems we should be requesting exactly one row, for any size of under-read, which could mean extremely poor performance in case of large under-reads.

I would suggest that the outer unconditional {{Math.max}} is a bad idea, has been (poorly) insulating us from this error, and that we should first be asserting that the calculation yields a value {{>= 0}} before setting to 1.","24/Aug/17 12:38;iamaleksey;Thanks for the review.

I've decided to tackle the related issue you in a [separate ticket|https://issues.apache.org/jira/browse/CASSANDRA-13794].

Will commit this once tests are happy.","29/Aug/17 11:37;iamaleksey;Didn’t get a clean CI run, but these are the issues flagged:

On 3.0 and 3.11, {{RemoveTest}} failed due to a port being used by another process. Passed locally, with and without compression.
On 3.0, {{ClientWarningsTest}} timed out. Passed locally.

5 dtest failures in 3.0 with unrelated failure reasons. Unfortunately currently unable to get a clean run on ASF Jenkins.

Committed to 3.0 as [a7cb009f8a3f4d0e0293111bfcfff3d404a37a89|https://github.com/apache/cassandra/commit/a7cb009f8a3f4d0e0293111bfcfff3d404a37a89] and merged with 3.11 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix buffer length comparison when decompressing in netty-based streaming,CASSANDRA-13899,13104716,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,pauloricardomg,pauloricardomg,25/Sep/17 10:16,12/Mar/19 14:19,13/Mar/19 22:34,04/Oct/17 07:05,4.0,,,,,,,,,,0,,,,"Streaming a single partition with ~100K rows fails with the following exception:

{noformat}
ERROR [Stream-Deserializer-/127.0.0.1:35149-a92e5e12] 2017-09-21 04:03:41,237 StreamSession.java:617 - [Stream #c2e5b640-9eab-11e7-99c0-e9864ca8da8e] Streaming error occurred on session with peer 127.0.0.1
org.apache.cassandra.streaming.StreamReceiveException: java.lang.RuntimeException: Last written key DecoratedKey(-1000328290821038380) >= current key DecoratedKey(-1055007227842125139)  writing into /home/paulo/.ccm/test/node2/data0/stresscql/typestest-482ac7b09e8d11e787cf85d073c
8e037/na-1-big-Data.db
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:63) ~[main/:na]
        at org.apache.cassandra.streaming.messages.IncomingFileMessage$1.deserialize(IncomingFileMessage.java:41) ~[main/:na]
        at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:55) ~[main/:na]
        at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:178) ~[main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{noformat}

Reproduction steps:
 * Create CCM cluster with 2 nodes
* Start only first node, disable hinted handoff
 * Run stress with the attached yaml: {{tools/bin/cassandra-stress ""user profile=largepartition.yaml n=10K ops(insert=1) no-warmup -node whitelist 127.0.0.1 -mode native cql3 compression=lz4 -rate threads=4 -insert visits=FIXED(100K) revisit=FIXED(100K)""}}
* Start second node, run repair on {{stresscql}} table - the exception above will be thrown.

I investigated briefly and haven't found anything suspicious. This seems to be related to CASSANDRA-12229 as I tested the steps above in a branch without that and the repair completed successfully. I haven't tested with a smaller number of rows per partition to see at which point it starts to be a problem.

We should probably add a regression dtest to stream large partitions to catch similar problems in the future.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Sep/17 10:17;pauloricardomg;largepartition.yaml;https://issues.apache.org/jira/secure/attachment/12888815/largepartition.yaml,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-09-26 01:06:56.716,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 04 07:06:46 UTC 2017,,,,,,0|i3khl3:,9223372036854775807,4.0,,,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,25/Sep/17 10:18;pauloricardomg;(cc [~jasobrown]),26/Sep/17 01:06;jasobrown;[~pauloricardomg] Thanks for the scripts and instructions - I was able to reproduce. Will have a patch hopefully in a day or two,"26/Sep/17 21:32;jasobrown;This turned out to be a simple, stupid bug - i used the wrong length to compare against when translating from CASSANDRA-10520.

Patch here:

||13899||
|[branch|https://github.com/jasobrown/cassandra/tree/13899]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/337/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13899]|

[~pauloricardomg] Do you mind reviewing?
","27/Sep/17 01:21;jasobrown;tbh, I don't think it's large partitions that necessarily triggered this bug; streaming just transfers the bytes of sstable Data file naively. I think it was something in the dataset and the way it compresses that exposed the bug. Thus, I'm not sure how to proceed with a dtest, especially one based on [~pauloricardomg]'s submitted patch (as highly useful as it is!).","28/Sep/17 03:07;pauloricardomg;bq. tbh, I don't think it's large partitions that necessarily triggered this bug; streaming just transfers the bytes of sstable Data file naively. I think it was something in the dataset and the way it compresses that exposed the bug. Thus, I'm not sure how to proceed with a dtest, especially one based on Paulo Motta's submitted patch (as highly useful as it is!).

Good point, I managed to reproduce this with 1k entries with the same YAML by setting {{chunk_length_in_kb=1}}. Maybe it's possible to reproduce with even fewer entries and the default generator with a large payload size and a very low {{chunk_length_in_kb}}. Would you mind trying that?

The patch looks good, but would be nice to have a regression dtest for this, since it's pretty subtle to catch.",28/Sep/17 13:44;jasobrown;[~pauloricardomg] Good call on dropping the {{chunk_length_in_kb}} down to 1 for the dtest. I can trigger the error on trunk with it and a much lower insert count in under 80 seconds total. I've turned it into a [dtest|https://github.com/jasobrown/cassandra-dtest/tree/13899]. Please review and let me know what you think.,03/Oct/17 19:26;jasobrown;[~pauloricardomg] do you think you can finish up the review on this soonish?,"04/Oct/17 02:57;pauloricardomg;I am away this week but this is simple enough so I managed to have a quick look during vacation. ;)

bq. I can trigger the error on trunk with it and a much lower insert count in under 80 seconds total. I've turned it into a dtest.

Awesome, dtest looks good to me - verified that it fails without the patch and passes with it - thanks! Marking as ready to commit.","04/Oct/17 07:05;jasobrown;Holy crap - I didn't know you were vacationing. Sorry about that, but thanks soo much for the review!!

committed as sha {{d080a73723d9aa402507c1ae04eef92ff6d44948}} to the cassandra repo, and as sha {{6ea3964d18b54b4e23b6e7ebf63ca42080e8404b}} to the dtests repo.",04/Oct/17 07:06;jasobrown;updated the ticket title to more accurately describe the problem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TriggerExecutor ignored original PartitionUpdate,CASSANDRA-13894,13104226,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,22/Sep/17 09:08,12/Mar/19 14:19,13/Mar/19 22:34,22/Sep/17 14:46,3.0.15,3.11.1,4.0,,,Legacy/Local Write-Read Paths,,,,,0,,,,"Since 3.0, the [TriggerExecutor.execute(PartitionUpdate)|https://github.com/jasonstack/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/triggers/TriggerExecutor.java#L82-L89] will only return augmented mutation, ignoring original update..

[Test|https://github.com/jasonstack/cassandra/commit/eb28844035242c4cb73c5148254f34672f2325da]to reproduce.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-22 14:45:33.89,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 22 15:54:27 UTC 2017,,,,,,0|i3kekv:,9223372036854775807,,,,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,"22/Sep/17 09:28;jasonstack;| source |
| [trunk|https://github.com/jasonstack/cassandra/commits/trigger-missing-update] |
| [3.11|https://github.com/jasonstack/cassandra/commits/trigger-missing-update-3.11] |
| [3.0|https://github.com/jasonstack/cassandra/commits/trigger-missing-update-3.0] |

CI is running.

changes:  concatenate original PartitionUpdate with augmented updates.","22/Sep/17 12:59;jasonstack;CircleCI passed. Failed some tests Internal CI , but seems not related, mostly on repair/sasi..",22/Sep/17 14:45;pauloricardomg;Good catch! Fix LGTM. Committed as {{51e6f2446e71c8bd2ce89480b7d30d5b9ed1546e}} and merge up to cassandra-3.11 and trunk. Thanks!,22/Sep/17 15:54;jasonstack;Thanks for reviewing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow existing nodes to use all peers in shadow round,CASSANDRA-13851,13100446,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,KurtG,KurtG,KurtG,07/Sep/17 12:36,12/Mar/19 14:19,13/Mar/19 22:34,17/Apr/18 17:16,3.11.3,4.0,,,,Local/Startup and Shutdown,,,,,0,,,,"In CASSANDRA-10134 we made collision checks necessary on every startup. A side-effect was introduced that then requires a nodes seeds to be contacted on every startup. Prior to this change an existing node could start up regardless whether it could contact a seed node or not (because checkForEndpointCollision() was only called for bootstrapping nodes). 

Now if a nodes seeds are removed/deleted/fail it will no longer be able to start up until live seeds are configured (or itself is made a seed), even though it already knows about the rest of the ring. This is inconvenient for operators and has the potential to cause some nasty surprises and increase downtime.

One solution would be to use all a nodes existing peers as seeds in the shadow round. Not a Gossip guru though so not sure of implications.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-15 15:15:18.298,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 18 00:04:56 UTC 2018,,,,,,0|i3jrdz:,9223372036854775807,3.6,,,,,,,beobal,beobal,,,,,,,,,,"15/Sep/17 15:15;beobal;Seeds have a secondary purpose in gossip, to help shorten convergence time (though this is a little contentious, see CASSANDRA-9206). 
If a node is restarted and cannot contact any seeds (especially if the configured seeds have been removed from the cluster), it's probably a good thing that operators are aware of that. ","15/Sep/17 17:00;jjirsa;Agree with [~KurtG] here that this is a regression. We should be able to start if all the seeds are down. Imagine doing a full cluster bounce (either all nodes at once, or one of each replica set), there's a pretty good chance you'll end up in a weird state trying to order restart based on seeds first, which is not ideal.","18/Sep/17 05:08;KurtG;Certainly agree we should make operators aware of that. Let's most definitely not do that by surprise ""you can't start C*"". Those are the worst types of surprise. Just needs a warning on startup that no live seeds are configured IMO.
I'll probably have a crack at this after NGCC, not going to assign myself to it yet though in case someone else wants to have at it.","01/Nov/17 10:26;KurtG;|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:13851]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:3.11-13851]|

Current changes pass through whatever peers exist in system.peers to the shadow round and will only use them if the first round of gossips to the seeds fail. So it will still try the seeds first, it's just if the shadow round goes longer than 5 seconds will peers be included. This allows a node to start if only peers are alive.
It also allows a node that doesn't need to bootstrap to start, even if it couldn't contact any peers or seeds. 

From what I can tell this works but open to ideas on other test cases/obvious things I've missed.

FWIW this also passed all the bootstrap dtests, if that means anything.

edit: updated dtest to actually fail when there is a timeout, rather than throwing an error.","30/Nov/17 21:16;jjirsa;Who wants to review a gossip patch? [~jasobrown] or [~jkni], you two have touched it most recently?
","01/Dec/17 13:28;jasobrown;I can add it to my (ever growing) queue, but no guarantees on promptness.","14/Dec/17 20:31;beobal;This looks pretty reasonable, but the thing that concerns me is that the 'starting unsafely' path can be reached in 2 ways:
1) when a new node comes up and has no seeds (other than itself) configured, and no persisted ring info. 
2) when a non-bootstrapping node starts up with seeds and/or peers it *could* contact, but it's unable to reach any of them within RING_DELAY.

I'm not worried about the former case, it is necessary for starting a single node cluster (or the first node in a multi node cluster if starts are staggered) and is also existing behaviour post CASSANDRA-10134.
The latter case though could lead to a regression of CASSANDRA-10134. If a new node comes up with the same address as an existing down node, and it is not bootstrapping (because it already bootstrapped, auto_bootstrap: false, or it's present in its own seed list) it will take over that address if it cannot perform the shadow round i.e. if partitioned.

bq. Imagine doing a full cluster bounce

There is already a mechanism in place to work around a whole cluster bounce. If the whole cluster is restarted, all seeds would be in their own shadow rounds. They report this to the any node contacting them and if all seeds are found to be in that state, the node exits its own shadow round and continues to start. In large clusters, you might have to increase RING_DELAY to give all seeds a chance to enter their shadow round before other nodes start to time theirs out.

Extending the shadow round targets to include known peers works well for the scenario where all seeds are down for a longer period, so I'm +1 on that. I just think that to avoid regression of 10134 we still need to hard fail if a node coming up is not able to contact any seeds *or* peers at all (unless case 1 above applies).

nits:
* 1 arg version of checkForEndpointCollision is unused
* Having 2 log statements at the end of doShadowRound is a bit meaningless, as in both cases we proceed even though the state of the cluster is unknown.
","18/Dec/17 04:01;KurtG;|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:13851]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:3.11-13851]|[utests|https://circleci.com/gh/kgreav/cassandra/51]|

Updated the patch so that a node that's not a seed won't be able to start if it can't contact any peers. TBH I don't think this is the best solution as it's still in part a regression on the old behaviour and requires you startup seeds first in a full cluster bounce, but if anyone still sees that as a major issue it can be done in a separate ticket as it's likely a bit of refactoring.

dtest has also been updated.","03/Jan/18 16:09;beobal;I'm +1 on this latest version, though it occurs to me that there is something else we could do to help full cluster bounces that are done in one shot (per-replica set or otherwise partial bounces will now proceed ok).

Failure to receive an ack within RING_DELAY will terminate the shadow round, fatally for a node not in it's own seed list. So if we make non-seeds remain in the SR for longer than seeds, (e.g. for RING_DELAY * 2), then as long as a single seed is contactable, startup should be able to proceed.
 
e.g. all peers have nodes 1, 2 & 3 configured as seeds, but 2 & 3 have failed. If the cluster is completely stopped and restarted, node1 will exit its SR after RING_DELAY and be available to ack the other nodes' syn requests. Once other, non-seeds start to come up, they will also now ack shadow round syns. 
This would increase startup times for a full bounce when some seeds are failing/missing, but in ""normal"" circumstances it would have no impact. 
It wouldn't help if all of the seeds 1, 2 & 3 were down during a full bounce, but I'd consider that tradeoff acceptable.
","17/Jan/18 02:05;KurtG;|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:13851]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:3.11-13851]|[utests|https://circleci.com/gh/kgreav/cassandra/54]|

Added another quick dtest to make sure that if a non-seed starts it will still join as long as the seed is started within {{RING_DELAY * 2}}.","14/Mar/18 03:09;KurtG;[~beobal] have you had a chance to review the latest? I've rebased both dtests and patch and also uploaded a branch for trunk (now that it no longer merges).
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:3.11-13851]|[trunk|https://github.com/apache/cassandra/compare]|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:13851]|
|[utests|https://circleci.com/gh/kgreav/cassandra/135]|[utests|https://circleci.com/gh/kgreav/cassandra/137]|","14/Mar/18 12:51;beobal;[~KurtG] sorry, I totally missed the last update, I'll try my best to look at it today.","14/Mar/18 17:21;beobal;Thanks for adding the extra delay for non-seeds, that all LTGM so I've kicked off some dtest runs on CircleCI. If they look good I'll commit everything and update here.
|3.11|[circleci workflow|https://circleci.com/workflow-run/b931896e-6ebb-4513-8f0c-7b6bb8486ee0]|
|trunk|[circleci workflow|https://circleci.com/workflow-run/36423e6e-1763-4274-b923-d78d2947a812]|

There were a couple of tweaks I made to the dtest, lmk if those are ok with you: [https://github.com/beobal/cassandra-dtest/commit/94dee685e10f8819752d7c27533ba8f59b7acb9b]

One small request for future stuff, it makes reviewing iterations on patches much easier if you don't rebase/squash once the review is ongoing. Additional commits + periodically merging from the base branch makes it a lot simpler to pick out the incremental changes during the course of a review. Thanks!","15/Mar/18 02:56;KurtG;Ah sure thing. Sorry about that.

dtest changes LGTM but looks like {{test_startup_non_seed_with_peers}} is failing on trunk (unrelated to changes). I'll take a look.","04/Apr/18 10:42;KurtG;[~beobal] Sorry for the delay, went on holidays and just got around to fixing the test. Turns out it was failing because the log message changed ever so slightly after CASSANDRA-7544 by removing the leading slash on the IP address. I also applied your dtest changes to my branch and made some minor formatting changes.

|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:3.11-13851]|[utests|https://circleci.com/gh/kgreav/cassandra/150]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:13851-trunk]|[utests|https://circleci.com/gh/kgreav/cassandra/148]|
|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:13851]|",12/Apr/18 02:57;KurtG;ping [~beobal]. keen on getting this in before something breaks it again. :),"12/Apr/18 07:06;beobal;[~KurtG] ack. I did see the update, but have been a bit snowed under this past week. I'll get it done by Monday, latest.","17/Apr/18 17:16;beobal;Thanks [~KurtG], latest CI (after rebases) looks good committed so I've to cassandra-3.11 in {{28ccf3fe3989d9d80063fe4d4bb048efe471936b}} and merged to trunk. 
I'll get the dtest committed directly.",18/Apr/18 00:04;KurtG;Thanks heaps [~beobal]!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ref bug in Scrub,CASSANDRA-13873,13102367,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,tjake,tjake,14/Sep/17 19:59,12/Mar/19 14:19,13/Mar/19 22:34,11/Dec/17 08:08,2.2.12,3.0.16,3.11.2,4.0,,Legacy/Tools,,,,,0,,,,"I'm hitting a Ref bug when many scrubs run against a node.  This doesn't happen on 3.0.X.  I'm not sure if/if not this happens with compactions too but I suspect it does.

I'm not seeing any Ref leaks or double frees.

To Reproduce:

{quote}
./tools/bin/cassandra-stress write n=10m -rate threads=100
./bin/nodetool scrub
#Ctrl-C
./bin/nodetool scrub
#Ctrl-C
./bin/nodetool scrub
#Ctrl-C
./bin/nodetool scrub
{quote}

Eventually in the logs you get:
WARN  [RMI TCP Connection(4)-127.0.0.1] 2017-09-14 15:51:26,722 NoSpamLogger.java:97 - Spinning trying to capture readers [BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-5-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-32-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-31-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-29-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-27-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-26-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-20-big-Data.db')],
*released: [BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-5-big-Data.db')],* 

This released table has a selfRef of 0 but is in the Tracker
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11155,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-10-19 02:46:54.007,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 05 08:20:13 UTC 2018,,,,,,0|i3k35z:,9223372036854775807,3.10,3.11.0,4.0,,,,,jkni,jkni,,,,,,,,,,"14/Sep/17 20:10;tjake;I should probably mention this happens when we cancel compactions.  In 3.0 they would just wait till previous runs finished, now we cancel them.","19/Oct/17 02:46;jkni;It looks like this situation can occur when referencing canonical sstables. As far as I can tell, the issue reproduces only when we have an sstable in a lifecycle transaction with no referencers other than its selfref. If the lifecycle transaction updates this sstable, we'll put a new instance of the sstable reader in the tracker. This causes no problems when getting live sstables, but the canonical sstables can also include sstable readers from the compacting set. In this case, the sstable reader that got updated will still be in the compacting set, but we won't be able to reference it when we try to select and reference canonical sstables, since its instance tidier has run when its last ref was released in the lifecycle transaction. Note that the global tidier doesn't run, since the updated sstable reader is still referenced. With the reproduction provided above in the multiple scrub, the scrubs will eventually proceed once the lifecycle transaction finishes, since it will put an updated sstablereader in the tracker. If there is a situation where a lifecyce transaction needed to select canonical sstables to proceed, this could cause a deadlock.

I pushed a branch at [c13873-2.2|https://github.com/jkni/cassandra/commit/ba70f70d97f648037e742a16bfdf1c8002d2be9c] that implements the simplest fix I can think of. The patch references the original sstables involved in a lifecycle transaction when we create the transaction, releasing these references whenever we do postCleanup or cancel an sstable reader from a transaction. I merged this forward and tests came back clean on all active branches. I'm not sure if there is some existing mechanism that should cover this case - maybe [~krummas] knows from reviewing [CASSANDRA-9699]?","19/Oct/17 11:03;krummas;bq. the scrubs will eventually proceed once the lifecycle transaction finishes,
but shouldn't cancelling the scrub-compaction also finish the txn?

I'll take review unless you had someone else lined up - -is this still ""critical"" after your analysis [~jkni]-? Edit: yup, looks like it","19/Oct/17 14:05;jkni;You're correct that cancelling will also finish the txn and allow operations to select and reference canonical sstables. In the specific repro that Jake provided, which is the case of multiple scrubs over the same cfs (an admittedly somewhat artificial case), we'll try to select and reference canonical sstables in the snapshot before cancelling the original scrub compaction, so the new scrubs will hang until the original scrub finishes.

That'd be great if you could review. I'm admittedly very unfamiliar with this part of the code, so I expect my initial patch is a rough sketch of the eventual solution.

As far as criticality goes, I could go either way. I know of no situations that this causes data loss or permanent deadlocks at this time, but it can potentially cause operations referencing canonical sstables to hang for long periods of time.",19/Oct/17 14:32;krummas;looks like this is a problem way back in 2.1 as well and my fix in CASSANDRA-10829 only made the window where this can happen smaller,"20/Oct/17 12:47;krummas;correction, there is only a tiny race just as we finish up compactions in 2.1 - not worth fixing at this point","20/Oct/17 15:03;krummas;The problem seems to be that we don't grab a reference before starting the compaction everywhere - we do it in [CompactionTask|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java#L177], but not during [cleanup|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1095-L1098] for example.

[~jkni] I think we should do your fix (and also remove the external ref-grabbing in CompactionTask), but maybe only in trunk? It feels a tiny bit safer to not touch {{LifecycleTransaction}}/{{CompactionTask}} in older versions and instead take the ref outside where we need it, but I'm not entirely convinced, wdyt?
",20/Oct/17 15:17;jkni;+1 - I'd like to introduce as few changes as possible to older versions here. That combination sounds good to me. Do you want to prepare the patch for older versions or would you like me to?,"24/Oct/17 12:26;krummas;https://github.com/krummas/cassandra/commits/marcuse/13873
https://github.com/krummas/cassandra/commits/marcuse/13873-3.0
https://github.com/krummas/cassandra/commits/marcuse/13873-3.11

https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/387/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/388/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/389/

https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873-3.0
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873-3.11",29/Nov/17 13:39;krummas;ping [~jkni] - should we port the trivial patch to trunk as well and get this in? We could improve it in another ticket I guess,"30/Nov/17 00:56;jkni;Sorry for the latency here - my fault. The patch looks good to me. I considered a few other cases where a similar problem might exist. It seems to me the same issue could exist in in the Splitter/Upgrader, but since they're offline, I don't know what future changes would require another operation to reference canonical sstables in parallel. I also don't see anything in anticompaction grabbing a ref; am I missing something there?

The patches look good for existing cases. Unfortunately, I let the dtests age out before taking a closer look, but I can rerun them after you look at the question above. I'm +1 to merging the relatively trivial patches through to trunk and opening a ticket to improve it later. As you've seen, I don't have a huge amount of bandwidth for this right now, so I'd rather not delay a definite improvement with only the promise of a better one. Thanks for the patience.","30/Nov/17 08:40;krummas;bq. Splitter/Upgrader, but since they're offline
yeah, don't think we need it here, we don't open early when doing offline operations and we should have no concurrent operations referencing sstables

bq. I also don't see anything in anticompaction grabbing a ref;
looks like we do it [here|https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/service/ActiveRepairService.java#L565-L584]

I'll prepare the patches and start the dtests","30/Nov/17 09:00;krummas;https://github.com/krummas/cassandra/commits/marcuse/13873
https://github.com/krummas/cassandra/commits/marcuse/13873-3.0
https://github.com/krummas/cassandra/commits/marcuse/13873-3.11
https://github.com/krummas/cassandra/commits/marcuse/13873-trunk

https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/444/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/445/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/446/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/447/

https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873-3.0
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873-3.11
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13873-trunk","01/Dec/17 03:38;jkni;Thanks for the patches and CI. Both your remarks look correct to me; frankly, I have no idea how I missed that in anticompaction.

Test results look good for the most part. There's a few flaky unit tests on 3.0/3.11 that appear to have failed the same way before the patch, pass for me locally, and appear to be at the limits of CircleCI's timeouts/resources. The 2.2 dtests timed out, so it seems worthwhile to trigger those again just in case. The only unusual failures on 3.0 dtests are a bunch of tests where Jolokia failed to attach for JMX. I'm not sure if this is a known environmental problem on ASF dtests, but I was unable to reproduce this elsewhere.

Overall, +1 to the patch for me, and this looks good to merge if none of the test issues I raised above worry you.","08/Dec/17 15:22;krummas;finally got the 2.2 tests run properly and the failures look unrelated: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/456/

I'll get this committed (on monday..)",08/Dec/17 23:08;jkni;Great. Thanks!,"11/Dec/17 08:08;krummas;and committed as {{3cd2c3c4ea4286562b2cb8443d6173ee251e6212}}, thanks","11/Jan/18 13:47;tsteinmaurer;Any chance to get this back-ported also to 2.1.x? We are having a lots of 2.1 nodes out there in production with no immediate plan to upgrade, but e.g. we can't have a combination of {{nodetool cleanup}} (e.g. after extending the cluster with additional nodes) and an hourly cron job taking snapshots. See linked issue CASSANDRA-11155. Thanks.",05/Feb/18 05:41;KurtG;[~tsteinmaurer] I couldn't reproduce the issue on my end. I suspect you need at least a few large SSTables to trigger the issue. Can you try building from [this branch|https://github.com/kgreav/cassandra/tree/13873-2.1] which is just latest 2.1 with the simple backport on top and seeing if it fixes the issue?,"05/Feb/18 08:20;tsteinmaurer;[~KurtG], thanks for the follow-up. Unfortunately, I don't have a larger environment available to give your back-port a try at the moment, cause we have work-arounded the issue for our scaled out clusters beyond our replication factor by disabling and then re-enabling the hourly cron job taking snapshots.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StrictLiveness for view row is not handled in AbstractRow,CASSANDRA-13883,13102947,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,18/Sep/17 12:41,12/Mar/19 14:19,13/Mar/19 22:34,25/Sep/17 06:44,3.0.15,3.11.1,4.0,,,Feature/Materialized Views,,,,,0,,,,"In {{AbstractRow.hasLiveData(nowInSecond)}}, it doesn't handle {{strictLiveness}} introduced in CASSANDRA-11500. The {{DataLimits}} counts the expired view row as live data and then the expired view row is purged in {{Row.purge()}}. When query with limit, we will get less data.

{code:title=test to reproduce}
    @Test
    public void testRegularColumnTimestampUpdates() throws Throwable
    {
        createTable(""CREATE TABLE %s ("" +
                    ""k int PRIMARY KEY, "" +
                    ""c int, "" +
                    ""val int)"");

        execute(""USE "" + keyspace());
        executeNet(protocolVersion, ""USE "" + keyspace());

        createView(""mv_rctstest"", ""CREATE MATERIALIZED VIEW %s AS SELECT * FROM %%s WHERE k IS NOT NULL AND c IS NOT NULL PRIMARY KEY (k,c)"");

        updateView(""UPDATE %s SET c = ?, val = ? WHERE k = ?"", 0, 0, 0);
        updateView(""UPDATE %s SET val = ? WHERE k = ?"", 1, 0);
        updateView(""UPDATE %s SET c = ? WHERE k = ?"", 1, 0);
        assertRows(execute(""SELECT c, k, val FROM mv_rctstest""), row(1, 0, 1));

        updateView(""TRUNCATE %s"");

        updateView(""UPDATE %s USING TIMESTAMP 1 SET c = ?, val = ? WHERE k = ?"", 0, 0, 0);
        updateView(""UPDATE %s USING TIMESTAMP 3 SET c = ? WHERE k = ?"", 1, 0);
        updateView(""UPDATE %s USING TIMESTAMP 2 SET val = ? WHERE k = ?"", 1, 0);
        updateView(""UPDATE %s USING TIMESTAMP 4 SET c = ? WHERE k = ?"", 2, 0);
        updateView(""UPDATE %s USING TIMESTAMP 3 SET val = ? WHERE k = ?"", 2, 0);

        // FIXME no rows return
        assertRows(execute(""SELECT c, k, val FROM mv_rctstest limit 1""), row(2, 0, 2));
        assertRows(execute(""SELECT c, k, val FROM mv_rctstest""), row(2, 0, 2));
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-25 06:44:30.81,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 25 09:08:19 UTC 2017,,,,,,0|i3k6pj:,9223372036854775807,,,,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,"20/Sep/17 01:52;jasonstack;| source | unit | dtest |
| [trunk|https://github.com/apache/cassandra/compare/trunk...jasonstack:CASSANDRA-13883-trunk?expand=1]| [passed|https://circleci.com/gh/jasonstack/cassandra/627] |   repair_tests.repair_test.TestRepair.dc_parallel_repair_test
repair_tests.repair_test.TestRepair.dc_repair_test
repair_tests.repair_test.TestRepair.local_dc_repair_test
repair_tests.repair_test.TestRepair.simple_parallel_repair_test
repair_tests.repair_test.TestRepair.thread_count_repair_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_30_test
auth_test.TestAuth.system_auth_ks_is_alterable_test
disk_balance_test.TestDiskBalance.disk_balance_decommission_test
cdc_test.TestCDC.test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space
cdc_test.TestCDC.test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space|
| [3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...jasonstack:CASSANDRA-13883-3.11?expand=1] | [passed|https://circleci.com/gh/jasonstack/cassandra/625] | upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_30_test
auth_test.TestAuth.system_auth_ks_is_alterable_test |
| [3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...jasonstack:CASSANDRA-13883-3.0?expand=1] | [passed|https://circleci.com/gh/jasonstack/cassandra/628]| global_row_key_cache_test.TestGlobalRowKeyCache.functional_test
repair_tests.incremental_repair_test.TestIncRepair.multiple_repair_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_30_test |
| [dtest|https://github.com/apache/cassandra-dtest/compare/master...jasonstack:CASSANDRA-13883?expand=1] |

CI looks good, failure seems unrelated.

{code}
Changes:
    1. Change {{AbstractRow.hasLiveData}} to check {{enforceStrictLiveness}}:  if livenessInfo is not live and enforceStrictLiveness, then there is not live data.
    2. For SPRC.group, use the first command to get {{enforceStrictLiveness}} since each command should be the same except for key.
{code}","25/Sep/17 06:44;pauloricardomg;Oh, it seems like this was in the original version of CASSANDRA-11500, and missed during the final refactor. Good catch!

Patch LGTM, committed as {{68bdf45477417c97fa6ed3840eee39b8390fd678}} on cassandra-3.0 and merged up to trunk, and cassandra-dtest commit as {{2a1ce8450d1876c3df58ea7e85d352c428de2ca2}}. Thanks!",25/Sep/17 09:08;jasonstack;Thanks for reviewing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GossipStage blocks because of race in ActiveRepairService,CASSANDRA-13849,13100389,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slapukhov,tvdw,tvdw,07/Sep/17 08:02,12/Mar/19 14:19,13/Mar/19 22:34,09/Nov/17 01:05,3.0.16,3.11.2,4.0,,,,,,,,0,patch,,,"Bad luck caused a kernel panic in a cluster, and that took another node with it because GossipStage stopped responding.

I think it's pretty obvious what's happening, here are the relevant excerpts from the stack traces :

{noformat}
""Thread-24004"" #393781 daemon prio=5 os_prio=0 tid=0x00007efca9647400 nid=0xe75c waiting on condition [0x00007efaa47fe000]
   java.lang.Thread.State: TIMED_WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x000000052b63a7e8> (a java.util.concurrent.CountDownLatch$Sync)
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
    at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)
    at org.apache.cassandra.service.ActiveRepairService.prepareForRepair(ActiveRepairService.java:332)
    - locked <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)
    at org.apache.cassandra.repair.RepairRunnable.runMayThrow(RepairRunnable.java:211)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)                                                                                                           at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:748)

""GossipTasks:1"" #367 daemon prio=5 os_prio=0 tid=0x00007efc5e971000 nid=0x700b waiting for monitor entry [0x00007dfb839fe000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.cassandra.service.ActiveRepairService.removeParentRepairSession(ActiveRepairService.java:421)
    - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)
    at org.apache.cassandra.service.ActiveRepairService.convict(ActiveRepairService.java:776)
    at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:306)
    at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:775)                                                                                                                at org.apache.cassandra.gms.Gossiper.access$800(Gossiper.java:67)
    at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:187)
    at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:748)

""GossipStage:1"" #320 daemon prio=5 os_prio=0 tid=0x00007efc5b9f2c00 nid=0x6fcd waiting for monitor entry [0x00007e260186a000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.cassandra.service.ActiveRepairService.removeParentRepairSession(ActiveRepairService.java:421)
    - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)                                                                                          at org.apache.cassandra.service.ActiveRepairService.convict(ActiveRepairService.java:776)
    at org.apache.cassandra.service.ActiveRepairService.onRestart(ActiveRepairService.java:744)
    at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:1049)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:1143)
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:49)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)                                                                                       at java.lang.Thread.run(Thread.java:748)
{noformat}

iow, org.apache.cassandra.service.ActiveRepairService.prepareForRepair holds a lock until the repair is prepared, which means waiting for other nodes to respond, which may die at exactly that moment, so they won't complete. Gossip will at the same time try to mark the node as down, but it requires that same lock :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Oct/17 12:13;slapukhov;CAS-13849.patch;https://issues.apache.org/jira/secure/attachment/12893522/CAS-13849.patch,30/Oct/17 17:21;slapukhov;CAS-13849_2.patch;https://issues.apache.org/jira/secure/attachment/12894797/CAS-13849_2.patch,01/Nov/17 10:37;slapukhov;CAS-13849_3.patch;https://issues.apache.org/jira/secure/attachment/12895158/CAS-13849_3.patch,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-10-23 12:14:19.032,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 09 01:05:08 UTC 2017,,,,,,0|i3jr1b:,9223372036854775807,3.0.14,,,,,,,bdeggleston,bdeggleston,,,,,,,,,,23/Oct/17 12:14;slapukhov;Proposed patch for the CAS-13849,"23/Oct/17 12:14;slapukhov;Idea is to replace intrinsic lock (synchronized) everywhere in ActiveRepairService with ReentrantLock, and release it before start waiting on prepareLatch in prepareForRepair.","24/Oct/17 13:18;spodxx@gmail.com;It's not really clear to me why we use a 1 hour timeout for the PrepareMessage callback in first place. The PrepareMessage should just propagate the repair session. If we really have to support such a long timeout period, we'd have to synchronize with any convict notification raised at some point while still waiting for the timeout. But simply lowering the timeout to something like 1 minute would make the gossip blocking behaviour much less likely and only block for at most that very period. ","30/Oct/17 17:21;slapukhov;Patch reducing 1 hour timeout for the PrepareMessage callback to 1 minute.
","30/Oct/17 17:25;slapukhov;Stefan, thank you for your response.

I have tried your idea in our test setting - it fixes the problem for us. I have uploaded this as a patch ([^CAS-13849_2.patch]), could you please take a look?","30/Oct/17 17:56;bdeggleston;I'd agree that the timeout probably doesn't need to be that long. In the meantime, you should probably re-evaluate whether this method actually needs to be synchronized. This method is unsynchronized in trunk, and I think it may be because it was a holdover from more synchronous repair code of the past. Just looking at it briefly in 3.0, the only thing that *might* need synchonization is the call to registerParentRepairSession, everything else if just method local message sending / receiving stuff.","31/Oct/17 16:28;slapukhov;Yes, ActiveRepairService.prepareForRepair is not synchronized in trunk, indeed.

I believe, however, it gives the way to a possible race condition.

In the receiving node org.apache.cassandra.repair.RepairMessageVerbHandler#doVerb method can be processing PREPARE_MESSAGE from coordinator node and run the ActiveRepairService.instance.registerParentRepairSession method. In the same time, receiving node org.apache.cassandra.tools.nodetool.Repair#execute method can also invoke the same ActiveRepairService.instance.registerParentRepairSession method. As a consequence, ActiveRepairService can be registered twice with the Gossiper.instance.register(this), for example.

Maybe it would be safer to have leave ActiveRepairService.prepareForRepair unsynchronized but make registerParentRepairSession synchronized.

What do you think?","31/Oct/17 17:56;bdeggleston;I think that's reasonable, given the stuff that happens in this method, and the relative infrequency that it's called. It would also fix a visibility issue with the registeredForEndpointChanges variable. We should also check that a parent repair session doesn't already exist for the given key before adding a new one.",01/Nov/17 10:39;slapukhov;Please take a look at this patch: [^CAS-13849_3.patch],"02/Nov/17 16:32;bdeggleston;Patch looks good. I've merged it up through trunk and started tests here:

|[3.0|https://github.com/bdeggleston/cassandra/tree/13849-3.0] | [utests|https://circleci.com/gh/bdeggleston/cassandra/152] | [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/408/]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/13849-3.11] | [utests|https://circleci.com/gh/bdeggleston/cassandra/153] | [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/409/]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/13849-trunk] | [utests|https://circleci.com/gh/bdeggleston/cassandra/154] | [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/410/] |

I'll commit once the tests are complete, assuming there aren't any problems.","09/Nov/17 01:05;bdeggleston;I ran the trunk unit tests locally and everything passed. The rest of the test failures were either flaky tests, or unrelated tests that passed locally. Committed as {{49edd70740e2efae3681cb79a391369bfb7de02e}}.

Thanks [~slapukhov]!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly close netty channels when a stream session ends,CASSANDRA-13905,13105221,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,jasobrown,jasobrown,26/Sep/17 19:31,12/Mar/19 14:19,13/Mar/19 22:34,29/Sep/17 16:36,4.0,,,,,Legacy/Streaming and Messaging,,,,,0,,,,Netty channels in stream sessions were not being closed correctly. TL;DR I was using a lambda that was not executing as it is lazily evaluated. This was causing a {{RejectedExecutionException}} at the end of some streaming-related dtests,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-29 16:17:46.559,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 29 16:36:04 UTC 2017,,,,,,0|i3kkp3:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,"26/Sep/17 19:35;jasobrown;Minor patch here:

||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/netty-RejectedExecutionException]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/336/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/netty-RejectedExecutionException]|

utest errors are unrelated, and number of dtest failures are lower (and none caused by {{RejectedExecutionEception}})

[~aweisberg] or [~pauloricardomg] : Would one of you mind reviewing?","29/Sep/17 16:17;aweisberg;+1 to the fix.

If you wanted to stick with the Streamisms you could do Collectors.toList() after the map and then wait on futures for that. Collectors are the don't be lazy step in streams. Although if you aren't being lazy I'm not sure the syntax for streams is really all that much clearer.","29/Sep/17 16:36;jasobrown;bq. Collectors are the don't be lazy step in streams

Yeah that's where I messed up originally :).

committed as sha {[ebefc96a8fe63aca5f324984f7f3147f10218643}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalStateException thrown by UPI.Serializer.hasNext() for some SELECT queries,CASSANDRA-13911,13105387,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,27/Sep/17 11:17,12/Mar/19 14:19,13/Mar/19 22:34,30/Sep/17 10:47,3.0.15,3.11.1,,,,Legacy/Coordination,,,,,0,,,,"Certain combinations of rows, in presence of per partition limit (set explicitly in 3.6+ or implicitly to 1 via DISTINCT) cause {{UnfilteredPartitionIterators.Serializer.hasNext()}} to throw {{IllegalStateException}} .

Relevant code snippet:

{code}
// We can't answer this until the previously returned iterator has been fully consumed,
// so complain if that's not the case.
if (next != null && next.hasNext())
    throw new IllegalStateException(""Cannot call hasNext() until the previous iterator has been fully consumed"");
{code}

Since {{UnfilteredPartitionIterators.Serializer}} and {{UnfilteredRowIteratorSerializer.serializer}} deserialize partitions/rows lazily, it is required for correct operation of the partition iterator to have the previous partition fully consumed, so that deserializing the next one can start from the correct position in the byte buffer. However, that condition won’t always be satisfied, as there are legitimate combinations of rows that do not consume every row in every partition.

For example, look at [this dtest|https://github.com/iamaleksey/cassandra-dtest/commits/13911].

In case we end up with a following pattern of rows:

{code}
node1, partition 0 | 0
node2, partition 0 |   x x
{code}

, where {{x}} and {{x}} a row tombstones for rows 1 and 2, it’s sufficient for {{MergeIterator}} to only look at row 0 in partition from node1 and at row tombstone 1 from node2 to satisfy the per partition limit of 1. The stopping merge result counter will stop iteration right there, leaving row tombstone 2 from node2 unvisited and not deseiralized. Switching to the next partition will in turn trigger the {{IllegalStateException}} because we aren’t done yet.

The stopping counter is behaving correctly, so is the {{MergeIterator}}. I’ll note that simply removing that condition is not enough to fix the problem properly - it’d just cause us to deseiralize garbage, trying to deserialize a new partition from a position in the bytebuffer that precedes remaining rows in the previous partition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-29 13:12:43.905,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 30 10:47:49 UTC 2017,,,,,,0|i3klpj:,9223372036854775807,,,,,,,,beobal,beobal,,,,,,,,,,"29/Sep/17 11:37;iamaleksey;Branches with fixes here: [3.0|https://github.com/iamaleksey/cassandra/commits/13911-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/13911-3.11], [4.0|https://github.com/iamaleksey/cassandra/commits/13911-4.0].

3.11 needs an extra dtest to show that the change in {{DataResolver}} is necessary (using {{PER PARTITION LIMIT}} > 1), working on it. Tests are running.",29/Sep/17 13:12;beobal;+1,"30/Sep/17 10:47;iamaleksey;Thanks.

[3.0 utests|https://circleci.com/gh/iamaleksey/cassandra/43] have the usual unrelated failures (mostly MV schema races). [3.11 utests|https://circleci.com/gh/iamaleksey/cassandra/44] have the common {{CommitLogSegmentManagerTest}} failure, and [4.0 utests|https://circleci.com/gh/iamaleksey/cassandra/45] have an unrelated {{StreamTransferTaskTest}} failure + the usual {{ViewFilteringTest}} timeouts. Basically baseline.

[3.0 dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/347/] only have the usual suspects (also we need to do something about {{Could not do a git clone}} issue already).  [3.11 dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/348/] and [4.0 dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/349/] are a bit more noisy but nothing related fails for a good reason. The flakiness and timeouts on jenkins are pretty bad overall though.

Committed as [1efdf330e291a41cd8051e0c1195f75b5d352370|https://github.com/apache/cassandra/commit/1efdf330e291a41cd8051e0c1195f75b5d352370] to 3.0 and merged with 3.11 and trunk. Dtest committed as [51ad68ec45c7a40de1c51b31651632f2e87ceaa4|https://github.com/apache/cassandra-dtest/commit/51ad68ec45c7a40de1c51b31651632f2e87ceaa4].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.nio.BufferOverflowException: null while flushing hints,CASSANDRA-13619,13080780,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,milansm,milansm,19/Jun/17 10:32,12/Mar/19 14:19,13/Mar/19 22:34,18/Sep/17 07:01,3.0.15,3.11.1,4.0,,,Legacy/Coordination,Legacy/Core,,,,0,,,,"I'm seeing the following exception running Cassandra 3.0.11 on 21 node cluster in two AWS regions when half of the nodes in one region go down, and the load is high on the rest of the nodes:

{code}
WARN  [SharedPool-Worker-10] 2017-06-14 12:57:15,017 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-10,5,main]: {}
java.lang.RuntimeException: java.nio.BufferOverflowException
        at org.apache.cassandra.service.StorageProxy$HintRunnable.run(StorageProxy.java:2549) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0-zing_17.03.1.0]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.11.jar:3.0.11]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0-zing_17.03.1.0]
Caused by: java.nio.BufferOverflowException: null
        at org.apache.cassandra.io.util.DataOutputBufferFixed.doFlush(DataOutputBufferFixed.java:52) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.write(BufferedDataOutputStreamPlus.java:195) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.writeUnsignedVInt(BufferedDataOutputStreamPlus.java:258) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithVIntLength(ByteBufferUtil.java:296) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.Columns$Serializer.serialize(Columns.java:405) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.SerializationHeader$Serializer.serializeForMessaging(SerializationHeader.java:407) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:120) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:87) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.serialize(PartitionUpdate.java:625) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.Mutation$MutationSerializer.serialize(Mutation.java:305) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.Hint$Serializer.serialize(Hint.java:141) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBuffer$Allocation.write(HintsBuffer.java:251) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBuffer$Allocation.write(HintsBuffer.java:230) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBufferPool.write(HintsBufferPool.java:61) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsService.write(HintsService.java:154) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.service.StorageProxy$11.runMayThrow(StorageProxy.java:2627) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.service.StorageProxy$HintRunnable.run(StorageProxy.java:2545) ~[apache-cassandra-3.0.11.jar:3.0.11]
        ... 5 common frames omitted
{code}

Relevant configurations from cassandra.yaml:

{code}
-cassandra_hinted_handoff_throttle_in_kb: 1024
 cassandra_max_hints_delivery_threads: 4
-cassandra_hints_flush_period_in_ms: 10000
-cassandra_max_hints_file_size_in_mb: 512
{code}

When I reduce -cassandra_hints_flush_period_in_ms: 10000 to 5000, the number of exceptions lowers significantly, but they are still present.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13339,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-07 12:50:23.056,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 18 07:01:14 UTC 2017,,,,,,0|i3gfq7:,9223372036854775807,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,07/Sep/17 12:50;krummas;[~milansm] could you post your schema and the modification statements you are running?,"08/Sep/17 06:50;milansm;[~krummas] here it is:

The schema:
{code:java}
CREATE TABLE IF NOT EXISTS keyspace.ids (
    id uuid,
    prov int,
    eid text,
    PRIMARY KEY (id, prov)
) WITH CLUSTERING ORDER BY (proc ASC)
    AND bloom_filter_fp_chance = 0.1
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = 'External mappings'
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy', 'sstable_size_in_mb': '200'}
    AND compression = {'enabled': 'false'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 2592000
    AND gc_grace_seconds = 86400
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}

The mutation statements:
{code:java}
UPDATE keyspace.ids USING TTL ? SET eid = ? where id = ? AND prov = ?
UPDATE keyspace.ids SET eid = ? where id = ? AND prov = ?
{code}","08/Sep/17 15:21;krummas;This reproduces pretty quickly with the following:
{code}
$ ccm create -n2 hints
$ ccm start
$ ccm node2 stop
$ tools/bin/cassandra-stress user profile=tools/cqlstress-singlepart.yaml ops\(insert=1\) -rate threads=65
{code}
where cqlstress-singlepart.yaml is [here|https://github.com/krummas/cassandra/blob/marcuse/stress_hints/tools/cqlstress-singlepart.yaml] - it just keeps updating a single partition

cc [~jasobrown] since this is most likely the same bug as CASSANDRA-13339 ","08/Sep/17 16:26;jasobrown;Yup, this definitely looks related. Thanks for the repro steps, it'll be a *huge* help!",08/Sep/17 19:09;krummas;[~milansm] which java version and operating system are you running?,"08/Sep/17 20:05;jasobrown;Crap - i couldn't get [~krummas]'s script to repro for me, and I tried on three different systems (Linux VM on mac laptop, macOS on laptop, macOS desktop). Still, Marcus and I have some avenues for investigation we're gonna dig into.","11/Sep/17 14:46;iamaleksey;On MacOS, with Oracle's ""1.8.0_144"", cannot reproduce either.",11/Sep/17 15:19;krummas;Does not repro with Oracles 1.8.0_144 for me either,"12/Sep/17 10:42;krummas;In {{PartitionUpdate}}, {{isBuilt}} is non-volatile, and it is set once the {{Holder}} ref has been updated. {{build()}} is synchronized but {{maybeBuild()}} where we check {{isBuilt}} is not. That means this is basically double checked locking, but since {{isBuilt}} is not volatile, the assignments in {{build()}} can be reordered, making {{isBuilt}} true before {{holder}} is assigned.

It stops reproducing if I set {{isBuilt = this.holder != null}} instead of {{isBuilt = true}} to make sure that {{holder}} is set before {{isBuilt}} but making {{isBuilt}} volatile should be the correct solution.","14/Sep/17 12:10;krummas;https://github.com/krummas/cassandra/commits/marcuse/13619-volatile
https://github.com/krummas/cassandra/commits/marcuse/13619-volatile-3.11
https://github.com/krummas/cassandra/commits/marcuse/13619-volatile-trunk

just running dtests for 3.0 to not waste build resources (this trivial patch really shouldn't cause any regressions....)
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/310/

Created CASSANDRA-13867 for the more complete fix.",15/Sep/17 10:08;iamaleksey;+1,"18/Sep/17 07:01;krummas;committed as {{bb3b332c4225660927}}, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition when closing stream sessions (4.0),CASSANDRA-13852,13100540,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,jasobrown,jasobrown,07/Sep/17 18:14,12/Mar/19 14:19,13/Mar/19 22:34,07/Sep/17 23:47,4.0,,,,,,,,,,0,,,,"bootstrap_test:TestBootstrap.manual_bootstrap_test is hanging due to a race condition that can occur when waiting for a streaming COMPLETE message and the remote side has already closed the connection. We do not check to see if we still have remaining bytes buffered for consumption in {{RebufferringByteBufDataInputPlus}}, but always go ahead and throw an {{EOFException}}. We should consume the bytes, then throw an {{EOFException}} on subsequent calls.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13839,CASSANDRA-13858,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-07 23:35:55.36,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 07 23:47:07 UTC 2017,,,,,,0|i3jryv:,9223372036854775807,,,,,,,,bdeggleston,bdeggleston,,,,,,,,,,07/Sep/17 18:15;jasobrown;This was discovered via bootstrap_test:TestBootstrap.manual_bootstrap_test hanging when running dtests,"07/Sep/17 18:18;jasobrown;Patch to fix this problem here:

||hang||
|[branch|https://github.com/jasobrown/cassandra/tree/bootstrap-dtest-hang]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/287]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/bootstrap-dtest-hang]|

There are other dtests problems happening, and I""m on those now, as well.",07/Sep/17 23:29;jasobrown;This fix should also resolve the hanging in system_auth_ks_is_alterable_test (auth_test.TestAuth),07/Sep/17 23:35;bdeggleston;+1,07/Sep/17 23:47;jasobrown;committed as sha {{83822d12d87dcb3aaad2b1e670e57ebef4ab1c36}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix short read protection for tables with no clustering columns,CASSANDRA-13880,13102665,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,15/Sep/17 19:18,12/Mar/19 14:19,13/Mar/19 22:34,19/Sep/17 10:21,3.0.15,3.11.1,,,,Legacy/Coordination,,,,,0,,,,"CASSANDRA-12872 fixed counting replica rows, so that we do now fetch more than one extra row if necessary.

Fixing the issue caused consistency_test.py:TestConsistency.test_13747 to start failing, by exposing a bug in the way we handle empty clusterings.

When {{moreContents()}} asks for another row and {{lastClustering}} is {{EMPTY}}, the response again (and again) contains the row with {{EMPTY}} clustering.

SRP assumes it’s a new row, counts it as one, gets confused and keeps asking for more, in a loop, again and again.

Arguably, a response to a read command with the following non-inclusive {{ClusteringIndexFilter}}:

{code}
command.clusteringIndexFilter(partitionKey).forPaging(metadata.comparator, Clustering.EMPTY, false);
{code}

... should return nothing at all rather than a row with an empty clustering.

Also arguably, SRP should not even attempt to fetch more rows if {{lastClustering == Clustering.EMPTY}}. In a partition key only column
we shouldn’t expect any more rows.

This JIRA is to fix the latter issue on SRP side - to modify SRP logic to short-circuit execution if {{lastClustering}} was an {{EMPTY}} one instead of querying pointlessly for non-existent extra rows.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-18 19:57:42.235,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 19 10:21:34 UTC 2017,,,,,,0|i3k4zz:,9223372036854775807,,,,,,,,benedict,benedict,,,,,,,,,,"18/Sep/17 16:35;iamaleksey;Branches with fixes: [3.0|https://github.com/iamaleksey/cassandra/commits/13880-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/13880-3.11], [4.0|https://github.com/iamaleksey/cassandra/commits/13880-4.0]. An isolated dtest [here|https://github.com/iamaleksey/cassandra-dtest/commits/13880].

The issue only triggers with a per partition limit set, doesn't with a regular limit. I am investigating why, but whatever the reason is, this fix is wanted/needed anyway - it does save us a pointless roundtrip in many scenarios.",18/Sep/17 19:57;benedict;+1,"19/Sep/17 10:21;iamaleksey;Thanks.

CircleCI run for [3.0|https://circleci.com/gh/iamaleksey/cassandra/34] has a bunch of annoying MV timeout issues again, and [dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/326/testReport/] have those annoying git clone issues, again, but everything seems alright otherwise.

Committed to 3.0 as [35e32f20ba8cba6cbfb1bee4252c0edd8684cdb1|https://github.com/apache/cassandra/commit/35e32f20ba8cba6cbfb1bee4252c0edd8684cdb1] and merged with 3.11 and trunk. Dtest committed as [163f82c2db0e86d4dd8f312b291ccd094891b986|https://github.com/apache/cassandra-dtest/commit/163f82c2db0e86d4dd8f312b291ccd094891b986].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tracing interferes with digest requests when using RandomPartitioner,CASSANDRA-13964,13110271,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,beobal,beobal,18/Oct/17 11:23,12/Mar/19 14:19,13/Mar/19 22:34,07/Nov/17 16:13,3.0.16,3.11.2,4.0,,,Legacy/Local Write-Read Paths,Legacy/Observability,,,,0,,,,"A {{ThreadLocal<MessageDigest>}} is used to generate the MD5 digest when a replica serves a read command and the {{isDigestQuery}} flag is set. The same threadlocal is also used by {{RandomPartitioner}} to decorate partition keys. So in a cluster with RP, if tracing is enabled the data digest is corrupted by the partitioner making tokens for the tracing mutations. This causes a digest mismatch on the coordinator, triggering a full data read on every read where CL > 1 (or speculative execution/read repair kick in).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-10-18 13:39:55.394,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 07 23:12:51 UTC 2017,,,,,,0|i3lenr:,9223372036854775807,3.0.15,3.11.1,4.0,,,,,jasobrown,jasobrown,,,3.0.0,,,,,,,"18/Oct/17 12:16;beobal;Added a second {{ThreadLocal<MessageDigest>}} for exclusive use by {{RandomPartitioner}}. 
New dtest: https://github.com/beobal/cassandra-dtest/tree/13964

||branch||utest||dtest||
|[13964-3.0|https://github.com/beobal/cassandra/tree/13964-3.0]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-3.0]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/379/]|
|[13964-3.11|https://github.com/beobal/cassandra/tree/13964-3.11]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-3.11]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/380/]|
|[13964-trunk|https://github.com/beobal/cassandra/tree/13964-trunk]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-trunk]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/381/]|
","18/Oct/17 13:39;jasobrown;I'm +1 on the patch and dtest. I ran the dtest w/o the patch can confirmed the failure, then with and confirmed success.

The only minor nit I have is on the dtest where you changed the {{prepare}} function. Maybe I'm missing something, but couldn't

{code}
        if not random_partitioner:
            cluster.set_partitioner(""org.apache.cassandra.dht.Murmur3Partitioner"")
        else:
            if random_partitioner:
                cluster.set_partitioner(""org.apache.cassandra.dht.RandomPartitioner"")
            else:
                cluster.set_partitioner(""org.apache.cassandra.dht.Murmur3Partitioner"")
{code}

be simplified as 

{code}
        if random_partitioner:
            cluster.set_partitioner(""org.apache.cassandra.dht.RandomPartitioner"")
        else:
            cluster.set_partitioner(""org.apache.cassandra.dht.Murmur3Partitioner"")
{code}

wrt branches to commit on, for sure we need this on 3.0 and 3.11. We also need it on trunk, but should wait to see what happens with CASSANDRA-13291?","31/Oct/17 19:35;beobal;[~jasobrown] thanks for the review & sorry for the delay in getting back to this. 

You're totally right about the dtest, [so I've made it sane as you suggested|https://github.com/beobal/cassandra-dtest/commit/edc48bc965e842628413cfd50a7a21071d7b098a]. As the upstream branches have moved on in since you reviewed, I've rebased & force pushed to trigger CI to run again. The addition of CASSANDRA-13291 to trunk was no issue as both branches took the same approach. Unless you have any concerns, I'll commit once CI is done. Thanks.

||branch||utest||dtest||
|[13964-3.0|https://github.com/beobal/cassandra/tree/13964-3.0]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-3.0]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/401/]|
|[13964-3.11|https://github.com/beobal/cassandra/tree/13964-3.11]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-3.11]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/402/]|
|[13964-trunk|https://github.com/beobal/cassandra/tree/13964-trunk]|[utest|https://circleci.com/gh/beobal/cassandra/tree/13964-trunk]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/403/]|","31/Oct/17 20:53;jasobrown;lgtm, and thanks for confirming my python skills aren't complete garbage :D","07/Nov/17 13:36;githubbot;GitHub user beobal opened a pull request:

    https://github.com/apache/cassandra-dtest/pull/10

    Add test for digest requests with RandomPartitioner and tracing enabled

    Patch by Sam Tunnicliffe; reviewed by Jason Brown for CASSANDRA-13964
    
    @ptnapoleon: Jason already gave this the once over, but if you have chance I'd appreciate your +1 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/beobal/cassandra-dtest 13964

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/10.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #10
    
----
commit edc48bc965e842628413cfd50a7a21071d7b098a
Author: Sam Tunnicliffe <sam@beobal.com>
Date:   2017-10-17T13:50:25Z

    Add test for digest requests with RandomPartitioner and tracing enabled
    
    Patch by Sam Tunnicliffe; reviewed by Jason Brown for CASSANDRA-13964

----
","07/Nov/17 16:13;beobal;The CI was generally good barring a couple of flaky-ish tests which I've checked are passing locally, so I've committed to 3.0 in {{58daf1376456289f97f0ef0b0daf9e0d03ba6b81}} and merged to 3.11 and trunk.","07/Nov/17 23:12;githubbot;Github user beobal closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/10
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BTree$Builder / io.netty.util.Recycler$Stack leaking memory,CASSANDRA-13929,13106635,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,tsteinmaurer,tsteinmaurer,03/Oct/17 07:58,12/Mar/19 14:19,13/Mar/19 22:34,08/Jun/18 19:12,3.11.3,,,,,Legacy/Core,,,,,2,,,,"Different to CASSANDRA-13754, there seems to be another memory leak in 3.11.0+ in BTree$Builder / io.netty.util.Recycler$Stack.

* heap utilization increase after upgrading to 3.11.0 => cassandra_3.11.0_min_memory_utilization.jpg
* No difference after upgrading to 3.11.1 (snapshot build) => cassandra_3.11.1_snapshot_heaputilization.png; thus most likely after fixing CASSANDRA-13754, more visible now
* MAT shows io.netty.util.Recycler$Stack as top contributing class => cassandra_3.11.1_mat_dominator_classes.png
* With -Xmx8G (CMS) and our load pattern, we have to do a rolling restart after ~ 72 hours

Verified the following fix, namely explicitly unreferencing the _recycleHandle_ member (making it non-final). In _org.apache.cassandra.utils.btree.BTree.Builder.recycle()_
{code}
        public void recycle()
        {
            if (recycleHandle != null)
            {
                this.cleanup();
                builderRecycler.recycle(this, recycleHandle);
                recycleHandle = null; // ADDED
            }
        }
{code}

Patched a single node in our loadtest cluster with this change and after ~ 10 hours uptime, no sign of the previously offending class in MAT anymore => cassandra_3.11.1_mat_dominator_classes_FIXED.png

Can' say if this has any other side effects etc., but I doubt.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Oct/17 07:59;tsteinmaurer;cassandra_3.11.0_min_memory_utilization.jpg;https://issues.apache.org/jira/secure/attachment/12890116/cassandra_3.11.0_min_memory_utilization.jpg,05/Oct/17 08:24;tsteinmaurer;cassandra_3.11.1_NORECYCLE_memory_utilization.jpg;https://issues.apache.org/jira/secure/attachment/12890495/cassandra_3.11.1_NORECYCLE_memory_utilization.jpg,03/Oct/17 08:03;tsteinmaurer;cassandra_3.11.1_mat_dominator_classes.png;https://issues.apache.org/jira/secure/attachment/12890120/cassandra_3.11.1_mat_dominator_classes.png,03/Oct/17 08:07;tsteinmaurer;cassandra_3.11.1_mat_dominator_classes_FIXED.png;https://issues.apache.org/jira/secure/attachment/12890121/cassandra_3.11.1_mat_dominator_classes_FIXED.png,03/Oct/17 08:02;tsteinmaurer;cassandra_3.11.1_snapshot_heaputilization.png;https://issues.apache.org/jira/secure/attachment/12890119/cassandra_3.11.1_snapshot_heaputilization.png,05/Feb/18 07:53;tsteinmaurer;cassandra_3.11.1_vs_3.11.2recyclernullingpatch.png;https://issues.apache.org/jira/secure/attachment/12909189/cassandra_3.11.1_vs_3.11.2recyclernullingpatch.png,23/Feb/18 10:27;tsteinmaurer;cassandra_heapcpu_memleak_patching_test_30d.png;https://issues.apache.org/jira/secure/attachment/12911703/cassandra_heapcpu_memleak_patching_test_30d.png,13/Feb/18 06:35;jay.zhuang;dtest_example_80_request.png;https://issues.apache.org/jira/secure/attachment/12910331/dtest_example_80_request.png,13/Feb/18 06:41;jay.zhuang;dtest_example_80_request_fix.png;https://issues.apache.org/jira/secure/attachment/12910333/dtest_example_80_request_fix.png,13/Feb/18 05:09;jay.zhuang;dtest_example_heap.png;https://issues.apache.org/jira/secure/attachment/12910320/dtest_example_heap.png,14/Feb/18 12:46;tsteinmaurer;memleak_heapdump_recyclerstack.png;https://issues.apache.org/jira/secure/attachment/12910578/memleak_heapdump_recyclerstack.png,,11.0,,,,,,,,,,,,,,,,,,,2017-10-03 11:47:45.139,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 08 19:11:41 UTC 2018,,,,,,0|i3ktc7:,9223372036854775807,3.11.0,3.8,,,,,,jasobrown,jasobrown,,,3.11.0,,,,,,,"03/Oct/17 11:47;tjake;The recycler is meant to cache objects for reuse. By nulling the handler you are effectively invalidating the cache every time. 

I don't think this is a leak but perhaps we should limit this cache to hold less items. (see the recycler constructor)","03/Oct/17 12:09;tsteinmaurer;[~tjake]: Thanks for the feedback about invalidating the cache. Not sure what actually is cached here, but without nulling the reference, I do see e.g. ~ 770K BTree$Builder instances on the heap. Infinite caching still sounds like a memory leak to me, but that's nitpicking now. :-)

Thanks again.",03/Oct/17 13:39;tjake;The default for recycler is 32k instances per thread. So perhaps change this to 8192 per thread and see if that makes a difference.,"03/Oct/17 16:53;tsteinmaurer;I can try a different max value, but what is supposed to be cached here and what area should be suffering without the cache? The reason why I'm asking is, that in our 9 node cluster, a single node is patched with the discussed change. I don't see any difference in CPU usage, GC, request latency etc., thus potentially looking at the wrong metrics.",03/Oct/17 17:27;tjake;This was part of CASSANDRA-9766 and addresses one of the main allocation culprits during streaming.,"04/Oct/17 06:11;tsteinmaurer;Ok, if this sort of caching is entirely only useful during streaming, this somehow explains why I do not see any difference between the nodes here, cause they process regular business and no repair, bootstrapping etc, thus I can't comment on the performance gain with the cache (possibly this has been tested in CASSANDRA-9766 anyway).

If the cache is useful for streaming, it still would definitely make sense IMHO, to make the max capacity configurable (with a somewhat useful small default value) via a system property, cassandra.yaml or whatever place is preferred here, cause ~ 2,4G+ heap usage for this sort of cache at -XmX8G does not make sense or perhaps the majority of Cassandra users don't scale out with smaller sized machines anymore.

As you have mentioned *per thread*. I guess we are talking about the number of threads serving client requests, aka e.g. {{native_transport_max_threads}}? If so, there should be somewhere a clear pointer in a comment, documentation etc., that heap usage is directly related to number of threads * configurable max size","05/Oct/17 08:27;tsteinmaurer;Patched our 9 node loadtest cluster with disabled recycling. Flat AVG heap utilization in a time-frame of 24hrs. => cassandra_3.11.1_NORECYCLE_memory_utilization.jpg

I don't see any negative impact for normal operations (haven't tested streaming) running without this cache, so this is probably a cost vs. benefit discussion, but e.g. in relation to other in-memory data structures for speeding up things (key cache, bloom filter), this kind of cache is questionable, IMHO.
","13/Oct/17 12:20;tsteinmaurer;Stable for a week now in our 9 node (m4.xlarge) loadtest cluster from a heap perspective with entirely disabling the recycler cache.

Any ideas if we can expect something in context of this ticket for 3.11.2? Thanks!

",11/Dec/17 13:36;tsteinmaurer;Yet another ping after 2 months of silence and the issue still being unassigned. Is this something which will be handled in the 3.11 series? Thanks!,"11/Dec/17 15:31;mshuler;You may have some better success getting eyes on this JIRA by attaching a proper patch for the 3.11 branch (and a separate trunk patch, if merge up isn't clean). You can do this with a simple text file attachment of the diff, or linking to a github branch.

Bonus points for adding a test that reproduces the problem/fix, as well as getting test run through circleci.

This JIRA can be assigned to yourself as the author and you can set the status to ""Patch Available"" when there's an actual patch/branch to review. Then you can possibly seek out a reviewer on the dev@ mailing list, if it's urgent.

It looks like there are currently 115 Patch Available tickets, so it may still take some time, but getting this set up for someone else to review would be a great step in getting the process rolling further than just a comment.
https://issues.apache.org/jira/issues/?jql=project%20%3D%20CASSANDRA%20AND%20status%20%3D%20%22Patch%20Available%22",11/Dec/17 15:33;mshuler;I set the fix version to {{3.11.x}} to indicate this is intended for the 3.11 series for you.,"22/Jan/18 19:32;tsteinmaurer;[~mshuler], my patch would look like as mentioned in the description, but I'm pretty sure it will get rejected, cause [~tjake] mentioned that this all has been added to cache something, especially useful for streaming, if I remember correctly. Due to lack of knowledge about the inner workings here, I can't provide anything more useful. Our loadtest environment is running a locally patched build with the nulling approach.","25/Jan/18 20:54;jjirsa;I chatted offline with Scott (who is far more familiar with the netty project than I am), and he noted that they've fixed a few recent bugs in that area of the code that COULD resolve leaks. I'm not confident this is actually a leak, vs just the recycler working as intended (but needing to be tuned), but perhaps we can consider bumping netty anyway (at least in trunk this is an easy decision, but it'd be interesting to find if netty 4.1.20 fixes the behavior you see in unpatched 3.11.1 [~tsteinmaurer] ).

 ","25/Jan/18 21:46;tsteinmaurer;[~jjirsa], thanks for the follow-up. Looks like a simple netty jar file replacement works, at least Cassandra starts up without any exceptions. I have now 1 of 9 nodes running with 3.11.1 unpatched + Netty 4.1.20.","25/Jan/18 21:51;jjirsa;h3. [netty-4.0.55|https://github.com/netty/netty/releases/tag/netty-4.0.55.Final] may be a better test, since 3.11.1 is running netty 4.0.44 now, 4.0.55 would be on the same major and appears to have many of the similar fixes to Recycler.",25/Jan/18 21:59;tsteinmaurer;Makes sense. Restarted the single node with Netty 4.0.55 now.,"31/Jan/18 13:05;tsteinmaurer;[~jjirsa], had 3.11.1 with Netty 4.0.55 on one node in parallel with 3.11.1 incl. ""nulling patch"" and Netty 4.0.44 on other nodes running over the weekend. Unpatched combo with Netty 4.0.55 unfortunately still shows an AVG heap utilization increase over 72hrs, while the patched show a constant AVG heap usage.","31/Jan/18 21:16;zznate;If [~norman] is around, would be great to get his input on whether this might be a leak or tuning issue with recycler. Thanks for your patience and willingness to test this out [~tsteinmaurer]. ","31/Jan/18 23:14;jay.zhuang;Hi [~tsteinmaurer], as [~tjake] suggested, would you please try to reduce the Recycler Capacity Per Thread ([Recycler.java:64|https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/Recycler.java#L64]), just restart the node with JVM option:
{noformat}
-Dio.netty.recycler.maxCapacityPerThread=1024
{noformat}

","01/Feb/18 08:23;norman;yeah 4.0.55.Final should have the ""fix"" as well:

 

[https://github.com/netty/netty/commit/b386ee3eaf35abd5072992d626de6ae2ccadc6d9#diff-23eafd00fcd66829f8cce343b26c236a]

 

That said maybe there are other issues. Would it be possible to share a heap-dump ?","01/Feb/18 13:30;tsteinmaurer;[~jay.zhuang], will let the node (1 out of 9) - previously, manually upgraded to Netty 4.0.55 - running with your mentioned Netty JVM option over the weekend. Thanks.",01/Feb/18 15:18;norman;[~tsteinmaurer] what about a heap dump ? Is this something you could provide ?,"05/Feb/18 07:57;tsteinmaurer;I have attached a new heap utilization chart for our 9 node loadtest environment.
 !cassandra_3.11.1_vs_3.11.2recyclernullingpatch.png|width=300! 

The marked node is running with the following configuration:
* Cassandra 3.11.1 public release
* Netty 4.0.55 (manually upgraded from 4.0.44 by simply replacing the jar file in cassandra/lib)
* -Dio.netty.recycler.maxCapacityPerThread=1024

This still results in a slow increase in AVG heap utilization

The other 8 nodes are running with 3.11.2 built from source with my recycleHandle ""nulling"" patch.

[~norman], I need to internally check if we need to do that via some sort of NDA then. Will report back. Thanks.","13/Feb/18 06:20;jay.zhuang;I think it's just caching the largest tree ever built ({{[BTree.java:907|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/btree/BTree.java#L907]}}) and getting bigger and bigger.

Here is an example with a large number of {{IN}} in a CQL, which causes high heap usage for {{$Stack}} :
{noformat}
session.execute_async(""select id from test.users where id in (%s)"" % ','.join(map(str, range(100000))))
{noformat}
Here is a dtest to reproduce that: {{[large_in_test.py|https://github.com/cooldoger/cassandra-dtest/blob/13929/large_in_test.py]}}
 {{$Stack}} uses 25% 10M heap: {{10M = 10 (request num) * 100K (element) * 8 (obj size)}}
 !dtest_example_heap.png! 
 We could increase heap size and then increase request number, element number gradually to make {{$Stack}} to 80%+ heap usage. And I'm sure there're other use cases could build large BTree. Seems the {{$Stack}} is unable to release until the {{Thread}} is released and by using {{ThreadPoolExecutor}}, the thread is not released.

I'd like to propose the following patch which cleans large BTree builder ({{> 1k elements}}) while recycling:
|Branch|uTest|
|[13929-3.11|https://github.com/cooldoger/cassandra/tree/13929-3.11]|[!https://circleci.com/gh/cooldoger/cassandra/tree/13929-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13929-3.11]|","13/Feb/18 06:41;jay.zhuang;After increasing the heap size from 1G to 4G, it could support 80 such requests at the same time ({{[large_in_test.py:12|https://github.com/cooldoger/cassandra-dtest/blob/13929/large_in_test.py#L12]}}), then the {{$Stack}} uses about 70% heap:
 !dtest_example_80_request.png! 

Tried the patch, it did fix the problem:
 !dtest_example_80_request_fix.png! ","14/Feb/18 07:51;jjirsa;[~tsteinmaurer] - available to test?

Patch looks straight forward to me, but any volunteers to review? Given its impact, probably deserves a more thorough review than I have time for. 


",14/Feb/18 07:52;norman;[~jay.zhuang] I would be interested what is contained in the `Stack` itself,"14/Feb/18 12:50;tsteinmaurer;[~norman], a concrete example for a *single* Recycler$Stack instance from our loadtest.
!memleak_heapdump_recyclerstack.png!

Looks like we have a lot of largish 32K object arrays, which seem to be empty as shallow heap = retained heap for this object arrays. So, a quick look on the patch of [~jay.zhuang], this might help. [~jjirsa], will re-patch a single node and report back after a considerable uptime.",14/Feb/18 13:12;norman;[~tsteinmaurer] yeah thanks I see... So looks more like a misusage for me then a netty bug. Cassandra may also consider to configure the `Recycler` with a more sane default value for this use-case (via the constructor).,"19/Feb/18 20:57;jay.zhuang;Seems like the performance is better without {{Recycler}}. Here is microbench test result to build a BTree with and without {{Recycler}} ([13929-3.11-perf|https://github.com/cooldoger/cassandra/tree/13929-3.11-perf]) (The score is operation per ms, higher is better)
{noformat}
     [java] Benchmark                      (dataSize)          (treeBuilder)   Mode  Cnt      Score       Error   Units
     [java] BTreeBuildBench.buildTreeTest           1  treeBuilderRecycleAdd  thrpt    6  23112.102 ? 17522.471  ops/ms
     [java] BTreeBuildBench.buildTreeTest           1         treeBuilderAdd  thrpt    6  46275.541 ? 60422.458  ops/ms
     [java] BTreeBuildBench.buildTreeTest           2  treeBuilderRecycleAdd  thrpt    6  23588.176 ? 16372.260  ops/ms
     [java] BTreeBuildBench.buildTreeTest           2         treeBuilderAdd  thrpt    6  42838.298 ? 25339.870  ops/ms
     [java] BTreeBuildBench.buildTreeTest           5  treeBuilderRecycleAdd  thrpt    6  24358.111 ? 24339.382  ops/ms
     [java] BTreeBuildBench.buildTreeTest           5         treeBuilderAdd  thrpt    6  60074.551 ? 47329.418  ops/ms
     [java] BTreeBuildBench.buildTreeTest          10  treeBuilderRecycleAdd  thrpt    6  21412.578 ?  6072.160  ops/ms
     [java] BTreeBuildBench.buildTreeTest          10         treeBuilderAdd  thrpt    6  50862.304 ? 30597.546  ops/ms
     [java] BTreeBuildBench.buildTreeTest          20  treeBuilderRecycleAdd  thrpt    6  15871.754 ?  5036.739  ops/ms
     [java] BTreeBuildBench.buildTreeTest          20         treeBuilderAdd  thrpt    6  33699.725 ? 10857.366  ops/ms
     [java] BTreeBuildBench.buildTreeTest          40  treeBuilderRecycleAdd  thrpt    6   5168.225 ?  1212.571  ops/ms
     [java] BTreeBuildBench.buildTreeTest          40         treeBuilderAdd  thrpt    6   8806.838 ?  7736.485  ops/ms
     [java] BTreeBuildBench.buildTreeTest         100  treeBuilderRecycleAdd  thrpt    6   2114.218 ?   639.589  ops/ms
     [java] BTreeBuildBench.buildTreeTest         100         treeBuilderAdd  thrpt    6   3213.333 ?   486.126  ops/ms
     [java] BTreeBuildBench.buildTreeTest        1000  treeBuilderRecycleAdd  thrpt    6    335.523 ?   101.230  ops/ms
     [java] BTreeBuildBench.buildTreeTest        1000         treeBuilderAdd  thrpt    6    386.678 ?   333.534  ops/ms
     [java] BTreeBuildBench.buildTreeTest       10000  treeBuilderRecycleAdd  thrpt    6     35.644 ?    32.171  ops/ms
     [java] BTreeBuildBench.buildTreeTest       10000         treeBuilderAdd  thrpt    6     44.250 ?     8.180  ops/ms
     [java] BTreeBuildBench.buildTreeTest      100000  treeBuilderRecycleAdd  thrpt    6      3.073 ?     2.165  ops/ms
     [java] BTreeBuildBench.buildTreeTest      100000         treeBuilderAdd  thrpt    6      4.651 ?     4.137  ops/ms
{noformat}
And I think the performance gain for CASSANDRA-9766 is not because of the {{Recycler}} for BTree builder.

With {{Recycler}}, it does reduce the GC by reserving the memory. But for P95 and sometimes P99, {{noRecycler}} is still better. Here is the test result with percentiles (score is time per operation, so smaller is better):
{noformat}
     [java] Benchmark                                            (dataSize)          (treeBuilder)    Mode       Cnt          Score       Error  Units
     [java] BTreeBuildBench.buildTreeTest                                 1  treeBuilderRecycleAdd  sample   9244739       1435.033 ?   111.857  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             1  treeBuilderRecycleAdd  sample                   92.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             1  treeBuilderRecycleAdd  sample                  638.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             1  treeBuilderRecycleAdd  sample                 1688.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             1  treeBuilderRecycleAdd  sample                 2292.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             1  treeBuilderRecycleAdd  sample                 4544.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            1  treeBuilderRecycleAdd  sample                13792.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           1  treeBuilderRecycleAdd  sample               124233.984              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             1  treeBuilderRecycleAdd  sample            104202240.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                 1         treeBuilderAdd  sample   9334292        880.361 ?    99.171  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             1         treeBuilderAdd  sample                   74.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             1         treeBuilderAdd  sample                  177.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             1         treeBuilderAdd  sample                  557.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             1         treeBuilderAdd  sample                 2376.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             1         treeBuilderAdd  sample                 9248.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            1         treeBuilderAdd  sample                19200.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           1         treeBuilderAdd  sample                92489.050              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             1         treeBuilderAdd  sample             66256896.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                 2  treeBuilderRecycleAdd  sample   9066713       1655.971 ?   127.345  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             2  treeBuilderRecycleAdd  sample                  101.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             2  treeBuilderRecycleAdd  sample                  756.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             2  treeBuilderRecycleAdd  sample                 1952.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             2  treeBuilderRecycleAdd  sample                 2684.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             2  treeBuilderRecycleAdd  sample                 5696.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            2  treeBuilderRecycleAdd  sample                16272.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           2  treeBuilderRecycleAdd  sample               189176.730              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             2  treeBuilderRecycleAdd  sample            124387328.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                 2         treeBuilderAdd  sample   9816463       1264.312 ?   161.134  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             2         treeBuilderAdd  sample                   91.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             2         treeBuilderAdd  sample                  209.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             2         treeBuilderAdd  sample                  579.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             2         treeBuilderAdd  sample                 2096.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             2         treeBuilderAdd  sample                 8104.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            2         treeBuilderAdd  sample                17088.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           2         treeBuilderAdd  sample               227162.522              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             2         treeBuilderAdd  sample             96731136.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                 5  treeBuilderRecycleAdd  sample   8811507       2044.981 ?   142.626  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             5  treeBuilderRecycleAdd  sample                  126.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             5  treeBuilderRecycleAdd  sample                  934.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             5  treeBuilderRecycleAdd  sample                 2384.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             5  treeBuilderRecycleAdd  sample                 3120.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             5  treeBuilderRecycleAdd  sample                 5808.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            5  treeBuilderRecycleAdd  sample                17280.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           5  treeBuilderRecycleAdd  sample               312320.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             5  treeBuilderRecycleAdd  sample            117702656.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                 5         treeBuilderAdd  sample   9992328       1044.948 ?   131.496  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00             5         treeBuilderAdd  sample                   90.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50             5         treeBuilderAdd  sample                  260.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90             5         treeBuilderAdd  sample                  607.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95             5         treeBuilderAdd  sample                 2156.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99             5         treeBuilderAdd  sample                 8384.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999            5         treeBuilderAdd  sample                17856.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999           5         treeBuilderAdd  sample                98048.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00             5         treeBuilderAdd  sample            132382720.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                10  treeBuilderRecycleAdd  sample   8757941       1896.943 ?   116.106  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            10  treeBuilderRecycleAdd  sample                  144.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            10  treeBuilderRecycleAdd  sample                  917.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            10  treeBuilderRecycleAdd  sample                 2152.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            10  treeBuilderRecycleAdd  sample                 2980.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            10  treeBuilderRecycleAdd  sample                 6648.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           10  treeBuilderRecycleAdd  sample                17056.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          10  treeBuilderRecycleAdd  sample               260050.739              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            10  treeBuilderRecycleAdd  sample             60030976.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                10         treeBuilderAdd  sample   9456081       1066.351 ?   109.690  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            10         treeBuilderAdd  sample                  127.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            10         treeBuilderAdd  sample                  333.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            10         treeBuilderAdd  sample                  534.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            10         treeBuilderAdd  sample                 1884.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            10         treeBuilderAdd  sample                 7264.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           10         treeBuilderAdd  sample                17760.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          10         treeBuilderAdd  sample               123570.150              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            10         treeBuilderAdd  sample             92274688.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                20  treeBuilderRecycleAdd  sample   8754165       2601.277 ?   131.160  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            20  treeBuilderRecycleAdd  sample                  246.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            20  treeBuilderRecycleAdd  sample                 1696.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            20  treeBuilderRecycleAdd  sample                 3680.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            20  treeBuilderRecycleAdd  sample                 4472.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            20  treeBuilderRecycleAdd  sample                 6864.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           20  treeBuilderRecycleAdd  sample                18720.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          20  treeBuilderRecycleAdd  sample               229269.350              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            20  treeBuilderRecycleAdd  sample            166461440.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                20         treeBuilderAdd  sample  10323487       1596.081 ?   150.936  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            20         treeBuilderAdd  sample                  230.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            20         treeBuilderAdd  sample                  627.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            20         treeBuilderAdd  sample                 1019.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            20         treeBuilderAdd  sample                 2500.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            20         treeBuilderAdd  sample                 7784.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           20         treeBuilderAdd  sample                19584.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          20         treeBuilderAdd  sample               176973.414              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            20         treeBuilderAdd  sample            103546880.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                40  treeBuilderRecycleAdd  sample   8413786       6104.243 ?   195.964  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            40  treeBuilderRecycleAdd  sample                  674.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            40  treeBuilderRecycleAdd  sample                 4136.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            40  treeBuilderRecycleAdd  sample                 7528.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            40  treeBuilderRecycleAdd  sample                 8960.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            40  treeBuilderRecycleAdd  sample                12976.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           40  treeBuilderRecycleAdd  sample                31008.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          40  treeBuilderRecycleAdd  sample              2341360.845              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            40  treeBuilderRecycleAdd  sample             79560704.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                                40         treeBuilderAdd  sample   8477065       4486.574 ?   230.760  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00            40         treeBuilderAdd  sample                  699.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50            40         treeBuilderAdd  sample                 2156.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90            40         treeBuilderAdd  sample                 4312.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95            40         treeBuilderAdd  sample                 5376.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99            40         treeBuilderAdd  sample                 9520.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999           40         treeBuilderAdd  sample                28032.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999          40         treeBuilderAdd  sample              4583153.664              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00            40         treeBuilderAdd  sample            163840000.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                               100  treeBuilderRecycleAdd  sample   7535010      11631.027 ?   225.736  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00           100  treeBuilderRecycleAdd  sample                 1826.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50           100  treeBuilderRecycleAdd  sample                 7504.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90           100  treeBuilderRecycleAdd  sample                16960.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95           100  treeBuilderRecycleAdd  sample                21408.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99           100  treeBuilderRecycleAdd  sample                31808.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999          100  treeBuilderRecycleAdd  sample                88576.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999         100  treeBuilderRecycleAdd  sample             10534912.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00           100  treeBuilderRecycleAdd  sample            100270080.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                               100         treeBuilderAdd  sample   7564546      10905.697 ?   235.164  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00           100         treeBuilderAdd  sample                 1636.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50           100         treeBuilderAdd  sample                 7280.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90           100         treeBuilderAdd  sample                14416.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95           100         treeBuilderAdd  sample                18976.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99           100         treeBuilderAdd  sample                32512.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999          100         treeBuilderAdd  sample                63040.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999         100         treeBuilderAdd  sample              8708838.195              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00           100         treeBuilderAdd  sample             60293120.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                              1000  treeBuilderRecycleAdd  sample   3117183     101556.600 ?   688.145  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00          1000  treeBuilderRecycleAdd  sample                15136.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50          1000  treeBuilderRecycleAdd  sample                80256.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90          1000  treeBuilderRecycleAdd  sample               169728.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95          1000  treeBuilderRecycleAdd  sample               206848.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99          1000  treeBuilderRecycleAdd  sample               295424.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999         1000  treeBuilderRecycleAdd  sample               573440.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999        1000  treeBuilderRecycleAdd  sample             16580608.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00          1000  treeBuilderRecycleAdd  sample             79953920.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                              1000         treeBuilderAdd  sample   3635046      81844.338 ?   695.009  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00          1000         treeBuilderAdd  sample                17120.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50          1000         treeBuilderAdd  sample                55936.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90          1000         treeBuilderAdd  sample               133120.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95          1000         treeBuilderAdd  sample               168448.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99          1000         treeBuilderAdd  sample               227584.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999         1000         treeBuilderAdd  sample               956367.872              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999        1000         treeBuilderAdd  sample             20004709.990              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00          1000         treeBuilderAdd  sample             75235328.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                             10000  treeBuilderRecycleAdd  sample    396416     806300.548 ?  5006.312  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00         10000  treeBuilderRecycleAdd  sample               178176.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50         10000  treeBuilderRecycleAdd  sample               578560.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90         10000  treeBuilderRecycleAdd  sample              1439744.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95         10000  treeBuilderRecycleAdd  sample              2101248.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99         10000  treeBuilderRecycleAdd  sample              2367488.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999        10000  treeBuilderRecycleAdd  sample             16711680.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999       10000  treeBuilderRecycleAdd  sample             27893461.811              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00         10000  treeBuilderRecycleAdd  sample             69468160.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                             10000         treeBuilderAdd  sample    544398     586738.267 ?  3652.668  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00         10000         treeBuilderAdd  sample               180992.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50         10000         treeBuilderAdd  sample               467456.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90         10000         treeBuilderAdd  sample               898048.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95         10000         treeBuilderAdd  sample              1052672.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99         10000         treeBuilderAdd  sample              1515520.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999        10000         treeBuilderAdd  sample             16334848.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999       10000         treeBuilderAdd  sample             28628756.070              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00         10000         treeBuilderAdd  sample             75759616.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                            100000  treeBuilderRecycleAdd  sample     44146    7258758.048 ? 85405.244  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00        100000  treeBuilderRecycleAdd  sample              1724416.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50        100000  treeBuilderRecycleAdd  sample              5701632.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90        100000  treeBuilderRecycleAdd  sample             11796480.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95        100000  treeBuilderRecycleAdd  sample             15056896.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99        100000  treeBuilderRecycleAdd  sample             28868608.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999       100000  treeBuilderRecycleAdd  sample             61781180.416              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999      100000  treeBuilderRecycleAdd  sample             72159775.949              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00        100000  treeBuilderRecycleAdd  sample            153092096.000              ns/op
     [java] BTreeBuildBench.buildTreeTest                            100000         treeBuilderAdd  sample     50573    6333830.725 ? 79209.874  ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.00        100000         treeBuilderAdd  sample              1839104.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.50        100000         treeBuilderAdd  sample              4587520.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.90        100000         treeBuilderAdd  sample             10764288.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.95        100000         treeBuilderAdd  sample             16629760.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.99        100000         treeBuilderAdd  sample             28737536.000              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.999       100000         treeBuilderAdd  sample             57427755.008              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p0.9999      100000         treeBuilderAdd  sample             92259640.934              ns/op
     [java] BTreeBuildBench.buildTreeTest:buildTreeTest?p1.00        100000         treeBuilderAdd  sample            117833728.000              ns/op
{noformat}

So I would suggest removing {{Recycler}} (similar to [dc9ed46|https://github.com/apache/cassandra/commit/dc9ed463417aa8028e77e91718e4f3d6ea563210#diff-6aa10752b68c93ed354d642f9bdbe814L27]).","22/Feb/18 17:59;jay.zhuang;Hi [~tjake], any suggestion on that? Should we remove {{Recycler}} or just reduce the large builder array size?",22/Feb/18 22:10;tjake;I'd prefer to reduce the array size vs removing but you have been at this for so long I'm starting to think it's not worth keeping around.  I'd like to re-setup the test I had to check the improvement of CASSANDRA-9766 and see how much of a impact it has.,"23/Feb/18 02:55;jay.zhuang;I tested the change with {{LongStreamingTest}}, seems no impact with/without {{Recycler}}:
{noformat}
With Recycler
    [junit] ERROR [main] 2018-02-22 16:57:56,189 SubstituteLogger.java:250 - Writer finished after 22 seconds....
    [junit] ERROR [main] 2018-02-22 16:58:16,480 SubstituteLogger.java:250 - Finished Streaming in 20.29 seconds: 23.66 Mb/sec
    [junit] ERROR [main] 2018-02-22 16:58:35,921 SubstituteLogger.java:250 - Finished Streaming in 19.44 seconds: 24.69 Mb/sec
    [junit] ERROR [main] 2018-02-22 16:59:25,719 SubstituteLogger.java:250 - Finished Compacting in 49.80 seconds: 19.44 Mb/sec

No Recycler
    [junit] ERROR [main] 2018-02-22 16:50:48,255 SubstituteLogger.java:250 - Writer finished after 22 seconds....
    [junit] ERROR [main] 2018-02-22 16:51:08,209 SubstituteLogger.java:250 - Finished Streaming in 19.95 seconds: 24.06 Mb/sec
    [junit] ERROR [main] 2018-02-22 16:51:27,624 SubstituteLogger.java:250 - Finished Streaming in 19.41 seconds: 24.72 Mb/sec
    [junit] ERROR [main] 2018-02-22 16:52:16,900 SubstituteLogger.java:250 - Finished Compacting in 49.28 seconds: 19.48 Mb/sec
{noformat}

Here is the patch, please review:
| Branch | uTest |
| [13929-3.11|https://github.com/cooldoger/cassandra/tree/13929-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13929-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13929-3.11] |
| [13929-trunk|https://github.com/cooldoger/cassandra/tree/13929-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/13929-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/13929-trunk] |",23/Feb/18 09:32;norman;Just a general comment.... The recycler only makes sense to use if creating the object is considered very expensive and or if you create / destroy a lot of these very frequently. Which means usually thousands per second.  So if this is not the case here I think it completely reasonable to not use the Recycler at all... As I have no idea really about the use-case I am just leave this here as general comment :),"23/Feb/18 10:37;tsteinmaurer;The following does not include the latest patches from Feb 22, but shows last 30d on a single node (m4.2xlarge, Xmx12G, CMS) out of our 9 node loadtest environment including various tests/patches we have applied.
 !cassandra_heapcpu_memleak_patching_test_30d.png|width=1280!
 * Blue line => AVG heap utilization
 * Orange line => AVG CPU utilization (not really related as usually compaction is overlaying anything else most likely)

Following timelines in the chart:
||Timeframe||Deployment||Comment/Result||
|Jan 25 - Feb 1|Cassandra 3.11 public + Netty 4.0.55|(!) Heap utilization increase|
|Feb 1 - Feb 6|Cassandra 3.11 public + Netty 4.0.55 + limiting Netty capacity per Thread|(!) Heap utilization increase|
|Feb 6 - Feb 14|Cassandra 3.11 public + Netty 4.0.55 + my recycleHandle = null patch|(/) Heap utilization stable|
|Feb 14 - Feb 23|Cassandra 3.11 public + Netty 4.0.55 + *without* recycleHandle = null patch + first [~jay.zhuang] patch from Feb 13|(/) Heap utilization stable, but slightly increased to previous|

Very high-level (although from the field) compared to [~jay.zhuang] tests and benchmarks, but possibly useful for a decision process, hopefully being included in 3.11.3. Thanks guys!","05/Mar/18 05:02;jay.zhuang;Hi [~tjake], are you interested in reviewing the patch? The trunk uTest failure is because of CASSANDRA-14119.","05/Jun/18 21:45;cscetbon;Hey guys, any news on that issue ?","07/Jun/18 13:44;jasobrown;On the whole, this looks pretty good. I'm running tests locally now, and will run in circleci shortly. 

[~jay.zhuang] There's one thing I'm wondering: in {{#reuse{}}}, we do not explicitly null out the {{values}} array, like we used to do in {{#cleanup()}}:

{code}
    Arrays.fill(values, null);
{code}

While every access of {{values}} seems safe, and uses the {{count}} to ensure proper sizing of any built {{Object[]}}, I'd like to be over-cautious and null out the array to prevent the possibility of data leak wherein a reuse of Builder erroneously gets extra data from the last use. This error doesn't seem like it's possible in the current code, but I'd like to future-proof this - it would be a complete nightmare to debug. 

wdyt?

","07/Jun/18 14:07;jasobrown;[~tsteinmaurer] if you've been running with some version of this patch (where recycling is not used), do you see any degradation in streaming?","07/Jun/18 15:49;jasobrown;running [~jay.zhuang]'s patch with a few minor cleanups:

||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13929-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13929-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13929-3.11]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13929-trunk]|
||
","07/Jun/18 16:17;tsteinmaurer;[~jasobrown], sorry I don't have any streaming benchmarks before/after.","07/Jun/18 16:34;jasobrown;[~tsteinmaurer] ok, not a problem. As long as you don't see streaming getting obviously slower (1 hour -> 2 hours, for example), I'll take that as a data point.","07/Jun/18 18:06;jay.zhuang;Thanks [~jasobrown] for the review and fix.

Yes, null out the {{values}} array before reusing is a good practice. +1 on that. Seems the dTest is failed for 3.11 branch, but I don't think it's caused by this patch. Please let me know if I could commit.",08/Jun/18 12:45;jasobrown;[~jay.zhuang] +1,08/Jun/18 19:11;jay.zhuang;Thanks [~jasobrown] again for the review. Committed as [{{ed5f834}}|https://github.com/apache/cassandra/commit/ed5f8347ef0c7175cd96e59bc8bfaf3ed1f4697a].,,,,,,,,,,,,,,,,,,,,,,,,,
"test_simple_strategy_counters - consistency_test.TestAccuracy always fails code=2000 [Syntax error in CQL query] message=""line 7:14 mismatched input 'AND' expecting EOF (... text, age int ) [AND]...)",CASSANDRA-14025,13118683,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jkni,mkjellman,mkjellman,15/Nov/17 22:15,12/Mar/19 14:19,13/Mar/19 22:34,16/Nov/17 19:31,,,,,,Legacy/Testing,,,,,0,,,,"test_simple_strategy_counters - consistency_test.TestAccuracy always fails code=2000 [Syntax error in CQL query] message=""line 7:14 mismatched input 'AND' expecting EOF (... text,                age int            ) [AND]...)

<Error from server: code=2000 [Syntax error in CQL query] message=""line 7:14 mismatched input 'AND' expecting EOF (... text,                age int            ) [AND]...)"">
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-dYXpHm
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '256',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Testing single dc, counters
dtest: DEBUG: Changing snitch for single dc case
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered
--------------------- >> end captured logging << ---------------------
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/cassandra/cassandra-dtest/consistency_test.py"", line 704, in test_simple_strategy_counters
    self._run_test_function_in_parallel(TestAccuracy.Validation.validate_counters, [self.nodes], [self.rf], combinations)
  File ""/home/cassandra/cassandra-dtest/consistency_test.py"", line 535, in _run_test_function_in_parallel
    self._start_cluster(save_sessions=True, requires_local_reads=requires_local_reads)
  File ""/home/cassandra/cassandra-dtest/consistency_test.py"", line 147, in _start_cluster
    self.create_tables(session, requires_local_reads)
  File ""/home/cassandra/cassandra-dtest/consistency_test.py"", line 156, in create_tables
    self.create_users_table(session, requires_local_reads)
  File ""/home/cassandra/cassandra-dtest/consistency_test.py"", line 178, in create_users_table
    session.execute(create_cmd)
  File ""cassandra/cluster.py"", line 2122, in cassandra.cluster.Session.execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""cassandra/cluster.py"", line 3982, in cassandra.cluster.ResponseFuture.result
    raise self._final_exception
'<Error from server: code=2000 [Syntax error in CQL query] message=""line 7:14 mismatched input \'AND\' expecting EOF (... text,                age int            ) [AND]...)"">\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-dYXpHm\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'memtable_allocation_type\': \'offheap_objects\',\n    \'num_tokens\': \'256\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ndtest: DEBUG: Testing single dc, counters\ndtest: DEBUG: Changing snitch for single dc case\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered\n--------------------- >> end captured logging << ---------------------'
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-16 17:21:57.022,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 16 19:31:22 UTC 2017,,,,,,0|i3mu27:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"16/Nov/17 17:21;jkni;Patch [here|https://github.com/jkni/cassandra-dtest/commit/eff8d51c9cdd16e39afa871cd69b4df24a5f858a]. This is a trivial fix to table creation syntax. Prior to [CASSANDRA-10857], this table had a {{WITH COMPACT STORAGE}} suffix. With that removed, we need to introduce options with {{WITH}} ourselves.","16/Nov/17 19:02;jjirsa;+1
",16/Nov/17 19:31;jkni;Thanks for the review! Committed as {{d67fd66253ebb8aabc7d90d7ff048a4c9a2ff2cf}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't use SHA256 when building merkle trees,CASSANDRA-14002,13116901,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,08/Nov/17 07:58,12/Mar/19 14:19,13/Mar/19 22:34,05/Mar/18 07:22,4.0,,,,,,,,,,0,,,,We should avoid using SHA-2 when building merkle trees as we don't need a cryptographic hash function for this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-08 19:06:33.35,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 05 07:22:56 UTC 2018,,,,,,0|i3mj3j:,9223372036854775807,,,,,,,,mkjellman,mkjellman,,,,,,,,,,"08/Nov/17 08:30;krummas;Patch for this [here|https://github.com/krummas/cassandra/commits/marcuse/nosharepairs] - it replaces the SHA-256 with Murmur3.

Murmur3 is only 128 bit though, so the patch instead hashes every value twice using 2 different murmur3 instances with different seeds to get the same number of bits as SHA-256. The approach used is similar to what guava does in its ConcatenatedHashFunction.

In my tests with semi-wide partitions (~100KB mean partition size) this reduces the time spent building merkle trees with at least 50%.

https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/422/
https://circleci.com/gh/krummas/cassandra/173

cc [~mkjellman]","08/Nov/17 19:06;mkjellman;awesome! I'm guessing we don't need to worry about the upgrade path here because streaming will be broken between majors anyways, right?","08/Nov/17 19:17;jasobrown;bq. we don't need to worry about the upgrade path here because streaming will be broken between majors anyways

This is correct, but not because ""streaming is broken between majors"". There are no guarantees that an un-upgraded node would be parse to read sstables streamed to it from an upgrade node, as the sstable format may have changed (we do not force/guarantee backward compatibility of the sstable format). Thus, we don't support streaming between major versions (although there's nothing that bluntly states this, at least as far as I've seen).
","08/Nov/17 20:44;mkjellman;So +1 from me. Comments look good and I don't even see any code style related nits. 50% faster is a bigger win than I expected when I saw it in profiling a long long long time ago to be honest... so that's super awesome news! 

Looks like a few repair related dtests did fail but looking at stdout it seems that this is weirdness with the Apache Jenkins instance and ccm not starting instances cleanly and not any fallout from this change.","09/Nov/17 08:27;krummas;thanks for the review!

bq. 50% faster is a bigger win than I expected when I saw it in profiling
yeah, I think the biggest win is actually that guavas streaming hashers actually buffer the data to be hashed instead of hashing them byte-for-byte: https://github.com/google/guava/blob/v23.3/guava/src/com/google/common/hash/AbstractStreamingHasher.java#L34

I'm rerunning the dtests [here|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/424/] before committing",09/Nov/17 09:17;krummas;also pushed a new commit after rebasing on latest trunk (upgrade guava to 23.3) if you could have a look [~mkjellman] (https://github.com/krummas/cassandra/commits/marcuse/nosharepairs),27/Feb/18 17:00;mkjellman;+1 to rebase.,"05/Mar/18 07:22;krummas;and committed as {{68b81372cd838808b304d58677cdc86f6ec35ffa}}, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disks can be imbalanced during replace of same address when using JBOD,CASSANDRA-14084,13121966,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,30/Nov/17 17:00,12/Mar/19 14:19,13/Mar/19 22:34,11/Dec/17 20:24,3.11.2,4.0,,,,,,,,,0,,,,"While investigating CASSANDRA-14083, I noticed that [we use the pending ranges to calculate the disk boundaries|https://github.com/apache/cassandra/blob/41904684bb5509595d11f008d0851c7ce625e020/src/java/org/apache/cassandra/db/DiskBoundaryManager.java#L91] when the node is bootstrapping.

The problem is that when the node is replacing a node with the same address, it [sets itself as normal locally|https://github.com/apache/cassandra/blob/41904684bb5509595d11f008d0851c7ce625e020/src/java/org/apache/cassandra/service/StorageService.java#L1449] (for other unrelated reasons), so the local ranges will be null and consequently the disk boundaries will be null. This will cause the sstables to be randomly spread across disks potentially causing imbalance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12344,CASSANDRA-14083,,,,,01/Dec/17 13:35;pauloricardomg;dtest14084.png;https://issues.apache.org/jira/secure/attachment/12900219/dtest14084.png,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-12-11 08:10:08.949,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 11 20:24:57 UTC 2017,,,,,,0|i3ne9r:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"30/Nov/17 17:18;pauloricardomg;This situation is reproduced by [this dest|https://github.com/pauloricardomg/cassandra-dtest/commit/1b96dfd855d1b2fc10cbb4cf2e4c95d236ecd951#diff-1ef92939c7765f8c4041bada71208eebR51].

The simple fix is to use normal tokens for replacement nodes with the same address:
* [3.11|https://github.com/pauloricardomg/cassandra/tree/3.11-14084]

CI looked clean when this was in CASSANDRA-13948, but I will submit again just to make sure this will not cause problems when committed separately.","01/Dec/17 13:36;pauloricardomg;testall is clean, dtest failures [seem unrelated|https://issues.apache.org/jira/secure/attachment/12900219/dtest14084.png]",11/Dec/17 08:10;krummas;+1,"11/Dec/17 20:24;pauloricardomg;Committed as {{50e6e721b2a81da7f11f60a2fa405fd46e5415d4}} to cassandra-3.11 and merged up to master, and dtest as {{3d2a6cc738d87d30cca8d747305a5899ccf3712d}}. Thanks for the review!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest code style check failed,CASSANDRA-14076,13121512,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,29/Nov/17 01:46,12/Mar/19 14:19,13/Mar/19 22:34,04/Dec/17 10:24,,,,,,Test/dtest,,,,,0,,,,"https://travis-ci.org/cooldoger/cassandra-dtest
{noformat}
$ flake8 --ignore=E501,F811,F812,F822,F823,F831,F841,N8,C9 --exclude=thrift_bindings,cassandra-thrift .
./consistency_test.py:547:17: E722 do not use bare except'
./consistency_test.py:976:49: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:976:51: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:981:63: E703 statement ends with a semicolon
./consistency_test.py:1037:49: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:1037:51: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:1054:46: E261 at least two spaces before inline comment
./consistency_test.py:1103:22: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:1103:24: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:1175:22: E251 unexpected spaces around keyword / parameter equals
./consistency_test.py:1175:24: E251 unexpected spaces around keyword / parameter equals
./counter_tests.py:59:24: E703 statement ends with a semicolon
./counter_tests.py:383:37: E261 at least two spaces before inline comment
./dtest.py:586:13: E722 do not use bare except'
./dtest.py:1130:1: E302 expected 2 blank lines, found 1
./nodetool_test.py:9:1: E302 expected 2 blank lines, found 1
./nodetool_test.py:78:1: W293 blank line contains whitespace
./nodetool_test.py:174:45: E261 at least two spaces before inline comment
./run_dtests.py:220:54: E221 multiple spaces before operator
./secondary_indexes_test.py:14:1: F401 'dtest.DtestTimeoutError' imported but unused
./secondary_indexes_test.py:17:1: F401 'tools.data.index_is_built' imported but unused
./secondary_indexes_test.py:21:1: E302 expected 2 blank lines, found 1
./sslnodetonode_test.py:15:1: E302 expected 2 blank lines, found 1
./sslnodetonode_test.py:191:1: W293 blank line contains whitespace
./sslnodetonode_test.py:191:1: W391 blank line at end of file
./system_keyspaces_test.py:6:1: E302 expected 2 blank lines, found 1
./system_keyspaces_test.py:28:59: E241 multiple spaces after ','
./system_keyspaces_test.py:50:62: E241 multiple spaces after ','
./write_failures_test.py:5:1: F401 'distutils.version.LooseVersion' imported but unused
./plugins/dtestcollect.py:1:1: F401 'collections.namedtuple' imported but unused
./plugins/dtestcollect.py:3:1: F401 'pprint.pprint' imported but unused
./plugins/dtestcollect.py:5:1: F401 'inspect' imported but unused
./plugins/dtestcollect.py:13:1: E302 expected 2 blank lines, found 1
./plugins/dtestcollect.py:44:9: E306 expected 1 blank line before a nested definition, found 0
./plugins/dtestcollect.py:62:22: E703 statement ends with a semicolon
./plugins/dtestcollect.py:64:1: E302 expected 2 blank lines, found 1
./plugins/dtesttag.py:1:1: F401 'collections.namedtuple' imported but unused
./plugins/dtesttag.py:4:1: F401 'pprint.pprint' imported but unused
./plugins/dtesttag.py:8:1: E302 expected 2 blank lines, found 1
./plugins/dtesttag.py:20:1: W293 blank line contains whitespace
./plugins/dtesttag.py:25:1: W293 blank line contains whitespace
./plugins/dtestxunit.py:43:1: F401 'doctest' imported but unused
./plugins/dtestxunit.py:46:1: F401 'traceback' imported but unused
./plugins/dtestxunit.py:62:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:66:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:70:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:76:29: E226 missing whitespace around arithmetic operator
./plugins/dtestxunit.py:84:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:107:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:126:1: E302 expected 2 blank lines, found 1
./plugins/dtestxunit.py:219:32: W503 line break before binary operator
./plugins/dtestxunit.py:269:25: E126 continuation line over-indented for hanging indent
./plugins/dtestxunit.py:277:25: E126 continuation line over-indented for hanging indent
./repair_tests/deprecated_repair_test.py:159:9: E741 ambiguous variable name 'l'
./repair_tests/incremental_repair_test.py:772:4: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:773:76: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:774:69: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:804:4: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:805:87: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:806:73: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:830:4: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:831:87: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:832:73: W291 trailing whitespace
./repair_tests/incremental_repair_test.py:855:55: E231 missing whitespace after ','
./repair_tests/incremental_repair_test.py:855:57: E231 missing whitespace after ','
./repair_tests/incremental_repair_test.py:855:59: E231 missing whitespace after ','
./tools/data.py:154:1: E302 expected 2 blank lines, found 1
./tools/data.py:167:34: W292 no newline at end of file
./upgrade_tests/thrift_upgrade_test.py:25:65: E261 at least two spaces before inline comment
./upgrade_tests/thrift_upgrade_test.py:28:1: E302 expected 2 blank lines, found 1
./upgrade_tests/thrift_upgrade_test.py:37:1: E302 expected 2 blank lines, found 1
{noformat}

Running {{pep8}} is also failed because it's renamed to {{pycodestyle}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14020,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-30 23:23:14.236,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 04 10:24:10 UTC 2017,,,,,,0|i3nbhj:,9223372036854775807,,,,,,,,philipthompson,philipthompson,,,,,,,,,,"30/Nov/17 23:22;jay.zhuang;Here is the patch, please review:
| Branch | TravisCI Build Status |
| [14076|https://github.com/cooldoger/cassandra-dtest/tree/14076] | [!https://travis-ci.org/cooldoger/cassandra-dtest.svg?branch=14076!|https://travis-ci.org/cooldoger/cassandra-dtest/builds/309766256] |","30/Nov/17 23:23;jjirsa;cc [~philipthompson]

(Also actual branch is https://github.com/cooldoger/cassandra-dtest/tree/14076 ) 
","01/Dec/17 10:21;philipthompson;+1, changes all look like valid, trivial [as in not behavior changing] pep8 cleanup.",04/Dec/17 10:24;philipthompson;Committed at 44fd9be91d38345bf666c2118693489f9a46de20. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Materialized view on table with TTL issue,CASSANDRA-14071,13120890,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,sbutnariu,sbutnariu,27/Nov/17 10:52,12/Mar/19 14:19,13/Mar/19 22:34,06/Dec/17 21:42,3.0.16,3.11.2,,,,Feature/Materialized Views,Legacy/Coordination,,,,0,correctness,,,"Materialized views that cluster by a column that is not part of table's PK and are created from tables that have *default_time_to_live* seems to malfunction.

Having this table
{code:java}
CREATE TABLE sbutnariu.test_bug (
    field1 smallint,
    field2 smallint,
    date timestamp,
    PRIMARY KEY ((field1), field2)
) WITH default_time_to_live = 1000;
{code}

and the materialized view
{code:java}
CREATE MATERIALIZED VIEW sbutnariu.test_bug_by_date AS SELECT * FROM sbutnariu.test_bug WHERE field1 IS NOT NULL AND field2 IS NOT NULL AND date IS NOT NULL PRIMARY KEY ((field1), date, field2) WITH CLUSTERING ORDER BY (date desc, field2 asc);
{code}

After inserting 3 rows with same PK (should upsert), the materialized view will have 3 rows.
{code:java}
insert into sbutnariu.test_bug(field1, field2, date) values (1, 2, toTimestamp(now()));
insert into sbutnariu.test_bug(field1, field2, date) values (1, 2, toTimestamp(now()));
insert into sbutnariu.test_bug(field1, field2, date) values (1, 2, toTimestamp(now()));

select * from sbutnariu.test_bug; /*1 row*/
select * from sbutnariu.test_bug_by_date;/*3 rows*/
{code}

If I remove the ttl and try again, it works as expected:
{code:java}
truncate sbutnariu.test_bug;
alter table sbutnariu.test_bug with default_time_to_live = 0;

select * from sbutnariu.test_bug; /*1 row*/
select * from sbutnariu.test_bug_by_date;/*1 row*/
{code}

I've tested on versions 3.0.14 and 3.0.15. The bug was introduced in 3.0.15, as in 3.0.14 it works as expected.",Cassandra 3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14193,CASSANDRA-11500,,,,,,06/Dec/17 21:27;pauloricardomg;14071-3.0-dtest.png;https://issues.apache.org/jira/secure/attachment/12900961/14071-3.0-dtest.png,06/Dec/17 21:27;pauloricardomg;14071-3.0-testall.png;https://issues.apache.org/jira/secure/attachment/12900960/14071-3.0-testall.png,06/Dec/17 21:27;pauloricardomg;14071-3.11-dtest.png;https://issues.apache.org/jira/secure/attachment/12900959/14071-3.11-dtest.png,06/Dec/17 21:27;pauloricardomg;14071-3.11-testall.png;https://issues.apache.org/jira/secure/attachment/12900958/14071-3.11-testall.png,06/Dec/17 21:27;pauloricardomg;14071-trunk-dtest.png;https://issues.apache.org/jira/secure/attachment/12900957/14071-trunk-dtest.png,06/Dec/17 21:27;pauloricardomg;14071-trunk-testall.png;https://issues.apache.org/jira/secure/attachment/12900956/14071-trunk-testall.png,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2017-11-27 13:49:48.462,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 26 17:15:39 UTC 2018,,,,,,0|i3n7n3:,9223372036854775807,3.0.15,,,,,,,pauloricardomg,pauloricardomg,,,3.0.15,,,,,,,27/Nov/17 13:49;pauloricardomg;Thanks for the bug report and reproduction steps. I think that this may be related to CASSANDRA-11500. Will have a look later this week (cc [~jasonstack]),"28/Nov/17 08:59;jasonstack;Thanks for the report. This is indeed an issue from CASSANDRA-11500.

In 11500, an expired livenessInfo concept is introduced to MV, in order to mark entire view row as dead without purging row cells as tombstone.
When table has default ttl, the MV row to be removed has a bigger {{localDeletionTime}} than the generated expired livenessInfo's. (see LivenessInfo.supersedes())

The fix will be: when livenessInfo timestamp ties,  MV expired livenessInfo (by checking TTL==MAX) will supersedes another. I will check again if there is anything left for MV reconciliation.","29/Nov/17 09:21;jasonstack;| source | utest | dtest |
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-14071] | [3.0|https://circleci.com/gh/jasonstack/cassandra/658] | failures not related |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-14071-3.11] | [3.11|https://circleci.com/gh/jasonstack/cassandra/656] | running |
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-14071-trunk] | [trunk|https://circleci.com/gh/jasonstack/cassandra/657] | failures not related |
| [dtest|https://github.com/apache/cassandra-dtest/compare/master...jasonstack:CASSANDRA-14071?expand=1]|

{Code}
Changes:
1. Added ExpiredLivenessInfo as subclass of ExpiringLivenessInfo, it's always {{expired}}(act as tombstone) regardless {{nowInSecs}} local time.
2. Added additional check in {{LivenessInfo.supersedes()}}. When timestamp tie, ExpiredLivenessInfo (by checking ttl==MAX) will supersedes another non-ExpiredLivenessInfo.
{Code}

[~pauloricardomg] what do you think?","01/Dec/17 21:19;pauloricardomg;bq. In 11500, an expired livenessInfo concept is introduced to MV, in order to mark entire view row as dead without purging row cells as tombstone. When table has default ttl, the MV row to be removed has a bigger localDeletionTime than the generated expired livenessInfo's. (see LivenessInfo.supersedes())

Good investigation! On #11500 we focused on the case were TTLs are mixed with ordinary writes but missed the overwrite with ttl case.

bq. The fix will be: when livenessInfo timestamp ties, MV expired livenessInfo (by checking TTL==MAX) will supersedes another. I will check again if there is anything left for MV reconciliation.

The approach looks good to me but even though this looks safe I think we should restrict this new resolution rule only to pk liveness info of views to ensure no other code path can be affected by this since this is a pretty critical path. What do you think? 

Also, besides the case with default ttl, this also affects overwrites with ttl right? Can you add a test with this case as well?

Thanks!","03/Dec/17 13:09;jasonstack;Thanks for the feedback.

bq. The approach looks good to me but even though this looks safe I think we should restrict this new resolution rule only to pk liveness info of views to ensure no other code path can be affected by this since this is a pretty critical path. What do you think?

Current TTL in insert/update request is at most 20 yrs defined in Attributes.java, but there is no TTL limit for table default_time_to_live. I think using a very large default_time_to_live is not reasonable because {{TTL + nowInSeconds}} can cause int32 overflow. 

I added an validation logic in {{TableParams}} to make sure default_time_to_live will never be {{Integer.MAX_VALUE}} which is reserved for MV..

Or do you think adding a method param, eg {{isView}}, to {{LivenessInfo.supersedes()}} is safer?

bq. Also, besides the case with default ttl, this also affects overwrites with ttl right? Can you add a test with this case as well?

Make sense, I have updated the test with user-provided TTL","05/Dec/17 03:30;jasonstack;As we discussed offline, it's better to keep table default_time_to_live having the same maximum as per-request ttl, ie. 20 years. 
* If ttl is 21 yrs, then ttl + nowInSecond(2017/12/03) will overflow int32, newly inserted data is dead.

Updated the patch and restarted CI.",05/Dec/17 09:20;sbutnariu;Great news that you find and are on your way of fixing the issue! Any idea on what version will this fix be launched?,06/Dec/17 01:38;jasonstack;It will be in latest bugfix version: 3.0.x/3.11.x/4.0,"06/Dec/17 01:39;jasonstack;CI looks good. There is some NPE on view unit test while creating keyspace, but it looks related to CASSANDRA-14010","06/Dec/17 02:27;pauloricardomg;Patch and tests look good, I just made this following minor changes:
* Improve docs a bit to give more information on how {{ExpiredLivenessInfo}} is used [here|https://github.com/pauloricardomg/cassandra/commit/0098a04f50db06266ecd54cd115de8b167c38862]
* Avoid unnecessary comparison on [LivenessInfo.supersedes|https://github.com/pauloricardomg/cassandra/commit/d4eafba373973c870158bcb2b005746f9f59aa2d].
* Reinstate [removed assertion|https://github.com/pauloricardomg/cassandra/commit/7a79e0837004415885b8cddaf2f45ba21d493bc3]

I rebased submitted CI with these changes:
* [3.0|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14071]
* [3.11|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14071-3.11]
* [trunk|https://github.com/pauloricardomg/cassandra/tree/CASSANDRA-14071-trunk]

Please let me know what do you think. ","06/Dec/17 02:33;jasonstack;LGTM, thanks for the review","06/Dec/17 21:32;pauloricardomg;Patch is very well tested, good job! CI looks as good as it gets (only unrelated failures, screenshots of internal CI attached). Committed patch to cassandra-3.0 branch as {{461af5b9a6f58b6ed3db78a879840816b906cac8}} and merged up to cassandra-3.11 and trunk.

Committed dtest as {{b5fde208857a11a13cabf8f2e00aca986d133b0f}} and {{ccc6e188b4b419dd4a0d8d1245a6138ab26d3d7e}}. Thanks!",26/Jan/18 17:15;aweisberg;I bisected and this causes two test failures in TTLTest. I bisected to 461af5b9a6f58b6ed3db78a879840816b906cac8. Created [CASSANDRA-14193|https://issues.apache.org/jira/browse/CASSANDRA-14193] to track it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Many sslnodetonode_test.TestNodeToNodeSSLEncryption tests failing with ""Please remove properties [optional, enabled] from your cassandra.yaml""",CASSANDRA-14075,13121496,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,mkjellman,mkjellman,29/Nov/17 00:32,12/Mar/19 14:19,13/Mar/19 22:34,01/Dec/17 13:32,,,,,,Legacy/Testing,,,,,0,,,,"Many sslnodetonode_test.TestNodeToNodeSSLEncryption dtests are failing on 3.11 with an exception on startup due to invalid yaml properties.

Unexpected error in node1 log, error: 
ERROR [main] 2017-11-18 21:01:54,781 CassandraDaemon.java:706 - Exception encountered during startup: Invalid yaml. Please remove properties [optional, enabled] from your cassandra.yaml 

Although ccm was updated in https://github.com/pcmanus/ccm/commit/eaaa425b70edb84786924516aee3920d685c0e53 to include a version check for >= 4.0, enabled and optional are emitted unconditionally in the actual dtest itself -- they should also be conditional on >= 4.0

{code:java}
node.set_configuration_options(values={
            'server_encryption_options': {
                'enabled': encryption_enabled,
                'optional': encryption_optional,
                'internode_encryption': internode_encryption,
                'keystore': kspath,
                'keystore_password': 'cassandra',
                'truststore': tspath,
                'truststore_password': 'cassandra',
                'require_endpoint_verification': endpoint_verification,
                'require_client_auth': client_auth,
            }
        })
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-30 19:04:04.954,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 01 13:32:40 UTC 2017,,,,,,0|i3nbdz:,9223372036854775807,,,,,,,,mkjellman,mkjellman,,,,,,,,,,"30/Nov/17 19:04;jasobrown;[~mkjellman]'s evaluations is correct: in CASSANDRA-10404, I didn't correctly support pre-4.0 in this dtest. Here is a [dtest patch|https://github.com/jasobrown/cassandra-dtest/tree/14075] that checks the cluster version and only adds the new props if the it's greater than or equal to 4.0.

Here are runs of the dtest patch against both 3.11 and trunk:

||3.11||trunk||
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14075-3.11]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14075-trunk]|
||


Note: I also ran this locally with jdk1.8.0_151, and started getting this warning:

{noformat}
Warning:
The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using ""keytool -importkeystore -srckeystore /tmp/tmpICn9py/ca.keystore -destkeystore /tmp/tmpICn9py/ca.keystore -deststoretype pkcs12"".
{noformat}

I've also updated {{sslkeygen.py}} in this patch with a trivial fix to eliminate the warning.
",30/Nov/17 22:56;mkjellman;looks good! +1,01/Dec/17 13:32;jasobrown;committed as sha {{9da3a2594bf75cbfd4852ee9b4b3e44c28ff618f}} to dtests repo.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add_dc_after_mv_network_replication_test - materialized_views_test.TestMaterializedViews fails due to invalid datacenter,CASSANDRA-14023,13118679,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,mkjellman,mkjellman,15/Nov/17 22:12,12/Mar/19 14:19,13/Mar/19 22:34,11/Apr/18 12:20,,,,,,Legacy/Testing,,,,,0,,,,"add_dc_after_mv_network_replication_test - materialized_views_test.TestMaterializedViews always fails due to:

<Error from server: code=2300 [Query invalid because of configuration issue] message=""Unrecognized strategy option {dc2} passed to NetworkTopologyStrategy for keyspace ks"">
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-16 10:07:41.565,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 10 12:11:43 UTC 2018,,,,,,0|i3mu1b:,9223372036854775807,,,,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,"15/Nov/17 22:13;mkjellman;Looking at the tests, all the keyspaces are created with SimpleStrategy so i don't know how this would ever have worked.","16/Jan/18 10:07;krummas;https://github.com/krummas/cassandra-dtest/commits/marcuse/14023

a few problems with the test
- we can't add dc2 before it has any nodes
- we need to run queries with LOCAL_ONE to make sure the reads don't spill over to dc1
- we need to run rebuild to actually get data in dc2",09/Feb/18 15:33;krummas;[~pauloricardomg] do you have time to review this?,"05/Apr/18 22:13;pauloricardomg;Sorry for the delay. LGTM, thanks!","10/Apr/18 12:11;krummas;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Many dtests fail with ConfigurationException: offheap_objects are not available in 3.0 when OFFHEAP_MEMTABLES=""true""",CASSANDRA-14056,13119115,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alourie,mkjellman,mkjellman,17/Nov/17 05:03,12/Mar/19 14:19,13/Mar/19 22:34,21/Jun/18 13:39,,,,,,Test/dtest,,,,,0,,,,"Tons of dtests are running when they shouldn't as it looks like the path is no longer supported.. we need to add a bunch of logic that's missing to fully support running dtests with off-heap memtables enabled (via the OFFHEAP_MEMTABLES=""true"" environment variable)

{code}[node2 ERROR] java.lang.ExceptionInInitializerError
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:394)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:361)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:577)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:554)
	at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:368)
	at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:305)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:129)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:106)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:887)
	at org.apache.cassandra.service.StartupChecks$9.execute(StartupChecks.java:354)
	at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:110)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:179)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697)
Caused by: org.apache.cassandra.exceptions.ConfigurationException: offheap_objects are not available in 3.0. They will be re-introduced in a future release, see https://issues.apache.org/jira/browse/CASSANDRA-9472 for details
	at org.apache.cassandra.config.DatabaseDescriptor.getMemtableAllocatorPool(DatabaseDescriptor.java:1907)
	at org.apache.cassandra.db.Memtable.<clinit>(Memtable.java:65)
	... 14 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-05 01:13:32.506,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 25 04:11:38 UTC 2018,,,,,,0|i3mwpz:,9223372036854775807,3.11.1,,,,,,,jasobrown,jasobrown,,,,,,,,,,05/Dec/17 01:13;alourie;I'll have a poke at this.,"07/Dec/17 02:41;alourie;[~mkjellman] I've added a check for a specific C* version to avoid actually running any tests if the C* version is 3.0.x and less than 3.4; the patch is  https://github.com/alourie/cassandra-dtest/commit/0a9661e3ed404856302ab05de4d51b2d65e9e872. Please have a look whether that's what you had in mind.

Thanks.",01/Feb/18 08:11;alourie;[~mkjellman] poke. I hope you find some time to have a look at the patch.,"01/Feb/18 10:58;jasobrown;[~alourie] Thanks for the patch. Is this still a problem after CASSANDRA-14134? At a minimum, [~mkjellman] killed off the environment varibles, and the literal string ""OFFHEAP_MEMTABLES"" doesn't exist {{master}} anymore.","06/Feb/18 03:42;alourie;[~jasobrown] [~mkjellman] Yes, this would still be an issue with the new dtests. Someone might want to test with the ""offheap_objects"" configuration option set. The new dtests supports this with a command-line parameter, ""--use-off-heap-memtables""; as a result, I've updated the patch and opened a PR at [https://github.com/apache/cassandra-dtest/pull/18.]

 

Please let me know if you think this is the appropriate solution, and if there are any problems with the patch.

Thanks!",27/Mar/18 04:46;alourie;[~jasobrown] [~mkjellman] Just checking with you if you have any more comments or feedback. Thanks!,"20/Jun/18 03:20;alourie;Hey [~jasobrown] [~mkjellman], could you spare few moments to look at the patch? Thanks!","20/Jun/18 13:31;jasobrown;[~alourie] PR doesn't seem to be able to show diffs. Can you take a look?

UPDATE: I see [this branch|https://github.com/alourie/cassandra-dtest/tree/CASSANDRA-14056]. I'll assume it's the correct one?","20/Jun/18 13:52;jasobrown;[~alourie] running dtests with your patch locally still yields the same error in the c* instance logs. I'm running like this:

{code}
pytest --cassandra-dir=/opt/dev/cassandra --use-off-heap-memtables jmx_test.py
{code}

Am I missing something? I ran with both 3.0 and 3.2. 

fwiw, I think your patch needs a less than *or equal* to 3.0 check, like this:

{code}
if args.use_off_heap_memtables and (""3.0"" <= CASSANDRA_VERSION < ""3.4""):
{code}
","20/Jun/18 13:53;jasobrown;ohh, your patch applies to run_dtests, not just invoking pytest directly.","21/Jun/18 05:08;alourie;[~jasobrown] You're right, I don't know how I tested before and it worked...in any case, I've fixed both issues. Now it checks the versions including 3.0 and when invoking pytest directly. The PR is at https://github.com/apache/cassandra-dtest/pull/18 and the branch is https://github.com/apache/cassandra-dtest/compare/master...alourie:CASSANDRA-14056. Please have a look again if you can.

Thanks!","21/Jun/18 13:39;jasobrown;Updated commit lgtm. committed as {{2313e39ee19ea12f304d3e6dd04c41d4f03798ca}}. Thanks!

[~alourie] Can you close the PR, as well?",22/Jun/18 22:43;jasobrown;committed followup sha {{b617ea1b0a2ce9dd0ec16af20d9d9e045ce3fdc7}} to fix a bad import line from {{run_dtests.py}},"23/Jun/18 11:32;jasobrown;uggh, one more followup commit, sha {{b76b49e6ac895ead13dde707ab606bdef99d4894}}",25/Jun/18 04:11;alourie;PR is closed too. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException when selecting column counter and consistency quorum,CASSANDRA-14167,13130995,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,fcofdezc,tristar,tristar,15/Jan/18 11:15,12/Mar/19 14:19,13/Mar/19 22:34,04/Jul/18 10:14,3.0.17,3.11.3,,,,Legacy/Coordination,,,,,0,,,,"This morning I upgraded my cluster from 3.11.0 to 3.11.1 and it appears when I perform a query on a counter specifying the column name cassandra throws the following exception:
{code:java}
WARN [ReadStage-1] 2018-01-15 10:58:30,121 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-1,5,main]: {}
java.lang.IndexOutOfBoundsException: null
java.nio.Buffer.checkIndex(Buffer.java:546) ~[na:1.8.0_144]
java.nio.HeapByteBuffer.getShort(HeapByteBuffer.java:314) ~[na:1.8.0_144]
org.apache.cassandra.db.context.CounterContext.headerLength(CounterContext.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.context.CounterContext.updateDigest(CounterContext.java:696) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.rows.AbstractCell.digest(AbstractCell.java:126) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.rows.AbstractRow.digest(AbstractRow.java:73) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.rows.UnfilteredRowIterators.digest(UnfilteredRowIterators.java:181) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.partitions.UnfilteredPartitionIterators.digest(UnfilteredPartitionIterators.java:263) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadResponse.makeDigest(ReadResponse.java:120) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadResponse.createDigestResponse(ReadResponse.java:87) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:345) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:50) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_144]
org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
{code}

Query works completely find on consistency level ONE but not on QUORUM. 
Is this possibly related to CASSANDRA-11726?","Cassandra 3.11.1

Ubuntu 14-04",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-20 15:22:56.147,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 04 10:14:47 UTC 2018,,,,,,0|i3owvj:,9223372036854775807,,,,,,,,slebresne,slebresne,,,,,,,,,,"20/Feb/18 15:22;dmody;I am facing the similar issue after upgrading from 3.0.15 to 3.11.1 .. 
The system.log file - has a bunch of these warnings- 
WARN  [ReadStage-15] 2018-02-20 14:39:49,929 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-15,5,main]: {}
java.lang.IndexOutOfBoundsException: null
        at java.nio.Buffer.checkIndex(Buffer.java:546) ~[na:1.8.0_141]
        at java.nio.HeapByteBuffer.getShort(HeapByteBuffer.java:314) ~[na:1.8.0_141]
        at org.apache.cassandra.db.context.CounterContext.headerLength(CounterContext.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.context.CounterContext.updateDigest(CounterContext.java:696) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.rows.AbstractCell.digest(AbstractCell.java:126) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.rows.AbstractRow.digest(AbstractRow.java:73) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.rows.UnfilteredRowIterators.digest(UnfilteredRowIterators.java:181) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators.digest(UnfilteredPartitionIterators.java:263) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.ReadResponse.makeDigest(ReadResponse.java:120) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.ReadResponse.createDigestResponse(ReadResponse.java:87) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:345) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:50) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_141]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_141]

Can someone please advise on how to resolve this? 
This is causing application failure to write data (it uses consistency level - local_quoram for both reads and writes). ",21/Feb/18 15:07;dmody;[~slebresne] [~iamaleksey] - Hi Aleksey and Sylvain - I saw https://issues.apache.org/jira/browse/CASSANDRA-11726 which is similar to the issue I'm facing - can you please let me know on how can I fix this? Should I downgrade to 3.11.0 from 3.11.1 - would that help in fixing this issue? ,"02/Mar/18 16:31;jwartnic;I am having the exact same problem. Upgraded to Cassandra 3.11.1. I have a reproducible case with a record.

I even tried copying the data out to a flat-file, truncating the table, and re-loading it back in. Then flushed the memtables and re-queried a single, random, record which fails with consistency LOCAL_QUORUM but works fine with consistency ONE OR works fine if you don't specify the column name (e.g. SELECT *). 

One workaround that worked for us - as this is NOT a production environment yet, was to set the RF=1 on the keyspaces that contained the ""counter"" tables. By doing that the CF of QUORUM equates to 1, which then fixes the issue (no read repairs or validations required). Obviously if this was production, I would have some concerns regarding data availability, but for now, this is a workaround that may work for someone else.

 ","11/Apr/18 06:39;sinorga;Hi, I got the same warn log with Cassandra 3.11.1, it also comes with another error log which seems to be related.
{code:java}
ERROR [ReadRepairStage:112269] 2018-04-11 05:44:46,563 CassandraDaemon.java:228 - Exception in thread Thread[ReadRepairStage:112269,5,main]
java.lang.IndexOutOfBoundsException: null
	at java.nio.Buffer.checkIndex(Buffer.java:546) ~[na:1.8.0_151]
	at java.nio.HeapByteBuffer.getShort(HeapByteBuffer.java:314) ~[na:1.8.0_151]
	at org.apache.cassandra.db.context.CounterContext.headerLength(CounterContext.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.context.CounterContext.updateDigest(CounterContext.java:696) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.AbstractCell.digest(AbstractCell.java:126) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.AbstractRow.digest(AbstractRow.java:73) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.rows.UnfilteredRowIterators.digest(UnfilteredRowIterators.java:181) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators.digest(UnfilteredPartitionIterators.java:263) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.ReadResponse.makeDigest(ReadResponse.java:120) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.db.ReadResponse$DataResponse.digest(ReadResponse.java:225) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.service.DigestResolver.compareResponses(DigestResolver.java:87) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.service.ReadCallback$AsyncRepairRunner.run(ReadCallback.java:233) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_151]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_151]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_151]
{code}
The environment Info:
 * Cluster nodes: 3
 * RF: 3
 * Some counter tables
 * Consistency Level

 * 
 ** Read: ONE
 ** Write: LOCAL_QUORUM

The problem is I can't find the exact query command to reproduce this issue so far. And I didn't get any error from the application level.
Is there any way to locate the query? Could It be triggered Cassandra internal jobs or write action?

Any suggestion is appreciated.

 ","22/Apr/18 11:11;fcofdezc;The issue seems to be related to an optimization (CASSANDRA-10657) making counter cell empty. 
Here you can find a [patch|https://github.com/apache/cassandra/compare/cassandra-3.11...fcofdez:CASSANDRA-14167?expand=1] that solves the issue.","23/Apr/18 08:06;slebresne;+1 on that patch. As mentioned by [~dhawalmody1] above, it is very similar to CASSANDRA-11726 (and indeed ultimately due to proper handling of CASSANDRA-10657).

[~fcofdezc] Would you have time to quickly setup CircleCI (http://cassandra.apache.org/doc/latest/development/testing.html#circleci) and run it on this? I can take care of it though if you prefer.","23/Apr/18 09:26;fcofdezc;[Here|https://circleci.com/gh/fcofdez/cassandra/8] it is. If you need something else, just tell me. Thanks for the review!",02/Jul/18 07:11;gzeska;[~slebresne] [~fcofdezc] what is the status of this issue ? It is going to be fixed with next release ? ,"04/Jul/18 10:14;slebresne;Sorry, it slipped to the cracks. CI was clean so committed. As the vote for 3.0.17 and 3.11.3 got -1ed yesterday, this should make the next releases, whenever those happen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DTEST] [TRUNK] repair_test.py::test_dead_coordinator is flaky due to JMX connection error from nodetool,CASSANDRA-14158,13129769,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,mkjellman,mkjellman,10/Jan/18 07:16,12/Mar/19 14:19,13/Mar/19 22:34,30/Jan/18 08:02,,,,,,Test/dtest,,,,,0,,,,"repair_test.py::test_dead_coordinator is flaky due to occasionally failing when a JMX connection error is propagated by nodetool.

the test has failed 4+ times for the same reason.

latest failure can be found in the artifacts for the following circleci run:
[https://circleci.com/gh/mkjellman/cassandra/538]

I *think* that this might be expected behavior for this test and we just need to catch any ToolError exceptions thrown and only fail if included stack is for any error other than ""JMX connection closed.""

{code}
stderr: error: [2018-01-10 07:07:55,178] JMX connection closed. You should check server log for repair status of keyspace system_traces(Subsequent keyspaces are not going to be repaired).
-- StackTrace --
java.io.IOException: [2018-01-10 07:07:55,178] JMX connection closed. You should check server log for repair status of keyspace system_traces(Subsequent keyspaces are not going to be repaired).
	at org.apache.cassandra.tools.RepairRunner.handleConnectionFailed(RepairRunner.java:104)
	at org.apache.cassandra.utils.progress.jmx.JMXNotificationProgressListener.handleNotification(JMXNotificationProgressListener.java:86)
	at javax.management.NotificationBroadcasterSupport.handleNotification(NotificationBroadcasterSupport.java:275)
	at javax.management.NotificationBroadcasterSupport$SendNotifJob.run(NotificationBroadcasterSupport.java:352)
	at javax.management.NotificationBroadcasterSupport$1.execute(NotificationBroadcasterSupport.java:337)
	at javax.management.NotificationBroadcasterSupport.sendNotification(NotificationBroadcasterSupport.java:248)
	at javax.management.remote.rmi.RMIConnector.sendNotification(RMIConnector.java:441)
	at javax.management.remote.rmi.RMIConnector.access$1200(RMIConnector.java:121)
	at javax.management.remote.rmi.RMIConnector$RMIClientCommunicatorAdmin.gotIOException(RMIConnector.java:1531)
	at javax.management.remote.rmi.RMIConnector$RMINotifClient.fetchNotifs(RMIConnector.java:1352)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.fetchOneNotif(ClientNotifForwarder.java:655)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.fetchNotifs(ClientNotifForwarder.java:607)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.doRun(ClientNotifForwarder.java:471)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.run(ClientNotifForwarder.java:452)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$LinearExecutor$1.run(ClientNotifForwarder.java:108)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-15 09:46:08.03,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 30 08:02:39 UTC 2018,,,,,,0|i3oq5j:,9223372036854775807,,,,,,,,beobal,beobal,,,,,,,,,,"15/Jan/18 09:46;githubbot;GitHub user krummas opened a pull request:

    https://github.com/apache/cassandra-dtest/pull/16

    catch and ignore ToolError

    CASSANDRA-14158

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/krummas/cassandra-dtest marcuse/14158

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/16.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #16
    
----
commit 08fb197b1097dfc9de16ccded05df15c8ea5a103
Author: Marcus Eriksson <marcuse@...>
Date:   2018-01-15T09:45:24Z

    catch and ignore ToolError

----
",15/Jan/18 09:46;krummas;https://github.com/apache/cassandra-dtest/pull/16,29/Jan/18 16:35;beobal;+1 LGTM,"30/Jan/18 08:02;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/16
","30/Jan/18 08:02;krummas;committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_no_base_column_in_view_pk_complex_timestamp_with_flush - materialized_views_test.TestMaterializedViews frequently fails in CI,CASSANDRA-14148,13128702,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,mkjellman,mkjellman,05/Jan/18 05:41,12/Mar/19 14:19,13/Mar/19 22:34,10/Apr/18 12:11,,,,,,Legacy/Testing,,,,,0,,,,"test_no_base_column_in_view_pk_complex_timestamp_with_flush - materialized_views_test.TestMaterializedViews frequently fails in CI

self = <materialized_views_test.TestMaterializedViews object at 0x7f849b25cf60>

    @since('3.0')
    def test_no_base_column_in_view_pk_complex_timestamp_with_flush(self):
>       self._test_no_base_column_in_view_pk_complex_timestamp(flush=True)

materialized_views_test.py:970: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
materialized_views_test.py:1066: in _test_no_base_column_in_view_pk_complex_timestamp
    assert_one(session, ""SELECT * FROM t"", [1, 1, None, None, None, 1])
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

session = <cassandra.cluster.Session object at 0x7f849b379390>
query = 'SELECT * FROM t', expected = [1, 1, None, None, None, 1], cl = None

    def assert_one(session, query, expected, cl=None):
        """"""
        Assert query returns one row.
        @param session Session to use
        @param query Query to run
        @param expected Expected results from query
        @param cl Optional Consistency Level setting. Default ONE
    
        Examples:
        assert_one(session, ""LIST USERS"", ['cassandra', True])
        assert_one(session, query, [0, 0])
        """"""
        simple_query = SimpleStatement(query, consistency_level=cl)
        res = session.execute(simple_query)
        list_res = _rows_to_list(res)
>       assert list_res == [expected], ""Expected {} from {}, but got {}"".format([expected], query, list_res)
E       AssertionError: Expected [[1, 1, None, None, None, 1]] from SELECT * FROM t, but got []",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-18 15:30:26.527,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 10 12:11:25 UTC 2018,,,,,,0|i3ojmf:,9223372036854775807,,,,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,"18/Jan/18 15:30;krummas;so, seems self.update_view(...) can take more than 10s in some cases - this means the ttl expires and the query does not return any data.

https://github.com/krummas/cassandra-dtest/commits/marcuse/14148","06/Apr/18 13:41;pauloricardomg;sorry for the delay. fix LGTM, executed multiplexer run and this seems to fix flakiness. Marking as ready to commit, thanks!","10/Apr/18 12:11;krummas;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chain commit log marker potential performance regression in batch commit mode,CASSANDRA-14194,13134074,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,aweisberg,aweisberg,26/Jan/18 18:37,12/Mar/19 14:19,13/Mar/19 22:34,05/Mar/18 22:39,3.0.17,3.11.3,4.0,,,Legacy/Core,Legacy/Testing,,,,0,,,,"CASSANDRA-13987 modified how the commit log sync thread works. I noticed that cql3.ViewTest and ViewBuilderTaskTest have been timing out, but only in CircleCI. When I jstack in CircleCI what I see is that the commit log writer thread is parked and the threads trying to append to the commit log are blocked waiting on it.

I tested the commit before 13987 and it passed in CircleCI and then I tested with 13987 and it timed out. I suspect this may be a general performance regression and not just a CircleCI issue.

And this is with write barriers disabled (sync thread doesn't actually sync) so it wasn't blocked in the filesystem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Jan/18 19:42;aweisberg;jstack.txt;https://issues.apache.org/jira/secure/attachment/12907918/jstack.txt,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-01-26 18:52:28.251,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 05 22:39:31 UTC 2018,,,,,,0|i3pf8n:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,26/Jan/18 18:52;jasobrown;[~aweisberg] any chance you might still have the jstack output (and can attach it here)?,"27/Jan/18 03:50;jjirsa; A release vote imminent; if we can’t get this reviewed and committed before that vote, the cause of the regression should be reverted until it can be fixed in 3.0.17/3.11.3

 

cc [~mshuler] ",27/Jan/18 14:22;aweisberg;We would need to benchmark to establish definitively it's a performance regression. Right now I just suspect it from the fact the tests take longer in CircleCI.,"27/Jan/18 16:20;jasobrown;I'm working on this next (i.e. Monday morning), so depending on what the definition of what `imminent` is, we can decide if we need to revert.

That would be a shame, since we do have definable data loss problems without that patch.","27/Jan/18 17:24;jasobrown;[~aweisberg] don't forget, there was a follow ticket (CASSANDRA-14108) where we cleaned up a few things. I suspect that's the ticket that would have introduced the regression, if any.

That being said, running {{ViewTest}} on my laptop at the following SHAs all completed within the same amount of time (on the 3.0 branch):
 * {{eb05025c0a768241ea61fd86db9a88cfd8f6e93e}} - commit before CASSANDRA-13987
 * {{05cb556f90dbd1929a180254809e05620265419b}} - CASSANDRA-13987
* {{010c477e4c1b8b452cc0fa33b3fdb6c286e4037d}} - current head of cassandra-3.0 (CASSANDRA-14181)

Now, this doesn't discount [~aweisberg]'s discovery here. Ariel, which branch was this on? 

I'll also do some more runs on circleci to compare... ","27/Jan/18 18:14;jasobrown;I've run the head of cassandra-3.11 (currently sha {{b8c12fba064fb0b6a3b6306b2670497434471920}}), and it completed in the same time that cassandra-3.0 did (no timeout). I ran trunk (sha {{6fc3699adb4f609f78a40888af9797bb205216b0}}), and it *did* timeout.

I'm running 3.0 and 3.11 in circleci to see if they timeout. If not, then I think the regression is on trunk alone, and I'll focus my effort there. Obviously, if the source of the source looks like it'll affect earlier versions, i'll dig into that,as well.","07/Feb/18 12:59;jasobrown;OK, so I've bisected both 3.0 and 3.11 locally, and I can't get them to timeout (or fail in other ways, outside of CASSANDRA-14219). I've also run them in circleci, and they do not timeout. Thus, to quote myself, I'm focusing on trunk for now. (Note: I did bisect trunk, and my initial CASSANDRA-13987 patch introduces the timeout)","07/Feb/18 13:26;jasobrown;[~jjirsa] I'm pretty sure this regression is related to the intersection of the group commit log patch (CASSANDRA-13530) and CASSANDRA-13987, thus only affecting trunk. Doesn't affect 3.0/3.11, so votes on those can proceed when they are ready.","02/Mar/18 22:45;jasobrown;OK, think I'm at the bottom of this. While trunk is the only branch that is flat-out failing the unit tests due to a problem (see below), 3.0 and 3.11 utests are performing about 5-10% slower (when using batch commit log mode), but that 5-10% isn't enough to cause the test to timeout.
||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/14194-v2-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/14194-v2-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/14194-v2-trunk]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14194-v2-3.0]|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14194-v2-3.11]|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14194-v2-trunk]|

This patch contains a set of small, subtle changes (the same across all three branches):

1) The main fix - There is a race in {{AbstractCommitLogService}} where the {{SyncRunnable}} thread where once we determine to call sync to disk (not just update the headers), we set {{syncRequested}} to false *after* calling {{commitLog.sync(true)}}. This allows us to have a race where {{commitLog.sync(true)}} begins it's marking work, but another mutation comes in, calls {{#requestExtraSync}} (which sets {{syncRequested}} to true, and calls unpark), and blocks in {{Allocation#awaitDiskSync()}}. If no other mutations come in (like in a unit test), {{AbstractCommitLogService}} will not {{commitLog.sync(true)}} (and {{CommitLogSegment#syncComplete}} will not release the blocked mutation) until the syncIntervalNanos is met - which, due to the introduction of the GroupCommitLog, is now hardcoded at 1000ms. By moving the {{syncRequested = false}} *before* the call to {{commitLog.sync(true)}} we eliminate the race.

2) related to #1, right before we choose to park the thread in {{SyncRunnable}}, we can check if {{syncRequested}} is true, and if so, avoid parking. This is a race between the sync thread completing the sync, but before it sleeps a new mutation calls {{#requestExtraSync}} (which unparks), and that mutation then gets stuck when the sync thread does park. If another mutation comes along it will unpark the sync thread, then both mutations can proceed; otherwise, the first mutation is blocked. This should only affect things like unit tests and *very* underutilized clusters. Note: I believe we've had this race since CASSANDRA-10202, but I'm not completely sure.

3) Fix a correctness problem, which, fortunately, has no practical implications - At the end of {{CommitLogSegment#sync()}} we always call {{syncComplete.signalAll()}}. {{syncComplete.signalAll()}} should only be called when we actually msync. Thus, if {{#sync()}} is called with {{flush}} = false, this could be improperly signalling any waiting threads. Luckily, {{AbstractCommitLogService}} should never be calling {{commitLog#sync(false)}} when in batch mode, so there is no improper signalling. However, for soundness of code, I'm moving the {{syncComplete.signalAll()}} call into the {{if(flush || close)}} block in {{CommitLogSegment#sync()}}.

4) {{AbstractCommitLogService#thread}} is now marked volatile. This field is not defined until the {{start()}} method (not the constructor), so we're not guaranteed visibility when the {{#requestExtraSync()}} and {{#awaitTermination()}} methods run. (NOTE: this field was final and defined in the ctor until CASSANDRA-8308, introduced c* 2.2) Admittedly, I'm not thrilled with putting yet another volatile on the commit log path (only in batch mode, in {{#requestExtraSync()}}), but ... correctness? Also, reading a volatile variable isn't much more expensive vs. a regulat LOAD instruction. wdyt? This one arguably could be backported to 2.2+, but that ticket was committed 3.5 years ago we have haven't heard anything, so maybe just 3.0+?","05/Mar/18 18:24;aweisberg;RE #1 I think this is a correct fix. Totally makes sense that clobbering the more work indicator after already starting to collect the next batch of work is going to cause this issue.

RE #2 This won't happen because LockSupport.unpark() will cause it to wake immediately I believe? https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/locks/LockSupport.html#unpark(java.lang.Thread)

RE #3  looks good.

RE #4 should be fine. It's only read when doing hideously expensive operations that dwarf the cost of a volatile read anyways.

+1","05/Mar/18 20:00;jasobrown;bq.  #2 This won't happen because LockSupport.unpark() will cause it to wake immediately I believe?

Reading the docs you linked, this sounds correct. I will remove that change on commit. (thanks! this was a good pointer)","05/Mar/18 22:39;jasobrown;committed as sha {{85fafd0c134cae5aa84133ad54d67f2dba28c953}}. Thanks for finding and reviewing, [~aweisberg]!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The inspectJvmOptions startup check can trigger some Exception on some JRE versions,CASSANDRA-14112,13124660,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,blerer,blerer,blerer,13/Dec/17 12:59,12/Mar/19 14:19,13/Mar/19 22:34,14/Dec/17 14:05,2.2.12,3.0.16,3.11.2,4.0,,Legacy/Core,,,,,0,,,,"[~adelapena] pointed out that the Startup check added by CASSANDRA-13006 can cause some Exception if Cassandra is run on a non GA version.
After investigation it seems that it can also be the case for major versions or some JRE 9 versions. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-13 13:26:08.715,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 14 14:05:22 UTC 2017,,,,,,0|i3nusf:,9223372036854775807,,,,,,,,adelapena,adelapena,,,,,,,,,,"13/Dec/17 13:05;blerer;I pushed a patch for 2.2 [here|https://github.com/apache/cassandra/compare/trunk...blerer:14112-2.2].
[~adelapena] could you review? ",13/Dec/17 13:26;adelapena;Sure thing!,"14/Dec/17 11:28;adelapena;The patch looks good to me, and confirmed that it allows the server to start with a couple of non-GA JRE versions, +1. 

I have left a few minor suggestions [at this commit|https://github.com/adelapena/cassandra/commit/c19029370ad973bf7b481446a68dd94ef2492c4b], feel free to merge them if they look good to you.","14/Dec/17 14:05;blerer;Thanks for the review.

Committed into 2.2 at b800f3c768289268193b0dd716be99a33f306dad and merged into 3.0, 3.11 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve commit log chain marker updating,CASSANDRA-14108,13124410,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,jasobrown,jasobrown,12/Dec/17 16:43,12/Mar/19 14:19,13/Mar/19 22:34,14/Dec/17 03:55,3.0.16,3.11.2,4.0,,,,,,,,0,,,,"CASSANDRA-13987 addressed the commit log behavior change that was introduced with CASSANDRA-3578. After that patch was committed, [~aweisberg] did his own review and found a bug as well as having some concerns about the configuration. He and I discussed offline, and agreed on some improvements. 

Instead of requiring users to configure a deep, dark implementation detail like the commit log chained markers (via {{commitlog_marker_period_in_ms}} in the yaml), we decided it is best to eliminate thew configuration and always update the chained markers (when in periodic mode). 

The bug [~aweisberg] found was when the chained marker update is not a value that evenly divides into the periodic sync mode value, we would not sync in an expected manner. For example if the marker interval is 9 seconds, and the sync interval is 10 seconds, we would update the markers at time9, but we would then sleep for another 9 seconds, and when we wake up at time18, it is then that we flush - 8 seconds later than we should have. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14292,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-12 18:56:41.793,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 14 03:55:18 UTC 2017,,,,,,0|i3nt8v:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,"12/Dec/17 16:43;jasobrown;Branches and tests here:

||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13987-followup-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/13987-followup-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13987-followup-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13987-followup-3.0]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13987-followup-3.11]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13987-followup-trunk]|
||

I've removed the confusing (and confusingly described) yaml property for setting the {{commitlog_marker_period_in_ms}}. Instead, I've hardcoded the marker interval to 100ms and it is always applied when a) using periodic mode, and b) not using compression or encryption.

I've fixed the bug that @ariel found by quantizing the {{commitlog_sync_period_in_ms}} to a multiple of the marker interval. At the worst, this would change the {{commitlog_sync_period_in_ms}} by up to 50 ms. I don't think anyone will really notice those milliseconds at a practical level, and if they do, they should be using batch or group mode.

I've also refactored {{AbstractCommitLogService}} so we can actually unit test it. The anonymous {{Runnable}} class that had 90% of the functionality of  {{AbstractCommitLogService}} is now a full-fledged, named, and accessible nested class, {{AbstractCommitLogService.SyncRunnable}}. To avoid being bound by actual wall clock timings for correctness testing, {{AbstractCommitLogService.SyncRunnable}} is using the {{Clock}} abstraction; this makes testing sane, safe, and repeatable. {{Clock}} was introduced in CASSANDRA-12016, and committed to 3.10. I've backported it to 3.0, but I feel it's quite safe and doesn't affect anything outside of itself.
","12/Dec/17 16:44;jasobrown;I've assigned [~aweisberg] as reviewer, but it'd be fantastic if [~beobal] could take a look, as well.","12/Dec/17 18:56;aweisberg;I am a little surprised we did this change originally to 3.0 and 3.11 since it's more of an enhancement.

[Extra line break|https://github.com/apache/cassandra/compare/trunk...jasobrown:13987-followup-trunk?expand=1#diff-8e54554f3635ce0e8411435dd5896f4dR81]. It's on all 3 branches.
[Another extra line break.https://github.com/apache/cassandra/compare/trunk...jasobrown:13987-followup-trunk?expand=1#diff-8e54554f3635ce0e8411435dd5896f4dR241]

I like the approach of quantizing. It's simple. Besides the whitespace +1.","13/Dec/17 13:23;beobal;Looks good to me, I just have a couple of trivial nits:
* The debug log in the ACLS constructor would be more informative if it included the sync interval. As is, the marker interval it does log will always be {{DEFAULT_MARKER_INTERVAL_MILLIS}}. 
* There's an extra whitespace introduced by this patch in the 3 arg ACLS constructor.
* (3.0 only) The string format specifier for the exception message in ACLS::start should be '%d', not '%f' as we're just printing the value in millis.
",14/Dec/17 03:55;jasobrown;committed as sha {{db788fe860dfd69f06ab97ae35fa67fcf2517b6d}}. Thanks [~aweisberg] and [~beobal].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index target doesn't correctly recognise non-UTF column names after COMPACT STORAGE drop,CASSANDRA-14104,13124185,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,11/Dec/17 17:39,12/Mar/19 14:19,13/Mar/19 22:34,19/Dec/17 10:02,3.0.16,3.11.2,4.0,,,,,,,,0,,,,"Creating a compact storage table with dynamic composite type, then running {{ALTER TALBE ... DROP COMPACT STORAGE}} and then restarting the node will crash Cassandra node, since the Index Target is fetched using hashmap / strict equality. We need to fall back to linear search when index target can't be found (which should not be happening often).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-3.0-dtest.png;https://issues.apache.org/jira/secure/attachment/12902803/ifesdjeen-14104-3.0-dtest.png,19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-3.0-testall.png;https://issues.apache.org/jira/secure/attachment/12902802/ifesdjeen-14104-3.0-testall.png,19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-3.11-dtest.png;https://issues.apache.org/jira/secure/attachment/12902801/ifesdjeen-14104-3.11-dtest.png,19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-3.11-testall.png;https://issues.apache.org/jira/secure/attachment/12902800/ifesdjeen-14104-3.11-testall.png,19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-trunk-dtest.png;https://issues.apache.org/jira/secure/attachment/12902799/ifesdjeen-14104-trunk-dtest.png,19/Dec/17 08:51;ifesdjeen;ifesdjeen-14104-trunk-testall.png;https://issues.apache.org/jira/secure/attachment/12902798/ifesdjeen-14104-trunk-testall.png,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2017-12-12 01:59:24.113,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 19 10:02:10 UTC 2017,,,,,,0|i3nrv3:,9223372036854775807,,,,,,,,jasonstack,jasonstack,,,,,,,,,,"11/Dec/17 18:29;ifesdjeen;Patch:

|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...ifesdjeen:14104-3.0]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...ifesdjeen:14104-3.11]|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:14104-trunk]|","12/Dec/17 01:59;jasonstack;LGTM, thanks for the fix.
","19/Dec/17 08:52;ifesdjeen;Attaching CI results. 
The failures seem unrelated.",19/Dec/17 10:02;ifesdjeen;Committed to 3.0 as [0521f8dc5d5e05c0530726e9549fa2481726a818|https://github.com/apache/cassandra/commit/0521f8dc5d5e05c0530726e9549fa2481726a818] and merged up to [3.11|https://github.com/apache/cassandra/commit/adc32ac836e90b8c4503030feb76ce031998ad80] and [trunk|https://github.com/apache/cassandra/commit/8764ef2da367fac64adad0821a34e6fa15da13c6].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
utest failed: DistributionSequenceTest.setSeed() and simpleSequence(),CASSANDRA-14106,13124249,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,12/Dec/17 00:53,12/Mar/19 14:19,13/Mar/19 22:34,20/Dec/17 00:24,4.0,,,,,Legacy/Testing,,,,,0,,,,"To reproduce:
{noformat}
$ ant stress-test -Dtest.name=DistributionSequenceTest
{noformat}

{noformat}
stress-test:
    [junit] Testsuite: org.apache.cassandra.stress.generate.DistributionSequenceTest
    [junit] Testsuite: org.apache.cassandra.stress.generate.DistributionSequenceTest Tests run: 4, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 0.08 sec
    [junit]
    [junit] Testcase: simpleSequence(org.apache.cassandra.stress.generate.DistributionSequenceTest):    FAILED
    [junit] expected:<5> but was:<4>
    [junit] junit.framework.AssertionFailedError: expected:<5> but was:<4>
    [junit]     at org.apache.cassandra.stress.generate.DistributionSequenceTest.simpleSequence(DistributionSequenceTest.java:37)
    [junit]
    [junit]
    [junit] Testcase: setSeed(org.apache.cassandra.stress.generate.DistributionSequenceTest):   FAILED
    [junit] expected:<5> but was:<4>
    [junit] junit.framework.AssertionFailedError: expected:<5> but was:<4>
    [junit]     at org.apache.cassandra.stress.generate.DistributionSequenceTest.setSeed(DistributionSequenceTest.java:111)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.stress.generate.DistributionSequenceTest FAILED
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14090,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-12 19:12:43.722,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 20 00:24:54 UTC 2017,,,,,,0|i3ns9b:,9223372036854775807,,,,,,,,bdeggleston,bdeggleston,,,,,,,,,,"12/Dec/17 05:51;jay.zhuang;cassandra-stress is also failed with exception {{/ by zero}}, (added debug info):

{noformat}
$ tools/bin/cassandra-stress user profile=tools/cqlstress-example.yaml 'ops(insert=1)' n=10 cl=ONE no-warmup -rate threads=1

...
java.lang.ArithmeticException: / by zero

	at org.apache.cassandra.stress.generate.PartitionIterator$MultiRowIterator.decompose(PartitionIterator.java:410)
	at org.apache.cassandra.stress.generate.PartitionIterator$MultiRowIterator.setLastRow(PartitionIterator.java:347)
	at org.apache.cassandra.stress.generate.PartitionIterator$MultiRowIterator.reset(PartitionIterator.java:282)
	at org.apache.cassandra.stress.generate.PartitionIterator.reset(PartitionIterator.java:107)
	at org.apache.cassandra.stress.operations.PartitionOperation.reset(PartitionOperation.java:115)
	at org.apache.cassandra.stress.operations.PartitionOperation.ready(PartitionOperation.java:101)
	at org.apache.cassandra.stress.StressAction$StreamOfOperations.nextOp(StressAction.java:352)
	at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:453)

...
FAILURE
java.lang.RuntimeException: Failed to execute stress action
	at org.apache.cassandra.stress.StressAction.run(StressAction.java:99)
	at org.apache.cassandra.stress.Stress.run(Stress.java:143)
	at org.apache.cassandra.stress.Stress.main(Stress.java:62)

Process finished with exit code 1
{noformat}","12/Dec/17 07:26;jay.zhuang;Fixed the failed unittest and added an uTest for CASSANDRA-14090, please review:
| Branch | uTest |
| [14106|https://github.com/cooldoger/cassandra/tree/14106] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14106.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14106] |",12/Dec/17 19:12;jjirsa;[~bdeggleston] - you recently touched this for CASSANDRA-14090 - and this undoes your change. Want to weigh in?,"12/Dec/17 22:49;bdeggleston;undoing that change is going to break stress. This will no longer work: {{tools/bin/cassandra-stress user profile=tools/cqlstress-example.yaml 'ops(insert=1,simple=1)' n=1000 cl=ONE -mode native cql3 -rate threads=100 fixed=1000/s}}

However, as [~jasonstack] pointed out, my change only fixed part of the problem, since I didn't fix the denominator in the return statement. I was just going to just ninja fix that, but it's worth seeing if that would fix this problem and doing that instead.

In the future, we might want to reject any one time static analysis fixes that aren't going to become part of the build process.","12/Dec/17 22:59;jay.zhuang;Hi [~bdeggleston], after changing the {{double d}} to {{float d}}, it won't break stress. In the patch, I also added an unittest to reproduce the issue introduced by {{lgmt.com}} change.

The problem with {{double}} is: in the last round, the value of {{d}} would be {{1.0000...004}} instead of {{1.0}}. Which causes the exception {{OutOfRangeException: 1 out of [0, 1] range}} in cassandra-stress gaussian distribution.","13/Dec/17 01:16;bdeggleston;If stress works properly, then it's ok with me.

edit: actually, taking another look at the code, [~jay.zhuang]'s change is the right thing to do here","13/Dec/17 17:21;bdeggleston;-I was thinking about this some more. I think that, in addition to your changes here, the denominator in the return statement should be changed to 50, since that's the number of iterations that will (and should) actually be performed.-

edit: nevermind [~jay.zhuang] corrected me offline",15/Dec/17 17:32;jay.zhuang;hi [~bdeggleston] would you mind being the reviewer?,"20/Dec/17 00:24;bdeggleston;+1, committed to trunk as {{1db54a1266a68d9c765c0fa248277635c4520f6d}}. Thanks [~jay.zhuang]!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when CAS encounters empty frozen collection,CASSANDRA-14087,13122153,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,KurtG,jens.b,jens.b,01/Dec/17 11:18,12/Mar/19 14:19,13/Mar/19 22:34,08/Mar/18 23:57,3.0.17,3.11.3,4.0,,,,,,,,0,LWT,,,"When a compare-and-set operation specifying an equality criterion with a non-{{null}} value encounters an empty collection ({{null}} cell), the server throws a {{NullPointerException}} and the query fails.

This does not happen for non-frozen collections.

There's a self-contained test case at [github|https://github.com/incub8/cassandra-npe-in-cas].

The stack trace for 3.11.0 is:

{code}
ERROR [Native-Transport-Requests-1] 2017-11-27 12:59:26,924 QueryMessage.java:129 - Unexpected error during query
java.lang.NullPointerException: null
        at org.apache.cassandra.cql3.ColumnCondition$CollectionBound.appliesTo(ColumnCondition.java:546) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.CQL3CasRequest$ColumnsConditions.appliesTo(CQL3CasRequest.java:324) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.CQL3CasRequest.appliesTo(CQL3CasRequest.java:210) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.service.StorageProxy.cas(StorageProxy.java:265) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithCondition(ModificationStatement.java:441) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:416) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:217) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:248) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:233) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.0.jar:3.11.0]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_151]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.0.jar:3.11.0]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151]
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-05 04:19:57.249,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 13 23:57:10 UTC 2018,,,,,,0|i3nff3:,9223372036854775807,3.0.0,3.11.0,3.11.1,,,,,bdeggleston,bdeggleston,,,,,,,,,,"05/Dec/17 04:19;KurtG;[3.0|https://github.com/apache/cassandra/compare/trunk...kgreav:14087-3.0]
We simply weren't checking (or testing) for this case. Trunk is fine as CASSANDRA-12981 fixed the issue.","20/Dec/17 18:07;bdeggleston;|3.0|3.11|
|[tests|https://circleci.com/gh/bdeggleston/cassandra/196]|[tests|https://circleci.com/gh/bdeggleston/cassandra/197]|",01/Feb/18 04:04;KurtG;[~bdeggleston] have you had a chance to review?,"07/Feb/18 20:50;bdeggleston;Sorry [~KurtG], I did some test runs and then forgot about them over the holidays. So I'm +1 on the patch for 3.0 and 3.11, however the test case you've added fails against trunk. Could you take a look at that?","08/Feb/18 01:07;KurtG;That's worrying, pretty sure it worked when I tested it :S. Thanks, I'll have a look.","19/Feb/18 03:17;KurtG;Turns out I didn't run the unit test on trunk, just manually tested on trunk. So we're throwing an {{AssertionError}} [here|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/cql3/Terms.java#L169] because a {{Lists.Marker}} is created out of the prepared statement used in the test. The assertion kind of implies you can't use a list in the condition of a prepared statement, but this was always possible previously. It's really unclear why that assert statement is there, and removing it solves all the problems. Anyway, LMK what you think. [~blerer] will likely know more about the assert.

[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14087-trunk]","19/Feb/18 08:09;blerer;[~KurtG] There are 2 {{of}} methods. One for a single term and one for a list of terms. For some reasons the wrong one is being picked.
I will have a look later today.","19/Feb/18 09:40;blerer;Sorry, I misunderstood the problem. You can remove the assertion. It is a mistake from my part. I was only considering the {{IN}} logic and forgot about 
 frozen lists.

Small nit. {{CQLTester}} has some {{list}}, {{set}} and {{map}} methods that you can use instead of the {{Collections}} {{singleton}} methods.It makes the code more readable. ","21/Feb/18 04:43;KurtG;Thanks [~blerer], I've updated both branches for the nit. 3.0 should still merge cleanly into 3.11.

[~bdeggleston] 'tis ready for review again.","06/Mar/18 17:45;bdeggleston;started another round of tests:
|3.0|3.11|trunk|
|[tests|https://circleci.com/workflow-run/79cca9f9-fb55-4a6e-b53d-862018222bc8]|[tests|https://circleci.com/workflow-run/eac51382-1994-4218-ab7b-ea6e466580f2]|[tests|https://circleci.com/workflow-run/c932587d-a6e8-463e-881b-c28bb76bc81b]|","08/Mar/18 23:57;bdeggleston;All the test failures that were already failing in their respective branches, so +1.

Committed as {{2c150980cc1bfea81fd039f304e74fc2fb30fb45}}. Thanks, and sorry for the delay getting this in","11/Mar/18 15:34;jasonstack;{quote}Sorry, I misunderstood the problem. You can remove the assertion. 
{quote}
It looks like the [assertion|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/cql3/Terms.java#L169] is not removed from trunk and causing new unit test to fail..","11/Mar/18 16:19;bdeggleston;oops, sorry about that. ninja removed it",12/Mar/18 01:53;jasonstack;Thanks!,13/Mar/18 23:57;KurtG;Thanks [~bdeggleston],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicSnitch creates a lot of garbage,CASSANDRA-14091,13122301,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,01/Dec/17 23:01,12/Mar/19 14:19,13/Mar/19 22:34,06/Dec/17 00:11,3.0.16,3.11.2,4.0,,,,,,,,0,,,,"The ExponentiallyDecayingReservoir snapshots we take during score updates generate a lot of garbage, and we call getSnapshot twice per endpoint when we only need to call it once.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-02 13:36:29.372,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 06 00:45:21 UTC 2017,,,,,,0|i3ngbr:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"02/Dec/17 00:19;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/14091-3.0] |[3.11|https://github.com/bdeggleston/cassandra/tree/14091-3.11] |[trunk|https://github.com/bdeggleston/cassandra/tree/14091-trunk]|
|[tests|https://circleci.com/gh/bdeggleston/cassandra/180] |[tests|https://circleci.com/gh/bdeggleston/cassandra/181] |[tests|https://circleci.com/gh/bdeggleston/cassandra/179] |",02/Dec/17 13:36;jasobrown;Nice find. +1,"06/Dec/17 00:11;bdeggleston;Thanks, committed as {{10ca7e47ca63c43b4e0ba593fb4c736130764af9}}","06/Dec/17 00:45;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra/pull/178
  
    Hi @Benmartin92 , That looks like a perfectly reasonable patch. It appears that @bdeggleston also noticed this in https://issues.apache.org/jira/browse/CASSANDRA-14091 (a week or so after your patch). Unfortunately, I don't think anyone noticed your patch, probably because we don't typically use Github PRs (ASF repo isn't writable on github, so we can't merge this PR directly) - our contribution workflow is through JIRA. Appreciate the contribution, sorry we didn't see it in time. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unittest failed: CommitLogSegmentManagerCDCTest.testReplayLogic,CASSANDRA-14066,13120073,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,21/Nov/17 22:10,12/Mar/19 14:19,13/Mar/19 22:34,20/Dec/17 08:34,4.0,,,,,Legacy/Testing,,,,,0,,,,"To reproduce:
{noformat}
$ ant test-cdc -Dtest.name=CommitLogSegmentManagerCDCTest
{noformat}

Error:
{noformat}
    [junit] Testcase: testReplayLogic(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest)-cdc:    FAILED
    [junit] Missing old CDCIndexData in new set after replay: CommitLog-7-1511301220821_cdc.idx,3497190
    [junit] List of CDCIndexData in new set of indexes after replay:
    [junit]    CommitLog-7-1511301220822_cdc.idx,3509214
    [junit]    CommitLog-7-1511301220823_cdc.idx,100
    [junit]
    [junit] junit.framework.AssertionFailedError: Missing old CDCIndexData in new set after replay: CommitLog-7-1511301220821_cdc.idx,3497190
    [junit] List of CDCIndexData in new set of indexes after replay:
    [junit]    CommitLog-7-1511301220822_cdc.idx,3509214
    [junit]    CommitLog-7-1511301220823_cdc.idx,100
    [junit]
    [junit]     at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testReplayLogic(CommitLogSegmentManagerCDCTest.java:345)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest FAILED
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-20 08:34:50.212,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 20 08:34:50 UTC 2017,,,,,,0|i3n2lz:,9223372036854775807,,,,,,,,spodxx@gmail.com,spodxx@gmail.com,,,,,,,,,,"22/Nov/17 00:11;jay.zhuang;It's a real issue with [{{CommitLogReplayer.replayFiles()}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L145]. For the CDC data, it's handling the wrong file (the right one + 1, and in the wrong list: [{{CommitLogReplayer.java:151}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L151]).

Sometimes, it's causing the following exception:
{noformat}
java.lang.ArrayIndexOutOfBoundsException: 4
at org.apache.cassandra.db.commitlog.CommitLogReplayer.replayFiles(CommittLogReplayer.java:155)
at org.apache.cassandra.db.commitlog.CDCCTestReplayer.examineCommitLog(CDCTestReplayer.java:48)
at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDDCTest.testReplayLogic(CommitLogSegmentManagerCDCTest.java:318)
{noformat}

Here is the fix, please review (it's only impacting trunk, 3.11 branch is fine):
| Branch | uTest |
| [14066-trunk|https://github.com/cooldoger/cassandra/tree/14066-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14066-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14066-trunk] |","20/Dec/17 08:34;spodxx@gmail.com;Thanks again, Jay!

Merged on trunk as 882c28230ef3b31bd17689d51e83ea624116c769

* [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/465/#showFailuresLink]
* [circleCI|https://circleci.com/gh/spodkowinski/cassandra/166#tests/containers/0]



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not expose compaction strategy index publicly,CASSANDRA-14082,13121912,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,30/Nov/17 13:22,12/Mar/19 14:19,13/Mar/19 22:34,26/Dec/17 12:55,3.11.2,4.0,,,,,,,,,0,,,,"Before CASSANDRA-13215 we used the compaction strategy index to decide which disk to place a given sstable, but now we can get this directly from the disk boundary manager and keep the compaction strategy index internal only.

This will ensure external consumers will use a consistent {{DiskBoundaries}} object to perform operations on multiple disks, rather than risking getting inconsistent indexes if the compaction strategy indexes change between successive calls to {{CSM.getCompactionStrategyIndex}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13948,,,,,,,,,,14/Dec/17 22:01;pauloricardomg;3.11-14082-dtest.png;https://issues.apache.org/jira/secure/attachment/12902161/3.11-14082-dtest.png,14/Dec/17 22:01;pauloricardomg;3.11-14082-testall.png;https://issues.apache.org/jira/secure/attachment/12902160/3.11-14082-testall.png,14/Dec/17 22:01;pauloricardomg;trunk-14082-dtest.png;https://issues.apache.org/jira/secure/attachment/12902159/trunk-14082-dtest.png,14/Dec/17 22:01;pauloricardomg;trunk-14082-testall.png;https://issues.apache.org/jira/secure/attachment/12902158/trunk-14082-testall.png,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2017-12-22 11:27:02.636,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 26 12:55:00 UTC 2017,,,,,,0|i3ndxr:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"30/Nov/17 13:26;pauloricardomg;Currently the scrubber and relocate sstables were relying on the compaction strategy index, so this patches change these operation to use a {{DiskBoundaries}} object instead and make {{CSM.getCompactionStrategyIndex}} private.

* [3.11 patch|https://github.com/pauloricardomg/cassandra/tree/3.11-14082]

Since this depends on CASSANDRA-13948, I will wait until that is committed before setting this as PA.","14/Dec/17 22:35;pauloricardomg;Rebased on top of latest now that CASSANDRA-13948, setting this to PA, tests look good:

||3.11||trunk||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14082]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14082]|
|[testall|https://issues.apache.org/jira/secure/attachment/12902160/3.11-14082-testall.png]|[testall|https://issues.apache.org/jira/secure/attachment/12902158/trunk-14082-testall.png]|
|[dtest|https://issues.apache.org/jira/secure/attachment/12902161/3.11-14082-dtest.png]|[dtest|https://issues.apache.org/jira/secure/attachment/12902159/trunk-14082-dtest.png]|",22/Dec/17 11:27;krummas;+1,26/Dec/17 12:55;pauloricardomg;Committed as {{e208a6a210d172b991b40fb66a4763e30b3e4d7d}} to cassandra-3.11 branch and merged up to master. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[DTEST] [TRUNK] TestTopology.movement_test is flaky; fails assert ""values not within 16.00% of the max: (851.41, 713.26)""",CASSANDRA-14156,13129753,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,mkjellman,mkjellman,10/Jan/18 04:39,12/Mar/19 14:19,13/Mar/19 22:34,27/Jul/18 07:44,,,,,,Test/dtest,,,,,0,dtest,,,"DTest* TestTopology.test_movement* is flaky. All of the testing so far (and thus all of the current known observed failures) have been when running against trunk. When the test fails, it always due to the assert_almost_equal assert.

{code}
AssertionError: values not within 16.00% of the max: (851.41, 713.26) ()
{code}

The following CircleCI runs are 2 examples with dtests runs that failed due to this test failing it's assert:
[https://circleci.com/gh/mkjellman/cassandra/487]
[https://circleci.com/gh/mkjellman/cassandra/526]

*p.s.* assert_almost_equal has a comment ""@params error Optional margin of error. Default 0.16"". I don't see any obvious notes for why the default is this magical 16% number. It looks like it was committed as part of a big bulk commit by Sean McCarthy (who I can't find on JIRA). If anyone has any history on the magic 16% allowed delta please share!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-12 15:32:14.533,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 27 07:44:22 UTC 2018,,,,,,0|i3oq1z:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"10/Jan/18 06:08;mkjellman;Failed again just now:
https://circleci.com/gh/mkjellman/cassandra/532

{code}
AssertionError: values not within 16.00% of the max: (870.33, 713.36) ()
{code}","12/Jan/18 15:32;krummas;in 3.11 and trunk we need to run {{relocatesstables}} to move tokens to their correct places, then we need to run a major compaction to make sure that there are no overlapping tokens

https://github.com/krummas/cassandra-dtest/commits/marcuse/14156",18/Jan/18 13:40;krummas;[~jasobrown] could you review?,"26/Jul/18 17:25;jasobrown;I was able to repro the flakey fail after 17 runs on my laptop. With [~krummas]'s patch, it ran 50 times without fail.

The [debug statement|https://github.com/krummas/cassandra-dtest/commit/42e9125c189f61dd31a2fc0a157b95e6ecea4cf1#diff-376c5cd8425c9b4b078f87a20db738b9R300] in the patch should be removed as it fails hard with the following error:

{noformat}
test_movement failed and was not selected for rerun.
	<class 'NameError'>
	name 'debug' is not defined
	[<TracebackEntry /opt/orig/1/opt/dev/cassandra-dtest/topology_test.py:300>]
{noformat}
 
As it's not essential, you can probably remove it. Otherwise, +1.","27/Jul/18 07:44;krummas;committed as {{2548ec6e664bf118943ea07d070c5bb863f90426}}, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SELECT JSON prints null on empty strings,CASSANDRA-14245,13139842,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,fcofdezc,nob13,nob13,21/Feb/18 09:11,12/Mar/19 14:19,13/Mar/19 22:34,29/Mar/18 06:57,3.11.3,4.0,,,,Legacy/CQL,,,,,0,,,,"SELECT JSON reports an empty string as null.

 

Example:
{code:java}
cqlsh:unittest> create table test(id INT, name TEXT, PRIMARY KEY(id));
cqlsh:unittest> insert into test (id, name) VALUES (1, 'Foo');
cqlsh:unittest> insert into test (id, name) VALUES (2, '');
cqlsh:unittest> insert into test (id, name) VALUES (3, null);

cqlsh:unittest> select * from test;

id | name
----+------
  1 |  Foo
  2 |     
  3 | null

(3 rows)

cqlsh:unittest> select JSON * from test;

[json]
--------------------------
{""id"": 1, ""name"": ""Foo""}
{""id"": 2, ""name"": null}
{""id"": 3, ""name"": null}

(3 rows){code}
 

This even happens, if the string is part of the Primary Key, which makes the generated string not insertable.

 
{code:java}
cqlsh:unittest> create table test2 (id INT, name TEXT, age INT, PRIMARY KEY(id, name));
cqlsh:unittest> insert into test2 (id, name, age) VALUES (1, '', 42);
cqlsh:unittest> select JSON * from test2;

[json]
------------------------------------
{""id"": 1, ""name"": null, ""age"": 42}

(1 rows)

cqlsh:unittest> insert into test2 JSON '{""id"": 1, ""name"": null, ""age"": 42}';
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Invalid null value in condition for column name""{code}
 

On an older version of Cassandra (3.0.8) does not have this problem.","Cassandra 3.11.2, Ubuntu 16.04 LTS

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-22 10:54:03.514,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 19 08:38:54 UTC 2018,,,,,,0|i3qesn:,9223372036854775807,,,,,,,,blerer,blerer,,,3.11.0,,,,,,,"22/Mar/18 10:54;fcofdezc;The problem is that the json serializer writes empty buffers as null instead of """".

[Patch|https://github.com/apache/cassandra/compare/cassandra-3.11...fcofdez:CASSANDRA-14245-3.11?expand=1]",28/Mar/18 12:58;blerer;The patch and the unit tests results look good. Thanks.,29/Mar/18 06:57;blerer;Committed into 3.11 at 5cbe08b6a84cfa51ffd952a7c997b9a5f5e46e92 and merged into trunk.,"29/Mar/18 07:18;nob13;Thanks for fixing.

 ","19/Sep/18 08:23;BrunoZ;Thanks but when you import such exported using CQL.json(String), you have : 

Error decoding JSON value for value: Value '' is not a valid blob representation: String representation of blob is missing 0x prefix: 
 com.datastax.driver.core.exceptions.InvalidQueryException: Error decoding JSON value for value: Value '' is not a valid blob representation: String representation of blob is missing 0x prefix: 
 at com.datastax.driver.core.Responses$Error.asException(Responses.java:148) ~C[cassandra-driver-core-3.2.0.jar:?]

Can you Reopen this ticket ?",19/Sep/18 08:38;blerer;[~BrunoZ] it is better if you open another ticket as this patch as already been released in the version 3.11.3.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReservedKeywords class is missing some reserved CQL keywords,CASSANDRA-14205,13135087,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,adelapena,adelapena,adelapena,31/Jan/18 13:52,12/Mar/19 14:19,13/Mar/19 22:34,02/Mar/18 10:49,3.11.x,4.x,,,,Legacy/CQL,,,,,0,,,,"The CQL keywords {{DEFAULT}}, {{UNSET}}, {{MBEAN}} and {{MBEANS}} (introduced by CASSANDRA-11424 and CASSANDRA-10091) are neither considered [unreserved keywords|https://github.com/apache/cassandra/blob/trunk/src/antlr/Parser.g#L1788-L1846] by the ANTLR parser, nor included in the [{{ReservedKeywords}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/cql3/ReservedKeywords.java] class.

The current parser behaviour is considering them as reserved keywords, in the sense that they can't be used as keyspace/table/column names, which seems right:
{code:java}
cassandra@cqlsh> CREATE KEYSPACE unset WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
SyntaxException: line 1:16 no viable alternative at input 'unset' (CREATE KEYSPACE [unset]...)
{code}
I think we should keep considering these keywords as reserved and add them to {{ReservedKeywords}} class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-01 09:31:56.238,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 01 20:32:56 UTC 2018,,,,,,0|i3plh3:,9223372036854775807,,,,,,,,blerer,blerer,,,,,,,,,,"31/Jan/18 13:59;adelapena;Here is the patch adding the missed reserved keywords:

||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:14205-3.11]||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:14205-trunk]||",01/Feb/18 09:31;blerer;Thanks for the patch +1.,"01/Feb/18 20:32;adelapena;Thanks for the review!

Committed to cassandra-3.11 as [6b00767427706124e016e4f471c2266899387163|https://github.com/apache/cassandra/commit/6b00767427706124e016e4f471c2266899387163] and merged to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
View replica is not written to pending endpoint when base replica is also view replica,CASSANDRA-14251,13140056,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,21/Feb/18 22:52,12/Mar/19 14:19,13/Mar/19 22:34,06/Mar/18 14:37,3.0.17,3.11.3,4.0,,,Feature/Materialized Views,,,,,0,,,,"From the [dev list|https://www.mail-archive.com/dev@cassandra.apache.org/msg12084.html]:

bq. There's an optimization that when we're lucky enough that the paired view replica is the same as this base replica, mutateMV doesn't use the normal view-mutation-sending code (wrapViewBatchResponseHandler) and just writes the mutation locally. In particular, in this case we do NOT write to the pending node (unless I'm missing something). But, sometimes all replicas will be paired with themselves - this can happen for example when number of nodes is equal to RF, or when the base and view table have the same partition keys (but different clustering keys). In this case, it seems the pending node will not be written at all...

This was a regression from CASSANDRA-13069 and the original behavior should be restored.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-05 06:56:43.436,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 06 14:37:36 UTC 2018,,,,,,0|i3qg3r:,9223372036854775807,3.0.15,3.11.1,,,,,,jasonstack,jasonstack,,,3.0.15,,,,,,,"21/Feb/18 23:21;pauloricardomg;Added regression [dtest|https://github.com/pauloricardomg/cassandra-dtest/commit/dbf6eba59da67edaa50930046fc440430fe61d2d] with different RFs and [restored|https://github.com/pauloricardomg/cassandra/commit/1a06c97a7896621f346b806e6369a820b75fdd75] correct behavior along with a NEWS.txt asking MV users potentially affected by this to run repair on the view to ensure all replicas are up to date.

||3.0||3.11||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-14251]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14251]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14251]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:14251]|

Will run internal CI and post results here when ready.","05/Mar/18 06:56;jasonstack;This patch LGTM,  thanks for the fix.","06/Mar/18 14:37;pauloricardomg;Updated {{NEWS.TXT}} wording to state that potentially affected users must run repair in the base table and subsequently on the views, after [mailing list discussion|https://www.mail-archive.com/dev@cassandra.apache.org/msg12128.html].

Committed fix as {{c67338989f17257d3be95212ca6ecb4b83009326}} to cassandra-3.0 and merged up to cassandra-3.11 and trunk and dtest as {{8fa87f63dec7dd636473b620071d264893a19df8}}. Thanks for the review [~jasonstack].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use zero as default score in DynamicEndpointSnitch,CASSANDRA-14252,13140065,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,dikanggu,dikanggu,dikanggu,21/Feb/18 23:46,12/Mar/19 14:19,13/Mar/19 22:34,20/Mar/18 22:19,4.0,,,,,Legacy/Coordination,,,,,0,,,,"The problem I want to solve is that I found in our deployment, one slow but alive data node can slow down the whole cluster, even caused timeout of our requests. 

We are using DynamicEndpointSnitch, with badness_threshold 0.1. I expect the DynamicEndpointSnitch switch to sortByProximityWithScore, if local data node latency is too high.

I added some debug log, and figured out that in a lot of cases, the score from remote data node was not populated, so the fallback to sortByProximityWithScore never happened. That's why a single slow data node, can cause huge problems to the whole cluster.

In this jira, I'd like to use zero as default score, so that we will get a chance to try remote data node, if local one is slow. 

I tested it in our test cluster, it improved the client latency in single slow data node case significantly.  

I flag this as a Bug, because it caused problems to our use cases multiple times.

 ==== logs ===

_2018-02-21_23:08:57.54145 WARN 23:08:57 [RPC-Thread:978]: sortByProximityWithBadness: after sorting by proximity, addresses order change to [ip1, ip2], with scores [1.0]_
 _2018-02-21_23:08:57.54319 WARN 23:08:57 [RPC-Thread:967]: sortByProximityWithBadness: after sorting by proximity, addresses order change to [ip1, ip2], with scores [0.0]_
 _2018-02-21_23:08:57.55111 WARN 23:08:57 [RPC-Thread:453]: sortByProximityWithBadness: after sorting by proximity, addresses order change to [ip1, ip2], with scores [1.0]_
 _2018-02-21_23:08:57.55687 WARN 23:08:57 [RPC-Thread:753]: sortByProximityWithBadness: after sorting by proximity, addresses order change to [ip1, ip2], with scores [1.0]_

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14555,,,,,,19/Mar/18 17:17;jay.zhuang;IMG_3180.jpg;https://issues.apache.org/jira/secure/attachment/12915165/IMG_3180.jpg,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-02-26 20:59:57.181,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 05 17:11:39 UTC 2018,,,,,,0|i3qg5r:,9223372036854775807,,,,,,,,jay.zhuang,jay.zhuang,,,,,,,,,,21/Feb/18 23:57;dikanggu;|[3.0 | https://github.com/DikangGu/cassandra/commit/163ad12c7c84cfd44932b814ce61c7a0145e9bcd]|[3.11 | https://github.com/DikangGu/cassandra/commit/980a7c36d410448b3714111b541193b68c952d34]|[trunk | https://github.com/DikangGu/cassandra/commit/fd00c51321f6252294b066a758bfaaa5ba810ea9]|[unit test | https://circleci.com/gh/DikangGu/cassandra/20]|,"26/Feb/18 20:59;szhou;This is an interesting change but I'm not sure it fixes all problems.

The code that you changed was introduced in CASSANDRA-13074, which also claims to fix slow node issue, by totally ignoring nodes that we don't have a score, no matter it's a node in local or remote data center. Now with your fix, we still give these (remote) nodes a try by assigning an artificially low score. However, isn't 0 the lowest score that could result in these slow/unresponsive remote nodes being picked up before other remote nodes that have normal scores (such as 1.0)?

Btw, badness_threshold=0.1 may be too conservative. We also disabled IO factor when calculating the scores through -Dcassandra.ignore_dynamic_snitch_severity=true. See CASSANDRA-11738 for details.","27/Feb/18 01:33;dikanggu;[~szhou], 1, We will need to populate the scores for all replicas first, which should be finished within seconds. After that, a local and healthy node will always have lower score than remotes nodes, then we can stick to local node, and only switch to remote node if local node is bad, right?  2. Yeah, we back-ported that patch and disabled the IO factor for severity already.","27/Feb/18 06:14;szhou;For nodes in the same remote data center, if we don't have score for one node because there is no read response yet and we set an artificially low score 0 for it, does it mean this node will be picked with higher probability than other nodes that have scores?","27/Feb/18 19:00;dikanggu;[~szhou], Yes, it's the warm up phase. We have to know the distance/latency differences between different replicas, otherwise we will have no way to fall back to remote replicas. One idea to limit unnecessary requests to remote replica is to only fall back when local node is really bad. Something like this:

if ({color:red}subsnitchScore > 0.5{color} && subsnitchScore > (sortedScoreIterator.next() * (1.0 + dynamicBadnessThreshold)))
            {
                sortByProximityWithScore(address, addresses);
                return;
            }

of course, the param 0.5 can be tunable.
","28/Feb/18 19:52;szhou;I think I haven't made it clear enough or I misunderstand your fix. Let's say you want to use nodes in remote data center because of whatever issue with local datacenter, my understanding is that:
- Either we don't use a node from remote data center that doesn't have score yet, because the reason could be that it's totally unresponsive to previous read requests but still responds to for example, gossip message, thus this node hasn't been marked as down. (A node doesn't have score may also because it hasn't received read request from the remote coordinator node yet, or all the scores got reset after dynamicResetInterval, both of which are less of a problem)
- Or maybe we can use it but it should be picked with lower probability than other nodes in the same remote data center.

Now you assign a low score of 0 to a node that doesn't have score yet. This means it will be picked with higher probability. If that node truly has problem (unresponsive to read requests), then your fix will cause higher latency. Having said that, I don't mind setting the node score as the highest one among all node scores from the same data center.","28/Feb/18 20:07;dikanggu;[~szhou], hmm, it's might be easier to explain offline. Anyway, what I'm trying to do is to set a default score which is 0, to any node in the cluster. Then, as Cassandra dispatch the requests, it will get the correct latency among different nodes, and have correct score for each one. The node is in local or remote does not matter a lot actually. Just in the function `sortByProximityWithBadness`, we will consider local node first. If local node is slow but alive, we can fall back to remote node. Without a default score, we will NEVER have a chance to talk to a remote node, as long as local node is alive. ","01/Mar/18 22:22;szhou;Talked with [~dikanggu] offline. Previously I thought that timeout wouldn't be counted as part of latency score. Actually it is, so setting replica score as 0 by default is less of a problem but only exposes a small vulnerability window:
Say if you have multiple replicas in a remote data center and you don't have score for one of them, thus it will be assigned score 0. This might cause traffic burst on this replica, for a short period of time and most of time it won't even be noticed.

This can be mitigated by assigning a larger score (such as maximum score of all the replicas) to the replica with null score. I'd defer this decision to [~dikanggu]. Otherwise the patch looks good to me.","01/Mar/18 23:26;dikanggu;Thanks [~szhou]!  [~tjake], [~jasobrown], do you want to take a look as well? ","19/Mar/18 17:36;jay.zhuang;Hi [~dikanggu], would you please help me to understand the scenario?
Assume there're 3 nodes: coordinator node, a degraded node, and a healthy node:

!IMG_3180.jpg!

When the issue happens, the coordinator node doesn't have the score for either degraded node nor healthy node, so it follows subsnitch ordering and always talk to the degraded node, is that right? Or are the coordinator node and the degraded node the same node?","19/Mar/18 18:27;dikanggu;[~jay.zhuang], the scenario is:
 # Coordinator and node A is in DC1, node B is another replica of node A and in DC2, we use DynamicEndpointSnitch and NetworkTopologyStrategy.
 # In normal situation, coordinator only talks to node A, it has correct score of node A. Coordinator never talks to node B, and do not have score for node B.
 # Then node A is degraded, it is slow but still alive. Coordinator set the score of node A to be very high, like 1.
 # But still, Coordinator do not have score for node B, which makes it never has the chance to talk to node B, which is a healthy of the replica in a different region.

 

My patch is provide a default score for node B, so coordinator will have chance to talk to node B at least once, to get the correct latency number between coordinator and node B, and can use it to decide whether to switch from node A to node B, if necessary.","19/Mar/18 21:59;jay.zhuang;Should ""Speculative Retry"" help in this situation?","19/Mar/18 22:22;dikanggu;Could be. We had serious problems with speculative retry before, which overloaded hot replicas, so we turned it off in our production clusters. 

Still I feel it's bad for dynamic endpoint snitch can not fallback due to lack of a default score. ","20/Mar/18 20:15;jay.zhuang;+1

Good catch. I created a dtest to reproduce the problem: [14252|https://github.com/cooldoger/cassandra-dtest/tree/14252]
Also when comparing 2 versions, the existing code uses {{0.0}} as default value: [{{DynamicEndpointSnitch.java:267}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java#L267]","20/Mar/18 22:19;dikanggu;Thanks  [~jay.zhuang], committed as *f109f200a3a7f6002d7e1f6cc67e9ef5bf5cb2df*

","20/Mar/18 22:34;jay.zhuang;Hi [~dikanggu], instead of committing the change to each branch separately, I think it would be better to merge up to later branches:
https://cassandra.apache.org/doc/latest/development/how_to_commit.html","20/Mar/18 23:35;dikanggu;[~jay.zhuang], sure, almost forgot that wiki.","21/Mar/18 09:20;blerer;[~dikanggu] If you do not merge the branches. The person behind you will be the one having to do it. When you do not expect it, it can cause you some problems.

Regarding the CHANGES.txt. You should only add the entry for the first branch that you modified and make sure that it appears in the {{merged from}} part of the other branches.","21/Mar/18 17:18;szhou;[~dikanggu] FYI I had a [fix|https://issues.apache.org/jira/browse/CASSANDRA-13261] for ""overloading"" issue long time before. Not sure if it's the issue that you had.
","23/Mar/18 14:51;iamaleksey;I fixed the incorrect CHANGES.txt. Please follow the agreed upon process when committing things, or ask around if you aren't sure how to do it next time. Thanks.","23/Mar/18 16:00;dikanggu;sure, will do.",05/Jul/18 17:11;jay.zhuang;The change is reverted from 3.0 and 3.11 branches. More details: CASSANDRA-14555,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chunk checksum test needs to occur before uncompress to avoid JVM crash,CASSANDRA-14284,13141910,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,blerer,giltene,giltene,01/Mar/18 21:12,12/Mar/19 14:19,13/Mar/19 22:34,10/Apr/18 12:15,2.1.21,2.2.13,3.0.17,3.11.3,4.0,Legacy/Core,,,,,0,,,,"While checksums are (generally) performed on compressed data, the checksum test when reading is currently (in all variants of C* 2.x, 3.x I've looked at) done [on the compressed data] only after the uncompress operation has completed. 

The issue here is that LZ4_decompress_fast (as documented in e.g. [https://github.com/lz4/lz4/blob/dev/lib/lz4.h#L214)] can result in memory overruns when provided with malformed source data. This in turn can (and does, e.g. in CASSANDRA-13757) lead to JVM crashes during the uncompress of corrupted chunks. The checksum operation would obviously detect the issue, but we'd never get to it if the JVM crashes first.

Moving the checksum test of the compressed data to before the uncompress operation (in cases where the checksum is done on compressed data) will resolve this issue.

-----------------------------

The check-only-after-doing-the-decompress logic appears to be in all current releases.

Here are some samples at different evolution points :

3.11.2:

[https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L146]

https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L207

 

3.5:

 [https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L135]

[https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L196]

2.1.17:

 [https://github.com/apache/cassandra/blob/cassandra-2.1.17/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L122]","The check-only-after-doing-the-decompress logic appears to be in all current releases.

Here are some samples at different evolution points :

3.11.2:

[https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L146]

https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L207

 

3.5:

 [https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L135]

[https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L196]

2.1.17:

 [https://github.com/apache/cassandra/blob/cassandra-2.1.17/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L122]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13757,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-28 12:50:25.183,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 10 12:15:56 UTC 2018,,,,,,0|i3qrhr:,9223372036854775807,,,,,,,,blambov,blambov,,,,,,,,,,"28/Mar/18 12:50;blerer;||Branches|[2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...blerer:14284-2.1]|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...blerer:14284-2.2]|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...blerer:14284-3.0]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:14284-3.11]|[trunk|https://github.com/apache/cassandra/compare/trunk...blerer:14284-trunk]|

 I ran the unit tests and the dtests on our internal CI and the failing tests seems unrelated.","28/Mar/18 13:29;blambov;The patches before 3.11 look good, but I'm not very happy with the separate buffer for the checksum in 3.11+. In fact, since we are doing the checksum immediately after the read now, why not include the checksum read with the compressed content? I.e. reserve four extra bytes at the end of the buffer, choose if checksum will be done before the read and adjust its length if so, then proceed like in the memmapped case.

Alternatively, you could use the first bytes of the uncompressed buffer.

The trunk patch seems to contain at least one odd character and the diff isn't displayed properly. Could you fix the line endings and upload it again?","28/Mar/18 15:40;giltene;The patch for 2.1 has an issue, I think: 2.1 (unlike the later versions) seems to support checksumming o either the compressed or uncompressed data (depending on what metadata.hasPostCompressionAdlerChecksums indicates). Only the checksum test of the compressed data can be moved to before the uncompress. The checksum in the uncompressed case has to remain after the uncompress.","29/Mar/18 12:55;blerer;[~blambov], [~giltene] Thanks for the reviews. I completely missed the pre-compression checksum logic in 2.1.

I force pushed some new patches for [2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...blerer:14284-2.1] , [3.1|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:14284-3.11], and [trunk|https://github.com/apache/cassandra/compare/trunk...blerer:14284-trunk] that address the different problems.",29/Mar/18 13:13;blambov;Shouldn't [this|https://github.com/apache/cassandra/compare/cassandra-2.1...blerer:14284-2.1#diff-67366d25fce65930f46564d6ea8e0f40R139] be using {{bytes}} instead of {{compressed.array()}}?,"29/Mar/18 14:34;blambov;The rest looks fine with one nit, which you can feel free to ignore: you could now extract the crc checking logic to a common method for the normal and memmapped case (in trunk, the normal+uncompressed case has to remain separate, though).","10/Apr/18 12:15;blerer;Committed into 2.1 at 34a1d5da58fb8edcad39633084541bb4162f5ede and merged into 2.2, 3.0, 3.11 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: ssl setting not read from cqlshrc in 3.11,CASSANDRA-14299,13143585,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,spodxx@gmail.com,tgbeck,tgbeck,08/Mar/18 15:38,12/Mar/19 14:19,13/Mar/19 22:34,17/Apr/18 08:58,3.11.3,,,,,,,,,,0,,,,"With CASSANDRA-10458 an option was added to read the {{--ssl}} flag from cqlshrc, however the commit seems to have been incorrectly merged or the changes were dropped somehow.

Currently adding the following has no effect:
{code:java}
[connection]
ssl = true{code}
When looking at the current tree it's obvious that the flag is not read: [https://github.com/apache/cassandra/blame/cassandra-3.11/bin/cqlsh.py#L2247]

However it should have been added with [https://github.com/apache/cassandra/commit/70649a8d65825144fcdbde136d9b6354ef1fb911]

The values like {{DEFAULT_SSL = False}}  are present, but the {{option_with_default()}} call is missing.

Git blame also shows no change to that line which would have reverted the change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-09 13:31:41.31,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 17 08:58:42 UTC 2018,,,,,,0|i3r1tz:,9223372036854775807,,,,,,,,jay.zhuang,jay.zhuang,,,,,,,,,,"09/Mar/18 13:31;spodxx@gmail.com;Looks like a merge oversight in [6d429cd|https://github.com/apache/cassandra/commit/6d429cd]. Trivial fix is linked, do you mind take a look [~ifesdjeen]?",13/Apr/18 20:19;jay.zhuang;+1,"17/Apr/18 08:58;spodxx@gmail.com;Merged as 845243db93a5add273f6e on 3.11

Thx Jay!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException with SELECT JSON using IN and ORDER BY,CASSANDRA-14286,13142178,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,fcofdezc,accek-invinets,accek-invinets,02/Mar/18 19:26,12/Mar/19 14:19,13/Mar/19 22:34,17/Apr/18 10:45,2.2.13,3.0.17,3.11.3,4.0,,Legacy/CQL,,,,,0,,,,"When running the following code:

{code}
public class CassandraJsonOrderingBug {
    public static void main(String[] args) {
        Session session = CassandraFactory.getSession();

        session.execute(""CREATE TABLE thebug ( PRIMARY KEY (a, b), a INT, b INT)"");
        try {
            session.execute(""INSERT INTO thebug (a, b) VALUES (20, 30)"");
            session.execute(""INSERT INTO thebug (a, b) VALUES (100, 200)"");
            Statement statement = new SimpleStatement(""SELECT JSON a, b FROM thebug WHERE a IN (20, 100) ORDER BY b"");
            statement.setFetchSize(Integer.MAX_VALUE);
            for (Row w: session.execute(statement)) {
                System.out.println(w.toString());
            }
        } finally {
            session.execute(""DROP TABLE thebug"");
        }
    }
}
{code}

The following exception is thrown server-side:

{noformat}
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.Collections$SingletonList.get(Collections.java:4815) ~[na:1.8.0_151]
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1297) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1284) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355) ~[na:1.8.0_151]
	at java.util.TimSort.sort(TimSort.java:220) ~[na:1.8.0_151]
	at java.util.Arrays.sort(Arrays.java:1512) ~[na:1.8.0_151]
	at java.util.ArrayList.sort(ArrayList.java:1460) ~[na:1.8.0_151]
	at java.util.Collections.sort(Collections.java:175) ~[na:1.8.0_151]
{noformat}

(full traceback attached)

The accessed index is the index of the sorted column in the SELECT JSON fields list.
Similarly, if the select clause is changed to

SELECT JSON b, a FROM thebug WHERE a IN (20, 100) ORDER BY b

then the query finishes, but the output is sorted incorrectly (by textual JSON representation):

{noformat}
Row[{""b"": 200, ""a"": 100}]
Row[{""b"": 30, ""a"": 20}]
{noformat}",Kubernetes cluster using cassandra:3.11.1 Docker image.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,02/Mar/18 19:23;accek-invinets;orderbug-traceback.txt;https://issues.apache.org/jira/secure/attachment/12912816/orderbug-traceback.txt,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-03-22 10:21:44.875,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 17 10:45:04 UTC 2018,,,,,,0|i3qt53:,9223372036854775807,,,,,,,,blerer,blerer,,,2.2.0,,,,,,,"22/Mar/18 10:21;fcofdezc;The problem was that columns were serialized as json and then discarded, while {{OrderingComparator}} expected those columns to be present. With this patch, all columns are present and only the json column is sent back to clients. It uses the same mechanism as the regular ordering of non-selected columns.

[Patch 2.2|https://github.com/apache/cassandra/compare/trunk...blerer:14286-2.2-review] 
[Patch 3.0|https://github.com/apache/cassandra/compare/trunk...fcofdez:14286-3.0?expand=1] 
[Patch 3.11|https://github.com/apache/cassandra/compare/trunk...fcofdez:14286-3.11?expand=1] 
[Patch trunk|https://github.com/apache/cassandra/compare/trunk...fcofdez:14286-trunk-2?expand=1] ",04/Apr/18 15:17;blerer;Patches and test results look fine. Thanks.,"05/Apr/18 15:45;blerer;I just noticed while merging a problem with the trunk patch. In trunk {{Selection}} is immutable but the patch is breaking that contract.

Sorry, for noticing that problem only now.","09/Apr/18 11:15;fcofdezc;I've updated the trunk patch with the suggested approach, let me know if there is something that should be changed. Thanks for the review.","17/Apr/18 08:33;blerer;I looked at the trunk patch. For queries with selection clauses having some processing like: {{SELECT JSON a, CAST(b AS FLOAT) FROM %s WHERE a IN (20, 100) ORDER BY b}}. The query will fail with an {{IndexOutOfBoundException}} during the {{Selection}} initialization.

I looked into the problem and pushed a correction to the patch [here|https://github.com/apache/cassandra/compare/trunk...blerer:14286-trunk-review]. CI results look good.

[~fcofdezc] Could you have a look at my changes and tell me if they make sense to you?","17/Apr/18 09:14;fcofdezc;I see the problem now, thanks for pointing out. The code seems less complicated with your approach, so +1. Only one nit: there is a {{println}} on {{Selection.java}}.","17/Apr/18 10:45;blerer;Committed into 2.2 at 594cde7937de6f848998bac35d35591f8a18aa10 and merged into 3.0, 3.11 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix batch commitlog sync regression,CASSANDRA-14292,13142731,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,06/Mar/18 01:55,12/Mar/19 14:19,13/Mar/19 22:34,07/Mar/18 12:12,3.0.17,3.11.3,4.0,,,,,,,,0,,,,"Prior to CASSANDRA-13987, in batch commitlog mode, commitlog will be synced to disk right after mutation comes.
 * haveWork semaphore is released in BatchCommitLogService.maybeWaitForSync
 * AbstractCommitlogService will continue and sync to disk

After C-13987, it makes a branch for chain maker flush more frequently in periodic mode. To make sure in batch mode CL still flushes immediately, it added {{syncRequested}} flag.
 Unfortunately, in 3.0 branch, this flag is not being set to true when mutation is waiting.

So in AbstractCommitlogService, it will not execute the CL sync branch until it reaches sync window(2ms)..
{code:java|title=AbstractCommitLogService.java}
if (lastSyncedAt + syncIntervalMillis <= pollStarted || shutdown || syncRequested)
{
    // in this branch, we want to flush the commit log to disk
    syncRequested = false;
    commitLog.sync(shutdown, true);
    lastSyncedAt = pollStarted;
    syncComplete.signalAll();
}
else
{
    // in this branch, just update the commit log sync headers
    commitLog.sync(false, false);
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,07/Mar/18 01:27;jasonstack;14292-3.0-dtest.png;https://issues.apache.org/jira/secure/attachment/12913305/14292-3.0-dtest.png,07/Mar/18 01:27;jasonstack;14292-3.0-unittest.png;https://issues.apache.org/jira/secure/attachment/12913306/14292-3.0-unittest.png,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2018-03-06 03:48:05.648,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 10:25:53 UTC 2018,,,,,,0|i3qwk7:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"06/Mar/18 02:02;jasonstack;|[3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-14292-3.0]| unit running |dtest running|

Changes:

1. Set {{syncRequested}} to true in {{BatchCommitLogService.maybeWaitForSync, so batch CL will sync immediately when mutation comes}}

2. Added unit test to verify batch CL sync and shutdown immediately","06/Mar/18 03:48;jasobrown;Please take a look to see if CASSANDRA-14194 (committed today) helps. I'll read this more thoroughly, as well.","06/Mar/18 05:05;jasonstack;CASSANDRA-14194 was solving race condition with multiple mutations. This ticket is to solve performance for single mutation.

In 3.0 branch with 13987, each mutation will have to wait for full batch-sync-window(2ms) for commitlog to sync.","06/Mar/18 16:21;aweisberg;[~jasobrown] looks like it wasn't registering to receive notification of the sync before requesting the sync. It's another way for {{syncRequested}} to get clobbered.

I'm +1 on this change. Is it a 3.11 bug as well?",06/Mar/18 16:24;jasonstack;It’s 3.0.16 only..,"07/Mar/18 00:19;jasobrown;[~jasonstack] Yeah, you are correct; I missed this case. This is the problem when working on the same patch across four branches; I'm sure I had something like this at one point in a 3.0 branch .... <sigh>

I'm +1 on the patch, as well. Thanks for the unit test - I like it enough that I'll forward port it to 3.11 and trunk when I commit.",07/Mar/18 12:12;jasobrown;committed as sha {{00c90c16e99fdfb153cb418f0e3486b62183986e}}. Thanks!,08/Mar/18 10:25;jasonstack;Thanks for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadCommandTest::testCountWithNoDeletedRow broken,CASSANDRA-14234,13138387,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,KurtG,KurtG,KurtG,14/Feb/18 01:04,12/Mar/19 14:19,13/Mar/19 22:34,14/Feb/18 13:27,3.11.2,4.0,,,,,,,,,0,,,,"{code}junit.framework.AssertionFailedError: expected:<1> but was:<5>
	at org.apache.cassandra.db.ReadCommandTest.testCountWithNoDeletedRow(ReadCommandTest.java:336){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-14 02:39:38.195,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 14 13:27:32 UTC 2018,,,,,,0|i3q5tj:,9223372036854775807,3.11.x,,,,,,,jasobrown,jasobrown,,,,,,,,,,14/Feb/18 01:09;KurtG;[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14234-3.11],"14/Feb/18 02:39;jasobrown;using {{git bisect}}, it looks like the commit that broke the unit test is {{9d649d69a56a91fcb06a3582b22606f0fe361f49}}, introduced in CASSANDRA-8527. ","14/Feb/18 02:54;jasobrown;patch looks clean/compiles/test succeeds on 3.11, but trunk fails to apply cleanly. [~KurtG] can you take a look at that, as well?",14/Feb/18 03:18;jasobrown;3.11 utest run [looks good|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14234-3.11].,"14/Feb/18 05:42;KurtG;[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14234-trunk]
Sorry 'bout that. Not sure why I naively expected it to merge up cleanly.","14/Feb/18 13:27;jasobrown;+1. committed as sha {{1d506f9d09c880ff2b2693e3e27fa58c02ecf398}}

Thanks for the quick work on this, [~KurtG]. I appreciate it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra does not respect hint window for CAS,CASSANDRA-14215,13136453,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,KurtG,arijit91,arijit91,06/Feb/18 02:38,12/Mar/19 14:19,13/Mar/19 22:34,15/Mar/18 14:27,3.0.17,3.11.3,4.0,,,Consistency/Hints,Legacy/Streaming and Messaging,,,,0,LWT,,,"On Cassandra 3.0.9, it was observed that Cassandra continues to write hints even though a node remains down (and does not come up) for longer than the default 3 hour window.

 

After doing ""nodetool setlogginglevel org.apache.cassandra TRACE"", we see the following log line in cassandra (debug) logs:
 StorageProxy.java:2625 - Adding hints for [/10.0.100.84]

 

One possible code path seems to be:

cas -> commitPaxos(proposal, consistencyForCommit, true); -> submitHint (in StorageProxy.java)

 

The ""true"" parameter above explicitly states that a hint should be recorded and ignores the time window calculation performed by the shouldHint method invoked in other code paths. Is there a reason for this behavior?

 

Edit: There are actually two stacks that seem to be producing hints, the ""cas"" and ""syncWriteBatchedMutations"" methods. I have posted them below.

 

A third issue seems to be that Cassandra seems to reset the timer which counts how long a node has been down after a restart. Thus if Cassandra is restarted on a good node, it continues to accumulate hints for a down node over the next three hours.

 

{code:java}
WARN [SharedPool-Worker-14] 2018-02-06 22:15:51,136 StorageProxy.java:2636 - Adding hints for [/10.0.100.84] with stack trace: java.lang.Throwable: at org.apache.cassandra.service.StorageProxy.stackTrace(StorageProxy.java:2608) at org.apache.cassandra.service.StorageProxy.submitHint(StorageProxy.java:2617) at org.apache.cassandra.service.StorageProxy.submitHint(StorageProxy.java:2603) at org.apache.cassandra.service.StorageProxy.commitPaxos(StorageProxy.java:540) at org.apache.cassandra.service.StorageProxy.cas(StorageProxy.java:282) at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithCondition(ModificationStatement.java:432) at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:407) at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:237) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222) at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) at java.lang.Thread.run(Thread.java:748) WARN
{code}

{code:java}
[SharedPool-Worker-8] 2018-02-06 22:15:51,153 StorageProxy.java:2636 - Adding hints for [/10.0.100.84] with stack trace: java.lang.Throwable: at org.apache.cassandra.service.StorageProxy.stackTrace(StorageProxy.java:2608) at org.apache.cassandra.service.StorageProxy.submitHint(StorageProxy.java:2617) at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:1247) at org.apache.cassandra.service.StorageProxy.syncWriteBatchedMutations(StorageProxy.java:1014) at org.apache.cassandra.service.StorageProxy.mutateAtomically(StorageProxy.java:899) at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:834) at org.apache.cassandra.cql3.statements.BatchStatement.executeWithoutConditions(BatchStatement.java:365) at org.apache.cassandra.cql3.statements.BatchStatement.execute(BatchStatement.java:343) at org.apache.cassandra.cql3.statements.BatchStatement.execute(BatchStatement.java:329) at org.apache.cassandra.cql3.statements.BatchStatement.execute(BatchStatement.java:324) at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:237) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222) at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) at java.lang.Thread.run(Thread.java:748)
{code}

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-06 23:19:16.865,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 15 14:26:19 UTC 2018,,,,,,0|i3ptw7:,9223372036854775807,3.0.16,3.11.2,,,,,,iamaleksey,iamaleksey,,,3.0.0,,,,,,,"06/Feb/18 23:19;jjirsa;Fairly sure this is working as intended; we should write hints beyond the hint window, we just won't replay hints beyond the hint window (so if a host is down for 4 hours, and hint lifetime is 3 hours, we'll lose the first hour of hints, but still replay the last 3).

Check out {{Hint.isLive()}} for more info, and let me know if we can close this if you agree.



","06/Feb/18 23:21;arijit91;[~jjirsa] Thanks for the response. What's the reason for writing hints beyond the hint window if there's no chance that they would be replayed? On our production cluster, this causes downtime as the Cassandra partition fills up quickly with hints when a node goes down.","06/Feb/18 23:29;arijit91;[~jjirsa] Is Hint.isLive a bit orthogonal to this, it seems to be dealing with gc grace period?

Also hints *are not written* in some code paths once the node is observed to be down for more than the window. Also the documentation around hints explicitly says that hints will not *be generated* when a node is down or longer than the window.

Finally, check out the bit around the node down timer being reset on Cassandra restarts. This does not matter if we are ok with generating hints beyond the max hint window, but I really don't think that we should be...

 ","06/Feb/18 23:30;jjirsa;We don't know when the node is going to come back up, and we don't know what you as a user expect to happen.

We guarantee we won't replay a hint past the time you specify, but as I explained, if your limit is N hours, and you're down for N + 1 hour, we can still replay N hours of data via hints, making you repair only 1 hour instead of N. For many users, this is preferable to replaying nothing.

There is a {{deleteAllHintsForEndpoint}} JMX target that would let you purge hints (manually), but it does perhaps seem like a missing feature that we don't more aggressively clean up hints that are expired. [~iamaleksey] any thoughts here?

Edit: reading your last comment, I may be wrong (it happens a lot, so maybe I shouldn't be surprised). Aleksey will know for sure. 
","07/Feb/18 23:44;KurtG;My understanding was that we'd keep hints for the first N hours, and then we'd stop storing them. We'd always replay whatever hints are stored and delete them after replaying. 

Code in {{org.apache.cassandra.service.StorageProxy#shouldHint}} seems to imply the above
{code:java}
boolean hintWindowExpired = Gossiper.instance.getEndpointDowntime(ep) > DatabaseDescriptor.getMaxHintWindow();
if (hintWindowExpired)
{
    HintsService.instance.metrics.incrPastWindow(ep);
    Tracing.trace(""Not hinting {} which has been down {} ms"", ep, Gossiper.instance.getEndpointDowntime(ep));
}
return !hintWindowExpired;
{code}
This sounds like an issue to me, pretty sure we shouldn't be storing anything that's past the hint window.

bq. Finally, check out the bit around the node down timer being reset on Cassandra restarts. This does not matter if we are ok with generating hints beyond the max hint window, but I really don't think that we should be...
 This is also not really intended. As checks are currently only performed on how long the node has been down if it comes up after 3 hours then goes back down straight away we'll effectively store 6 hours of hints for the node. It's probably reasonable to only store a maximum of {{max_hint_window_in_ms}}. We might be able to get away with just looking at the timestamp of the earliest hint for the node and using that if it's prior to the current downtime.

I'll have a look at these in the next few days.
{quote}There is a deleteAllHintsForEndpoint JMX target that would let you purge hints (manually), but it does perhaps seem like a missing feature that we don't more aggressively clean up hints that are expired.
{quote}
[~VincentWhite] was looking at this recently while trying to solve a problem where hint replaying would get stuck and continuously replay a single hint if it happened to fail on the receiving side continuously (usually due to receiving node being overloaded and never acknowledging the hint in time). I recall that he also had problems because we have no service to clean up hints in this case. He's on leave this week but would probably have some input around this.","08/Feb/18 00:20;arijit91;Thanks [~KurtG]. Yes that's the main issue, we're storing hints beyond the hint window.

About the Cassandra restarts, I meant restarts on the *live node* and not on the *dead node* also end up resetting the timer. Though the point you mentioned is valid as well.","08/Feb/18 01:13;KurtG;Huh yeah, that makes sense. I missed that part but definitely also an issue. I've already started working on the patch so I'll take the ticket.","08/Feb/18 12:24;iamaleksey;It could be the case that LWT write path is not respecting hint window. If it is, I'm pretty sure it's *not* intended.

Everything else here I *think* works as intended - behaved like this since the very beginning of hints. Whether or not that intention was ever optimal is a different matter, and it's not immediately obvious to me it it was or wasn't.","24/Feb/18 20:44;arijit91;Just to confirm [~KurtG], will the fix that you are working on also make it to the 3.0 branch?","26/Feb/18 00:58;KurtG;[~arijit91] Will have 2 separate patches up at some point today. One for CAS problem which will be for >=3.0, and another for the hint window problem which will be just targeted at trunk. Don't really want to change hint window behaviour in any minors.

As far as I can tell the batch statements are adhering to {{StorageProxy::shouldHint()}} so no fix necessary there.","08/Mar/18 09:04;KurtG;So I know I said today... but then I realised there were some issues with the tests I'd written and instead got distracted by another ticket.

I've got two patches, one for the CAS problem (which should go to 3.0, 3.11, and trunk), and a patch for the non-persistent hint window problem which is aimed only at trunk. 

CAS patches:
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14215-3.0]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14215-3.11]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14215-trunk-1]|

Hint window patch:
|[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14215-trunk-2]|

There's a dtest for each patch, at the moment both are in a single branch.
|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:14215]|
","09/Mar/18 13:19;iamaleksey;The CAS patch looks good to me, and I agree it's fine for all of 3.0, 3.11, and trunk.

I'm not yet sure about the change to the semantic of hint window, however. And either way, we should be committing both in the same ticket.

Can you please split it out into a separate JIRA? In that case I'll +1 the CAS patch, and we can keep discussion about the window change and review it separately.

Cheers.","13/Mar/18 03:33;KurtG;Thanks [~iamaleksey].

Created CASSANDRA-14309 for hint window patch.

Branches are the same for C*

|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14215-3.0]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14215-3.11]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14215-trunk-1]| 

I've split out the dtest for CAS.
|[dtest|https://github.com/apache/cassandra-dtest/compare/master...kgreav:14215-2]|",15/Mar/18 14:26;iamaleksey;Committed to 3.0 as [fdc61cb6a74163402181d133519462f0ab1b504d|https://github.com/apache/cassandra/commit/fdc61cb6a74163402181d133519462f0ab1b504d] and merged into 3.11 and trunk. Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in repair_tests.incremental_repair_test.TestIncRepair.multiple_repair_test ,CASSANDRA-13515,13070511,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,aweisberg,aweisberg,09/May/17 17:39,12/Mar/19 14:16,13/Mar/19 22:34,26/Nov/18 23:08,,,,,,Test/dtest,,,,,0,dtest,test-failure,test-failure-fresh,"{noformat}
 Failed 7 times in the last 24 runs. Flakiness: 47%, Stability: 70%
Error Message

errors={<Host: 127.0.0.2 datacenter1>: ReadTimeout('Error from server: code=1200 [Coordinator node timed out waiting for replica nodes\' responses] message=""Operation timed out - received only 0 responses."" info={\'received_responses\': 0, \'required_responses\': 1, \'consistency\': \'ONE\'}',)}, last_host=127.0.0.2
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-uYGL17
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'memtable_allocation_type': 'offheap_objects',
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory
    conn = cls(host, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__
    self._connect_socket()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory
    conn = cls(host, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__
    self._connect_socket()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory
    conn = cls(host, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__
    self._connect_socket()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory
    conn = cls(host, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__
    self._connect_socket()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Host 127.0.0.1 has been marked down
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__
    self._connection.set_keyspace_blocking(self._keyspace)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking
    raise conn_exc
ConnectionException: Problem while setting keyspace: InvalidRequest('Error from server: code=2200 [Invalid query] message=""Keyspace \'ks\' does not exist""',)
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.1
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
dtest: DEBUG: insert data
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
dtest: DEBUG: bringing down node 3
dtest: DEBUG: inserting additional data into node 1 and 2
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool
    new_pool = HostConnection(host, distance, self)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__
    self._connection = session.cluster.connection_factory(host.address)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory
    conn = cls(host, *args, **kwargs)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__
    self._connect_socket()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
dtest: DEBUG: restarting and repairing node 3
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
dtest: DEBUG: stopping node 2
dtest: DEBUG: inserting data in nodes 1 and 3
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
dtest: DEBUG: start and repair node 2
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.connection: WARNING: Heartbeat failed for connection (140695947871376) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.connection: WARNING: Heartbeat failed for connection (140695354796304) to 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140695383359824) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.5', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140693197814160) to 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140695354906960) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.connection: WARNING: Heartbeat failed for connection (140695947873424) to 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140695366436368) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.connection: WARNING: Heartbeat failed for connection (140695354794256) to 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140695925137168) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.5', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.4, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.4', 9042)]. Last error: Connection refused
cassandra.connection: WARNING: Heartbeat failed for connection (140695367411408) to 127.0.0.3
cassandra.connection: WARNING: Heartbeat failed for connection (140695363867088) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.5', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.5', 9042)]. Last error: Connection refused
cassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool
cassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn't already
cassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool
cassandra.cluster: INFO: Connection pools established for node 127.0.0.3
cassandra.cluster: INFO: Connection pools established for node 127.0.0.2
dtest: DEBUG: replace node and check data integrity
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.5', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.4, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [('127.0.0.4', 9042)]. Last error: Connection refused
cassandra.connection: WARNING: Heartbeat failed for connection (140694303505168) to 127.0.0.3
cassandra.cluster: WARNING: Host 127.0.0.3 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.3', 9042)]. Last error: Connection refused
cassandra.protocol: WARNING: Server warning: Aggregation query used without partition key
dtest: DEBUG: Retrying read after timeout. Attempt #0
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 409, in multiple_repair_test
    assert_one(session, ""SELECT COUNT(*) FROM ks.cf LIMIT 200"", [149])
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/cassandra-dtest/tools/assertions.py"", line 128, in assert_one
    res = session.execute(simple_query)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2018, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 3822, in result
    raise self._final_exception
'errors={<Host: 127.0.0.2 datacenter1>: ReadTimeout(\'Error from server: code=1200 [Coordinator node timed out waiting for replica nodes\\\' responses] message=""Operation timed out - received only 0 responses."" info={\\\'received_responses\\\': 0, \\\'required_responses\\\': 1, \\\'consistency\\\': \\\'ONE\\\'}\',)}, last_host=127.0.0.2\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-uYGL17\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'memtable_allocation_type\': \'offheap_objects\',\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__\n    self._connect_socket()\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__\n    self._connect_socket()\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__\n    self._connect_socket()\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__\n    self._connect_socket()\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Host 127.0.0.1 has been marked down\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.1:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.policies: INFO: Using datacenter \'datacenter1\' for DCAwareRoundRobinPolicy (via host \'127.0.0.1\'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 334, in __init__\n    self._connection.set_keyspace_blocking(self._keyspace)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 797, in set_keyspace_blocking\n    raise conn_exc\nConnectionException: Problem while setting keyspace: InvalidRequest(\'Error from server: code=2200 [Invalid query] message=""Keyspace \\\'ks\\\' does not exist""\',)\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.pool: INFO: Successful reconnection to 127.0.0.1, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.1 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.1\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ndtest: DEBUG: insert data\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ndtest: DEBUG: bringing down node 3\ndtest: DEBUG: inserting additional data into node 1 and 2\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: WARNING: Failed to create connection pool for new host 127.0.0.3:\nTraceback (most recent call last):\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 2327, in run_add_or_renew_pool\n    new_pool = HostConnection(host, distance, self)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/pool.py"", line 331, in __init__\n    self._connection = session.cluster.connection_factory(host.address)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/cluster.py"", line 1111, in connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 324, in factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 299, in __init__\n    self._connect_socket()\n  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest-offheap/venv/src/cassandra-driver/cassandra/connection.py"", line 363, in _connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: Connection pool could not be created, not marking node 127.0.0.3 up\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ndtest: DEBUG: restarting and repairing node 3\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ndtest: DEBUG: stopping node 2\ndtest: DEBUG: inserting data in nodes 1 and 3\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ndtest: DEBUG: start and repair node 2\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.connection: WARNING: Heartbeat failed for connection (140695947871376) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.connection: WARNING: Heartbeat failed for connection (140695354796304) to 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140695383359824) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.5\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140693197814160) to 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140695354906960) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.2\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.connection: WARNING: Heartbeat failed for connection (140695947873424) to 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140695366436368) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.connection: WARNING: Heartbeat failed for connection (140695354794256) to 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140695925137168) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.5\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.4, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.4\', 9042)]. Last error: Connection refused\ncassandra.connection: WARNING: Heartbeat failed for connection (140695367411408) to 127.0.0.3\ncassandra.connection: WARNING: Heartbeat failed for connection (140695363867088) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.5\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.5\', 9042)]. Last error: Connection refused\ncassandra.pool: INFO: Successful reconnection to 127.0.0.3, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.3 may be up; will prepare queries and open connection pool\ncassandra.pool: INFO: Successful reconnection to 127.0.0.2, marking node up if it isn\'t already\ncassandra.cluster: INFO: Host 127.0.0.2 may be up; will prepare queries and open connection pool\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.3\ncassandra.cluster: INFO: Connection pools established for node 127.0.0.2\ndtest: DEBUG: replace node and check data integrity\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.5, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.5\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.4, scheduling retry in 600.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.4\', 9042)]. Last error: Connection refused\ncassandra.connection: WARNING: Heartbeat failed for connection (140694303505168) to 127.0.0.3\ncassandra.cluster: WARNING: Host 127.0.0.3 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.3, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [(\'127.0.0.3\', 9042)]. Last error: Connection refused\ncassandra.protocol: WARNING: Server warning: Aggregation query used without partition key\ndtest: DEBUG: Retrying read after timeout. Attempt #0\n--------------------- >> end captured logging << ---------------------'
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,2017-05-09 17:39:39.0,,,,,,0|i3eq1j:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index redistribution breaks SASI index,CASSANDRA-14055,13118958,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jrwest,lboutros,lboutros,16/Nov/17 15:51,12/Mar/19 14:15,13/Mar/19 22:34,14/May/18 19:48,3.11.3,4.0,,,,Feature/SASI,,,,,0,patch,sasi,,"During index redistribution process, a new view is created.
During this creation, old indexes should be released.

But, new indexes are ""attached"" to the same SSTable as the old indexes.

This leads to the deletion of the last SASI index file and breaks the index.

The issue is in this function : [https://github.com/apache/cassandra/blob/9ee44db49b13d4b4c91c9d6332ce06a6e2abf944/src/java/org/apache/cassandra/index/sasi/conf/view/View.java#L62]



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Feb/18 23:40;jrwest;14055-jrwest-3.11.patch;https://issues.apache.org/jira/secure/attachment/12911170/14055-jrwest-3.11.patch,19/Feb/18 23:40;jrwest;14055-jrwest-trunk.patch;https://issues.apache.org/jira/secure/attachment/12911171/14055-jrwest-trunk.patch,12/Feb/18 23:55;jrwest;CASSANDRA-14055-jrwest.patch;https://issues.apache.org/jira/secure/attachment/12910301/CASSANDRA-14055-jrwest.patch,23/Nov/17 11:49;lboutros;CASSANDRA-14055.patch;https://issues.apache.org/jira/secure/attachment/12899051/CASSANDRA-14055.patch,17/Nov/17 17:36;lboutros;CASSANDRA-14055.patch;https://issues.apache.org/jira/secure/attachment/12898241/CASSANDRA-14055.patch,16/Nov/17 15:57;lboutros;CASSANDRA-14055.patch;https://issues.apache.org/jira/secure/attachment/12898006/CASSANDRA-14055.patch,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2017-11-20 14:43:23.021,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri May 18 11:53:51 UTC 2018,,,,,,0|i3mvrb:,9223372036854775807,3.10,3.11.1,,,,,,jasobrown,jasobrown,,,,,,,,,,"16/Nov/17 15:56;lboutros;Here is a first little patch with a unit test and a simple fix.

I have somme issues with the unit test ""SASIIndexTest.testIndexMemtableSwitching"".
It works alone, but fails if the whole test is runned. 

Any help would be really appreciated.",17/Nov/17 17:36;lboutros;A new patch with unit tests fixed.,"20/Nov/17 14:43;githubbot;GitHub user ludovic-boutros opened a pull request:

    https://github.com/apache/cassandra/pull/174

    CASSANDRA-14055: Index redistribution breaks SASI index

    During index redistribution process, a new view is created.
    During this creation, old indexes should be released.
    
    But, new indexes are ""attached"" to the same SSTable as the old indexes.
    
    This leads to the deletion of the last SASI index file and breaks the index.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ludovic-boutros/cassandra fix/CASSANDRA-14055

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/174.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #174
    
----
commit 532ed86090c27e51c745c57678cd19ff4b606a0c
Author: lboutros@flatironsjouve.com <lboutros@flatironsjouve.com>
Date:   2017-11-20T14:39:41Z

    CASSANDRA-14055: Index redistribution breaks SASI index

----
","22/Nov/17 14:52;lboutros;It seems to be a bit more complex.
There is a case that I'm not able to reproduce in a unit test which breaks as well SASI index, but I'm able to reproduce it in debug mode (and in production of course :( ).","23/Nov/17 11:46;lboutros;I've updated the PR with a more complex unit test and a better fix.

I will add another patch here for reference.","23/Nov/17 14:29;githubbot;Github user ludovic-boutros commented on the issue:

    https://github.com/apache/cassandra/pull/174
  
    I have added a flag in order to keep index files in the index release function.
    This way I can prevent deletions.
    
    Any advise on the patch ?
",30/Nov/17 21:12;jjirsa;[~ifesdjeen] are you still reviewing SASI patches or do we need to find someone else?,"07/Dec/17 17:02;lboutros;Any news on this ? We are currently using the patch in production, but that would be better to use an official version of C* :). 

[~ifesdjeen], do you need some help ?
","07/Dec/17 20:34;ifesdjeen;Sorry for not getting to it. I'm currently a bit overloaded with other things, I hope to get to it next week. If anyone has capacity for it - just assign yourself as a reviewer!","31/Jan/18 10:05;lboutros;[~ifesdjeen] do you think you will have some time to review this patch soon ? I really would like to see it included in 3.11.2 :D

Thx.","31/Jan/18 23:25;jrwest;Hi [~lboutros],

One of the original authors of SASI here. I've been taking a look at this issue and your patch. Using the provided test against the {{cassandra-3.11}} branch (fc3357a00e2b6e56d399f07c5b81a82780c1e143), I see three different failure cases – two related directly to this issue and one tangentially related. More details on those below. With respect to this issue in particular, the three scenarios cause the test to fail because {{IndexSummaryManager}} ends up creating a new {{View}} where {{oldSSTables}} and {{newIndexes}} have overlapping values. This occurs because the {{IndexSummaryManager}} may ""update"" (re-open) an {{SSTableReader}} for an index already in the view. I believe this is unique to {{IndexSummaryManager}} and I am able to make your tests pass* without your patch by ensuring that there is no overlap between {{oldSStables}} and {{newIndexes}} (favoring {{newIndexes}}). Your patch looks to do this as well, though the approach is a bit different.

One thing I am curious about in your patch is the {{keepFile}} changes to {{SSTableIndex#release}}. Generally, this concerns me because it seems to be working around improper reference counting rather than correcting the reference counting itself. Also, while using the provided test, I am unable to hit a case where the condition {{obsolete.get() || sstableRef.globalCount() == 0}} is true. I see the file missing in the {{View}} but not on disk itself. Could you elaborate a bit more on the need for this change and your use of the {{keepFile}} flag?

The three failure scenarios I see using the provided test are:
h5. 8 keys returned - sequential case

In this scenario, at the time when the query that fails runs, the {{View}} is missing the most recently flushed sstable. As mentioned previously, this is because the intersection of {{oldSSTables}} and {{newIndexes}} is non-empty. This can be fixed* by ensuring nothing in {{newIndexes}} is in {{oldSSTables}}. I call this the sequential case because the compaction that occurs during the test completes before the index summary redistribution begins to create a new {{View}}. This is also addressed by your patch.
h5. 8 keys returned - race case

This scenario is similar to the previous one but has the additional issue of triggering improper {{SSTableIndex}} reference counting. From the perspective of the provided test, the failure scenario is the same and the fix* is as well. The issue occurs because of a race between compaction and index redistribution's creation of new {{View}} instances. This causes redistribution to create two {{View}} instances, the first of which is thrown away due to a failed compare and swap. The problem is the side-effects (calling {{SSTableIndex#release}}) have occurred already inside the creation of the garbage {{View}}, causing the reference count for the index to drop below 0. I see this issue as a separate one from this ticket and have filed [CASSANDRA-14207|https://issues.apache.org/jira/browse/CASSANDRA-14207]. It is not fixed by the previously mentioned change and while I haven't checked in detail, I don't think the provided patch addresses this either.
h5. 0 keys returned

This scenario is similar to the first but there are three threads involved in the race: the compaction, the flushing of the last memtable, and the index redistribution. In this case, the end result is an empty {{View}}, which leads to no keys being returned since the system thinks there are no indexes to search. This is fixed* by what I mentioned previously and occurs because index redistribution re-opens both sstables in the original {{View}} instead of just one. It is also addressed by your patch. 

 

I am curious if you see any other failure scenarios besides these three and, in particular, if you can elaborate on and provide examples of the issues you see regarding the files being missing on disk and the need for the {{keepFile}} change.

\* While this fix makes the provided test pass I am still verifying its correct from the reference counting perspective.","01/Feb/18 11:04;lboutros;Hi [~jrwest],

first, thank you for reviewing this patch.
 I will try to give answers to your questions.

Your global analysis is correct. The idea of this patch was to change as few things as possible.
 I do not see any other failure scenarios currently.
 We are using this patch in production with success since the end of november.

Regarding the {{keepFile}} change, with my last patch, I can reproduce the file deletion with the {{forceFlush}} boolean set to {{true}}.

You can just add a conditional breakpoint with {{keepFile && (obsolete.get() || sstableRef.globalCount() == 0)}} in the {{release}} function.
 It will stop on each attempt of index redistribution with {{forceFlush}} active (second part of the test).

With my limited knowledge of the global code, I did not see any issue in the reference counting process with my patch.
 But again, I'm quite new with this code :).","09/Feb/18 14:51;lboutros;Hi [~jrwest],

by any chance, did you have some time to work on this ? :)

Thx in advance.","09/Feb/18 19:55;jrwest;Hi [~lboutros],

My apologies for the delay. I am waiting on internal review of my version of the patch so you can take a look. I believe this patch accomplishes the same thing but with less changes and doesn't affect the reference counting in SSTableIndex. I hope to have it posted for your review and testing early next week.","12/Feb/18 08:30;lboutros;Hi [~jrwest],

no problem. I would be glad to review your patch. It will be a good way to learn :).","12/Feb/18 23:54;jrwest;Hi [~lboutros],

Attached my patch for your review and testing. If you could verify this does the right thing in your environments that would be especially helpful since I have been unable to replicate the deleted file issue – I only see the sstables removed from the SASI View.

The gist of the patch is to ensure the intersection of {{oldSStables}} and {{newIndexes}} is always empty. Your patch was doing the same by not checking {{oldSSTables}} in the second for-loop, but this approach doesn't require the changes to {{SSTableIndex#release}}.

I re-used your test but removed the version that runs with the data entirely in-memory since that won't be affected by index redistribution.","13/Feb/18 12:52;jasobrown;For sanity sake, I'm running the utests on [~jrwest]'s patch:

||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/14055-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/14055-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14055-3.11]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14055-trunk]|
||
","13/Feb/18 13:08;jasobrown;[~jrwest] patch does not compile on trunk. can you take a look?

{noformat}
build-test:
    [javac] Compiling 587 source files to /orig/opt/dev/cassandra/build/test/classes
    [javac] /orig/opt/dev/cassandra/test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java:946: error: cannot find symbol
    [javac]         int minIndexInterval = store.metadata.params.minIndexInterval;
    [javac]                                              ^
    [javac]   symbol:   variable params
    [javac]   location: variable metadata of type TableMetadataRef
    [javac] /orig/opt/dev/cassandra/test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java:955: error: cannot find symbol
    [javac]             store.metadata.minIndexInterval(minIndexInterval);
    [javac]                           ^
    [javac]   symbol:   method minIndexInterval(int)
    [javac]   location: variable metadata of type TableMetadataRef
    [javac] /orig/opt/dev/cassandra/test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java:961: error: cannot find symbol
    [javac]         store.metadata.minIndexInterval(minIndexInterval);
    [javac]                       ^
    [javac]   symbol:   method minIndexInterval(int)
    [javac]   location: variable metadata of type TableMetadataRef
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 3 errors
{noformat}","13/Feb/18 17:57;lboutros;Hi [~jrwest],

thx for your patch. The main difference between the two patches is that you do not release old {{SSTableIndex}} objects at all.

This way you do not remove the index file. On the other hand, don't you think you will have an issue in the reference count ? I'm not sure but in my understanding, the global count will never go down to zero. But perhaps am I wrong.




 

 ","14/Feb/18 17:06;jrwest;[~jasobrown], sorry for the trunk issues. The way {{TableMetadata}} is accessed/stored was changed and the test will need to be modified as a result. Will post a separate patch for trunk. 

[~lboutros], In my testing, the primary issue I saw was that files were removed from the SASI {{View}} that shouldn't be. The test writes 5 sstables (with sequence numbers 1-4 & 6) and during the test a compaction typically happens (that generates a sstable with generation 5 from sstables 1-4). The final SASI {{View}} when the queries are performed should contain either (1-4, 6) or (5, 6)*. The test fails by returning 8 keys instead of 10 when the SASI {{View}} ends up containing only sstable 5 or by returning 0 keys instead of 10 when the SASI {{View}} ends up empty. 

The issue occurs when index redistribution completes. Depending on the interleaving* of events (the memtable flush, compaction, and redistribution), redistribution re-opens sstable 6, and sometimes re-opens sstable 5. This results in an {{SSTableListChangedNotification}}, which in turn results in the creation of a new {{View}},  where {{added=[6]}} (or {{added=[5,6]}}) and {{removed=[6]}} (or {{removed=[5,6]}}). The SASI {{View}} was written assuming these two sets were disjoint, which is why any reader in {{oldSSTables}} caused the index to be closed. This is incorrect in both cases because sstables 5 and 6 are indeed the active data files (5 contains keys 0-8, and 6 contains keys 9 & 10). 

Regarding the ref counting, we want to maintain one reference to sstables 5 & 6 via their SSTableIndex instance but we’ve created a second reference and one needs to be closed. This is ensured by the {{newView.containsKey(sstable.descriptor)}} part of the conditional (so we are still indeed calling {{#release()}} on one instance). As I am writing this, however, I am realizing we want to keep a reference to the newer index, which references the newer SSTable instance and my patch does the opposite — keeping the old instance. I will post an updated patch along with my trunk patch for internal review, but the gist is to change the order we iterate over the old view and new indexes to favor new index instances.

NOTE: I've ignored https://issues.apache.org/jira/browse/CASSANDRA-14207 above

*I've found a few other interleavings by using another machine, but the general issue is the same.","14/Feb/18 17:32;lboutros;[~jrwest], 

??As I am writing this, however, I am realizing we want to keep a reference to the newer index, which references the newer SSTable instance and my patch does the opposite — keeping the old instance. I will post an updated patch along with my trunk patch for internal review, but the gist is to change the order we iterate over the old view and new indexes to favor new index instances.??

That was the point of my initial patch. But I aggree, if we can increment the global ref count with the new index before releasing the old one and therefore prevent the index file deletion, that would be better.","19/Feb/18 23:39;jrwest;[~lboutros]/[~jasobrown], some updates:

 
 I have attached two new patches. One for trunk and one of 3.11. Unfortunately, the test changes in trunk don't work well on 3.11 so we can't have one patch. The primary changes in this patch are to change the order we iterate over the indexes to ensure we retain the newer instance of {{SSTableIndex}} and thus the newer {{SSTableReader}}. I also changed the code to clone the {{oldSSTables}} collection since its visible outside the {{View}} constructor. 
||3.11||Trunk||
|[branch|https://github.com/jrwest/cassandra/tree/14055-jrwest-3.11]|[branch|https://github.com/jrwest/cassandra/tree/14055-jrwest-trunk]|
|[utests|https://circleci.com/gh/jrwest/cassandra/tree/14055-jrwest-3.11]|[utests|https://circleci.com/gh/jrwest/cassandra/tree/14055-jrwest-trunk]|

NOTE: same utests are failing on [trunk|https://circleci.com/gh/jrwest/cassandra/25] and I'm still working on getting dtests running with my CircleCI setup. 

 

Also, I spoke with some colleagues including [~beobal] and [~krummas] about the use of {{sstableRef.globalCount()}} to determine when to delete the SASI index file. I've come to the conclusion that its use at all is wrong because it represents the number of references to the instance, not globally. Given index summary redistribution, this isn't a safe assumption. Looking back at the original SASI patches, I am not sure why it got merged this way. The [patches|https://github.com/xedin/sasi/blob/master/src/java/org/apache/cassandra/db/index/sasi/SSTableIndex.java#L120] used {{sstable.isMarkedCompacted()}} but the [merged code|https://github.com/apache/cassandra/commit/72790dc8e34826b39ac696b03025ae6b7b6beb2b#diff-4873bb6fcef158ff18d221571ef2ec7cR124] used {{sstableRef.globalCount()}}. Fixing this is a larger undertaking, so I propose we split that work into a separate ticket and focus this one on SASI's failure to account for index redistribution in the {{View}}. The work covered by the other ticket would entail either a) deleting the SASI index files as part of {{SSTableTidier}} or by moving {{SSTableIndex}} to use {{Ref}} and implementing a tidier specific to it.","20/Feb/18 08:50;lboutros;[~jrwest],

I understand your changings and your explanations on the reference counting issue.
It looks good to me.

Thank you.",21/Feb/18 17:12;jrwest;[~lboutros] Great! Thanks for taking a look. I've created https://issues.apache.org/jira/browse/CASSANDRA-14248. ,22/Feb/18 17:19;lboutros;[~jrwest] do you have an idea when this will be committed ?,22/Feb/18 18:02;jrwest;[~lboutros] its in [~jasobrown]'s queue to give it one more review but I hope next week.,12/Apr/18 09:25;lboutros;Any news on this ? Thx.,"14/May/18 19:48;jasobrown;Sorry for the delay in getting this reviewed. Patch lgtm, and committed as sha {{ab8348c578c0bb2d3baefaf387b4d9bc67f4c861}}. Thanks, [~lboutros] and [~jrwest]!",18/May/18 11:53;lboutros;Thank you [~jasobrown].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_pep8_compliance - cqlsh_tests.cqlsh_tests.TestCqlsh: pep8 has been renamed to pycodestyle (GitHub issue #466),CASSANDRA-14020,13118675,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mkjellman,mkjellman,mkjellman,15/Nov/17 22:07,12/Mar/19 14:15,13/Mar/19 22:34,15/Jan/18 13:22,,,,,,Legacy/Testing,,,,,0,,,,"test_pep8_compliance - cqlsh_tests.cqlsh_tests.TestCqlsh always fails due to us catching a informative warning from the pip8 tool.. looks like we just need to swap out the usage

/home/cassandra/env/local/lib/python2.7/site-packages/pep8.py:2124: UserWarning: 

pep8 has been renamed to pycodestyle (GitHub issue #466)
Use of the pep8 tool will be removed in a future release.
Please install and use `pycodestyle` instead.

$ pip install pycodestyle
$ pycodestyle ...

  '\n\n'",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14021,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-28 00:53:11.124,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 15 13:22:05 UTC 2018,,,,,,0|i3mu0f:,9223372036854775807,,,,,,,,jay.zhuang,jay.zhuang,,,,,,,,,,15/Nov/17 22:08;mkjellman;https://github.com/mkjellman/cassandra-dtest/commit/0a5a2b1df8fb2d51102822a00ff15bd70f9b4e63,"28/Nov/17 00:53;jay.zhuang;+1 for the change.

But we still need to fix the python style issue:
{noformat}
AssertionError: /Users/zjay/ws/cassandra/bin/cqlsh.py:130:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/bin/cqlsh.py:376:1: E305 expected 2 blank lines after class or function definition, found 0
/Users/zjay/ws/cassandra/bin/cqlsh.py:2369:17: E722 do not use bare except'
/Users/zjay/ws/cassandra/bin/cqlsh.py:2374:17: E722 do not use bare except'
/Users/zjay/ws/cassandra/bin/cqlsh.py:2433:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/copyutil.py:171:13: E722 do not use bare except'
/Users/zjay/ws/cassandra/pylib/cqlshlib/copyutil.py:225:13: E722 do not use bare except'
/Users/zjay/ws/cassandra/pylib/cqlshlib/copyutil.py:2150:17: E741 ambiguous variable name 'l'
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:37:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:146:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:379:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:601:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:611:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:642:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:694:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:854:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:928:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1134:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1153:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1201:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1255:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1318:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1375:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1409:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1558:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/cql3handling.py:1583:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/displaying.py:102:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:48:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:59:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:103:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:211:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:240:1: E305 expected 2 blank lines after class or function definition, found 0
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:262:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:276:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:305:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:315:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:351:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:369:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:479:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:525:1: E305 expected 2 blank lines after class or function definition, found 0
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:551:1: E305 expected 2 blank lines after class or function definition, found 0
/Users/zjay/ws/cassandra/pylib/cqlshlib/formatting.py:584:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/pylexotron.py:521:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/wcwidth.py:130:1: E305 expected 2 blank lines after class or function definition, found 1
/Users/zjay/ws/cassandra/pylib/cqlshlib/wcwidth.py:324:1: E305 expected 2 blank lines after class or function definition, found 1
{noformat}","28/Nov/17 01:08;mkjellman;[~jay.zhuang] my bad, i forgot to link the two jira's... the pycodestyle issue is handled in CASSANDRA-14021 (commit available there)...","28/Nov/17 18:35;jay.zhuang;Thanks [~mkjellman]
Fixing [linter_check.sh|https://github.com/apache/cassandra-dtest/blob/master/linter_check.sh#L10] here: CASSANDRA-14076","15/Jan/18 13:22;jasobrown;committed as sha {{0d468af9e2617a4a9083e1e527e3a1731e613fcc}}

Thanks, [~mkjellman] and [~jay.zhuang]!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_pycodestyle_compliance - cqlsh_tests.cqlsh_tests.TestCqlsh code style errors,CASSANDRA-14021,13118677,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mkjellman,mkjellman,mkjellman,15/Nov/17 22:10,12/Mar/19 14:15,13/Mar/19 22:34,15/Jan/18 14:10,2.1.20,2.2.12,3.0.16,3.11.2,4.0,,,,,,0,cqlsh,,,"Once we commit CASSANDRA-14020, we'll need to cleanup all of the errors that pycodestyle has found to get the test passing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10066,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-28 01:26:11.428,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 15 14:10:51 UTC 2018,,,,,,0|i3mu0v:,9223372036854775807,,,,,,,,jay.zhuang,jay.zhuang,,,,,,,,,,15/Nov/17 22:10;mkjellman;https://github.com/mkjellman/cassandra/commit/4a7ef7b2b8fd7133af418626ada79b2b6696ad85,"28/Nov/17 01:26;jay.zhuang;The change looks good to me. Seems like the patch is for trunk. Would you please apply the change to branches since {{cassandra-2.1}}, as the dTest tests the version since 2.1: https://github.com/apache/cassandra-dtest/blob/master/cqlsh_tests/cqlsh_tests.py#L42","15/Jan/18 14:10;jasobrown;While I'm quite sure we won't release another 2.1, I've backported all the way to 2.1 and every version up to trunk (your welcome, [~mkjellman] :D).

 

committed as sha \{{f8d73a3acb00d807d09aa33e1612c89389b18480}}.

 

Thanks, all!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nodetool cleanup on KS with no replicas should remove old data, not silently complete",CASSANDRA-13526,13071316,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,jjirsa,jjirsa,11/May/17 20:11,12/Mar/19 14:15,13/Mar/19 22:35,07/Dec/17 05:51,3.0.16,3.11.2,4.0,,,Local/Compaction,,,,,0,usability,,,"From the user list:

https://lists.apache.org/thread.html/5d49cc6bbc6fd2e5f8b12f2308a3e24212a55afbb441af5cb8cd4167@%3Cuser.cassandra.apache.org%3E

If you have a multi-dc cluster, but some keyspaces not replicated to a given DC, you'll be unable to run cleanup on those keyspaces in that DC, because [the cleanup code will see no ranges and exit early|https://github.com/apache/cassandra/blob/4cfaf85/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L427-L441]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-9652,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-05-11 22:34:07.549,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 07 13:40:17 UTC 2017,,,,,,0|i3ev0f:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"11/May/17 22:34;jaid;The issue I am seeing on C* cluster with the below setup

Cassandra version : 2.1.16
Datacenters: 4 DC
RF: NetworkTopologyStrategy with 3 RF in each DC
Keyspaces: 50 keyspaces, few replicating to one DC and few replicating to multiple DC

","06/Jul/17 04:45;jasonstack;| branch | unit | [dtest|https://github.com/jasonstack/cassandra-dtest/commits/CASSANDRA-13526] |
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526] |  [pass|https://circleci.com/gh/jasonstack/cassandra/182] | bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test known |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-3.11]|  [pass|https://circleci.com/gh/jasonstack/cassandra/186] | pass |
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-3.0]|  [pass|https://circleci.com/gh/jasonstack/cassandra/181] | offline_tools_test.TestOfflineTools.sstableofflinerelevel_test  auth_test.TestAuth.system_auth_ks_is_alterable_test |
| [2.2|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13526-2.2]|  [pass|https://circleci.com/gh/jasonstack/cassandra/185] |  ttl_test.TestTTL.collection_list_ttl_test |

unit test all passed, some irrelevant dtests failed.

when no local range && node has joined token ring,  clean up will remove all base local sstables.  ",12/Jul/17 02:27;jasonstack;[~jjirsa] could you review ? thanks..,"16/Jul/17 06:59;jjirsa;Thanks [~jasonstack] .  I glanced at it and it looked reasonable, though I'll do a more thorough review next week.

Since it's a bug fix (and a pretty serious one at that), it seems like we should have patches for at least 3.0 and 3.11 , and perhaps even 2.1 and 2.2. Are you able to port your fix to 3.0 and 3.11? 

We should also add a unit test to make sure we prevent this sort of regression again in the future.



",16/Jul/17 07:06;jasonstack;[~jjirsa] thanks for reviewing. {{trunk}} was draft for review. I will prepare for older branches and more tests.,"16/Jul/17 07:24;jjirsa;[~jasonstack] if you give me a few days I'll do a real review, and you can backport after that if it's easier
",16/Jul/17 07:42;jasonstack;sure. it's not urgent.,"19/Jul/17 04:59;jjirsa;Patch looks good to me, dtest looks good as well, with two comments:

1) New dtest repo is https://github.com/apache/cassandra-dtest

2) You should remove dc1 from the replication strategy [here|https://github.com/riptano/cassandra-dtest/commit/15bf712988fb50ae29994da246dec186beff69bd#diff-9d7bd37d410a5598b9700b71476845ebR159] to be very explicit about what we expect to happen.

Would you backport to 3.0 and 3.11 ? 
","19/Jul/17 14:54;jasonstack;thanks for reviewing, I will back port to 3.0/3.11 this week.  I was stuck in other issues..","20/Jul/17 03:24;githubbot;GitHub user jasonstack opened a pull request:

    https://github.com/apache/cassandra-dtest/pull/1

    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remov…

    JIRA: https://issues.apache.org/jira/browse/CASSANDRA-13526   pending for 2.2/3.0/3.11

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jasonstack/cassandra-dtest-1 CASSANDRA-13526

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/1.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1
    
----
commit 3c8877c0fa3eb998ed2ee9945ebb8d43687e65fa
Author: Zhao Yang <zhaoyangsingapore@gmail.com>
Date:   2017-07-20T03:18:18Z

    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remove old data, not silently complete

----
","20/Jul/17 05:41;githubbot;Github user jasonstack closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/1
","24/Jul/17 06:32;jasonstack;[~jjirsa] sorry for the delay, I updated the dtest result for 2.2/3.0/3.11/trunk, some irrelevant dtests failed. I skipped 2.1 since this is not critical.","28/Jul/17 16:37;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra-dtest/pull/1
  
    (You could leave the PR open and I'll close it on merge with CASSANDRA-13526 )

","29/Jul/17 03:11;githubbot;Github user jasonstack commented on the issue:

    https://github.com/apache/cassandra-dtest/pull/1
  
    thanks..
","29/Jul/17 03:11;githubbot;GitHub user jasonstack reopened a pull request:

    https://github.com/apache/cassandra-dtest/pull/1

    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remov…

    JIRA: https://issues.apache.org/jira/browse/CASSANDRA-13526   pending for 2.2/3.0/3.11

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jasonstack/cassandra-dtest CASSANDRA-13526

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/1.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1
    
----
commit ccb6e81451f3d9ca0d192c508beaeef0959e56fc
Author: Zhao Yang <zhaoyangsingapore@gmail.com>
Date:   2017-07-20T03:18:18Z

    CASSANDRA-13526: nodetool cleanup on KS with no replicas should remove old data, not silently complete

----
","20/Oct/17 23:41;jjirsa;Hi [~jasonstack] Really appreciate your patience in the time it's taken me to back to this. I hope to review it this weekend.

[~krummas] / [~iamaleksey] - what do you folks think about versions here? 2.2 or 3.0? 

","04/Dec/17 21:35;iamaleksey;I'd probably go with 3.0+ only, but 2.2 is acceptable too.","04/Dec/17 23:03;jjirsa;I've rebased your patch and I'm re-running CI, just because it took me so very long to review this patch.

Generally the patches look fine, but I don't understand why you're running this method twice [here|https://github.com/jasonstack/cassandra/commit/b51c46565adf0d765ac6ded831469a2eca2939d8#diff-ba6d3d8e296151fc283ef11ac4594e62R211] (and in the very similar helper below it)?

I'm inclined to remove one of those calls. Other than that, marking as ready-to-commit, and I'll merge when CI finishes.
",06/Dec/17 10:06;jasonstack;[~jjirsa] it's a mistake in 3.11 PR.. thanks for the fix.,"07/Dec/17 05:51;jjirsa;Thank you so much for the patch and your patience. Committed to 3.0 as {{090f418831be4e4dace861fda380ee4ec27cec35}} and merged up, fixing the 3.11 test on the way.

","07/Dec/17 06:04;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/1
",07/Dec/17 13:40;jasonstack;Thanks for reviewing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool scrub/cleanup/upgradesstables exit code,CASSANDRA-13542,13073772,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,22/May/17 06:28,12/Mar/19 14:15,13/Mar/19 22:35,02/Jun/17 13:53,3.0.14,3.11.0,4.0,,,,,,,,0,,,,We exit nodetool with success if we fail marking sstables as compacting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-05-22 16:20:55.484,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 02 13:53:55 UTC 2017,,,,,,0|i3fa4n:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"22/May/17 06:36;krummas;https://github.com/krummas/cassandra/tree/marcuse/exitcode
https://github.com/krummas/cassandra/tree/marcuse/exitcode-3.11
https://github.com/krummas/cassandra/tree/marcuse/exitcode-trunk","22/May/17 16:20;jjirsa;+1
","02/Jun/17 13:53;krummas;committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null Pointer exception at SELECT JSON statement,CASSANDRA-13592,13079207,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,panibus,panibus,12/Jun/17 13:33,12/Mar/19 14:15,13/Mar/19 22:35,07/Jul/17 16:17,2.2.11,3.0.15,3.11.1,4.0,,Legacy/CQL,,,,,0,beginner,,,"A Nulll pointer exception appears when the command

{code}
SELECT JSON * FROM examples.basic;

---MORE---
<Error from server: code=0000 [Server error] message=""java.lang.NullPointerException"">

Examples.basic has the following description (DESC examples.basic;):
CREATE TABLE examples.basic (
    key frozen<tuple<uuid, int>> PRIMARY KEY,
    wert text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}

The error appears after the ---MORE--- line.

The field ""wert"" has a JSON formatted string.",Debian Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Jun/17 13:33;panibus;system.log;https://issues.apache.org/jira/secure/attachment/12872697/system.log,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-06-27 07:39:44.105,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 03 16:51:45 UTC 2017,,,,,,0|i3g613:,9223372036854775807,3.10,,,,,,,blerer,blerer,,,,,,,,,,"27/Jun/17 07:39;jasonstack;In Json path, the bytebuffer is being consumed with position = capacity. So in page-state, no partition key bytes are written.  

Using bf.duplicate() would fix this issue.

{code}
    public static List<ByteBuffer> rowToJson(List<ByteBuffer> row, ProtocolVersion protocolVersion, ResultSet.ResultMetadata metadata)
    {
        StringBuilder sb = new StringBuilder(""{"");
        for (int i = 0; i < metadata.names.size(); i++)
        {
            if (i > 0)
                sb.append("", "");

            ColumnSpecification spec = metadata.names.get(i);
            String columnName = spec.name.toString();
            if (!columnName.equals(columnName.toLowerCase(Locale.US)))
                columnName = ""\"""" + columnName + ""\"""";

            ByteBuffer buffer = row.get(i);
            sb.append('""');
            sb.append(Json.quoteAsJsonString(columnName));
            sb.append(""\"": "");
            if (buffer == null)
                sb.append(""null"");
            else
                // use duplicate() to avoid buffer being consumed
                sb.append(spec.type.toJSONString(buffer.duplicate(), protocolVersion));
        }
        sb.append(""}"");
        return Collections.singletonList(UTF8Type.instance.getSerializer().serialize(sb.toString()));
    }
{code}","28/Jun/17 04:03;jasonstack;|| source || junit-result || dtest-result||
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592] | [https://circleci.com/gh/jasonstack/cassandra/65] | | 

let the user of 'type.toJSONString()' to handle BB position changes.","28/Jun/17 16:16;blerer;Thanks for the patch.

The {{pagingOnToJsonQuery()}} test is passing even without the change to {{Selection::rowToJson}}. After looking into it seems that the problem is in fact in {{TupleType::toJSONString}}. The method is actually moving the buffer position while it should not. Which is why simple types like {{int}} or {{text}} are not affected by the problem.
I guess that we might have similar problems with the {{list}}, {{set}}, {{map}} or UDT types. So, it will be nice to add some extra tests for those type as well.

I would put the tests in {{JsonTest}} instead of in {{PagingQueryTest}}.

The {{json}} support has been introduced in 2.2 so we will also need some patches for 2.2, 3.0, 3.11. ","29/Jun/17 05:01;jasonstack;Thanks for reviewing.. I will update the 2.0/3.0/3.11 branch when trunk CI finishes.

""list , set, map or UDT types"" are not allowed in key, ideally it shouldn't cause issues. I think only key buffer will be reused eg. paging-state, subsequent rows deserialization. 
I have included them in the test.

One more issue is in ToJsonFct.

{code}
    public ByteBuffer execute(ProtocolVersion protocolVersion, List<ByteBuffer> parameters) throws InvalidRequestException
    {
        assert parameters.size() == 1 : ""Expected 1 argument for toJson(), but got "" + parameters.size();
        ByteBuffer parameter = parameters.get(0);
        if (parameter == null)
            return ByteBufferUtil.bytes(""null"");
        // same..
        return ByteBufferUtil.bytes(argTypes.get(0).toJSONString(parameter.duplicate(), protocolVersion));
    }
{code}","29/Jun/17 08:26;blerer;{quote}""list , set, map or UDT types"" are not allowed in key{quote}
They are, as long as they are {{frozen}}.

Sorry, it seems that I was not clear. Fixing the problem in {{rowToJson}} or in the {{ToJsonFct}} is in my opinion wrong. Most of the types will not change the buffer position in the {{toJSONString}} method and I will argue that it is the correct behavior. The  {{toJSONString}} methods should not change the buffer position.
If you fix it at a higher level, like in the {{rowToJson}} method or in the {{ToJsonFct}} class, you let a potential bug in the code that can hit us later if we add some new code that use the {{toJSONString}} method.
Moreover, your changes create in most of the cases some unnecessary objects.

The proper way to fix that problem is to fix the {{toJSONString}} methods that changes the buffer position which seems to be the one of: {{TupleType}}, {{MapType}}, {{ListType}} and {{SetType}}.   ","29/Jun/17 14:28;jasonstack;Thanks.. Yes, it's better solve it within {{type.toJSONString}}

I will add test the specification of {{type.toJSONString}}.  all should not change buffer position.

Got a question about {{DurationType.toJSONString()}}, imo, it should return string value with double quote {{""-2h9m""}}, like {{TimeType.toJSONString()}}.   but it only returns string value {{-2h9m}} .  Is it expected?  it would be different from user's json input. 

One more thing is about: EmptyType. it should directly {{return ""\""\"""";}} instead of using parent method which causes null-pointer exception.  EmptyType seems not used for json-path, but good to keep it safe.","30/Jun/17 06:18;jasonstack;|| source || junit-result || dtest-result||
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592] | [junit|https://circleci.com/gh/jasonstack/cassandra/84]  | {{cql_tests.py:SlowQueryTester.local_query_test}} failed on trunk
{{cql_tests.py:SlowQueryTester.remote_query_test}} failed on trunk
{{bootstrap_test.TestBootstrap.simultaneous_bootstrap_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13506]
| 
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-3.11] |  [junit|https://circleci.com/gh/jasonstack/cassandra/82] | {{topology_test.TestTopology.size_estimates_multidc_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13229]
{{cqlsh_tests.cqlsh_tests.TestCqlsh.test_describe}} [known|https://issues.apache.org/jira/browse/CASSANDRA-13250] | 
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-3.0] |  [junit|https://circleci.com/gh/jasonstack/cassandra/83] | {{auth_test.TestAuth.system_auth_ks_is_alterable_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13113]
{{offline_tools_test.TestOfflineTools.sstableofflinerelevel_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-12617]
{{repair_tests.incremental_repair_test.TestIncRepair.multiple_repair_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13515]| 
| [2.2|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-2.2] |  [junit|https://circleci.com/gh/jasonstack/cassandra/85] | passed | 

1. in {{listType, mapType, setType, TupleType}}.toJSONString(), keep buffer position the same.
2. change {{DurationType}}.toJSONString() to {{return ""\"""" +.... +""\"""";}} (with double-quote) to be consistent with user json input
3. change {{EmptyType}}.toJSONString() to directly {{return ""\""\"""";}}, otherwise parent method throws NPE.",30/Jun/17 10:18;jasonstack;I have created [ticket|https://issues.apache.org/jira/browse/CASSANDRA-13650] for {{cql_tests.py:SlowQueryTester.local_query_test}} & {{cql_tests.py:SlowQueryTester.remote_query_test}},"03/Jul/17 13:36;blerer;The patch looks mostly good. I just have the following nits:
* The unit test: {{JsonTest::testPagingWithJsonQuery}} is hard to read. It seems to me that the loop does not bring much and makes the code harder to understand. I would remove it, even if it results in a longer method.
* In the same method you should check the output and not only the number of rows. The returned values could be invalid even if the number of returned rows is the good one.
* I would replace {{/** Converts a value to a JSON string. buffer position not changed */}} by something like:
{code}
/** 
 * Converts the specified value into its JSON representation. 
 * <p>The buffer position will stay the same.</p>
* @param buffer the value to convert
* @param protocolVersion the protocol version to use for the conversion
* @return  a JSON string representing the specified value
 */
{code}
","04/Jul/17 03:06;jasonstack;|| source || junit-result || dtest-result||
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592] | [junit|https://circleci.com/gh/jasonstack/cassandra/102]  | {{cql_tests.py:SlowQueryTester.local_query_test}} {{cql_tests.py:SlowQueryTester.remote_query_test}} [known|https://issues.apache.org/jira/browse/CASSANDRA-13592]
{{bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13576]
| 
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-3.11] |  [junit|https://circleci.com/gh/jasonstack/cassandra/99] | {{topology_test.TestTopology.size_estimates_multidc_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-13229]
{{cqlsh_tests.cqlsh_tests.TestCqlsh.test_describe}} [known|https://issues.apache.org/jira/browse/CASSANDRA-13250]| 
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-3.0] |  [junit|https://circleci.com/gh/jasonstack/cassandra/100] | {{offline_tools_test.TestOfflineTools.sstableofflinerelevel_test}}[known|https://issues.apache.org/jira/browse/CASSANDRA-12617] 
{{cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_with_single_cor}} [known|https://issues.apache.org/jira/browse/CASSANDRA-13244] | 
| [2.2|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13592-cassandra-2.2] |  [junit|https://circleci.com/gh/jasonstack/cassandra/97] | passed | 


Updated java doc and split the test cases. ","07/Jul/17 16:15;blerer;Thanks for the changes. 
The paging tests were not using paging. I fixed that before committing.","07/Jul/17 16:17;blerer;Committed into 2.2 at cb6fad3efcd7cd3dc87d02ca7e8e97eb277a66ab and merged into 3.0, 3.11 and trunk.","02/Nov/17 22:50;mildebrandt;I'm getting almost exactly the same stacktrace using Cassandra 3.11.1:
{noformat}
java.lang.NullPointerException: null
        at org.apache.cassandra.dht.Murmur3Partitioner.getHash(Murmur3Partitioner.java:230) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.dht.Murmur3Partitioner.decorateKey(Murmur3Partitioner.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.config.CFMetaData.decorateKey(CFMetaData.java:627) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.service.pager.PartitionRangeQueryPager.<init>(PartitionRangeQueryPager.java:44) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.db.PartitionRangeReadCommand.getPager(PartitionRangeReadCommand.java:268) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.statements.SelectStatement.getPager(SelectStatement.java:475) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:288) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:118) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:530) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:507) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:146) ~[apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.1.jar:3.11.1]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.1.jar:3.11.1]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
{noformat}

It can be recreated with this project:
https://github.com/eyeofthefrog/CASSANDRA-13592

I think it's the same root cause, but let me know if I should open another issue. ","03/Nov/17 08:42;blerer;It is a different issue. Could you open another ticket?
","03/Nov/17 16:51;mildebrandt;Sure, https://issues.apache.org/jira/browse/CASSANDRA-13991
Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a workaround for overly large read repair mutations,CASSANDRA-13975,13112044,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,25/Oct/17 18:09,12/Mar/19 14:15,13/Mar/19 22:35,13/Nov/17 13:21,3.0.16,3.11.2,,,,Legacy/Coordination,,,,,0,,,,"It's currently possible for {{DataResolver}} to accumulate more changes to read repair that would fit in a single serialized mutation. If that happens, the node receiving the mutation would fail, and the read would time out, and won't be able to proceed until the operator runs repair or manually drops the affected partitions.

Ideally we should either read repair iteratively, or at least split the resulting mutation into smaller chunks in the end. In the meantime, for 3.0.x, I suggest we add logging to catch this, and a -D flag to allow proceeding with the requests as is when the mutation is too large, without read repair.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-09 11:46:19.161,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 13 13:21:30 UTC 2017,,,,,,0|i3lp6f:,9223372036854775807,,,,,,,,beobal,beobal,,,,,,,,,,"02/Nov/17 15:44;iamaleksey;A straight-forward change pushed [here|https://github.com/iamaleksey/cassandra/commits/13975-3.0]. Unit test run [here|https://circleci.com/gh/iamaleksey/cassandra/63], dtest run [here|https://builds.apache.org/job/Cassandra-devbranch-dtest/407/].

","09/Nov/17 11:46;beobal;LGTM, modulo an exceeding minor nit about inconsistent whitespace at the top of {{sendRepairMutation}}, feel free to fix on commit.","13/Nov/17 13:21;iamaleksey;Thanks, committed as [f1e850a492126572efc636a6838cff90333806b9|https://github.com/apache/cassandra/commit/f1e850a492126572efc636a6838cff90333806b9] to 3.0 and merged up with 3.11 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Acquire read lock before accessing CompactionStrategyManager fields,CASSANDRA-14139,13127391,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,27/Dec/17 17:46,12/Mar/19 14:15,13/Mar/19 22:35,10/Jan/18 20:44,3.11.2,4.0,,,,,,,,,0,,,,"There are a few methods in {{CompactionStrategyManager}} accessing the repaired/unrepaired compaction strategy lists without using the read lock, what could cause issues like the one below:

{noformat}
ERROR [CompactionExecutor:1] 2017-12-22 12:17:12,320 CassandraDaemon.java:141 - Exception in thread Thread[CompactionExecutor:1,5,main]
java.lang.IndexOutOfBoundsException: Index: 0, Size: 1
    at java.util.ArrayList.rangeCheck(ArrayList.java:657)
    at java.util.ArrayList.get(ArrayList.java:433)
    at org.apache.cassandra.db.compaction.CompactionStrategyManager.supportsEarlyOpen(CompactionStrategyManager.java:1262)
    at org.apache.cassandra.db.ColumnFamilyStore.supportsEarlyOpen(ColumnFamilyStore.java:558)
    at org.apache.cassandra.io.sstable.SSTableRewriter.construct(SSTableRewriter.java:119)
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.<init>(CompactionAwareWriter.java:91)
    at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.<init>(DefaultCompactionWriter.java:57)
    at org.apache.cassandra.db.compaction.CompactionTask.getCompactionAwareWriter(CompactionTask.java:293)
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:200)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:90)
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:101)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:310)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
    at java.lang.Thread.run(Thread.java:748)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14150,,,,,,,,,29/Dec/17 18:20;pauloricardomg;3.11-14139-dtest.png;https://issues.apache.org/jira/secure/attachment/12904035/3.11-14139-dtest.png,29/Dec/17 18:20;pauloricardomg;3.11-14139-testall.png;https://issues.apache.org/jira/secure/attachment/12904036/3.11-14139-testall.png,29/Dec/17 18:20;pauloricardomg;trunk-14139-dtest.png;https://issues.apache.org/jira/secure/attachment/12904037/trunk-14139-dtest.png,29/Dec/17 18:20;pauloricardomg;trunk-14139-testall.png;https://issues.apache.org/jira/secure/attachment/12904038/trunk-14139-testall.png,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2018-01-06 17:58:46.387,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 10 20:44:08 UTC 2018,,,,,,0|i3objr:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"29/Dec/17 18:26;pauloricardomg;It seems we missed grabbing the read lock for some operations on CASSANDRA-13948, what can cause IndexOutOfBounds/NullPointers such the ones above during compaction strategy reload, so this patch basically gets the lock on the few methods were not doing it.

CI looks good. Trunk patch is slightly different because there are some additional methods due to CASSANDRA-9143. Can you take a look [~krummas]? Thanks!

||3.11||trunk||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-14139]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-14139]|
|[testall|https://issues.apache.org/jira/secure/attachment/12904036/3.11-14139-testall.png]|[testall|https://issues.apache.org/jira/secure/attachment/12904038/trunk-14139-testall.png]|
|[dtest|https://issues.apache.org/jira/secure/attachment/12904035/3.11-14139-dtest.png]|[dtest|https://issues.apache.org/jira/secure/attachment/12904037/trunk-14139-dtest.png]|
","06/Jan/18 17:58;krummas;* for the {{supportsEarlyOpen}} method we can instead store the boolean in {{startup}} like we do with {{fanout}} and {{shouldDefragment}}
* in trunk, for {{getRepaired}}, {{getUnrepaired}} and {{getPendingRepairManagers}} we don't need the readlock - we return the list directly (and when refreshing we clear the same list) - to make this safe we would need to copy the list and return the copy, but those methods are only used in tests so I doubt we need it (maybe add a comment though)
","10/Jan/18 13:16;pauloricardomg;Thanks for the review! See follow-up below:

bq. for the supportsEarlyOpen method we can instead store the boolean in startup like we do with fanout and shouldDefragment

good catch, fixed!

bq. in trunk, for getRepaired, getUnrepaired and getPendingRepairManagers we don't need the readlock - we return the list directly (and when refreshing we clear the same list) - to make this safe we would need to copy the list and return the copy, but those methods are only used in tests so I doubt we need it (maybe add a comment though)

even though this is only used in test, for consistency I opted for copying the list and returning a copying, so it is safe in case anybody uses it outside testing.

Updated patch with fixes above, CI looks good. Please let me know what do you think.","10/Jan/18 13:38;krummas;lgtm, +1",10/Jan/18 20:44;pauloricardomg;Committed as {{fe0ee85c71faada0acb48a65f249575c65bf0972}} to cassandra-3.11 and merged up to master. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ViewComplexTest broken in trunk,CASSANDRA-14230,13138050,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,KurtG,KurtG,KurtG,13/Feb/18 03:08,12/Mar/19 14:15,13/Mar/19 22:35,14/Feb/18 00:00,,,,,,,,,,,0,,,,"testUpdateWithColumnTimestampBiggerThanPkWithFlush
testUpdateWithColumnTimestampBiggerThanPkWithoutFlush
testUpdateColumnNotInViewWithFlush
testUpdateColumnNotInViewWithoutFlush

All fail with:
{code}
    [junit] junit.framework.AssertionFailedError: Expected error message to contain 'Cannot drop column v2 on base table with materialized views', but got 'Cannot drop column v2 on base table table_20 with materialized views.'
{code}
Because the error message changed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14219,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-14 00:00:52.976,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 14 00:00:52 UTC 2018,,,,,,0|i3q3qn:,9223372036854775807,,,,,,,,,,,,,,,,,,,13/Feb/18 03:51;KurtG;|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14230-3.11]|[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:14230-trunk]|,"14/Feb/18 00:00;jasobrown;Thanks for the patch, [~KurtG], but I think we beat you to it ;)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogStressTest timeout in 3.11,CASSANDRA-14143,13127569,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,28/Dec/17 19:20,12/Mar/19 14:15,13/Mar/19 22:35,10/Jan/18 14:48,3.11.2,,,,,Legacy/Testing,,,,,0,testing,,,"[~jasobrown] fixed the CommitLogStressTest timeout issue as part of CASSANDRA-13530, but it's only in trunk, it would be better to backport the unittest change to 3.11.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-10 14:48:46.527,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 10 14:48:46 UTC 2018,,,,,,0|i3ocnb:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"28/Dec/17 21:25;jay.zhuang;Backport unittest test fix from trunk (CASSANDRA-13530), please review:
| Branch | uTest |
| [14143-3.11|https://github.com/cooldoger/cassandra/tree/14143-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14143-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14143-3.11] |","10/Jan/18 14:48;jasobrown;+1, and committed as sha {{62e46f71903b339d962c4dcb3d2c04991c391a68}} to 3.11 only

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change to AlterTableStatement logging breaks MView tests,CASSANDRA-14219,13136837,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,djoshi3,jasobrown,jasobrown,07/Feb/18 12:41,12/Mar/19 14:15,13/Mar/19 22:35,13/Feb/18 23:42,3.0.16,3.11.2,4.0,,,,,,,,0,,,,"looks like [~dbrosius]'s ninja commit {{7df36056b12a13b60097b7a9a4f8155a1d02ff62}} to improve the logging of {{AlterTableStatement}} breaks some MView tests that check the exception message. I see about six failed tests from {{ViewComplexTest}} that have messages similar to this:
{noformat}
junit.framework.AssertionFailedError: Expected error message to contain 'Cannot drop column m on base table with materialized views', but got 'Cannot drop column m on base table table_6 with materialized views.'{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-09 23:59:11.898,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 14 00:07:58 UTC 2018,,,,,,0|i3pw93:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"09/Feb/18 23:59;djoshi3;||14219||
|[branch|https://github.com/dineshjoshi/cassandra/tree/CASSANDRA-14219]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/CASSANDRA-14219]|
||","13/Feb/18 23:26;KurtG;Missed this ticket and accidentally created a duplicate and did the same patch yesterday CASSANDRA-14230.... It's also broken on trunk FTR.

 

In regards to patch, you can just use currentTable() to get the CF name.","13/Feb/18 23:42;jasobrown;+1.Thanks, [~djoshi3]. Committed as sha {{890f319142ddd3cf2692ff45ff28e71001365e96}}

For posterity here's my ports to 3.0/3.11/trunk and the test runs; {{ViewComplexTest}} no longer fails.

||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/3.0.16-candidate]|[branch|https://github.com/jasobrown/cassandra/tree/3.11.2-candidate]|[branch|https://github.com/jasobrown/cassandra/tree/14291-trunk]|
|[utests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/3.0.16-candidate]|[utests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/3.11.2-candidate]|[utests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14291-trunk]|
||
",14/Feb/18 00:07;djoshi3;[~KurtG] I originally used \{{currentTable()}} but then followed the pattern you had in other commits.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ArrayIndexOutOfBoundsException while executing query,CASSANDRA-13949,13108547,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,lrodriguez2002cu,lrodriguez2002cu,11/Oct/17 10:03,12/Mar/19 14:15,13/Mar/19 22:35,17/Oct/17 13:16,3.11.2,4.0,,,,Legacy/CQL,,,,,0,,,,"While executing a query on a  table contaninig a field with a (escaped) json, the following exception occurs:

java.lang.ArrayIndexOutOfBoundsException: null
        at org.codehaus.jackson.io.JsonStringEncoder.quoteAsString(JsonStringEncoder.java:141) ~[jackson-core-asl-1.9.2.jar:1.9.2]
        at org.apache.cassandra.cql3.Json.quoteAsJsonString(Json.java:45) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.db.marshal.UTF8Type.toJSONString(UTF8Type.java:66) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.selection.Selection.rowToJson(Selection.java:291) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.getOutputRow(Selection.java:431) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.build(Selection.java:417) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:763) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:400) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:378) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:251) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:79) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:217) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:248) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:233) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.0.jar:3.11.0]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.0.jar:3.11.0]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]

Find attached the schema of the table, the insertion query with the data provoking the failure, and the failing query.
 ",Setup of 3 servers y using docker image [https://github.com/docker-library/cassandra/blob/ca3c9df03cab318d34377bba0610c741253b0466/3.11/Dockerfile],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Oct/17 13:01;jasobrown;13949.png;https://issues.apache.org/jira/secure/attachment/12891472/13949.png,11/Oct/17 09:56;lrodriguez2002cu;insert.cql;https://issues.apache.org/jira/secure/attachment/12891451/insert.cql,11/Oct/17 09:59;lrodriguez2002cu;query.cql;https://issues.apache.org/jira/secure/attachment/12891450/query.cql,11/Oct/17 10:02;lrodriguez2002cu;schema.cql;https://issues.apache.org/jira/secure/attachment/12891449/schema.cql,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2017-10-11 13:07:55.677,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 17 13:16:41 UTC 2017,,,,,,0|i3l4jr:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"11/Oct/17 13:07;jasobrown;I tried to reproduce using your attached scripts (very handy, thanks!) on a local running instance of casandra, but was unable to reproduce the {{NullPointerException}}. I did however, get a very curious output in the {{value}} field: at four point in the large json blob, there are a few Kb of printed null characters {{\x00}}. See the attached image. I hexdump'ed your insert.cql and it looked legit (no stray characters).

I then checked our json libs, and they are quite old - jackson*-1.9.2, committed in 2011! The latest jackson jars are version 1.9.13 on maven central (from 2013). I naively replaced the jars in lib/, and I didn't get the null character output anymore.

[~lrodriguez2002cu] Can you try simply replacing the jars and see if the NPE stops occurring? If it does, then I'll create a more formal patch, run tests, and so on.","11/Oct/17 15:09;lrodriguez2002cu;Yes [~jasobrown], I will try to replace the jars. I did note those characters because at some point it gave a different error:

""llegal unquoted character ((CTRL-CHAR, code 0)): has to be escaped using backslash to be included in string value at [Source: java.io.StringReader@2de1af30; line: 1, column: 1228]"" but as you said there is not evidence that those are inserted.

I didn't try to replace them because I made a simple project with that version (1.9.2) of jackson, trying the encode function failing in this case, but the problem didn't occurr.  This is  the repository in case you or anyone want check it [https://github.com/lrodriguez2002cu/cassandra-issue-tests].

What I will try to setup the an environment  with a docker image and map the libs to a volume so  that the libraries can be replaced easily, and see if this fixes  the problem and maybe see possible impacts  in other parts. ","13/Oct/17 14:03;lrodriguez2002cu;Hi [~jasobrown] I have tested as promised an image with the libraries replaced using a newer (still old 1.9.13) version. You mentioned you did not get the error, It happens if you run again the query requesting the json. With the new libraries version, the issue apparently gets solved, in this repository [https://github.com/lrodriguez2002cu/cassandra-issue-images] I have created docker images with the cql files copied inside and the commands for initializing the database and so on, so that you can see the behavior. 

Thanks for the follow up.

","13/Oct/17 20:28;jasobrown;bq. still old 1.9.13

According to maven central, [1.9.13 is the most current version|http://search.maven.org/#search%7Cga%7C1%7Corg.codehaus.jackson] of jackson.

bq. It happens if you run again the query requesting the json 

I did run it a bunch of times, but if the updated jackson is working for you, let's just move ahead on that. Patch coming shortly.","13/Oct/17 21:08;jasobrown;I've created a simple patch which just updates the jackson jars:

||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13949-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13949-trunk]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/370/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/371/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13949-3.11]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13949-trunk]|
",14/Oct/17 11:03;lrodriguez2002cu;Thank you very much!,"16/Oct/17 20:24;jjirsa;3.11 patch looks good to me
4.0 patch is obviously the same, though the dtests look pretty messy. Can you run that again.

+1 if / once that trunk dtest comes back clean.

","17/Oct/17 13:07;jasobrown;I ran the [dtests again|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/374] for trunk, and the results are still a bitt messy. I compared with the trunk dtests on apache jenkins and cassci, and they are also having problems on the paging_tests and write_failure_tests. Thus I think this patch is probably safe to apply. Committing shortly.","17/Oct/17 13:16;jasobrown;committed as sha {{bbda20155ae0f3443cbb5fee0659234e81b2e914}}. 

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SStable ordering by max timestamp in SinglePartitionReadCommand,CASSANDRA-14010,13117962,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,jonathan.pellby,jonathan.pellby,13/Nov/17 15:11,12/Mar/19 14:15,13/Mar/19 22:35,07/Dec/17 22:14,3.0.16,3.11.2,4.0,,,Legacy/Local Write-Read Paths,,,,,0,correctness,,,"We have a test environment were we drop and create keyspaces and tables several times within a short time frame. Since upgrading from 3.11.0 to 3.11.1, we are seeing a lot of create statements failing. See the logs below:
{code:java}
2017-11-13T14:29:20.037986449Z WARN Directory /tmp/ramdisk/commitlog doesn't exist
2017-11-13T14:29:20.038009590Z WARN Directory /tmp/ramdisk/saved_caches doesn't exist
2017-11-13T14:29:20.094337265Z INFO Initialized prepared statement caches with 10 MB (native) and 10 MB (Thrift)
2017-11-13T14:29:20.805946340Z INFO Initializing system.IndexInfo
2017-11-13T14:29:21.934686905Z INFO Initializing system.batches
2017-11-13T14:29:21.973914733Z INFO Initializing system.paxos
2017-11-13T14:29:21.994550268Z INFO Initializing system.local
2017-11-13T14:29:22.014097194Z INFO Initializing system.peers
2017-11-13T14:29:22.124211254Z INFO Initializing system.peer_events
2017-11-13T14:29:22.153966833Z INFO Initializing system.range_xfers
2017-11-13T14:29:22.174097334Z INFO Initializing system.compaction_history
2017-11-13T14:29:22.194259920Z INFO Initializing system.sstable_activity
2017-11-13T14:29:22.210178271Z INFO Initializing system.size_estimates
2017-11-13T14:29:22.223836992Z INFO Initializing system.available_ranges
2017-11-13T14:29:22.237854207Z INFO Initializing system.transferred_ranges
2017-11-13T14:29:22.253995621Z INFO Initializing system.views_builds_in_progress
2017-11-13T14:29:22.264052481Z INFO Initializing system.built_views
2017-11-13T14:29:22.283334779Z INFO Initializing system.hints
2017-11-13T14:29:22.304110311Z INFO Initializing system.batchlog
2017-11-13T14:29:22.318031950Z INFO Initializing system.prepared_statements
2017-11-13T14:29:22.326547917Z INFO Initializing system.schema_keyspaces
2017-11-13T14:29:22.337097407Z INFO Initializing system.schema_columnfamilies
2017-11-13T14:29:22.354082675Z INFO Initializing system.schema_columns
2017-11-13T14:29:22.384179063Z INFO Initializing system.schema_triggers
2017-11-13T14:29:22.394222027Z INFO Initializing system.schema_usertypes
2017-11-13T14:29:22.414199833Z INFO Initializing system.schema_functions
2017-11-13T14:29:22.427205182Z INFO Initializing system.schema_aggregates
2017-11-13T14:29:22.427228345Z INFO Not submitting build tasks for views in keyspace system as storage service is not initialized
2017-11-13T14:29:22.652838866Z INFO Scheduling approximate time-check task with a precision of 10 milliseconds
2017-11-13T14:29:22.732862906Z INFO Initializing system_schema.keyspaces
2017-11-13T14:29:22.746598744Z INFO Initializing system_schema.tables
2017-11-13T14:29:22.759649011Z INFO Initializing system_schema.columns
2017-11-13T14:29:22.766245435Z INFO Initializing system_schema.triggers
2017-11-13T14:29:22.778716809Z INFO Initializing system_schema.dropped_columns
2017-11-13T14:29:22.791369819Z INFO Initializing system_schema.views
2017-11-13T14:29:22.839141724Z INFO Initializing system_schema.types
2017-11-13T14:29:22.852911976Z INFO Initializing system_schema.functions
2017-11-13T14:29:22.852938112Z INFO Initializing system_schema.aggregates
2017-11-13T14:29:22.869348526Z INFO Initializing system_schema.indexes
2017-11-13T14:29:22.874178682Z INFO Not submitting build tasks for views in keyspace system_schema as storage service is not initialized
2017-11-13T14:29:23.700250435Z INFO Initializing key cache with capacity of 25 MBs.
2017-11-13T14:29:23.724357053Z INFO Initializing row cache with capacity of 0 MBs
2017-11-13T14:29:23.724383599Z INFO Initializing counter cache with capacity of 12 MBs
2017-11-13T14:29:23.724386906Z INFO Scheduling counter cache save to every 7200 seconds (going to save all keys).
2017-11-13T14:29:23.984408710Z INFO Populating token metadata from system tables
2017-11-13T14:29:24.032687075Z INFO Global buffer pool is enabled, when pool is exhausted (max is 125.000MiB) it will allocate on heap
2017-11-13T14:29:24.214123695Z INFO Token metadata:
2017-11-13T14:29:24.304218769Z INFO Completed loading (14 ms; 8 keys) KeyCache cache
2017-11-13T14:29:24.363978406Z INFO No commitlog files found; skipping replay
2017-11-13T14:29:24.364005238Z INFO Populating token metadata from system tables
2017-11-13T14:29:24.394408476Z INFO Token metadata:
2017-11-13T14:29:24.709411652Z INFO Preloaded 0 prepared statements
2017-11-13T14:29:24.719332880Z INFO Cassandra version: 3.11.1
2017-11-13T14:29:24.719355969Z INFO Thrift API version: 20.1.0
2017-11-13T14:29:24.719359443Z INFO CQL supported versions: 3.4.4 (default: 3.4.4)
2017-11-13T14:29:24.719362103Z INFO Native protocol supported versions: 3/v3, 4/v4, 5/v5-beta (default: 4/v4)
2017-11-13T14:29:24.766102400Z INFO Initializing index summary manager with a memory pool size of 25 MB and a resize interval of 60 minutes
2017-11-13T14:29:24.778800183Z INFO Starting Messaging Service on /172.17.0.2:7000 (eth0)
2017-11-13T14:29:24.783832188Z WARN No host ID found, created 62452b7c-33ae-40e6-859c-1d7c803aaea8 (Note: This should happen exactly once per node).
2017-11-13T14:29:24.897281778Z INFO Loading persisted ring state
2017-11-13T14:29:24.904217782Z INFO Starting up server gossip
2017-11-13T14:29:25.003802973Z INFO This node will not auto bootstrap because it is configured to be a seed node.
2017-11-13T14:29:25.047674499Z INFO Generated random tokens. tokens are [-6736304773851341012, 3437071596424929702, 4372058337604769145, -306854781937968525, -4419476154597297006, 4339837665480866486, 2052026232731139893, -5761537575805252593, -4477540978357776290, 6263754683045286998, 3670054894619378302, -4326549778810780939, 7187409938161102814, 7030537377703307755, -2757270254308154659, -1953637968902719055, -7235425703069930259, 7123794193321014835, 349308827967095711, 997472983569031481, 992257140226393205, -4045122629441468253, 4149955653388319941, -3690032393349188278, 3528068129562283633, -5057394127379238561, -4944743272177354946, 1371473468273321389, -2771267888257678908, -2379074055482922854, 8800628062632970014, 6016352719444925532, -6458243637210081043, -7131512441131507433, -6135681286390467242, -7886878247827491401, -3964432859204941604, -7124853795154335905, 4536647221115220987, 4518363137218750861, -3945920538919881061, -8569890499152898728, -2228677668104169495, -4004623128783039030, -6849460601197629451, -1787645289665343374, -9004089114738085395, -8444847561386064840, -7719025430480017932, -5020575591450775929, -3535144847803187721, 7252524597471726426, -2582131369519057623, 3737595811793840609, -7248797595897252845, -7065188032269288840, -6731826791431802176, -2970075663731571587, -2619987499373344925, -2698285069650269138, -8589822844420136511, 2658120945314344720, -3710290429036098141, 134530136452862749, 3703742438909992913, 3460544540911930621, 8673891706698173777, 2853177281247015813, 13977464647778584, 2404057737490125388, -6759648287860184451, 744453319830059045, -688104893800828924, 3356383003502762348, 9054641886966810357, 2317130729058165506, -5810663910204725460, 2577132949237273515, 6326216055185945365, 1376570278575995967, 8758101809469842945, -2892126907778256351, -1716283861287440286, 3040640159143123724, 4243935966006505554, -6827972097309863039, 3055912546894309570, -3992773844369808712, -4717007910267923035, -846198401308205724, -3924870907185309086, 1746803312676010060, 6821355560067598541, -5786385588878319458, 3085551110635941848, 7832310180114101987, -9149254679798945822, 3124836728424468300, -100875121723899324, -7606007094353527325, 270256410769436649, -3016541299722946307, 6864985654287583845, 8465468836551135602, 7372808321676939792, -2815261206329145311, -2044219183173664775, -5342853768228072396, 3636940711408324184, -2772742494800447004, -8420993393273439531, -1530882172522252534, 8236427746033013128, -8939749738449264357, -571957476330656311, 6462994120934510138, -2744633996286755268, 1001793370994802364, 6170004027360887596, 383603396273760626, 184737756504479596, -4799447088893889554, 1038205033737034383, 2078124248957773983, -5177819727898656480, 1588469358432181111, 2476693400197902714, 246839957213783595, -7804622995667946321, 3516202677463047183, 7649126752776473673, -3286662198144050257, 2592926684883421936, 6953901594207876325, 8920684239689152479, -2427878301857439455, -6527468054932471540, -4117125961852289967, -2833593154725933249, 2548273043767381234, -814886098184093796, -1113961241682560435, -8364806058670744019, -86067309810855914, -7325813350040495905, -2651532619332818109, -3028501296208600216, 2638649530375347897, -3870517833780069551, 3770751443844709295, -7272035856681375921, -6750394828506790417, 3368553496734537183, 8516129492713951191, 4435960977618718666, 638690551817702460, -7462842134093200053, -7312636473795422279, 3825550639500258186, -490674188267611204, 8488259904981422083, 4436678791994058329, 5971819389544487212, 5777643219857256454, 6295906877222880293, -6635403410495817577, -7125973103119231247, 2275471188158109929, -6554337501188391642, -4759608795508681126, -7655250005358224912, 9106670136441382451, -9080117178764089351, 5094764588972879219, -3599769156391426161, 6116955962236377408, -1734768840951819839, 7826627278264825770, -2624139016757063818, -4122417151587476614, -6757251857390630385, 2099124804383862824, -3162332634454027278, 4826222794133551270, 9122652158513265055, 1734656138981660315, 972980826344778639, -1746779194020635548, -3426944282250211269, -3857828063692993065, 1895243495321867610, -8828035583443240909, -4705856469629722102, -8519546521146945353, -2150150551733933931, 8281585304878501119, -2775028105733898661, 2087277989579187052, -4016777313261130077, 2747128117959922334, -1398884803916585873, 7188260080368469340, -3880993098463994199, 3574665846011083154, 5260683239918360122, 5817587463499837044, 38978473621576635, 2680910834841463710, 6083561971466189055, 7236937177408808074, -3600112532662592989, -4559800196660261967, 8276688045060113438, 5496539762676760591, -2999626688519766687, 8917068693185637310, 2348378561310644717, 7605443413072783308, 5729359499569394810, -782345069306605591, 1165004403533704355, -8301882560002322767, 2008499890787626408, -6211027251975593898, 7406423735628820605, -3204398339633370684, -7917412446164112725, -106645076087724250, -1186720400780396653, -8676089669972641821, -1970508303671183113, -7283082875075535628, -3469652138221449481, -3310949358194646693, 6449384223770405185, -3602652844861890703, -7845236015467185307, -4548809972889727666, -8898627491921139823, 5187965699546741544, 295363921125698104, -8013235493809339368, -6747271362503076577, 1102625310233591704, -2543233385033476145, -6197912327393001665, 118165474822979356, -4838870266722406438, -5797141823778124932, -1506683916229985698, 9139710449103348665, -1571612701117454805, 8031141543284728427, 8472337544063987034, 3222463867738580103, 8210687258187437204]
2017-11-13T14:29:25.092248590Z INFO Create new Keyspace: KeyspaceMetadata{name=system_traces, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=2}}, tables=[org.apache.cassandra.config.CFMetaData@3bc5ed95[cfId=c5e99f16-8677-3914-b17e-960613512345,ksName=system_traces,cfName=sessions,flags=[COMPOUND],params=TableParams{comment=tracing sessions, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=0, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [client command coordinator duration request started_at parameters]],partitionKeyColumns=[session_id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.UUIDType,columnMetadata=[client, command, session_id, coordinator, request, started_at, duration, parameters],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@1a296ffd[cfId=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksName=system_traces,cfName=events,flags=[COMPOUND],params=TableParams{comment=tracing events, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=0, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.TimeUUIDType),partitionColumns=[[] | [activity source source_elapsed thread]],partitionKeyColumns=[session_id],clusteringColumns=[event_id],keyValidator=org.apache.cassandra.db.marshal.UUIDType,columnMetadata=[activity, event_id, session_id, source, thread, source_elapsed],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}
2017-11-13T14:29:25.394141160Z INFO Not submitting build tasks for views in keyspace system_traces as storage service is not initialized
2017-11-13T14:29:25.408584506Z INFO Initializing system_traces.events
2017-11-13T14:29:25.424314845Z INFO Initializing system_traces.sessions
2017-11-13T14:29:25.483133136Z INFO Create new Keyspace: KeyspaceMetadata{name=system_distributed, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}}, tables=[org.apache.cassandra.config.CFMetaData@2884b38b[cfId=759fffad-624b-3181-80ee-fa9a52d1f627,ksName=system_distributed,cfName=repair_history,flags=[COMPOUND],params=TableParams{comment=Repair history, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.TimeUUIDType),partitionColumns=[[] | [coordinator exception_message exception_stacktrace finished_at parent_id range_begin range_end started_at status participants]],partitionKeyColumns=[keyspace_name, columnfamily_name],clusteringColumns=[id],keyValidator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type),columnMetadata=[status, id, coordinator, finished_at, participants, exception_stacktrace, parent_id, range_end, range_begin, exception_message, keyspace_name, started_at, columnfamily_name],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7fcc80b2[cfId=deabd734-b99d-3b9c-92e5-fd92eb5abf14,ksName=system_distributed,cfName=parent_repair_history,flags=[COMPOUND],params=TableParams{comment=Repair history, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [exception_message exception_stacktrace finished_at keyspace_name started_at columnfamily_names options requested_ranges successful_ranges]],partitionKeyColumns=[parent_id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.TimeUUIDType,columnMetadata=[requested_ranges, exception_message, keyspace_name, successful_ranges, started_at, finished_at, options, exception_stacktrace, parent_id, columnfamily_names],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7e500004[cfId=5582b59f-8e4e-35e1-b913-3acada51eb04,ksName=system_distributed,cfName=view_build_status,flags=[COMPOUND],params=TableParams{comment=Materialized View build status, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UUIDType),partitionColumns=[[] | [status]],partitionKeyColumns=[keyspace_name, view_name],clusteringColumns=[host_id],keyValidator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type),columnMetadata=[view_name, status, keyspace_name, host_id],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}
2017-11-13T14:29:25.598604284Z INFO Not submitting build tasks for views in keyspace system_distributed as storage service is not initialized
2017-11-13T14:29:25.602132560Z INFO Initializing system_distributed.parent_repair_history
2017-11-13T14:29:25.624580018Z INFO Initializing system_distributed.repair_history
2017-11-13T14:29:25.624605811Z INFO Initializing system_distributed.view_build_status
2017-11-13T14:29:25.682205208Z INFO JOINING: Finish joining ring
2017-11-13T14:29:25.808448539Z INFO Create new Keyspace: KeyspaceMetadata{name=system_auth, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[org.apache.cassandra.config.CFMetaData@3c28c0da[cfId=5bc52802-de25-35ed-aeab-188eecebb090,ksName=system_auth,cfName=roles,flags=[COMPOUND],params=TableParams{comment=role definitions, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [can_login is_superuser salted_hash member_of]],partitionKeyColumns=[role],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[salted_hash, member_of, role, can_login, is_superuser],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@2e0f771e[cfId=0ecdaa87-f8fb-3e60-88d1-74fb36fe5c0d,ksName=system_auth,cfName=role_members,flags=[COMPOUND],params=TableParams{comment=role memberships lookup table, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | []],partitionKeyColumns=[role],clusteringColumns=[member],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[role, member],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@4fabdebb[cfId=3afbe79f-2194-31a7-add7-f5ab90d8ec9c,ksName=system_auth,cfName=role_permissions,flags=[COMPOUND],params=TableParams{comment=permissions granted to db roles, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | [permissions]],partitionKeyColumns=[role],clusteringColumns=[resource],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[role, resource, permissions],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7103b8de[cfId=5f2fbdad-91f1-3946-bd25-d5da3a5c35ec,ksName=system_auth,cfName=resource_role_permissons_index,flags=[COMPOUND],params=TableParams{comment=index of db roles with permissions granted on a resource, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | []],partitionKeyColumns=[resource],clusteringColumns=[role],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[resource, role],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}
2017-11-13T14:29:25.934019252Z INFO Not submitting build tasks for views in keyspace system_auth as storage service is not initialized
2017-11-13T14:29:25.953887674Z INFO Initializing system_auth.resource_role_permissons_index
2017-11-13T14:29:25.957358898Z INFO Initializing system_auth.role_members
2017-11-13T14:29:25.967935061Z INFO Initializing system_auth.role_permissions
2017-11-13T14:29:25.995449692Z INFO Initializing system_auth.roles
2017-11-13T14:29:26.193856408Z INFO Netty using native Epoll event loop
2017-11-13T14:29:26.247676724Z INFO Using Netty Version: [netty-buffer=netty-buffer-4.0.44.Final.452812a, netty-codec=netty-codec-4.0.44.Final.452812a, netty-codec-haproxy=netty-codec-haproxy-4.0.44.Final.452812a, netty-codec-http=netty-codec-http-4.0.44.Final.452812a, netty-codec-socks=netty-codec-socks-4.0.44.Final.452812a, netty-common=netty-common-4.0.44.Final.452812a, netty-handler=netty-handler-4.0.44.Final.452812a, netty-tcnative=netty-tcnative-1.1.33.Fork26.142ecbb, netty-transport=netty-transport-4.0.44.Final.452812a, netty-transport-native-epoll=netty-transport-native-epoll-4.0.44.Final.452812a, netty-transport-rxtx=netty-transport-rxtx-4.0.44.Final.452812a, netty-transport-sctp=netty-transport-sctp-4.0.44.Final.452812a, netty-transport-udt=netty-transport-udt-4.0.44.Final.452812a]
2017-11-13T14:29:26.247705469Z INFO Starting listening for CQL clients on /0.0.0.0:9042 (unencrypted)...
2017-11-13T14:29:26.309591159Z INFO Not starting RPC server as requested. Use JMX (StorageService->startRPCServer()) or nodetool (enablethrift) to start it
2017-11-13T14:29:36.275846037Z INFO Created default superuser role 'cassandra'
2017-11-13T14:29:40.333918591Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:40.434399612Z INFO Create new table: org.apache.cassandra.config.CFMetaData@c74a94b[cfId=1572b410-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:40.566922871Z INFO Initializing my_keyspace.schema_version
2017-11-13T14:29:42.719380089Z INFO Drop Keyspace 'my_keyspace'
2017-11-13T14:29:43.124510221Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:43.243928493Z INFO Create new table: org.apache.cassandra.config.CFMetaData@1a0616e9[cfId=171e8f50-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:43.284700491Z INFO Initializing my_keyspace.schema_version
2017-11-13T14:29:44.706916652Z INFO Drop Keyspace 'my_keyspace'
2017-11-13T14:29:44.924446999Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:44.993983743Z INFO Create new table: org.apache.cassandra.config.CFMetaData@7338ccab[cfId=182996b0-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:45.078407254Z INFO Initializing my_keyspace.schema_version
2017-11-13T14:29:46.244137923Z INFO Drop Keyspace 'my_keyspace'
2017-11-13T14:29:46.500351100Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:46.575419551Z INFO Create new table: org.apache.cassandra.config.CFMetaData@229f3694[cfId=191b97d0-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:46.617101680Z ERROR Unexpected error during query
2017-11-13T14:29:46.617126436Z java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.617130194Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:404) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617133358Z at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:549) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617135966Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:356) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617138576Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:341) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617141018Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:321) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617143454Z at org.apache.cassandra.cql3.statements.CreateTableStatement.announceMigration(CreateTableStatement.java:89) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617145953Z at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617148372Z at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617150806Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617153201Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617155595Z at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617157962Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617160377Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617162787Z at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617166295Z at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617168898Z at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617171389Z at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617173808Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.617184008Z at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617186971Z at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617189340Z at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
2017-11-13T14:29:46.617191666Z Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.617193951Z at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
2017-11-13T14:29:46.617196258Z at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
2017-11-13T14:29:46.617198553Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:400) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617200927Z ... 20 common frames omitted
2017-11-13T14:29:46.617203114Z Caused by: java.lang.NullPointerException: null
2017-11-13T14:29:46.617205382Z at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:273) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617207766Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspaceParams(SchemaKeyspace.java:956) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617210107Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:943) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617212462Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesOnly(SchemaKeyspace.java:937) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617214868Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1363) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617217261Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1342) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617220404Z at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:567) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617222948Z at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617225287Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.617227589Z at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
2017-11-13T14:29:46.617229894Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
2017-11-13T14:29:46.617232175Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
2017-11-13T14:29:46.617234514Z at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617236990Z ... 1 common frames omitted
2017-11-13T14:29:46.621331936Z ERROR Exception in thread Thread[MigrationStage:1,5,main]
2017-11-13T14:29:46.621360645Z java.lang.NullPointerException: null
2017-11-13T14:29:46.621364339Z at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:273) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621373614Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspaceParams(SchemaKeyspace.java:956) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621376363Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:943) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621378927Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesOnly(SchemaKeyspace.java:937) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621381395Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1363) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621384992Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1342) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621387567Z at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:567) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621390255Z at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621392722Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
2017-11-13T14:29:46.621395153Z at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
2017-11-13T14:29:46.621397502Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
2017-11-13T14:29:46.621399919Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
2017-11-13T14:29:46.621402347Z at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621404867Z at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
2017-11-13T14:29:46.626625652Z ERROR Unexpected exception during request
2017-11-13T14:29:46.626650886Z java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.626654840Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:404) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626658003Z at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:549) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626660570Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:356) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626663155Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:341) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626665745Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:321) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626676412Z at org.apache.cassandra.cql3.statements.CreateTableStatement.announceMigration(CreateTableStatement.java:89) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626679497Z at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626682051Z at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626684610Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626687059Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626689495Z at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626691956Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626694391Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626696869Z at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626700811Z at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626703433Z at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626705926Z at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626708464Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.626710858Z at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626713448Z at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626715868Z at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
2017-11-13T14:29:46.626718281Z Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.626720647Z at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
2017-11-13T14:29:46.626723006Z at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
2017-11-13T14:29:46.626725392Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:400) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626727820Z ... 20 common frames omitted
2017-11-13T14:29:46.626730100Z Caused by: java.lang.NullPointerException: null
2017-11-13T14:29:46.626735106Z at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:273) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626737800Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspaceParams(SchemaKeyspace.java:956) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626740362Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:943) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626742804Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesOnly(SchemaKeyspace.java:937) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626745273Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1363) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626747719Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1342) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626750759Z at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:567) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626753445Z at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626755900Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.626758684Z at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
2017-11-13T14:29:46.626761055Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
2017-11-13T14:29:46.626763436Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
2017-11-13T14:29:46.626765871Z at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626768418Z ... 1 common frames omitted{code}

Steps to reproduce:
1. Start cassandra
2. Start cqlsh and paste the following in quick succession:
{code:java}
USE system;
DROP KEYSPACE IF EXISTS my_keyspace;
CREATE KEYSPACE my_keyspace WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 1};
USE my_keyspace;
CREATE TABLE schema_version (id int primary key, version int, migration_lock text);
INSERT INTO schema_version (id, version) values (1, 0);{code}
3. Once fourth time or so , we'll see:
{code:java}
cqlsh:system> CREATE KEYSPACE my_keyspace WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 1};
ServerError: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException{code}
or
{code:java}
cqlsh:my_keyspace> CREATE TABLE schema_version (id int primary key, version int, migration_lock text);
ServerError: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-17 00:48:52.689,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 07 22:14:05 UTC 2017,,,,,,0|i3mplz:,9223372036854775807,,,,,,,,blerer,blerer,,,,,,,,,,"17/Nov/17 00:48;VincentWhite;I had a look at this and assumed that org.apache.cassandra.schema.SchemaKeyspace#fetchKeyspaceParams() was just not getting any rows returned when it queried the system_schema.keyspaces table. In fact in my testing it was getting a row returned but it only contained the primary key and null's for both other columns. I didn't dig too deep into this but it doesn't seem like it should happen, it's probably worth someone with a more intimate knowledge of the read path taking a look. Also on my machine I could only trigger this exception with multiple clients looping CREATE/DROP commands and it was still relatively rare.  
","29/Nov/17 19:56;jjordan;I just saw this on some tests today as well.  The issue seems to be that the drop is happening concurrently with tables being initialized:

{quote}
2017-11-13T14:29:40.566922871Z INFO Initializing my_keyspace.schema_version
2017-11-13T14:29:42.719380089Z INFO Drop Keyspace 'my_keyspace'
2017-11-13T14:29:43.124510221Z INFO Create new Keyspace: 
{quote}","06/Dec/17 08:25;jasonstack;| patch  |  test  | dtest|
| [3.0 |https://github.com/apache/cassandra/compare/cassandra-3.0...jasonstack:CASSANDRA-14010-3.0?expand=1 ] |
| [3.11 |https://github.com/apache/cassandra/compare/cassandra-3.11...jasonstack:CASSANDRA-14010-3.11?expand=1 ] |
| [trunk | https://github.com/apache/cassandra/compare/trunk...jasonstack:CASANDRA-14010-trunk?expand=1] |


It turns out that the query in {{fetchKeyspaceParams()}} gets incomplete data from memtable.

{code}
process:
  0. drop ks with ts1 
  1. apply create ks mutation with t2 (t2>t1)
  2. flush memtables including ""system_schema.keyspaces"" table
  3. select keyspace_name from ""system_schema.keyspaces"" table in {{fetchKeyspaceOnly()}} causing ""defragmenting"" (at the end of SPRC.queryMemtableAndSSTablesInTimestampOrder()) to insert the selected data into memtable
  4. select * from ""system_schema.keyspaces"" table in {{fetchKeyspaceParams()}} getting incomplete data(row with liveness of t2 and deletion of t1, no regular columns) from memtable. First sstable's maxtimestamp is smaller than memtable data's deletion time(drop ks time, t1) because sstables are sorted by maxTS in ascending order and other newer sstables are skipped...

The correct order is descending to eliminate older sstables.
{code}

The patch is to make sure sstables are compared with max-timestamp in descending order...

The reason that it only happened on 3.11 is related to {{queriedColumn in ColumnFilter}} and value skipping added in 3.x.  (a bit complex...)

When no non-pk column is selected, the {{queried}} columns in ColumnFilter.builder will be initialized as empty, thus when processing the query in #3, unselected columns(eg. durable_wirtes, replication) are skipped in Cell.Serializer: helper.canSkipValue().

But in trunk, due to CASSANDRA-7396, when no non-pk column is selected, the {{queried}} columns in ColumnFilter.builder will be initialized as null, thus unselected columns are not skipped, later put into memtable. (lost the benefit of value skipping)","06/Dec/17 08:53;jasonstack;If we ignore the complexity of defragmenting, columnfilter, etc... It can be reproduced easily:
{code:title=reproduce}
        createTable(""CREATE TABLE %s (k1 int, v1 int, v2 int, PRIMARY KEY (k1))"");
        ColumnFamilyStore cfs = ColumnFamilyStore.getIfExists(keyspace(), currentTable());
        cfs.disableAutoCompaction();

        execute(""INSERT INTO %s(k1,v1,v2) VALUES(1,1,1)  USING TIMESTAMP 5"");
        cfs.forceBlockingFlush();

        execute(""INSERT INTO %s(k1,v1,v2) VALUES(1,1,2)  USING TIMESTAMP 8"");
        cfs.forceBlockingFlush();

        execute(""INSERT INTO %s(k1) VALUES(1)  USING TIMESTAMP 7"");
        // deletion 6 shadow sstable-1 with ts=5 ...
        execute(""DELETE FROM %s USING TIMESTAMP 6 WHERE k1 = 1"");

        assertRows(execute(""SELECT * FROM %s WHERE k1=1""), row(1, 1, 2));
{code}
","06/Dec/17 14:41;jasonstack;CI looks good..

","06/Dec/17 15:10;blerer;Thanks for the patch. The fix looks good :-)
Small nit: The unit test can be simplified by using {{disableCompaction()}} instead of {{cfs.disableAutoCompaction()}} and {{flush()}} instead of {{cfs.forceBlockingFlush()}}. ","06/Dec/17 17:07;jjirsa;{{SSTableReader.maxTimestampComparator}} is used in LCS:

{code}
                if (candidates.size() > MAX_COMPACTING_L0)
                {
                    // limit to only the MAX_COMPACTING_L0 oldest candidates
                    candidates = new HashSet<>(ageSortedSSTables(candidates).subList(0, MAX_COMPACTING_L0));
                    break;
                }
...

    private List<SSTableReader> ageSortedSSTables(Collection<SSTableReader> candidates)
    {
        List<SSTableReader> ageSortedCandidates = new ArrayList<>(candidates);
        Collections.sort(ageSortedCandidates, SSTableReader.maxTimestampComparator);
        return ageSortedCandidates;
    }

{code}

Changing it to be oldest first violates at least the comment and the intent. Probably need to introduce a new {{Comparator<SSTableReader>}} like {{maxTimestampComparatorDescending}}

","06/Dec/17 18:01;jjordan;[~jjirsa] CASSANDRA-13776 accidentally changed the definition of the maxTimestampComparator while trying to simplify code.
From CASSANDRA-13776:

{code}
-    public static final Comparator<SSTableReader> maxTimestampComparator = new Comparator<SSTableReader>()
 -    {
 -        public int compare(SSTableReader o1, SSTableReader o2)
 -        {
 -            long ts1 = o1.getMaxTimestamp();
 -            long ts2 = o2.getMaxTimestamp();
 -            return (ts1 > ts2 ? -1 : (ts1 == ts2 ? 0 : 1));
 -        }
 -    };
 +    public static final Comparator<SSTableReader> maxTimestampComparator = (o1, o2) -> Long.compare(o1.getMaxTimestamp(), o2.getMaxTimestamp());
{code}

This is just putting it back like it was before the CASSANDRA-13776.  So this is how it has worked up until 13776 went in.","06/Dec/17 18:21;jjirsa;Ok so LCS is wrong, created  CASSANDRA-14099 to follow up there.
",07/Dec/17 00:54;jasonstack;Thanks for the review.. fix nits and restarted CI.  CI looks good.,07/Dec/17 22:14;pauloricardomg;Committed as {{a9225f90e205a7c2b24a4ad4a32d0961067005b0}} to cassandra-3.0 and merged up to cassandra-3.11 and trunk. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid grabbing the read lock when checking LCS fanout and if compaction strategy should do defragmentation,CASSANDRA-13930,13106677,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,03/Oct/17 11:32,12/Mar/19 14:15,13/Mar/19 22:35,10/Oct/17 11:00,3.11.2,4.0,,,,Local/Compaction,,,,,0,,,,"We grab the read lock when checking whether the compaction strategy benefits from defragmentation, avoid that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-10-07 07:05:47.55,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 10 11:00:55 UTC 2017,,,,,,0|i3ktlj:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"03/Oct/17 11:32;krummas;https://github.com/krummas/cassandra/commits/marcuse/defrag
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/356/
https://circleci.com/gh/krummas/cassandra/137","07/Oct/17 07:05;jjirsa;Patch looks good. Dtests look good, -circleci won’t load for me- circle looks good. 

What do you think about a similar fix for fanout? Maybe not worth the effort, since it's not in as hot of a path, and it's more likely to change than defrag? 

+1 to commit ","09/Oct/17 08:35;krummas;bq. What do you think about a similar fix for fanout? 
makes sense, at least it doesn't hurt (until we want to have LCS change fanout dynamically or something)

pushed up a new commit to the same branch and rerunning tests
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/363/
https://circleci.com/gh/krummas/cassandra/148","09/Oct/17 17:45;jjirsa;lgtm if dtests are happy (expect it should be fine). 
",09/Oct/17 18:57;eduard.tudenhoefner;changes LGTM. Looks like majority of the failed dtests are because they couldn't clone the repo.,"10/Oct/17 11:00;krummas;and committed as {{f3cf1c019e0298dd04f6a0d7396b5fe4a93e6f9a}}, thanks! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reload compaction strategies when JBOD disk boundary changes,CASSANDRA-13948,13108507,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,11/Oct/17 06:51,12/Mar/19 14:15,13/Mar/19 22:35,08/Dec/17 18:51,3.11.2,4.0,,,,Local/Compaction,,,,,1,jbod-aware-compaction,,,"The thread dump below shows a race between an sstable replacement by the {{IndexSummaryRedistribution}} and {{AbstractCompactionTask.getNextBackgroundTask}}:

{noformat}
Thread 94580: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=175 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt() @bci=1, line=836 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(java.util.concurrent.locks.AbstractQueuedSynchronizer$Node, int) @bci=67, line=870 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(int) @bci=17, line=1199 (Compiled frame)
 - java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock() @bci=5, line=943 (Compiled frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.handleListChangedNotification(java.lang.Iterable, java.lang.Iterable) @bci=359, line=483 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(org.apache.cassandra.notifications.INotification, java.lang.Object) @bci=53, line=555 (Interpreted frame)
 - org.apache.cassandra.db.lifecycle.Tracker.notifySSTablesChanged(java.util.Collection, java.util.Collection, org.apache.cassandra.db.compaction.OperationType, java.lang.Throwable) @bci=50, line=409 (Interpreted frame)
 - org.apache.cassandra.db.lifecycle.LifecycleTransaction.doCommit(java.lang.Throwable) @bci=157, line=227 (Interpreted frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.commit(java.lang.Throwable) @bci=61, line=116 (Compiled frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.commit() @bci=2, line=200 (Interpreted frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish() @bci=5, line=185 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryRedistribution.redistributeSummaries() @bci=559, line=130 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionManager.runIndexSummaryRedistribution(org.apache.cassandra.io.sstable.IndexSummaryRedistribution) @bci=9, line=1420 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries(org.apache.cassandra.io.sstable.IndexSummaryRedistribution) @bci=4, line=250 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries() @bci=30, line=228 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager$1.runMayThrow() @bci=4, line=125 (Interpreted frame)
 - org.apache.cassandra.utils.WrappedRunnable.run() @bci=1, line=28 (Interpreted frame)
 - org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run() @bci=4, line=118 (Compiled frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.runAndReset() @bci=47, line=308 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask) @bci=1, line=180 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run() @bci=37, line=294 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(java.lang.Runnable) @bci=1, line=81 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$8.run() @bci=4 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)
{noformat}

{noformat}
Thread 94573: (state = IN_JAVA)
 - java.util.HashMap$HashIterator.nextNode() @bci=95, line=1441 (Compiled frame; information may be imprecise)
 - java.util.HashMap$KeyIterator.next() @bci=1, line=1461 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.View$3.apply(org.apache.cassandra.db.lifecycle.View) @bci=20, line=268 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.View$3.apply(java.lang.Object) @bci=5, line=265 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.Tracker.apply(com.google.common.base.Predicate, com.google.common.base.Function) @bci=13, line=133 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.Tracker.tryModify(java.lang.Iterable, org.apache.cassandra.db.compaction.OperationType) @bci=31, line=99 (Compiled frame)
 - org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(int) @bci=84, line=139 (Compiled frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(int) @bci=105, line=119 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run() @bci=84, line=265 (Interpreted frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.run() @bci=42, line=266 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(java.lang.Runnable) @bci=1, line=81 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$8.run() @bci=4 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)
{noformat}

This particular node remain in this state forever, indicating {{LeveledCompactionStrategyTask.getNextBackgroundTask}} was looping indefinitely.

What happened is that sstable references were replaced on the tracker by the {{IndexSummaryRedistribution}} thread, so the {{AbstractCompactionStrategy.getNextBackgroundTask}} could not create the transaction with the old references, and the {{IndexSummaryRedistribution}} could not update the sstable reference in the compaction strategy because {{AbstractCompactionStrategy.getNextBackgroundTask}} was holding the {{CompactionStrategyManager}} lock.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13980,,,CASSANDRA-14083,CASSANDRA-13943,CASSANDRA-10099,,,,01/Dec/17 13:49;pauloricardomg;13948dtest.png;https://issues.apache.org/jira/secure/attachment/12900222/13948dtest.png,01/Dec/17 13:49;pauloricardomg;13948testall.png;https://issues.apache.org/jira/secure/attachment/12900221/13948testall.png,06/Dec/17 13:38;pauloricardomg;3.11-13948-dtest.png;https://issues.apache.org/jira/secure/attachment/12900865/3.11-13948-dtest.png,06/Dec/17 13:38;pauloricardomg;3.11-13948-testall.png;https://issues.apache.org/jira/secure/attachment/12900864/3.11-13948-testall.png,12/Oct/17 22:17;dkinder;debug.log;https://issues.apache.org/jira/secure/attachment/12891830/debug.log,13/Nov/17 08:20;pauloricardomg;dtest13948.png;https://issues.apache.org/jira/secure/attachment/12897298/dtest13948.png,30/Nov/17 00:47;pauloricardomg;dtest2.png;https://issues.apache.org/jira/secure/attachment/12899922/dtest2.png,08/Nov/17 17:04;llambiel;threaddump-cleanup.txt;https://issues.apache.org/jira/secure/attachment/12896686/threaddump-cleanup.txt,03/Nov/17 21:09;llambiel;threaddump.txt;https://issues.apache.org/jira/secure/attachment/12895966/threaddump.txt,03/Nov/17 08:12;llambiel;trace.log;https://issues.apache.org/jira/secure/attachment/12895588/trace.log,06/Dec/17 13:38;pauloricardomg;trunk-13948-dtest.png;https://issues.apache.org/jira/secure/attachment/12900863/trunk-13948-dtest.png,06/Dec/17 13:38;pauloricardomg;trunk-13948-testall.png;https://issues.apache.org/jira/secure/attachment/12900862/trunk-13948-testall.png,12.0,,,,,,,,,,,,,,,,,,,2017-10-11 07:17:17.312,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 11 20:23:18 UTC 2017,,,,,,0|i3l4b3:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"11/Oct/17 07:13;pauloricardomg;I think we can get rid of the {{while (true)}} loops on {{*CompactionStrategy.getNextBackgroundTask}} when not able to lock sstables for compaction, and just submit a new background task when receiving any notifications from the tracker to ensure a new compaction will operate on the most updated references.

I also removed the {{CompactionStrategyManager.replaceFlushed}} method because it should no longer be necessary because a new background compaction candidate will be submitted when receiving an {{SSTableAddedNotification}} from the tracker. 

Similarly we no longer need to call {{CompactionManager.instance.submitBackground}} after a {{BackgroundCompactionCandidate}} because it will already be called after receiving an {{SSTableListChangedNotification}}, so we centralize calls to {{CompactionManager.instance.submitBackground}} on {{CompactionStrategyManager.handleNotification}}.

I added a unit test to check that {{*CompactionStrategy.getNextBackgroundTask}} never blocks indefinitely, even when not able to lock sstables in the tracker.

A race there should be pretty unlikely, but in case it happens I logged a warning to detect potential problems if it happens frequently due to some wrong condition: {{""Could not acquire references for compacting SSTables {} which is not a problem per se, unless it happens frequently, in which case it must be reported. Will retry later.""}}.

Patch available [here|https://github.com/pauloricardomg/cassandra/tree/3.11-13948]

Mind having a look [~krummas]?

I submitted internal CI, will post the results here once available.","11/Oct/17 07:17;krummas;sure, I'll review","12/Oct/17 22:13;dkinder;Just a heads up, I have been seeing these deadlocks happen easily, so I am running your patch [~pauloricardomg] in addition to Marcus's[patch|https://github.com/krummas/cassandra/commits/marcuse/13215] from CASSANDRA-13215.

I do see a large number of ""Could not acquire references for compacting SSTable"" happening, in bursts. Will upload a log file.

I also see some of this:
{noformat}
java.lang.AssertionError: Memory was freed
        at org.apache.cassandra.io.util.Memory.checkBounds(Memory.java:344) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.util.Memory.getInt(Memory.java:291) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.IndexSummary.getPositionInSummary(IndexSummary.java:148) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.IndexSummary.fillTemporaryKey(IndexSummary.java:162) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.IndexSummary.binarySearch(IndexSummary.java:121) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.format.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:1370) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.format.SSTableReader.estimatedKeysForRanges(SSTableReader.java:1326) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:441) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.findDroppableSSTable(LeveledCompactionStrategy.java:503) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:121) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:124) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:262) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_144]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_144]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_144]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_144]
{noformat}

UPDATE: the node does successfully complete compactions for a while but gradually does fewer and fewer and then stops compacting altogether, even though compactionstats says there are pending compactions:

{noformat}
dkinder@seu-walker-fs01:~$ nodetool compactionstats
pending tasks: 102
- walker.links: 102

dkinder@seu-walker-fs01:~$
{noformat}
It stays this way and periodically prints out some ""Could not acquire references for compacting SSTable"" messages. It is able to compact some more if I restart the node.

tablestats for walker.links:
{noformat}
Keyspace : walker
        Read Count: 16952
        Read Latency: 10.388668062765454 ms.
        Write Count: 277291
        Write Latency: 0.0186555207345352 ms.
        Pending Flushes: 0
                Table: links
                SSTable count: 8507
                SSTables in each level: [73/4, 81/10, 297/100, 2078/1000, 2402, 3635, 0, 0, 0]
                Space used (live): 4902698702556
                Space used (total): 4902698702556
                Space used by snapshots (total): 13788057680993
                Off heap memory used (total): 9835235
                SSTable Compression Ratio: -1.0
                Number of partitions (estimate): 19996043
                Memtable cell count: 360248
                Memtable data size: 36729792
                Memtable off heap memory used: 0
                Memtable switch count: 0
                Local read count: 0
                Local read latency: NaN ms
                Local write count: 360248
                Local write latency: 0.017 ms
                Pending flushes: 0
                Percent repaired: 0.0
                Bloom filter false positives: 0
                Bloom filter false ratio: 0.00000
                Bloom filter space used: 8538560
                Bloom filter off heap memory used: 8470504
                Index summary off heap memory used: 1364731
                Compression metadata off heap memory used: 0
                Compacted partition minimum bytes: 43
                Compacted partition maximum bytes: 190420296972
                Compacted partition mean bytes: 247808
                Average live cells per slice (last five minutes): NaN
                Maximum live cells per slice (last five minutes): 0
                Average tombstones per slice (last five minutes): NaN
                Maximum tombstones per slice (last five minutes): 0
                Dropped Mutations: 2
{noformat}",17/Oct/17 09:27;krummas;[~pauloricardomg] did you get those CI results?,"19/Oct/17 12:29;pauloricardomg;I think I was able to get to the bottom of this issue with the help of [~dkinder]'s logs

It turns out that the {{ColumnFamilyStore}} (and subsequently the {{CompactionStrategyManager}}) is initialized before gossip is settled, so the node's local ranges are not properly computed during startup, causing the computed disk boundaries to not match the actual boundaries. 

This happens because {{GossipingPropertyFileSnitch}} fallbacks to the {{FilePropertySnitch}}, so when a node is not found is gossip it will pick the DCs/racks from the {{cassandra-topology.properties}} file, and if it's not defined there it will use the {{DEFAULT}} dc/rack and mess up the local ranges and disk boundary computation.

The log lines below show the following steps:
* {{GossipingPropertyFileSnitch}} falling back to {{PropertyFileSnitch}}
* Compactions on system keyspaces being scheduled on the same disk, while compaction on user keyspaces being run on SSTables from different disks, indicating the disk boundaries were not calculated correctly for NTS keyspaces
* After gossip settles, lot's of {{SSTable from level 0 is not on corresponding level in the leveled manifest}} warnings, indicating the disk boundary layout changed after gossip settled but the compaction strategies were not reloaded with the new layout

{code:none}
INFO  [main] 2017-10-12 14:39:32,928 GossipingPropertyFileSnitch.java:64 - Loaded cassandra-topology.properties for compatibility
DEBUG [CompactionExecutor:26] 2017-10-12 15:02:09,442 CompactionTask.java:155 - Compacting (fe490ea1-af98-11e7-b5a1-57bcefdac924) [/srv/disk6/cassandra-data/walker/domain_info/.domain_info_claim_tok_idx/mc-18386-big-Data.db:level=0, /srv/disk9/cassandra-data/walker/domain_info/.domain_info_claim_tok_idx/mc-18387-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:31] 2017-10-12 15:02:09,442 CompactionTask.java:155 - Compacting (fe490ea0-af98-11e7-b5a1-57bcefdac924) [/srv/disk10/cassandra-data/system/peers/mc-1671-big-Data.db:level=0, /srv/disk10/cassandra-data/system/peers/mc-1659-big-Data.db:level=0, /srv/disk10/cassandra-data/system/peers/mc-1656-big-Data.db:level=0, /srv/disk10/cassandra-data/system/peers/mc-1690-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:17] 2017-10-12 15:02:09,442 CompactionTask.java:155 - Compacting (fe490ea8-af98-11e7-b5a1-57bcefdac924) [/srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2625-big-Data.db:level=0, /srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2605-big-Data.db:level=0, /srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2597-big-Data.db:level=0, /srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2617-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:29] 2017-10-12 15:02:09,952 CompactionTask.java:155 - Compacting (fe9a8a00-af98-11e7-b5a1-57bcefdac924) [/srv/disk11/cassandra-data/walker/domain_info/.domain_info_dispatched_idx/mc-18474-big-Data.db:level=0, /srv/disk3/cassandra-data/walker/domain_info/.domain_info_dispatched_idx/mc-18473-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:18] 2017-10-12 15:04:08,939 CompactionTask.java:155 - Compacting (45865ca0-af99-11e7-b5a1-57bcefdac924) [/srv/disk8/cassandra-data/walker/links/mc-6571323-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566335-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566315-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566361-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571043-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566330-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566322-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571261-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571335-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6570145-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566346-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571249-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571311-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566307-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571285-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566341-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6570765-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571299-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566352-big-Data.db:level=1, ]
DEBUG [CompactionExecutor:30] 2017-10-12 15:04:08,866 DiskBoundaryManager.java:69 - Cached ring version 84 != current ring version 86
DEBUG [CompactionExecutor:30] 2017-10-12 15:04:08,867 DiskBoundaryManager.java:83 - Refreshing disk boundary cache for walker.links
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,888 LeveledCompactionStrategy.java:140 - Could not acquire references for compacting SSTables [BigTableReader(path='/srv/disk2/cassandra-data/walke
r/links/mc-6566879-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,892 LeveledCompactionStrategy.java:140 - Could not acquire references for compacting SSTables [BigTableReader(path='/srv/disk11/cassandra-data/walk
er/links/mc-6566381-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.
DEBUG [CompactionExecutor:30] 2017-10-12 15:04:08,939 CompactionTask.java:255 - Compacted (2f095f90-af99-11e7-b5a1-57bcefdac924) 1 sstables to [/srv/disk1/cassandra-data/walker/links/mc-6571348-big
,] to level=4.  271.077MiB to 271.077MiB (~100% of original) in 37,729ms.  Read Throughput = 7.185MiB/s, Write Throughput = 7.185MiB/s, Row Throughput = ~116,380/s.  741 total partitions merged to 
741.  Partition merge counts were {1:741, }
DEBUG [CompactionExecutor:18] 2017-10-12 15:04:08,939 CompactionTask.java:155 - Compacting (45865ca0-af99-11e7-b5a1-57bcefdac924) [/srv/disk8/cassandra-data/walker/links/mc-6571323-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566335-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566315-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566361-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571043-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566330-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566322-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571261-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571335-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6570145-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566346-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571249-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571311-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566307-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571285-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566341-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6570765-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571299-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566352-big-Data.db:level=1, ]
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,941 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571323-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566335-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566315-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566361-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566330-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566322-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571261-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571335-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566346-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571311-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566307-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,943 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571285-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,943 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566341-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,943 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566352-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
{code}

From the above I draw the following conclusions:
1. We should wait for gossip to settle before starting compactions, since the disk boundaries may be dependent on gossip when any gossiping snitch is used. (done on [this commit|https://github.com/pauloricardomg/cassandra/commit/d5bd9eab53ab423dde024400c299f01e335cdf4c])
2. We should reload the compaction strategies when the disk layout changes, so SSTables are correctly mapped to their corresponding compaction strategies. (done on [this commit|https://github.com/pauloricardomg/cassandra/commit/073bfa4e548ac807b523a919288a5a71379bfd21], in addition to testing and some other simplifications on {{CompactionStrategyManager}})
3. A race when acquiring references on {{CompactionStrategyManager.getNextBackgroundTask}} is quite common when there are multiple concurrent compactions, since there will be parallel flushes on multiple disks, so rather than removing the {{while true}} loop to acquire references, I updated the compaction strategies to stop when the current candidate is the same as before, indicating there is a race with the tracker ([commit|https://github.com/pauloricardomg/cassandra/commit/3f971daa48036f14ea7aba7fd1d56150e78415b3]).

I will work on dtests to simulate the above case as well as test disk boundary refreshing, but this [branch|https://github.com/pauloricardomg/cassandra/tree/3.11-13948] should be ready for a first round of review (cc [~krummas]). I will submit a round of unit and dtests on internal CI and will post the results here when ready.
","19/Oct/17 12:46;krummas;have not had a close look yet, but did you see CASSANDRA-13215 ?","19/Oct/17 12:49;krummas;bq. It turns out that the ColumnFamilyStore (and subsequently the CompactionStrategyManager) is initialized before gossip is settled, so the node's local ranges are not properly computed during startup, causing the computed disk boundaries to not match the actual boundaries.
we should probably fix that instead (or, as well) - flushing sstables also depends on the boundaries, and we can't delay that until gossip has settled (commitlog replay might have to flush)","19/Oct/17 15:54;pauloricardomg;bq. we should probably fix that instead (or, as well) - flushing sstables also depends on the boundaries, and we can't delay that until gossip has settled (commitlog replay might have to flush)

The reason why the ring boundaries were not computed correctly on startup was because the {{GossipingPropertyFileSnitch}} did not have rack/dc info about all nodes on gossip so it fallback to the sample {{conf/cassandra-rackdc.properties}} file from {{PropertyFileSnitch}} so I created CASSANDRA-13970 to fix that.

In any case, even with that fixed, the {{CompactionStrategyManager}} (CSM) does not reload its compaction strategies when the disk boundaries are updated (either because of range movements, when the node first joins the ring, or a disk breaks) what can cause {{CompactionStrategyManager.getCompactionStrategyIndex}} to return a different sstable->disk assignment to the one currently on the CSM, so some SStables may not be correctly updated in the compaction strategies after being updated/replaced/removed.  Perhaps this is better illustrated by [this test|https://github.com/pauloricardomg/cassandra/commit/073bfa4e548ac807b523a919288a5a71379bfd21#diff-f9c882c974db60a710cf1f195cfdb801R95].

So it shouldn't be a problem if on a race with gossip flush writes an SSTable to a wrong disk, as long as after the boundary is updated, the SSTable is placed on the correct compaction strategy and its compaction output will be placed in the correct disk (should probably add a test with this scenario).

Also this is a bit orthogonal to CASSANDRA-13215 - the objective there is to cache the disk boundary computation, while here is to make sure CSM can gracefully handle boundary changes.","19/Oct/17 17:30;krummas;bq. Also this is a bit orthogonal to CASSANDRA-13215
I was thinking that with 13215 we will have a central point which will keep track of when we need to refresh compaction strategies (we could notify CSM once cache in the DiskBoundaryManager has been invalidated for example)

bq. So it shouldn't be a problem if on a race with gossip flush writes an SSTable to a wrong disk,
The flushed sstable will have wrong boundaries, it will have tokens that shouldn't be on that disk, does not really matter if the first token is on the correct disk - more tokens might be on the correct disk than not - this is why 6696 didn't reload compaction strategies after boundary changes. I didn't really consider the case from these logs though, where we are completely wrong at startup, but then go back to the correct pre-restart state, then it totally makes sense to reload them, and I agree, we should always do it for consistency.

Could we create a new ticket for that though as it is not really related to the problem we are trying to solve here","01/Nov/17 04:21;pauloricardomg;bq. I was thinking that with 13215 we will have a central point which will keep track of when we need to refresh compaction strategies (we could notify CSM once cache in the DiskBoundaryManager has been invalidated for example)

Even with that we will probably need to cache the current boundaries on the CSM, to prevent a race where the disk boundaries change in the boundary manager and a flush puts an SSTable in the wrong strategy (due to the index having changed) before the CSM is notified about the boundary change - quite unlikely but still possible.

bq. Could we create a new ticket for that though as it is not really related to the problem we are trying to solve here

Actually the issue in the original ticket description only surfaced because the compaction strategies were not properly reloaded after the disk boundary changes, which is the core issue to solve here, so I will update the ticket description to better reflect that.

I added two new dtests to check that the compaction strategies are being properly reloaded when the disk boundary changes due to bootstrap, decommission and delayed join ([here|https://github.com/pauloricardomg/cassandra-dtest/commit/d53f73419e68eb6925b5baf06824b80d0ccf30b7]) and was able to reproduce the deadlock in current 3.11/trunk.

While debugging these dtests I found that reloading the compaction strategy manager when receiving a notification from the tracker can cause an SSTable to be added twice to the {{LeveledManifest}} (first during re-initialization of the CS and second when processing the SSTableAddedNotification), so I stopped reloading the CSM when receiving a notification from the tracker and added a warning on {{LeveledManifest}} when trying to add an SSTable which is already present ([here|https://github.com/pauloricardomg/cassandra/commit/a3eaa8a408cf0e8c5524062fa0fdee9a1eb0d6c0]) - this shouldn't be a problem since we maybeReload the CSM before submitting a new background tasks.

In summary this patch makes the following changes:
1) Reload compaction strategies when JBOD disk boundary changes ([commit|https://github.com/pauloricardomg/cassandra/commit/efb2afb22792a06d83020ac7097154593b9e684d])
2) Ensure compaction strategies do not loop indefinitely when not able to acquire Tracker lock ([commit|https://github.com/pauloricardomg/cassandra/commit/9fdb8f0fb40954a8ed9570cb568a7084de4c80c5])
3) Only enable compaction strategies after gossip settles to prevent unnecessary relocation work ([commit|https://github.com/pauloricardomg/cassandra/commit/c524ff724f2ca9e7eed59cb07f81b9211098fb5c])
4) Do not reload compaction strategies when receiving notifications and log warning when an SSTable is added multiple times to LCS ([commit|https://github.com/pauloricardomg/cassandra/commit/a3eaa8a408cf0e8c5524062fa0fdee9a1eb0d6c0])

The CI of the previous version of this patch was successful, so this is ready for review. I submitted another round on 3.11 and trunk with the latest version and will update the results here when ready.

* [3.11 patch|https://github.com/pauloricardomg/cassandra/tree/3.11-13948]
* [trunk patch|https://github.com/pauloricardomg/cassandra/tree/trunk-13948]

Please let me know what do you think.","01/Nov/17 17:02;llambiel;I tried your patch on 3.11.2 and got the following errors:


{code:java}
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,397 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@51f52b91) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@1358582595:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103504-big-Data.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,413 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@70a08046) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@632323950:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103503-big-Data.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,413 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5d6161ea) to class org.apache.cassandra.io.util.FileHandle$Cleanup@1594052942:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103502-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,429 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2bd55858) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@230164803:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103502-big-Data.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,429 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5b00472f) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@508355616:Memory@[7f6b54130b10..7f6b54136f10) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,429 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1f5a7829) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@1390774416:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@b8594dc) to class org.apache.cassandra.io.util.FileHandle$Cleanup@1913719912:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103503-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3ec6a933) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1083770739:Memory@[7f6b5453ff30..7f6b54546330) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@671d48c8) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@496335375:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1611d7bf) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@515635345:Memory@[7f6b540f7dc0..7f6b540fe1c0) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@136db886) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@622788070:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3daa7ad5) to class org.apache.cassandra.io.util.FileHandle$Cleanup@2090103425:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103504-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2435c1d) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1348493438:Memory@[7f6b546ebd80..7f6b546f2180) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3e72d475) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@1638775104:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,431 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7916a2dc) to class org.apache.cassandra.io.util.FileHandle$Cleanup@715546128:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103505-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,446 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@25fe64d) to class org.apache.cassandra.io.util.MmappedRegions$Tidier@238299391:/var/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103505-big-Data.db was not released before the reference was garbage collected
ERROR [CompactionExecutor:71] 2017-11-01 17:51:36,872 CassandraDaemon.java:228 - Exception in thread Thread[CompactionExecutor:71,1,main]
java.lang.AssertionError: null
	at org.apache.cassandra.io.compress.CompressionMetadata$Chunk.<init>(CompressionMetadata.java:474) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:239) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.updateState(MmappedRegions.java:163) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:73) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:61) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.map(MmappedRegions.java:104) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:362) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:290) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:179) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:134) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.writers.MaxSSTableSizeWriter.realAppend(MaxSSTableSizeWriter.java:98) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:141) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:201) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:268) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$t
{code}

Since we are heavily impacted by this bug (see also CASSANDRA-13980), I'm ok to test as soon as you've an updated version of the patch.
","01/Nov/17 17:34;krummas;[~llambiel] is that with the patch posted today (or yesterday depending on your tz)?
","01/Nov/17 17:42;llambiel;Yes [~krummas], build including the latest patch.","01/Nov/17 22:28;pauloricardomg;bq. I tried your patch on 3.11.2 and got the following errors:

Thanks for reporting, can you attach the full debug.log leading to that? What is the compaction strategy of the block table? Was this a one-off error or is it repeating periodically?","01/Nov/17 22:53;llambiel;The strategy is LCS, node with 9 data locations. The error is repeating frequently. I'll attach a debug log tomorrow and make some additional tests.","01/Nov/17 23:08;pauloricardomg;bq.  I'll attach a debug log tomorrow and make some additional tests.

Thanks, since the exception seems related to early re-opening, it would be nice if during your tests you could try setting {{sstable_preemptive_open_interval_in_mb=-1}} on {{cassandra.yaml}} to check if the errors will go away (and return once you re-enable it).","02/Nov/17 20:17;llambiel;Did additional testing and wasn't able to reproduce :-/ 

I'll try the patch on more representative nodes in the coming days and report back any issue.","03/Nov/17 04:12;pauloricardomg;bq. Did additional testing and wasn't able to reproduce :-/ 

this looks similar to CASSANDRA-12743, so I wonder if it's an existing race that showed up due to the large compaction backlog after the deadlock was fixed.

bq. I'll try the patch on more representative nodes in the coming days and report back any issue.

sounds good, if you manage to reproduce it would be nice if you could change the log level of the {{org.apache.cassandra.db.compaction}} package to {{TRACE}}.","03/Nov/17 08:15;llambiel;Ok I was able to reproduce it on one node. I've attached the trace log. It's unfiltered since I didn't managed to filter only to org.apache.cassandra.db.compaction



","03/Nov/17 21:08;llambiel;I've deployed the patch on a few big nodes. I've not seen the error popping up so far.

However I'm still facing issues with compactions. These are big nodes with with a big CF, holding many SSTables and pending compactions. According the thread dump it seems to be stuck around getNextBackgroundTask. Compactions are still being processed for the other keyspace. Beside that the node is running normally. Some nodetool commands takes time to proceed like compactionstats. Debug log doesn't show any error.

{code:java}
CREATE TABLE blobstore.block (
    inode uuid,
    version timeuuid,
    block bigint,
    offset bigint,
    chunksize int,
    payload blob,
    PRIMARY KEY ((inode, version, block), offset)
) WITH CLUSTERING ORDER BY (offset ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy', 'enabled': 'true', 'tombstone_compaction_interval': '60', 'tombstone_threshold': '0.2', 'unchecked_tombstone_compaction': 'false'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 172000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}


{code:java}
Keyspace : blobstore
	Read Count: 97019
	Read Latency: 2.4842547026871027 ms.
	Write Count: 472590
	Write Latency: 0.060107954040500226 ms.
	Pending Flushes: 0
		Table: block
		SSTable count: 43373
		SSTables in each level: [18890/4, 115/10, 198/100, 1905/1000, 9451, 12814, 0, 0, 0]
		Space used (live): 4839933810943
		Space used (total): 4839933815913
		Space used by snapshots (total): 0
		Off heap memory used (total): 3273703284
		SSTable Compression Ratio: 0.9416884172984209
		Number of partitions (estimate): 2925826
		Memtable cell count: 41542
		Memtable data size: 2631688187
		Memtable off heap memory used: 2638649871
		Memtable switch count: 7
		Local read count: 87281
		Local read latency: 2.186 ms
		Local write count: 465591
		Local write latency: 0.124 ms
		Pending flushes: 0
		Percent repaired: 4.01
		Bloom filter false positives: 297882
		Bloom filter false ratio: 0.69198
		Bloom filter space used: 5111208
		Bloom filter off heap memory used: 4764232
		Index summary off heap memory used: 3360917
		Compression metadata off heap memory used: 626928264
		Compacted partition minimum bytes: 61
		Compacted partition maximum bytes: 186563160
		Compacted partition mean bytes: 1797922
		Average live cells per slice (last five minutes): 8.641592920353983
		Maximum live cells per slice (last five minutes): 258
		Average tombstones per slice (last five minutes): 1.0
		Maximum tombstones per slice (last five minutes): 1
		Dropped Mutations: 0

{code}

{code:java}
nodetool compactionstats
pending tasks: 3362
- blobstore.block: 3362
{code}","06/Nov/17 13:33;krummas;[~llambiel] I would guess you also need to apply CASSANDRA-13215 for that many sstables, but I doubt both these apply cleanly now, hopefully we'll get them in real soon",06/Nov/17 14:07;llambiel;Thanks [~krummas] The patches from [~pauloricardomg] already reduced the startup time by at least 10x,"07/Nov/17 09:06;pauloricardomg;bq. Ok I was able to reproduce it on one node. I've attached the trace log. It's unfiltered since I didn't managed to filter only to org.apache.cassandra.db.compaction

I wasn't able to track down the root cause of this condition from the logs, but a similar issue was reported on CASSANDRA-12743, so I think this is some kind of race condition showing up due to the amount of concurrent compactions happening and is not a consequence of this fix, so I prefer to investigate this separately. If you still see this issue please feel free to reopen CASSANDRA-12743 with details.

bq. However I'm still facing issues with compactions. These are big nodes with with a big CF, holding many SSTables and pending compactions. According the thread dump it seems to be stuck around getNextBackgroundTask. Compactions are still being processed for the other keyspace. Beside that the node is running normally. Some nodetool commands takes time to proceed like compactionstats. Debug log doesn't show any error.

After having a look at the thread dump, it turns out that my previous patch generated a lock contention between compaction and cleanup, because each removed SSTable from cleanup generated a {{SSTableDeletingNotification}} and my previous patch submitted a new compaction task after each received notification which competed with the next {{SSTableDeletingNotification}} for the {{writeLock}} - making things slow overall, so I updated the patch to only submit a new compaction after receiving a flush notification as it was before, so this should be fixed now. [~llambiel]  would you mind trying the latest version now?

[~krummas] this should be ready for review now, the latest version already got a clean CI run, but I resubmitted a new internal CI run after doing the minor fix above and will update here when ready.

Summary of changes:
1) Reload compaction strategies when JBOD disk boundary changes ([commit|https://github.com/pauloricardomg/cassandra/commit/6cab7e0a31a638cc4a957c4ecfa592035d874058])
2) Ensure compaction strategies do not loop indefinitely when not able to acquire Tracker lock ([commit|https://github.com/pauloricardomg/cassandra/commit/3ef833d1e56c25f67bc8a3b49acf97b2efdf401d])
3) Only enable compaction strategies after gossip settles to prevent unnecessary relocation work ([commit|https://github.com/pauloricardomg/cassandra/commit/eaf63dc3d52566ce0c4f91bbfec478305597f014])
4) Do not reload compaction strategies when receiving notifications and log warning when an SSTable is added multiple times to LCS ([commit|https://github.com/pauloricardomg/cassandra/commit/3e61df70025e704ee0c9d6ee8754ccdd38f5ab6d])

Patches
* [3.11|https://github.com/pauloricardomg/cassandra/tree/3.11-13948]
* [trunk|https://github.com/pauloricardomg/cassandra/tree/trunk-13948]

I wonder if now that CSM caches the disk boundaries we can make the handling of notifications use the readLock instead of the writeLock, to reduce contention when there is a high number of concurrent compactors, do you see any potential problems with this? Even if the notification handling races with getNextBackground task, as long as the individual compaction strategies are synchronized getNextBackground task should get a consistent view of the strategy sstables when there is a concurrent notification from the tracker.","08/Nov/17 17:13;llambiel;Compactions are now processing as expected with your latest patch :)

However I'm facing an issue with nodetool cleanup, dunno if it is related or not.

Starting a cleanup cancel the ongoing compactions (which is expected from my understanding) and then get lost. Not performing any cleanup nor processing the pending compactions. 1 thread is using 100% of a core all the time. The log doesn't show any error. I've attached a thread dump.

I'm happy to open another Jira if it's not related.","08/Nov/17 20:44;pauloricardomg;bq. Starting a cleanup cancel the ongoing compactions (which is expected from my understanding) and then get lost. Not performing any cleanup nor processing the pending compactions. 1 thread is using 100% of a core all the time. The log doesn't show any error. I've attached a thread dump.

this looks similar to CASSANDRA-13362 and unrelated to this issue. looking at the jstack it seems like there is some kind of deadlock on logback, do you have enough disk space in your log dir? did you set your log level back to DEBUG from TRACE level (which can be quite verbose and generate too many log rotations - what may be contributing to the logback problem)?",09/Nov/17 03:55;pauloricardomg;Just a heads up that internal CI looks good for the latest version of the patch (there are a bunch of unrelated failures on trunk). I will rebase this on top of CASSANDRA-13215 after the first round of review.,"10/Nov/17 08:29;krummas;Just had a first pass of the commits, and in general it looks good, I added a few comments on github

In general it feels a bit inconsistent about when it calls the {{CSM#maybeReload()}} - with 13215 in, perhaps we could just always call it as it will be cheap? I'll do a more thorough review of that last commit once it has been rebased on top of 13215","13/Nov/17 03:14;pauloricardomg;bq. Just had a first pass of the commits, and in general it looks good, I added a few comments on github

Thanks for the review! Addressed github comments and rebased this on top of CASSANDRA-13215 on this v2 branch: [3.11-13948-v2|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:3.11-13948-v2]

I forgot to link the dtest branch here last time: [13948-dtest|https://github.com/pauloricardomg/cassandra-dtest/tree/13948].

Will submit a new CI and post results here when ready.

bq. In general it feels a bit inconsistent about when it calls the CSM#maybeReload() - with 13215 in, perhaps we could just always call it as it will be cheap? I'll do a more thorough review of that last commit once it has been rebased on top of 13215

You mean not calling it on {{handleNotification}} or inconsistent use in general across {{CSM}}? Calling {{maybeReload}} on {{handleNotification}} is unnecessary, because if the CSM strategy is reloaded then the notification is no longer necessary - since all added or removed SSTables will already be in the correct place after reload - so we just call it whenever building a new compaction task - perhaps we could try reloading on {{handleNotification}} and avoid delivering the notification if the reload is successful?","13/Nov/17 08:21;pauloricardomg;Testall passed with no failures, and [dtest failures|https://issues.apache.org/jira/secure/attachment/12897298/dtest13948.png] look unrelated.","23/Nov/17 07:59;krummas;I'm +1 on the code, just a few small comments;

* A few unused imports in CSM
* Could we update the class comment in CSM to cover the locking strategy (when do we need the read / write lock) and when maybeReload is called?","27/Nov/17 19:06;pauloricardomg;Thanks for the review!

After rebasing this on top of CASSANDRA-13215 and addressing your latest comments, I noticed a few things which could be improved and did the following updates:
* Since blacklisting a directory will refresh the disk boundaries, we only need to reload strategies when the disk boundary changes or the table parameters change. To avoid equals comparison every time we call {{maybeReload}}, I moved the {{isOutOfDate}} check from the {{DiskBoundaryManager}} to the {{DiskBoundaries}} object - which is invalidated when there are any boundary changes. ([commit|https://github.com/pauloricardomg/cassandra/commit/662cd063ca2e1c382ba3cd5dc8032b0d3f12683c])
* I thought that it no longer makes sense to expose the compaction strategy index to outside the compaction strategy manager since it's possible to get the correct disk placement directly from {{CFS.getDiskBoundaries}}. This should prevent races when the {{CompactionStrategyManager}} reloads boundaries between successive calls to {{CSM.getCompactionStrategyIndex}}. [This commit|https://github.com/pauloricardomg/cassandra/commit/abd1340b000d4596d71f00e5de8507de967ee7a5] updates {{relocatesstables}} and {{scrub}} to use {{CFS.getDiskBoundaries}} instead, and make {{CSM.getCompactionStrategyIndex}} private.
* I found it a bit hard to reason about when to use {{maybeReload}} to write the documentation and made its use consistent across {{CompactionStrategyManager}} on [this commit|https://github.com/pauloricardomg/cassandra/commit/c0926e99edb1ffdcda16640eda6faf8e78da9e46]) (as you suggested before) along with the documentation. I kept the previous call to {{maybeReload}} from {{ColumnFamilyStore.reload}}, but we could probably avoid this and make {{maybeReload}} private-only as this is being called on pretty much every operation.

It feels like we can simplify this and get rid of these locks altogether (or at least greatly reduce their scope) by encapsulating the disk boundaries and compaction strategies in an immutable object accessed with an atomic reference and pessimistically cancel any tasks with an old placement when the strategies are reloaded. This is a significant refactor of {{CompactionStrategyManager}} so we should probably do it another ticket.

I submitted internal CI with the [latest branch|https://github.com/pauloricardomg/cassandra/tree/3.11-13948] and will post the results here when ready. I will create a trunk version after this follow-up is reviewed.","28/Nov/17 01:04;pauloricardomg;Canceling patch while I address some test failures, will resubmit when CI is green.","30/Nov/17 00:57;pauloricardomg;There were test failures on:
* testall: {{CompactionsCQLTest.testSetLocalCompactionStrategy}} and {{testTriggerMinorCompactionSTCSNodetoolEnabled}}
* dtest: {{disk_balance_test.TestDiskBalance.disk_balance_bootstrap_test}}

{{testSetLocalCompactionStrategy}} and {{testTriggerMinorCompactionSTCSNodetoolEnabled}} were failing because when the strategy was updated via JMX, these manually set configurations were not surviving the compaction strategy reload - this was not introduced by this, but would also happen in case a directory was blacklisted before. This was fixed [on this commit|https://github.com/pauloricardomg/cassandra/commit/11c9a130d9cb7a6cfc5a039fdf79963f7e779d08]

While investigating why the strategies were reloaded even without a ring change on the tests above, I noticed that {{Keyspace.createReplicationStrategy}} was being called multiple times (on {{Keyspace}} construction and {{setMetadata}}), so I updated to only invalidate the disk boundaries when the replication settings actually change ([here|https://github.com/pauloricardomg/cassandra/commit/8a398a5d0d261178547946ac4e457f9abeb90f18]).

After the fix above, {{disk_balance_bootstrap_test}} started failing with imbalanced disks because the disk boundaries were not being invalidated after the joining node broadcasted its tokens via gossip, so {{TokenMetadata.getPendingRanges(keyspace, FBUtilities.getBroadcastAddress())}} was returning empty during disk boundary creation and causing imbalance. This is not failing on trunk because the double invalidation above during keyspace creation was causing the compaction strategy manager to reload the strategies with the correct ring placement during streaming. The fix to this is to invalidate the cached ring after gossiping the local tokens ([here|https://github.com/pauloricardomg/cassandra/commit/007d596ffe0c5f965cf398646c52daa8f73c5c46]).

This made me realize that when replacing a node with the same address, even though the node is on bootstrap mode, it doesn't have any pending range, because it sets its token to normal state during bootstrap, what will cause its boundaries to not be computed correctly. I added a [dtest|https://github.com/pauloricardomg/cassandra-dtest/commit/8d48b166c9bfce51f9ab6c3abd73dfd4779a7c04] to show this and a [fix|https://github.com/pauloricardomg/cassandra/commit/6efd9cd454ce2fbfd40e592b6aaeda9debdb1c2b].

Finally, I didn't find a good reason to pass {{ColumnFamilyStore}} as argument to {{getDiskBoundaries}}, so I updated it to make it a field instead ([here|https://github.com/pauloricardomg/cassandra/commit/5df0d5ebed67aaae6ef9350d25b602af2a1702cf]).

I submitted internal CI, and testall is green and dtest failures [seem unrelated|https://issues.apache.org/jira/secure/attachment/12899922/dtest2.png]. Setting to patch available as this should be ready for a new round of review now. Thanks!","30/Nov/17 08:00;krummas;This ticket is getting quite big and very hard to review

Could we split out all the pre-existing bugs in other tickets and get them committed separately? Especially [this|https://github.com/pauloricardomg/cassandra/commit/007d596ffe0c5f965cf398646c52daa8f73c5c46] as it involves tokenmetadata.

bq. Finally, I didn't find a good reason to pass ColumnFamilyStore as argument to getDiskBoundaries
we shouldn't leak {{this}} from the CFS constructor, I know we do it with CSM, but that is an ancient leftover that we should probably refactor away

edit: seems we leak this in many places in the cfs constructor, but lets avoid it in this case as it really doesn't hurt passing cfs to the methods","30/Nov/17 13:13;pauloricardomg;bq. This ticket is getting quite big and very hard to review

I tried to make things easier by splitting in different commits, but I agree it became a bit complicated for review.

bq. Could we split out all the pre-existing bugs in other tickets and get them committed separately? Especially this as it involves tokenmetadata.

The problem is that some bugs (even though were pre-existing) only started showing up after this, so they have a dependency on this. 

I reorganized [this branch|https://github.com/pauloricardomg/cassandra/tree/3.11-13948] to keep only things essential to this ticket, created CASSANDRA-14079 and CASSANDRA-14081 with unrelated minor fixes, and will create two follow-up tickets which depend on this.

This should be ready for review now, please let me know if some of the changes are not clear for you and needs better explanation. CI looked clean before the reorganization, but I will resubmit with the essential ticket just to make sure we didn't miss anything:

* [3.11 patch|https://github.com/pauloricardomg/cassandra/tree/3.11-13948]
* [dtest|https://github.com/pauloricardomg/cassandra-dtest/tree/13948]",01/Dec/17 13:50;pauloricardomg;CI looks good (unrelated [testall|https://issues.apache.org/jira/secure/attachment/12900221/13948testall.png] and [dtest|https://issues.apache.org/jira/secure/attachment/12900222/13948dtest.png] failures).,04/Dec/17 14:08;krummas;+1 on the 3.11 patch,"04/Dec/17 14:09;krummas;need to check the trunk patch as well, cancelling ""ready to commit"" (i hope)","06/Dec/17 14:35;pauloricardomg;bq. +1 on the 3.11 patch

Thanks for the review!

bq. need to check the trunk patch as well, cancelling ""ready to commit"" (i hope)

The merge went smoothly, most of the conflicts were related to CASSANDRA-9143, so I [updated|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-13948#diff-f9c882c974db60a710cf1f195cfdb801R113] {{CompactionStrategyManagerTest}} to mark a subset of sstables as repaired and pending repair to make sure sstables are being assigned the correct strategies for repaired and pending repair sstables.

However, there were 2 test failures in the trunk branch after the merge:
1. [testSetLocalCompactionStrategy|https://github.com/pauloricardomg/cassandra/blob/41416af426c41cdd38e157f38fb440342bca4dd0/test/unit/org/apache/cassandra/db/compaction/CompactionsCQLTest.java#L163]
2. [disk_balance_bootstrap_test|https://github.com/pauloricardomg/cassandra-dtest/blob/73d7a8e1deb5eab05867d804933621062c2f6762/disk_balance_test.py#L34]

1. was failing [here|https://github.com/pauloricardomg/cassandra/blob/41416af426c41cdd38e157f38fb440342bca4dd0/test/unit/org/apache/cassandra/db/compaction/CompactionsCQLTest.java#L175] because {{ALTER TABLE t WITH gc_grace_seconds = 1000}} was causing the manually set compaction strategy to be replaced by the strategy defined on the schema. After investigation, it turned out that the disk boundaries were being invalidated due to the schema reload (introduced by CASSANDRA-9425), and {{maybeReload(TableMetadata)}} was causing the compaction strategies to be reloaded with the schema settings instead of the manually set settings. In order to fix this, I split {{maybeReload}} in the original {{maybeReload(TableMetadata)}}, which should be called externally by {{ColumnFamilyStore}} and only reloads the strategies when the schema table parameters change, and {{maybeReloadDiskBoundaries}} which is used internally and reloads the compaction strategies with the same table settings when the disk boundaries are invalidated [here|https://github.com/apache/cassandra/commit/de5916e7c4f37736d5e1d06f0fc2b9c082b6bb99].

2.since the local ranges are not defined when the bootstrapping node starts, the disk boundaries are empty, but before CASSANDRA-9425 the boundaries were invalidated during keyspace construction ([here|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/config/Schema.java#L388]), so the correct boundaries were used during streaming. After CASSANDRA-9425  the boundaries were no longer reloaded during keyspace creation ([here|https://github.com/apache/cassandra/blob/4c80eeece37d79f434078224a0504400ae10a20d/src/java/org/apache/cassandra/schema/Schema.java#L138]), so the empty boundaries were used during streaming and the disks were imbalanced. I had exactly the same problem on CASSANDRA-14083 ([here|https://issues.apache.org/jira/browse/CASSANDRA-14083?focusedCommentId=16272918&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16272918]). The solution is to invalidate the disk boundaries after the tokens are set during bootstrap ([here|https://github.com/apache/cassandra/commit/a37bbda45142e1b351908a4ff5196eb08e92082b]).

After these two fixes, the tests were passing (failures seem unrelated - test screenshots from internal CI below):

||3.11||trunk||dtest||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-13948]|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-13948]|[branch|https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:13948]|
|[testall|https://issues.apache.org/jira/secure/attachment/12900864/3.11-13948-testall.png]|[testall|https://issues.apache.org/jira/secure/attachment/12900862/trunk-13948-testall.png]|
|[dtest|https://issues.apache.org/jira/secure/attachment/12900865/3.11-13948-dtest.png]|[dtest|https://issues.apache.org/jira/secure/attachment/12900863/trunk-13948-dtest.png]|","06/Dec/17 16:35;krummas;this LGTM, +1, just one comment and want to mention one possible issue I just realized:
{code}
        // If reloaded, SSTables will be placed in their correct locations
        // so there is no need to process notification
        if (maybeReloadDiskBoundaries())
            return;
{code}
so the code above, being run in {{handleListChangedNotification}}, {{handleRepairStatusChangedNotification}}, {{handleDeletingNotification}} and {{handleFlushNotification}} in CSM, should this call be run inside the read lock? My concern is that if something refreshes the boundaries right before the call, we might double-add/remove sstables in the compaction strategies. This is handled by the fix you made in CASSANDRA-14079, but checking (or re-checking) with the lock should make sure we avoid the double-adding at all I think. I guess it would need some refactoring since we can't upgrade the read lock to a write lock. This ticket has dragged on long enough, so we could open a new ticket for this I guess since I don't think it will be a problem currently.

trunk comment, feel free to address on commit;
* lets remove the deprecated {{public AbstractCompactionTask getUserDefinedTask(Collection<SSTableReader> sstables, int gcBefore)}} in CSM (and the {{validateForCompaction}} boolean to {{List<AbstractCompactionTask> getUserDefinedTasks(Collection<SSTableReader> sstables, int gcBefore, boolean validateForCompaction)}})","08/Dec/17 18:51;pauloricardomg;bq. My concern is that if something refreshes the boundaries right before the call, we might double-add/remove sstables in the compaction strategies.

Good catch, even though unlikely this is indeed a possible scenario.

bq. This ticket has dragged on long enough, so we could open a new ticket for this I guess since I don't think it will be a problem currently.

Agreed, created CASSANDRA-14103 to fix that and I commented a possible fix proposal there.

bq. trunk comment, feel free to address on commit;

Done, and I got too excited and removed it from 3.11 as well by mistake, so reinstated it on this commit: {{16bcbb9256392dc5364f2bd592f45649080935dc}}

Committed as {{25e46f05294fd42c111f2f1d5881082d97c572ea}} to cassandra-3.11 and merge up to master. Thanks for the review!",11/Dec/17 20:23;pauloricardomg;Committed dtest as {{debe3780a4694c978f2516e565e071782dc7b2c8}}. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Properly close StreamCompressionInputStream to release any ByteBuf,CASSANDRA-13906,13105265,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,jasobrown,jasobrown,26/Sep/17 23:38,12/Mar/19 14:15,13/Mar/19 22:35,03/Oct/17 19:24,4.0,,,,,,,,,,0,,,,"When running dtests for trunk (4.x) that perform some streaming, sometimes a {{ByteBuf}} is not released properly, and we get this error in the logs (causing the dtest to fail):

{code}
ERROR [MessagingService-NettyOutbound-Thread-4-2] 2017-09-26 13:42:37,940 Slf4JLogger.java:176 - LEAK: ByteBuf.release() was not called before it's garbage-collected. Enable advanced leak reporting to find out where the leak occurred. To enable advanced leak reporting, specify the JVM option '-Dio.netty.leakDetection.level=advanced' or call ResourceLeakDetector.setLevel() See http://netty.io/wiki/reference-counted-objects.html for more information.
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-29 16:34:18.786,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 03 19:24:00 UTC 2017,,,,,,0|i3kkyv:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,"26/Sep/17 23:38;jasobrown;The cause of the leaked {{ByteBuf}} was due to a wrapping class not being closed, and thus never had the chance to call {{release()}} on the {{ByteBuf}}.

||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/netty-leak]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/338/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/netty-leak]|

I ran one of the failing dtests (repair_tests/incremental_repair_test.py:TestIncRepair.multiple_repair_test) on my laptop, and without the patch could reproduce within 2-3 runs. With the patch, I ran it fifty time and could not reproduce.

[~aweisberg] Would you mind reviewing?","29/Sep/17 16:34;aweisberg;So to bikeshed this all to hell. Wouldn't the most idiomatic way to do this be to have TrackedDataInputPlus implement Closable and use try with resources?

In fact try with resources will let you declare all the Closeable things in the same try block and clean up if one of the things you are using to wrap throws in it's constructor preventing the wrapped resource from leaking.","29/Sep/17 16:49;jasobrown;bq. Wouldn't the most idiomatic way to do this be to have {{TrackedDataInputPlus}} implement {{Closable}} and use try with resources

I thought about that when I was fixing this, but  {{TrackedDataInputPlus}} wraps a {{DataInput}}, which does not declare a {{#close()}} method. In a {{TrackedDataInputPlus#close()}} I could check if the wrapped instance implements {{Closeable}} and invoke it if it does. wdyt?

There are a couple of other uses of  {{TrackedDataInputPlus}} ({{HintMessage}}, {{IndexedEntry}} called on load saved cache path), but they should not be affected by {{TrackedDataInputPlus}} implementing {{Closeable}} as the are not allocated via try-with-resources. 

Note: if we choose to make this change, which is reasonable, I can also cleanup {{CompressedStreamReader}} to also allocate {{TrackedDataInputPlus}} in a try-with-resources - it has the same concerns as {{StreamReader}} that you raised. Branch coming shortly ...",29/Sep/17 17:02;aweisberg;If you implement Closable it's going to cause warnings any place that doesn't use try with resources. Given the other usages I think what you have done is probably fine although you can still switch to try with resources just for {{StreamCompressionInputStream}}.,"29/Sep/17 18:01;jasobrown;OK, I pushed up a fresh branch using try-with-resources for both {{StreamReader}} and {{CompressedStreamReader}}. utests and dtests have been been kicked off, as well.","29/Sep/17 19:55;aweisberg;Is releasing references to a buffer using the {{refCnt()}} a good idea? Isn't that kind of abusing the idiom of reference counting by not counting?

Otherwise looks good.

","29/Sep/17 22:24;jasobrown;bq. Isn't that kind of abusing the idiom of reference counting by not counting?

That is true to a degree, but I'm never sure if any code would, in some broken way, become executed twice and fail on the refCnt decrement and mask some other problem. I could set the buffer to null after a simple call to {{release()}}. wdyt?","02/Oct/17 22:27;aweisberg;I agree reference counting in Java is fraught due to the lack of Destructors and other plumbing.
 
So do we always expect the refcnt to be one or some known number? Then we should assert that at runtime and take an error path if it's not true (after releasing the resources) or maybe just do rate limited logging.

If use after free is a concern we might want to make sure the buffer is set to null so we get an NPE instead of a segfault. Then the  code incorrectly using the buffer will signal an error as well.",03/Oct/17 18:29;aweisberg;+1,"03/Oct/17 18:58;jasobrown;[~aweisberg] and I discussed offline, and I'll revert the overly cautious (and perhaps incorrect) change around that refCnt.",03/Oct/17 19:24;jasobrown;committed as sha {{982ab93a2f8a0f5c56af9378f65d3e9e430000b9}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integer overflows with Amazon Elastic File System (EFS),CASSANDRA-13808,13097643,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,blerer,betalb,betalb,25/Aug/17 15:54,12/Mar/19 14:15,13/Mar/19 22:35,29/Sep/17 07:58,3.11.1,4.0,,,,Legacy/Core,Local/Config,,,,0,,,,"Integer overflow issue was fixed for cassandra 2.2, but in 3.8 new property was introduced in config that also derives from disk size  {{cdc_total_space_in_mb}}, see CASSANDRA-8844

It should be updated too https://github.com/apache/cassandra/blob/6b7d73a49695c0ceb78bc7a003ace606a806c13a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java#L484",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13067,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-25 19:44:21.427,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 29 07:58:27 UTC 2017,,,,,,0|i3jaof:,9223372036854775807,,,,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,25/Sep/17 19:44;blerer;I pushed a patch for the problem [here|https://github.com/apache/cassandra/compare/trunk...blerer:13808-3.11].,"26/Sep/17 08:19;ifesdjeen;+1, LGTM

For the record, the fix is the same as in [CASSANDRA-13067].",29/Sep/17 07:57;blerer;Thanks for the review.,29/Sep/17 07:58;blerer;Committed into cassandra-3.11 at 0bc45aa46766625698e6e4c47085dfe94766c7df and merged into trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in cqlsh_tests.cqlsh_tests.CqlLoginTest.test_list_roles_after_login,CASSANDRA-13847,13100296,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,adelapena,jkni,jkni,06/Sep/17 21:02,12/Mar/19 14:15,13/Mar/19 22:35,13/Sep/17 17:05,2.1.x,2.2.x,,,,Legacy/Testing,Legacy/Tools,,,,0,test-failure,,,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/546/testReport/cqlsh_tests.cqlsh_tests/CqlLoginTest/test_list_roles_after_login

This test was added for [CASSANDRA-13640]. The comments seem to indicated this is only a problem on 3.0+, but the added test certainly seems to reproduce the problem on 2.1 and 2.2. Even if the issue does affect 2.1/2.2, it seems insufficiently critical for 2.1, so we need to limit the test to run on 2.2+ at the very least, possibly 3.0+ if we don't fix the cause on 2.2.

Thoughts [~adelapena]?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13640,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-07 08:04:16.507,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 13 17:04:52 UTC 2017,,,,,,0|i3jqgn:,9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Sep/17 08:04;adelapena;Right, the bug also affects 2.1 and 2.2.

Here is the patch to solve it in 2.2 and the patch for dtests with the {{@since}} tag:
||[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...adelapena:13847-2.2]||[dtest|https://github.com/apache/cassandra-dtest/compare/master...adelapena:13847]||

Regarding 2.1, I agree this is indeed insufficiently critical, and there's a clear workaround (exiting cqlsh and logging back).",12/Sep/17 09:39;jasonstack;+1 for the change. sorry the overlook last time.,"13/Sep/17 17:04;adelapena;Thanks for the review.

Committed to 2.2 as [43e2a107072bc86c0e26bc2036a61a9ad600f213|https://github.com/apache/cassandra/commit/43e2a107072bc86c0e26bc2036a61a9ad600f213].

Dtest patch committed as [3435b0f2121fa4c6099098e562d0bf5f4bd78d5f|https://github.com/apache/cassandra-dtest/commit/3435b0f2121fa4c6099098e562d0bf5f4bd78d5f].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Header only commit logs should be filtered before recovery,CASSANDRA-13918,13105867,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,28/Sep/17 21:12,12/Mar/19 14:15,13/Mar/19 22:35,29/Sep/17 22:39,3.0.15,3.11.1,4.0,,,,,,,,0,,,,"Commit log recovery will tolerate commit log truncation in the most recent log file found on disk, but will abort startup if problems are detected in others. 

Since we allocate commit log segments before they're used though, it's possible to get into a state where the last commit log file actually written to is not the same file that was most recently allocated, preventing startup for what should otherwise be allowable incomplete final segments.

Excluding header only files on recovery should prevent this from happening.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-29 18:10:26.763,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 29 22:39:28 UTC 2017,,,,,,0|i3komv:,9223372036854775807,,,,,,,,beobal,beobal,,,,,,,,,,"28/Sep/17 22:33;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/13918-3.0]|[utest|https://circleci.com/gh/bdeggleston/cassandra/127]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/342/]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/13918-3.11]|[utest|https://circleci.com/gh/bdeggleston/cassandra/126]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/343/]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/13918-trunk]|[utest|https://circleci.com/gh/bdeggleston/cassandra/128]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/344/]|",29/Sep/17 18:10;beobal;LGTM. Minuscule nit: there's an unused import in {{CommitLogReader}} on the 3.11 branch.,"29/Sep/17 18:11;bdeggleston;Thanks, will fix on commit","29/Sep/17 21:36;bdeggleston;utests look good on all branches, dtests look good on 3.0 & 3.11. trunk dtests aborted, but looks like it was due to an existing issue.",29/Sep/17 22:39;bdeggleston;committed as {{95839aae2fde28fa29b16741de6bd52c0697843f}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RangeTombstoneMarker and PartitionDeletion is not properly included in MV,CASSANDRA-13787,13096892,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,23/Aug/17 02:03,12/Mar/19 14:15,13/Mar/19 22:35,20/Sep/17 13:20,3.0.15,3.11.1,4.0,,,Feature/Materialized Views,Legacy/Local Write-Read Paths,,,,0,,,,"Found two problems related to MV tombstone. 

1. Range-tombstone-Marker being ignored after shadowing first row, subsequent base rows are not shadowed in TableViews.

    If the range tombstone was not flushed, it was used as deleted row to shadow new updates. It works correctly.
    After range tombstone was flushed, it was used as RangeTombstoneMarker and being skipped after shadowing first update. The bound of RangeTombstoneMarker seems wrong, it contained full clustering, but it should contain range or it should be multiple RangeTombstoneMarkers for multiple slices(aka. new updates)

-2. Partition tombstone is not used when no existing live data, it will resurrect deleted cells. It was found in 11500 and included in that patch.- (Merged in CASSANDRA-11500)


In order not to make 11500 patch more complicated, I will try fix range/partition tombstone issue here.


{code:title=Tests to reproduce}
    @Test
    public void testExistingRangeTombstoneWithFlush() throws Throwable
    {
        testExistingRangeTombstone(true);
    }

    @Test
    public void testExistingRangeTombstoneWithoutFlush() throws Throwable
    {
        testExistingRangeTombstone(false);
    }

    public void testExistingRangeTombstone(boolean flush) throws Throwable
    {
        createTable(""CREATE TABLE %s (k1 int, c1 int, c2 int, v1 int, v2 int, PRIMARY KEY (k1, c1, c2))"");

        execute(""USE "" + keyspace());
        executeNet(protocolVersion, ""USE "" + keyspace());

        createView(""view1"",
                   ""CREATE MATERIALIZED VIEW view1 AS SELECT * FROM %%s WHERE k1 IS NOT NULL AND c1 IS NOT NULL AND c2 IS NOT NULL PRIMARY KEY (k1, c2, c1)"");

        updateView(""DELETE FROM %s USING TIMESTAMP 10 WHERE k1 = 1 and c1=1"");


        if (flush)
            Keyspace.open(keyspace()).getColumnFamilyStore(currentTable()).forceBlockingFlush();

        String table = KEYSPACE + ""."" + currentTable();
        updateView(""BEGIN BATCH "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 0, 0, 0, 0) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 0, 1, 0, 1) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 0, 1, 0) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 1, 1, 1) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 2, 1, 2) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 3, 1, 3) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 2, 0, 2, 0) USING TIMESTAMP 5; "" +
                ""APPLY BATCH"");

        assertRowsIgnoringOrder(execute(""select * from %s""),
                                row(1, 0, 0, 0, 0),
                                row(1, 0, 1, 0, 1),
                                row(1, 2, 0, 2, 0));
        assertRowsIgnoringOrder(execute(""select k1,c1,c2,v1,v2 from view1""),
                                row(1, 0, 0, 0, 0),
                                row(1, 0, 1, 0, 1),
                                row(1, 2, 0, 2, 0));
    }

    @Test
    public void testExistingParitionDeletionWithFlush() throws Throwable
    {
        testExistingParitionDeletion(true);
    }

    @Test
    public void testExistingParitionDeletionWithoutFlush() throws Throwable
    {
        testExistingParitionDeletion(false);
    }

    public void testExistingParitionDeletion(boolean flush) throws Throwable
    {
        // for partition range deletion, need to know that existing row is shadowed instead of not existed.
        createTable(""CREATE TABLE %s (a int, b int, c int, d int, PRIMARY KEY (a))"");

        execute(""USE "" + keyspace());
        executeNet(protocolVersion, ""USE "" + keyspace());

        createView(""mv_test1"",
                   ""CREATE MATERIALIZED VIEW %s AS SELECT * FROM %%s WHERE a IS NOT NULL AND b IS NOT NULL PRIMARY KEY (a, b)"");

        Keyspace ks = Keyspace.open(keyspace());
        ks.getColumnFamilyStore(""mv_test1"").disableAutoCompaction();

        execute(""INSERT INTO %s (a, b, c, d) VALUES (?, ?, ?, ?) using timestamp 0"", 1, 1, 1, 1);
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""), row(1, 1, 1, 1));

        // remove view row
        updateView(""UPDATE %s using timestamp 1 set b = null WHERE a=1"");
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""));
        // remove base row, no view updated generated.
        updateView(""DELETE FROM %s using timestamp 2 where a=1"");
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""));

        // restor view row with b,c column. d is still tombstone
        updateView(""UPDATE %s using timestamp 3 set b = 1,c = 1 where a=1""); // upsert
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""), row(1, 1, 1, null));
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13299,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-05 12:56:24.968,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 21 04:57:24 UTC 2017,,,,,,0|i3j62v:,9223372036854775807,,,,,,,,slebresne,slebresne,,,,,,,,,,"23/Aug/17 09:46;jasonstack;The first problem is not exactly a MV bug, it's SSTableReader issue of handling multiple slices which is probably only used by MV. The existing code consumes the {{original RangeTombstoneMarker(eg. Deletion@c1=1@10)}} for the first slice to generate a RangeTombstoneMarker(eg. Deletion@c1=1&c1=0@10) with first slice's clustering. So there is no tombstones generated for other slices. The fix is to make sure tombstones are generated for each slices within the range of original RangeTombstoneMarker and do not close the openMarker til reading its paired closeMarker.

The second problem is that if there is no existing live data, empty existing row is given to the ViewUpdateGenerator which may resurrect deleted cells. The fix is to include current-open-deletion into the empty existing row. So view could shadow the deleted cells.


[Draft|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13787-3.0] patch base on cassandra-3.0 . This needs to be fixed in 3.0/3.11/trunk.

","05/Sep/17 12:56;slebresne;Sorry for the delay reviewing, but patch lgtm (though, to make extract sure we're on the same page, this appear to only include the fix for the ""first problem"" since the ""second problem"" seems to have been included in CASSANDRA-11500).

Have you run CI on this? Also, if you could provide branches for all branches, that would be amazing (the fix itself should merge up cleanly, but I suspect the tests may require a few minor updates).","05/Sep/17 12:58;jasonstack;Thanks for reviewing.

I will prepare branches for 3.0/3.1/trunk. I have removed the code for second issue since 11500 is merged..","06/Sep/17 15:25;jasonstack;| source |  test  |   dtest |
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13787-trunk] | [passed|https://circleci.com/gh/jasonstack/cassandra/571] | repair_tests.repair_test.TestRepair.simple_parallel_repair_test
repair_tests.repair_test.TestRepair.test_dead_sync_initiator
rebuild_test.TestRebuild.rebuild_ranges_test
repair_tests.repair_test.TestRepair.dc_repair_test
topology_test.TestTopology.simple_decommission_test
disk_balance_test.TestDiskBalance.disk_balance_decommission_test
repair_tests.repair_test.TestRepair.simple_sequential_repair_test
repair_tests.repair_test.TestRepair.thread_count_repair_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
cdc_test.TestCDC.test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space
cdc_test.TestCDC.test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space|
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13787-3.11] | [passed|https://circleci.com/gh/jasonstack/cassandra/573] |upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_30_test |
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13787-3.0] | [passed|https://circleci.com/gh/jasonstack/cassandra/572] | rebuild_test.TestRebuild.simple_rebuild_test
global_row_key_cache_test.TestGlobalRowKeyCache.functional_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_22_test
upgrade_internal_auth_test.TestAuthUpgrade.upgrade_to_30_test
auth_test.TestAuth.system_auth_ks_is_alterable_test|

Failing dtests are either failing on trunk or passed on local.
","20/Sep/17 08:51;slebresne;+1, nothing seems unrelated to this here.","20/Sep/17 13:20;slebresne;Alright, committed, thanks.",21/Sep/17 04:57;jasonstack;thanks for reviewing~,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix trigger example on 4.0,CASSANDRA-13796,13097331,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,cnlwsu,cnlwsu,cnlwsu,24/Aug/17 14:54,12/Mar/19 14:15,13/Mar/19 22:35,24/Aug/17 15:36,4.0,,,,,,,,,,0,,,,{{CFMetadata.cfName}} was moved to {{name}} not {{table}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-24 14:56:36.164,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 30 05:28:18 UTC 2017,,,,,,0|i3j8rr:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"24/Aug/17 14:56;githubbot;GitHub user clohfink opened a pull request:

    https://github.com/apache/cassandra/pull/140

    Fix trigger example for CASSANDRA-13796

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/clohfink/cassandra 13796

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/140.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #140
    
----

----
","24/Aug/17 15:36;jasobrown;Committed as sha {{2e5847d29bbdd45fd4fc73f071779d91326ceeba}}. Thanks!

This closes #140","30/Aug/17 05:26;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra/pull/140
  
    @clohfink  - looks like this was merged in 2e5847d29bbdd45fd4fc73f071779d91326ceeba , can you close?

","30/Aug/17 05:28;githubbot;Github user clohfink closed the pull request at:

    https://github.com/apache/cassandra/pull/140
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PendingRepairManager.getNextBackgroundTask throwing IndexOutOfBoundsException,CASSANDRA-13769,13095155,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,16/Aug/17 21:38,12/Mar/19 14:15,13/Mar/19 22:35,18/Aug/17 17:33,4.0,,,,,,,,,,0,,,,"If all the repair sessions managed by a PendingRepairManager are can be cleaned up and we call getNextBackgroundTask, we'll try to pull an element out of an empty list and throw an exception.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-17 11:03:47.127,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 18 17:33:07 UTC 2017,,,,,,0|i3ivhb:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"16/Aug/17 21:43;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13769]
[utest|https://circleci.com/gh/bdeggleston/cassandra/88]",17/Aug/17 11:03;krummas;+1,18/Aug/17 17:33;bdeggleston;committed as {{c066f126e20180ae0230b7d6a61666152149a79f}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in nodetool upgradesstables,CASSANDRA-13718,13089054,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jjirsa,hkroger,hkroger,21/Jul/17 14:22,12/Mar/19 14:15,13/Mar/19 22:35,17/Aug/17 23:07,,,,,,,,,,,0,,,,"When upgrading from 2.2.8 to Cassandra 3.11 we were able to upgrade all other sstables except 1 file on 3 nodes (out of 4). Those are related to 2 different tables.

Upgrading sstables fails with ConcurrentModificationException.

{code}
$ nodetool upgradesstables
error: null
-- StackTrace --
java.util.ConcurrentModificationException
	at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1211)
	at java.util.TreeMap$KeyIterator.next(TreeMap.java:1265)
	at org.apache.cassandra.utils.StreamingHistogram.flushHistogram(StreamingHistogram.java:168)
	at org.apache.cassandra.utils.StreamingHistogram.update(StreamingHistogram.java:124)
	at org.apache.cassandra.utils.StreamingHistogram.update(StreamingHistogram.java:96)
	at org.apache.cassandra.io.sstable.metadata.MetadataCollector.updateLocalDeletionTime(MetadataCollector.java:209)
	at org.apache.cassandra.io.sstable.metadata.MetadataCollector.update(MetadataCollector.java:182)
	at org.apache.cassandra.db.rows.Cells.collectStats(Cells.java:44)
	at org.apache.cassandra.db.rows.Rows.lambda$collectStats$0(Rows.java:102)
	at org.apache.cassandra.utils.btree.BTree.applyForwards(BTree.java:1242)
	at org.apache.cassandra.utils.btree.BTree.apply(BTree.java:1197)
	at org.apache.cassandra.db.rows.BTreeRow.apply(BTreeRow.java:172)
	at org.apache.cassandra.db.rows.Rows.collectStats(Rows.java:97)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter$StatsCollector.applyToRow(BigTableWriter.java:237)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:141)
	at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:110)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:173)
	at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:135)
	at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:65)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:141)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:201)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:428)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:315)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
	at java.lang.Thread.run(Thread.java:745)
{code}",Cassandra 3.11 on Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13756,,,CASSANDRA-13756,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-17 23:07:38.546,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 18 06:55:35 UTC 2017,,,,,,0|i3humf:,9223372036854775807,,,,,,,,,,,,3.11.0,,,,,,,24/Jul/17 12:46;hkroger;This affects also user defined repair on that single file. Probably also other operations?,"17/Aug/17 23:07;jjirsa;Resolving this as a dupe of CASSANDRA-13756 , you reported it first, but I arbitrarily chose that one for my github branch name, so going with that. Thanks for the report!
",18/Aug/17 06:55;hkroger;This is good news! :),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CircleCI tests fail because *stress-test* isn't a valid target,CASSANDRA-13775,13095425,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,eduard.tudenhoefner,eduard.tudenhoefner,eduard.tudenhoefner,17/Aug/17 18:40,12/Mar/19 14:15,13/Mar/19 22:35,18/Aug/17 17:39,2.1.19,2.2.11,3.0.15,,,Build,,,,,0,CI,,,"*stress-test* was added to CircleCI in CASSANDRA-13413 (2.1+) but the target itself got introduced in CASSANDRA-11638 (3.10).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-17 19:31:15.817,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 25 15:22:58 UTC 2017,,,,,,0|i3ix53:,9223372036854775807,2.1.18,2.2.10,3.0.14,,,,,mshuler,mshuler,,,,,,,,,,"17/Aug/17 18:51;eduard.tudenhoefner;Branch: https://github.com/nastra/cassandra/tree/13775-21
Test: https://circleci.com/gh/nastra/cassandra/7",17/Aug/17 19:31;mshuler;Looks good - will commit in a few!,"17/Aug/17 19:35;mshuler;Commit {{3c0c4620f2}} on cassandra-2.1 and merged up. Thanks, Ed.","18/Aug/17 13:37;krummas;I think this might still be broken:
{code}
find ./build/test/output/ -iname ""*.xml"" -exec cp {} $CIRCLE_TEST_REPORTS/junit/ \;
find: `./build/test/output/': No such file or directory

find ./build/test/output/ -iname ""*.xml"" -exec cp {} $CIRCLE_TEST_REPORTS/junit/ \; returned exit code 1
{code}

Most will have their circleci configured to 4 containers, so it would make sense to keep using 4. Patch to just move the {{ant eclipse-warnings}} to container 3 here: https://github.com/krummas/cassandra/commits/marcuse/13775-update

and tests running here (note, different branch name, but it was just renamed for clarity): https://circleci.com/gh/krummas/cassandra/78","18/Aug/17 14:45;krummas;seems eclipse-test doesn't exist in 2.1 either, running {{test-clientutil-jar}} for 2.1 and {{eclipse-test}} for the rest: https://circleci.com/gh/krummas/cassandra/80","18/Aug/17 15:00;mshuler;Thanks for catching these additional details, Marcus - reopening.","18/Aug/17 15:27;eduard.tudenhoefner;[~krummas] your code changes lgtm, so +1 once tests pass","18/Aug/17 16:47;krummas;added another commit as the test-clientutil-jar did not generate a report, could you have a look [~eduard.tudenhoefner]?",18/Aug/17 16:58;eduard.tudenhoefner;verified locally that report is generated. Waiting for https://circleci.com/gh/krummas/cassandra/81 to finish,18/Aug/17 17:13;eduard.tudenhoefner;[~krummas] the new artifact shows up in https://81-12804511-gh.circle-artifacts.com/3/tmp/circle-junit.Xa1F9EO/junit/TEST-org.apache.cassandra.serializers.ClientUtilsTest.xml so +1 for merging,"18/Aug/17 17:20;krummas;and committed, thanks",18/Aug/17 17:39;eduard.tudenhoefner;Follow up fix was committed as [a3498d5|https://github.com/apache/cassandra/commit/a3498d5eb7b5b5418b32491238524b33f20347dd] to *cassandra-2.1* and merged upstream.,25/Aug/17 15:22;krummas;created CASSANDRA-13807 as another follow-up,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra removenode makes Gossiper Thread hang forever,CASSANDRA-13562,13075922,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,30/May/17 18:37,12/Mar/19 14:15,13/Mar/19 22:35,12/Jul/17 05:45,3.0.14,,,,,Legacy/Core,,,,,0,,,,"We have seen nodes in Cassandra (3.0.11) ring gets into split-brain somehow. We don't know exact reproducible steps but here is our observation:

Let's assume we have 5 node cluster n1,n2,n3,n4,n5. In this bug when do nodetool status on each node then each one has different view of DN node

e.g.
n1 sees n3 as DN and other nodes are UN
n3 sees n4 as DN and other nodes are UN
n4 sees n5 as DN and other nodes are UN and so on...

One thing we have observed is once n/w link is broken and restored then sometimes nodes go into this split-brain mode but we still don't have exact reproducible steps.

Please let us know if I am missing anything specific here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13308,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-05 00:31:51.791,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 12 05:43:27 UTC 2017,,,,,,0|i3fnef:,9223372036854775807,,,,,,,,,,,,,,,,,,,05/Jul/17 00:31;jasonstack;you may consider checking `phi_convict_threshold` in cassandra.yaml and turning on the logging for phi value in gossip. then you will get a better idea why c* node thinks another as down node.,"12/Jul/17 05:43;chovatia.jaydeep@gmail.com;I analyzed stack trace when Cassandra goes into split-brain mode and found that Gossiper thread is stuck at following stack location forever for HintsDispatchExecutor.java to complete, and HintsDispatchExecutor.java executor thread is blocked in delivering hints to the node being removed. They are going in dead-lock state and thats the reason behind this split brain. 

{quote}
""GossipStage:1"" #310
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000ab000720> (a java.util.concurrent.FutureTask)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
	at java.util.concurrent.FutureTask.get(FutureTask.java:191)
	at org.apache.cassandra.hints.HintsDispatchExecutor.completeDispatchBlockingly(HintsDispatchExecutor.java:112)
	at org.apache.cassandra.hints.HintsService.excise(HintsService.java:323)
	at org.apache.cassandra.service.StorageService.excise(StorageService.java:2265)
	at org.apache.cassandra.service.StorageService.excise(StorageService.java:2278)
	at org.apache.cassandra.service.StorageService.handleStateRemoving(StorageService.java:2234)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1690)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:2474)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:1060)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:1143)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:76)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
	at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1527007086.run(Unknown Source)
	at java.lang.Thread.run(Thread.java:745)
{quote}


Here are the reproducible steps:
1. Created Cassandra 3.0.13 cluster with few nodes (say 5 nodes)
2. Set {{hinted_handoff_throttle_in_kb}} to 1 (so that hint propagation will take time, we must hit removenode while hints are in-preogress to reproduce this issue)
3. Start a load on this cluster specifically write traffic
4. Purposefully shutdown one node and let hints build 
5. Restart node momentarily and make sure all nodes are in UN state, wait for 30 seconds to 1 min. so that {{HintsDispatchExecutor.java}} starts dispatching hints to the node
6. Kill Cassandra on that node again
7. Try removing that down node using {{nodetool removenode force}} or {{nodetool assassinate}}, at this point check {{nodetool status}} on each node and you will see they are in split-brain mode due to Gossip thread is stuck. At this point the only way to come out of this situation is to reboot Cassandra.

Fix for this problem is to do {{future.cancel}}, upon further investigation I found that it has already fixed as part of CASSANDRA-13308. I have tried reproducing this with 3.0.14 and it is no longer reproduced in 3.0.14.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Large amount of CPU used by epoll_wait(.., .., .., 0)",CASSANDRA-13651,13083792,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,burmanm,iksaif,iksaif,30/Jun/17 14:03,12/Mar/19 14:15,13/Mar/19 22:35,24/Aug/18 15:12,4.0,,,,,,,,,,0,pull-request-available,,,"I was trying to profile Cassandra under my workload and I kept seeing this backtrace:
{code}
epollEventLoopGroup-2-3 State: RUNNABLE CPU usage on sample: 240ms
io.netty.channel.epoll.Native.epollWait0(int, long, int, int) Native.java (native)
io.netty.channel.epoll.Native.epollWait(int, EpollEventArray, int) Native.java:111
io.netty.channel.epoll.EpollEventLoop.epollWait(boolean) EpollEventLoop.java:230
io.netty.channel.epoll.EpollEventLoop.run() EpollEventLoop.java:254
io.netty.util.concurrent.SingleThreadEventExecutor$5.run() SingleThreadEventExecutor.java:858
io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run() DefaultThreadFactory.java:138
java.lang.Thread.run() Thread.java:745
{code}

At fist I though that the profiler might not be able to profile native code properly, but I wen't further and I realized that most of the CPU was used by {{epoll_wait()}} calls with a timeout of zero.

Here is the output of perf on this system, which confirms that most of the overhead was with timeout == 0.

{code}
Samples: 11M of event 'syscalls:sys_enter_epoll_wait', Event count (approx.): 11594448
Overhead  Trace output                                                                                                                                                                                           ◆
  90.06%  epfd: 0x00000047, events: 0x7f5588c0c000, maxevents: 0x00002000, timeout: 0x00000000                                                                                                                   ▒
   5.77%  epfd: 0x000000b5, events: 0x7fca419ef000, maxevents: 0x00001000, timeout: 0x00000000                                                                                                                   ▒
   1.98%  epfd: 0x000000b5, events: 0x7fca419ef000, maxevents: 0x00001000, timeout: 0x000003e8                                                                                                                   ▒
   0.04%  epfd: 0x00000003, events: 0x2f6af77b9c00, maxevents: 0x00000020, timeout: 0x00000000                                                                                                                   ▒
   0.04%  epfd: 0x0000002b, events: 0x121ebf63ac00, maxevents: 0x00000040, timeout: 0x00000000                                                                                                                   ▒
   0.03%  epfd: 0x00000026, events: 0x7f51f80019c0, maxevents: 0x00000020, timeout: 0x00000000                                                                                                                   ▒
   0.02%  epfd: 0x00000003, events: 0x7fe4d80019d0, maxevents: 0x00000020, timeout: 0x00000000
{code}

Running this time with perf record -ag for call traces:
{code}
# Children      Self       sys       usr  Trace output                                                                        
# ........  ........  ........  ........  ....................................................................................
#
     8.61%     8.61%     0.00%     8.61%  epfd: 0x000000a7, events: 0x7fca452d6000, maxevents: 0x00001000, timeout: 0x00000000
            |
            ---0x1000200af313
               |          
                --8.61%--0x7fca6117bdac
                          0x7fca60459804
                          epoll_wait

     2.98%     2.98%     0.00%     2.98%  epfd: 0x000000a7, events: 0x7fca452d6000, maxevents: 0x00001000, timeout: 0x000003e8
            |
            ---0x1000200af313
               0x7fca6117b830
               0x7fca60459804
               epoll_wait
{code}

That looks like a lot of CPU used to wait for nothing. I'm not sure if pref reports a per-CPU percentage or a per-system percentage, but that would be still be 10% of the total CPU usage of Cassandra at the minimum.

I went further and found the code of all that: We schedule a lot of {{Message::Flusher}} with a deadline of 10 usec (5 per messages I think) but netty+epoll only support timeouts above the milliseconds and will convert everything bellow to 0.

I added some traces to netty (4.1):
{code}
diff --git a/transport-native-epoll/src/main/java/io/netty/channel/epoll/EpollEventLoop.java b/transport-native-epoll/src/main/java/io/netty/channel/epoll/EpollEventLoop.java
index 909088fde..8734bbfd4 100644
--- a/transport-native-epoll/src/main/java/io/netty/channel/epoll/EpollEventLoop.java
+++ b/transport-native-epoll/src/main/java/io/netty/channel/epoll/EpollEventLoop.java
@@ -208,10 +208,15 @@ final class EpollEventLoop extends SingleThreadEventLoop {
         long currentTimeNanos = System.nanoTime();
         long selectDeadLineNanos = currentTimeNanos + delayNanos(currentTimeNanos);
         for (;;) {
-            long timeoutMillis = (selectDeadLineNanos - currentTimeNanos + 500000L) / 1000000L;
+            long timeoutNanos = selectDeadLineNanos - currentTimeNanos + 500000L;
+            long timeoutMillis =  timeoutNanos / 1000000L;
+            System.out.printf(""timeoutNanos: %d, timeoutMillis: %d | deadline: %d - now: %d | hastask: %d\n"",
+                    timeoutNanos, timeoutMillis,
+                    selectDeadLineNanos, currentTimeNanos, hasTasks() ? 1 : 0);
             if (timeoutMillis <= 0) {
                 if (selectCnt == 0) {
                     int ready = Native.epollWait(epollFd.intValue(), events, 0);
+                    System.out.printf(""ready: %d\n"", ready);
                     if (ready > 0) {
                         return ready;
                     }
{code}

And this gives :

{code}
timeoutNanos: 1000500000, timeoutMillis: 1000 | deadline: 2001782341816510 - now: 2001781341816510 | hastask: 0
timeoutNanos: 1000500000, timeoutMillis: 1000 | deadline: 2001782342087239 - now: 2001781342087239 | hastask: 0
timeoutNanos: 1000500000, timeoutMillis: 1000 | deadline: 2001782342166947 - now: 2001781342166947 | hastask: 0
timeoutNanos: 508459, timeoutMillis: 0 | deadline: 2001781342297987 - now: 2001781342289528 | hastask: 0
ready: 0
timeoutNanos: 508475, timeoutMillis: 0 | deadline: 2001781342357719 - now: 2001781342349244 | hastask: 0
ready: 0
timeoutNanos: 509327, timeoutMillis: 0 | deadline: 2001781342394822 - now: 2001781342385495 | hastask: 0
ready: 0
timeoutNanos: 509339, timeoutMillis: 0 | deadline: 2001781342430192 - now: 2001781342420853 | hastask: 0
ready: 0
timeoutNanos: 509510, timeoutMillis: 0 | deadline: 2001781342461588 - now: 2001781342452078 | hastask: 0
ready: 0
timeoutNanos: 509493, timeoutMillis: 0 | deadline: 2001781342495044 - now: 2001781342485551 | hastask: 0
ready: 0
{code}

The nanosecond timeout all come from {{eventLoop.schedule(this, 10000, TimeUnit.NANOSECONDS);}} in {{Message::Flusher}}.

Knowing that, I'm not sure what would be best to do, and I have a hard time understanding Message::Flusher, but to me it looks like trying to schedule less tasks would probably help and I didn't think anything obvious that could be done with netty.

Changing {{if (++runsWithNoWork > 5)}} to 2 seems to help a little bit, but that isn't really significant.",,"iksaif commented on pull request #151: [CASSANDRA-13651]: Reduce epoll/timerfd CPU usage
URL: https://github.com/apache/cassandra/pull/151
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/19 08:53;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,01/Aug/17 16:52;iksaif;cpu-usage.png;https://issues.apache.org/jira/secure/attachment/12879868/cpu-usage.png,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-06-30 20:00:49.058,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 24 15:12:24 UTC 2018,,,,,,0|i3gy9r:,9223372036854775807,,,,,,,,benedict,benedict,,,,,,,,,,"30/Jun/17 18:33;iksaif;Things to check or try (for me):
* io.netty.eventLoopThreads
* Check if we could use the same eventloop instead of starting two
* Create a custom SelectStrategy that skips looking at fds if there is a scheduled task happening in a few microseconds
* Try to understand why Message::Flusher currently works this way",30/Jun/17 20:00;jasobrown;/cc [~norman],"01/Jul/17 04:54;iksaif;Also check:
* https://github.com/netty/netty/issues/1759
* https://gist.github.com/jadbaz/47d98da0ead2e71659f343b14ef05de6
* Benchmark batching vs. stupid writeAndFlush()
* It's unclear why sending the response is done in the flusher right now
* https://github.com/spotify/netty-batch-flusher","03/Jul/17 09:20;iksaif;I cooked a patch to use spotify's netty-batch-flusher instead of the current flusher. Here are some results:

{code}
normal:

Results:
Op rate                   :    4,220 op/s  [insert: 4,220 op/s]
Partition rate            :    4,220 pk/s  [insert: 4,220 pk/s]
Row rate                  :   41,851 row/s [insert: 41,851 row/s]
Latency mean              :    0.2 ms [insert: 0.2 ms]
Latency median            :    0.2 ms [insert: 0.2 ms]
Latency 95th percentile   :    0.2 ms [insert: 0.2 ms]
Latency 99th percentile   :    0.3 ms [insert: 0.3 ms]
Latency 99.9th percentile :    0.4 ms [insert: 0.4 ms]
Latency max               :   65.5 ms [insert: 65.5 ms]
Total partitions          :    100,000 [insert: 100,000]
Total errors              :          0 [insert: 0]
Total GC count            : 6
Total GC memory           : 3.473 GiB
Total GC time             :    0.4 seconds
Avg GC time               :   60.0 ms
StdDev GC time            :    5.1 ms
Total operation time      : 00:00:23
{code}

{code}
batched:

Results:
Op rate                   :    4,344 op/s  [insert: 4,344 op/s]
Partition rate            :    4,344 pk/s  [insert: 4,344 pk/s]
Row rate                  :   43,121 row/s [insert: 43,121 row/s]
Latency mean              :    0.2 ms [insert: 0.2 ms]
Latency median            :    0.2 ms [insert: 0.2 ms]
Latency 95th percentile   :    0.2 ms [insert: 0.2 ms]
Latency 99th percentile   :    0.3 ms [insert: 0.3 ms]
Latency 99.9th percentile :    0.4 ms [insert: 0.4 ms]
Latency max               :   63.4 ms [insert: 63.4 ms]
Total partitions          :    100,000 [insert: 100,000]
Total errors              :          0 [insert: 0]
Total GC count            : 6
Total GC memory           : 3.467 GiB
Total GC time             :    0.4 seconds
Avg GC time               :   60.0 ms
StdDev GC time            :    3.3 ms
Total operation time      : 00:00:23
{code}

So slightly more QPS, but more interestingly, the epoll thread now uses ~4 times less CPU. I'll try to do a full scale benchmark on a bigger workload with 3 nodes tomorrow.

Patch at https://github.com/iksaif/cassandra/tree/cassandra-13651-trunk","04/Jul/17 11:28;iksaif;I ran tests on a 3 node cluster, I can confirm that not using scheduled tasks and using a simpler batcher removes all the {{epoll_wait(..., 0)}} calls. This reduces the CPU used by epoll threads.
I need to take more time do check how efficient the batching still is, and compare the context switchs with and without it.","25/Jul/17 12:35;iksaif;Latest patch (https://github.com/iksaif/cassandra/tree/cassandra-13651-trunk)  tested with an actual workload, I attached the screenshot.
Looks like we can get ~2% of CPU back with -Dcassandra.netty_flush_delay_nanoseconds=0 and some more with -Dio.netty.eventLoopThreads=6. This doesn't not seem to affect latency

!Screenshot (5).png|thumbnail!","01/Aug/17 16:52;iksaif;!cpu-usage.png!

Almost ~8% of CPU saving after updating all three nodes.","02/Aug/17 05:18;jjirsa;That's really interesting. [~norman] - if you get a few minutes to glance, always appreciate your thoughts here. 

Also, [~iksaif] - can you share a bit more detail on your test? Is it just stress? Have you tested with many connected clients? Any immediate reason you suspect that CPU went down (significantly?), but throughput remains unchanged (do you see the next bottleneck, the op rate on that stress run looks pretty low, even for a single machine cluster?)","02/Aug/17 05:32;norman;Sorry for the late response.. didn't see the mention :(

So yes netty uses `epoll_wait` which only supports milli-seconds resulution so everything smaller then this will just cause `epoll_wait(...)` be called with a `0` and so a non-blocking check of ready fds. What we could do in our native transport implementation is that we make use of `timerfd` [1] to schedule timeouts but again this would only work for the case of using the native epoll transport and not when you use the nio transport (which works on all OS). So I think what you really want to do is have timeouts of >= 1ms. 

Comments and ideas welcome :)
","02/Aug/17 06:50;iksaif;[~jjirsa]: The cassandra-stress report is for https://github.com/criteo/biggraphite/blob/master/tools/stress/biggraphite.yaml. The bottleneck here was the lack of parallelization on the client I guess.
The screenshot is the actual workload of BigGraphite: 3 nodes, 100 connected clients.


[~norman]: Both timeouts of 1ms or no timeouts would achieve the same thing I guess. I tested with no timeout as it implied less changes to the actual logic (see https://github.com/iksaif/cassandra/tree/cassandra-13651-trunk). Currently (I believe that) the message itself it written after the delay, increasing the timeout will increase the latency of every operations. In my test I simply disable the timeout and schedule the flush task as soon as I can, this doesn't reduce the opportunities for batching that much if you keep the number of epoll threads low.","02/Aug/17 13:47;jjirsa;[~iksaif] - this is not a review (and I think there are better people around to review this than myself, like either Norman, Jason, Jake or Ariel), but I'm unable to get a clean test run with your patch on trunk with either netty 4.0.44 or 4.1.13. You mention in the first post that you added some traces to 4.1 - can you confirm which version of netty you tested with? Can you also confirm whether or not {{ant test -Dtest.name=NativeTransportServiceTest}} passes cleanly for you?","02/Aug/17 14:39;norman;Also for the record Scott Mitchell (another core netty dev) just created a PR in netty to support micro seconds timeouts when using the native epoll transport:

https://github.com/netty/netty/pull/7042

That said I still need to review it in detail and its not merged yet.","02/Aug/17 14:52;iksaif;https://github.com/iksaif/cassandra/commit/c05f2eef6abc8066b69e50dc5025f17e17871f0c should fix the test.

I'm running with 4.0.44. The production test is using 3.11 as a base but I'm able to start trunk on my dev machine.
","03/Aug/17 06:57;iksaif;Using timerfd is something that I looked at, but I though that it would be easier to just change the code in Cassandra for now. I'll be out in the next three weeks but I'll definitivement try a patched version of netty when I'm back.",18/Aug/17 18:39;norman;[~iksaif] FYI we will merge the change to use timerfd today and so it will be part of the next Netty release. That said I think what you suggested (changing the Cassandra code) may make more sense in general.,"28/Aug/17 07:16;iksaif;Great ! I'm good with either bumping netty to this version or merging my patch, [~jjirsa] what do you think ?","14/Sep/17 12:52;iksaif;Ping, anything against https://github.com/iksaif/cassandra/commits/cassandra-13651-trunk ? If not I'll send a proper pull request.","14/Sep/17 22:50;jasobrown;[~iksaif] I just took a read through your patch.  Here's my thoughts:

- The refactoring of {{Dispatcher#FlushItem}} to {{DelayedFlusher}} is unnecessary. There's nothing functionally incorrect with the existing implementation. (I'll posit that the nested class within a nested class is ... non-standard.)
- {{NativeTransportService#defaultWorkerGroup}} is confusing. Why not just use the existing {{NativeTransportService#workerGroup}}?
- {{NativeTransportService#getDefaultWorkerGroup}} isn't thread safe, and so you could end up with mutiple {{EventLoopGroup}}s group created - although this maybe not a problem in practice.
- Scott's netty patch landed in 4.1.15. Let's update trunk for that.
- Minor nit: I'd prefer to rename {{netty_flush_delay_nanoseconds}} to something like {{native_transport_flush_delay_nanoseconds}} to make it explicit that this property will only affect the netty use in the native transport and not the internode messaging or streaming.

Honestly, I think the patch can be reduced to simply adding the {{FLUSH_DELAY}} logic to [{{Message.Dispatcher.Flusher#run}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/Message.java#L488], and updating netty. Or possibly just update netty and call it a day. [~iksaif] let me know if you can run a test with just the updated netty library.","15/Sep/17 06:25;iksaif;Ok, here is the plan (for myself):
* Separate the ""use only one worker group"" patch. It's useful because it creates less threads, but isn't really directly related to this.
* Update netty to 4.1.15 on our setup (without -Dnetty_flush_delay_nanoseconds) and see the effects
* Set -Dnetty_flush_delay_nanoseconds and see the effects. Depending on the results, propose a simpler version of the patch.
","15/Sep/17 18:06;jasobrown;[~iksaif] This all sounds great! A quick note: if you are testing on something lower than trunk, I think you can use netty 4.0.51 as that will also have Scott's patch (and will be generally more compatible with everything pre-CASSANDRA-8457)","18/Sep/17 13:09;githubbot;GitHub user iksaif opened a pull request:

    https://github.com/apache/cassandra/pull/151

    [CASSANDRA-13651]: Reduce epoll/timerfd CPU usage

    - Bump Netty to 4.1.15
    - Add a setting to schedule flushes immediatly  

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/iksaif/cassandra cassandra-13651-trunk

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/151.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #151
    
----
commit 969dcee971242ff08a2ba1baf9e3139935e85ee8
Author: Corentin Chary <c.chary@criteo.com>
Date:   2017-09-18T12:12:46Z

    Bump netty to 4.1.15
    
    This is to take advantages of the improvements from https://github.com/netty/netty/pull/7042

commit 594ef9eee5ad853eaec4234f9a11bc12e030ae6c
Author: Corentin Chary <c.chary@criteo.com>
Date:   2017-09-18T12:26:35Z

    CASSANDRA-13651: Reduce CPU used by epoll_wait() / timerfd_create()
    
    By setting -Dcassandra.native_transport_flush_delay_nanoseconds=0 one can
    schedule the flush() immediatly which will be more efficient when
    using epoll() on Linux by reducing the number of calls to epoll_wait().
    
    This is simmilarly more efficient on version of netty that use timerfd
    to get timeouts with microsecond resolution when calling epoll().
    
    On those platforms this can save up to 10% of CPU.

----
","18/Sep/17 13:11;iksaif;Results: the calls to timerfd end up costing almost as much as what epoll_wait() before. It is still more efficient to execute instead of schedule.

I added a patch to bump netty to 4.1.15, and a way simpler version of my previous patch that allows one to configure the task delay.","18/Sep/17 21:45;jasobrown;Cool. Thanks for the patch. The code itself is trivial (adding a configuration property), however I liked what you had before: 

{code}
if (FLUSH_DELAY > 0)
    eventLoop.schedule(this, FLUSH_DELAY, TimeUnit.NANOSECONDS);
else
    eventLoop.execute(this);
{code}

That way if the {{FLUSH_DELAY}} is 0, skip creating a scheduled task in netty and just submit it for execution. wdyt?

I'd like to run this in my test environment to verify if there's any change on my side. It may take a few days due to other on-going work.","19/Sep/17 08:18;iksaif;Thanks for double-checking that, I pushed the wrong version. This should now be fixed.",23/Nov/17 08:47;iksaif;ping ?,"09/May/18 11:37;burmanm;I've done a follow up patch to this also, available here:

[https://github.com/burmanm/cassandra/commit/eb8af9a47b00a836fbb4e155aa64024250470ba7]

I could not find a use-case where (at least using epoll) the timer would be beneficial. It's going to eat CPU all the time in any case, and I verified this by creating a small looping C program that did the same thing using epoll & timerfd directly (around ~4% CPU usage from the C program and ~6% from the Netty version when both are only rescheduling the job). So, I did the following to Flusher:
 * Removed automatic rescheduling by the Flusher
 * Instead, after every write we schedule the flush during the next eventloop invocation. If multiple writes are successful before that, they're flushed together

In my tests, this both improves the throughput slightly, but more importantly, it also decreases latency. On Azure D8s_V3 instances, the latency is decreased by 0.3ms with mediocre and more punishing loads. Obviously, the profiler shows that we spent more time in the writevAddresses(), but then EpollWait0 is reduced to next to nothing during stress run.

At this point I would suggest removing the batching completely and then return to this issue once we can sanely batch the work of encode also (and make that pipeline then flush at the end). 

 ","21/Aug/18 15:51;benedict;So, I assume we're now defaulting to epoll in most cases, and this behaviour comes from a period where this wasn't the default (and was probably poorly justified at the time - AFAICR we used to benchmark with only a single connection, where this behaviour would be more beneficial).

It's a shame we no longer have any standard benchmarking tools for the project, but it seems we have multiple data points demonstrating a win (or no loss), and the code is simpler after the patch.

So, I'm +1 on the patch.  I will get a circleci run going shortly.",21/Aug/18 15:54;norman;I am no committer but the netty project lead so from the point of view of netty usage I am also +1 on this.,"21/Aug/18 15:58;jjirsa;There are two patches here, [~iksaif]'s patch and [~burmanm]'s follow-patch, which did you each +1?
","21/Aug/18 16:06;benedict;The latest, i.e. [~burmanm]'s.

It simplifies the code, and as I say, I'm not sure the original patch was as well justified as we thought - our benchmarking methodology was flawed at the time (using a single connection).

I suppose arguably there's value in maintaining the current behaviour for those users with a single connection and without epoll, but since epoll is now the default it's probably better to improve code clarity.

I'm open to dispute, of course, in which case we can revisit Corentin's patch (or try to reproduce the old benchmarks and see what we might be losing in modern C* in the worst case).  In this case, I would probably prefer to have a LegacyFlusher and a Flusher - the latter the cleaned code contributed by [~burmanm], the former the old unadulterated code, and to select between them based on the config property (with a default being determined by epoll usage status)","24/Aug/18 08:39;burmanm;I added a commit https://github.com/burmanm/cassandra/commit/681ab256e0af50827169d88931fadf89b3cd3770 to tree https://github.com/burmanm/cassandra/tree/netty_flusher which allows selecting either the new behavior (default) or the old one. I'm open to better naming and description ;) I've made the new behavior enabled for default now, or do you still wish to have the old behavior as default for non-epoll cases?","24/Aug/18 08:50;benedict;Looks good.  The user-visible property should probably be adjacent to the native_transport settings, though, and prefixed by the same name; perhaps be named something like {{native_transport_flush_in_batches_legacy}}","24/Aug/18 10:21;burmanm;Sounds a lot better name. Changed to that, rebased to current trunk: https://github.com/burmanm/cassandra/commit/30e4dda2d3c3e6124fe118d61da798d216ad20b9",24/Aug/18 15:12;benedict;Committed to [4.0|https://github.com/apache/cassandra/commit/96ef514917e5a4829dbe864104dbc08a7d0e0cec] with some small edits.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bootstrap streaming failed suspect due to secondary index,CASSANDRA-13726,13089549,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,wavelet,wavelet,24/Jul/17 14:35,12/Mar/19 14:15,13/Mar/19 22:35,25/Jul/17 08:35,,,,,,,,,,,0,,,,"Hi,

We have a Cassandra DC with 31 nodes,but it's always failed when adding the 32nd node.
The following is the error log.And we suspect it's due to the secondary index,but it never happens before.Could you please advise?

Thanks
{code}
INFO  [CompactionExecutor:3] 2017-07-24 22:07:40,774 ColumnFamilyStore.java:917 - Enqueuing flush of compactions_in_progress: 149 (0%) on-heap, 20 (0%) off-heap
INFO  [MemtableFlushWriter:286] 2017-07-24 22:07:40,775 Memtable.java:347 - Writing Memtable-compactions_in_progress@581635159(0.008KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:286] 2017-07-24 22:07:40,775 Memtable.java:382 - Completed flushing /data/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-tmp-ka-1159-Data.db (0.000KiB) for commitlog position ReplayPosition(segmentId=1500866839309, position=11929032)
INFO  [CompactionExecutor:3] 2017-07-24 22:07:40,796 CompactionTask.java:274 - Compacted 15 sstables to [/data/cassandra/data/ecommercedata/ecommerce_baitiao_bill-c1a7f8d0660511e781e2077cc592d79d/ecommercedata-ecommerce_baitiao_bill-ka-40,].  293,793 bytes to 288,251 (~98% of original) in 108ms = 2.545348MB/s.  1,024 total partitions merged to 1,016.  Partition merge counts were {1:1008, 2:8, }
INFO  [StreamReceiveTask:5] 2017-07-24 22:07:43,351 ColumnFamilyStore.java:917 - Enqueuing flush of ecommerce_baitiao_record.baitiao_state: 2228978 (0%) on-heap, 6183142 (0%) off-heap
INFO  [CompactionExecutor:3] 2017-07-24 22:07:43,352 ColumnFamilyStore.java:917 - Enqueuing flush of compactions_in_progress: 349 (0%) on-heap, 266 (0%) off-heap
INFO  [MemtableFlushWriter:287] 2017-07-24 22:07:43,353 Memtable.java:347 - Writing Memtable-ecommerce_baitiao_record.baitiao_state@1651345625(4.792MiB serialized bytes, 89948 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:286] 2017-07-24 22:07:43,353 Memtable.java:347 - Writing Memtable-compactions_in_progress@284311350(0.197KiB serialized bytes, 9 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:286] 2017-07-24 22:07:43,353 Memtable.java:382 - Completed flushing /data/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-tmp-ka-1160-Data.db (0.000KiB) for commitlog position ReplayPosition(segmentId=1500866839309, position=13336501)
INFO  [MemtableFlushWriter:287] 2017-07-24 22:07:43,542 Memtable.java:382 - Completed flushing /data/cassandra/data/ecommercedata/ecommerce_baitiao_record-c94692e012cd11e682501f0d5e9f1e54/ecommercedata-ecommerce_baitiao_record.baitiao_state-tmp-ka-15-Data.db (1.593MiB) for commitlog position ReplayPosition(segmentId=1500866839309, position=13336501)
INFO  [StreamReceiveTask:5] 2017-07-24 22:07:43,558 SecondaryIndexManager.java:174 - Index build of [ecommerce_baitiao_record.baitiao_state] complete
INFO  [StreamReceiveTask:5] 2017-07-24 22:07:43,558 StreamResultFuture.java:181 - [Stream #15d5ffa0-7020-11e7-8b09-d510dd9f6293] Session with /xxx.xxx.xxx.xxx is complete
WARN  [StreamReceiveTask:5] 2017-07-24 22:07:43,559 StreamResultFuture.java:208 - [Stream #15d5ffa0-7020-11e7-8b09-d510dd9f6293] Stream failed
ERROR [main] 2017-07-24 22:07:43,561 CassandraDaemon.java:583 - Exception encountered during startup
java.lang.RuntimeException: Error during boostrap: Stream failed
        at org.apache.cassandra.dht.BootStrapper.bootstrap(BootStrapper.java:87) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:1166) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:944) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:740) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:617) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:391) [apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:566) [apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:655) [apache-cassandra-2.1.18.jar:2.1.18]
Caused by: org.apache.cassandra.streaming.StreamException: Stream failed
        at org.apache.cassandra.streaming.management.StreamEventJMXNotifier.onFailure(StreamEventJMXNotifier.java:85) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1172) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202) ~[guava-16.0.jar:na]
        at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:209) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:185) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:413) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.streaming.StreamSession.maybeCompleted(StreamSession.java:700) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.streaming.StreamSession.taskCompleted(StreamSession.java:661) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at org.apache.cassandra.streaming.StreamReceiveTask$OnCompletionRunnable.run(StreamReceiveTask.java:179) ~[apache-cassandra-2.1.18.jar:2.1.18]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
WARN  [StorageServiceShutdownHook] 2017-07-24 22:07:43,564 Gossiper.java:1462 - No local state or state is in silent shutdown, not announcing shutdown
INFO  [StorageServiceShutdownHook] 2017-07-24 22:07:43,564 MessagingService.java:734 - Waiting for messaging service to quiesce
INFO  [ACCEPT-/172.16.135.52] 2017-07-24 22:07:43,565 MessagingService.java:1020 - MessagingService has terminated the accept() thread
INFO  [CompactionExecutor:3] 2017-07-24 22:07:43,571 CompactionTask.java:141 - Compacting [SSTableReader(path='/data/cassandra/data/peoplephonenumber/people_by_peopleid-38befca077fd11e5aa40f56762d1c5e6/peoplephonenumber-people_by_peopleid-ka-10-Data.db'), SSTableReader(path='/data/cassandra/data/peoplephonenumber/people_by_peopleid-38befca077fd11e5aa40f56762d1c5e6/peoplephonenumber-people_by_peopleid-ka-12-Data.db'), SSTableReader(path='/data/cassandra/data/peoplephonenumber/people_by_peopleid-38befca077fd11e5aa40f56762d1c5e6/peoplephonenumber-people_by_peopleid-ka-11-Data.db'), SSTableReader(path='/data/cassandra/data/peoplephonenumber/people_by_peopleid-38befca077fd11e5aa40f56762d1c5e6/peoplephonenumber-people_by_peopleid-ka-14-Data.db')]
{code}",Cassandra 2.1.18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-25 00:51:13.283,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 25 23:59:05 UTC 2017,,,,,,0|i3hxnr:,9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Jul/17 00:51;KurtG;You need to check both sides of the stream for errors. Have a look at the corresponding log on ""xxx.xxx.xxx.xxx"" for errors that could explain the failure.","25/Jul/17 08:27;wavelet;Thanks for the reply.the peer side says the  streaming_socket_timeout_in_ms is too low.Although I have set the new node with streaming_socket_timeout_in_ms=86400000

ERROR [STREAM-IN-/172.16.135.52] 2017-07-25 11:55:08,034 StreamSession.java:505 - [Stream #b5594a10-70ec-11e7-b9ad-d510dd9f6293] Streaming socket tim
ed out. This means the session peer stopped responding or is still processing received data. If there is no sign of failure in the other end or a ver
y dense table is being transferred you may want to increase streaming_socket_timeout_in_ms property. Current value is 7200ms.",25/Jul/17 23:59;KurtG;You should set {{streaming_socket_timeout_in_ms=86400000}} on all the nodes. Both sides of the streaming connection need a larger timeout. 7200 is way too low.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""ignore"" option is ignored in sstableloader",CASSANDRA-13721,13089233,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,varuna,syegournov,syegournov,22/Jul/17 11:56,12/Mar/19 14:15,13/Mar/19 22:35,23/Jul/17 14:29,3.11.1,4.0,,,,Legacy/Tools,,,,,0,patch,,,"If ignore option is set on the command line sstableloader still streams to the nodes excluded.

I believe the issue is in the [https://github.com/apache/cassandra/blob/dfb90b1458ac6ee427f9e329b45c764a3a0a0c06/src/java/org/apache/cassandra/tools/LoaderOptions.java] - the LoaderOptions constructor does not set the ""ignores"" field from the the ""builder.ignores""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Jul/17 17:48;varuna;CASSANDRA-13721.patch;https://issues.apache.org/jira/secure/attachment/12878508/CASSANDRA-13721.patch,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-07-22 17:47:43.769,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 23 14:34:40 UTC 2017,,,,,,0|i3hvpz:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,3.11.0,,,,,,,"22/Jul/17 17:47;varuna;Yes, constructor does not set the {{ignores}}. I'm providing patch for this. 
Also need to check for other versions.",22/Jul/17 17:49;varuna;fixed constructor of {{LoaderOptions}},23/Jul/17 04:14;varuna;cassandra {{tag >= 3.4}} are having this bug.,"23/Jul/17 14:29;jasobrown;[~varuna] nice find. +1

committed to 3.11 and trunk as sha {{c3a19f3554113682b4d37fe6a7f49bc810e5ddf6}}. Thanks!",23/Jul/17 14:34;varuna;Thank you so much!! It was my first patch which got accepted!! :) ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in topology_test.TestTopology.size_estimates_multidc_test,CASSANDRA-13567,13076316,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jkni,mshuler,mshuler,31/May/17 21:20,12/Mar/19 14:15,13/Mar/19 22:35,22/Jun/17 15:38,,,,,,,,,,,0,dtest,test-failure,,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_novnode_dtest/367/testReport/topology_test/TestTopology/size_estimates_multidc_test

{noformat}
Error Message

Expected [['-3736333188524231709', '-2688160409776496397'], ['-6639341390736545756', '-3736333188524231709'], ['-9223372036854775808', '-6639341390736545756'], ['8473270337963525440', '8673615181726552074'], ['8673615181726552074', '-9223372036854775808']] from SELECT range_start, range_end FROM system.size_estimates WHERE keyspace_name = 'ks2', but got [[u'-3736333188524231709', u'-2688160409776496397'], [u'-9223372036854775808', u'-6639341390736545756'], [u'8673615181726552074', u'-9223372036854775808']]
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-yNH4mu
dtest: DEBUG: Done setting configuration options:
{   'num_tokens': None,
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: Creating cluster
dtest: DEBUG: Setting tokens
dtest: DEBUG: Starting cluster
dtest: DEBUG: Nodetool ring output 
Datacenter: dc1
==========
Address    Rack        Status State   Load            Owns                Token                                       
                                                                          8473270337963525440                         
127.0.0.1  r1          Up     Normal  92.11 KB        39.49%              -6639341390736545756                        
127.0.0.1  r1          Up     Normal  92.11 KB        39.49%              -2688160409776496397                        
127.0.0.2  r1          Up     Normal  92.1 KB         66.19%              -2506475074448728501                        
127.0.0.2  r1          Up     Normal  92.1 KB         66.19%              8473270337963525440                         

Datacenter: dc2
==========
Address    Rack        Status State   Load            Owns                Token                                       
                                                                          8673615181726552074                         
127.0.0.3  r1          Up     Normal  92.1 KB         94.32%              -3736333188524231709                        
127.0.0.3  r1          Up     Normal  92.1 KB         94.32%              8673615181726552074                         

  Warning: ""nodetool ring"" is used to output all the tokens of a node.
  To view status related info of a node use ""nodetool status"" instead.


  
dtest: DEBUG: Creating keyspaces
cassandra.policies: INFO: Using datacenter 'dc1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc2> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered
dtest: DEBUG: Refreshing size estimates
dtest: DEBUG: Checking node1_1 size_estimates primary ranges
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc2> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/topology_test.py"", line 107, in size_estimates_multidc_test
    ['8673615181726552074', '-9223372036854775808']])
  File ""/home/automaton/cassandra-dtest/tools/assertions.py"", line 170, in assert_all
    assert list_res == expected, ""Expected {} from {}, but got {}"".format(expected, query, list_res)
'Expected [[\'-3736333188524231709\', \'-2688160409776496397\'], [\'-6639341390736545756\', \'-3736333188524231709\'], [\'-9223372036854775808\', \'-6639341390736545756\'], [\'8473270337963525440\', \'8673615181726552074\'], [\'8673615181726552074\', \'-9223372036854775808\']] from SELECT range_start, range_end FROM system.size_estimates WHERE keyspace_name = \'ks2\', but got [[u\'-3736333188524231709\', u\'-2688160409776496397\'], [u\'-9223372036854775808\', u\'-6639341390736545756\'], [u\'8673615181726552074\', u\'-9223372036854775808\']]\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-yNH4mu\ndtest: DEBUG: Done setting configuration options:\n{   \'num_tokens\': None,\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ndtest: DEBUG: Creating cluster\ndtest: DEBUG: Setting tokens\ndtest: DEBUG: Starting cluster\ndtest: DEBUG: Nodetool ring output \nDatacenter: dc1\n==========\nAddress    Rack        Status State   Load            Owns                Token                                       \n                                                                          8473270337963525440                         \n127.0.0.1  r1          Up     Normal  92.11 KB        39.49%              -6639341390736545756                        \n127.0.0.1  r1          Up     Normal  92.11 KB        39.49%              -2688160409776496397                        \n127.0.0.2  r1          Up     Normal  92.1 KB         66.19%              -2506475074448728501                        \n127.0.0.2  r1          Up     Normal  92.1 KB         66.19%              8473270337963525440                         \n\nDatacenter: dc2\n==========\nAddress    Rack        Status State   Load            Owns                Token                                       \n                                                                          8673615181726552074                         \n127.0.0.3  r1          Up     Normal  92.1 KB         94.32%              -3736333188524231709                        \n127.0.0.3  r1          Up     Normal  92.1 KB         94.32%              8673615181726552074                         \n\n  Warning: ""nodetool ring"" is used to output all the tokens of a node.\n  To view status related info of a node use ""nodetool status"" instead.\n\n\n  \ndtest: DEBUG: Creating keyspaces\ncassandra.policies: INFO: Using datacenter \'dc1\' for DCAwareRoundRobinPolicy (via host \'127.0.0.1\'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc2> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered\ndtest: DEBUG: Refreshing size estimates\ndtest: DEBUG: Checking node1_1 size_estimates primary ranges\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 dc2> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 dc1> discovered\n--------------------- >> end captured logging << ---------------------'
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/May/17 21:27;mshuler;node1.log;https://issues.apache.org/jira/secure/attachment/12870653/node1.log,31/May/17 21:27;mshuler;node1.log;https://issues.apache.org/jira/secure/attachment/12870652/node1.log,31/May/17 21:27;mshuler;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12870654/node1_debug.log,31/May/17 21:27;mshuler;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12870655/node1_gc.log,31/May/17 21:27;mshuler;node2.log;https://issues.apache.org/jira/secure/attachment/12870656/node2.log,31/May/17 21:27;mshuler;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12870657/node2_debug.log,31/May/17 21:27;mshuler;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12870658/node2_gc.log,31/May/17 21:27;mshuler;node3.log;https://issues.apache.org/jira/secure/attachment/12870659/node3.log,31/May/17 21:27;mshuler;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12870660/node3_debug.log,31/May/17 21:27;mshuler;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12870661/node3_gc.log,,,10.0,,,,,,,,,,,,,,,,,,,2017-06-22 15:38:17.905,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 22 15:38:17 UTC 2017,,,,,,0|i3fptz:,9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Jun/17 15:38;jkni;This test was added after [CASSANDRA-9639], which only went into 3.0.11+. It was version-gated in dtest commit [3cf276e966f253a49df91293a1a0b46620192c59|https://github.com/riptano/cassandra-dtest/commit/3cf276e966f253a49df91293a1a0b46620192c59].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock during CommitLog replay when Cassandra restarts,CASSANDRA-13587,13078797,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,09/Jun/17 23:05,12/Mar/19 14:15,13/Mar/19 22:35,30/Aug/17 13:38,3.0.15,,,,,Legacy/Core,,,,,0,,,,"Possible deadlock found when Cassandra is replaying commit log and at the same time Mutation gets triggered by SSTableReader(SystemKeyspace.persistSSTableReadMeter). As a result Cassandra restart hangs forever

Please find details of stack trace here:

*Frame#1* This thread is trying to apply {{persistSSTableReadMeter}} mutation and as a result it has called {{writeOrder.start()}} in {{Keyspace.java:533}}
but there are no Commitlog Segments available because {{createReserveSegments (CommitLogSegmentManager.java)}} is not yet {{true}} 

Hence this thread is blocked on {{createReserveSegments}} to become {{true}}, please note this thread has already started {{writeOrder}}

{quote}
""pool-11-thread-1"" #251 prio=5 os_prio=0 tid=0x00007fe128478400 nid=0x1b274 waiting on condition [0x00007fe1389a0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
        at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
        at org.apache.cassandra.db.commitlog.CommitLogSegmentManager.advanceAllocatingFrom(CommitLogSegmentManager.java:277)
        at org.apache.cassandra.db.commitlog.CommitLogSegmentManager.allocate(CommitLogSegmentManager.java:196)
        at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:260)
        at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:540)
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:421)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:210)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:215)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:224)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternalWithoutCondition(ModificationStatement.java:566)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:556)
        at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:295)
        at org.apache.cassandra.db.SystemKeyspace.persistSSTableReadMeter(SystemKeyspace.java:1181)
        at org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy$1.run(SSTableReader.java:2202)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{quote}

*Frame#2* This thread is trying to recover commit logs and as a result it tries to flush Memtable by calling following code:
{{futures.add(Keyspace.open(SystemKeyspace.NAME).getColumnFamilyStore(SystemKeyspace.BATCHES).forceFlush());}}
As a result Frame#3 (below) gets created

{quote}
""main"" #1 prio=5 os_prio=0 tid=0x00007fe1c64ec400 nid=0x1af29 waiting on condition [0x00007fe1c94a1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
parking to wait for  <0x00000006370da0c0> (a com.google.common.util.concurrent.ListenableFutureTask)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
        at java.util.concurrent.FutureTask.get(FutureTask.java:191)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:383)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.blockForWrites(CommitLogReplayer.java:207)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:182)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:161)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:295)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697)
{quote}

*Frame#3* This thread is waiting at {{writeBarrier.await();}} in {{ColumnFamilyStore.java:1027}} 
but {{writeBarrier}} is locked by thread in Frame#1, and Frame#1 thread is waiting for more CommitlogSegements to be available. 
Frame#1 thread will not get new segment because variable {{createReserveSegments(CommitLogSegmentManager.java)}} is not yet {{true}}. 
This variable gets set to {{true}} after successful execution of Frame#2.

Here we can see Frame#3 and Frame#1 are in deadlock state and Cassandra restart hangs forever.
 
{quote}
""MemtableFlushWriter:5"" #433 daemon prio=5 os_prio=0 tid=0x00007e7a4b8b0400 nid=0x1dea8 waiting on condition [0x00007e753c2ca000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
        at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
        at org.apache.cassandra.utils.concurrent.OpOrder$Barrier.await(OpOrder.java:419)
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1027)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
        at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1527007086.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:745)


""MemtablePostFlush:3"" #432 daemon prio=5 os_prio=0 tid=0x00007e7a4b8b0000 nid=0x1dea7 waiting on condition [0x00007e753c30b000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
 parking to wait for  <0x00000006370d9cd0> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
        at org.apache.cassandra.db.ColumnFamilyStore$PostFlush.call(ColumnFamilyStore.java:941)
        at org.apache.cassandra.db.ColumnFamilyStore$PostFlush.call(ColumnFamilyStore.java:924)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
        at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1527007086.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:745)
{quote}

*Reproducible steps*: Reproducing this problem is tricky as it involves multiple conditions to happen at the same time and is timing bases, so I have done some small code change to reproduce this:
1. Create a Keyspace and table
2. Inject data until there are few SSTables generated and CommitLog available
3. Kill Cassandra process
4. Use the custom code (in the attached file ""Reproduce_CASSANDRA-13587.txt"") on top of 3.0.14 branch 
5. Build Cassandra jar and use this custom jar
6. Restart Cassandra
    Here you will see Cassandra is hanging forever
7. Now apply this fix on top of ""Reproduce_CASSANDRA-13587.txt"", and repeat step-6
    Here you should see Cassandra is starting normally

*Solution*: I am proposing that we should enable variable {{createReserveSegments(CommitLogSegmentManager.java)}} before recovering any CommitLogs in CommitLog.java file
so this will not block Frame#1 from acquiring new segment as a result Frame#1 will finish and then Frame#2 will also finish.
Please note, this variable {{createReserveSegments}} has been removed from the trunk branch as part of (https://issues.apache.org/jira/browse/CASSANDRA-10202), also in the trunk branch CommitLog segments gets created when needed. So as per my understanding enabling this variable before CommitLog recovery should not create any other side effect, please let me know your comments.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Jun/17 23:12;chovatia.jaydeep@gmail.com;13587-3.0.txt;https://issues.apache.org/jira/secure/attachment/12872375/13587-3.0.txt,12/Jun/17 23:21;chovatia.jaydeep@gmail.com;Reproduce_CASSANDRA-13587.txt;https://issues.apache.org/jira/secure/attachment/12872780/Reproduce_CASSANDRA-13587.txt,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-08-29 23:04:30.368,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 30 13:38:55 UTC 2017,,,,,,0|i3g4cv:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,09/Jun/17 23:12;chovatia.jaydeep@gmail.com;Please find patch attached.,09/Jun/17 23:15;chovatia.jaydeep@gmail.com;Please find GitHub fix here: https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:CASSANDRA-13587,12/Jun/17 23:21;chovatia.jaydeep@gmail.com;Reproducible steps patch,"29/Aug/17 23:04;jasobrown;[~chovatia.jaydeep@gmail.com] Thanks for the patch. I've created a branch on my repo and pushed to circleci (for unit tests) and apache jenkins (for dtests)

||3.0||
|[branch|https://github.com/jasobrown/cassandra/tree/13587-3.0]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/232/console]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13587-3.0]|

As {{CommitLogSegmentManager}} is reworked on the 3.11 and trunk branches, it makes a fix on 3.0 less critical. I agree your patch is straight-forward (and easy to review), I'm going to need to ask one or two other folks who are more familiar with the CL path to take a look as well.",29/Aug/17 23:37;jay.zhuang;We deployed the patch to our prod clusters locally for about 2 months now. So far it looks good.,"29/Aug/17 23:45;jasobrown;well, damn, [~jay.zhuang] you should have said so 2 months ago :P. Srsly, though, that is good to hear. If the tests pass, I'll go ahead and commit.","30/Aug/17 13:38;jasobrown;I reread the 3.0 CommitLog code this morning, and determined this change is actually pretty safe. We don't recycle the replayed commit log files (just delete them as of 2.2), so the {{createReserveSegments}} is pretty useless anyway.

+1 and committed as sha {{d03c046acbcfe3e3a9d8fafa628030cc3fc40f34}} to 3.0 only.

Thanks for the patch [~chovatia.jaydeep@gmail.com], and for the nice analysis!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure: snapshot_test.py:TestSnapshot.test_basic_snapshot_and_restore,CASSANDRA-13836,13099215,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,01/Sep/17 13:02,12/Mar/19 14:14,13/Mar/19 22:35,01/Sep/17 13:58,4.0,,,,,,,,,,0,,,,Looks like sstableloader always tries to use SSL since CASSANDRA-12229 and that makes the dtest hang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-01 13:24:54.654,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 01 13:58:20 UTC 2017,,,,,,0|i3jjsv:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"01/Sep/17 13:05;krummas;https://github.com/krummas/cassandra/commits/marcuse/13836

also seems we now again need the System.exit(0) there, we should probably have a look at that as well

[~jasobrown] could you review?","01/Sep/17 13:24;jasobrown;Do we need to uncomment the {{System.exit()}} in {{BulkLoader}}? That was not one the changes from CASSANDRA-12229, but from CASSANDRA-10637 (1.5 years ago)

Otherwise +1","01/Sep/17 13:27;krummas;I'm guessing CASSANDRA-12229 or CASSANDRA-8457 introduced some non-daemon threads so that we need the System.exit again? sstableloader exits fine before those two went in, but after it hangs","01/Sep/17 13:49;jasobrown;[~krummas] and I discussed offline, and we'll commit his current patch as-is, and open a new ticket for the daemon threads issue. This way we can unblock dtests on trunk.

UPDATE: created CASSANDRA-13837",01/Sep/17 13:58;krummas;committed as {{fb0e0019e76eb96659904}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAPTURE not easilly usable with PAGING,CASSANDRA-13743,13092403,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iksaif,iksaif,iksaif,04/Aug/17 08:23,12/Mar/19 14:14,13/Mar/19 22:35,11/Aug/17 01:03,4.0,,,,,Legacy/Tools,,,,,0,,,,See https://github.com/iksaif/cassandra/commit/7ed56966a7150ced44c375af307685517d7e09a3 for a patch fixing that.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-04 19:18:36.106,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 07:19:13 UTC 2017,,,,,,0|i3ietj:,9223372036854775807,,,,,,,,cnlwsu,cnlwsu,,,,,,,,,,"04/Aug/17 19:18;cnlwsu;+1, if you make a pull request against trunk a bot will pick it up and update link here.","11/Aug/17 01:03;jjirsa;Thanks guys, committed as {{ed0243954f9ab9c5c68a4516a836ab3710891d5b}}

","17/Aug/17 09:49;rgerard;As a side note, I was reading the commit logs and found that the commit message and changelog badly reference this ticket.
In both, CASSANDRA-13473 is used but this ticket is CASSANDRA-13743
https://github.com/apache/cassandra/commit/ed0243954f9ab9c5c68a4516a836ab3710891d5b","17/Aug/17 19:02;jjirsa;Thanks [~rgerard] - I've updated the [CHANGES log|https://github.com/apache/cassandra/commit/c0dc77ed4fa3b16558ce6f92c4ff076b890afc49] appropriately (but I'm not going to back out the commit to fix it there).

- Jeff",28/Aug/17 07:19;iksaif;Thanks for merging it and fixing it :),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure in bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test,CASSANDRA-13576,13077785,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,michael.hamm,michael.hamm,06/Jun/17 20:18,12/Mar/19 14:14,13/Mar/19 22:35,17/Aug/17 14:41,4.0,,,,,,,,,,0,dtest,test-failure,,"example failure:

http://cassci.datastax.com/job/trunk_offheap_dtest/445/testReport/bootstrap_test/TestBootstrap/consistent_range_movement_false_with_rf1_should_succeed_test

{noformat}
Error Message
31 May 2017 04:28:09 [node3] Missing: ['Starting listening for CQL clients']:
INFO  [main] 2017-05-31 04:18:01,615 YamlConfigura.....
See system.log for remainder
{noformat}

{noformat}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/bootstrap_test.py"", line 236, in consistent_range_movement_false_with_rf1_should_succeed_test
    self._bootstrap_test_with_replica_down(False, rf=1)
  File ""/home/automaton/cassandra-dtest/bootstrap_test.py"", line 278, in _bootstrap_test_with_replica_down
    jvm_args=[""-Dcassandra.consistent.rangemovement={}"".format(consistent_range_movement)])
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 696, in start
    self.wait_for_binary_interface(from_mark=self.mark)
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 514, in wait_for_binary_interface
    self.watch_log_for(""Starting listening for CQL clients"", **kwargs)
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 471, in watch_log_for
    raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
""31 May 2017 04:28:09 [node3] Missing: ['Starting listening for CQL clients']:\nINFO  [main] 2017-05-31 04:18:01,615 YamlConfigura.....\n
{noformat}

{noformat}
-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-PKphwD\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'memtable_allocation_type': 'offheap_objects',\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ncassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\ncassandra.protocol: WARNING: Server warning: When increasing replication factor you need to run a full (-full) repair to distribute the data.\ncassandra.connection: WARNING: Heartbeat failed for connection (139927174110160) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 32.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 64.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 128.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 256.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\n--------------------- >> end captured logging << ---------------------""
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13654,,,,,,,,,17/Aug/17 13:02;krummas;Screen Shot 2017-08-17 at 14.46.00.png;https://issues.apache.org/jira/secure/attachment/12882355/Screen+Shot+2017-08-17+at+14.46.00.png,06/Jun/17 20:18;michael.hamm;node1.log;https://issues.apache.org/jira/secure/attachment/12871676/node1.log,06/Jun/17 20:18;michael.hamm;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12871677/node1_debug.log,06/Jun/17 20:18;michael.hamm;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12871678/node1_gc.log,06/Jun/17 20:18;michael.hamm;node2.log;https://issues.apache.org/jira/secure/attachment/12871679/node2.log,06/Jun/17 20:18;michael.hamm;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12871680/node2_debug.log,06/Jun/17 20:18;michael.hamm;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12871681/node2_gc.log,06/Jun/17 20:18;michael.hamm;node3.log;https://issues.apache.org/jira/secure/attachment/12871682/node3.log,06/Jun/17 20:18;michael.hamm;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12871683/node3_debug.log,06/Jun/17 20:18;michael.hamm;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12871684/node3_gc.log,,,10.0,,,,,,,,,,,,,,,,,,,2017-06-19 17:28:07.155,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 17 14:41:50 UTC 2017,,,,,,0|i3fy3z:,9223372036854775807,,,,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,"19/Jun/17 17:28;jkni;It looks like this has consistently been failing, with or without vnodes, since [CASSANDRA-4650] was committed.","04/Jul/17 05:25;jasonstack;{quote}
useStrictConsistency = cassandra.consistent.rangemovement(default true) && !replacing;

useStrictSource = useStrictConsistency && tokens != null && tokenMetadata.getSizeOfAllEndpoints() != strategy.getReplicationFactor(); 

if (useStrictSource == true)
  -> getRangeFetchMap() // it handles the case of {{useStrictConsistency=false}} and rf=1 added in [11848|https://issues.apache.org/jira/browse/CASSANDRA-11848?focusedCommentId=15299052&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15299052]
else 
  -> getOtimizedRangeFetchMap()  // added in 4650, it doesn't handle the case of {{useStrictConsistency=false}} and rf=1.  so test failed.  added in [4650|https://issues.apache.org/jira/browse/CASSANDRA-4650]
{quote}

one simple fix would be change {{getOtimizedRangeFetchMap}} to handle the special case when {{rf=1 and useStrictConsistency=false}}..   

my understanding on [4650|https://issues.apache.org/jira/browse/CASSANDRA-4650] is that it values throughput over consistency. ( the optimization is applied when {{cassandra.consistent.rangemovement=false}} or {{node count equals to RF(rare cases)}})

[~jkni] what do you think?",07/Jul/17 15:09;jasonstack;:D  LGTM,"14/Aug/17 16:14;krummas;not sure what happened here [~ifesdjeen] why did you remove your branch+comment?

anyway, [here|https://github.com/krummas/cassandra/tree/marcuse/13576] is a patch to not optimise if rf == 1

dtest run:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/182","15/Aug/17 11:24;ifesdjeen;[~krummas] I can't recall the details anymore, iirc someone mentioned it should've been fixed in the scope of some bigger issue, so I retracted my changes, although it seems that it was never committed.

I've pushed the changes [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13576-trunk] again, and I can say they're more or less identical to yours, I've just extracted {{AbstractReplicationStrategy}} to the separate variable to avoid looking it up several times.","15/Aug/17 16:54;krummas;I'm fine just not optimising rf = 1

As my patch has spent more than 15h in queue on builds.apache.org, lets commit that one once it is finished? Could you review [~ifesdjeen]?","16/Aug/17 08:33;ifesdjeen;+1, LGTM","17/Aug/17 14:09;krummas;dtests failures look flaky (and the ones that could be related fail on trunk as well): [^Screen Shot 2017-08-17 at 14.46.00.png]

waiting for a clean circleci build before committing: https://circleci.com/gh/krummas/cassandra/72","17/Aug/17 14:41;krummas;circle ci looks ok - {{testFixedSize - org.apache.cassandra.db.commitlog.CommitLogStressTest}} failed, but that is unrelated

commited as 4f5bf0b67d2e0a93595cc8061018b20aa2309566",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure: repair_tests.incremental_repair_test:TestIncRepair.consistent_repair_test,CASSANDRA-13755,13093930,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jkni,bdeggleston,bdeggleston,10/Aug/17 22:03,12/Mar/19 14:14,13/Mar/19 22:35,10/Aug/17 22:35,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-10 23:25:53.817,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 10 23:25:53 UTC 2017,,,,,,0|i3inzj:,9223372036854775807,,,,,,,,bdeggleston,bdeggleston,,,,,,,,,,10/Aug/17 22:24;bdeggleston;patch by [~jkni] here: https://github.com/jkni/cassandra-dtest/commit/f55f78b093fc668dc5cc9d1fc72f66dc5a9bf3a6,10/Aug/17 22:35;bdeggleston;committed as {{013efa11f3d7bd2e3f64a4a5a865ff5dad565552}} thanks!,10/Aug/17 23:25;jkni;Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more (incremental) repair metrics,CASSANDRA-13531,13071898,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,15/May/17 07:33,12/Mar/19 14:14,13/Mar/19 22:35,15/May/17 14:45,4.0,,,,,,,,,,0,,,,"Patch adds the following metrics;
* time spent anticompacting data before participating in a consistent repair
* time spent creating merkle trees
* time spent syncing data in a repair
* approximate number of bytes read while creating merkle trees
* number of partitions read creating merkle trees
* number of bytes read while doing anticompaction
* number of bytes where the whole sstable was contained in a repairing range so that we only mutated the repair status
* ratio of how much we anticompact vs how much we could mutate the repair status
* total time spent as a repair coordinator
* total time spent preparing for repair

https://github.com/krummas/cassandra/commits/marcuse/metrics-trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-05-15 13:47:58.355,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon May 15 14:45:36 UTC 2017,,,,,,0|i3eylr:,9223372036854775807,,,,,,,,cnlwsu,cnlwsu,,,,,,,,,,15/May/17 07:34;krummas;could you review this [~cnlwsu] ?,15/May/17 13:47;cnlwsu;+1 lgtm,"15/May/17 14:45;krummas;thanks, committed

also added some docs about the new metrics to doc/source/operating/metrics.rst",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cdc column addition strikes again,CASSANDRA-13382,13059334,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,27/Mar/17 09:26,12/Mar/19 14:14,13/Mar/19 22:35,28/Mar/17 12:43,3.11.0,,,,,,,,,,0,,,,"This is a followup of CASSANDRA-12697, where the patch mistakenly only handled the {{system_schema.tables}} table, while the {{cdc}} column has been added to {{system_schema.views}} table.

The patch is pretty trivial, though this highlight that we don't seem to have upgrade tests for materialized views.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-27 15:14:09.659,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 12:43:18 UTC 2017,,,,,,0|i3ctqf:,9223372036854775807,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"27/Mar/17 09:33;slebresne;Simple patch below:
| [13382-3.11|https://github.com/pcmanus/cassandra/commits/13382-3.11] | [utests|http://cassci.datastax.com/job/pcmanus-13382-3.11-testall] | [dtests|http://cassci.datastax.com/job/pcmanus-13382-3.11-dtest] |

I'll try to write a new upgrade dtest to show this, but as I've never really been able to run upgrade tests locally, I won't promise to sink 2 days of my time into it if it doesn't go smoothly.","27/Mar/17 12:59;slebresne;Alright, dtest to reproduce is [here|https://github.com/riptano/cassandra-dtest/pull/1454].",27/Mar/17 15:14;JoshuaMcKenzie;Try not to --author my name on this one. :),27/Mar/17 15:38;jjordan;+1 Looks good to me.  addViewToSchemaMutation calls addTableParamsToRowBuilder which was covered by the fix in CASSANDRA-12236.  So the only MV cdc thing that still needs catching is in makeUpdateForSchema which this patch covers.,"27/Mar/17 16:27;jjirsa;Minor nit that the comment is grammatically confusing (probably accidental if -> is):

{code}
+     * The tables to which we added the cdc column. This is used in {@link #makeUpdateForSchema} below to make sure we skip that
+     * column is cdc is disabled as the columns breaks pre-cdc to post-cdc upgrades (typically, 3.0 -> 3.X).
{code}

","28/Mar/17 09:27;iamaleksey;+1, LGTM too.","28/Mar/17 12:43;slebresne;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incremental repair coordinator sometimes doesn't send commit messages,CASSANDRA-13673,13084912,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,05/Jul/17 18:04,12/Mar/19 14:14,13/Mar/19 22:35,06/Jul/17 17:33,4.0,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-06 07:21:24.93,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:33:43 UTC 2017,,,,,,0|i3h55j:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"05/Jul/17 20:24;bdeggleston;If the repair executor was shutdown before the commit message is sent, none of the replicas will receive commit messages, so none of IR sessions will complete. This is because the repair complete callback shuts down the executor in question. For some reason I made the message sending happen in another thread, but since MessagingService.send* just puts stuff on a queue (or worst case starts a few messaging threads), this seems like overkill. Making the sendMessage stuff happen synchronously fixes the issue.

[trunk|https://github.com/bdeggleston/cassandra/tree/13673]
[utests|https://circleci.com/gh/bdeggleston/cassandra/63]",06/Jul/17 07:21;krummas;+1,06/Jul/17 17:33;bdeggleston;committed as {{3234c0704a4fef08dedc4ff78f4ded3b9226fe80}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve TRUNCATE performance with many sstables,CASSANDRA-13909,13105327,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,27/Sep/17 07:33,12/Mar/19 14:14,13/Mar/19 22:35,02/Oct/17 07:42,3.0.15,3.11.1,4.0,,,,,,,,0,,,,"Truncate is very slow in 3.0, mostly due to {{LogRecord.make}} listing all files in the directory for every sstable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-28 01:11:36.269,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 02 07:42:22 UTC 2017,,,,,,0|i3klcf:,9223372036854775807,,,,,,,,Stefania,Stefania,,,,,,,,,,"27/Sep/17 07:38;krummas;patch [here|https://github.com/krummas/cassandra/commits/marcuse/bulkobsolete] - it adds a bulk obsoletion for truncate
dtests: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/340/
utests: https://circleci.com/gh/krummas/cassandra/120

[~Stefania] do you have time to review this?

edit: my small benchmarks show that this reduces truncate time on a 1600sstable node from 60s to 6s","28/Sep/17 01:11;Stefania;Sure, I can review it.","28/Sep/17 06:37;Stefania;LGTM - I only found 3 nits, see comments [here|https://github.com/krummas/cassandra/commit/9ea54dc47f75d0296dde4f74fd6f4d3de392f991].

There seem to be timeouts in both the dtests (they timed out on manual_join_test) and the unit tests. I doubt very much that they are related to this patch but the main [builds|https://builds.apache.org/view/A-D/view/Cassandra/] are not affected by timeouts (although they did not run in almost a month). I've verified that the unit test that timed out passes locally.

+1 to commit if you are sure that these timeouts are happening on other branches too.


","28/Sep/17 06:47;krummas;Thanks for the review

Seems I linked the wrong dtest build, running them again [here|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/341] before committing

I have seen the unit test failure before, so that shouldn't be caused by this","29/Sep/17 01:19;Stefania;14 failures in the dtests, but they all failed 14+ times in older builds too, mostly are due to {{Could not do a git clone}}. 

I think we can merge this.","29/Sep/17 07:17;krummas;Didn't merge cleanly to 3.11/trunk, so running dtests on those branches before merging
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/345/
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/346/","02/Oct/17 07:42;krummas;Thanks for the review, didn't see anything suspicious in the test results, committed as {{b32a9e6452c78e6ad08e371314bf1ab7492d0773}} to 3.0 and merged up",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pending repair info was added in 4.0,CASSANDRA-13420,13062067,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,06/Apr/17 10:34,12/Mar/19 14:14,13/Mar/19 22:35,13/Apr/17 11:40,4.0,,,,,,,,,,0,,,,"Pending repair information was actually added in 4.0

https://github.com/krummas/cassandra/commits/marcuse/pendingrepairversion",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-10 17:35:53.62,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 13 11:40:42 UTC 2017,,,,,,0|i3dalb:,9223372036854775807,,,,,,,,bdeggleston,bdeggleston,,,,,,,,,,10/Apr/17 17:35;bdeggleston;+1,"13/Apr/17 11:40;krummas;and committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WriteResponseHandlerTest is sensitive to test execution order,CASSANDRA-13421,13062240,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,06/Apr/17 20:32,12/Mar/19 14:14,13/Mar/19 22:35,02/May/17 21:27,4.0,,,,,Legacy/Testing,,,,,0,,,,One of the tests checks statistics that aren't reset between tests. If the test doesn't run first then the expected values are incorrect.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-14 20:37:45.838,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue May 02 21:27:49 UTC 2017,,,,,,0|i3dbnr:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"06/Apr/17 21:59;aweisberg;||Code|utests||
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13421-trunk?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/9]|","14/Apr/17 20:37;jjirsa;lgtm, +1
",02/May/17 21:27;aweisberg;Committed as [d4933a019d8a717029444887f5f8a72d61cedd95|https://github.com/apache/cassandra/commit/d4933a019d8a717029444887f5f8a72d61cedd95],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
max_hints_delivery_threads does not work,CASSANDRA-13329,13056184,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,Gerrrr,Fuud,Fuud,15/Mar/17 06:25,12/Mar/19 14:14,13/Mar/19 22:35,07/Apr/17 10:34,3.11.0,4.0,,,,,,,,,0,lhf,,,"HintsDispatchExecutor creates JMXEnabledThreadPoolExecutor with corePoolSize  == 1 and maxPoolSize==max_hints_delivery_threads and unbounded LinkedBlockingQueue.

In this configuration additional threads will not be created.

Same problem with PerSSTableIndexWriter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-24 12:36:03.377,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 21 09:40:21 UTC 2017,,,,,,0|i3caaf:,9223372036854775807,,,,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,"24/Mar/17 12:36;Gerrrr;{{JMXEnabledThreadPoolExecutor}} (used by {{HintsDispatchExecutor}} and {{PerSSTableIndexWriter}}) extends {{DebuggableThreadPoolExecutor}}.

According to the docs on {{DebuggableThreadPoolExecutor}} it works in the following way:
1. If fewer than corePoolSize threads are running, the Executor always prefers adding a new thread rather than queuing.
2. If corePoolSize or more threads are running, the Executor always prefers queuing a request rather than adding a new thread.
3. If a request cannot be queued, a new thread is created unless this would exceed maximumPoolSize, in which case, the task will be rejected.

In both {{HintsDispatchExecutor}} and {{PerSSTableIndexWriter}}, {{JMXEnabledThreadPoolExecutor}} is constructed with corePoolSize equal to 1, maximumPoolSize equal to some constant and a work queue being unbounded {{LinkedBlockingQueue}}. In that setup when there are no tasks running, the new incoming task will add a thread to the pool (according to #1). However, because the queue is unbounded, according to #2 all the consequent tasks will be added to the queue instead of adding threads to the pool. Having corePoolSize equal to maximumPoolSize solves the problem because then the pool will maintain maximumPoolSize threads and submit tasks to them before queueing.

*Link to the branch*: https://github.com/Gerrrr/cassandra/tree/13329-3.10","28/Mar/17 14:58;ifesdjeen;+1, LGTM

I've written a small test to verify the behaviour [here|https://github.com/ifesdjeen/cassandra/commit/8c2a322bf0bdaf0d198384992caa793f8289b533] although I do not think we should include it into the final patch, since it's rather a configuration/initialisation problem.

And triggered CI for 3.11 and trunk: 

|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13329-trunk]|[dtest|https://cassci.datastax.com/job/ifesdjeen-13329-trunk-dtest/]|[utest|https://cassci.datastax.com/job/ifesdjeen-13329-trunk-testall/]|
|[3.11|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13329-3.11]|[dtest|https://cassci.datastax.com/job/ifesdjeen-13329-trunk-dtest/]|[utest|https://cassci.datastax.com/job/ifesdjeen-13329-3.11-testall/]|",07/Apr/17 10:34;ifesdjeen;Committed to 3.11 as [8eeea07f5f74eb86403e84464107b75c5063cf6b|https://github.com/apache/cassandra/commit/8eeea07f5f74eb86403e84464107b75c5063cf6b] and merged to [trunk|https://github.com/apache/cassandra/commit/a0a0e823e3095b9a47a3d11b3d58b14c039e1ced],"10/Apr/17 19:33;alekiv;[~ifesdjeen], [~Gerrrr]: do you have plans to fix this problem(max_hints_delivery_threads part) in 3.0.x version?","10/Apr/17 19:56;Gerrrr;Hey [~alekiv],

Thank you for the suggestion! I have created a branch for 3.0 - https://github.com/Gerrrr/cassandra/tree/13329-3.0
The fix there is only for HintsDispatchExecutor since there was no PerSSTableIndexWriter.","04/Jul/17 09:11;alekiv;Hello [~Gerrrr],

do you have plans to create pull request for 3.0.x version?","16/Jul/17 12:38;Gerrrr;Hi [~ifesdjeen],
Can you please check the patch for 3.0 https://github.com/Gerrrr/cassandra/tree/13329-3.0 ?

Thank you!","21/Jul/17 09:40;ifesdjeen;Committed the backport to 3.0 with [b337c690d321f2e4d7ebbbb0a1b8a90f986d21e9|https://github.com/apache/cassandra/commit/b337c690d321f2e4d7ebbbb0a1b8a90f986d21e9], merged up to [3.11|https://github.com/apache/cassandra/commit/bf0a4b9cd6324e9c5adfe8cdd72ecb1c3a70568a] and [trunk|https://github.com/apache/cassandra/commit/8d2fa65f8a7abbc27b0c3ff0820af2945fcf7496]. I've also taken a chance to make call syntax consistent across the branches (1-arg call instead of passing same var twice).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incremental repair not streaming correct sstables,CASSANDRA-13328,13056123,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,14/Mar/17 22:51,12/Mar/19 14:14,13/Mar/19 22:35,16/Mar/17 22:42,4.0,,,,,,,,,,0,,,,"Looks like I forgot to update the logic in {{StreamSession.getSSTableSectionsForRanges}} for CASSANDRA-9143. As a result, the updated incremental repair is still streaming all unrepaired sstables for the requested token range.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-15 07:19:04.206,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 16 22:42:22 UTC 2017,,,,,,0|i3c9wv:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,14/Mar/17 22:55;bdeggleston;|[branch|https://github.com/bdeggleston/cassandra/tree/CASSANDRA-13328]|[dtest|http://cassci.datastax.com/view/Dev/view/bdeggleston/job/bdeggleston-CASSANDRA-13328-dtest/]|[testall|http://cassci.datastax.com/view/Dev/view/bdeggleston/job/bdeggleston-CASSANDRA-13328-testall/]|,15/Mar/17 07:19;krummas;+1,16/Mar/17 22:42;bdeggleston;Committed as d6a701ea11c919938cb09b0fca2ea0ec7ad2123b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra can't start because of unknown type <usertype> exception,CASSANDRA-13739,13092181,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,Andrey.Khashchin,Andrey.Khashchin,03/Aug/17 12:27,12/Mar/19 14:14,13/Mar/19 22:35,05/Aug/17 15:26,3.7,,,,,Legacy/Core,Legacy/CQL,,,,0,,,,"OS: CentOS Linux release 7.2.1511 (Core)
Kernel: 3.10.0-327.36.1.el7.x86_64
Cassandra: v 3.7

Issue:

Cassandra is not able to start after restart of the node with following error in system.log:

ERROR [main] 2017-08-03 12:10:28,633 CassandraDaemon.java:731 - Exception encountered during startup
org.apache.cassandra.exceptions.InvalidRequestException: Unknown type <name of user type>
        at org.apache.cassandra.cql3.CQL3Type$Raw$RawUT.prepare(CQL3Type.java:751) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.cql3.CQL3Type$Raw$RawCollection.prepare(CQL3Type.java:667) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.cql3.CQL3Type$Raw$RawCollection.prepareInternal(CQL3Type.java:644) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.Types$RawBuilder$RawUDT.lambda$prepare$34(Types.java:313) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[na:1.8.0_102]
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) ~[na:1.8.0_102]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[na:1.8.0_102]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[na:1.8.0_102]
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[na:1.8.0_102]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:1.8.0_102]
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[na:1.8.0_102]
        at org.apache.cassandra.schema.Types$RawBuilder$RawUDT.prepare(Types.java:314) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.Types$RawBuilder.build(Types.java:263) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchTypes(SchemaKeyspace.java:920) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:891) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesWithout(SchemaKeyspace.java:869) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchNonSystemKeyspaces(SchemaKeyspace.java:857) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:136) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:126) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:251) [apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:585) [apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:714) [apache-cassandra-3.7.0.jar:3.7.0]

Thank you for your help!",Development,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-03 13:51:46.447,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 05 15:26:50 UTC 2017,,,,,,0|i3idgf:,9223372036854775807,3.7,,,,,,,,,,,3.7,,,,,,,"03/Aug/17 13:51;iamaleksey;Seems like you have dropped a user type that some other types still depend on. It shouldn't happen normally because of validations, but they aren't perfect.

Have you dropped a type recently?

If not, then it might be a corruption issue, and you might have to copy over schema sstables from another node to the failing one.","03/Aug/17 14:47;Andrey.Khashchin;Aleksey, we really have heavy dependencies between several usertypes and tables but as I know there were no direct DDL actions against this particular usertype mentioned in the system.log
At the same time, there was VM redeployment and this fact may lead to some data corruption, indeed.

Do we need to copy sstables related to the custom keyspace (which contains original usertype mentioned in the log) or do we need to copy some system keyspaces (like system_schema) maybe? Unfortunately, we have one node configuration so there are no other nodes in the cluster but there are some similar standalone installations (QA) of Cassandra on other hosts","03/Aug/17 14:53;iamaleksey;You'd need to copy over system_schema.types sstables. Just make sure the data you are copying over is a superset, so that replacing your node's data wouldn't cause a loss of anything.","03/Aug/17 16:37;Andrey.Khashchin;We copied system_schema.types sstables from another one machine to the failing one, original error disappeared but we have new issue, very similar to previous but related to another one usertype in different custom keyspace  ",05/Aug/17 15:26;Andrey.Khashchin;Original issue was resolved by deleting extra sstables in system_schema.type folder which were generated as result of VM redeployment. Cassandra is able to continue to boot now.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in repair_tests.incremental_repair_test.TestIncRepair.sstable_marking_test,CASSANDRA-13372,13058607,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,sean.mccarthy,sean.mccarthy,23/Mar/17 14:20,12/Mar/19 14:14,13/Mar/19 22:35,11/May/17 20:22,,,,,,Test/dtest,,,,,0,dtest,test-failure,,"example failure:

http://cassci.datastax.com/job/trunk_dtest/1525/testReport/repair_tests.incremental_repair_test/TestIncRepair/sstable_marking_test

{code}
Error Message

'Repaired at: 0' unexpectedly found in 'SSTable: /tmp/dtest-qoNeEc/test/node1/data0/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-4-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1490129948985001\nMaximum timestamp: 1490129952789002\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: -9222701292667950301 (key=5032394c323239385030)\nLast token: -3062233317334255711 (key=3032503434364f4e4f30)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf\nReplay positions covered: {CommitLogPosition(segmentId=1490129923946, position=42824)=CommitLogPosition(segmentId=1490129923946, position=2605214)}\ntotalColumnsSet: 16550\ntotalRows: 3310\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0              3310\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        1                 0\n258                     3309                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 3310\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1490129948985001\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\nSSTable: /tmp/dtest-qoNeEc/test/node1/data1/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-5-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1490129948987000\nMaximum timestamp: 1490129952789004\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: -3060251208033125494 (key=33364e4b313936504b30)\nLast token: 2923054332122545251 (key=344b3634354b35353430)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf\nReplay positions covered: {CommitLogPosition(segmentId=1490129923946, position=42824)=CommitLogPosition(segmentId=1490129923946, position=2605214)}\ntotalColumnsSet: 16565\ntotalRows: 3313\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0              3313\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        1                 0\n258                     3312                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 3313\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1490129948987000\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\nSSTable: /tmp/dtest-qoNeEc/test/node1/data2/keyspace1/standard1-3674b7a00e7911e78a4625bec3430063/na-6-big\nPartitioner: org.apache.cassandra.dht.Murmur3Partitioner\nBloom Filter FP chance: 0.010000\nMinimum timestamp: 1490129948984000\nMaximum timestamp: 1490129952789003\nSSTable min local deletion time: 2147483647\nSSTable max local deletion time: 2147483647\nCompressor: -\nTTL min: 0\nTTL max: 0\nFirst token: 2925175199546211606 (key=3250365032354e4c3231)\nLast token: 9222137691148971235 (key=4c30334f32394d4c3031)\nEstimated droppable tombstones: 0.0\nSSTable Level: 0\nRepaired at: 0\nPending repair: 45a396b0-0e79-11e7-841e-2d88b3d470cf\nReplay positions covered: {CommitLogPosition(segmentId=1490129923946, position=42824)=CommitLogPosition(segmentId=1490129923946, position=2605214)}\ntotalColumnsSet: 16885\ntotalRows: 3377\nEstimated tombstone drop times:\nCount               Row Size        Cell Count\n1                          0                 0\n2                          0                 0\n3                          0                 0\n4                          0                 0\n5                          0              3377\n6                          0                 0\n7                          0                 0\n8                          0                 0\n10                         0                 0\n12                         0                 0\n14                         0                 0\n17                         0                 0\n20                         0                 0\n24                         0                 0\n29                         0                 0\n35                         0                 0\n42                         0                 0\n50                         0                 0\n60                         0                 0\n72                         0                 0\n86                         0                 0\n103                        0                 0\n124                        0                 0\n149                        0                 0\n179                        0                 0\n215                        1                 0\n258                     3376                 0\n310                        0                 0\n372                        0                 0\n446                        0                 0\n535                        0                 0\n642                        0                 0\n770                        0                 0\n924                        0                 0\n1109                       0                 0\n1331                       0                 0\n1597                       0                 0\n1916                       0                 0\n2299                       0                 0\n2759                       0                 0\n3311                       0                 0\n3973                       0                 0\n4768                       0                 0\n5722                       0                 0\n6866                       0                 0\n8239                       0                 0\n9887                       0                 0\n11864                      0                 0\n14237                      0                 0\n17084                      0                 0\n20501                      0                 0\n24601                      0                 0\n29521                      0                 0\n35425                      0                 0\n42510                      0                 0\n51012                      0                 0\n61214                      0                 0\n73457                      0                 0\n88148                      0                 0\n105778                     0                 0\n126934                     0                 0\n152321                     0                 0\n182785                     0                 0\n219342                     0                 0\n263210                     0                 0\n315852                     0                 0\n379022                     0                 0\n454826                     0                 0\n545791                     0                 0\n654949                     0                 0\n785939                     0                 0\n943127                     0                 0\n1131752                    0                 0\n1358102                    0                 0\n1629722                    0                 0\n1955666                    0                 0\n2346799                    0                 0\n2816159                    0                 0\n3379391                    0                 0\n4055269                    0                 0\n4866323                    0                 0\n5839588                    0                 0\n7007506                    0                 0\n8409007                    0                 0\n10090808                   0                 0\n12108970                   0                 0\n14530764                   0                 0\n17436917                   0                 0\n20924300                   0                 0\n25109160                   0                 0\n30130992                   0                 0\n36157190                   0                 0\n43388628                   0                 0\n52066354                   0                 0\n62479625                   0                 0\n74975550                   0                 0\n89970660                   0                 0\n107964792                  0                 0\n129557750                  0                 0\n155469300                  0                 0\n186563160                  0                 0\n223875792                  0                 0\n268650950                  0                 0\n322381140                  0                 0\n386857368                  0                 0\n464228842                  0                 0\n557074610                  0                 0\n668489532                  0                 0\n802187438                  0                 0\n962624926                  0                 0\n1155149911                 0                 0\n1386179893                 0                 0\n1663415872                 0                 0\n1996099046                 0                 0\n2395318855                 0                 0\n2874382626                 0                  \n3449259151                 0                  \n4139110981                 0                  \n4966933177                 0                  \n5960319812                 0                  \n7152383774                 0                  \n8582860529                 0                  \n10299432635                 0                  \n12359319162                 0                  \n14831182994                 0                  \n17797419593                 0                  \n21356903512                 0                  \n25628284214                 0                  \n30753941057                 0                  \n36904729268                 0                  \n44285675122                 0                  \n53142810146                 0                  \n63771372175                 0                  \n76525646610                 0                  \n91830775932                 0                  \n110196931118                 0                  \n132236317342                 0                  \n158683580810                 0                  \n190420296972                 0                  \n228504356366                 0                  \n274205227639                 0                  \n329046273167                 0                  \n394855527800                 0                  \n473826633360                 0                  \n568591960032                 0                  \n682310352038                 0                  \n818772422446                 0                  \n982526906935                 0                  \n1179032288322                 0                  \n1414838745986                 0                  \nEstimated cardinality: 3377\nEncodingStats minTTL: 0\nEncodingStats minLocalDeletionTime: 1442880000\nEncodingStats minTimestamp: 1490129948984000\nKeyType: org.apache.cassandra.db.marshal.BytesType\nClusteringTypes: [org.apache.cassandra.db.marshal.UTF8Type]\nStaticColumns: {C3:org.apache.cassandra.db.marshal.BytesType, C4:org.apache.cassandra.db.marshal.BytesType, C0:org.apache.cassandra.db.marshal.BytesType, C1:org.apache.cassandra.db.marshal.BytesType, C2:org.apache.cassandra.db.marshal.BytesType}\nRegularColumns: {}\n'
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/repair_tests/incremental_repair_test.py"", line 332, in sstable_marking_test
    self.assertNotIn('Repaired at: 0', out)
  File ""/usr/lib/python2.7/unittest/case.py"", line 810, in assertNotIn
    self.fail(self._formatMessage(msg, standardMsg))
  File ""/usr/lib/python2.7/unittest/case.py"", line 410, in fail
    raise self.failureException(msg)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Mar/17 14:20;sean.mccarthy;node1.log;https://issues.apache.org/jira/secure/attachment/12860149/node1.log,23/Mar/17 14:20;sean.mccarthy;node1_debug.log;https://issues.apache.org/jira/secure/attachment/12860147/node1_debug.log,23/Mar/17 14:20;sean.mccarthy;node1_gc.log;https://issues.apache.org/jira/secure/attachment/12860148/node1_gc.log,23/Mar/17 14:20;sean.mccarthy;node2.log;https://issues.apache.org/jira/secure/attachment/12860152/node2.log,23/Mar/17 14:20;sean.mccarthy;node2_debug.log;https://issues.apache.org/jira/secure/attachment/12860150/node2_debug.log,23/Mar/17 14:20;sean.mccarthy;node2_gc.log;https://issues.apache.org/jira/secure/attachment/12860151/node2_gc.log,23/Mar/17 14:20;sean.mccarthy;node3.log;https://issues.apache.org/jira/secure/attachment/12860155/node3.log,23/Mar/17 14:20;sean.mccarthy;node3_debug.log;https://issues.apache.org/jira/secure/attachment/12860153/node3_debug.log,23/Mar/17 14:20;sean.mccarthy;node3_gc.log;https://issues.apache.org/jira/secure/attachment/12860154/node3_gc.log,,,,9.0,,,,,,,,,,,,,,,,,,,2017-05-11 20:22:46.648,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu May 11 20:22:46 UTC 2017,,,,,,0|i3cp8v:,9223372036854775807,,,,,,,,,,,,,,,,,,,"11/May/17 20:22;bdeggleston;This hasn't failed since March, and CASSANDRA-13454 would have fixed any problems with sstables not being promoted to repaired after a repair",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra core connector - Guava incompatibility: Detected incompatible version of Guava in the classpath. You need 16.0.1 or higher,CASSANDRA-13524,13071092,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,michael.hornung@salt-solutions.de,michael.hornung@salt-solutions.de,11/May/17 08:43,12/Mar/19 14:14,13/Mar/19 22:35,11/May/17 09:43,,,,,,,,,,,0,,,,"Hallo,

with my application I have a AKKA-http microservice which want’s to acess a cassandra database table from scala.
Therefore I included this dependency in SBT:
         ""com.datastax.cassandra"" % ""cassandra-driver-core""       % ""3.2.0""
In my scalafile I have this coding:
--------------------------------------------------------------------------
….
import com.datastax.driver.core.Cluster
import com.google.common.util.concurrent._
….
    val cassandraHost    = ""localhost""
    val keyStore         = ""data4service""
    //setup cassandra
    val cluster = {
                    Cluster.builder()
                      .addContactPoint(cassandraHost)
                      .withCredentials(""user"", ""password"")
                      .build()
                  }
    //connect to cassandra keystore
    val session = cluster.connect(keyStore)
    val product = ""123e4567-e89b-12d3-a456-426655440003""
    val select = s""SELECT quantity FROM stock WHERE product = $product""
    val result = session.execute(select)   
….
--------------------------------------------------------------------------

During build guava Version 19.0 is downloaded automatically

Unfortunately if I run my application I get this Error during runtime: 
--------------------------------------------------------------------------
com.datastax.driver.core.exceptions.DriverInternalError: Detected incompatible version of Guava in the classpath. You need 16.0.1 or higher.
        at com.datastax.driver.core.GuavaCompatibility.selectImplementation(GuavaCompatibility.java:138)
        at com.datastax.driver.core.GuavaCompatibility.<clinit>(GuavaCompatibility.java:52)
--------------------------------------------------------------------------

that is not logical bevause Guava 19.0 is on the system. Can you help me?

Kind regards,
Michael
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11/May/17 08:43;michael.hornung@salt-solutions.de;build.sbt;https://issues.apache.org/jira/secure/attachment/12867522/build.sbt,11/May/17 08:43;michael.hornung@salt-solutions.de;error_log.txt;https://issues.apache.org/jira/secure/attachment/12867521/error_log.txt,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,2017-05-11 08:43:29.0,,,,,,0|i3etmn:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlshlib tests fail due to compact table,CASSANDRA-14007,13117404,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,jkni,jkni,09/Nov/17 21:55,12/Mar/19 14:14,13/Mar/19 22:35,22/Feb/18 17:32,4.0,,,,,Legacy/Testing,,,,,0,,,,"The pylib/cqlshlib tests fail on initialization with the error {{SyntaxException: <Error from server: code=2000 \[Syntax error in CQL query\] message=""Compact tables are not allowed in Cassandra starting with 4.0 version."">}}. 

The table {{dynamic_columns}} is created {{WITH COMPACT STORAGE}}. Since [CASSANDRA-10857], this is no longer supported. It looks like dropping the COMPACT STORAGE modifier is enough for the tests to run, but I haven't looked if we should instead remove the table and all related tests entirely, or if there's an interesting code path covered by this that we should test in a different way now. [~ifesdjeen] might know at a glance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10857,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-13 10:46:25.326,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 22 17:24:43 UTC 2018,,,,,,0|i3mm6n:,9223372036854775807,4.0,,,,,,,snazy,snazy,,,,,,,,,,13/Nov/17 10:46;ifesdjeen;I've just re-ran all the dtests and they seem to be clean. Or do we run the cqlshlib tests in some other way?..,"13/Nov/17 16:19;jkni;Yeah, the cqlshlib tests have their own script to run and don't run as part of dtests. See [https://github.com/apache/cassandra-builds/blob/f0e63d66269f9086c3a0393a24a55577d21b4454/build-scripts/cassandra-cqlsh-tests.sh] for an example of how to run them.","15/Nov/17 10:58;ifesdjeen;The patch is trivial and can be found [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-14007], there was no real reason to have this table as compact and it doesn't change any output in this case.

However, there are still plenty cqllib tests failing from what seems to be a concurrency or capture issue:

{code}
AssertionError: unexpected echo 'select * from ascii_with_special_chars where k in (0, 1,, 2, 3);' instead of 'select * from ascii_with_special_chars where k in (0, 1, 2, 3);'
{code}

Other tests fail with other duplicated characters. However, this should probably be handled outside of scope of this ticket.",22/Feb/18 17:24;snazy;Patch LGTM. One minor nit: change {{CREATE COLUMNFAMILY}} to {{CREATE TABLE}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set javac encoding to utf-8 in 2.1 and 2.2.,CASSANDRA-13466,13065958,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,21/Apr/17 18:06,12/Mar/19 14:12,13/Mar/19 22:35,21/Apr/17 20:46,2.1.18,2.2.10,,,,Build,,,,,0,,,,"There are non-ASCII characters in 2.1 and 2.2 source code, but javac is not configured to interpret source files in UTF-8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-21 20:10:23.503,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 21 20:46:03 UTC 2017,,,,,,0|i3dxzb:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"21/Apr/17 20:04;aweisberg;||code|utests||
|[2.1|https://github.com/apache/cassandra/compare/cassandra-2.1...aweisberg:cassandra-13466-2.1?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-13466-2%2E1]|
|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...aweisberg:cassandra-13466-2.2?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-13466-2%2E2]|",21/Apr/17 20:10;jasobrown;+1,21/Apr/17 20:46;aweisberg;Committed as [572fef8a06172bd925c478ec16540e8e28e84bd7|https://github.com/apache/cassandra/commit/572fef8a06172bd925c478ec16540e8e28e84bd7],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in IR cleanup when columnfamily has no sstables,CASSANDRA-13585,13078494,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,09/Jun/17 02:15,12/Mar/19 14:12,13/Mar/19 22:35,09/Jun/17 10:55,4.0,,,,,,,,,,0,incremental_repair,,,"On {{PendingAntiCompaction::abort()}} , we try to release refs to sstables and a lifecycle transaction that can be null if there are no sstables in the table.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-06-09 08:27:39.039,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 09 10:55:41 UTC 2017,,,,,,0|i3g2hj:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,09/Jun/17 02:24;jjirsa;|| [trunk branch|https://github.com/jeffjirsa/cassandra/tree/cassandra-13585] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13585] |[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/81/] ||,"09/Jun/17 02:25;jjirsa;Either [~yukim] , [~krummas] or [~bdeggleston] up for quick review?
",09/Jun/17 08:27;krummas;+1,"09/Jun/17 10:55;jjirsa;Thanks Marcus, committed as {{ff29d609979aae2c584ec6e6cd370da56945dc18}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SASI full-text search queries using standard analyzer do not work in multi-node environments,CASSANDRA-13512,13070394,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,09/May/17 12:00,12/Mar/19 14:11,13/Mar/19 22:35,12/Jul/17 10:06,,,,,,Feature/SASI,,,,,0,,,,"SASI full-text search queries using standard analyzer do not work in multi-node environments. Standard Analyzer will rewind the buffer and search term will be empty for any node other than coordinator, so will return no results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-06-23 09:24:41.486,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 12 10:06:07 UTC 2017,,,,,,0|i3epcf:,9223372036854775807,,,,,,,,adelapena,adelapena,,,,,,,,,,"09/May/17 12:20;ifesdjeen;|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...ifesdjeen:13512-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13512-3.11-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13512-3.11-dtest/]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:13512-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13512-trunk-testall/]|[dtest|http://cassci.datastax.com/view/Dev/view/ifesdjeen/job/ifesdjeen-13512-trunk-dtest/]|","23/Jun/17 09:24;adelapena;The patch looks good to me, with some nits about the [dtest|https://github.com/ifesdjeen/cassandra-dtest/tree/13512-master]:
* There should be a {{@since('3.11')}} annotation in [{{multinode_full_text_search}}|https://github.com/ifesdjeen/cassandra-dtest/blob/55334ebb724eb5224572c7f58c4f6157bf1780cd/cql_tests.py#L1444].
* I seems there is a typo [here|https://github.com/ifesdjeen/cassandra-dtest/blob/55334ebb724eb5224572c7f58c4f6157bf1780cd/cql_tests.py#L1442], ""dests"" by ""dtests"".
* This is only a suggestion, I think it would be nice to break [this line|https://github.com/ifesdjeen/cassandra-dtest/blob/55334ebb724eb5224572c7f58c4f6157bf1780cd/cql_tests.py#L1466] to satisfy the 120 char width. ","12/Jul/17 10:06;ifesdjeen;Thank you for the review & sorry for taking so long to get back. I've made the suggested changes to [dtest|https://github.com/riptano/cassandra-dtest/pull/1492]

Committed to 3.11 with [ab640b2123826fd67d31860a9f0ca8a4224e3845|https://github.com/apache/cassandra/commit/ab640b2123826fd67d31860a9f0ca8a4224e3845] and merged up to [trunk|https://github.com/apache/cassandra/commit/86964da69d36bdf3afd65ed77ecccd9dff24757b].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure in bootstrap_test.TestBootstrap.simultaneous_bootstrap_test,CASSANDRA-13506,13070185,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,aweisberg,aweisberg,08/May/17 21:50,12/Mar/19 14:11,13/Mar/19 22:35,26/Nov/18 23:08,,,,,,Test/dtest,,,,,0,dtest,test-failure,test-failure-fresh,"{noformat}
Failed 11 times in the last 30 runs. Flakiness: 62%, Stability: 63%
Error Message

errors={<Host: 127.0.0.2 datacenter1>: ReadTimeout('Error from server: code=1200 [Coordinator node timed out waiting for replica nodes\' responses] message=""Operation timed out - received only 0 responses."" info={\'received_responses\': 0, \'required_responses\': 1, \'consistency\': \'ONE\'}',)}, last_host=127.0.0.2
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-VsuThg
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.1 datacenter1> discovered
cassandra.protocol: WARNING: Server warning: Aggregation query used without partition key
dtest: DEBUG: Retrying read after timeout. Attempt #0
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest/cassandra-dtest/bootstrap_test.py"", line 659, in simultaneous_bootstrap_test
    assert_one(session, ""SELECT count(*) from keyspace1.standard1"", [500000], cl=ConsistencyLevel.ONE)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest/cassandra-dtest/tools/assertions.py"", line 128, in assert_one
    res = session.execute(simple_query)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest/venv/src/cassandra-driver/cassandra/cluster.py"", line 2018, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-trunk-dtest/venv/src/cassandra-driver/cassandra/cluster.py"", line 3822, in result
    raise self._final_exception
'errors={<Host: 127.0.0.2 datacenter1>: ReadTimeout(\'Error from server: code=1200 [Coordinator node timed out waiting for replica nodes\\\' responses] message=""Operation timed out - received only 0 responses."" info={\\\'received_responses\\\': 0, \\\'required_responses\\\': 1, \\\'consistency\\\': \\\'ONE\\\'}\',)}, last_host=127.0.0.2\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-VsuThg\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.1 datacenter1> discovered\ncassandra.protocol: WARNING: Server warning: Aggregation query used without partition key\ndtest: DEBUG: Retrying read after timeout. Attempt #0\n--------------------- >> end captured logging << ---------------------'
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,2017-05-08 21:50:40.0,,,,,,0|i3eo1z:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DTEST] cdc_test::TestCDC::test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space assertion always fails (Extra items in the left set),CASSANDRA-14146,13128387,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,mkjellman,mkjellman,04/Jan/18 02:08,12/Mar/19 14:11,13/Mar/19 22:35,08/Jun/18 22:19,,,,,,Test/dtest,,,,,0,,,,"Dtest cdc_test::TestCDC::test_insertion_and_commitlog_behavior_after_reaching_cdc_total_space always fails on an assertion.

the assert is the final step of the test and it checks that pre_non_cdc_write_cdc_raw_segments == _get_cdc_raw_files(node.get_path())

This fails 100% of the time locally, 100% of the time on circleci executed under pytest, and 100% of the time for the past 40 test runs on ASF Jenkins runs against trunk.

This is the only test failure (excluding flaky one-off failures) remaining on the pytest dtest branch. I'm going to annotate the test with a skip marker (including a reason reference to this JIRA)... when it's fixed we should also remove the skip annotation from the test.

{code}
>       assert pre_non_cdc_write_cdc_raw_segments == _get_cdc_raw_files(node.get_path())
E       AssertionError: assert {'/tmp/dtest-...169.log', ...} == {'/tmp/dtest-v...169.log', ...}
E         Extra items in the left set:
E         '/tmp/dtest-vrn4k8ov/test/node1/cdc_raw/CommitLog-7-1515030005097.log'
E         '/tmp/dtest-vrn4k8ov/test/node1/cdc_raw/CommitLog-7-1515030005098.log'
E         Extra items in the right set:
E         '/tmp/dtest-vrn4k8ov/test/node1/cdc_raw/CommitLog-7-1515030005099.log'
E         '/tmp/dtest-vrn4k8ov/test/node1/cdc_raw/CommitLog-7-1515030005100.log'
E         Use -v to get the full diff
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-06 19:25:39.963,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 06 19:25:39 UTC 2018,,,,,,0|i3ohof:,9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Jan/18 19:25;jay.zhuang;https://github.com/apache/cassandra-dtest/pull/6 fixes the issue.
cc [~JoshuaMcKenzie]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove unused cassandra.yaml setting, max_value_size_in_mb, from 2.2.9",CASSANDRA-13625,13081259,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,j.casares,j.casares,j.casares,20/Jun/17 23:25,12/Mar/19 14:11,13/Mar/19 22:35,22/Jun/17 00:05,2.2.11,,,,,,,,,,0,lhf,,,"{{max_value_size_in_mb}} is currently in the 2.2.9 cassandra.yaml, but does not make reference of the config in any place within its codebase:

https://github.com/apache/cassandra/blob/cassandra-2.2.9/conf/cassandra.yaml#L888-L891

CASSANDRA-9530, which introduced {{max_value_size_in_mb}}, has it's Fix Version/s marked as 3.0.7, 3.7, and 3.8.

Let's remove the {{max_value_size_in_mb}} from the cassandra.yaml from {{>= 2.2.9, < 3}}.

{NOFORMAT}
~/repos/cassandra[(HEAD detached at cassandra-2.2.9)] (joaquin)$ grep -r max_value_size_in_mb .
conf/cassandra.yaml:# max_value_size_in_mb: 256
{NOFORMAT}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-06-21 09:14:30.989,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 22 00:05:13 UTC 2017,,,,,,0|i3giof:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,20/Jun/17 23:27;j.casares;Here's a PR for the single-file fix: https://github.com/apache/cassandra/pull/124.,"21/Jun/17 09:14;rha;Hi, I don't understand why do you want to remove this setting, it's still used in trunk:

{code}
$ grep -IRn getMaxValueSize
test/unit/org/apache/cassandra/db/compaction/BlacklistingCompactionsTest.java:86:        maxValueSize = DatabaseDescriptor.getMaxValueSize();
test/unit/org/apache/cassandra/io/sstable/SSTableWriterTestBase.java:84:        maxValueSize = DatabaseDescriptor.getMaxValueSize();
test/unit/org/apache/cassandra/io/sstable/SSTableCorruptionDetectionTest.java:91:        maxValueSize = DatabaseDescriptor.getMaxValueSize();
src/java/org/apache/cassandra/db/ClusteringPrefix.java:377:                                : (isEmpty(header, offset) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : types.get(offset).readValue(in, DatabaseDescriptor.getMaxValueSize()));
src/java/org/apache/cassandra/db/ClusteringPrefix.java:531:                          : (Serializer.isEmpty(nextHeader, i) ? ByteBufferUtil.EMPTY_BYTE_BUFFER : serializationHeader.clusteringTypes().get(i).readValue(in, DatabaseDescriptor.getMaxValueSize()));
src/java/org/apache/cassandra/db/rows/Cell.java:246:                    value = header.getType(column).readValue(in, DatabaseDescriptor.getMaxValueSize());
src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java:1056:            DecoratedKey key = metadata.partitioner.decorateKey(metadata.partitionKeyType.readValue(in, DatabaseDescriptor.getMaxValueSize()));
src/java/org/apache/cassandra/config/DatabaseDescriptor.java:1054:    public static int getMaxValueSize()
{code}

{{getMaxValueSize()}} occurrences: https://github.com/apache/cassandra/search?q=getMaxValueSize","21/Jun/17 16:09;j.casares;[~rha], I'm not for removing the setting from trunk, just from the 2.2.x cassandra.yaml where the setting has never been implemented.

If you were to switch to the cassandra-2.2.9 branch and run your grep script again, it should come back with no references.","21/Jun/17 16:16;rha;Ok! I understood {{>= 2.2.9}} and not {{>= 2.2.9, < 3}}","21/Jun/17 16:24;j.casares;Sorry for that confusion, I've updated the main description. Thanks for pointing that out! :)","22/Jun/17 00:05;jjirsa;Committed in {{082af0a9ba6b5dde26055fcb9ddd2085e4240381}} , merged {{-s ours}} through 3.0/3.11/trunk, so it's only in 2.2.11.

Thanks!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sstableloader_with_failing_2i_test - sstable_generation_loading_test.TestSSTableGenerationAndLoading fails: Expected [['k', 'idx']] ... but got [[u'k', u'idx', None]]",CASSANDRA-14037,13118698,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jkni,mkjellman,mkjellman,15/Nov/17 22:24,12/Mar/19 14:10,13/Mar/19 22:35,29/Nov/17 17:53,,,,,,Legacy/Testing,,,,,0,,,,"sstableloader_with_failing_2i_test - sstable_generation_loading_test.TestSSTableGenerationAndLoading fails: Expected [['k', 'idx']] ... but got [[u'k', u'idx', None]]

Expected [['k', 'idx']] from SELECT * FROM system.""IndexInfo"" WHERE table_name='k', but got [[u'k', u'idx', None]]
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-2My0fh
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
cassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal
    return self._try_connect(host)
  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect
    connection = self._cluster.connection_factory(host.address, is_control_connection=True)
  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory
    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)
  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory
    conn = cls(host, *args, **kwargs)
  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket
    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))
error: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
--------------------- >> end captured logging << ---------------------
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/cassandra/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/cassandra/cassandra-dtest/sstable_generation_loading_test.py"", line 333, in sstableloader_with_failing_2i_test
    assert_one(session, """"""SELECT * FROM system.""IndexInfo"" WHERE table_name='k'"""""", ['k', 'idx'])
  File ""/home/cassandra/cassandra-dtest/tools/assertions.py"", line 130, in assert_one
    assert list_res == [expected], ""Expected {} from {}, but got {}"".format([expected], query, list_res)
'Expected [[\'k\', \'idx\']] from SELECT * FROM system.""IndexInfo"" WHERE table_name=\'k\', but got [[u\'k\', u\'idx\', None]]\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-2My0fh\ndtest: DEBUG: Done setting configuration options:\n{   \'initial_token\': None,\n    \'num_tokens\': \'32\',\n    \'phi_convict_threshold\': 5,\n    \'range_request_timeout_in_ms\': 10000,\n    \'read_request_timeout_in_ms\': 10000,\n    \'request_timeout_in_ms\': 10000,\n    \'truncate_request_timeout_in_ms\': 10000,\n    \'write_request_timeout_in_ms\': 10000}\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\ncassandra.cluster: WARNING: [control connection] Error connecting to 127.0.0.1:\nTraceback (most recent call last):\n  File ""cassandra/cluster.py"", line 2781, in cassandra.cluster.ControlConnection._reconnect_internal\n    return self._try_connect(host)\n  File ""cassandra/cluster.py"", line 2803, in cassandra.cluster.ControlConnection._try_connect\n    connection = self._cluster.connection_factory(host.address, is_control_connection=True)\n  File ""cassandra/cluster.py"", line 1195, in cassandra.cluster.Cluster.connection_factory\n    return self.connection_class.factory(address, self.connect_timeout, *args, **kwargs)\n  File ""cassandra/connection.py"", line 332, in cassandra.connection.Connection.factory\n    conn = cls(host, *args, **kwargs)\n  File ""/home/cassandra/env/src/cassandra-driver/cassandra/io/asyncorereactor.py"", line 344, in __init__\n    self._connect_socket()\n  File ""cassandra/connection.py"", line 371, in cassandra.connection.Connection._connect_socket\n    raise socket.error(sockerr.errno, ""Tried connecting to %s. Last error: %s"" % ([a[4] for a in addresses], sockerr.strerror or sockerr))\nerror: [Errno 111] Tried connecting to [(\'127.0.0.1\', 9042)]. Last error: Connection refused\n--------------------- >> end captured logging << ---------------------'",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-29 05:52:59.465,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 29 17:53:00 UTC 2017,,,,,,0|i3mu5j:,9223372036854775807,,,,,,,,,,,,,,,,,,,"29/Nov/17 05:52;jkni;One-line patch available [here|https://github.com/jkni/cassandra-dtest/commit/cb77eecfd7fa7a4a584dee46784c543ec9e6e43c]. It looks like this was just an oversight from [CASSANDRA-10857]. Since IndexInfo was originally declared as compact, we need to expect the value column when selecting all columns. This change was already made in one other place in the same test. [~ifesdjeen] to review if interested.","29/Nov/17 14:21;ifesdjeen;+1 [~jkni], I had the same patch locally but I didn't get to submit it. Thank you!",29/Nov/17 17:53;jkni;Thanks for the review. Patch committed to cassandra-dtest as {{fc68a0de8d05082a0a78196695572ff2346179c4}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstabledump tries to delete a file,CASSANDRA-14166,13130544,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,KurtG,Python_Max,Python_Max,12/Jan/18 15:04,12/Mar/19 14:09,13/Mar/19 22:35,08/Mar/18 01:50,3.0.17,3.11.3,4.0,,,Legacy/Tools,,,,,0,,,,"Directory /var/lib/cassandra/data/<keyspace>/<table> has cassandra:cassandra owner.
An error happens when sstabledump executed on file in that directory by regular user:


{code:java}
$ sstabledump mc-56801-big-Data.db
Exception in thread ""main"" FSWriteError in /var/lib/cassandra/data/<keyspace>/<table>/mc-56801-big-Summary.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:142)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:159)
        at org.apache.cassandra.io.sstable.format.SSTableReader.saveSummary(SSTableReader.java:935)
        at org.apache.cassandra.io.sstable.format.SSTableReader.saveSummary(SSTableReader.java:920)
        at org.apache.cassandra.io.sstable.format.SSTableReader.load(SSTableReader.java:788)
        at org.apache.cassandra.io.sstable.format.SSTableReader.load(SSTableReader.java:731)
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:516)
        at org.apache.cassandra.io.sstable.format.SSTableReader.openNoValidation(SSTableReader.java:396)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:191)
Caused by: java.nio.file.AccessDeniedException: /var/lib/cassandra/data/<keyspace>/<table>/mc-56801-big-Summary.db
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
        at java.nio.file.Files.delete(Files.java:1126)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:136)
        ... 8 more

{code}

I have changed bloom_filter_fp_chance for that table couple months ago, so I believe that's the reason why SSTableReader wants to recreate summary file. But when used in sstabledump it should not try to modify / delete any files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-12 16:57:51.775,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 12 16:57:51 UTC 2018,,,,,,0|i3ouuv:,9223372036854775807,3.11.1,,,,,,,,,,,,,,,,,,"12/Jan/18 16:57;jjirsa;Somewhat related to CASSANDRA-11163 , but {{sstabledump}} especially really shouldn't be making any changes.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't skip corrupt sstables on startup,CASSANDRA-13620,13080802,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,19/Jun/17 12:02,12/Mar/19 14:09,13/Mar/19 22:35,28/Aug/17 13:43,3.0.15,3.11.1,4.0,,,,,,,,0,,,,"If we get an IOException when opening an sstable on startup, we just [skip|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java#L563-L567] it and continue starting

we should use the DiskFailurePolicy and never explicitly catch an IOException here",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-7377,,,,,,,,,24/Aug/17 07:04;krummas;13620-3.0.png;https://issues.apache.org/jira/secure/attachment/12883498/13620-3.0.png,24/Aug/17 07:04;krummas;13620-3.11.png;https://issues.apache.org/jira/secure/attachment/12883497/13620-3.11.png,24/Aug/17 07:04;krummas;13620-trunk.png;https://issues.apache.org/jira/secure/attachment/12883499/13620-trunk.png,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-07-05 21:30:31.181,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 13:43:22 UTC 2017,,,,,,0|i3gfuv:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,"19/Jun/17 13:40;krummas;Patch to catch IOExceptions and rethrow them as CorruptSSTableExceptions - if we get an IOException when opening an sstable, it should be considered corrupt

https://github.com/krummas/cassandra/commits/marcuse/13620
https://github.com/krummas/cassandra/commits/marcuse/13620-3.11
https://github.com/krummas/cassandra/commits/marcuse/13620-trunk

The trunk version removes the {{throws IOException}} from all open* methods, I kept it in the earlier versions as I guess it can be considered a public API",05/Jul/17 21:30;aweisberg;Can you run the dtests on this just to make sure the change in logging and signatures doesn't break any tests?,"06/Jul/17 06:52;krummas;https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/116/
Only running for 3.0 so far, if they look good I'll trigger for the other branches","24/Aug/17 07:05;krummas;3.0:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/210/testReport/
[^13620-3.0.png]

3.11:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/208/testReport/
[^13620-3.11.png]

trunk:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/207/testReport/
[^13620-trunk.png]

trying to figure out if any of the failures are caused by this patch","24/Aug/17 19:08;aweisberg;I recognize all of them except ttl_is_respected_on_repair and test_13691.

13691 failed on the 3.0 branch. ttl_is_respected_on_repair failed maybe once in the last 30 builds on the 3.0 branch, but not on 3.11 or trunk.

The issue looks like the environment?


{noformat}
Error Message

[Errno 20] Not a directory: '/tmp/dtest-Bx7qYJ/test/node2/bin/cassandra'
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-Bx7qYJ
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.connection: WARNING: Heartbeat failed for connection (140182908452560) to 127.0.0.2
cassandra.cluster: WARNING: Host 127.0.0.2 has been marked down
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
cassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 320, in run
    self.setUp()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-devbranch-dtest/cassandra-dtest/ttl_test.py"", line 349, in setUp
    self.cluster.populate(2).start()
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-devbranch-dtest/venv/local/lib/python2.7/site-packages/ccmlib/cluster.py"", line 393, in start
    p = node.start(update_pid=False, jvm_args=jvm_args, profile_options=profile_options, verbose=verbose, quiet_start=quiet_start, allow_root=allow_root)
  File ""/home/jenkins/jenkins-slave/workspace/Cassandra-devbranch-dtest/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 617, in start
    os.chmod(launch_bin, os.stat(launch_bin).st_mode | stat.S_IEXEC)
""[Errno 20] Not a directory: '/tmp/dtest-Bx7qYJ/test/node2/bin/cassandra'\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-Bx7qYJ\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 512.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.connection: WARNING: Heartbeat failed for connection (140182908452560) to 127.0.0.2\ncassandra.cluster: WARNING: Host 127.0.0.2 has been marked down\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\ncassandra.pool: WARNING: Error attempting to reconnect to 127.0.0.2, scheduling retry in 16.0 seconds: [Errno 111] Tried connecting to [('127.0.0.2', 9042)]. Last error: Connection refused\n--------------------- >> end captured logging << ---------------------""
{noformat}
","25/Aug/17 06:29;krummas;yeah, the suspicious tests pass locally multiple times","28/Aug/17 13:43;krummas;Committed as {{dfbe3fabd266493e69}}, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool repair will hang,CASSANDRA-13839,13099465,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,jasonstack,jasonstack,03/Sep/17 16:46,12/Mar/19 14:09,13/Mar/19 22:35,09/Sep/17 09:31,4.0,,,,,Legacy/Streaming and Messaging,,,,,0,,,,"After CASSANDRA-12229(fc92db2b9b56c143516026ba29cecdec37e286bb) merged, repair  may hang forever.

Here is the [dtest|https://github.com/jasonstack/cassandra-dtest-riptano/commits/CASSANDRA-13299-apache] to reproduce. 
bq. nosetests -s -v materialized_views_test.py:TestMaterializedViews.throttled_partition_update_test

looks like some RemoteSyncTask are not completed ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 09 09:31:08 UTC 2017,,,,,,0|i3jlc7:,9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Sep/17 04:48;jasonstack;[~jasobrown] when you have time, could you have a look?",09/Sep/17 09:31;jasonstack;This is fixed in CASSANDRA-13852 (Commit: 83822d12d87dcb3aaad2b1e670e57ebef4ab1c36),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential AssertionError during ReadRepair of range tombstone and partition deletions,CASSANDRA-13719,13089070,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,21/Jul/17 15:02,12/Mar/19 14:09,13/Mar/19 22:35,24/Aug/17 09:40,3.0.15,3.11.1,,,,Legacy/Coordination,,,,,0,,,,"When reconciling range tombstones for read repair in {{DataResolver.RepairMergeListener.MergeListener}}, when we check if there is ongoing deletion repair for a source, we don't look for partition level deletions which throw off the logic and can throw an AssertionError.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-22 10:05:18.623,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 09:40:13 UTC 2017,,,,,,0|i3hupz:,9223372036854775807,,,,,,,,blambov,blambov,,,,,,,,,,"21/Jul/17 15:46;slebresne;Pushed fix on the following branches:
| [3.0|https://github.com/pcmanus/cassandra/tree/13719-3.0] | [3.11|https://github.com/pcmanus/cassandra/tree/13719-3.11] | [trunk|https://github.com/pcmanus/cassandra/tree/13719-trunk]|

There is a first commit that basically only add additional logging when the assertion that this triggers happen. I used it to debug the problem but I think it would be wise to commit it because it's the 2nd time we get an assertion error triggered in that method (the first being CASSANDRA-13719) and I'd rather we have more info to debug if this happens a 3rd time (not that I hope it will). The additional logging doesn't do anything unless an AssertionError is triggered, so it's kind of safe to add, it's just a minor code pollution if this is never useful again, which isn't a big deal.

The 2nd commit is the real fix and includes a unit test","21/Aug/17 12:36;slebresne;Gentle ping that I've just rebased the branches above and this is still good for review. I've tried running CI with CircleCI and [it looks ok|https://circleci.com/gh/pcmanus/cassandra/3#tests/containers/0] (the build is marked failed, but afaict it's just the eclipse-warnings target that failed and from the logs, it's not even clear it's a genuine failure; all unit tests looks good however) though it's the first time I try it and I'm not sure if/how to run dtests yet. ","22/Aug/17 10:05;blambov;Is it not possible to have both a partition deletion repair and a condition suitable for the [{{markerToRepair == null}} branch|https://github.com/apache/cassandra/commit/8900a8dd9c2a66dfb601031f0905edffc427557f#diff-8781f9483cca1cfc87145c767295cc79R360]? E.g. partition deletion with time 10, range tombstone with time 11 between 1 and 10, with the other source having only a range tombstone with time 11 between 2 and 3? Moreover, if the second source includes a range tombstone with time 10 between 4 and 5 we may get trouble from [the other side of that branch|https://github.com/apache/cassandra/commit/8900a8dd9c2a66dfb601031f0905edffc427557f#diff-8781f9483cca1cfc87145c767295cc79R375].

 I'm not sure this can actually happen in practice, but it does not look too hard to fix: I think we need to check that the current deletion is not the same as the partition level deletion repair in all cases that generate tombstones.","23/Aug/17 13:17;slebresne;You are right, those were genuine problems, thanks. I force-pushed an update to the same branches to fix and added an additional unit tests for this particular problem.",23/Aug/17 15:54;blambov;LGTM,"24/Aug/17 09:40;slebresne;Alright, committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GH PR #131 - Refactoring to primitive functional interfaces in AuthCache.java,CASSANDRA-13732,13090332,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,26/Jul/17 21:14,12/Mar/19 14:09,13/Mar/19 22:35,28/Jul/17 00:03,4.0,,,,,Feature/Authorization,,,,,0,,,,"https://github.com/apache/cassandra/pull/131

Refactor to avoid unnecessary boxing/unboxing in auth cache.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,https://github.com/apache/cassandra/pull/131,https://github.com/apache/cassandra/pull/131,,,,,,,,,9223372036854775807,,,Fri Jul 28 00:03:38 UTC 2017,,,,,,0|i3i23z:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"27/Jul/17 00:14;jjirsa;Assigning to myself, actual author is {{Ameya Ketkar}}

Pushed to CI here: 
Unit tests: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13732
Dtests : https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/149/ (I think, but ASF Jenkins is unhappy right now)
","28/Jul/17 00:03;jjirsa;lgtm, committed as {{a5dff2f79621d7527a3837c0028d2e8b61d16e42}}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Import cassandra-dtest project into it's own repository ,CASSANDRA-13613,13080276,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,zznate,zznate,16/Jun/17 00:29,12/Mar/19 14:08,13/Mar/19 22:35,27/Aug/17 19:13,,,,,,Test/dtest,,,,,0,,,,"Given the completion of CASSANDRA-13584, we can now import {{cassandra-dtest}} into ASF infrastructure. This is a top level issue for tracking the bits and pieces of this task. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-13 15:02:56.553,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 13 15:02:56 UTC 2017,,,,,,0|i3gcmf:,9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Jul/17 15:02;mshuler;I've updated the Jenkins Job DSL seed to pull cassandra-dtest from the ASF git mirror.
https://git1-us-west.apache.org/repos/asf?p=cassandra-builds.git;a=commitdiff;h=b0f3195e4e85b1f7dddb63823a88216cd3a024a4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle mutateRepaired failure in nodetool verify,CASSANDRA-13933,13106871,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sumanth.pasupuleti,krummas,krummas,04/Oct/17 06:31,12/Mar/19 14:08,13/Mar/19 22:35,07/Feb/18 08:13,3.0.16,3.11.2,4.0,,,,,,,,0,lhf,,,See comment here: https://issues.apache.org/jira/browse/CASSANDRA-13922?focusedCommentId=16189875&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16189875,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Feb/18 21:33;sumanth.pasupuleti;CASSANDRA-13933-3.0.txt;https://issues.apache.org/jira/secure/attachment/12909160/CASSANDRA-13933-3.0.txt,04/Feb/18 21:33;sumanth.pasupuleti;CASSANDRA-13933-3.11.txt;https://issues.apache.org/jira/secure/attachment/12909161/CASSANDRA-13933-3.11.txt,04/Feb/18 21:33;sumanth.pasupuleti;CASSANDRA-13933-trunk.txt;https://issues.apache.org/jira/secure/attachment/12909162/CASSANDRA-13933-trunk.txt,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2018-01-31 15:29:29.854,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 07 08:13:31 UTC 2018,,,,,,0|i3kusf:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"31/Jan/18 15:29;sumanth.pasupuleti;[~krummas] I would like to work on submitting a patch for this. Would you be able to assign this to me, and mark the status to be InProgress? I am not sure if I have access to do it.",31/Jan/18 15:34;krummas;also note CASSANDRA-14201,"01/Feb/18 01:18;sumanth.pasupuleti;[~krummas] I am done with my changes and should be ready to submit the commit for review tomorrow. However, your commit (thanks for pointing me to 14201) [https://github.com/krummas/cassandra/commit/1489a5b66f60c733aaa1749d2d2ad36457e21824] would have merge conflicts with my changes. Let me know if I should wait for your changes to be in trunk, and submit changes on top of yours.","01/Feb/18 12:26;krummas;no it is fine, I can rebase on top of your patch","01/Feb/18 23:38;sumanth.pasupuleti;[~krummas] Submitted the patch. Please let me know if you have any comments.

Ended up going with an info log as against warn log (where I can potentially log the exception details). Curious to know your thoughts.","02/Feb/18 15:27;krummas;Thanks for the patch [~sumanth.pasupuleti]

I think this looks good in general, small comments;
* I don't think we should declare {{throws CorruptSSTableException}} since it is a {{RuntimeException}}
* we could remove the {{throws IOException}} from {{markAndThrow}} (and its callers) now since we always throw {{CorruptSSTableException}} instead

We need to apply this to 3.0, 3.11 and trunk, can you provide patches?","02/Feb/18 16:31;jjordan;This is just a drive by comment, but please be wary of changing the exception signature of exposed functions in 3.x, as depending on what is involved here it could break external plugin's/extensions.  (Probably not relevant here since this is in the repair path, which I don't think we have any extension points in, but just like to remind people of that.)","02/Feb/18 20:21;sumanth.pasupuleti;Thanks [~krummas] . Will be incorporating the comments and submitting patches for 3.0 and 3.11 too.

[~jjordan] this is good to know, especially for a newbie in this community like me. Will keep this in mind. Thank you!","04/Feb/18 21:35;sumanth.pasupuleti;[~krummas] attached patches for 3.0, 3.11 and trunk","07/Feb/18 08:13;krummas;committed as {{7885a703d6dae8c3c6e5a6af632c6a23342593fc}}

note that I decided to keep the method signatures in 3.0 and 3.11 to avoid breaking anyone depending on this, the {{throws IOException}} is removed in trunk

thanks for the patch!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RTs at index boundaries in 2.x sstables can create unexpected CQL row in 3.x,CASSANDRA-14008,13117589,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,jjirsa,10/Nov/17 14:57,12/Mar/19 14:08,13/Mar/19 22:35,12/Dec/17 18:00,3.0.16,3.11.2,,,,Legacy/Local Write-Read Paths,,,,,1,correctness,,,"In 2.1/2.2, it is possible for a range tombstone that isn't a row deletion and isn't a complex deletion to appear between two cells with the same clustering. The 8099 legacy code incorrectly treats the two (non-RT) cells as two distinct CQL rows, despite having the same clustering prefix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-12 20:09:34.078,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 12 18:00:17 UTC 2017,,,,,,0|i3mnbb:,9223372036854775807,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"12/Nov/17 20:09;KurtG;So this kind of sounds like an issue we've been investigating, but we've had trouble finding the clustering that's actually causing the problem. Do you have an example to reproduce, or even just an example row to compare with?

If it's the same problem we've found that in some cases after upgrade to 3.11 the SSTable containing the data will also be corrupt.","12/Nov/17 21:26;jjirsa;It does create an invalid (or similarly broken)  sstable on compaction/upgradesstables, and while I've got a fix for the original problem done, I need to think about the right way to un-break the resulting 3.0 sstable (which should be do-able, based on what I've seen so far).




","13/Nov/17 11:41;iamaleksey;We can probably generate an sstable that triggers this bug relatively easily for a regression test (nice to have, but won't block the patch on lack of it).

And, as Jeff mentions, it would be nice to find a way to un-break 3.0 sstables where the damage's been done already, in a follow-up JIRA.","01/Dec/17 00:39;jjirsa;The raw patches that fix the bug in LegacyLayout are at 

|| Branch || CI ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-14008] | [!https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-14008.svg?style=svg!|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-14008/] | 
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-14008] | [!https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-14008.svg?style=svg!|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-14008/]| 

I was hoping to actually have a solution to un-breaking the broken 3.0 sstables in the same patch, but it's proving to be more difficult than I anticipated. I haven't yet tried to make some sample sstables for regression tests, I agree it'd be nice to have those. 

Please glance at the code, and I'll work on the regression sstables before committing.",01/Dec/17 11:56;iamaleksey;+1,"11/Dec/17 19:40;jjirsa;[~iamaleksey] can you check both branches for a new regression test please?

Exact commits are https://github.com/jeffjirsa/cassandra/commit/eff1f18fcd80b4860bb5812142d196e94b6ae2a1 and 
https://github.com/jeffjirsa/cassandra/commit/a85befc4fc3c0e44f6751a3e6472afecaefc4b77 ","12/Dec/17 12:09;iamaleksey;Validated the test, both logically, and by running {{LegacyLayoutTest}} in both branches, with and without the fix reverted, ensuring that it doesn't fail, then fails, respectively.

+1",12/Dec/17 18:00;jjirsa;Thanks Aleksey. Committed as {{4a2b516a3488ab04ee3338e74397b8c6d69e6d43}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mishandling of cells for removed/dropped columns when reading legacy files,CASSANDRA-13939,13107161,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,05/Oct/17 08:27,12/Mar/19 14:08,13/Mar/19 22:35,06/Oct/17 14:21,3.0.16,3.11.2,,,,Legacy/Local Write-Read Paths,,,,,0,,,,"The tl;dr is that there is a bug in reading legacy files that can manifests itself with a trace looking like this:
{noformat}
Exception (java.lang.IllegalStateException) encountered during startup: One row required, 2 found
java.lang.IllegalStateException: One row required, 2 found
    at org.apache.cassandra.cql3.UntypedResultSet$FromResultSet.one(UntypedResultSet.java:84)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readTableTimestamp(LegacySchemaMigrator.java:254)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readTable(LegacySchemaMigrator.java:244)
    at org.apache.cassandra.schema.LegacySchemaMigrator.lambda$readTables$7(LegacySchemaMigrator.java:238)
    at org.apache.cassandra.schema.LegacySchemaMigrator$$Lambda$126/591203139.accept(Unknown Source)
    at java.util.ArrayList.forEach(ArrayList.java:1249)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readTables(LegacySchemaMigrator.java:238)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readKeyspace(LegacySchemaMigrator.java:187)
    at org.apache.cassandra.schema.LegacySchemaMigrator.lambda$readSchema$4(LegacySchemaMigrator.java:178)
    at org.apache.cassandra.schema.LegacySchemaMigrator$$Lambda$123/1612073393.accept(Unknown Source)
    at java.util.ArrayList.forEach(ArrayList.java:1249)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readSchema(LegacySchemaMigrator.java:178)
{noformat}

The reason this can happen has to do with the handling of legacy files. Legacy files are cell based while the 3.0 storage engine is primarily row based, so we group those cells into rows early in the deserialization process (in {{UnfilteredDeserializer.OldFormatDeserializer}}), but in doing so, we can only consider a row finished when we've either reach the end of the partition/file, or when we've read a cell that doesn't belong to that row.  That second case means that when the deserializer returns a given row, the underlying file pointer may actually not positioned at the end of that row, but rather it may be past the first cell of the next row (which the deserializer remembers for future use). Long story short, when we try to detect if we're logically past our current index block in {{AbstractIterator.IndexState#isPastCurrentBlock(}}), we can't simply rely on the file pointer, which again may be a bit more advanced that we logically are, and that's the reason for the ""correction"" in that method. That correction is really just the amount of bytes remembered but not yet used in the deserializer.

That ""correction"" is sometimes wrong however and that's due to the fact that in {{LegacyLayout#readLegacyAtom}}, if we get a cell for an dropped or removed cell, we ignore that cell (which, in itself, is fine). Problem is that this skipping is done within the {{LegacyLayout#readLegacyAtom}} method but without {{UnfilteredDeserializer.OldFormatDeserializer}} knowing about it. As such, the size of the skipped cell ends up being accounted in the ""correction"" bytes for the next cell we read. Lo and behold, if such cell for a removed/dropped column is both the last cell of a CQL row and just before an index boundary (pretty unlikely in general btw, but definitively possible), then the deserializer will count its size with the first cell of the next row, which happens to also be the first cell of the next index block.  And when the code then tries to figure out if it crossed an index boundary, it will over-correct. That is, the {{indexState.updateBlock()}} call at the start of {{SSTableIterator.ForwardIndexedReader#computeNext}} will not work correctly.  This can then make the code return a row that is after the requested slice end (and should thus not be returned) because it doesn't compare that row to said requested end due to thinking it's not on the last index block to read, even though it genuinely is.

Anyway, the whole explanation is a tad complex, but the fix isn't: we need to move the skipping of cells for removed/dropped column a level up so the deserializer knows about it and don't silently count their size in the next atom size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-10-06 12:34:19.826,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 23 12:42:45 UTC 2018,,,,,,0|i3kwkf:,9223372036854775807,,,,,,,,snazy,snazy,,,,,,,,,,"05/Oct/17 08:38;slebresne;Patch available in the following branches:
| [3.0|https://github.com/pcmanus/cassandra/commits/13939-3.0] | [CircleCI|https://circleci.com/gh/pcmanus/cassandra/7] |
| [3.11|https://github.com/pcmanus/cassandra/commits/13939-3.11] | [CircleCI|https://circleci.com/gh/pcmanus/cassandra/8] |     ",06/Oct/17 12:34;snazy;+1,"06/Oct/17 14:21;slebresne;Committed thanks (there were a few failures on the CI builds, but that really doesn't seem related (the code only updates code that is involved in reading old sstables, and none of those tests was doing that)).","23/Mar/18 12:42;gdesalas;Hi Sylvain 

can you help me to understand if this particular issue is solved in version 3.11.1 please?

We are now in version 3.0.9 and planning next week to move to 3.11.1 because of this particular issue

I´ll appreciate your comments

thank you in advance

Regards

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove initialDirectories from CFS,CASSANDRA-13928,13106622,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,03/Oct/17 06:46,12/Mar/19 14:07,13/Mar/19 22:35,15/Dec/17 12:49,4.0,,,,,,,,,,0,,,,"The initialDirectories added in CASSANDRA-8671 is quite confusing and I don't think it is needed anymore, it should be removed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-10-03 14:27:51.266,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 15 12:49:53 UTC 2017,,,,,,0|i3kt9r:,9223372036854775807,,,,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,03/Oct/17 06:47;krummas;cc [~eduard.tudenhoefner],03/Oct/17 06:50;krummas;It also makes implementing CASSANDRA-13215 less straight-forward,03/Oct/17 14:27;jjordan;[~krummas] why do you think it is not needed anymore?  How does a compaction strategy control where things are created without it?,"03/Oct/17 14:38;krummas;bq. How does a compaction strategy control where things are created without it?
[~jjordan] they probably shouldn't right now, and I don't think this code is used anymore.",03/Oct/17 15:01;jjordan;(y),03/Oct/17 17:03;eduard.tudenhoefner;[~krummas] yes I think it's safe to remove the *initialDirectories* stuff.,"24/Oct/17 06:31;krummas;https://github.com/krummas/cassandra/commits/marcuse/13928

rebuilding unit tests here:
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F13928
dtests look bad but similar to other branches:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/383/","12/Dec/17 22:52;pauloricardomg;LGTM, can you just rebase (now that CASSANDRA-13948 is in) and submit a new test run? Feel free to commit when CI is clean.","13/Dec/17 09:02;krummas;https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/461/
https://circleci.com/gh/krummas/cassandra/195","15/Dec/17 12:49;krummas;thanks, committed, CI errors look unrelated",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong logger name in AnticompactionTask,CASSANDRA-13343,13056770,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,szhou,szhou,szhou,16/Mar/17 19:34,12/Mar/19 14:21,13/Mar/19 22:35,16/Mar/17 20:33,2.2.10,3.0.13,3.11.0,4.0,,,,,,,0,,,,"We have the below code in AnticompactionTask.java. The parameter is wrong.
{code}
private static Logger logger = LoggerFactory.getLogger(RepairSession.class);
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Mar/17 19:37;szhou;CASSANDRA-13343-v1.patch;https://issues.apache.org/jira/secure/attachment/12859153/CASSANDRA-13343-v1.patch,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-03-16 20:22:12.698,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 16 20:33:38 UTC 2017,,,,,,0|i3cdwn:,9223372036854775807,3.0.10,,,,,,,jasobrown,jasobrown,,,,,,,,,,16/Mar/17 19:39;szhou;[~pauloricardomg] do you mind taking a quick review? Just one line change.,"16/Mar/17 20:22;jasobrown;+1. This goes back to 2.2, so I'll fix 'em all. Thanks for finding this trivial one!",16/Mar/17 20:33;jasobrown;committed as sha {{a69f6885556a147837f15098fd3aef5de756fad5}}. thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create meaningful toString() methods,CASSANDRA-13653,13084097,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,djanand,jjirsa,jjirsa,03/Jul/17 01:51,12/Mar/19 14:21,13/Mar/19 22:35,05/Apr/18 12:21,4.0,,,,,,,,,,0,lhf,low-hanging-fruit,,"True low-hanging fruit, good for a first-time contributor:

There are a lot of classes without meaningful {{toString()}} implementations. Some of these would be very nice to have for investigating bug reports.

Some good places to start: 

- CQL3 statements (UpdateStatement, DeleteStatement, etc), QueryOptions, and Restrictions

Some packages not to worry about: 

- Deep internals that don't already have them (org.apache.cassandra.db.rows/partitions/etc)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-03 04:40:40.154,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 05 12:21:42 UTC 2018,,,,,,0|i3h05b:,9223372036854775807,,,,,,,,djoshi3,djoshi3,,,,,,,,,,"03/Jul/17 04:40;dbrosius;imo, 

    @Override
    public String toString() {
        return ToStringBuilder.reflectionToString(this, ToStringStyle.SHORT_PREFIX_STYLE);
    }

can be added as a snippet in your favorite IDE, and you can blow thru lots of classes, quickly. And does a pretty good job.",09/Jul/17 06:05;agacha;Hi I am new to open source contribution.I am following this link as guide to contributing to Cassandra https://wiki.apache.org/cassandra/HowToContribute .I want to work on this issue but I can find assign button to assign this to myself. Do I need to request any access for this?,"09/Jul/17 06:13;varuna;[~agacha] You don't need to request just assign it to yourself and provide a patch. 
* Reviewers will review the patch
* Committers will commit it ",25/Sep/17 05:19;djanand;Hi I am new to open source contribution as well. Though the ticket is already assigned I tired to take a stab at this. [13653-3.11|https://github.com/djanand/cassandra/tree/13653-3.11] [~jjirsa]/[~varuna]/[~dbrosius] - Could you please review ? Please let me know if there other classes which require toString() impl. I can make the required changes. Are there any other branches where I need to propagate this change ?,25/Mar/18 04:54;djoshi3;[~djanand] thank you for the patch. I can review it. I took a quick look at the patch and you have created it against 3.11. Typically you should start off with trunk unless it's a bug fix to an existing version.,"25/Mar/18 22:05;djanand;[~djoshi3] - Thank you. I have made the changes against trunk now ([13653-trunk|https://github.com/djanand/cassandra/tree/13653-trunk]). toString() changes have been made in the following packages cql3.statements , cql3.restrictions, cql3, cql3.conditions. Could you please review the same ? ","26/Mar/18 22:55;zznate;[~djanand] Took a quick peek - looks like you have a format issue as the diffs for each class are essentially the whole file. Can you double check you have adhered to the formatting guidelines? [http://cassandra.apache.org/doc/latest/development/code_style.html]

Appreciate the effort, regardless. ","27/Mar/18 04:51;djoshi3;Hi [~djanand], other than [~zznate]'s feedback. I did a quick spot check and noticed that you're using Apache commons {{ToStringBuilder.reflectionToString}}. It uses reflection and is not the best when it comes to performance. I would err on the side of caution and use {{StringBuilder}} or concat Strings (the compiler should convert them to the builder equivalent) or at the very least use the builder version of {{ToStringBuilder}}.

See: https://antoniogoncalves.org/2015/06/30/who-cares-about-tostring-performance/","27/Mar/18 05:15;jjirsa;[~djoshi3] -  [~dbrosius] specifically recommended using reflectionToString - most of the toString methods here won't be in a hot path, and really the only point of this is to make debugging slightly easier. If any of these ARE called in any sort of meaningfully hot path, they shouldn't use reflection, but we should try to verify that before proceeding.

 

 

 ","27/Mar/18 05:23;djanand;[~djoshi3] - Thanks for the feedback. I went through the links provided and my reasoning is based on these two points. One, this will be used for debugging purposes and there are not many places where we would log/print the entire class. Second, if there are additions/deletions to the class variables developers need not worry about the toString() implementation. Having said that please help me understand if there will be plenty of logging in cql3.statements package in which case this might effect performance as you mentioned.
ps:sorry for the inconvenience on the patch. Will address Nate's feedback 
",31/Mar/18 22:59;djanand;[~djoshi3] / [~jjirsa] / [~dbrosius] - I have incorporated the feedback with the latest trunk. Could you please take a look at it . [13653-trunk|https://github.com/djanand/cassandra/commit/4366955769dcda39fba6e4bf427045f3da9ac457]. I am open to any suggestions to make changes. Feel free to comment on the commits/per file for any specific file related issues. Please suggest any benchmarks which can be run if you think otherwise. ,"03/Apr/18 05:15;djoshi3;Hi [~djanand] Your IDE seems to have inserted wildcard imports. For example see {{QueryOptions}}. Other than that, I think LGTM.",05/Apr/18 04:39;djanand;Hi [~djoshi3] - Thank you. Removed them and squashed the [commit |https://github.com/djanand/cassandra/commit/f9d6982c9d54d07938aaec84d54c4e4b536b0ed9],"05/Apr/18 12:21;jasobrown;Thanks for addressing [~djoshi3]'s last comment. LGTM, as well. committed as sha {{946aaa7b06f2ccd697e31dcc15c29468da523311}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix exception logging that should be consumed by placeholder to 'getMessage()' for new slf4j version,CASSANDRA-13723,13089462,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasonstack,jasonstack,jasonstack,24/Jul/17 10:09,12/Mar/19 14:21,13/Mar/19 22:35,10/Aug/17 06:22,4.0,,,,,,,,,,0,,,,"The wrong tracing log will fail {{materialized_views_test.py:TestMaterializedViews.view_tombstone_test}} and impact clients.

Current log: {{Digest mismatch: {} on 127.0.0.1}}

Expected log: {{Digest mismatch: org.apache.cassandra.service.DigestMismatchException: Mismatch for key DecoratedKey....... on 127.0.0.1}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Jul/17 10:19;jasonstack;CASSANDRA-13723.patch;https://issues.apache.org/jira/secure/attachment/12878600/CASSANDRA-13723.patch,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-07-24 13:17:48.323,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 10 10:05:34 UTC 2017,,,,,,0|i3hx4n:,9223372036854775807,,,,,,,,spodxx@gmail.com,spodxx@gmail.com,,,,,,,,,,"24/Jul/17 12:47;jasonstack;It only affects trunk by  CASSANDRA-12996

All tests passed except for bootstrap_test(known) in dtest.

","24/Jul/17 13:17;jasobrown;Should we wait until the dust settles on CASSANDRA-12996 before making this change? It seems unfortunate to need to call {{#toString()}} on an object before passing it to {{Tracing}}/{{MessageFormatter}}.

Otherwise patch is trivial and fine with me",24/Jul/17 14:12;jasonstack;thanks for reviewing. let's wait for CASSANDRA-12996 to settle.,"25/Jul/17 08:20;spodxx@gmail.com;I've created ticket  [SLF4J-416|https://jira.qos.ch/browse/SLF4J-416] for this on the SLF4J tracker and it looks like this is a known issue, with an open [PR|https://github.com/qos-ch/slf4j/pull/166] created half a year ago.

Is it really just this one log message that is affected?",25/Jul/17 08:58;jasonstack;the affected range is unknown.. need to go through the codebase. the digest log is luckily being caught by dtest,"25/Jul/17 09:29;spodxx@gmail.com;There seem to be a couple of statements that would have to be fixed, see

{noformat}grep -r logger src/java |grep '{}' |grep ', [e|t])'{noformat}

Running {{FileUtilsTest.testFolderSize}} will show the new behaviour:

{noformat}
ERROR [main] 2017-07-25 11:22:14,389 FileUtils.java:490 - Error while getting build/test/cassandra/data/testFolderSize/i_dont_exist folder size. {}
java.nio.file.NoSuchFileException: build/test/cassandra/data/testFolderSize/i_dont_exist
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) ~[na:1.8.0_112]
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[na:1.8.0_112]
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[na:1.8.0_112]
...
{noformat}

The logger will not substitute the placeholder and print the hold stack trace instead. ","26/Jul/17 06:51;jasonstack;thanks, I will update the patch this week","27/Jul/17 08:20;jasonstack;if switch to {{e.toString()}} everywhere, C*  may generate many unnecessary String objects(gc) even if logger level is NONE.

By checking:  {{if(logger.isXXXEnable())}} would help, but it's troublesome.

how about using {{e.getMessage()}} instead of {{e.toString()}}?  this will affect driver's trace and server log.","27/Jul/17 09:22;spodxx@gmail.com;I think we'd just have to change any log messages that intent to print the exception as represented by a log format parameter, e.g. {{logger.debug(""Error! {}"", e)}}. In this case we can simply replace the exception with {{e.getMessage()}}. But we don't have to change log messages that would print the stacktrace,
 e.g. {{logger.debug(""Error!"", e)}}.","07/Aug/17 11:48;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13723] | [unit|https://circleci.com/gh/jasonstack/cassandra/368] | irrelevant
bootstrap_test.TestBootstap.consistent_range_movement_false_with_rf1_should_succeed_test
ttl_test.TestDistributedTTL.ttl_is_respected_on_repair_test
|
| [dtest|https://github.com/riptano/cassandra-dtest/commits/CASSANDRA-13723] | \ | \ |

changed {{logger.debug(""Error! {}"", e)}} (exception consumed by placeholder)  to {{logger.debug(""Error! {}"", e.getMessage())}} for new slf4j version
changed dtest to grep new digest mismatch log in tracing: {{Digest mismatch: Mismatch for key DecoratedKey.....}}",10/Aug/17 03:41;jasonstack;[~spodxx@gmail.com] [~jasobrown] could you please review it? thanks,"10/Aug/17 06:22;spodxx@gmail.com;Merged as ba87ab4e954ad2
Thanks!",10/Aug/17 10:05;jasonstack;Thank you.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better bootstrap failure message when blocked by (potential) range movement,CASSANDRA-13744,13092482,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,michaelsembwever,michaelsembwever,michaelsembwever,04/Aug/17 13:41,12/Mar/19 14:21,13/Mar/19 22:35,16/Aug/17 03:11,3.11.1,4.0,,,,,,,,,0,,,,"The UnsupportedOperationException thrown from {{StorageService.joinTokenRing(..)}} when it's detected that other nodes are bootstrapping|leaving|moving offers no information as to which are those other nodes.

In a large cluster this might not be obvious nor easy to discover, gossipinfo can hold information that takes a bit of effort to uncover. Even when it is easily seen it's helpful to have it confirmed.

Attached is the patch that provides a more thorough exception message to the failed bootstrap attempt.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-04 16:43:35.996,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 16 03:12:19 UTC 2017,,,,,,0|i3ifav:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"04/Aug/17 16:43;jjirsa;Patch looks good to me, but before we commit, can you kick off a unit test run with circleci or jenkins, and a dtest run using the ASF jenkins? Just to be sure we don't 
 break any existing tests. 

",05/Aug/17 02:53;michaelsembwever;ofc [~jjirsa] (have also been looking if there's any tests around this code…),"05/Aug/17 04:21;michaelsembwever;|| branch || testall || dtest ||
| [cassandra-3.11_13744|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-3.11_13744]	| [testall|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_13744]	| [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/163] |
| [trunk_13744|https://github.com/thelastpickle/cassandra/tree/mck/trunk_13744]	| [testall|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_13744]	| [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/164] |
",05/Aug/17 13:39;jasobrown;+1. tests looks good except for the 3.11 dtest that self-imploded through no fault of this patch.,07/Aug/17 04:26;michaelsembwever;…and the trunk dtest was aborted. will add CHANGES.txt and restart both dtest.,"07/Aug/17 04:37;michaelsembwever;Updated CHANGES.txt and triggered dtest rebuild…

|| branch || testall || dtest ||
| [cassandra-3.11_13744|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-3.11_13744]	| [testall|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_13744]	| [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/165] |
| [trunk_13744|https://github.com/thelastpickle/cassandra/tree/mck/trunk_13744]	| [testall|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_13744]	| [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/166] |
","07/Aug/17 04:39;michaelsembwever;[~jjirsa], [~jasobrown], can either of you add yourself to the ""Reviewer"" field so I know who to refer to as reviewer in the commit msg, please.

Dtests are running again.","11/Aug/17 00:59;jjirsa;I'll claim it, not only because I'm first, but because I'm not sure if Jason's +1 carries through on the new tests.
","16/Aug/17 03:12;michaelsembwever;thanks [~jjirsa], committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor doc update: Replaced non-ASCII dash in command line,CASSANDRA-13374,13058776,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,24/Mar/17 01:05,12/Mar/19 14:21,13/Mar/19 22:35,28/Mar/17 12:23,4.0,,,,,,,,,,0,,,,"Minor doc update to replace non-ascii code, for copy-paste.
Not sure if it's the right way to report it, or should I use GitHub PR?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Mar/17 01:06;jay.zhuang;13374-3.11.patch;https://issues.apache.org/jira/secure/attachment/12860261/13374-3.11.patch,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-03-28 12:23:14.46,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 12:23:14 UTC 2017,,,,,,0|i3cqaf:,9223372036854775807,3.11.x,,,,,,,jasobrown,jasobrown,,,,,,,,,,"28/Mar/17 12:23;jasobrown;[~jay.zhuang] while not ""official"" yet, CASSANDRA-13256 has some good instructions on contributing doc improvements.

That being said, I'm +1'ing this patch and committing it as sha {{380a614f1c10e34e456c428d9c3986991437b97f}}. Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlshrc.sample uses incorrect option for time formatting,CASSANDRA-14243,13139644,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,alexott,alexott,alexott,20/Feb/18 12:21,12/Mar/19 14:21,13/Mar/19 22:35,02/Mar/18 07:17,4.0,,,,,Legacy/Tools,,,,,0,lhf,,,"Sample cqlshrc file in conf/cqlshrc.sample uses incorrect option (datetimeformat) instead of correct one (time_format).

The datetimeformat is the sub-option of the COPY FROM command, not cqlsh itself.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-20 12:24:59.226,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 02 07:17:20 UTC 2018,,,,,,0|i3qdkn:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"20/Feb/18 12:24;githubbot;GitHub user alexott opened a pull request:

    https://github.com/apache/cassandra/pull/197

    fix for CASSANDRA-14243

    `datetimeformat` option that was used in the `conf/cqlshrc.sample` file is sub-option of
    the `COPY FROM` command, not of the `cqlsh` itself.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/alexott/cassandra CASSANDRA-14243

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/197.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #197
    
----
commit 53df3c118f4bb73494020b8414401540121a4c5d
Author: Alex Ott <alexott@...>
Date:   2018-02-20T12:23:21Z

    fix for CASSANDRA-14243
    
    `datetimeformat` option that was used in the `conf/cqlshrc.sample` file is sub-option of
    the `COPY FROM` command, not of the `cqlsh` itself.

----
","02/Mar/18 07:17;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/197
",02/Mar/18 07:17;jjirsa;Thanks for the patch [~alexott] - committed as {{73fa27cc6ae35a16ab8b828f9872ce59063aa83d}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trivial log format error,CASSANDRA-14105,13124191,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,11/Dec/17 18:29,12/Mar/19 14:21,13/Mar/19 22:35,12/Dec/17 01:41,4.0,,,,,,,,,,0,,,,"The same issue as CASSANDRA-13551
The ""{}"" is not needed for: {{log.error(String, Throwable)}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-11 19:49:48.348,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 12 01:41:01 UTC 2017,,,,,,0|i3nrwf:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"11/Dec/17 18:32;jay.zhuang;| Branch | uTest |
| [14105|https://github.com/cooldoger/cassandra/tree/14105] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14105.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14105] |","11/Dec/17 19:49;jjirsa;+1
",12/Dec/17 00:54;jay.zhuang;I created a JIRA for failed utest: CASSANDRA-14106. It's not related to this change.,"12/Dec/17 01:41;jjirsa;Thanks! Committed as {{e18a49a2399a3fe667c3c08d7350b7528614f0a6}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
node map does not handle InetAddressAndPort correctly.,CASSANDRA-14216,13136464,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,06/Feb/18 04:04,12/Mar/19 14:21,13/Mar/19 22:35,07/Feb/18 16:49,4.0,,,,,Legacy/Core,,,,,0,,,,"Collection of node information in nodeMap does not use the correct types for accessing data. Since these maps are keyed by Strings, they are not metatype-safe, and so i can't be certain what data was meant to be in them. I'm assuming it was meant that host and port information should be used, but perhaps it's just host.

 

I have created a patch assuming it's host and port info.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Feb/18 04:05;dbrosius;14216.txt;https://issues.apache.org/jira/secure/attachment/12909360/14216.txt,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-02-06 11:45:11.561,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,,,Wed Feb 07 16:49:21 UTC 2018,,,,,,0|i3ptyn:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,06/Feb/18 11:45;jasobrown;/cc [~aweisberg],"07/Feb/18 16:03;aweisberg;You are correct it's supposed to be the host and port string and we also do need to unwrap the address from the InetAddressAndPort.
+1

Thanks for spotting this. Are you finding these things with manual testing, or a linter or some other static analysis tool?","07/Feb/18 16:49;dbrosius;pushed to trunk as d0b34d383c20f5add8b8d7d454b4460aace0c939

 

findbugs/fb-contrib",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clock-dependent integer overflow in tests CellTest and RowsTest,CASSANDRA-13866,13102102,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jkni,jkni,jkni,13/Sep/17 23:27,12/Mar/19 14:21,13/Mar/19 22:35,15/Sep/17 21:40,3.0.15,3.11.1,4.0,,,Legacy/Testing,,,,,0,,,,"These tests create timestamps from Unix time, but this is done as int math with the result stored in a long. This means that if the test is run at certain times, like 1505177731, corresponding to Tuesday, September 12, 2017, 12:55:31, the test can have two timestamps separated by a single second that reverse their ordering when multiplied by 1000000, such as 1505177731 -> 2147149504 and 1505177732 -> -2146817792. This causes a variety of test failures, since it changes the reconciliation order of these cells.

Note that I've tagged this as trivial because the problem is in the manual construction of timestamps in the test; I know of nowhere  that we make this mistake with real data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10266,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-14 00:00:30.121,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 15 21:40:29 UTC 2017,,,,,,0|i3k1j3:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"13/Sep/17 23:48;jkni;Trivial patches for [3.0|https://github.com/jkni/cassandra/tree/CASSANDRA-13866-3.0], [3.11|https://github.com/jkni/cassandra/tree/CASSANDRA-13866-3.11], and [trunk|https://github.com/jkni/cassandra/tree/CASSANDRA-13866-trunk]. The 3.0 patch merges forward cleanly.","14/Sep/17 00:00;jjirsa;lgtm.
",15/Sep/17 21:40;jkni;Thanks! Committed to 3.0 branch as {{d79fc9a2258d10e8a54fd4136d5544e10ad3ddda}} and merged forward.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The size of a byte is not 2,CASSANDRA-14057,13119243,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,17/Nov/17 14:45,12/Mar/19 14:21,13/Mar/19 22:35,18/Nov/17 13:19,3.0.16,3.11.2,,,,Legacy/Streaming and Messaging,,,,,0,,,,"{{DataLimits}} serializer uses {{TypeSizes.sizeof((byte)limits.kind().ordinal())}} in it's {{serializedSize}} method, but {{TypeSizes}} does not (on 3.0/3.11) have a {{sizeof(byte)}} override, so {{sizeof(short)}} is picked and it returns 2, which is wrong.

This is actually fixed on trunk, [~jasobrown] committed the fix as part of CASSANDRA-8457, but it wasn't committed in 3.0/3.11 and that still feel dodgy to leave as is.

To be clear, I don't think it's a problem as of now: it does break the size computed for read commands, and thus the payload size of such message, but said payload size is not use (for read requests that is).

Still, not reason to leave something wrong like this and risk a future bug when the fix is trivial.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-17 18:48:01.287,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 18 13:19:00 UTC 2017,,,,,,0|i3mxi7:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"17/Nov/17 14:48;slebresne;I've pushed the super simple fix [here|https://github.com/pcmanus/cassandra/commits/14057] (I suppose I could have almost ninja-fixed but well). [~jasobrown], mind having a very quick double check since you did the exact same on trunk?","17/Nov/17 18:48;jasobrown;Damnit, shame on me for not backporting that fix (or just fixing it outside of CASSANDRA-8457).

+1, [~slebresne], and thanks for taking the time to make sure we do this correctly.","18/Nov/17 13:19;slebresne;No problem, thanks for the quick review, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Doc of read_repair_chance and dclocal_read_repair_chance default values is wrong,CASSANDRA-14016,13118640,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,KurtG,tzach,tzach,15/Nov/17 20:24,12/Mar/19 14:21,13/Mar/19 22:35,16/Nov/17 19:06,4.0,,,,,Legacy/Documentation and Website,,,,,0,documentation,,,"In the documentation, default values for 
read_repair_chance is 0.1
dclocal_read_repair_chance is 0.0
source: http://cassandra.apache.org/doc/latest/cql/ddl.html#other-table-options

In the code, the default values are the other way around
{quote}
public static final double DEFAULT_READ_REPAIR_CHANCE = 0.0;
public static final double DEFAULT_DCLOCAL_READ_REPAIR_CHANCE = 0.1;
{quote}
source: https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/schema/TableParams.java#L63-L64",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-16 05:48:11.354,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 16 19:06:41 UTC 2017,,,,,,0|i3mtsn:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"16/Nov/17 05:48;githubbot;GitHub user kgreav opened a pull request:

    https://github.com/apache/cassandra/pull/173

    Correct default [dclocal_]read_repair_chance documentation

    CASSANDRA-14016

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/kgreav/cassandra 14016

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/173.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #173
    
----
commit d52110f02306b3033b748eee52dffce2525ea9dc
Author: kurt <kurt@instaclustr.com>
Date:   2017-11-16T05:43:55Z

    Correct default [dclocal_]read_repair_chance documentation

----
","16/Nov/17 05:49;KurtG;Thanks. [pr|https://github.com/apache/cassandra/pull/173]
FYI for small doc updates we do just accept a github PR. Feel free to submit them if you find any issues like this one.","16/Nov/17 19:06;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/173
","16/Nov/17 19:06;jjirsa;Thanks, committed as {[f1f6ed609943cc2908835070a4d1b622759464a8}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SelectTest.testMixedTTLOnColumnsWide is flaky,CASSANDRA-13764,13094662,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jjirsa,jkni,jkni,15/Aug/17 02:21,12/Mar/19 14:21,13/Mar/19 22:35,29/Aug/17 22:53,3.0.15,3.11.1,4.0,,,Legacy/Testing,,,,,0,,,,"{{org.apache.cassandra.cql3.validation.operations.SelectTest.testMixedTTLOnColumnsWide}} is flaky. This is because it inserts rows and then asserts their contents using {{ttl()}} in the select, but if the test is sufficiently slow, the remaining ttl may change by the time the select is run. Anecdotally, {{testSelectWithAlias}} in the same class uses a fudge factor of 1 second that would fix all the failures I've seen, but it might make more sense to measure the elapsed time in the test and calculate the acceptable variation from that time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13711,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-29 19:50:53.599,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 29 22:53:25 UTC 2017,,,,,,0|i3isg7:,9223372036854775807,,,,,,,,jkni,jkni,,,,,,,,,,15/Aug/17 16:06;jkni;This also affects {{SelectTest.testMixedTTLOnColumns}}.,"29/Aug/17 19:50;jjirsa;[~slebresne] says he's ok with ninja'ing this in, but in case you ( [~jkni] ) want to review before I do it (I need to push for Circle anyway just to be safe, so this'll sit here until Circle gives me a green run):

3.0: https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13764 (Circle: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13764 )
3.11: https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13764 (Circle:  https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13764 )
trunk: https://github.com/jeffjirsa/cassandra/tree/cassandra-13764 (Circle: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13764 )","29/Aug/17 21:47;jkni;The idea looks sound to me - I think you need to remove the TTL column from the select [here|https://github.com/jeffjirsa/cassandra/commit/a1e49db69622de11a996d09105e5ebf3b54c58c3#diff-7f5981228f9d9428fb164aa91316aa85R2976], as you did in {{testMixedTTLOnColumnsWide}}. If you agree, I'm comfortable with you doing that on commit and don't need to rereview if CI looks good.","29/Aug/17 22:53;jjirsa;Thanks Joel.

Committed to 3.0 as {{67ac1496cc9e7d9d15be28b9536e9cdbce42473d}} and merged to 3.11 and trunk WITHOUT adding CHANGES.txt entry, because it's just a test fix and never hit a release (not quite a ninja).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trivial intellij junit run fix,CASSANDRA-14169,13131122,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,16/Jan/18 02:28,12/Mar/19 14:21,13/Mar/19 22:35,14/Feb/18 15:35,4.0,,,,,,,,,,0,,,,"Unable to run {{[LegacySSTableTest|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java#L63]}} in the Intellij, because the {{[legacy-sstable-root|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java#L96]}} is not defined.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-16 03:59:26.54,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 14 15:35:43 UTC 2018,,,,,,0|i3oxnr:,9223372036854775807,,,,,,,,spodxx@gmail.com,spodxx@gmail.com,,,,,,,,,,"16/Jan/18 02:39;jay.zhuang;| Branch | uTest |
| [14169|https://github.com/cooldoger/cassandra/tree/14169] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14169.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14169] |","16/Jan/18 03:59;VincentWhite;I wonder if we should include the rest of the parameters that are normally included by build.xml e.g. 

{code:java}
 <jvmarg value=""-Dmigration-sstable-root=${test.data}/migration-sstables""/>
 <jvmarg value=""-Dcassandra.ring_delay_ms=1000""/>
 <jvmarg value=""-Dcassandra.tolerate_sstable_size=true""/>
 <jvmarg value=""-Dcassandra.skip_sync=true"" />{code}


I don't know if it should be its own ticket, but I also noticed this the exception message isn't particularly helpful since it outputs the wrong variable

{code: title=org.apache.cassandra.io.sstable.LegacySSTableTest#defineSchema | java}
        String scp = System.getProperty(LEGACY_SSTABLE_PROP);
        Assert.assertNotNull(""System property "" + LEGACY_SSTABLE_ROOT + "" not set"", scp);
{code}

I believe it is meant to be:

{code: title=org.apache.cassandra.io.sstable.LegacySSTableTest#defineSchema | java}
        String scp = System.getProperty(LEGACY_SSTABLE_PROP);
        Assert.assertNotNull(""System property "" + LEGACY_SSTABLE_PROP + "" not set"", scp);
{code}","16/Jan/18 06:34;jay.zhuang;Thanks [~VincentWhite] for the review.

bq. <jvmarg value=""-Dmigration-sstable-root=${test.data}/migration-sstables""/>}}
The migration-sstables is removed in CASSANDRA-12716, removing them in the {{build.xml}}.

bq. <jvmarg value=""-Dcassandra.ring_delay_ms=1000""/>
It wouldn't impact the test, but nice to have. added.

bq. <jvmarg value=""-Dcassandra.tolerate_sstable_size=true""/>
It's just to print warning message: [LeveledCompactionStrategy.java:69|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java#L69]. I think it's good to have them in IDE debug.

bq. <jvmarg value=""-Dcassandra.skip_sync=true"" />
It's added in CASSANDRA-9403. would be nice to have. added.


Also for {{LEGACY_SSTABLE_ROOT}} -> {{LEGACY_SSTABLE_PROP}}, changed.

The patch is updated.

","14/Feb/18 15:35;spodxx@gmail.com;Thanks Jay!
Merged as 7a424bc2a79dce98817054057dd3a479b202f09e to trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect documentation about CASSANDRA_INCLUDE priority,CASSANDRA-14175,13132116,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,moneytoo,moneytoo,19/Jan/18 08:24,12/Mar/19 14:21,13/Mar/19 22:35,19/Jan/18 13:17,3.0.16,3.11.2,4.0,,,Legacy/Documentation and Website,,,,,1,,,,"In _bin/cassandra_ the comments say:
{quote}The lowest priority search path is the same directory as the startup script...
{quote}
However the ""same directory"" currently has the *highest* priority:
{code:java}
    # Locations (in order) to use when searching for an include file.
    for include in ""`dirname ""$0""`/cassandra.in.sh"" \
                   ""$HOME/.cassandra.in.sh"" \
                   /usr/share/cassandra/cassandra.in.sh \
                   /usr/local/share/cassandra/cassandra.in.sh \
                   /opt/cassandra/cassandra.in.sh; do
        if [ -r ""$include"" ]; then
            . ""$include""
            break
        fi
    done
{code}
It looks like around the release of v 2.0.0 the order was changed but the comments stayed the same.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-19 13:17:21.163,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 19 13:17:21 UTC 2018,,,,,,0|i3p3r3:,9223372036854775807,3.11.1,,,,,,,,,,,2.0.0,,,,,,,"19/Jan/18 13:17;jasobrown;lol - nice find ;) Ninja-committed as sha {{1cb91eaaaad8169a7b680f1f6ab6b1418ce56e61}}

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix comparison of address and port for repair and messages,CASSANDRA-14225,13137315,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,09/Feb/18 01:14,12/Mar/19 14:21,13/Mar/19 22:35,13/Feb/18 23:24,4.0,,,,,Legacy/Core,,,,,0,,,,"compare both host and port, not just host",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,09/Feb/18 01:16;dbrosius;14225.txt;https://issues.apache.org/jira/secure/attachment/12909861/14225.txt,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-02-09 17:30:13.061,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,,,Tue Feb 13 23:24:13 UTC 2018,,,,,,0|i3pz7b:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,09/Feb/18 01:17;dbrosius;mind reviewing?,"09/Feb/18 17:30;aweisberg;Did you want me as the reviewer or assignee?

{{FBUtilities.getBroadcastNativeAddressAndPort()}} is for the port used by the CQL protocol. Maybe we should javadoc that. You want {{FBUtilities.getBroadcastAddressAndPort()}} for the storage port. There is a missing newline between the new import and the start of the class comments.

In RepairRunnable the diff introduces a tab when constructing the InetAddressAndPort.

Otherwise +1.",10/Feb/18 04:34;aweisberg;I also realized you probably can't assume that source_port is in the row. You will need to check if it is there and supply a default value if it isn't. I think it's the storage port you use in this case. I need to check.,"13/Feb/18 17:33;aweisberg;Testing in CircleCI https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14225
https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-14225?expand=1",13/Feb/18 23:24;aweisberg;+1 Committed as [834f2a6ecdb8974839762bf4e9c5fed32163f9c8|https://github.com/apache/cassandra/commit/834f2a6ecdb8974839762bf4e9c5fed32163f9c8] thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
logs directory for gc.log doesn't exist on first start,CASSANDRA-14142,13127488,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,alexott,alexott,28/Dec/17 08:51,12/Mar/19 14:21,13/Mar/19 22:35,21/Oct/18 09:12,4.0,,,,,Local/Config,,,,,0,lhf,,,"This was originally reported at https://stackoverflow.com/questions/47976248/gc-log-file-error-when-running-cassandra.

This is very minor problem related to timing of 'logs' directory creation - when Cassandra starts first time, this directory doesn't exist, and created when Cassandra starts to write system.log & debug.log files. But this directory is referenced in the -Xloggc command line parameter of JVM, causing following warning:

{{Java HotSpot(TM) 64-Bit Server VM warning: Cannot open file bin/../logs/gc.log due to No such file or directory}}

The fix is to check existence of this directory in the cassandra-env, and create it.

","Unix & Windows environments, when starting freshly downloaded tarball

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-28 08:55:58.879,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 21 09:12:13 UTC 2018,,,,,,0|i3oc5b:,9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Dec/17 08:55;githubbot;GitHub user alexott opened a pull request:

    https://github.com/apache/cassandra/pull/183

    fix for CASSANDRA-14142

    On the first start of Cassandra JVM complains about absence of `${CASSANDRA_HOME}/logs/gc.log` file.
    
    This pull request fixes this issue - cassandra-env scripts now check for existence of the 'logs' directory and create it if it doesn't exist.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/alexott/cassandra CASSANDRA-14142

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/183.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #183
    
----
commit 11a3f90e705c66e2d344b34fe2d31e6327819895
Author: Alex Ott <alexott@...>
Date:   2017-12-28T08:52:55Z

    fix for CASSANDRA-14142
    
    cassandra-env scripts now check for existence of the 'logs' directory and create it if it
    doesn't exist.

----
","16/Oct/18 23:18;kirktrue;There's a patch for this on GitHub. If it needs to be a proper patch, as per the contributor guidelines, I can do that on behalf of Alex, just let me know.


Thanks.",17/Oct/18 07:37;alexott;[~kirktrue] - I can rebase my patch to the trunk. Under the proper patch you mean to generate a diff & attach it? Should I also add the entry to CHANGES.txt? Anything else?,"20/Oct/18 00:20;kirktrue;[~alexott] - sorry for the confusion. I was merely trying to ping the reviewers to see if that was the reason it hasn't been reviewed/accepted. According to the contributor guidelines on the web site, submitting a patch and updating {{CHANGES.txt}} is the usual path.","21/Oct/18 09:11;alexott;It looks like that it was fixed with this commit: [https://github.com/alexott/cassandra/commit/6ba2fb9395226491872b41312d978a169f36fcdb] as part of work on CASSANDRA-9608 (Java 11 support) - the patch was very long in the review, so I haven't seen the changes.

I'll mark this bug as fixed.",21/Oct/18 09:12;alexott;It was fixed as part of work on CASSANDRA-9608,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sstablemetadata incorrect date string for ""EncodingStats minLocalDeletionTime:""",CASSANDRA-14132,13126418,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,VincentWhite,VincentWhite,VincentWhite,21/Dec/17 00:52,12/Mar/19 14:21,13/Mar/19 22:35,09/Feb/18 12:51,4.0,,,,,,,,,,0,,,,"There is a unit mismatch in the outputing of EncodingStats minLocalDeletionTime. EncodingStats.minLocalDeletionTime is stored in seconds but is being interpenetrated as milliseconds when converted to a date string. 

Patch: [Trunk|https://github.com/vincewhite/cassandra/commit/fa9ef1dede3067dffb65042ed4bdca08de042a0e]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-09 12:51:17.563,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,,,Fri Feb 09 12:51:17 UTC 2018,,,,,,0|i3o5k7:,9223372036854775807,,,,,,,,,,,,4.x,,,,,,,09/Feb/18 12:51;jasobrown;+1. committed as sha {{714703a08dfb965df40f4dad6ba83196ff95156f}}. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool tablestats/cfstats output has inconsistent formatting for latency,CASSANDRA-14233,13138357,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,sproberts92,sproberts92,sproberts92,13/Feb/18 22:47,12/Mar/19 14:21,13/Mar/19 22:35,14/Feb/18 13:04,3.11.2,4.0,,,,,,,,,0,,,,"Latencies are reported at keyspace level with `ms.` and at table level with `ms`.

There should be no trailing `.` as it is not a sentence and `.` is not part of the abbreviation.

This is also present in 2.x with `nodetool cfstats`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-14 00:08:30.301,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 14 23:36:22 UTC 2018,,,,,,0|i3q5mv:,9223372036854775807,3.11.1,,,,,,,cnlwsu,cnlwsu,,,2.0.0,,,,,,,"13/Feb/18 22:57;sproberts92;Very simple to fix:

-[https://github.com/sproberts92/cassandra/commit/0eeee5ccb4851bf2ab1f35237ec901e1426a9bda]-

but I am confused about which branch to commit to - docs state start with the oldest branch? As the file has been moved around a lot and names changed, I'm not sure if this will propagate to 3.x cleanly.",14/Feb/18 00:08;cnlwsu;Can you just make it for trunk? 2.0 isnt taking changes anymore and 2.1 is pretty much reserved to data loss bugs.,"14/Feb/18 06:54;githubbot;GitHub user sproberts92 opened a pull request:

    https://github.com/apache/cassandra/pull/195

    Resolve nodetool formatting in CASSANDRA-14233

    Remove trailing ""."" from latency reports at keyspace level.
    
    For [CASSANDRA-14233](https://issues.apache.org/jira/browse/CASSANDRA-14233).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sproberts92/cassandra CASSANDRA-14233

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/195.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #195
    
----
commit ae6e74ac0f31161be10481f2f4c76cebfe45d7b0
Author: Samuel Roberts <sproberts92@...>
Date:   2018-02-14T06:44:50Z

    Resolve nodetool formatting in CASSANDRA-14233
    
    Remove trailing ""."" from latency reports at keyspace level.

----
","14/Feb/18 08:04;sproberts92;Ok, I've done it against trunk.

Also, Gary Stewart from NL wants the world to know that he was the one who noticed the inconstancy, so let that be recorded here for posterity.","14/Feb/18 13:04;jasobrown;+1. committed as sha {{d10e6ac606c6b484c75bb850de7a754b75ad5eca}}.

Thanks for the patch!","06/Mar/18 09:44;githubbot;Github user spodkowinski commented on the issue:

    https://github.com/apache/cassandra/pull/195
  
    Please close the PR as the patch already has been merged in d10e6ac606c6b484c75bb850de7a754b75ad5eca
","14/Mar/18 23:36;githubbot;Github user sproberts92 closed the pull request at:

    https://github.com/apache/cassandra/pull/195
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix native protocol v5 spec for new_metadata_id position in Rows response,CASSANDRA-13986,13115488,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ifesdjeen,omichallat,omichallat,01/Nov/17 22:24,12/Mar/19 14:21,13/Mar/19 22:35,08/Nov/17 12:15,,,,,,,,,,,0,,,,"There's a mistake in the protocol specification for CASSANDRA-10786. In `native_protocol_v5.spec`, section 4.2.5.2:

{code}
4.2.5.2. Rows

  Indicates a set of rows. The rest of the body of a Rows result is:
    <metadata><rows_count><rows_content>
  where:
    - <metadata> is composed of:
        <flags><columns_count>[<new_metadata_id>][<paging_state>][<global_table_spec>?<col_spec_1>...<col_spec_n>]
{code}
The last line should be:
{code}
        <flags><columns_count>[<paging_state>][<new_metadata_id>][<global_table_spec>?<col_spec_1>...<col_spec_n>]
{code}
That is, if there is both a paging state and a new metadata id, the paging state comes *first*, not second.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10786,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-08 12:14:45.122,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 08 12:14:45 UTC 2017,,,,,,0|i3maef:,9223372036854775807,4.0,,,,,,,,,,,,,,,,,,08/Nov/17 12:14;ifesdjeen;Ninja-committed to {{trunk}} with [65ff3e6d9e15060786fe5fdec92005b9932cab08|https://github.com/apache/cassandra/commit/65ff3e6d9e15060786fe5fdec92005b9932cab08],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change repair midpoint logging from  CASSANDRA-13052,CASSANDRA-13603,13079879,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,14/Jun/17 20:59,12/Mar/19 14:21,13/Mar/19 22:35,05/Sep/17 19:02,3.0.15,3.11.1,4.0,,,Legacy/Streaming and Messaging,,,,,0,,,,"In CASSANDRA-13052 , we changed the way we handle repairs on small ranges to make them more sane in general, but {{MerkleTree.differenceHelper}} now erroneously logs at error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-06-19 11:37:48.27,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 05 19:02:14 UTC 2017,,,,,,0|i3ga6f:,9223372036854775807,,,,,,,,bdeggleston,bdeggleston,,,,,,,,,,"14/Jun/17 21:15;jjirsa;|| Branch || Testall || Dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13603] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13603] | [asf dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/88/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13603] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13603] | [asf dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/89/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13603] | [circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13603] | [asf dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/90/] |


For the reviewer: I really have mixed feelings about {{MerkleTree.differenceHelper}} in general. In particular, it presumes that there are differences in the trees, which is always true in the existing codepath, but may not be true later if someone misuses it in the future. It seems [fairly straightforward|https://github.com/jeffjirsa/cassandra/commit/cee5a2d75e8e867dc20f3d94f0fe5df360210d46] to make it properly handle the case where the trees are consistent for a given range, though I'm not sure it's necessary.

Thoughts [~spodxx@gmail.com] / [~bdeggleston] ? 
","19/Jun/17 11:37;spodxx@gmail.com;I agree that it's easy to misuse MerkleTree.differenceHelper(). It should only really be called from MerkleTree.difference() and should have a  {{@VisibleForTesting}} annotation to indicate that it's internal use only. As for the suggested changes around midpoint result handling, I'm not really convinced that we should use this value to check if we should stop tree traversal. Although we should be at a leaf nodes already when getting an invalid midpoint value, we wouldn't be able to tell otherwise without any kind of error message, blindly following this assumption. This may cover other potential errors.

We already check {{node instanceof Leaf}} before recursively calling differenceHelper() with a sub-range. I'd suggest to do the same in difference(). It just doesn't make sense to call differenceHelper() with two leaf nodes, doesn't it?","20/Jun/17 21:56;jjirsa;{quote}
We already check node instanceof Leaf before recursively calling differenceHelper() with a sub-range. I'd suggest to do the same in difference(). It just doesn't make sense to call differenceHelper() with two leaf nodes, doesn't it?
{quote}

I think you're right - pushed a new commit to each branch that checks to see if either node is {{instanceof Leaf}} and avoids that tree traversal. Also marked {{MerkleTree.differenceHelper}} as {{@VisibleForTesting}}

I think this makes the midpoint check below irrelevant, but may protect us from another bug later? Considering switching it to an assert rather than the block that exists now?

{code}
        if (midpoint.equals(active.left) || midpoint.equals(active.right))
{code} 



","21/Jun/17 10:12;spodxx@gmail.com;We should continue to check the midpoint return value. Even if it's just for troubleshooting similar problems in the future. 

Throwing an AssertionError instead of just logging an error message is probably not a good idea in 2.2. See my comment from before the patch:

{quote}Unfortunately we can't throw here to abort the validation process, as the code is executed in it's own thread with the caller waiting for a condition to be signaled after completion and without an option to indicate an error (2.x only).
{quote}

But 3.0+ should be able to deal with failed SyncTasks, as those are now handled as futures. SyncTask.run is currently missing any exception handling, but that could be changed.","29/Aug/17 23:39;jjirsa;Would love to revisit this.

[~spodxx@gmail.com] / [~bdeggleston] , pushed rebased versions, and here's shortcut links to combined diffs for the three branches: 

https://github.com/apache/cassandra/compare/cassandra-3.0...jeffjirsa:cassandra-3.0-13603?ws=1
https://github.com/apache/cassandra/compare/cassandra-3.11...jeffjirsa:cassandra-3.11-13603?ws=1 
https://github.com/apache/cassandra/compare/trunk...jeffjirsa:cassandra-13603?ws=1

Either of you see anything else you'd like to change that I didn't catch already? Either of you up to mark yourselves as formal reviewer? ","30/Aug/17 18:43;bdeggleston;I think the current state of {{MerkleTree.differenceHelper}} is sort of the wrong approach for what it's trying to do in general. The current implementation has weird edge cases like this, and is too complex and difficult to follow. Also, it shares some of it's responsibilities with {{difference}}. 

Really, we're just trying to recursively compare these trees and record the largest contiguous out of sync ranges. The only complication here is that a given invocation of the method doesn't know if it's adjacent range is also out of sync, so it has to defer to the caller to do something if it's range is {{FULLY_INCONSISTENT}}. I put together an alternate implementation [here|https://github.com/bdeggleston/cassandra/tree/differencer-cleanup].

Having said all that, those changes are out of scope for this ticket and 3.0, so I'm +1 on the changes as proposed, and will open another ticket to refactor differece/differenceHelper.",30/Aug/17 18:51;bdeggleston;opened CASSANDRA-13830,"05/Sep/17 19:02;jjirsa;Thanks. Committed as {{bc5c2316c5acfc1ee0ef101a3ebf00d03c89e283}} to 3.0 and merged 3.11 and trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor GcCompactionTest to avoid boxing,CASSANDRA-13941,13107685,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,07/Oct/17 06:27,12/Mar/19 14:21,13/Mar/19 22:35,07/Oct/17 06:31,4.0,,,,,,,,,,0,,,,"From GH PR #116:  https://github.com/apache/cassandra/pull/116

{quote}
Hello,

I am a graduate student at Oregon State University and as a part of my research and subject CS562 Applied Software Engineering project (Study and Refactoring of Functional Interface in Java), I have done refactoring of the Function<T,Integer> to ToIntFunction. My project deals with the boxing and unboxing of Wrapper class and the primitive data-types. I want to know that whether the open source community is ready to accept these micro-optimizing refactorings or not.

Thank you,
Harsh Thakor
{quote}

Trivial patch, but it's right and low risk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,https://github.com/apache/cassandra/pull/116,,,,,,,,,9223372036854775807,,,Sat Oct 07 06:31:06 UTC 2017,,,,,,0|i3kzrz:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"07/Oct/17 06:31;jjirsa;Committed to 4.0 as {{2ecadc88e4407ebd4b3325f42e0296e1e1dc8944}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trivial typo in JavaDriverClient.java,CASSANDRA-13355,13057705,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,jjirsa,jjirsa,20/Mar/17 21:49,12/Mar/19 14:21,13/Mar/19 22:35,20/Mar/17 21:51,4.0,,,,,Tool/stress,,,,,0,,,,Upstream github PR from Ian Macalinao https://github.com/apache/cassandra/pull/97 - simple typo fix. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,https://github.com/apache/cassandra/pull/97,https://github.com/apache/cassandra/pull/97,,,,,,,,,9223372036854775807,,,Mon Mar 20 21:51:43 UTC 2017,,,,,,0|i3cjon:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,20/Mar/17 21:51;jjirsa;Committed to trunk as {{5b8b1ce26cd073a44ddf7c7a6750da409a343eba}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in snitch.rst,CASSANDRA-13520,13070922,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,donchy,donchy,donchy,10/May/17 19:10,12/Mar/19 14:21,13/Mar/19 22:35,15/May/17 16:51,4.0,,,,,Legacy/Documentation and Website,,,,,0,,,,A patch to fix the typo: https://github.com/dongqixue/cassandra/commit/c70496884967e166859abce597e737e4f77b1ddc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-05-15 16:51:59.575,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon May 15 16:51:59 UTC 2017,,,,,,0|i3eskv:,9223372036854775807,,,,,,,,bdeggleston,bdeggleston,,,,,,,,,,15/May/17 16:51;bdeggleston;Committed as {{b87f79798ca244006906c41e31d35a84112b54be}}. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anticompaction can cause noisy log messages,CASSANDRA-13684,13086131,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,11/Jul/17 02:49,12/Mar/19 14:20,13/Mar/19 22:35,11/Jul/17 18:16,4.0,,,,,,,,,,0,,,,Anticompaction can cause unnecessarily noisy log messages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-11 17:19:22.836,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 11 18:16:39 UTC 2017,,,,,,0|i3hcn3:,9223372036854775807,,,,,,,,bdeggleston,bdeggleston,,,,,,,,,,11/Jul/17 02:52;jjirsa;Trivial logging change [here|https://github.com/jeffjirsa/cassandra/tree/cassandra-13684] ,"11/Jul/17 02:53;jjirsa;[~krummas] or [~bdeggleston] up for review?
",11/Jul/17 17:19;bdeggleston;+1,"11/Jul/17 18:16;jjirsa;Committed as {{ebd0aaefe54d8a1349a54d904831e1d9e5e812bf}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix documentation typo,CASSANDRA-13686,13086466,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mshuler,andrewwuan,andrewwuan,12/Jul/17 04:50,12/Mar/19 14:20,13/Mar/19 22:35,05/Mar/18 14:56,3.11.3,4.0,,,,Legacy/Documentation and Website,,,,,0,docuentation,,,"Fix documentation typo under {quote}doc/html/cql/definitions.html#constants{quote}
and
{quote}doc/html/cql/ddl.html#the-clustering-columns{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,12/Jul/17 04:50;andrewwuan;fix.patch;https://issues.apache.org/jira/secure/attachment/12876759/fix.patch,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-03-05 14:56:31.67,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,,,Mon Mar 05 14:56:31 UTC 2018,,,,,,0|i3heov:,9223372036854775807,3.10,,,,,,,,,,,,,,,,,,05/Mar/18 14:56;mshuler;[d6982cd|https://github.com/apache/cassandra/commit/d6982cd221ae6482cbe1cb796fe73d61160a89e0] committed to cassandra-3.11 branch and merged to trunk. Thank you for the patch [~andrewwuan].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nodetool listsnapshots output is missing a newline, if there are no snapshots",CASSANDRA-13568,13076432,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,arnd,arnd,arnd,01/Jun/17 08:34,12/Mar/19 14:20,13/Mar/19 22:35,22/Jun/17 02:19,3.0.15,3.11.1,4.0,,,Tool/nodetool,,,,,0,,,,"When there are no snapshots, the nodetool listsnaphots command output is missing a newline, which gives a somewhat bad user experience:

{code}
root@cassandra2:~# nodetool listsnapshots
Snapshot Details: 
There are no snapshotsroot@cassandra2:~# 
{code}

I",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Jun/17 08:37;arnd;0001-CASSANDRA-13568-add-newline-to-output-if-there-are-n.patch;https://issues.apache.org/jira/secure/attachment/12870758/0001-CASSANDRA-13568-add-newline-to-output-if-there-are-n.patch,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-06-22 01:57:46.226,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 22 02:19:20 UTC 2017,,,,,,0|i3fqjr:,9223372036854775807,3.10,,,,,,,jjirsa,jjirsa,,,,,,,,,,"01/Jun/17 08:37;arnd;From fe868e2da9b79977d7819bbeb92f69264abef803 Mon Sep 17 00:00:00 2001
From: Arnd Hannemann <arnd@arndnet.de>
Date: Thu, 1 Jun 2017 10:34:52 +0200
Subject: [PATCH] CASSANDRA-13568: add newline to output if there are no
 snaphots

Before this patch, the nodetool listsnaphots command output is missing a newline,
if there are no snaphots.
---
 src/java/org/apache/cassandra/tools/nodetool/ListSnapshots.java | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/src/java/org/apache/cassandra/tools/nodetool/ListSnapshots.java b/src/java/org/apache/cassandra/tools/nodetool/ListSnapshots.java
index 1b3065bf11..4c22906609 100644
--- a/src/java/org/apache/cassandra/tools/nodetool/ListSnapshots.java
+++ b/src/java/org/apache/cassandra/tools/nodetool/ListSnapshots.java
@@ -42,7 +42,7 @@ public class ListSnapshots extends NodeToolCmd
             final Map<String,TabularData> snapshotDetails = probe.getSnapshotDetails();
             if (snapshotDetails.isEmpty())
             {
-                System.out.printf(""There are no snapshots"");
+                System.out.println(""There are no snapshots"");
                 return;
             }

-- 
2.11.0
","22/Jun/17 01:57;jjirsa;lgtm, will commit as soon as we bump all the versions for the recent releases.
","22/Jun/17 02:19;jjirsa;Actually [~mshuler] gave me the go-ahead to commit now, so it's committed to 3.0 as {{96bd3d53637d95ce268e7d4521a62bb400bc161b}} and merged up.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trivial format error in StorageProxy,CASSANDRA-13551,13074640,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,24/May/17 20:36,12/Mar/19 14:20,13/Mar/19 22:35,20/Jun/17 17:06,4.0,,,,,,,,,,0,,,,"Maybe I should just ninja it: 

{code}
diff --git a/src/java/org/apache/cassandra/service/StorageProxy.java b/src/java/org/apache/cassandra/service/StorageProxy.java
index ea082d5..1ab8dd6 100644
--- a/src/java/org/apache/cassandra/service/StorageProxy.java
+++ b/src/java/org/apache/cassandra/service/StorageProxy.java
@@ -1319,7 +1319,7 @@ public class StorageProxy implements StorageProxyMBean
                 }
                 catch (Exception ex)
                 {
-                    logger.error(""Failed to apply mutation locally : {}"", ex);
+                    logger.error(""Failed to apply mutation locally"", ex);
                 }
             }
@@ -1345,7 +1345,7 @@ public class StorageProxy implements StorageProxyMBean
                 catch (Exception ex)
                 {
                     if (!(ex instanceof WriteTimeoutException))
-                        logger.error(""Failed to apply mutation locally : {}"", ex);
+                        logger.error(""Failed to apply mutation locally"", ex);
                     handler.onFailure(FBUtilities.getBroadcastAddress());
                 }
             }

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-05-24 20:39:46.829,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 20 17:06:29 UTC 2017,,,,,,0|i3ffhj:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,24/May/17 20:39;aweisberg;+1,"20/Jun/17 17:06;jjirsa;Thanks Ariel for the review and the nudge.

Committed to trunk as {{f21202e83f308ea22cd430499da60aebbfa8ffbc}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed unregistering mbean during drop keyspace,CASSANDRA-13346,13056967,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,Lerh Low,gabor.auth,gabor.auth,17/Mar/17 11:15,12/Mar/19 14:06,13/Mar/19 22:35,06/Jun/17 18:59,3.0.14,3.11.0,4.0,,,Feature/Materialized Views,,,,,0,lhf,,,"All node throw exceptions about materialized views during drop keyspace:
{code}

WARN  [MigrationStage:1] 2017-03-16 16:54:25,016 ColumnFamilyStore.java:535 - Failed unregistering mbean: org.apache.cassandra.db:type=Tables,keyspace=test20160810,table=unit_by_account
java.lang.NullPointerException: null
        at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106) ~[na:1.8.0_121]
        at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097) ~[na:1.8.0_121]
        at java.util.concurrent.ConcurrentHashMap$KeySetView.remove(ConcurrentHashMap.java:4569) ~[na:1.8.0_121]
        at org.apache.cassandra.metrics.TableMetrics.release(TableMetrics.java:712) ~[apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.ColumnFamilyStore.unregisterMBean(ColumnFamilyStore.java:570) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:527) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:517) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.Keyspace.unloadCf(Keyspace.java:365) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.Keyspace.dropCf(Keyspace.java:358) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.config.Schema.dropView(Schema.java:744) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.schema.SchemaKeyspace.lambda$mergeSchema$373(SchemaKeyspace.java:1287) [apache-cassandra-3.9.0.jar:3.9.0]
        at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_121]
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1287) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1256) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:51) ~[apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.9.0.jar:3.9.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_121]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{code}
",Cassandra 3.9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Apr/17 00:01;Lerh Low;13346-3.0.X.txt;https://issues.apache.org/jira/secure/attachment/12865234/13346-3.0.X.txt,27/Apr/17 00:01;Lerh Low;13346-3.X.txt;https://issues.apache.org/jira/secure/attachment/12865235/13346-3.X.txt,18/May/17 04:55;Lerh Low;13346-trunk.txt;https://issues.apache.org/jira/secure/attachment/12868679/13346-trunk.txt,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-03-26 03:21:06.181,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 06 18:59:04 UTC 2017,,,,,,0|i3cf4f:,9223372036854775807,3.0.13,3.10,,,,,,cnlwsu,cnlwsu,,,,,,,,,,26/Mar/17 03:21;jeromatron;Is there any context around this?  Were you doing any other schema modification at the time?  Also was there any other effect other than this error in the logs?,"04/Apr/17 00:59;Andrew Efimov;Hi Jeremy, Gábor
I have the same issue in 3.10, in my case, this occurs only for materialized views at the end of the test phase.
Also I am using org.cassandraunit for testing in embedded mode.
{noformat}
java.lang.NullPointerException: null
	at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentHashMap$KeySetView.remove(ConcurrentHashMap.java:4569) ~[na:1.8.0_121]
	at org.apache.cassandra.metrics.TableMetrics.release(TableMetrics.java:713) ~[cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.db.ColumnFamilyStore.unregisterMBean(ColumnFamilyStore.java:577) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:534) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:524) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.db.Keyspace.unloadCf(Keyspace.java:370) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.db.Keyspace.dropCf(Keyspace.java:363) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.config.Schema.dropView(Schema.java:704) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.schema.SchemaKeyspace.lambda$mergeSchema$17(SchemaKeyspace.java:1313) [cassandra-all-3.10.jar:3.10]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_121]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1313) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1282) [cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:535) ~[cassandra-all-3.10.jar:3.10]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[cassandra-all-3.10.jar:3.10]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_121]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) ~[cassandra-all-3.10.jar:3.10]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{noformat}","04/Apr/17 04:02;jjirsa;[~gabor.auth] - is {{test20160810.unit_by_account}} a materialized view as well? 

","04/Apr/17 05:17;gabor.auth;[~jjirsa]: yes, as I mentioned in the description: ""All node throw exceptions about materialized views during drop keyspace"". :)

All nodes throws this exception about all materialized view of the keyspace during drop keyspace command.","04/Apr/17 05:20;gabor.auth;""Is there any context around this?""

Hm... I saw only this exception.

""Were you doing any other schema modification at the time?""

No, only `DROP KEYSPACE test20160810`.

""Also was there any other effect other than this error in the logs?""

As I see, everything is okay but this exception.","04/Apr/17 10:49;Andrew Efimov;I guess that the problem may be in the difference of metrics types for Materialized view and Table:
{{at org.apache.cassandra.metrics.TableMetrics.release(TableMetrics.java:713)}}
{{TableMetrics}} does not find the metrics for view, can only be used for Table.","04/Apr/17 11:11;Andrew Efimov;{{TableMetrics.release}} does not release metric with name {{ViewReadTime}}
because it is not created, but release method tries to remove this metric with NPE:
{noformat}
C* 3.10
org.apache.cassandra.metrics.TableMetrics: 669

        // We do not want to capture view mutation specific metrics for a view
        // They only makes sense to capture on the base table
        if (cfs.metadata.isView())
        {
            viewLockAcquireTime = null;
            viewReadTime = null;
        }
        else
        {
            viewLockAcquireTime = createTableTimer(""ViewLockAcquireTime"", cfs.keyspace.metric.viewLockAcquireTime);
            viewReadTime = createTableTimer(""ViewReadTime"", cfs.keyspace.metric.viewReadTime);
        }
{noformat}","27/Apr/17 00:11;Lerh Low;This is quite easily reproduced, we just have to create a materialized view and try to drop keyspace. 

{code}
CREATE KEYSPACE mvtest WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor': 1 };
CREATE TABLE mvtest.tobedropped (
    foo int,
    bar text,
    baz text,
    PRIMARY KEY (foo, bar)
);

CREATE MATERIALIZED VIEW mvtest.explosion AS
    SELECT foo, bar, baz FROM mvtest.tobedropped WHERE
    foo IS NOT NULL AND bar IS NOT NULL AND baz IS NOT NULL
PRIMARY KEY (foo, bar, baz);

INSERT INTO mvtest.tobedropped (foo, bar, baz) VALUES (1, 'baz', 'bokusapp');
INSERT INTO mvtest.tobedropped (foo, bar, baz) VALUES (2, 'baz', 'vitamin');
INSERT INTO mvtest.tobedropped (foo, bar, baz) VALUES (2, 'backgammon', 'gin');

DROP KEYSPACE mvtest;
{code}

As [~Andrew Efimov] mentioned, this seems to be because {{ViewLockReadTime}} and {{ViewLockAcquireTime}} are both set to null for materialized views (they don't make sense for materialized views and was decided to be that way based on [CASSANDRA-10323|https://issues.apache.org/jira/browse/CASSANDRA-10323]). So the call to {{Metrics.getMetrics().get(name.getMetricName())}} returns null, which throws the Exception as the {{remove}} method does not allow {{null}} values (For the implementation of the set in {{allTableMetrics}}. I've attached a patch for both 3.0.X and 3.X since it's a relatively small change - it looks like it's just a case of trying to unregister a metric from the registry that doesn't exist so we should just ignore it when it's {{null}} (which is only when it's releasing view metrics). I've retested it on my local and it works...Any feedbacks are welcome! ([~carlyeks], [~cnlwsu]]...?), or guidance on writing tests if necessary (It doesn't seem like there are any metrics tests though there are metrics dtests, I'll try taking a look at dtests). ","27/Apr/17 03:15;cnlwsu;+1 for patch. Big +1 for some dtests, the registering and removing of metrics around keyspace and table drops+creations has come up a few times (notice all hard coded entries in {{release}} that has drifted in and out of date).","27/Apr/17 03:38;cnlwsu;so actually taking a second look there may be something more to this. The patch will still work though.

The {{release}} call is iterating over {{all}} registered mbeans, not just the ones for that table so whenever there are metrics that exist in some tables and not others, and gets dropped, it will throw the NPE. It really seems we should not be using a big static map to store these, but just a local one to store the ones we create for that table. The tricky thing is the ""all"" metrics which span multiple tables but that can probably be handled by getting a list of the columnfamilystore's and iterating over them instead of trying to keep a separate registry in sync.

Patch does fix it though and is a much simpler fix.","27/Apr/17 04:26;Lerh Low;Hey Chris,

Thanks for answering. I'll try to fit dtests into my schedule and take a look at how it works/how to write one. 

With regards to your more recent comment, my impression is {{all}} is a hashmap of metric names to their aliases.
 {{allTableMetrics}} is a map of each metric to their set of metrics for each Table ({{String, Set<String>}}, which is used in the aggregation metrics over all Tables (e.g Readlatency key will have all the tables' read latency in its Set of values). If we had it use a local one to store the ones we create for the table, then it would be harder to aggregate over all known Tables. 

Open to any suggestions though, as usual :) ","28/Apr/17 14:08;cnlwsu;the ""all"" map is actually for metrics that have a global representation thats aggregated. For example, theres a global sstables per read metric that aggregates all the individual table and keyspace metrics so you can see the entire nodes sstables per read to see if any are high instead of walking through entire table set to find out.","01/May/17 06:20;Lerh Low;Hi Chris,

While debugging, {{all}} is just a map of names to values. I think what you're referring to is {{allTableMetrics}}, which in that case then yep - as you mentioned, the downside is now we would need some way of aggregating over all metrics for the global ones, but if you'd rather it that way I can look into it. 

I've also gotten ready a dtest here: https://github.com/riptano/cassandra-dtest/pull/1467 Any feedbacks/code style/whatever are more than welcome. ","09/May/17 22:44;Lerh Low;Thought I should also add, this causes metrics not to be dropped properly, so there will still be metrics in the registry for tables that should have already been dropped as a result of this Exception..",09/May/17 23:42;cnlwsu;which can potentially be bad since recreating same named table may cause mbean naming conflicts and failures,"18/May/17 02:11;Lerh Low;Hi Chris,

Looking into it a little bit more, this is another way to do it - just retain enough information in {{allTableMetrics}} so we know what metrics are held by which CF. Or, at least, we can iterate through it to find the metrics that are held for a particular CF, and release all of them. In this case, the changes will look something like this: https://github.com/apache/cassandra/compare/trunk...juiceblender:cassandra-13346

Then {{allTableMetrics}} becomes something we keep in sync with the {{MetricsRegistry}}. In fact it's more or less identical or isomorphic to {{MetricsRegistry}}. Just with the way we currently register metrics, we use this method: 
{code}
public String getMetricName()
        {
            return MetricRegistry.name(group, type, name, scope);
        }
{code}
which adds a {{.}} in between each value. 
So that eventually, an entry in my branch's {{allTableMetrics}} looks like so: 
{{org.apache.cassandra.metrics:type=Table,keyspace=system_distributed,scope=repair_history,name=TotalDiskSpaceUsed -> The actual metric}}

While {{Metrics.getMetrics()}} looks like so: 
{{org.apache.cassandra.metrics.Table.BloomFilterFalsePositives.system.built_views -> The actual metric}}

From here, there are a few options forward:
i) We stick with the original patch
ii) We go ahead with what's in the branch, the dtest still works for it.
iii) We totally get rid of {{allTableMetrics}} and just use the existing {{MetricsRegistry}}. To make it really safe for the aggregating metrics, I feel we would need some way to construct (or parse) a {{MetricName}} object from the String returned from {{Metrics.getMetrics()}} (It's a Map<String, Metric> unfortunately). In this case we will have to iterate over every metric that we ever registered compared to just the table metrics in {{allTableMetrics}}, but it should be relatively fine because we don't release metrics often
iv) We try and have each CFStore keep a local copy of its TableMetric. When constructing TableMetric for a CFStore, we will have to get a list of CFStores (I would need some guidance on this, from a viewManager somewhere?) and get all their respective gauges and aggregate them that way. Something like iii). 

I'm for either i) or iii), any thoughts? 
",18/May/17 02:20;cnlwsu;I like idea of {{i}} immediately and {{iii}} being done in follow up. It could use some refactoring/cleaning up.,18/May/17 04:41;Lerh Low;Let's go with {{i}} then. Would you be able to review and commit please if it's ok or if anything else needs to be done? :) The dtest is here: https://github.com/riptano/cassandra-dtest/pull/1467,"18/May/17 04:48;Lerh Low;Sorry, forgot about trunk. Updating. ","18/May/17 04:55;Lerh Low;Attached, trunk is the same as 3.10.",18/May/17 14:07;cnlwsu;I cant commit but +1 from me,"18/May/17 20:59;Lerh Low;Ahhh I see, thanks for going through all that with me though :)","05/Jun/17 23:06;jjirsa;[~cnlwsu] / [~Lerh Low] - just to be clear, which patches here are ready to commit? The ones attached to the JIRA, or the github URL https://github.com/apache/cassandra/compare/trunk...juiceblender:cassandra-13346 ? 

 ",05/Jun/17 23:24;Lerh Low;[~jjirsa] The ones attached to the JIRA :) ,"06/Jun/17 18:59;jjirsa;Committed as {{40ad3cf4dd384bede595edce4617534ca904f1ed}}, thanks all!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Legacy caching options can prevent 3.0 upgrade,CASSANDRA-13384,13059612,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,28/Mar/17 04:20,12/Mar/19 14:06,13/Mar/19 22:35,29/Mar/17 00:32,3.0.13,3.11.0,,,,Legacy/Distributed Metadata,,,,,0,,,,"In 2.1, we wrote caching options as a JSONified map, but we tolerated raw strings [""ALL"", ""KEYS_ONLY"", ""ROWS_ONLY"", and ""NONE""|https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/cache/CachingOptions.java#L42].

If a 2.1 node with any of these strings is upgraded to 3.0, the legacy schema migration will fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-28 17:38:35.404,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 29 00:32:37 UTC 2017,,,,,,0|i3cvfz:,9223372036854775807,,,,,,,,jjordan,jjordan,,,3.0.0,,,,,,,"28/Mar/17 17:30;jjirsa;No patch for trunk, we don't expect 2.x -> 4.0 upgrades, they have to go through 3.0, which will handle the upgrade. 

|| branch || utest || dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13384]  | [testall|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13384-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13384-dtest/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13384]  | [testall|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13384-testall/] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13384-dtest/] |

","28/Mar/17 17:38;jjordan;Change looks good to me, but maybe we should add an upgrade dtest which triggers the issue?","28/Mar/17 18:31;jjirsa;How strong is your desire to see a dtest? I think it's a reasonable ask, but it's also a LOT of effort. 

The dtest upgrade logic that exists requires a consistent native proto version through the test. Since 2.0 and 3.0 don't have a common native proto, none of the existing rolling tests can be used - we'd have to write a new harness that shuts down the session and reconnects midway through the test, which is do-able, but a fair amount of effort for something that seems like it's testable with a unit test.

","28/Mar/17 19:02;jjirsa;Note: 3.11 seems to have made {{testReport/junit/org.apache.cassandra.schema/LegacySchemaMigratorTest}} flakey

{code}
    [junit] Testcase: testMigrateLegacyCachingOptions(org.apache.cassandra.schema.LegacySchemaMigratorTest):	FAILED
    [junit] This assertion failure is probably due to accessing Schema.instance from client-mode tools - See CASSANDRA-8143.
    [junit] junit.framework.AssertionFailedError: This assertion failure is probably due to accessing Schema.instance from client-mode tools - See CASSANDRA-8143.
    [junit] 	at org.apache.cassandra.config.CFMetaData.<init>(CFMetaData.java:286)
    [junit] 	at org.apache.cassandra.config.CFMetaData.<init>(CFMetaData.java:65)
    [junit] 	at org.apache.cassandra.config.CFMetaData$Builder.build(CFMetaData.java:1328)
    [junit] 	at org.apache.cassandra.config.CFMetaData.compile(CFMetaData.java:427)
    [junit] 	at org.apache.cassandra.db.SystemKeyspace.compile(SystemKeyspace.java:435)
    [junit] 	at org.apache.cassandra.db.SystemKeyspace.<clinit>(SystemKeyspace.java:116)
    [junit] 	at org.apache.cassandra.schema.LegacySchemaMigrator.<clinit>(LegacySchemaMigrator.java:65)
    [junit] 	at org.apache.cassandra.schema.LegacySchemaMigratorTest.testMigrateLegacyCachingOptions(LegacySchemaMigratorTest.java:106)
    [junit]
    [junit]
    [junit] Testcase: testMigrate(org.apache.cassandra.schema.LegacySchemaMigratorTest):	Caused an ERROR
    [junit] Could not initialize class org.apache.cassandra.db.SystemKeyspace
    [junit] java.lang.NoClassDefFoundError: Could not initialize class org.apache.cassandra.db.SystemKeyspace
    [junit] 	at org.apache.cassandra.config.Schema.<init>(Schema.java:70)
    [junit] 	at org.apache.cassandra.config.Schema.<clinit>(Schema.java:49)
    [junit] 	at org.apache.cassandra.cql3.functions.UDFunction.<init>(UDFunction.java:215)
    [junit] 	at org.apache.cassandra.cql3.functions.JavaBasedUDFunction.<init>(JavaBasedUDFunction.java:190)
    [junit] 	at org.apache.cassandra.cql3.functions.UDFunction.create(UDFunction.java:233)
    [junit] 	at org.apache.cassandra.schema.LegacySchemaMigratorTest.keyspaceWithUDFs(LegacySchemaMigratorTest.java:374)
    [junit] 	at org.apache.cassandra.schema.LegacySchemaMigratorTest.keyspacesToMigrate(LegacySchemaMigratorTest.java:293)
    [junit] 	at org.apache.cassandra.schema.LegacySchemaMigratorTest.testMigrate(LegacySchemaMigratorTest.java:72)
    [junit]
{code}

Investigating to see if it's a test problem or regression.
","28/Mar/17 20:58;jjirsa;There's a second commit that more explicitly initializes the tests, which handles potential problems in ordering. With that change, I've executed the unit test 150 times locally and once on CI without error. 

Back to you, [~jjordan] - how strongly do you feel that the dtest is necessary?","28/Mar/17 22:14;jjordan;So I did some more investigation here with some test upgrades myself and after creating a table on 2.0 and upgrading to 2.1 the schema seemingly has the correct json formatted thing in it.  But if you then immediately upgrade to 3.0 with a kill -9 stop, 3.0 fails to start.  If you instead flush before upgrading to 3.0 the json version of the schema is flushed to disk, and everything is fine.  I'm not sure it is worth adding ugly legacy conversion hacks in 3.x for this super edge case of ""upgrade as fast as I can without flushing"".","28/Mar/17 22:17;jjordan;Anyway, this code works.  Adding basically dead code to 3.x seems ugly to me, but it is possible to hit this if you upgrade fast enough, so I guess it may be worth adding it, since we can drop it on merge to master.","28/Mar/17 22:20;jjirsa;I intend to drop it on the merge to master - would only exist for 3.0 and 3.11
",28/Mar/17 22:23;jjordan;+1,"29/Mar/17 00:32;jjirsa;Committed to 3.0 as {{6edc26824747b204fefc31478db833667d5d5892}}, merged into 3.11, and then skipped trunk (did {{merge -s ours}}, but only updated CHANGES, so the code will not exist for 4.0).

Thanks [~jjordan] . ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unittest CipherFactoryTest failed on MacOS,CASSANDRA-13370,13058362,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,22/Mar/17 18:59,12/Mar/19 14:06,13/Mar/19 22:35,24/Mar/17 17:28,3.11.x,4.x,,,,Legacy/Testing,,,,,0,,,,"Seems like MacOS(El Capitan) doesn't allow writing to {{/dev/urandom}}:
{code}
$ echo 1 > /dev/urandom
echo: write error: operation not permitted
{code}
Which is causing CipherFactoryTest failed:
{code}
$ ant test -Dtest.name=CipherFactoryTest
...
    [junit] Testsuite: org.apache.cassandra.security.CipherFactoryTest
    [junit] Testsuite: org.apache.cassandra.security.CipherFactoryTest Tests run: 7, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 2.184 sec
    [junit]
    [junit] Testcase: buildCipher_SameParams(org.apache.cassandra.security.CipherFactoryTest):  Caused an ERROR
    [junit] setSeed() failed
    [junit] java.security.ProviderException: setSeed() failed
    [junit]     at sun.security.provider.NativePRNG$RandomIO.implSetSeed(NativePRNG.java:472)
    [junit]     at sun.security.provider.NativePRNG$RandomIO.access$300(NativePRNG.java:331)
    [junit]     at sun.security.provider.NativePRNG.engineSetSeed(NativePRNG.java:214)
    [junit]     at java.security.SecureRandom.getDefaultPRNG(SecureRandom.java:209)
    [junit]     at java.security.SecureRandom.<init>(SecureRandom.java:190)
    [junit]     at org.apache.cassandra.security.CipherFactoryTest.setup(CipherFactoryTest.java:50)
    [junit] Caused by: java.io.IOException: Operation not permitted
    [junit]     at java.io.FileOutputStream.writeBytes(Native Method)
    [junit]     at java.io.FileOutputStream.write(FileOutputStream.java:313)
    [junit]     at sun.security.provider.NativePRNG$RandomIO.implSetSeed(NativePRNG.java:470)
...
{code}

I'm able to reproduce the issue on two Mac machines. But not sure if it's affecting all other developers.

{{-Djava.security.egd=file:/dev/urandom}} was introduced in:
CASSANDRA-9581

I would suggest to revert the [change|https://github.com/apache/cassandra/commit/ae179e45327a133248c06019f87615c9cf69f643] as {{pig-test}} is removed ([pig is no longer supported|https://github.com/apache/cassandra/commit/56cfc6ea35d1410f2f5a8ae711ae33342f286d79]).
Or adding a condition for MacOS in build.xml.

[~aweisberg] [~jasobrown] any thoughts?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Mar/17 16:34;jay.zhuang;13370-trunk-update.txt;https://issues.apache.org/jira/secure/attachment/12860183/13370-trunk-update.txt,22/Mar/17 21:31;jay.zhuang;13370-trunk.txt;https://issues.apache.org/jira/secure/attachment/12860012/13370-trunk.txt,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-03-22 19:18:43.363,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 24 17:28:41 UTC 2017,,,,,,0|i3cnqf:,9223372036854775807,4.x,,,,,,,aweisberg,aweisberg,,,,,,,,,,"22/Mar/17 19:18;aweisberg;It would be nice to have tests not block on secure random in the environments where that block for an unfortunate amount of time. I looked and I couldn't find a way to have SHA1PRNG or a fast seed generator be the default. I suspect there is a configuration out there that will initialize quickly, but I couldn't find it.

I would +1 switching to something that works on OS X in the interim.

",22/Mar/17 21:34;jay.zhuang;[~aweisberg] how about this fix: [718f67d|https://github.com/cooldoger/cassandra/commit/718f67d711c15b0d9dbebce3065064c73efd85e5]?,"23/Mar/17 09:54;spodxx@gmail.com;Jay, shouldn't simply removing the seed be enough? Do you still have to remove the egd path to get rid of the error?","23/Mar/17 14:36;aweisberg;I think we should remove the seed anyways so that subsequent usage of secure random doesn't also fail only on OX X. These tests have been failing for a long time without being fixed.
","23/Mar/17 16:25;aweisberg;Oh, I misunderstood. So it's removing the seed that will stop Java from writing to /dev/random. Yes I think that would be a better approach.","23/Mar/17 16:37;jay.zhuang;Make sense. Thanks [~spod]
Updated the [patch|https://github.com/cooldoger/cassandra/commit/e89ac4407f387dc9607b21d3ef9ece6d4bda4bd8], passed the test locally on MacOS and Linux.","23/Mar/17 17:01;aweisberg;Sorry to keep changing my mind. Still digesting the fact that we can fix just this one test and keep using /dev/urandom. I checked and we don't use seeding of SecureRandom outside of this test. So I propose going with your original solution of using SHA1PRNG, seeding it the way the test does so that the test is deterministic as originally intended, and not changing anything in build.xml.

||Code|utests||
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...aweisberg:cassandra-13370-3.11?expand=1]|[utests|https://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-cassandra-13370-3.11-testall/2/]|","23/Mar/17 18:21;spodxx@gmail.com;-I don't think seeding SecureRandom does what you think it does, Ariel. The provided seed will just get ""mixed"" with the current RNG seed. This is different from e.g. seeding java.util.Random and will not make the test deterministic.-

(does only seem to apply to the native PRNG, just tested with SHA1PRNG and it worked as described by you, my mistake here)
","23/Mar/17 18:46;aweisberg;Well it's still news to me that NativePRNG does mixing. Thanks for bringing it up. I guess the test author didn't rely on it being deterministic.

The right thing to do anyways is to generate a random seed and log it. So you get fuzzing but you can still reproduce a failure. I amended my original commit.","23/Mar/17 19:07;spodxx@gmail.com;The initial stacktrace from the description shows that NativePRNG tries do write the seed into /dev/urandom. Any values written there will not reset the seed system wide (which would be funny), but simply provide a bit of additional entropy. SHA1RPNG seems to work differently though, but the [javadoc|https://docs.oracle.com/javase/8/docs/api/java/security/SecureRandom.html#setSeed-byte:A-] isn't very clear either what exactly to expect.",24/Mar/17 16:57;aweisberg;[~jay.zhuang] does this work for you? I don't want to rewrite your patch without running it by you.,"24/Mar/17 17:03;jay.zhuang;[~aweisberg] yes, sure. Thanks for improving that.",24/Mar/17 17:28;aweisberg;Committed as [ee7023e324cdd3b3442b04ad4b0b1f4b33921d35|https://github.com/apache/cassandra/commit/ee7023e324cdd3b3442b04ad4b0b1f4b33921d35],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDA fails without input rows,CASSANDRA-13399,13061049,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,snazy,PAF,PAF,03/Apr/17 07:34,12/Mar/19 14:06,13/Mar/19 22:35,02/May/17 23:18,3.11.0,,,,,Legacy/CQL,,,,,0,,,,"When creating the following user defined AGGREGATION and FUNCTION:

{code:title=init.cql|borderStyle=solid}
CREATE FUNCTION state_group_and_total(state map<uuid, int>, type uuid)
    RETURNS NULL ON NULL INPUT
    RETURNS map<uuid, int>
    LANGUAGE java AS '
        Integer count = (Integer) state.get(type);

        count = (count == null ? 1 : count + 1);
        state.put(type, count);

        return state;
    ';

CREATE OR REPLACE AGGREGATE group_and_total(uuid)
    SFUNC state_group_and_total
    STYPE map<uuid, int>
    INITCOND {};
{code}

And creating a statement like:

{code}
SELECT group_and_total(""id"") FROM mytable;
{code}

When mytable is empty, it throws the following null assertion

{code}
ERROR [Native-Transport-Requests-1] 2017-04-03 07:25:09,787 Message.java:623 - Unexpected exception during request; channel = [id: 0xd7d9159b, L:/172.19.0.2:9042 - R:/172.19.0.3:43444]
java.lang.AssertionError: null
        at org.apache.cassandra.cql3.functions.UDAggregate$2.compute(UDAggregate.java:189) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.AggregateFunctionSelector.getOutput(AggregateFunctionSelector.java:53) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.Selection$SelectionWithProcessing$1.getOutputRow(Selection.java:592) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.getOutputRow(Selection.java:430) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.build(Selection.java:424) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:763) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:400) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:378) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:251) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:79) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:217) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:248) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:233) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.10.jar:3.10]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:366) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:357) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.10.jar:3.10]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]

{code}

Even if my FUNCTION only returns state, it creates that assertion null.

Thank you in advance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13375,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-03 08:25:54.27,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue May 02 23:18:26 UTC 2017,,,,,,0|i3d4b3:,9223372036854775807,3.10,,,,,,,blerer,blerer,,,,,,,,,,"03/Apr/17 08:25;snazy;This is a regression in 3.x.

||cassandra-3.0|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...snazy:13399-uda-state-3.0]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-3.0-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-3.0-dtest/lastSuccessfulBuild/]
||cassandra-3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:13399-uda-state-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-3.11-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-3.11-dtest/lastSuccessfulBuild/]
||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:13399-uda-state-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13399-uda-state-trunk-dtest/lastSuccessfulBuild/]

Fix in 3.11 branch. The new utest is applied to 3.0, which does not show this behaviour.",02/May/17 19:24;blerer;Thanks for the patch. It looks good to me. ,"02/May/17 23:18;snazy;Thanks!
Committed as [388c961e4bb8f0a414c6aa700c67cd76eaf01046|https://github.com/apache/cassandra/commit/388c961e4bb8f0a414c6aa700c67cd76eaf01046] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11]
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If there are multiple values for a key, CQL grammar choses last value. This should not be silent or should not be allowed.",CASSANDRA-13369,13058350,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nachiket_patil,nachiket_patil,nachiket_patil,22/Mar/17 18:48,12/Mar/19 14:06,13/Mar/19 22:35,08/May/17 20:52,3.11.0,4.0,,,,Legacy/CQL,,,,,0,,,,"If through CQL, multiple values are specified for a key, grammar parses the map and last value for the key wins. This behavior is bad.
e.g. 
{code}
CREATE KEYSPACE Excalibur WITH REPLICATION = {'class': 'NetworkTopologyStrategy', 'dc1': 2, 'dc1': 5};
{code}

Parsing this statement, 'dc1' gets RF = 5. This can be catastrophic, may even result in loss of data. This behavior should not be silent or not be allowed at all.  
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Mar/17 22:17;nachiket_patil;3.X.diff;https://issues.apache.org/jira/secure/attachment/12860748/3.X.diff,08/May/17 18:20;aweisberg;test_stdout.txt;https://issues.apache.org/jira/secure/attachment/12866959/test_stdout.txt,27/Mar/17 22:17;nachiket_patil;trunk.diff;https://issues.apache.org/jira/secure/attachment/12860749/trunk.diff,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-03-25 17:24:34.774,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:49:44 UTC 2017,,,,,,0|i3cnnr:,9223372036854775807,2.1.x,3.0.x,4.x,,,,,aweisberg,aweisberg,,,,,,,,,,"25/Mar/17 17:24;jjirsa;*This is not a review*, just a quick glance. However:

{code}
+            // Create a keyspace
+            execute(""CREATE KEYSPACE testABC WITH replication = {'class' : 'NetworkTopologyStrategy', '"" + DATA_CENTER + ""' : 2}"");
+
+            // try modifying the keyspace
+            assertInvalidThrow(SyntaxException.class, ""CREATE KEYSPACE testABC WITH replication = {'class' : 'NetworkTopologyStrategy', '"" + DATA_CENTER + ""' : 2, '"" + DATA_CENTER + ""' : 3 }"");
+            execute(""ALTER KEYSPACE testABC WITH replication = {'class' : 'NetworkTopologyStrategy', '"" + DATA_CENTER + ""' : 3}"");
{code}

You're creating the keyspace, then your comment says modify, but the CQL query is another {{CREATE}}, which will definitely fail. You're checking for {{SyntaxException}}, so I suspect the test is doing the right thing, but it's a bit confusing. Would be better if that was {{ALTER}}",27/Mar/17 22:49;nachiket_patil;[~jjirsa] Thanks. Fixed.,"02/May/17 19:42;aweisberg;||Code|utests|dtests||
|[3.11|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-13369-3.11?expand=1]|[utests|https://circleci.com/gh/aweisberg/cassandra/268]||","08/May/17 18:20;aweisberg;bootstrap_test.TestBootstrap.consistent_range_movement_false_with_replica_down_should_succeed_test
bootstrap_test.TestBootstrap.simultaneous_bootstrap_test
cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_blogposts
materialized_views_test.TestMaterializedViews.clustering_column_test
materialized_views_test.TestMaterializedViews.clustering_column_test
paxos_tests.TestPaxos.contention_test_many_threads
secondary_indexes_test.TestPreJoinCallback.write_survey_test
topology_test.TestTopology.size_estimates_multidc_test
topology_test.TestTopology.size_estimates_multidc_test",08/May/17 20:52;aweisberg;Committed as [https://github.com/apache/cassandra/commit/1a83efe2047d0138725d5e102cc40774f3b14641|1a83efe2047d0138725d5e102cc40774f3b14641]. Thanks.,06/Jul/17 17:49;aweisberg;[~jeromatron] Another ticket hit with the materialized view hammer?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool repair can hang forever if we lose the notification for the repair completing/failing,CASSANDRA-13480,13067566,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mbyrd,mbyrd,mbyrd,28/Apr/17 01:19,12/Mar/19 14:06,13/Mar/19 22:35,29/Jun/17 19:17,4.0,,,,,Tool/nodetool,,,,,0,repair,,,"When a Jmx lost notification occurs, sometimes the lost notification in question is the notification which let's RepairRunner know that the repair is finished (ProgressEventType.COMPLETE or even ERROR for that matter).
This results in nodetool process running the repair hanging forever. 

I have a test which reproduces the issue here:
https://github.com/Jollyplum/cassandra-dtest/tree/repair_hang_test

To fix this, If on receiving a notification that notifications have been lost (JMXConnectionNotification.NOTIFS_LOST), we instead query a new endpoint via Jmx to receive all the relevant notifications we're interested in, we can replay those we missed and avoid this scenario.

It's possible also that the JMXConnectionNotification.NOTIFS_LOST itself might be lost and so for good measure I have made RepairRunner poll periodically to see if there were any notifications that had been sent but we didn't receive (scoped just to the particular tag for the given repair).

Users who don't use nodetool but go via jmx directly, can still use this new endpoint and implement similar behaviour in their clients as desired.
I'm also expiring the notifications which have been kept on the server side.
Please let me know if you've any questions or can think of a different approach, I also tried setting:
 JVM_OPTS=""$JVM_OPTS -Djmx.remote.x.notification.buffer.size=5000""
but this didn't fix the test. I suppose it might help under certain scenarios but in this test we don't even send that many notifications so I'm not surprised it doesn't fix it.
It seems like getting lost notifications is always a potential problem with jmx as far as I can tell.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-8076,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-28 02:49:42.172,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri May 18 21:23:11 UTC 2018,,,,,,0|i3e7w7:,9223372036854775807,2.1.16,3.0.13,4.x,,,,,cnlwsu,cnlwsu,,,,,,,,,,"28/Apr/17 02:49;cnlwsu;how many notifications you see doesnt impact the notification buffer. JMX will create a buffer of notifications and cycle through them indexing new events as they are created. The JMX client will request events with the last index it has seen. Since the server does not store the state of the clients or know what they are listening for, ALL events regardless of listening state are appended into buffer. Even if nothing is listening to them all the storage notifications, the streaming notifications, the jvm hotspot notifications are being pushed onto that buffer. If your client takes too long between polling it will get lost notifications (and it will tell you how many it lost). 5000 still may not be nearly enough, but its gonna cost the heap dearly to make that value too large.

Nodetool actually used to just shut down on lost notifications, but in some clusters/workloads its almost impossible for a client to keep up. In CASSANDRA-7909 it was just starting to be logged. Querying a different endpoint wouldnt really help, only the repair coordinator has the events and it doesnt keep it around (and its cycled outta buffer). We could in theory pull expose a JMX operation that checks the repair_history table or current repair states to determine if the repair has been completed or errored out, and on lost notifications call it to make sure we did not miss a complete event.","28/Apr/17 23:01;mbyrd;So the patch I have currently also caches the notifications for repairs for a limited time on the co-ordinator, it was initially targeting a release where we didn't yet have the repair history tables.
I suppose there is a concern that caching these notifications could under some circumstances cause unwanted extra heap usage. 
(Similarly to the notifications buffer, although at least here we're only caching a subset that we care more about)
So using the repair history tables instead and exposing this information by imx seems like a reasonable alternative.
There are perhaps a couple of kinks to work out, but I'll have a go at adapting the patch that I have to work in this way.
For one we only have the cmd id int sent back to the nodetool process (rather than the parent session id which the internal table is partition keyed off)
We could either keep track of the cmd id int -> parent session uuid in the co-ordinator, either in memory cached to expire or in another internal table,
or we could parse the uuid out of the notification sent for the start of the parent repair.
Parsing the message is a bit brittle though and not full proof in theory (we could miss that notification also).
Ideally I suppose running a repair could return and communicate on the basis of the parent session uuid rather than the int cmd id, but this is a pretty major overhaul and has all sorts of compatibility questions.","19/Jun/17 23:01;mbyrd;||Trunk|||
|[branch|https://github.com/Jollyplum/cassandra/tree/13480]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/98/]|
|[testall|https://circleci.com/gh/Jollyplum/cassandra/14]|
","19/Jun/17 23:03;githubbot;GitHub user Jollyplum opened a pull request:

    https://github.com/apache/cassandra/pull/122

    When lost notifications occur and periodically, check for the parent …

    …repair status and exit if we've completed/failed
    
    patch by Matt Byrd, reviewed by Chris Lohfink for CASSANDRA-13480

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Jollyplum/cassandra 13480

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/122.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #122
    
----
commit b4c0a5a65ef94b1013793adb088ca11f563ff14b
Author: Matt Byrd <matthew.l.byrd@gmail.com>
Date:   2017-05-27T00:03:17Z

    When lost notifications occur and periodically, check for the parent repair status and exit if we've completed/failed
    patch by Matt Byrd, reviewed by Chris Lohfink for CASSANDRA-13480

----
",29/Jun/17 14:56;cnlwsu;+1,"29/Jun/17 19:17;jjirsa;Thanks all, committed into 4.0 as {{20d5ce8b9b587be2f0b7bc5765254e8dc6e0bd3b}}
","12/Oct/17 22:16;githubbot;Github user Jollyplum closed the pull request at:

    https://github.com/apache/cassandra/pull/122
","27/Feb/18 16:48;tania.engel@quest.com;[~mbyrd] : I have reason to believe I just hit this in 3.11.1, I at the very least ran into a repair which has never completed on an 11 node cluster. Is there a way to get this fix in 3.11?","17/May/18 23:26;michaelsembwever;I can see immediate benefit to users of Reaper with this, so would like to see it back ported to 3.11.x

[~jjirsa], what are your thoughts?

Regarding the patch:
 - the only public signature added are:
 -- the method {{List<String> StorageServiceMBean.getParentRepairStatus(int cmd)}}
 -- the enum {{ActiveRepairService.ParentRepairStatus}}
 -- the method {{ActiveRepairService.recordRepairStatus(…)}}
 - the addition of the 100k entry cache for ParentRepairStatus events

While it seems to be reasonably safe, if it is back ported I wonder if the {{getParentRepairStatus(..)}} method should be marked as {{@Beta}}?",18/May/18 21:23;rustyrazorblade;Assuming it back ports cleanly I'm +1 on bringing it to 3.11.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQLSH error when using 'login' to switch users,CASSANDRA-13640,13082866,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,adelapena,adelapena,adelapena,27/Jun/17 16:08,12/Mar/19 14:06,13/Mar/19 22:35,24/Aug/17 16:09,3.0.x,3.11.x,4.x,,,Legacy/CQL,,,,,0,,,,"Using {{PasswordAuthenticator}} and {{CassandraAuthorizer}}:

{code}
bin/cqlsh -u cassandra -p cassandra
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.0.14-SNAPSHOT | CQL spec 3.4.0 | Native protocol v4]
Use HELP for help.
cassandra@cqlsh> create role super with superuser = true and password = 'p' and login = true;
cassandra@cqlsh> login super;
Password:
super@cqlsh> list roles;

'Row' object has no attribute 'values'
{code}

When we initialize the Shell, we configure certain settings on the session object such as
{code}
self.session.default_timeout = request_timeout
self.session.row_factory = ordered_dict_factory
self.session.default_consistency_level = cassandra.ConsistencyLevel.ONE
{code}
However, once we perform a LOGIN cmd, which calls do_login(..), we create a new cluster/session object but actually never set those settings on the new session.

It isn't failing on 3.x. 

As a workaround, it is possible to logout and log back in and things work correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-21 14:35:27.652,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 16:09:15 UTC 2017,,,,,,0|i3gskn:,9223372036854775807,3.0.14,,,,,,,jasonstack,jasonstack,,,,,,,,,,27/Jun/17 18:22;adelapena;First draft of the patch [here|https://github.com/adelapena/cassandra/commit/95207a99402b15e32d75ad8e339e9f14d91a1b6e].,"29/Jun/17 08:33;adelapena;[Here|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13640-3.0] is the patch copying the session attributes into the new session created on {{do_login}}. 

A simple dtest reproducing the problem can be found [here|https://github.com/riptano/cassandra-dtest/compare/master...adelapena:CASSANDRA-13640].

Although this bug only affects to 3.0, the root cause is present in all the other branches (2.1, 2.2, 3.x and trunk). It doesn't affect to other versions because printing is done in a slightly different way, but relying on this would be IMHO quite risky. Also, the usage of {{LOGIN}} statement could lead to end up using a different default timeout or consistency level than the initially specified. So, I think that the patch should be applied to all branches. What do you think?",29/Jun/17 10:44;adelapena;I ran the patch on our internal CI. There are not failures for the unit tests and the failing dtests are not related to the change.,"21/Aug/17 14:35;jasonstack;The fix looks good. 3.11/trunk are also broken for the same reason, need to fix as well.","23/Aug/17 15:23;adelapena;Right. There are the patches for 3.0, 3.11, trunk and dtest:
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:13640-3.0]||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:13640-3.11]||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:13640-trunk]||[dtest|https://github.com/apache/cassandra-dtest/compare/master...adelapena:13640]||

Thanks for the review.",24/Aug/17 11:20;jasonstack;Thansk for the patch. +1,"24/Aug/17 16:09;adelapena;Committed as [9497191f5bab126c4d83ccbe023554fd6ea95257|https://github.com/apache/cassandra/commit/9497191f5bab126c4d83ccbe023554fd6ea95257].

Dtest committed as [da6ad8317e18ebaa5e8b428df79d1da086a19dd9|https://github.com/apache/cassandra-dtest/commit/da6ad8317e18ebaa5e8b428df79d1da086a19dd9].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool upgradesstables/scrub/compact ignores system tables,CASSANDRA-13410,13061492,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,04/Apr/17 17:45,12/Mar/19 14:06,13/Mar/19 22:35,05/Apr/17 17:13,3.0.13,3.11.0,4.0,,,,,,,,0,,,,"CASSANDRA-11627 changed the behavior of nodetool commands that work across all keyspaces. Sometimes it's OK (not compacting system.peers when you call compact probably isn't going to anger anyone), but sometimes it's not (disableautocompaction, flush, upgradesstables, etc).

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-11627,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-05 10:32:42.996,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 05 17:13:32 UTC 2017,,,,,,0|i3d71j:,9223372036854775807,,,,,,,,krummas,krummas,,,3.0.6,,,,,,,"04/Apr/17 19:34;jjirsa;|| branch || utests || dtests ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13410] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13410] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13410-dtest/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13410] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13410] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13410-dtest/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13410] | [testall|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13410] | [dtest|http://cassci.datastax.com/job/jeffjirsa-cassandra-13410-dtest/] |

","04/Apr/17 19:37;jjirsa;Reviewer: ignore the circle.yml commit, it's only there to enable me to run unit tests. The single line patch applies to all 3 versions cleanly.
",05/Apr/17 10:32;krummas;+1,"05/Apr/17 13:37;pauloricardomg;For the record this also affects 2.2 and was actually introduced by CASSANDRA-5483, as reported by CASSANDRA-11830, so I'm closing that as duplicate of this. But given 2.2 is nearly in critical-fixes-only mode, and would require a different patch I think it's fine to keep this 3.0+ only.","05/Apr/17 17:13;jjirsa;Committed as {{6efb44b3ab5816cb7c2b007dba68b3224f899ac8}} to 3.0 and merged up. 

Agreeing with [~pauloricardomg] , skipping 2.2 as it's not critical.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V5 protocol flags decoding broken,CASSANDRA-13443,13063713,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,snazy,snazy,snazy,13/Apr/17 07:06,12/Mar/19 14:06,13/Mar/19 22:35,13/Apr/17 09:24,3.11.0,4.0,,,,,,,,,0,,,,"Since native protocol version 5 we deserialize the flags in {{org.apache.cassandra.cql3.QueryOptions.Codec#decode}} as follows:
{code}
            EnumSet<Flag> flags = Flag.deserialize(version.isGreaterOrEqualTo(ProtocolVersion.V5)
                                                   ? (int)body.readUnsignedInt()
                                                   : (int)body.readByte());
{code}

This works until the highest bit (0x80) is not used. {{readByte}} must be changed to {{readUnsignedByte}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-04-13 07:48:58.897,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 13 09:24:28 UTC 2017,,,,,,0|i3dkq7:,9223372036854775807,,,,,,,,Stefania,Stefania,,,,,,,,,,"13/Apr/17 07:09;snazy;||cassandra-3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:13443-proto-flags-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13443-proto-flags-3.11-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13443-proto-flags-3.11-dtest/lastSuccessfulBuild/]
||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:13443-proto-flags-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13443-proto-flags-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13443-proto-flags-trunk-dtest/lastSuccessfulBuild/]
","13/Apr/17 07:48;Stefania;Nice catch, +1, provided tests pass.","13/Apr/17 09:24;snazy;Thank you!

Committed as [0a438d59e65ee79bca7ffc44b8b958e62448e5c3|https://github.com/apache/cassandra/commit/0a438d59e65ee79bca7ffc44b8b958e62448e5c3] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11]
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incremental repair prepare phase can cause nodetool to hang in some failure scenarios,CASSANDRA-13672,13084910,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,05/Jul/17 18:00,12/Mar/19 14:06,13/Mar/19 22:35,06/Jul/17 17:38,4.0,,,,,,,,,,0,,,,Also doesn't log anything helpful,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-06 07:24:08.95,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:38:28 UTC 2017,,,,,,0|i3h553:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"05/Jul/17 20:33;bdeggleston;This patch improves error logging and handling of prepare phase failures. It also sets exceptions on coordinator sessions whenever a session fails, so nodetool doesn't hang.

[trunk|https://github.com/bdeggleston/cassandra/tree/13672]
[utests|https://circleci.com/gh/bdeggleston/cassandra/64]",06/Jul/17 07:24;krummas;+1,06/Jul/17 17:38;bdeggleston;committed as {{af37489092ca90bca336538adad02fb5ba859945}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodes compute their own gcBefore times for validation compactions,CASSANDRA-13671,13084909,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,05/Jul/17 17:54,12/Mar/19 14:06,13/Mar/19 22:35,06/Jul/17 17:45,4.0,,,,,,,,,,0,,,,"{{doValidationCompaction}} computes {{gcBefore}} based on the time the method is called. If different nodes start validation on different seconds, tombstones might not be purged consistently, leading to over streaming.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-06 09:51:54.127,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:45:29 UTC 2017,,,,,,0|i00hc3:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"05/Jul/17 18:21;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13671]
[utests|https://circleci.com/gh/bdeggleston/cassandra/62]

Patch gets nowInSec on repair coordinator side and transmits it to other nodes in validation request. Purging is then done on all replicas against a common nowInSec value.",06/Jul/17 09:51;krummas;+1,06/Jul/17 17:45;bdeggleston;committed as {{9fdec0a82851f5c35cd21d02e8c4da8fc685edb2}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql_tests:SlowQueryTester.local_query_test and cql_tests:SlowQueryTester.remote_query_test failed on trunk,CASSANDRA-13650,13083734,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasonstack,jasonstack,jasonstack,30/Jun/17 10:12,12/Mar/19 14:06,13/Mar/19 22:35,06/Jul/17 13:34,4.x,,,,,Legacy/Distributed Metadata,,,,,0,,,,"cql_tests.py:SlowQueryTester.local_query_test failed on trunk
cql_tests.py:SlowQueryTester.remote_query_test failed on trunk
SHA: fe3cfe3d7df296f022c50c9c0d22f91a0fc0a217


It's due to the dtest unable to find {{'SELECT \* FROM ks.test1'}} pattern from log.
but in the log, following info is showed: 
{{MonitoringTask.java:173 - 1 operations were slow in the last 10 msecs: <SELECT val FROM ks.test1 LIMIT 5000>, time 102 msec - slow timeout 10 msec}}

ColumnFilter.toString() should return {{*}}, but return normal column {{val}} instead ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-06 13:32:31.807,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 13:32:31 UTC 2017,,,,,,0|i3gxwv:,9223372036854775807,,,,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,"01/Jul/17 10:08;jasonstack;|| source || junit-result || dtest-result||
| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13650] | [junit|https://circleci.com/gh/jasonstack/cassandra/90]  | {{bootstrap_test.py:TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test}} failed on trunk..| 

1. refactor column-filter to allow {{queried = null}} to represent {{wildcard (*)}} queries. (back to what the java comment described.)",01/Jul/17 12:16;jasonstack;[~ifesdjeen] could you review it? ,"06/Jul/17 13:32;ifesdjeen;Thank you for the patch! Committed with a minor change: took the {{queried == null}} and put it to the ternary operator instead of the outer {{if}} (the way it used to be in pre-13004 code).

Committed to trunk with [9359e1e977361774daf27e80112774210e55baa4|https://github.com/apache/cassandra/commit/9359e1e977361774daf27e80112774210e55baa4]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Properly evict pstmts from prepared statements cache,CASSANDRA-13641,13082875,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,snazy,snazy,snazy,27/Jun/17 16:40,12/Mar/19 14:06,13/Mar/19 22:35,28/Jun/17 19:20,3.11.1,,,,,,,,,,0,,,,Prepared statements that are evicted from the prepared statements cache are not removed from the underlying table {{system.prepared_statements}}. This can lead to issues during startup.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-06-28 07:35:22.966,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 28 19:20:55 UTC 2017,,,,,,0|i3gsmn:,9223372036854775807,,,,,,,,blerer,blerer,,,3.10,,,,,,,"27/Jun/17 17:46;snazy;||cassandra-3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:13641-evict-pstmt-3.11]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13641-evict-pstmt-3.11-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13641-evict-pstmt-3.11-dtest/lastSuccessfulBuild/]
||trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:13641-evict-pstmt-trunk]|[testall|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13641-evict-pstmt-trunk-testall/lastSuccessfulBuild/]|[dtest|http://cassci.datastax.com/view/Dev/view/snazy/job/snazy-13641-evict-pstmt-trunk-dtest/lastSuccessfulBuild/]
","28/Jun/17 07:35;blerer;Thanks for the patches, they look good to me.","28/Jun/17 19:20;snazy;Thanks for the quick review!

Committed as [9562b9b69e08b84ec1e8e431a846548fa8a83b44|https://github.com/apache/cassandra/commit/9562b9b69e08b84ec1e8e431a846548fa8a83b44] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11]
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
converting expired ttl cells to tombstones causing unnecessary digest mismatches,CASSANDRA-13643,13082943,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,27/Jun/17 21:02,12/Mar/19 14:06,13/Mar/19 22:35,14/Jul/17 17:55,3.0.15,3.11.1,4.0,,,,,,,,0,,,,"In [{{AbstractCell#purge}}|https://github.com/apache/cassandra/blob/26e025804c6777a0d124dbc257747cba85b18f37/src/java/org/apache/cassandra/db/rows/AbstractCell.java#L77]  , we convert expired ttl'd cells to tombstones, and set the the local deletion time to the cell's expiration time, less the ttl time. Depending on the timing of the purge, this can cause purge to generate tombstones that are otherwise purgeable. If compaction for a row with ttls isn't at the same state between replicas, this will then cause digest mismatches between logically identical rows, leading to unnecessary repair streaming and read repairs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13561,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-03 13:34:18.921,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 01 17:39:59 UTC 2017,,,,,,0|i3gt1r:,9223372036854775807,,,,,,,,slebresne,slebresne,,,,,,,,,,"27/Jun/17 21:11;bdeggleston;Patch here: https://github.com/bdeggleston/cassandra/tree/13643

Could you take a look at this [~slebresne]? I've just called purge on the created tombstone, which I wouldn't think would cause any problems.",27/Jun/17 23:02;bdeggleston;unit test run here: https://circleci.com/gh/bdeggleston/cassandra/55,"03/Jul/17 13:34;slebresne;Sound reasonable, +1 (though would be nice to have a dtest run as well to be sure). ","03/Jul/17 14:34;slebresne;As an aside, running from memory but I'm reasonably confident this is a regression from 3.0 in that we use to do the transformation of 'expired cell' to 'equivalent tombstone' in the deserialization code, with the purge code running afterwards, so that if the generated tombstone is directly purgeable, it would be purged right away. So I suppose the argument can be made that this is a bug and hence putting it in 3.0/3.11. I don't have a huge preference either way.","05/Jul/17 20:37;bdeggleston;Sound good, I'll apply to them as well. Thanks.

[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/112/]",14/Jul/17 17:55;bdeggleston;Committed as {{a033f51651e1a990adca795f92d683999c474151}},"01/Aug/17 17:39;jjirsa;Missing a fixver btw.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move processing of EchoMessage response to gossip stage,CASSANDRA-13713,13088897,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,20/Jul/17 21:10,12/Mar/19 14:06,13/Mar/19 22:35,29/Mar/18 12:43,4.0,,,,,Legacy/Distributed Metadata,,,,,0,,,,"Currently, when a node receives an {{EchoMessage}}, is sends a simple ACK reply back (see {{EchoVerbHandler}}). The ACK is sent on the small message connection, and because it is 'generically' typed as {{Verb.REQUEST_RESPONSE}}, is consumed on a {{Stage.REQUEST_RESPONSE}} thread. The proper thread for this response to be consumed is {{Stage.GOSSIP}}, that way we can move more of the updating of the gossip state to a single, centralized thread, and less abuse of gossip's shared mutable state can occur.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-26 19:41:28.346,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 29 12:43:25 UTC 2018,,,,,,0|i3htnj:,9223372036854775807,,,,,,,,jkni,jkni,,,,,,,,,,"20/Jul/17 21:23;jasobrown;A simple fix available here:

||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13713-trunk]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13713-trunk/]|

This incorrect behavior has been around for a long time. However, I'm not sure how far back to apply the change (the change will be the same for back to 2.1). The existing execution of {{Gossiper#realMarkAlive}} is already asynchronous (nothing is dependent on it, per se); the only downside is another (short-lived, once per peer) task to be executed on the gossip stage. I feel that's a tiny price to pay for reducing the number of different threads that can modify the state of gossip.
","26/Jul/17 19:41;jkni;The simple change here seems good - do you think it's worth doing something to map the echo response directly to the gossip stage and gossip connection? This seems like a good fix for the present state, but I think perhaps a different verb or a way to map individual REQUEST_RESPONSE verbs to a stage/connection makes sense on a longer term.","26/Jul/17 21:35;jasobrown;bq. do you think it's worth doing something to map the echo response directly to the gossip stage and gossip connection?

CASSANDRA-13714 :)","02/Aug/17 17:30;jkni;Ah - guess I missed that one. LGTM to the change here. I don't have any strong opinions as to where it goes in 2.2+. While we don't have any known problems related to this, we also know bugs can lurk in this subsystem for a long time, and this seems like a safe preventative measure.",29/Mar/18 12:43;jasobrown;Decided to be very cautious and only committed to trunk (4.0) as sha {{674addd03701abf1bce7d6b47978761c00d0a431}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Safely handle empty buffers when outputting to JSON,CASSANDRA-13868,13102326,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,14/Sep/17 17:40,12/Mar/19 14:06,13/Mar/19 22:35,15/Sep/17 11:30,2.2.11,3.0.15,3.11.1,4.0,,,,,,,0,,,,"As discussed in CASSANDRA-13734, when {{Selection#rowToJSON}} encounters an empty {{ByteBuffer}}, a {{BufferUnderflowException}} can be thrown. At a minimum let's be defensive and not throw an error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-15 11:30:29.371,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 15 11:30:29 UTC 2017,,,,,,0|i3k2wn:,9223372036854775807,,,,,,,,beobal,beobal,,,2.2.8,,,,,,,"14/Sep/17 18:00;jasobrown;Patch here

||2.2||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13868-2.2]|[branch|https://github.com/jasobrown/cassandra/tree/13868-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/13868-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13868-trunk]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13868-2.2]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13868-3.0]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13868-3.11]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13868-trunk]|
","15/Sep/17 11:30;beobal;Thanks [~jasobrown], I took the liberty of committing to cassandra-2.2 in {{a8e2dc52409ce0dc7476d60b3adee34c547f0b14}} and merging to 3.0/3.11/trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractTokenTreeBuilder#serializedSize returns wrong value when there is a single leaf and overflow collisions,CASSANDRA-13869,13102346,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jrwest,jrwest,jrwest,14/Sep/17 19:05,12/Mar/19 14:06,13/Mar/19 22:35,15/Sep/17 21:10,3.11.1,4.0,,,,Feature/SASI,,,,,0,,,,In the extremely rare case where a small token tree (< 248 values) has overflow collisions the size returned by AbstractTokenTreeBuilder#serializedSize is incorrect because it fails to account for the overflow collisions. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Sep/17 17:17;jrwest;0001-Fix-AbstractTokenTreeBuilder-serializedSize-when-the.patch;https://issues.apache.org/jira/secure/attachment/12887388/0001-Fix-AbstractTokenTreeBuilder-serializedSize-when-the.patch,15/Sep/17 17:17;jrwest;attb-serialized-size-bug-test.patch;https://issues.apache.org/jira/secure/attachment/12887387/attb-serialized-size-bug-test.patch,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-09-15 17:20:34.876,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 15 21:10:27 UTC 2017,,,,,,0|i3k313:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,15/Sep/17 17:18;jrwest;Attached two patches. attb-serialized-size-bug-test.patch is patch that can be applied to trunk illustrate the issue with a failing test. The other is the fix against trunk and some improved testing. ,"15/Sep/17 17:20;jasobrown;||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13869-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13869-trunk]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/322/]|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/321/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13869-3.11]|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13869-trunk]|
","15/Sep/17 21:10;jasobrown;+1. committed as sha {{436761ed6afbfdc333c6642f6b4418d44712a0b7}}

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyFactoryTest is failing in trunk on MacOS,CASSANDRA-13831,13098839,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,iamaleksey,iamaleksey,31/Aug/17 10:36,12/Mar/19 14:06,13/Mar/19 22:35,07/Sep/17 18:04,4.0,,,,,Legacy/Testing,,,,,0,,,,"Example failure:

{code}
    [junit] Testcase: getEventLoopGroup_EpollWithoutIoRatioBoost(org.apache.cassandra.net.async.NettyFactoryTest):	Caused an ERROR
    [junit] failed to load the required native library
    [junit] java.lang.UnsatisfiedLinkError: failed to load the required native library
    [junit] 	at io.netty.channel.epoll.Epoll.ensureAvailability(Epoll.java:78)
    [junit] 	at io.netty.channel.epoll.EpollEventLoop.<clinit>(EpollEventLoop.java:53)
    [junit] 	at io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:134)
    [junit] 	at io.netty.channel.epoll.EpollEventLoopGroup.newChild(EpollEventLoopGroup.java:35)
    [junit] 	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:84)
    [junit] 	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:58)
    [junit] 	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:47)
    [junit] 	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:59)
    [junit] 	at io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:104)
    [junit] 	at io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:91)
    [junit] 	at io.netty.channel.epoll.EpollEventLoopGroup.<init>(EpollEventLoopGroup.java:68)
    [junit] 	at org.apache.cassandra.net.async.NettyFactory.getEventLoopGroup(NettyFactory.java:175)
    [junit] 	at org.apache.cassandra.net.async.NettyFactoryTest.getEventLoopGroup_Epoll(NettyFactoryTest.java:187)
    [junit] 	at org.apache.cassandra.net.async.NettyFactoryTest.getEventLoopGroup_EpollWithoutIoRatioBoost(NettyFactoryTest.java:205)
    [junit] Caused by: java.lang.ExceptionInInitializerError
    [junit] 	at io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:33)
    [junit] 	at org.apache.cassandra.service.NativeTransportService.useEpoll(NativeTransportService.java:162)
    [junit] 	at org.apache.cassandra.net.async.NettyFactory.<clinit>(NettyFactory.java:94)
    [junit] 	at org.apache.cassandra.net.async.NettyFactoryTest.getEventLoopGroup_Nio(NettyFactoryTest.java:216)
    [junit] 	at org.apache.cassandra.net.async.NettyFactoryTest.getEventLoopGroup_NioWithoutIoRatioBoost(NettyFactoryTest.java:211)
    [junit] Caused by: java.lang.IllegalStateException: Only supported on Linux
    [junit] 	at io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:189)
    [junit] 	at io.netty.channel.epoll.Native.<clinit>(Native.java:61)
{code}

It's obviously caused by epoll being unavailable on MacOS, but the tests should still be made to pass.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-31 12:29:59.47,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 07 18:04:44 UTC 2017,,,,,,0|i3jhzz:,9223372036854775807,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"31/Aug/17 12:29;jasobrown;pushed a fix for the test:

||13831||
|[branch|https://github.com/jasobrown/cassandra/tree/13831]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13831]|
","04/Sep/17 15:39;iamaleksey;This makes the tests pass but still shits all over the logs.

{code}
    [junit] WARN  [main] 2017-09-04 16:33:51,193 NettyFactory.java:98 - epoll not availble {}
    [junit] java.lang.ExceptionInInitializerError: null
    [junit] 	at io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:33) ~[netty-all-4.1.14.Final.jar:4.1.14.Final]
    [junit] 	at org.apache.cassandra.service.NativeTransportService.useEpoll(NativeTransportService.java:162) ~[main/:na]
    [junit] 	at org.apache.cassandra.net.async.NettyFactoryTest.<clinit>(NettyFactoryTest.java:65) ~[classes/:na]
    [junit] 	at java.lang.Class.forName0(Native Method) ~[na:1.8.0_144]
    [junit] 	at java.lang.Class.forName(Class.java:264) ~[na:1.8.0_144]
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:380) ~[ant-junit.jar:na]
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1182) ~[ant-junit.jar:na]
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1033) ~[ant-junit.jar:na]
    [junit] Caused by: java.lang.IllegalStateException: Only supported on Linux
    [junit] 	at io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:189) ~[netty-all-4.1.14.Final.jar:4.1.14.Final]
    [junit] 	at io.netty.channel.epoll.Native.<clinit>(Native.java:61) ~[netty-all-4.1.14.Final.jar:4.1.14.Final]
    [junit] 	... 8 common frames omitted
{code}

Could you maybe modify {{NativeTransportService.useEpoll()}} to look at the OS first before trying to call {{Epoll.isAvailable()}}, or fix it some other way?","05/Sep/17 11:59;jasobrown;Printing the error is useful on linux to know why epoll might not be available, but I agree it's not necessary on other platforms. Fixing.","05/Sep/17 12:07;jasobrown;rebased and pushed a change that adds an additional check to see if the OS is linux, and only then log that epoll is unavailable.

UPDATE: just saw your comment about {{NativeTransportService}}. Will move the logging there, from {{NettyFactory}}","07/Sep/17 12:43;iamaleksey;kk, just mark as Patch Available when ready.","07/Sep/17 14:01;jasobrown;ahh, sorry, already pushed that change about 10 minutes after the last UPDATE tag. Marking Patch Available now.","07/Sep/17 17:21;iamaleksey;+1. On commit, can you please fix your recent CHANGES.txt entries in the log to start with upper case, like every other line in the list?",07/Sep/17 18:04;jasobrown;committed as sha {{bab76eba958241bbfca33e9fc6fe663ce230b034}} to trunk. And I've cleaned up the CHANGES entrie3s as per [~iamaleksey]'s request.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hanging threads in BulkLoader,CASSANDRA-13837,13099226,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,01/Sep/17 13:50,12/Mar/19 14:06,13/Mar/19 22:35,08/Sep/17 10:32,4.0,,,,,,,,,,0,,,,[~krummas] discovered some threads that were not closing correctly when he fixed CASSANDRA-13836. We suspect this is due to CASSANDRA-8457/CASSANDRA-12229.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13836,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-05 15:45:50.51,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 08 10:32:42 UTC 2017,,,,,,0|i3jjvb:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"05/Sep/17 15:45;mshuler;This appears to be hanging up the trunk dtest jobs in random tests. We've been looking for a single dtest that might be problematic, but there doesn't seem to be a clear pattern on a particular test.

Any way we can bump the priority on this from Minor?",05/Sep/17 16:25;jkni;I pointed [~mshuler] here prematurely - my bad. It looks like the System.exit uncommented in [CASSANDRA-13836] resolves the hang we were seeing on that test. The remaining hangs seem to be repair-related and unrelated to this ticket.,"07/Sep/17 20:56;jasobrown;Patch here:

||13837||
|[branch|https://github.com/jasobrown/cassandra/tree/13837]|
|[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/293/]|
|[utests|https://circleci.com/gh/jasobrown/cassandra/tree/13837]|

Basically, we just need to make the netty {{EventLoopGroup}} threads daemon threads",07/Sep/17 21:09;jasobrown;[~krummas] do you mind reviewing?,"08/Sep/17 07:52;krummas;LGTM, +1 (dtests didn't really run though, but sstableloader exits fine manually)

I just noticed [this|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/streaming/StreamResultFuture.java#L215] - I doubt that should be WARN? Feel free to remove or change to INFO on commit","08/Sep/17 10:32;jasobrown;committed as sha {{6d4e056c923b7367fc4199ae73dedea893d8f6b8}}. Did a follow up ninja-commit ({{6c0e6693f44753fde87fdec36c648aa0ff02097c}}) to remove that log line [~krummas] pointed out because I wasn't smart enough to remove it on the original commit.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactionManager sometimes wrongly determines that a background compaction is running for a particular table,CASSANDRA-13801,13097575,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dimitarndimitrov,dimitarndimitrov,dimitarndimitrov,25/Aug/17 11:51,12/Mar/19 14:06,13/Mar/19 22:35,12/Dec/17 19:53,2.2.12,3.0.16,3.11.2,4.0,,Local/Compaction,,,,,0,,,,"Sometimes after writing different rows to a table, then doing a blocking flush, if you alter the compaction strategy, then run background compaction and wait for it to finish, {{CompactionManager}} may decide that there's an ongoing compaction for that same table.
This may happen even though logs don't indicate that to be the case (compaction may still be running for system_schema tables).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Dec/17 19:32;dimitarndimitrov;c13801-2.2-testall.png;https://issues.apache.org/jira/secure/attachment/12900929/c13801-2.2-testall.png,06/Dec/17 19:32;dimitarndimitrov;c13801-3.0-testall.png;https://issues.apache.org/jira/secure/attachment/12900928/c13801-3.0-testall.png,06/Dec/17 19:32;dimitarndimitrov;c13801-3.11-testall.png;https://issues.apache.org/jira/secure/attachment/12900927/c13801-3.11-testall.png,06/Dec/17 19:32;dimitarndimitrov;c13801-trunk-testall.png;https://issues.apache.org/jira/secure/attachment/12900926/c13801-trunk-testall.png,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2017-12-12 19:53:43.406,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 12 19:53:43 UTC 2017,,,,,,0|i3ja9b:,9223372036854775807,3.11.0,,,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,"06/Dec/17 19:31;dimitarndimitrov;It turns out that the problem does not necessarily require altering the compaction strategy.
It seems to be rooted in a potential problem with counting the CF compaction requests, that can eventually lead to a skipped background compaction.

The wrong counting can happen if the counting multiset increment [here|https://github.com/apache/cassandra/blob/95b43b195e4074533100f863344c182a118a8b6c/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L197] gets delayed and happens after the corresponding counting multiset decrement already happened [here|https://github.com/apache/cassandra/blob/95b43b195e4074533100f863344c182a118a8b6c/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L284].

Here are the branches with the proposed changes, as well as a Byteman test that can be used to demonstrate the issue.
testall results look good (3.0 and trunk each have 1 seemingly unrelated, flaky test failing).
dtest results will be added soon.

| [2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...dimitarndimitrov:c13801-2.2] | [testall|^c13801-2.2-testall.png] |
| [3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...dimitarndimitrov:c13801-3.0] | [testall|^c13801-3.0-testall.png] |
| [3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...dimitarndimitrov:c13801-3.11] | [testall|^c13801-3.11-testall.png] |
| [trunk|https://github.com/apache/cassandra/compare/trunk...dimitarndimitrov:c13801-trunk] | [testall|^c13801-trunk-testall.png] |
","12/Dec/17 19:53;pauloricardomg;Good catch, patch and tests LGTM! I looked the results of the dtests on internal CI and they look good (unrelated failures), can you just attach the screen shots so they are public?

I will commit this to 2.2 even though it is in critical fixes mode because it's pretty simple/safe and the only way to solve it is to restart the node if it ever gets into this nasty state.

I added a comment on [CompactionManager.submitBackground|https://github.com/apache/cassandra/commit/35e6d61361e699908d73c277da7d9ac3390f6e5d#diff-d4e3b82e9bebfd2cb466b4a30af07fa4R159] extracted from CASSANDRA-4310 to explain the reasoning behind keeping track of compacting cfs.

Committed to 2.2 as {{35e6d61361e699908d73c277da7d9ac3390f6e5d}} and merged up to cassandra-3.0, cassandra-3.11 and trunk. Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validation compactions can cause orphan sstable warnings,CASSANDRA-13786,13096866,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,22/Aug/17 23:00,12/Mar/19 14:06,13/Mar/19 22:35,12/Sep/17 20:34,4.0,,,,,,,,,,0,,,,"I've seen LevelledCompactionStrategy occasionally logging: {quote}<sstable_name> from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.""{quote} warnings from a ValidationExecutor thread.

What's happening here is that a compaction running concurrently with the validation is promoting (or demoting) sstables as part of an incremental repair, and an sstable has changed hands by the time the validation compaction gets around to getting scanners for it. The sstable isolation/synchronization done by validation compactions is a lot looser than normal compactions, so seeing this happen isn't very surprising. Given that it's harmless, and not unexpected, I think it would be best to not log these during validation compactions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-22 23:13:17.936,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 12 20:34:04 UTC 2017,,,,,,0|i3j5xb:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"22/Aug/17 23:01;bdeggleston;I have a fix pushed [here|https://github.com/bdeggleston/cassandra/commits/orphan-sstables], but I don't like that it's changing the compaction strategy interface, anyone have any ideas on alternate solutions?

/cc [~krummas] [~jjirsa]
","22/Aug/17 23:13;jjirsa;Not a review and haven't read the code, but you're just targeting 4.0? Seems like if we're going to break an interface, that's probably the time to do it.
","23/Aug/17 00:49;bdeggleston;Yeah for sure. It just seems like changing the interface is an oversize solution to the minor annoyance of getting periodic warnings in the logs. Maybe we don't need the log entry anymore, or it can be demoted to info, or there's a solution I haven't though of. That said, there probably aren't many 3rd party compaction strategies written against vanilla Cassandra these days.",23/Aug/17 06:39;krummas;Lets just drop it to DEBUG? It is pretty unclear what a user needs to do to avoid/fix it but keeping it around to make debugging easier might make sense?,"24/Aug/17 20:10;bdeggleston;[~kohlisankalp] and I were talking about this, and he mentioned that even if the operator can't do anything to fix, it's still good to know it's happening. I coded up a more ""correct"" solution. [This branch|https://github.com/bdeggleston/cassandra/tree/orphan-sstables2] adjusts how mutating repairedAt is synchronized, and how getScanners is synchronized. So that the scenario causing the warning in this case shouldn't be able to happen.

1. for {{getScanners}}, it moves the organizing of sstables into unrepaired/pending/repaired into the same read locked block that actually gets the scanners
2. adds a {{mutateRepaired}} method to {{CompactionStrategyManager}} that is used to mutate the repaired time and move the sstables between compaction strategies with a write lock held.","28/Aug/17 13:17;krummas;makes sense, added 2 comments on the gh branch","29/Aug/17 21:47;bdeggleston;Fixed

[trunk|https://github.com/bdeggleston/cassandra/tree/orphan-sstables2]
[utest|https://circleci.com/gh/bdeggleston/cassandra/tree/orphan-sstables2]
[dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/230/]",30/Aug/17 07:45;krummas;restarted dtests: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/236/,"31/Aug/17 09:09;krummas;code lgtm, but dtest runs seem to die, restarted again: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/256",11/Sep/17 23:00;bdeggleston;rebased on latest trunk: [dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/297/],"12/Sep/17 20:34;bdeggleston;Got a clean utest run locally, dtest failures were also all failing on trunk.
Committed as {{7d4d1a32581ff40ed1049833631832054bcf2316}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dropping a table doesn't drop its dropped columns,CASSANDRA-13730,13089965,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasonstack,duarten,duarten,25/Jul/17 18:05,12/Mar/19 14:06,13/Mar/19 22:35,03/Aug/17 13:43,3.0.15,3.11.1,,,,Legacy/Distributed Metadata,,,,,0,,,,"I'm not sure if this is intended or not, but currently a table's dropped columns are not dropped when the table itself is dropped:

{noformat}
cqlsh> create keyspace ks WITH replication={ 'class' : 'SimpleStrategy', 'replication_factor' : 1 } ;
cqlsh> use ks;
cqlsh:ks> create table  test (pk text primary key, c1 int);
cqlsh:ks> alter table test drop c1;
cqlsh:ks> drop table test;
cqlsh:ks> select * from system_schema.dropped_columns where keyspace_name = 'ks' and table_name = 'test';

 keyspace_name | table_name | column_name | dropped_time                    | kind    | type
---------------+------------+-------------+---------------------------------+---------+------
            ks |       test |          c1 | 2017-07-25 17:53:47.651000+0000 | regular |  int

(1 rows)
{noformat}

This can have surprising consequences when creating another table with the same name.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-25 20:11:35.481,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 03 13:30:45 UTC 2017,,,,,,0|i3hzun:,9223372036854775807,3.11.0,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"25/Jul/17 20:11;iamaleksey;It's most certainly unintended, and I can confirm that the necessary code is missing from {{SchemaKeyspace.makeDropTableMutation()}} method.","25/Jul/17 20:17;iamaleksey;It's also really old, going all the way to 2.0. Should fix in 2.1 and upwards.",25/Jul/17 20:30;duarten;Weren't the dropped columns just a map before 3.0?,"25/Jul/17 20:35;iamaleksey;bq. Weren't the dropped columns just a map before 3.0?

Totally was. My memory is apparently garbage. Thanks.","26/Jul/17 00:54;iamaleksey;The reason nobody noticed before is that it shouldn't affect you, normally. Assuming the clock working even remotely correctly, timestamps of new columns in the new table will all be higher than dropped column times in the deleted table.

You'd be affected if you set timestamps manually to values < time of new table creation.

You'll also be affected by being unable to re-add certain columns to schema - there are some validations in place forbidding you to re-add columns with types incompatible with dropped columns' types (for collections in particular).

It's still a bug though and should be fixed, but is a minor one.

Unassigning myself as it's less impactful as I initially assumed. If nobody else picks it up, I will eventually fix it, once I don't have more important things on my plate.

Feel free to take over. And thanks for spotting it in the first place (:","30/Jul/17 03:15;jasonstack;This might affect backup/restore.. 

1. backup
2. drop column
3. drop table
4. restore schema
5. restore sstable
6. data missing for dropped column","30/Jul/17 08:43;stoneFang;[~jasonstack]
why restore process need to  check system_schema.dropped_columns?
should put system_schema.dropped_table first","31/Jul/17 02:44;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13730-trunk] | [unit|https://circleci.com/gh/jasonstack/cassandra/295] | irrelevant
 compaction_test.TestCompaction_with_LeveledCompactionStrategy.fanout_size_test
 materialized_views_test.TestMaterializedViews.view_tombstone_test (known)
 bootstrap_test.TestBootstrap.consistent_range_movement_false_with_rf1_should_succeed_test(known) |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13730-3.11] | [unit|https://circleci.com/gh/jasonstack/cassandra/296] | passed dtest |
| [3.0|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13730-3.0] | [unit|https://circleci.com/gh/jasonstack/cassandra/287] | irrelevant
 offline_tools_test.TestOfflineTools.sstableofflinerelevel_test
auth_test.TestAuth.system_auth_ks_is_alterable_test |
| [dtest|https://github.com/riptano/cassandra-dtest/commits/CASSANDRA-13730] | 

clear corresponding dropped_columns entries when table is dropped.","02/Aug/17 17:09;iamaleksey;[~jasonstack] Looking good, only have one small nit: {{columnName.toString()}} call is redundant, as {{columnName}} is already a string. Should remove that call. Also, my (subjective) preference would be to pass {{DroppedColumn}} and not {{String}} as {{columnName}} to {{dropDroppedColumnFromSchemaMutation()}} method, for consistency with other similar methods in the class.

Unit test and dtest look good to me.",03/Aug/17 04:47;jasonstack;thanks for reviewing. updated 3.0/3.1 branch.,"03/Aug/17 10:10;iamaleksey;Thanks! LGTM, will commit soonish.","03/Aug/17 13:30;iamaleksey;Committed to 3.0 as [d9eabd3d0cbf1287aa7d01bc23dd8e39c3acf232|https://github.com/apache/cassandra/commit/d9eabd3d0cbf1287aa7d01bc23dd8e39c3acf232] and merged into 3.11 and trunk, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Orphan hint file gets created while node is being removed from cluster,CASSANDRA-13740,13092315,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,03/Aug/17 22:04,12/Mar/19 14:06,13/Mar/19 22:35,30/Apr/18 18:06,3.0.17,3.11.3,4.0,,,Consistency/Hints,Legacy/Core,,,,0,,,,"I have found this new issue during my test, whenever node is being removed then hint file for that node gets written and stays inside the hint directory forever. I debugged the code and found that it is due to the race condition between [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L195] and [HintsWriteExecutor.java::closeWriter | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L106]
. 
 
*Time t1* Node is down, as a result Hints are being written by [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L195]
*Time t2* Node is removed from cluster as a result it calls [HintsService.java-exciseStore | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L327] which removes hint files for the node being removed
*Time t3* Mutation stage keeps pumping Hints through [HintService.java::write | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L145] which again calls [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L215] and new orphan file gets created

I was writing a new dtest for {CASSANDRA-13562, CASSANDRA-13308} and that helped me reproduce this new bug. I will submit patch for this new dtest later.

I also tried following to check how this orphan hint file responds:
1. I tried {{nodetool truncatehints <node>}} but it fails as node is no longer part of the ring
2. I then tried {{nodetool truncatehints}}, that still doesn’t remove hint file because it is not yet included in the [dispatchDequeue | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsStore.java#L53]


Reproducible steps:
Please find dTest python file {{gossip_hang_test.py}} attached which reproduces this bug.

Solution:
This is due to race condition as mentioned above. Since {{HintsWriteExecutor.java}} creates thread pool with only 1 worker, so solution becomes little simple. Whenever we [HintService.java::excise | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L303] a host, just store it in-memory, and check for already evicted host inside [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L215]. If already evicted host is found then ignore hints.

Jaydeep",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Aug/17 22:01;chovatia.jaydeep@gmail.com;13740-3.0.15.txt;https://issues.apache.org/jira/secure/attachment/12884139/13740-3.0.15.txt,03/Aug/17 22:03;chovatia.jaydeep@gmail.com;gossip_hang_test.py;https://issues.apache.org/jira/secure/attachment/12880316/gossip_hang_test.py,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-08-04 19:30:38.572,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 30 18:08:50 UTC 2018,,,,,,0|i3ie9z:,9223372036854775807,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,03/Aug/17 22:37;chovatia.jaydeep@gmail.com;Please find patch with this fix attached.,"04/Aug/17 19:30;jay.zhuang;Hi [~chovatia.jaydeep@gmail.com] nice catch. One comment about your patch, I don't think it's a good idea to have {{static final Set<UUID> evictedHostIds}} set in {{HintsStore}}, as the HintsStore instance is for one hints file. How about adding a field {{private boolean isEvicted = false;}}? To indicate if the target host is evicted or not.","04/Aug/17 22:11;chovatia.jaydeep@gmail.com;Thanks [~jay.zhuang] for review comments.

{{HintsStore}} object may get created multiple times for same {{hostId}} as following:

*time t1* removenode thread calls [catalog.exciseStore | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L327 ]
and it removes from [Map<UUID, HintsStore> stores | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsCatalog.java#L41]
*time t2* HintWriter thread calls [HintsStore::get | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsCatalog.java#L85] which creates a new {{HintStore}} object and will write hints again.

For this bug specifically above mentioned scenario is been happening, please let me know your comments.

I think we should atleast move {{static final Set<UUID> evictedHostIds}} from {{HintStore.java}} to {{HintService.java}} which is a singleton?","05/Aug/17 00:13;chovatia.jaydeep@gmail.com;Hi [~jay.zhuang] Please find patch file ""13740-2_3.0.15.txt"" attached with your review comments incorporated.","08/Aug/17 22:10;githubbot;GitHub user jaydeepkumar1984 opened a pull request:

    https://github.com/apache/cassandra/pull/136

    CASSANDRA-13740 orphan hint file get created

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jaydeepkumar1984/cassandra 13740-3.0

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/136.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #136
    
----
commit 993d5891e69f5cda402ae2158e06914f653a644d
Author: Jaydeepkumar Chovatia <chovatia.jaydeep@gmail.com>
Date:   2017-08-03T22:34:26Z

    CASSANDRA-13740 orphan hint file get created

----
",08/Aug/17 22:19;chovatia.jaydeep@gmail.com;Please find pull request against Cassandra-3.0 here: https://github.com/apache/cassandra/pull/136/commits/993d5891e69f5cda402ae2158e06914f653a644d,"09/Aug/17 12:24;iamaleksey;The patch likely works, but I think we can do better.

Some of the issues I have with it:
1. It introduces a dependency on {{HintsService}} and {{StorageService}} to {{HintsWriteExecutor}}
2. It introduces a dependency on {{HintsService}} to {{HintsStore}}

When designing the current iteration of hints I was very careful to design the system in a top-down way without any interleaving that’s avoidable. Each class is a dumb as possible on its own, and as you go up, you just compose dumb classes that by themselves know nothing of layers above them.

As for the problem itself, we do acknowledge that “The worst that can happen if we don't get everything right is a hints file (or two) remaining undeleted.” - comments to {{excise()}}, it’s more of a known limitation than a bug. But of course we can improve on it. What is a problem, however, is the inability to programmatically remove those orphan files via JMX. {{nodetool truncatehints}} should get results no matter what, and it should be fixed.

If we want to deal with the orphans for sure - and I don’t see why not improve this as well - I suggest you do so in a different way. Perhaps as last step of {{excise()}} schedule an task - on {{ScheduledExecutors.optionalTasks}} to clean up any orphans, if any, after some delay.",09/Aug/17 21:56;chovatia.jaydeep@gmail.com;Thanks [~iamaleksey] for the code review. I will change it as per your suggestion and will provide updated patch.,"11/Aug/17 23:17;chovatia.jaydeep@gmail.com;Hi [~iamaleksey]

I have modified code as per your review comments, please find it attached ""13740-3.0.15.txt""
Also please find same patch here: https://github.com/jaydeepkumar1984/cassandra/commit/173fce0362246595d26b24196d6690223d132d5e

I will create patch for 3.11 as well as will run {{circleci}} after receiving your review comments.

Jaydeep","21/Aug/17 21:01;iamaleksey;Hey. There are a few [code style|https://wiki.apache.org/cassandra/CodeStyle] issues: we don't use {{final}} for arguments and local variables, brackets go to new lines always. And the patch doesn't wait for {{closeWriter}} future to be completed.

And a more interesting issue is that of the delay. {{RING_DELAY}} doesn't have anything to do with hints. What does is write timeouts, and {{MessagingService}} 's timeout reporter the callbacks expiring map firing - that's where the race ultimately is.

Also, we aren't fixing the issue of {{nodetool truncatehints}} not being able to clean up after we excise.

The more I think about it, the more I'm inclined to just correct that last issue and leave everything else be as is (and also commit your unit tests, thanks for those).","24/Aug/17 16:29;chovatia.jaydeep@gmail.com;Thanks for the review comments, I will change it and send updated one soon.","28/Aug/17 22:00;chovatia.jaydeep@gmail.com;Hi [~tuxslayer]

Isn’t the race condition is as following:

*Thread T1 - Time 1:* Removes {{HintStore}} by calling [HintsService.instance.excise |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L2268] but
at this time node has not yet been removed from [tokenMetadata  |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L133]

*Thread T2 - Time 2:* Mutation stage does a lookup to [tokenMetadata  | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageProxy.java#L2617] and finds node as valid hence it dumps hint for it.

*Thread T1 - Time 3:* Now removes node from [tokenMetadata  | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L2270] 

Hence I decided it is safer to sleep for {{StorageService.RING_DELAY}} and then schedule optional {{removeOrphanHintFiles}}  task. Please let me know your comments.

I have also incorporated your review comment, please find updated patch attached as well as here: https://github.com/jaydeepkumar1984/cassandra/commit/16d4ab3316ab71a9a3b96ab67944384092be40ca

Jaydeep","11/Sep/17 21:36;chovatia.jaydeep@gmail.com;Hi [~iamaleksey]

Can you please review my latest patch?

Jaydeep","13/Sep/17 16:57;iamaleksey;A bit busy currently, sorry. Will have a look as soon as I can.","06/Mar/18 05:47;chovatia.jaydeep@gmail.com;Hi [~iamaleksey]

 A gentle ping. If you have some time then can you please help me close this?

 Jaydeep","15/Mar/18 19:19;iamaleksey;Sure. So what the current patch is doing is it does excise, and then, in essence, schedules another excise in {{RING_DELAY}}?

In other words, we simply don't trust the immediate excise, and rely on the follow-up one. In that case, to make sure that hints for writes that will time out at some later point get deleted from disk, shouldn't we just say ""alright, let's just delay the first excise, instead of doing the immediate one and then another one""?","15/Mar/18 19:26;iamaleksey;And the changes to {{deleteAllHints()}} I don't fully understand. I don't think it's the responsibility of the method to close any writers. The contract is (as I understand it) - please remove all written hints files at this point. One problem is that for catalogues that aren't loaded, if some files are remaining, they won't be deleted - but this change doesn't address it. More importantly, I think that with excise fixed, that would be less of a problem and probably not needed to fix..

So let's leave {{deleteAllHints()}} alone. And also leave excise more or less alone, but call it instead at the end of {{StorageProxy.excise()}}, with a delay. Not sure why you picked {{RING_DELAY}} for the delay though.. can you explain? ","16/Mar/18 17:40;chovatia.jaydeep@gmail.com;Thanks [~iamaleksey] for the review.

Reason behind {{RING_DELAY}} is as following, in this fix one thing is clear that we have to delay {{StorageProxy.excise()}} which means we have to put some sleep. So we have two options to put sleep:
 1. Hardcode some random value say for example delay {{StorageProxy.excise()}} for 10 seconds
 OR
 2. Other nodes in the ring will no longer accept writes once they learn that given node is no longer part of the ring. Hence I have used {{RING_DELAY}} which is general delay used at [many places|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/gms/Gossiper.java#L553] and after this delay we can assume ring has stabilized. So my theory is that once ring has stabilized then everyone in the ring would have learnt about node that just left and at this time it is safe to do {{StorageProxy.excise(). }}Please let me know if my understanding is not correct, I can change it to some hardcoded value say 20 seconds.

I will incorporate other code review comments and will send you updated patch soon.","19/Mar/18 22:57;chovatia.jaydeep@gmail.com;Hi [~iamaleksey]

Please find updated patch here:
||trunk||3.0||
|[patch|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:13740-trunk?expand=1]|[patch|https://github.com/apache/cassandra/compare/cassandra-3.0...jaydeepkumar1984:CASSANDRA-13740_1?expand=1]|
|[utest|https://circleci.com/gh/jaydeepkumar1984/cassandra/46]|[utest|https://circleci.com/gh/jaydeepkumar1984/cassandra/44]|

Jaydeep","30/Apr/18 18:05;iamaleksey;Made some changes and committed to 3.0 as [b2f6ce961f38a3e4cd744e102026bf7a471056c9|https://github.com/apache/cassandra/commit/b2f6ce961f38a3e4cd744e102026bf7a471056c9] and merged upwards.

Changes made:
- fixed {{excise()}} to properly handle non-existing stores instead of re-initializing them
- changed the delay to be min rpc timeout + write rpc timeout, which roughly the time you may expect a new hint written after node decom

Thank you for you patience and for the added tests.",30/Apr/18 18:08;chovatia.jaydeep@gmail.com;Thank You!!! [~iamaleksey],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nodetool compact and flush fail with ""error: null""",CASSANDRA-13897,13104479,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,Stefania,corydoras,corydoras,23/Sep/17 07:57,12/Mar/19 14:06,13/Mar/19 22:35,23/Oct/17 02:05,3.11.2,,,,,Legacy/Local Write-Read Paths,,,,,1,,,,"{{nodetool flush}} and {{nodetool compact}} return an error message that is not clear. This could probably be improved. Both of my two nodes return this error.

{{nodetool flush}} Will return this error the first 2-3 times you invoke it, then the error temporarily disappears. {{nodetool compress}} always returns this error message no matter how many times you invoke it.

I have tried deleting saved_caches, commit logs, doing nodetool compact/rebuild/scrub, and nothing seems to remove the error. 

{noformat}
cass@s5:~/apache-cassandra-3.11.0$ nodetool compact
error: null
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.cache.ChunkCache$CachingRebufferer.<init>(ChunkCache.java:222)
	at org.apache.cassandra.cache.ChunkCache.wrap(ChunkCache.java:175)
	at org.apache.cassandra.io.util.FileHandle$Builder.maybeCached(FileHandle.java:412)
	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:381)
	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:331)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinal(BigTableWriter.java:333)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinalEarly(BigTableWriter.java:318)
	at org.apache.cassandra.io.sstable.SSTableRewriter.switchWriter(SSTableRewriter.java:322)
	at org.apache.cassandra.io.sstable.SSTableRewriter.doPrepare(SSTableRewriter.java:370)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.doPrepare(CompactionAwareWriter.java:111)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish(Transactional.java:184)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.finish(CompactionAwareWriter.java:121)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:220)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:733)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
	at java.lang.Thread.run(Thread.java:748)

{noformat}
","* Apache Cassandra 3.11.0
* Linux 4.4.0-92-generic #115-Ubuntu SMP Thu Aug 10 09:04:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
* jdk1.8.0_144",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-10-09 03:46:07.715,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 23 02:05:22 UTC 2017,,,,,,0|i3kg4n:,9223372036854775807,3.11.0,3.11.1,,,,,,snazy,snazy,,,3.11.0,,,,,,,"09/Oct/17 03:46;jjirsa;Second report of this on 3.11.0 on the user list

https://lists.apache.org/thread.html/37730e41d91110710935e82eb06a9a7c6d720f21443351d416194f89@%3Cuser.cassandra.apache.org%3E","16/Oct/17 03:22;corydoras;I can confirm upgrading to 3.11.1 does not resolve the problem. Also, the error is occurring with `nodetool compact` and I think it is not successfully completing the compaction in some or all cases.

Every time I restart a node I have to wipe the commit logs to get the node to boot, and when I do this data sometimes disappears and deleted data sometimes reappears","16/Oct/17 03:36;corydoras;Here is my config (git diff -U0) against default 3.11.1 configuration


{code:java}
+++ b/cassandra-topology.properties
@@ -18,16 +18,2 @@
-192.168.1.100=DC1:RAC1
-192.168.2.200=DC2:RAC2
-
-10.0.0.10=DC1:RAC1
-10.0.0.11=DC1:RAC1
-10.0.0.12=DC1:RAC2
-
-10.20.114.10=DC2:RAC1
-10.20.114.11=DC2:RAC1
-
-10.21.119.13=DC3:RAC1
-10.21.119.10=DC3:RAC1
-
-10.0.0.13=DC1:RAC2
-10.21.119.14=DC3:RAC2
-10.20.114.15=DC2:RAC2
+10.1.1.125=dc1:rack1
+10.1.1.241=dc2:rack1
@@ -36 +22 @@
-default=DC1:r1
+default=dc1:rack1
@@ -41 +27 @@ default=DC1:r1
-fe80\:0\:0\:0\:202\:b3ff\:fe1e\:8329=DC1:RAC3
+#fe80\:0\:0\:0\:202\:b3ff\:fe1e\:8329=DC1:RAC3


{code}

And 


{code:java}
diff --git a/cassandra.yaml b/cassandra.yaml
index e847e54..cd30717 100644
--- a/cassandra.yaml
+++ b/cassandra.yaml
@@ -10 +10 @@
-cluster_name: 'Test Cluster'
+cluster_name: 'mycluster'
-          - seeds: ""127.0.0.1""
+          - seeds: ""10.1.1.125""
-listen_address: localhost
+listen_address: 10.1.1.125
-rpc_address: localhost
+rpc_address: 10.1.1.125
-# broadcast_rpc_address: 1.2.3.4
+broadcast_rpc_address: 10.1.1.125
-endpoint_snitch: SimpleSnitch
+endpoint_snitch: GossipingPropertyFileSnitch
{code}
","18/Oct/17 01:52;Stefania;The assertion that fails is {{assert Integer.bitCount(chunkSize) == 1;}}, the caching rebufferer is requiring a power of two for its chunks. However chunk sizes derive from buffer sizes, which are multiples of page cache sizes (4096) and, for example, 4096 * 3 is not a power of two. We should prepare a patch. Meanwhile, as a workaround, you can use mmap to read data, which will bypass the chunk cache, by setting {{disk_access_mode: mmap}} in the yaml.","18/Oct/17 02:46;corydoras;Thanks, that was the pointer in the right direction to get me over the line on fixing it for my install. I left that setting at {{auto}}, but updated which version of Java I am using. Apparently I was using a 32bit version, so switching to a 64 bit version apparently enabled mmap.

I switched from:


{code:java}
cass:~$ java -version
java version ""1.8.0_144""
Java(TM) SE Runtime Environment (build 1.8.0_144-b01)
Java HotSpot(TM) Server VM (build 25.144-b01, mixed mode)
{code}

To


{code:java}
cass:~$ java -version
java version ""1.8.0_151""
Java(TM) SE Runtime Environment (build 1.8.0_151-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode)

{code}
","18/Oct/17 02:54;Stefania;bq. so switching to a 64 bit version apparently enabled mmap.

That's correct. {{disk_access_mode: auto}} defaults to standard on 32-bit and mmap on 64-bit, from {{DatabaseDescriptor}}:

{code}
        if (conf.disk_access_mode == Config.DiskAccessMode.auto)
        {
            conf.disk_access_mode = hasLargeAddressSpace() ? Config.DiskAccessMode.mmap : Config.DiskAccessMode.standard;
            indexAccessMode = conf.disk_access_mode;
            logger.info(""DiskAccessMode 'auto' determined to be {}, indexAccessMode is {}"", conf.disk_access_mode, indexAccessMode);
        }
{code}
","18/Oct/17 07:55;Stefania;Patch for 3.11 is available [here|https://github.com/stef1927/cassandra/tree/13897-3.11]. It rounds up to the next power of two since we cap at 64 KB. 

CircleCI was having github access problems, so I've launched the CI jobs on our internal servers.
","18/Oct/17 08:44;Stefania;[~blambov], can the {{CachingRebufferer}} be changed to only require a multiple of 4096 rather than a power of two? [~snazy] and myself are worried that neither rounding up nor rounding down to a power of two feels too right as in both cases we may end up being quite far from the initial value derived from the data and the disk optimization strategy. As far as I have understood, the rebufferer requires a power of two to calculate the page position, can it not align to 4096 multiples instead?","18/Oct/17 08:56;blambov;The cache will also take buffers from the buffer pool of that size. If the size does not divide 64k, space will be wasted in each buffer, and this could cause buffer management problems. If we don't change the pool to gracefully handle this, I think it's safer to just round the size.","19/Oct/17 02:00;Stefania;Thanks for the explanation [~blambov], it looks like we should continue rounding to powers of two for the chunk cache. 

I've extended the [patch|https://github.com/stef1927/cassandra/tree/13897-3.11] to either round up or down [~snazy]. This can be configured via a yaml property that it only documented in config, not in the actual yaml file. This property is called {{file_cache_round_up}} and by default it will be false when the disk optimization strategy is ssd, therefore rounding down, and it will be true when it is set to spinning, therefore rounding up only for spinning disks.
","20/Oct/17 15:48;snazy;+1 (assuming CI looks good)
One nit: make the new Config field a primitive instead of a boxed one.
","23/Oct/17 02:05;Stefania;Thanks for the review!

Committed to 3.11 as {{5b23054f10f4d6553e8dacbf53bd59e552f2a031}} and merged into trunk.

bq. make the new Config field a primitive instead of a boxed one.

It cannot be a primitive because we need {{== null}} to determine if the user set this value or not.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fromJson(null) throws java.lang.NullPointerException on Cassandra,CASSANDRA-13891,13104006,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,eribeiro,marcel@lisemarie.com,marcel@lisemarie.com,21/Sep/17 16:15,12/Mar/19 14:06,13/Mar/19 22:35,05/Apr/18 13:24,2.2.13,3.0.17,3.11.3,4.0,,Legacy/CQL,,,,,0,,,,"Basically, {{fromJson}} throws a {{java.lang.NullPointerException}} when NULL is passed, instead of just returning a NULL itself. Say I create a UDT and a table as follows:
{code:java}
create type type1
(
id int,
name text
);

create table table1
(
id int,
t FROZEN<type1>,

primary key (id)
);{code}
And then try and insert a row as such:

{{insert into table1 (id, t) VALUES (1, fromJson(null));}}

I get the error: {{java.lang.NullPointerException}}

This works as expected: {{insert into table1 (id, t) VALUES (1, null);}}

Programmatically, one does not always know when a UDT will be null, hence me expecting {{fromJson}} to just return NULL.",Cassandra 3.11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Jan/18 21:05;eribeiro;CASSANDRA-13891.patch;https://issues.apache.org/jira/secure/attachment/12907184/CASSANDRA-13891.patch,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-01-22 21:16:06.491,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 05 13:24:00 UTC 2018,,,,,,0|i3kd7z:,9223372036854775807,3.11.0,3.9,,,,,,jasobrown,jasobrown,,,,,,,,,,"22/Jan/18 21:16;eribeiro;Hi [~marcel@lisemarie.com], and [~jdarcy], I am still finding my way again after a long period of absence from Cassandra dev, so I would like to see if I am missing something here. Thanks!","23/Jan/18 22:16;jjirsa;Setting assignee and patch-availab.e

 ",23/Jan/18 23:44;eribeiro;Thanks! :),"24/Jan/18 16:53;blerer;Thanks for the patch, it looks good to me.
Could you run CI on a 2.2 patched branch?
If the tests run fine I am +1. ","26/Jan/18 22:05;eribeiro;Hi [~blerer],

Thanks for taking your time to look at this patch.

I have setup a 2.2 patched branch: [https://github.com/eribeiro/cassandra/tree/13891-2.2]

I have never used CircleCI before (the project is using this to build and test C* right?), so I am lost about how to build and run the CI on my 2.2 patched branch.

I have created a CircleCI account via Github authentication, but it looks like it tries to build a only master and even so it spews an error message. Could you point me at links where I can find how to solve those issues?

PS: Running {{JsonTest}} on both cassandra-2.2 and 13891-2.2 branches: it threw some errors I didn't see when the patch was 3.9+

 

Thanks again! ","30/Jan/18 13:51;blerer;{quote}I have never used CircleCI before (the project is using this to build and test C* right?), so I am lost about how to build and run the CI on my 2.2 patched branch.

I have created a CircleCI account via Github authentication, but it looks like it tries to build a only master and even so it spews an error message. Could you point me at links where I can find how to solve those issues?{quote}

Unfortunately, I cannot help you here. I always used our internal CI.","30/Jan/18 17:21;jjirsa;{quote}
I have created a CircleCI account via Github authentication, but it looks like it tries to build a only master and even so it spews an error message. Could you point me at links where I can find how to solve those issues?
{quote}

Once you create a new account, you put the circle yml into the new branch, and on push, circle should see you push to the new branch and build it.

If you make the account before you push, it probably won't try to build the new branch. Just re-push to trigger the build.
","03/Apr/18 12:39;jasobrown;ftr, this error only occurs in 3.11 and trunk. Thus, I've ported [~eribeiro]'s patches to those branches and run tests.

||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/13891-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/13891-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13891-3.11]|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/13891-trunk]|
||

Failing tests are unrelated and were previously unhealthy.

committed as sha {{28bd6c2a031e76b725dd773b949070962555698a}}. Thanks!
","03/Apr/18 16:57;eribeiro;Hey, [~jasobrown], thank you very much. I was going to do this today, but you beat me to it. :) Thanks again and sorry for taking too long to look again at this issue.

PS: fyi, the first incarnation of this patch was adressed to trunk, but I was asked to port it to 2.2 first. 

 

Cheers!",04/Apr/18 13:11;blerer;As [~eribeiro] pointed it out we also need patches for 2.2 and 3.0.,"04/Apr/18 13:49;jasobrown;[~blerer] The bug didn't happen on 2.2 or 3.0 when I tried it.

[~eribeiro] Can you confirm if the bug happens on 2.2 and 3.0? If so, I'll backport the patch.","04/Apr/18 13:53;blerer;[~jasobrown] I did not tested it on 2.2 and 3.0. I just assumed that we had this bug since the begining. Nevertheless, I would prefer if we added the unit tests to all the versions.","04/Apr/18 13:54;eribeiro;Sure! Gonna try to repro on 2.2 and 3.0 for double check, no problem. Thanks again, Jason!","04/Apr/18 13:57;jasobrown;bq. I would prefer if we added the unit tests to all the versions.

This is fair enough. I'll wait for the results of [~eribeiro]'s testing and then do the backporting of the required parts.",04/Apr/18 14:07;blerer;Thanks.,"04/Apr/18 17:24;eribeiro;[~jasobrown], the second test I created below fails with {{{{NullPointerException}}}}, but everything passes after the little fix. FYI, the test that fails on 2.2 and 3.0 is below:


{code:java}
execute(""INSERT INTO %s (k, frozenmapval) VALUES (?, fromJson(?))"", 0, null);
assertRows(execute(""SELECT k, frozenmapval FROM %s WHERE k = ?"", 0), row(0, null));{code}
 

I have open the following PRs targeting 2.2 and 3.0, respectively. I don't have access to a CI so it would be nice if you could run those tests. :)

[https://github.com/apache/cassandra/pull/215] 

[https://github.com/apache/cassandra/pull/216]

Oh, and let me know if I missed something, please. 

Thanks!",05/Apr/18 13:24;jasobrown;+1 to backporting to 2.2 and 3.0. committed as sha {{2e5e11d66d41038bee8d2f81eb013f735d233def}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SecondaryIndexManagerTest.assert[Not]MarkedAsBuilt produces flaky tests,CASSANDRA-13965,13110326,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,adelapena,adelapena,adelapena,18/Oct/17 15:19,12/Mar/19 14:06,13/Mar/19 22:35,23/Nov/17 18:55,4.x,,,,,Feature/2i Index,Legacy/Testing,,,,0,,,,"The methods [{{SecondaryIndexManagerTest.assertMarkedAsBuilt}}|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/index/SecondaryIndexManagerTest.java#L554-L557] and [{{SecondaryIndexManagerTest.assertNotMarkedAsBuilt}}|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/index/SecondaryIndexManagerTest.java#L559-L562] produce occasional test failures. 

These methods assume that there aren't any other indexes in the {{system.IndexInfo}} table than those created by the calling test. However, it is possible to find indexes built for other tests (not only {{SecondaryIndexManagerTest}}) that rely on {{CQLTester.afterTest}} to cleanup to drop their created indexes, because this method is asynchronous. So, it is possible to reach the {{SecondaryIndexManagerTest.assert(Not)MarkedAsBuilt}} calls before the indexes created by the previous test have been cleaned up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-22 08:46:02.624,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 23 18:55:09 UTC 2017,,,,,,0|i3lezz:,9223372036854775807,4.x,,,,,,,snazy,snazy,,,,,,,,,,18/Oct/17 15:30;adelapena;[Here|https://github.com/apache/cassandra/compare/trunk...adelapena:13965-trunk] is a patch modifying the methods to only verify the presence/absence in {{system.IndexInfo}} table of the index of interest.,"22/Nov/17 08:46;snazy;+1

Thanks for the patch!","23/Nov/17 18:55;adelapena;Committed to trunk as [2d2879db7a2fe8b0c25d08f67c81c88454e1527c|https://github.com/apache/cassandra/commit/2d2879db7a2fe8b0c25d08f67c81c88454e1527c].

Thanks for reviewing!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
trunk eclipse-warnings,CASSANDRA-14061,13119560,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,20/Nov/17 06:23,12/Mar/19 14:06,13/Mar/19 22:35,08/Mar/18 23:35,4.0,,,,,Legacy/Testing,,,,,0,,,,"{noformat}
eclipse-warnings:
    [mkdir] Created dir: /home/ubuntu/cassandra/build/ecj
     [echo] Running Eclipse Code Analysis.  Output logged to /home/ubuntu/cassandra/build/ecj/eclipse_compiler_checks.txt
     [java] ----------
     [java] 1. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java (at line 59)
     [java] 	return new SSTableIdentityIterator(sstable, key, partitionLevelDeletion, file.getPath(), iterator);
     [java] 	^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     [java] Potential resource leak: 'iterator' may not be closed at this location
     [java] ----------
     [java] 2. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java (at line 79)
     [java] 	return new SSTableIdentityIterator(sstable, key, partitionLevelDeletion, dfile.getPath(), iterator);
     [java] 	^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     [java] Potential resource leak: 'iterator' may not be closed at this location
     [java] ----------
     [java] 2 problems (2 errors)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14296,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-27 13:30:52.411,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 23:35:07 UTC 2018,,,,,,0|i3mzgn:,9223372036854775807,,,,,,,,spodxx@gmail.com,spodxx@gmail.com,,,,,,,,,,"20/Nov/17 08:07;jay.zhuang;Seems they're not the real problem, here is the patch to {{SupressWarning}}:
| Branch | uTest |
| [14061|https://github.com/cooldoger/cassandra/tree/14061] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14061.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14061] |

Also, making {{ant eclipse-warnings}} required in circleci build.",27/Nov/17 13:30;spodxx@gmail.com;Can't reproduce the issue on trunk. Ant eclipse-warnings will finish without any errors. ,"27/Nov/17 17:36;jay.zhuang;[~spodxx@gmail.com]
I'm still able to reproduce it, for example: https://circleci.com/gh/cooldoger/cassandra/148#tests/containers/0
{noformat}
eclipse-warnings:
    [mkdir] Created dir: /home/ubuntu/cassandra/build/ecj
     [echo] Running Eclipse Code Analysis.  Output logged to /home/ubuntu/cassandra/build/ecj/eclipse_compiler_checks.txt
     [java] ----------
     [java] 1. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java (at line 59)
     [java] 	return new SSTableIdentityIterator(sstable, key, partitionLevelDeletion, file.getPath(), iterator);
     [java] 	^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     [java] Potential resource leak: 'iterator' may not be closed at this location
     [java] ----------
     [java] 2. ERROR in /home/ubuntu/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java (at line 79)
     [java] 	return new SSTableIdentityIterator(sstable, key, partitionLevelDeletion, dfile.getPath(), iterator);
     [java] 	^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     [java] Potential resource leak: 'iterator' may not be closed at this location
     [java] ----------
     [java] 2 problems (2 errors)
{noformat}
And the last trunk has the same problem: https://circleci.com/gh/cooldoger/cassandra/149#queue-placeholder/containers/0
It won't fail the build, so I would suggest to make {{ant eclipse-warnings}} required in the build.","30/Nov/17 21:10;jjirsa;[~spodxx@gmail.com] interested in being the official reviewer on this?
","01/Dec/17 12:01;spodxx@gmail.com;[~jay.zhuang], if we're going to make eclipse-warnings a hard requirement for the build, shouldn't we also fix the reported issues instead of suppressing them? Doing both seems to be a bit ambivalent to me. ","02/Dec/17 20:57;jay.zhuang;I think the warnings are false alert, maybe could add {{iterator.close();}} here: [SSTableIdentityIterator.java:175|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java#L175]. The warning is introduced in CASSANDRA-13299. cc [~jasonstack]","11/Dec/17 15:39;spodxx@gmail.com;[~jay.zhuang], do you plan to change your patch or keep it as is at this point?","11/Dec/17 18:09;jay.zhuang;[~spodxx@gmail.com] For adding {{iterator.close();}} I don't know. Anyway, it's doing nothing: [AbstractIterator.java:86|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/AbstractIterator.java#L86] and won't fix the warning. What do you think?","07/Mar/18 13:07;spodxx@gmail.com;Looks like {{eclipse-warnings}} is breaking [trunk-test-all|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-test-all/] on b.a.o, so we should get this fixed at some point. There's also a new type of error. Would you like to rebase and look at this again, [~jay.zhuang]? The suggested SuppressWarnings annotation should be safe, so let's not unnecessarily complicate matters and go with the suggested changes.
/cc [~aweisberg]","07/Mar/18 23:48;jay.zhuang;CASSANDRA-14296 would fix the new ones.

For the warnings in {{SSTableIdentityIterator.java}}, it cannot be reproduced consistently: [Cassandra-trunk-test-all: 499|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-test-all/498/console] vs. [Cassandra-trunk-test-all: 499|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-test-all/498/console].

Anyway, I rebased to the trunk to suppress warnings:
| Branch | uTest |
| [14061|https://github.com/cooldoger/cassandra/tree/14061] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14061.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14061] |
",08/Mar/18 12:29;spodxx@gmail.com;+1,"08/Mar/18 12:32;jasonstack;Actually, I am not sure how the warning is introduced in 13299.. Thanks for fixing it!","08/Mar/18 23:35;jay.zhuang;Thanks for the review.
Committed as [a7141e6|https://github.com/apache/cassandra/commit/a7141e6c9df03287567c22c76372e166fc83d18e].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Root logging formatter broken in dtests,CASSANDRA-14059,13119377,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jkni,jkni,jkni,18/Nov/17 00:26,12/Mar/19 14:06,13/Mar/19 22:35,05/Dec/17 03:13,,,,,,Test/dtest,,,,,0,,,,"Since the ccm dependency in dtest was bumped to {{3.1.0}} in {{7cc06a086f89ed76499837558ff263d84337acba}}, when dtests are run with --nologcapture, errors of the following form are printed:
{code}
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/logging/__init__.py"", line 861, in emit
    msg = self.format(record)
  File ""/usr/lib64/python2.7/logging/__init__.py"", line 734, in format
    return fmt.format(record)
  File ""/usr/lib64/python2.7/logging/__init__.py"", line 469, in format
    s = self._fmt % record.__dict__
KeyError: 'current_test'
Logged from file dtest.py, line 485
{code}

This is because CCM no longer installs a basic root logger configuration, which is probably a more correct behavior than what it did prior to this change. Now, dtest installs its own basic root logger configuration which writes to 'dtest.log' using the formatter {{'%(asctime)s,%(msecs)d %(name)s %(current_test)s %(levelname)s %(message)s'}}. This means that anything logging a message must provide the current_test key in its extras map. The dtest {{debug}} and {{warning}} functions do this, but logging from dependencies doesn't, producing these {{KeyError}} s. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-27 14:27:23.694,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 05 03:13:28 UTC 2017,,,,,,0|i3mybz:,9223372036854775807,,,,,,,,spodxx@gmail.com,spodxx@gmail.com,,,,,,,,,,"18/Nov/17 00:31;jkni;Patch here: [https://github.com/jkni/cassandra-dtest/commit/91e860da6b5959df02d14cc56b0d5c09a2926a83]. This removes the field from the formatter and instead concatenates it to the message inside dtest logging functions. It also changes the way we construct the CURRENT_TEST global. In my testing, the test id already contained the method name, so method names were duplicated in log output.

There may be a convincing argument to removing the dtest.log logger configuration entirely. It appears to have been missing for some time without anyone noticing. For now, I introduced a behavior that's close to the original intention of the dtest.log, as far as I can tell.","27/Nov/17 14:27;spodxx@gmail.com;Sounds reasonable. I'd also suggest to add another character after the test method name, to make it easier to recognize where the actual message starts.

{noformat}
""15:08:46,800 dtest DEBUG paging_test.TestPagingData.group_by_paging_test Done setting configuration options:""
{noformat}",30/Nov/17 21:11;jjirsa;[~spodxx@gmail.com] can I mark you as reviewer here as well? ,01/Dec/17 17:41;jkni;Thanks for taking a look. Any suggestions for the extra character to serve as the delimiter? More whitespace? I have very few opinions here.,01/Dec/17 18:04;spodxx@gmail.com;Maybe something like a '-' character? I don't mind that much either.,01/Dec/17 18:56;jkni;Works for me - okay with you if I go ahead and commit this with that change?,01/Dec/17 19:07;spodxx@gmail.com;+1,01/Dec/17 22:22;jkni;Thanks for the review! Committed to cassandra-dtest as {{c1bcc18664cd9e9035f05a98ed23e763173fafd9}}.,"04/Dec/17 23:55;jjirsa;FWIW, this has some fallout - seems like a fair number of callers (especially of {{debug}} aren't passing in a simple string, but a {{list}}) :

{code}
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/cassandra/cassandra-dtest/tools/decorators.py"", line 48, in wrapped
    f(obj)
  File ""/home/cassandra/cassandra-dtest/cqlsh_tests/cqlsh_tests.py"", line 1447, in test_client_warnings
    debug(fut.warnings)
  File ""/home/cassandra/cassandra-dtest/dtest.py"", line 187, in debug
    LOG.debug(CURRENT_TEST + ' - ' + msg)
""cannot concatenate 'str' and 'list' objects
{code}

{code}
 File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/cassandra/cassandra-dtest/jmx_test.py"", line 225, in test_compactionstats
    debug(jmx.read_attribute(compaction_manager, 'CompactionSummary'))
  File ""/home/cassandra/cassandra-dtest/dtest.py"", line 187, in debug
    LOG.debug(CURRENT_TEST + ' - ' + msg)
'cannot concatenate \'str\' and \'list\' objects
{code}

Dunno if you want to re-open or open a new JIRA for it.
","05/Dec/17 00:30;jkni;I'm really sorry about that. I ran this with a subset of dtests that were clearly inadequate to cover all cases here.

Reopening and I'll supply a patch on this issue shortly.","05/Dec/17 00:52;jkni;Follow-up commit here https://github.com/jkni/cassandra-dtest/commit/179bf9f16a03394aee9715b745bfd4773c5b05ba

It uses string formatting instead of concatenation, so we'll automatically stringify lists. I smoke tested this on the old set as well as manually on the tests listed above. I'm running a full set of dtests as well, but this is very likely no worse than the current situation if there's support for committing it sooner. Otherwise, I'll commit once tests are clean.","05/Dec/17 01:16;jjirsa;lgtm!
",05/Dec/17 03:13;jkni;Committed the fix to cassandra-dtest as {{413b18a87d9416446cf4adec5f8483ad408b3e83}}. Thanks for taking a look and noticing the error.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`ant javadoc` task broken due to UTF-8 characters in multiple source files,CASSANDRA-14154,13129255,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jgrassler,jgrassler,jgrassler,08/Jan/18 14:29,12/Mar/19 14:06,13/Mar/19 22:35,10/Jan/18 13:18,3.0.16,3.11.2,4.0,,,Build,,,,,0,,,,"Several source files contain UTF-8 characters in String literals. When building the {{javadoc}} target with ant ({{ant javadoc}}), these will trip up javadoc, which defaults to ASCII encoding. See the {{build.log}} for what I did and the resulting output.

I created a patch that will fix the problem ({{javadoc-encoding.patch}}), which is attached.

I encountered this problem in 3.11.1, but I haven't checked whether other versions are affected as well.","Built on OpenSUSE Tumbleweed. I used the following java packages when building:

{quote}
titan% rpm -qa | grep java
javapackages-local-4.7.0+git20170331.ef4057e7-1.5.x86_64
java-1_8_0-openjdk-devel-1.8.0.151-1.3.x86_64
python3-javapackages-4.7.0+git20170331.ef4057e7-1.5.x86_64
java-1_8_0-openjdk-headless-1.8.0.151-1.3.x86_64
timezone-java-2017c-1.3.noarch
java-1_8_0-openjdk-1.8.0.151-1.3.x86_64
libjavascriptcoregtk-1_0-0-2.4.11-7.6.x86_64
libjavascriptcoregtk-4_0-18-2.18.4-1.1.x86_64
javapackages-tools-4.7.0+git20170331.ef4057e7-1.5.x86_64
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Jan/18 14:26;jgrassler;build.log;https://issues.apache.org/jira/secure/attachment/12905094/build.log,08/Jan/18 14:27;jgrassler;javadoc-encoding.patch;https://issues.apache.org/jira/secure/attachment/12905093/javadoc-encoding.patch,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2018-01-10 13:18:53.899,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,,,Wed Jan 10 13:18:53 UTC 2018,,,,,,0|i3omzj:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,3.11.1,,,,,,,"10/Jan/18 13:18;jasobrown;Nice find. +1 and committed as sha {{fde05f4f1b4ad814acf79bed61500aaf2ebe39d6}}.

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loss of digits when doing CAST from varint/bigint to decimal,CASSANDRA-14170,13131535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,blerer,danfiala,danfiala,17/Jan/18 10:22,12/Mar/19 14:06,13/Mar/19 22:35,14/Mar/18 09:34,3.11.3,4.0,,,,Legacy/CQL,,,,,0,,,,"Cast functions from numeric types to decimal type are implemented as conversion to double first and then from double to decimal: [https://github.com/apache/cassandra/compare/trunk...blerer:10310-3.0#diff-6aa4a8f76df6c30c5bb4026b8c9251eeR80].

This can cause loss of digits for big values stored in varint or bigint. It is probably unexpected because decimal can store such values precisely.

Examples:

{{cqlsh> CREATE TABLE cast_bigint_test(k int PRIMARY KEY, bigint_clmn bigint);}}
 {{cqlsh> INSERT INTO cast_bigint_test(k, decimal_clmn) VALUES(2, 9223372036854775807);}}
 {{cqlsh> SELECT CAST(bigint_clmn AS decimal) FROM cast_bigint_test;}}
 {{cast(bigint_clmn as decimal)}}
 {{------------------------------}}
 {{9.223372036854776E+18}}
 {{(1 rows)}}

{{cqlsh> CREATE TABLE cast_varint_test (k int PRIMARY KEY, varint_clmn varint);}}
 {{cqlsh> INSERT INTO cast_varint_test(k, varint_clmn) values(2, 1234567890123456789);}}
 {{cqlsh> SELECT CAST(varint_clmn AS decimal) FROM cast_varint_test;}}
 {{cast(varint_clmn as decimal)}}
 {{------------------------------}}
1.23456789012345677E+18
 {{(1 rows)}}

 ",Tested with Cassandra 3.11.1 but this issue is present since the implementation of cast functions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-15 09:29:19.619,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 14 09:34:02 UTC 2018,,,,,,0|i3p073:,9223372036854775807,,,,,,,,adelapena,adelapena,,,,,,,,,,"15/Feb/18 09:29;blerer;[~danfiala] Thanks for reporting the problem.

I pushed a branch [here|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:14170-3.11] to fix the problem and add some extra tests.

The patch can be merge without trouble into trunk and the Unit tests runs fine on both branches. We do not have DTests for the {{CAST}} operations.

[~adelapena] Could you review?","05/Mar/18 12:46;adelapena;Overall the patch looks good to me. However, the new test [{{testNoLossOfPrecisionForCastToDecimal}}|https://github.com/blerer/cassandra/blob/4b1f5804837ae3164bb183fcb62077683621651a/test/unit/org/apache/cassandra/cql3/functions/CastFctsTest.java#L205-L212] misses the {{@Test}} annotation. Indeed, it fails due to [a column names mismatch at the insert statement|https://github.com/blerer/cassandra/blob/4b1f5804837ae3164bb183fcb62077683621651a/test/unit/org/apache/cassandra/cql3/functions/CastFctsTest.java#L208]. I think it should be [this way|https://github.com/adelapena/cassandra/commit/be44415a3f61e13720889cfd44482ee1142e8c1d]. If it's ok for you it can be addressed during commit.

Nitpick: there is a misssed blank line in {{getDecimalConversionFunction}} JavaDoc, [here|https://github.com/adelapena/cassandra/commit/86f8121f95adb60eef85edbcaf9fb3917ef5207f].",14/Mar/18 09:33;blerer;Thanks for picking up my mistakes :-). I was far to quick on that one. ,14/Mar/18 09:34;blerer;Committed into 3.11 at 3fa7c0894449b5c6033a6c4f47ec3292d07268b8 and merged into trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Comma at the end of the end of the seed list is interpretated as localhost,CASSANDRA-14285,13142066,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nicolas.guyomar,usernkey,usernkey,02/Mar/18 10:43,12/Mar/19 14:06,13/Mar/19 22:35,02/Mar/18 22:33,4.0,,,,,Local/Config,,,,,0,,,,"Seeds: '10.1.20.10,10.1.21.10,10.1.22.10,'  cause a flood of the debug log with messages like this one.

DEBUG [MessagingService-Outgoing-localhost/127.0.0.1-Gossip] 2018-02-28 15:53:57,314 OutboundTcpConnection.java:545 - Unable to connect to localhost/[127.0.0.1|http://127.0.0.1/]

This code provide by Nicolas Guyomar provide the reason of the issue.

In SImpleSeedProvider : 
 
String[] hosts = ""10.1.20.10,10.1.21.10,10.1.22.10,"".split("","", -1);
List<InetAddress> seeds = new ArrayList<InetAddress>(hosts.length);
for (String host : hosts)
{
System.out.println(InetAddress.getByName(host.trim()));
}
 
output : 
/[10.1.20.10|http://10.1.20.10/]
/[10.1.21.10|http://10.1.21.10/]
/[10.1.22.10|http://10.1.22.10/]
localhost/[127.0.0.1|http://127.0.0.1/]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-02 11:08:08.523,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 06 01:23:23 UTC 2018,,,,,,0|i3qsgf:,9223372036854775807,3.11.1,,,,,,,jrwest,jrwest,,,,,,,,,,"02/Mar/18 11:08;nicolas.guyomar;Could this one be added to Cassandra ? [https://github.com/nicolasguyomar/cassandra/commit/0cb5cb565c84a49ed08b216bb2a0e75ec1d2fa2c]

First small contribution","02/Mar/18 20:33;jrwest;+1. As an added bonus it also fixes the case where there is a double comma in the string. Don't see an interpretation of the config where the operator intended to include localhost as a seed so I think the ""breaking"" change is a good one (i.e no one should be expecting this behavior). Since its breaking, I guess we can ignore back porting to 3.11 – the patch doesn't apply clean either. 

I wanted to ask you to add a test but it looks like that will be a pain given how {{SimpleSeedProvider}} is currently implemented.

Branch: [https://github.com/jrwest/cassandra/tree/14285-trunk]

utests: [https://circleci.com/gh/jrwest/cassandra/37]

 ","02/Mar/18 22:33;jasobrown;committed as sha {{2dbbbc57dbb70886417c888e2bf8e01da78ff6ee}}. Thanks, all!","06/Sep/18 01:23;KurtG;IDK how many people were doing it, or if we do it elsewhere in ccm/our testing, but this also means you can't have an empty seed list - which used to resolve to localhost. Honestly, that's probably less surprising, especially considering it's not documented at the moment, but thought I'd mention it here because it surprised me.

FTR, if you're using {{ccm add}}, you now _have_ to specify {{-s}} for at least one node. If people think this is annoying or find other issues we can easily add to this patch to handle that case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogSegmentManagerCDCTest is flaky,CASSANDRA-14195,13134127,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jay.zhuang,aweisberg,aweisberg,26/Jan/18 22:19,12/Mar/19 14:06,13/Mar/19 22:35,30/Jan/18 15:10,4.0,,,,,Legacy/Testing,,,,,0,,,,"This fails fairly reliably in CircleCI and in a few minutes if you run it in a loop on a MacOS laptop.

I see two failures.
{noformat}
    [junit] Testcase: testRetainLinkOnDiscardCDC(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest):	Caused an ERROR
    [junit] Rejecting mutation to keyspace cql_test_keyspace. Free up space in build/test/cassandra/cdc_raw:0 by processing CDC logs.
    [junit] org.apache.cassandra.exceptions.CDCWriteException: Rejecting mutation to keyspace cql_test_keyspace. Free up space in build/test/cassandra/cdc_raw:0 by processing CDC logs.
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDC.throwIfForbidden(CommitLogSegmentManagerCDC.java:136)
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDC.allocate(CommitLogSegmentManagerCDC.java:108)
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:272)
    [junit] 	at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:604)
    [junit] 	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:481)
    [junit] 	at org.apache.cassandra.db.Mutation.apply(Mutation.java:191)
    [junit] 	at org.apache.cassandra.db.Mutation.apply(Mutation.java:196)
    [junit] 	at org.apache.cassandra.db.Mutation.apply(Mutation.java:205)
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testRetainLinkOnDiscardCDC(CommitLogSegmentManagerCDCTest.java:256)
{noformat}

and

{noformat}
    [junit] Testcase: testCompletedFlag(org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest):	FAILED
    [junit] Index file not written: build/test/cassandra/cdc_raw:0/CommitLog-7-1517005121474_cdc.idx
    [junit] junit.framework.AssertionFailedError: Index file not written: build/test/cassandra/cdc_raw:0/CommitLog-7-1517005121474_cdc.idx
    [junit] 	at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testCompletedFlag(CommitLogSegmentManagerCDCTest.java:210)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-29 00:12:18.673,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 30 15:10:00 UTC 2018,,,,,,0|i3pfkf:,9223372036854775807,,,,,,,,JoshuaMcKenzie,JoshuaMcKenzie,,,,,,,,,,"29/Jan/18 00:12;jay.zhuang;The problem is because [{{testCompletedFlag()}}|https://github.com/apache/cassandra/commit/e9da85723a8dd40872c4bca087a03b655bd2cacb#diff-7bfedffd12a11d61fa013c6a5894c102R191] and [{{testReplayLogic()}}|https://github.com/apache/cassandra/commit/e9da85723a8dd40872c4bca087a03b655bd2cacb#diff-7bfedffd12a11d61fa013c6a5894c102R279] don't reset the {{CDCSpaceInMB}} after running, which impacts the other tests.
Here is the fix, please review:
| Branch | uTest |
| [14195|https://github.com/cooldoger/cassandra/tree/14195] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14195.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14195] |",29/Jan/18 17:19;JoshuaMcKenzie;+1. Good catch.,29/Jan/18 17:44;aweisberg;Thanks that did seem to fix it.,30/Jan/18 11:46;jasobrown;Did any one commit yet? :),"30/Jan/18 14:09;JoshuaMcKenzie;Not yet; env is super rusty. I should be able to get to it today.

 

Thankfully it's in a bit of the code that's not changing frequently so I can get away with being slow. =/

 

Sorry about that [~jay.zhuang]!",30/Jan/18 15:10;JoshuaMcKenzie;[Committed|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=69db2359ee0889cb4a57aec179b9821ff442d26b]. Thanks Jay!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sub-range selection for non-frozen collections should return null instead of empty,CASSANDRA-14182,13132670,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,blerer,blerer,blerer,22/Jan/18 12:29,12/Mar/19 14:06,13/Mar/19 22:35,24/Jan/18 15:12,,,,,,,,,,,0,,,,"For non frozen collections, Cassandra cannot differentiate an empty collection from a {{null}} one. Due to that Cassandra returns always {{null}} for non-frozen empty collection.

When selecting a sub range from a non-frozen collection, if the range does not contains any data an empty collection will be returned. It is counter intuitive and a {{null}} value should be returned instead.
 
{code:sql}
CREATE TABLE IF NOT EXISTS t (k int PRIMARY KEY, v set<int>);

INSERT INTO t (k, v) VALUES (1, {});
SELECT v FROM t; -- null
SELECT v[1] FROM t; -- null
SELECT v[1..] FROM t; -- null

INSERT INTO t (k, v) VALUES (1, {0});
SELECT v FROM t; -- {0}
SELECT v[1] FROM t; -- null
SELECT v[1..] FROM t; -- {}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-22 13:45:12.99,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 24 15:12:00 UTC 2018,,,,,,0|i3p6lr:,9223372036854775807,,,,,,,,adelapena,adelapena,,,,,,,,,,"22/Jan/18 13:32;blerer;I pushed a patch for that problem [here|https://github.com/apache/cassandra/compare/trunk...blerer:14182-trunk].

[~adelapena] could you review?",22/Jan/18 13:45;adelapena;Sure thing!,"22/Jan/18 15:59;adelapena;Nice patch, +1 assuming CI looks good.

Just a trivial detail that can be addressed during commit: can you change [this {{row(null)}}|https://github.com/blerer/cassandra/blob/bc0d444c18a963465e8758cbb3759030a6442932/test/unit/org/apache/cassandra/cql3/validation/entities/CollectionsTest.java#L1818] by {{row((Map) null)}} to prevent the build warning, as it is done [here|https://github.com/blerer/cassandra/blob/bc0d444c18a963465e8758cbb3759030a6442932/test/unit/org/apache/cassandra/cql3/validation/entities/CollectionsTest.java#L1874]?",24/Jan/18 14:27;blerer;Thanks for the review. CI looks good.,24/Jan/18 15:12;blerer;Committed into trunk at 4de7a65ed9f3c97658a80dd64032ad6e82e9d58b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra.spec needs to require ant-junit,CASSANDRA-14180,13132515,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,troels@arvin.dk,troels@arvin.dk,troels@arvin.dk,21/Jan/18 15:41,12/Mar/19 14:06,13/Mar/19 22:35,22/Jan/18 12:46,3.0.16,3.11.2,4.0,,,Packaging,,,,,0,,,,"I tried rebuilding cassandra-3.11.1-1.src.rpm on a Centos 7 host which had ant installed, but not the ""ant-junit"" package; that failed with a somewhat cryptic error message. It turnout out I needed to have the ""ant-junit"" package installed, as well. So for the cassandra.spec file, I suggest that the following line be added just below the existing BuildRequires line:

{{BuildRequires: ant-junit >= 1.9}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-22 09:48:14.824,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 22 12:46:37 UTC 2018,,,,,,0|i3p5nb:,9223372036854775807,,,,,,,,spodxx@gmail.com,spodxx@gmail.com,,,,,,,,,,"22/Jan/18 09:48;spodxx@gmail.com;I think you're right. But how did you manage to build the source package at all? Mine was missing a version and revision macro declaration before rpmbuild was able to finish successfully.

We use to create these values dynamically and [pass them|https://github.com/apache/cassandra-builds/blob/master/docker/build-rpms.sh#L69] as {{rpmbuild --define=""version ..""}} during our release process. Maybe we should turn that into a string substitution in the spec instead.","22/Jan/18 10:28;troels@arvin.dk;I ran the following:
rpmbuild -bb --define ""version 3.11.1"" --define ""revision 4"" cassandra.spec

(And for it to compile, I also had to add a patch to the spec file, due to the recent JMX incompatibilities described in case CASSANDRA-14173).

It would certainly be nice for those building RPMs, if the version and revision values were part of the spec file itself.","22/Jan/18 12:46;spodxx@gmail.com;Thanks for reporting!

Committed as 0628520a9be69bb42a0ba73859888a5a8af83b27 to cassandra-3.0 and merged upwards.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle incompletely written hint descriptors during startup,CASSANDRA-14080,13121897,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,alourie,alekiv,alekiv,30/Nov/17 12:06,12/Mar/19 14:06,13/Mar/19 22:35,29/Mar/18 13:05,3.0.17,3.11.3,4.0,,,Consistency/Hints,,,,,0,,,,"Continuation of CASSANDRA-12728 bug.

Problem: Cassandra didn't start due to 0 size hints files

Log form v3.0.14:
{code:java}
INFO  [main] 2017-11-28 19:10:13,554 StorageService.java:575 - Cassandra version: 3.0.14
INFO  [main] 2017-11-28 19:10:13,555 StorageService.java:576 - Thrift API version: 20.1.0
INFO  [main] 2017-11-28 19:10:13,555 StorageService.java:577 - CQL supported versions: 3.4.0 (default: 3.4.0)
ERROR [main] 2017-11-28 19:10:13,592 CassandraDaemon.java:710 - Exception encountered during startup
org.apache.cassandra.io.FSReadError: java.io.EOFException
        at org.apache.cassandra.hints.HintsDescriptor.readFromFile(HintsDescriptor.java:142) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[na:1.8.0_141]
        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ~[na:1.8.0_141]
        at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[na:1.8.0_141]
        at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[na:1.8.0_141]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[na:1.8.0_141]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[na:1.8.0_141]
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[na:1.8.0_141]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:1.8.0_141]
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[na:1.8.0_141]
        at org.apache.cassandra.hints.HintsCatalog.load(HintsCatalog.java:65) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.hints.HintsService.<init>(HintsService.java:88) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.hints.HintsService.<clinit>(HintsService.java:63) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.StorageProxy.<clinit>(StorageProxy.java:121) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at java.lang.Class.forName0(Native Method) ~[na:1.8.0_141]
        at java.lang.Class.forName(Class.java:264) ~[na:1.8.0_141]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:585) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:570) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:346) [apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569) [apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697) [apache-cassandra-3.0.14.jar:3.0.14]
Caused by: java.io.EOFException: null
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:803) ~[na:1.8.0_141]
        at org.apache.cassandra.hints.HintsDescriptor.deserialize(HintsDescriptor.java:237) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.hints.HintsDescriptor.readFromFile(HintsDescriptor.java:138) ~[apache-cassandra-3.0.14.jar:3.0.14]
        ... 20 common frames omitted
{code}



After several 0 size hints files deletion Cassandra started successfully.

Jeff Jirsa added a comment - Yesterday
Aleksandr Ivanov can you open a new JIRA and link it back to this one? It's possible that the original patch didn't consider 0 byte files (I don't have time to go back and look at the commit, and it was long enough ago that I've forgotten) - were all of your files 0 bytes?

Not all, 8..10 hints files were with 0 size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12728,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-30 12:10:22.996,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 29 13:03:52 UTC 2018,,,,,,0|i3nduf:,9223372036854775807,3.0.14,4.x,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"30/Nov/17 12:10;iamaleksey;There has been a resolved JIRA some time ago to not write out empty hints files - don't have the number handy, but try searching for it.","30/Nov/17 12:19;alekiv;[~iamaleksey], this particular report is about handling empty hints files during Cassandra start. Seems currently this situation is not handled properly.","30/Nov/17 12:21;iamaleksey;Sorry. I don't mean that you shouldn't file/have filed this JIRA. Just saying that the similar one we closed recenlty-ish might have some useful context, so you might want to look it up and link to this one.",30/Nov/17 18:22;jjirsa;Probably: CASSANDRA-13740,04/Dec/17 02:48;alourie;A patch for trunk is in https://github.com/apache/cassandra/compare/trunk...alourie:CASSANDRA-14080,"04/Dec/17 11:56;iamaleksey;[~jjirsa] Nope. Talking about a JIRA specifically about writing out empty hint buffers into empty files.

That would do it - although I'd prefer to just slap a second filter call instead of combining the two here.

What I'd prefer even more is if we looked into why an empty hint file gets written in the first place and taking care of it. Which would have the added benefit of not having confusing empty files around.",04/Dec/17 11:58;iamaleksey;CASSANDRA-11090 (https://github.com/apache/cassandra/commit/1f626087c8819b75f17fcbe757603fc0026d3cc1).,05/Dec/17 00:16;alourie;[~iamaleksey] That should prevent empty hint files to be written as much as C* is concerned. The problem is that we have no way to reproduce creation of the empty hint files and for all we know these hint files could have been created from external circumstances such as corrupt disks. We should still handle 0 length files but this isn't a ticket for a long drawn out investigation with possibly no results.,"05/Dec/17 12:50;iamaleksey;If that's the intent, then maybe we should probably generalize it beyond handling empty files? As that's just one of many possible manifestations of corruption.","07/Dec/17 04:19;alourie;[~iamaleksey] Would you mind elaborating, please, which other manifestations of corruption you have in mind? I thought crc32 checks are supposed to cover those other cases.

Thanks.","13/Dec/17 13:55;iamaleksey;Having any incomplete header, perhaps? For the purposes of this ticket, does it matter if the file has size 0 or size 1?","18/Dec/17 15:05;alourie;[~iamaleksey] 

I was under the impression that CRC check would handle that.
I had a look at the deserialization code that does the CRC check, and the problem is that _any_ CRC check error (including incomplete header or file size 0) will end up as an FSError, which will cause C* to fail on start. So I've reworked a patch from just checking the size to be > 0 to ignoring the corrupted files instead (including one of size 0). Would that be a better solution for the types of issues you were thinking about?

Thanks.",01/Feb/18 08:19;alourie;[~iamaleksey] it would be great if you could give a feedback on the previous comment. Thanks!,27/Mar/18 04:42;alourie;[~iamaleksey] I would appreciate if you could have a look at this. Thanks!,29/Mar/18 12:09;iamaleksey;The logic is sound. Let me have a look and commit.,"29/Mar/18 12:36;iamaleksey;So, arguably the original code was written somewhat lazily (by me).

Swallowing any IOException there and re-trhowing as an FSError, while making the code slightly simpler - by avoiding the need to handle IOException properly elsewhere - was too blunt.

We should still throw the FSError on checksum mismatches - but not in other scenarios. I'll expand a bit, on top of your patch, to differentiate between these two different exceptions.","29/Mar/18 13:03;iamaleksey;Committed to 3.0 as [68079e4b2ed4e58dbede70af45414b3d4214e195|https://github.com/apache/cassandra/commit/68079e4b2ed4e58dbede70af45414b3d4214e195] and merged up with 3.11 and trunk, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
group by select queries query results differ when using select * vs select fields,CASSANDRA-14209,13135435,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,blerer,dmodha,dmodha,01/Feb/18 14:18,12/Mar/19 14:04,13/Mar/19 22:35,02/Mar/18 10:48,3.11.3,,,,,,,,,,0,,,,"{{I get two different out with these 2 queries.  The only difference between the 2 queries is that one does ‘select *’ and other does ‘select specific fields’ without any aggregate functions.}}

{{I am using Apache Cassandra 3.10.}}


{{Consistency level set to LOCAL_QUORUM.}}
{{cassandra@cqlsh> select * from wp.position where account_id = 'user_1';}}

{{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}
{{------------+-------------+---------+----------------+------------------+----------+----------------+---------------------------------}}
{{ user_1 | AMZN | 2 | 1239.2 | 0 | 1011 | null | 2018-01-25 17:18:07.158000+0000}}
{{ user_1 | AMZN | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}}

{{(2 rows)}}
{{cassandra@cqlsh> select * from wp.position where account_id = 'user_1' group by security_id;}}

{{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}
{{------------+-------------+---------+----------------+------------------+----------+----------------+---------------------------------}}
{{ user_1 | AMZN | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}}

{{(1 rows)}}
{{cassandra@cqlsh> select account_id,security_id, counter, avg_exec_price,quantity, update_time from wp.position where account_id = 'user_1' group by security_id ;}}

{{ account_id | security_id | counter | avg_exec_price | quantity | update_time}}
{{------------+-------------+---------+----------------+----------+---------------------------------}}
{{ user_1 | AMZN | 2 | 1239.2 | 1011 | 2018-01-25 17:18:07.158000+0000}}

{{(1 rows)}}


{{Table Description:}}
{{CREATE TABLE wp.position (}}
{{ account_id text,}}
{{ security_id text,}}
{{ counter bigint,}}
{{ avg_exec_price double,}}
{{ pending_quantity double,}}
{{ quantity double,}}
{{ transaction_id uuid,}}
{{ update_time timestamp,}}
{{ PRIMARY KEY (account_id, security_id, counter)}}
{{) WITH CLUSTERING ORDER BY (security_id ASC, counter DESC)}}{{ }}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Feb/18 14:12;dmodha;Re group by select queries.txt;https://issues.apache.org/jira/secure/attachment/12908815/Re+group+by+select+queries.txt,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-02-14 09:58:15.978,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 02 10:48:53 UTC 2018,,,,,,0|i3pnmf:,9223372036854775807,,,,,,,,adelapena,adelapena,,,,,,,,,,"14/Feb/18 09:58;blerer;The problem is that the {{Selection}} used for building the result set is not the good one for wildcard queries with {{GROUP BY}}.  

I pushed some patches for [3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...blerer:14209-3.11] and [trunk|https://github.com/apache/cassandra/compare/trunk...blerer:14209-trunk] to fix that problem and add some testing for it.

[~adelapena] Could you review?","14/Feb/18 13:12;adelapena;The patch with the thorough tests looks good to me, +1",02/Mar/18 10:48;blerer;Committed in 3.11 at 515f07b5ac75b15015401e89c379b29c788ba5a3 and merged into trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Startup checker should wait for count rather than percentage,CASSANDRA-14297,13143384,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jolynch,jolynch,jolynch,07/Mar/18 22:52,12/Mar/19 14:03,13/Mar/19 22:35,12/Nov/18 17:47,4.0,,,,,Local/Startup and Shutdown,,,,,1,pull-request-available,,,"As I commented in CASSANDRA-13993, the current wait for functionality is a great step in the right direction, but I don't think that the current setting (70% of nodes in the cluster) is the right configuration option. First I think this because 70% will not protect against errors as if you wait for 70% of the cluster you could still very easily have {{UnavailableException}} or {{ReadTimeoutException}} exceptions. This is because if you have even two nodes down in different racks in a Cassandra cluster these exceptions are possible (or with the default {{num_tokens}} setting of 256 it is basically guaranteed). Second I think this option is not easy for operators to set, the only setting I could think of that would ""just work"" is 100%.

I proposed in that ticket instead of having `block_for_peers_percentage` defaulting to 70%, we instead have `block_for_peers` as a count of nodes that are allowed to be down before the starting node makes itself available as a coordinator. Of course, we would still have the timeout to limit startup time and deal with really extreme situations (whole datacenters down etc).

I started working on a patch for this change [on github|https://github.com/jasobrown/cassandra/compare/13993...jolynch:13993], and am happy to finish it up with unit tests and such if someone can review/commit it (maybe [~aweisberg]?).

I think the short version of my proposal is we replace:
{noformat}
block_for_peers_percentage: <percentage needed up, defaults to 70%>
{noformat}

with either
{noformat}
block_for_peers: <number that can be down, defaults to 1>
{noformat}

or, if we want to do even better imo and enable advanced operators to finely tune this behavior (while still having good defaults that work for almost everyone):
{noformat}
block_for_peers_local_dc:  <number that can be down, defaults to 1>
block_for_peers_each_dc: <number that can be down, defaults to sys.maxint>
block_for_peers_all_dcs: <number that can be down, defaults to sys.maxint>
{noformat}

For example if an operator knows that they must be available at {{LOCAL_QUORUM}} they would set {{block_for_peers_local_dc=1}}, if they use {{EACH_QUOURM}} they would set {{block_for_peers_local_dc=1}}, if they use {{QUORUM}} (RF=3, dcs=2) they would set {{block_for_peers_all_dcs=2}}. Naturally everything would of course have a timeout to prevent startup taking too long.
",,"Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225709488
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    --- End diff --
    
    If the goal is to not return unavailable shouldn't the default really be 1 since LOCAL_QUORUM and QUORUM are more common? I feel like if this doesn't do what people want with the defaults it makes it a lot less useful?
    
    All this does is add 10 seconds to startup which seems really minor. A part of me even wonders why fuss with any of this if we are just going to wait 10 seconds? Just wait until everything is up and if it doesn't happen in 10 seconds continue to do what we were going to do anyways?
    
    The connection priming stuff I get. That is good and I'm glad we added that.
    
    The name also doesn't really make sense anymore since this is not the number we are blocking for it's the number we aren't blocking for.
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225725532
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -213,4 +261,11 @@ boolean incrementAndCheck(InetAddressAndPort address)
                 return acks.computeIfAbsent(address, addr -> new AtomicInteger(0)).incrementAndGet() == threshold;
             }
         }
    +
    +    private String fmtBlocker(Integer size, Integer count)
    --- End diff --
    
    Is this a more complicated Math.max?
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225724502
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -17,13 +17,16 @@
      */
     package org.apache.cassandra.net;
     
    +import java.util.Collections;
     import java.util.HashSet;
     import java.util.Map;
     import java.util.Set;
     import java.util.concurrent.ConcurrentHashMap;
     import java.util.concurrent.CountDownLatch;
     import java.util.concurrent.TimeUnit;
     import java.util.concurrent.atomic.AtomicInteger;
    +import java.util.function.Function;
    +import java.util.stream.Collectors;
     
     import com.google.common.annotations.VisibleForTesting;
     import com.google.common.collect.Sets;
    --- End diff --
    
    Unused import
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225721508
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    -
    -        if (peers.isEmpty())
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        if (peers.size() == 1 && peers.contains(localAddress))
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        Set<InetAddressAndPort> myLocalPeers = peers.stream()
    +                                                    .collect(Collectors.groupingBy(getDatacenter, toSet()))
    +                                                    .getOrDefault(getDatacenter.apply(FBUtilities.getBroadcastAddressAndPort()),
    +                                                                  Collections.emptySet());
    +
    +
    +        logger.info(""choosing to block until no more than {}/{} local and no more than {}/{} global peers are still DOWN; max time to wait = {} seconds"",
    --- End diff --
    
    This logs the local node as a peer since you aren't removing it. Seems like it's not a desired side effect.
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225718303
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -39,7 +42,9 @@
     import org.apache.cassandra.locator.InetAddressAndPort;
     import org.apache.cassandra.net.async.OutboundConnectionIdentifier.ConnectionType;
     import org.apache.cassandra.utils.FBUtilities;
    +import org.jboss.byteman.synchronization.CountDown;
    --- End diff --
    
    Unused import
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225719987
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
    --- End diff --
    
    This change appears to remove the ability to disable this? It's an interesting change because it means we will always prime the connections which sounds kind of useful.
    
    The most you can do is crank down the timeout or set the number of hosts you won't wait for very high which has a similar impact.
;16/Oct/18 22:15;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225720782
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    +    public int block_for_peers_all_dcs = Integer.MAX_VALUE;
    --- End diff --
    
    QUORUM is at least a little common? Seems like defaulting to no waiting for remote DCs might not be the right default? It's a question of which do people dislike more? Getting unavailables when a node restarts or potentially waiting extra time on startup?
;16/Oct/18 22:15;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225732037
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    --- End diff --
    
    > If the goal is to not return unavailable shouldn't the default really be 1 since LOCAL_QUORUM and QUORUM are more common? I feel like if this doesn't do what people want with the defaults it makes it a lot less useful?
    
    I thought that defaulting to handling the case that the drivers default to `LOCAL_ONE` was most sensible, although I can make it `1` if you prefer and say the defaults are for `LOCAL_QUORUM` (I don't have strong opinions other than we shouldn't be waiting by default on remote DCs).
    
    > All this does is add 10 seconds to startup which seems really minor. A part of me even wonders why fuss with any of this if we are just going to wait 10 seconds? Just wait until everything is up and if it doesn't happen in 10 seconds continue to do what we were going to do anyways?
    
    In practice you wait way less than 10 seconds (e.g. on the test 200 node 4.0 cluster we handshake with the whole local DC in about ... 500ms). I personally think that most users would rather their database wait O(minutes) than throw errors in the general case, but from the earlier conversations in CASSANDRA-13993 it seemed like folks were hesitant to wait so long on startup e.g. when doing ccm clusters or the such.
    
    > The name also doesn't really make sense anymore since this is not the number we are blocking for it's the number we aren't blocking for.
    
    How about ... `startup_max_down_local_dc_peers` and `startup_max_down_peers`?
    
    

;16/Oct/18 22:44;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225732092
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -39,7 +42,9 @@
     import org.apache.cassandra.locator.InetAddressAndPort;
     import org.apache.cassandra.net.async.OutboundConnectionIdentifier.ConnectionType;
     import org.apache.cassandra.utils.FBUtilities;
    +import org.jboss.byteman.synchronization.CountDown;
    --- End diff --
    
    ack
;16/Oct/18 22:44;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225732818
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
    --- End diff --
    
    Yea that was my idea, that we should always prime the connections but we can just not wait (if you set the local dc option to a really large number then it basically primes and immediately returns).
;16/Oct/18 22:47;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225733894
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    +    public int block_for_peers_all_dcs = Integer.MAX_VALUE;
    --- End diff --
    
    I agree and 10s isn't that big a deal (imo) for users that don't want to wait for remote dcs, but I was trying to compromise between the old settings (70% == could literally have a whole DC down with 3 DCs) which appear to be motivated from a perspective of ""don't block my startup"" and what I personally think that we shouldn't be saying we're ready to coordinate until we're actually ready to coordinate.
    
    I am happy to put whatever default gets this merged ;-) We'll probably internally be setting the local setting to `1` (so `LOCAL_QUORUM`) and the remote to a really large number. But that's just our perspective...
;16/Oct/18 22:53;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225734481
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    -
    -        if (peers.isEmpty())
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        if (peers.size() == 1 && peers.contains(localAddress))
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        Set<InetAddressAndPort> myLocalPeers = peers.stream()
    +                                                    .collect(Collectors.groupingBy(getDatacenter, toSet()))
    +                                                    .getOrDefault(getDatacenter.apply(FBUtilities.getBroadcastAddressAndPort()),
    +                                                                  Collections.emptySet());
    +
    +
    +        logger.info(""choosing to block until no more than {}/{} local and no more than {}/{} global peers are still DOWN; max time to wait = {} seconds"",
    --- End diff --
    
    Yea ... I was debating if this should be here or down below 122 where we remove it. I was thinking that users would want to see their cluster size in the denominator (e.g. `choosing to block until no more than 1/200` nodes) even if strictly speaking we just immediately succeed on the local host.
    
    I don't have strong preferences, I'll take the local node out of the count.
;16/Oct/18 22:56;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225734572
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -17,13 +17,16 @@
      */
     package org.apache.cassandra.net;
     
    +import java.util.Collections;
     import java.util.HashSet;
     import java.util.Map;
     import java.util.Set;
     import java.util.concurrent.ConcurrentHashMap;
     import java.util.concurrent.CountDownLatch;
     import java.util.concurrent.TimeUnit;
     import java.util.concurrent.atomic.AtomicInteger;
    +import java.util.function.Function;
    +import java.util.stream.Collectors;
     
     import com.google.common.annotations.VisibleForTesting;
     import com.google.common.collect.Sets;
    --- End diff --
    
    ack
;16/Oct/18 22:56;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225734880
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -213,4 +261,11 @@ boolean incrementAndCheck(InetAddressAndPort address)
                 return acks.computeIfAbsent(address, addr -> new AtomicInteger(0)).incrementAndGet() == threshold;
             }
         }
    +
    +    private String fmtBlocker(Integer size, Integer count)
    --- End diff --
    
    Heh, yea it is I just thought that it was slightly clearer what it was doing (so we don't print `Integer.MAX_VALUE` to the log. I'll change it to `Math.max` instead.
;16/Oct/18 22:58;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r225735927
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    +    public int block_for_peers_all_dcs = Integer.MAX_VALUE;
    --- End diff --
    
    This one of those things where I want to ask around and see if we can get away with just waiting 10 seconds if a node is down.
    
    Seems like the solution for CCM is to just have CCM set the config it wants. I'm also wondering if CCM actually waits the 10 seconds anyways just because it starts one node at a time and waits for a specific log message a lot of the time and I'm not sure if that message is before or after the connectivity checker.
    
    I have to go take a look at how CCM works. 
;16/Oct/18 23:04;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r228386047
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
    --- End diff --
    
    In the latest version I added the ability to disable this by setting a negative timeout. I guess it's probably a good idea to have an opt out option since we're post freeze.
;26/Oct/18 01:52;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r228386128
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    +    public int block_for_peers_all_dcs = Integer.MAX_VALUE;
    --- End diff --
    
    I played around with CCM and it appears to work fine (starting a cluster, stopping it, starting and stopping nodes etc). I think we should be good on this front.
;26/Oct/18 01:53;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r228386194
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -73,56 +80,84 @@ public static StartupClusterConnectivityChecker create(int targetPercent, int ti
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenter)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    -
    -        if (peers.isEmpty())
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        if (peers.size() == 1 && peers.contains(localAddress))
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        Set<InetAddressAndPort> myLocalPeers = peers.stream()
    +                                                    .collect(Collectors.groupingBy(getDatacenter, toSet()))
    +                                                    .getOrDefault(getDatacenter.apply(FBUtilities.getBroadcastAddressAndPort()),
    +                                                                  Collections.emptySet());
    +
    +
    +        logger.info(""choosing to block until no more than {}/{} local and no more than {}/{} global peers are still DOWN; max time to wait = {} seconds"",
    --- End diff --
    
    I changed it to remove the local peer like before :-)
;26/Oct/18 01:53;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r228386285
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -213,4 +261,11 @@ boolean incrementAndCheck(InetAddressAndPort address)
                 return acks.computeIfAbsent(address, addr -> new AtomicInteger(0)).incrementAndGet() == threshold;
             }
         }
    +
    +    private String fmtBlocker(Integer size, Integer count)
    --- End diff --
    
    I killed it.
;26/Oct/18 01:54;githubbot;600","Github user jolynch commented on the issue:

    https://github.com/apache/cassandra/pull/212
  
    Ok, I've rebased and incorporated the feedback from IRC/jira as well. Please let me know if there are further changes I need to make. 
;26/Oct/18 01:55;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229106050
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -380,9 +380,23 @@
         public RepairCommandPoolFullStrategy repair_command_pool_full_strategy = RepairCommandPoolFullStrategy.queue;
         public int repair_command_pool_size = concurrent_validations;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially considers all other peers as DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how long we wait for peers to
    +     * connect before we make this node available as a coordinator. Furthermore, if this feature is enabled
    +     * (timeout >= 0) Cassandra initiates the non gossip channel internode connections on startup as well and waits
    --- End diff --
    
    This part of the description doesn't match the description for block_for_peers_timeot_in_secs. It sounds like now it differentiates between disabled, and prime the connections.
;29/Oct/18 21:31;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229107287
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    --- End diff --
    
    Should this be a warning? They have pretty clearly asked for it?
;29/Oct/18 21:35;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229110247
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    --- End diff --
    
    Could this be a set multimap?
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229113284
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
     
             for (InetAddressAndPort peer : peers)
    +        {
                 if (Gossiper.instance.isAlive(peer) && alivePeers.add(peer) && acks.incrementAndCheck(peer))
    -                latch.countDown();
    +            {
    +                String datacenter = getDatacenter.apply(peer);
    +                if (latchMap.containsKey(datacenter))
    +                    latchMap.get(datacenter).countDown();
    +            }
    +        }
    +
    +        boolean succeeded = Uninterruptibles.awaitUninterruptibly(latchMap.get(localDc), timeoutNanos, TimeUnit.NANOSECONDS);
    +        for (String datacenter: latchMap.keySet())
    +        {
    +            if (datacenter.equals(localDc))
    --- End diff --
    
    Then you don't need this exception?
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229113240
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
     
             for (InetAddressAndPort peer : peers)
    +        {
                 if (Gossiper.instance.isAlive(peer) && alivePeers.add(peer) && acks.incrementAndCheck(peer))
    -                latch.countDown();
    +            {
    +                String datacenter = getDatacenter.apply(peer);
    +                if (latchMap.containsKey(datacenter))
    +                    latchMap.get(datacenter).countDown();
    +            }
    +        }
    +
    +        boolean succeeded = Uninterruptibles.awaitUninterruptibly(latchMap.get(localDc), timeoutNanos, TimeUnit.NANOSECONDS);
    --- End diff --
    
    Why do you need the loop and this? Since you removed other DCs from peersByDC shouldn't there already be only the local DC in the latch map?
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229113469
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    --- End diff --
    
    Another candidate for datacenterToY. Not quite sure of the perfect name for Y. I am fine with using dc if you want shorter.
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229114775
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    --- End diff --
    
    This important bit of behavior is tucked away here and I only know about it because I went looking. Can you add a mostly redundant comment mentioning that you remove all but the local DC so that we only wait on nodes in the local DC?
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229120761
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -36,13 +36,34 @@
     import org.apache.cassandra.gms.Gossiper;
     import org.apache.cassandra.gms.HeartBeatState;
     import org.apache.cassandra.locator.InetAddressAndPort;
    +import org.apache.cassandra.utils.FBUtilities;
     
     import static org.apache.cassandra.net.async.OutboundConnectionIdentifier.ConnectionType.SMALL_MESSAGE;
     
     public class StartupClusterConnectivityCheckerTest
     {
    -    private StartupClusterConnectivityChecker connectivityChecker;
    +    private StartupClusterConnectivityChecker localQuorumConnectivityChecker;
    +    private StartupClusterConnectivityChecker globalQuorumConnectivityChecker;
    +    private StartupClusterConnectivityChecker noopChecker;
    +    private StartupClusterConnectivityChecker zeroWaitChecker;
    +
    +    private static final int NUM_PER_DC = 6;
         private Set<InetAddressAndPort> peers;
    +    private Set<InetAddressAndPort> peersA;
    +    private Set<InetAddressAndPort> peersAMinusLocal;
    +    private Set<InetAddressAndPort> peersB;
    +    private Set<InetAddressAndPort> peersC;
    +
    +    private String getDatacenter(InetAddressAndPort endpoint)
    +    {
    +        if (peersA.contains(endpoint))
    +            return ""datacenterA"";
    +        if (peersB.contains(endpoint))
    +            return ""datacenterB"";
    +        else if (peersC.contains(endpoint))
    +            return ""datacenterC"";
    +        return ""NA"";
    --- End diff --
    
    Should probably return null
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229120810
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -69,33 +113,102 @@ public void tearDown()
         @Test
         public void execute_HappyPath()
         {
    -        Sink sink = new Sink(true, true);
    +        Sink sink = new Sink(true, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertTrue(connectivityChecker.execute(peers));
    +        Assert.assertTrue(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NotAlive()
         {
    -        Sink sink = new Sink(false, true);
    +        Sink sink = new Sink(false, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NoConnectionsAcks()
         {
    -        Sink sink = new Sink(true, false);
    +        Sink sink = new Sink(true, false, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
    +    }
    +
    +    @Test
    +    public void execute_LocalQuorum()
    +    {
    +        // local peer plus 3 peers from same dc shouldn't pass (4/6)
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 3);
    +        checkAvailable(localQuorumConnectivityChecker, available, false, true);
    +
    +        // local peer plus 4 peers from same dc should pass (5/6)
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        checkAvailable(localQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_GlobalQuorum()
    +    {
    +        // local dc passing shouldn'nt pass globally with two hosts down in datacenterB
    --- End diff --
    
    typo
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229121401
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -69,33 +113,102 @@ public void tearDown()
         @Test
         public void execute_HappyPath()
         {
    -        Sink sink = new Sink(true, true);
    +        Sink sink = new Sink(true, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertTrue(connectivityChecker.execute(peers));
    +        Assert.assertTrue(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NotAlive()
         {
    -        Sink sink = new Sink(false, true);
    +        Sink sink = new Sink(false, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NoConnectionsAcks()
         {
    -        Sink sink = new Sink(true, false);
    +        Sink sink = new Sink(true, false, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
    +    }
    +
    +    @Test
    +    public void execute_LocalQuorum()
    +    {
    +        // local peer plus 3 peers from same dc shouldn't pass (4/6)
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 3);
    +        checkAvailable(localQuorumConnectivityChecker, available, false, true);
    +
    +        // local peer plus 4 peers from same dc should pass (5/6)
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        checkAvailable(localQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_GlobalQuorum()
    +    {
    +        // local dc passing shouldn'nt pass globally with two hosts down in datacenterB
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        copyCount(peersB, available, NUM_PER_DC - 2);
    +        copyCount(peersC, available, NUM_PER_DC - 1);
    +        checkAvailable(globalQuorumConnectivityChecker, available, false, true);
    +
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC -2);
    +        copyCount(peersB, available, NUM_PER_DC - 1);
    +        copyCount(peersC, available, NUM_PER_DC - 1);
    +        checkAvailable(globalQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_Noop()
    +    {
    +        checkAvailable(noopChecker, new HashSet<>(), true, false);
    +    }
    +
    +    @Test
    +    public void execute_ZeroWait()
    +    {
    +        checkAvailable(zeroWaitChecker, new HashSet<>(), false, false);
    --- End diff --
    
    Maybe add a test that does zero wait, but also checks the connections are made?
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229114904
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
    --- End diff --
    
    Comment that pings are sent to all peers even if latchMap doesn't contain a latch for them.
;29/Oct/18 22:30;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r229109638
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    --- End diff --
    
    Bikeshedding, but datacenterMap is not very descriptive. What it is is a peerToDatacenter. You can also omit mpa it's kind of redundant with XtoY which automatically implies it's a map relationship. Multimaps are handles by making Y plural. I really think it's worth using better naming here.
    
    Also pulling out getDataCenter is a few characters more succinct, but I would rather see XtoY::get repeated a few times. If it's not referring to the source then the name also needs to be more representative like getDatacenterFromPeer which is basically just another way of saying peerToDatacenter::get.
;29/Oct/18 22:30;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231680735
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -380,9 +380,23 @@
         public RepairCommandPoolFullStrategy repair_command_pool_full_strategy = RepairCommandPoolFullStrategy.queue;
         public int repair_command_pool_size = concurrent_validations;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially considers all other peers as DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how long we wait for peers to
    +     * connect before we make this node available as a coordinator. Furthermore, if this feature is enabled
    +     * (timeout >= 0) Cassandra initiates the non gossip channel internode connections on startup as well and waits
    --- End diff --
    
    Ok, I reworded it. I think that having an escape hatch makes sense, what do you think? 
;07/Nov/18 21:20;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231681120
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    --- End diff --
    
    I thought it should be for the same reason we warn them if they set the timeout to more than 100, but I don't care too much. I removed it.
;07/Nov/18 21:21;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231688756
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    --- End diff --
    
    Sure thing, makes sense and I've made both refactors.
;07/Nov/18 21:46;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231688792
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    --- End diff --
    
    Sure, done.
;07/Nov/18 21:46;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231691984
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
     
             for (InetAddressAndPort peer : peers)
    +        {
                 if (Gossiper.instance.isAlive(peer) && alivePeers.add(peer) && acks.incrementAndCheck(peer))
    -                latch.countDown();
    +            {
    +                String datacenter = getDatacenter.apply(peer);
    +                if (latchMap.containsKey(datacenter))
    +                    latchMap.get(datacenter).countDown();
    +            }
    +        }
    +
    +        boolean succeeded = Uninterruptibles.awaitUninterruptibly(latchMap.get(localDc), timeoutNanos, TimeUnit.NANOSECONDS);
    --- End diff --
    
    I think the loop is requires because this main method can race with the callbacks that are triggered as part of {{sendPingMessages}}. There is no point in the {{containsKey}} though so I just refactored back to what was there before.
;07/Nov/18 21:55;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231693230
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
     
             for (InetAddressAndPort peer : peers)
    +        {
                 if (Gossiper.instance.isAlive(peer) && alivePeers.add(peer) && acks.incrementAndCheck(peer))
    -                latch.countDown();
    +            {
    +                String datacenter = getDatacenter.apply(peer);
    +                if (latchMap.containsKey(datacenter))
    +                    latchMap.get(datacenter).countDown();
    +            }
    +        }
    +
    +        boolean succeeded = Uninterruptibles.awaitUninterruptibly(latchMap.get(localDc), timeoutNanos, TimeUnit.NANOSECONDS);
    +        for (String datacenter: latchMap.keySet())
    +        {
    +            if (datacenter.equals(localDc))
    --- End diff --
    
    Makes sense, removed
;07/Nov/18 21:58;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231695413
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    --- End diff --
    
    I called it `dcToRemainingPeers` to try to convey what it was counting down. Let me know if that's not clear and I'll change it.
;07/Nov/18 22:05;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231695792
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    --- End diff --
    
    Ack, added the comment.
;07/Nov/18 22:06;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231696207
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    +                        "" the first user query"");
             if (timeoutSecs > 100)
                 logger.warn(""setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless"", timeoutSecs);
             long timeoutNanos = TimeUnit.SECONDS.toNanos(timeoutSecs);
     
    -        return new StartupClusterConnectivityChecker(targetPercent, timeoutNanos);
    +        return new StartupClusterConnectivityChecker(timeoutNanos, blockForRemoteDcs);
         }
     
         @VisibleForTesting
    -    StartupClusterConnectivityChecker(int targetPercent, long timeoutNanos)
    +    StartupClusterConnectivityChecker(long timeoutNanos, boolean blockForRemoteDcs)
         {
    -        this.targetPercent = Math.min(100, Math.max(0, targetPercent));
    +        this.blockForRemoteDcs = blockForRemoteDcs;
             this.timeoutNanos = timeoutNanos;
         }
     
         /**
          * @param peers The currently known peers in the cluster; argument is not modified.
    +     * @param getDatacenterSource A function for mapping peers to their datacenter.
          * @return true if the requested percentage of peers are marked ALIVE in gossip and have their connections opened;
          * else false.
          */
    -    public boolean execute(Set<InetAddressAndPort> peers)
    +    public boolean execute(Set<InetAddressAndPort> peers, Function<InetAddressAndPort, String> getDatacenterSource)
         {
    -        if (targetPercent == 0 || peers == null)
    +        if (peers == null || this.timeoutNanos < 0)
                 return true;
     
             // make a copy of the set, to avoid mucking with the input (in case it's a sensitive collection)
             peers = new HashSet<>(peers);
    -        peers.remove(FBUtilities.getBroadcastAddressAndPort());
    +        InetAddressAndPort localAddress = FBUtilities.getBroadcastAddressAndPort();
    +        String localDc = getDatacenterSource.apply(localAddress);
     
    +        peers.remove(localAddress);
             if (peers.isEmpty())
                 return true;
     
    -        logger.info(""choosing to block until {}% of the {} known peers are marked alive and connections are established; max time to wait = {} seconds"",
    -                    targetPercent, peers.size(), TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        // make a copy of the datacenter mapping (in case gossip updates happen during this method or some such)
    +        Map<InetAddressAndPort, String> datacenterMap = peers.stream()
    +                                                             .collect(Collectors.toMap(k -> k, getDatacenterSource));
    +        Function<InetAddressAndPort, String> getDatacenter = datacenterMap::get;
     
    -        long startNanos = System.nanoTime();
    +        Map<String, Set<InetAddressAndPort>> peersByDc = peers.stream()
    +                                                              .collect(Collectors.groupingBy(getDatacenter,
    +                                                                                             Collectors.toSet()));
    +
    +        if (!blockForRemoteDcs)
    +        {
    +            peersByDc.keySet().retainAll(Collections.singleton(localDc));
    +            logger.info(""Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
    +        else
    +        {
    +            logger.info(""Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s"",
    +                        TimeUnit.NANOSECONDS.toSeconds(timeoutNanos));
    +        }
     
             AckMap acks = new AckMap(3);
    -        int target = (int) ((targetPercent / 100.0) * peers.size());
    -        CountDownLatch latch = new CountDownLatch(target);
    +        Map<String, CountDownLatch> latchMap = new HashMap<>(peersByDc.size());
    +        for (Map.Entry<String, Set<InetAddressAndPort>> entry: peersByDc.entrySet())
    +        {
    +            latchMap.put(entry.getKey(), new CountDownLatch(Math.max(entry.getValue().size() - 1, 0)));
    +        }
    +
    +        long startNanos = System.nanoTime();
     
             // set up a listener to react to new nodes becoming alive (in gossip), and account for all the nodes that are already alive
    -        Set<InetAddressAndPort> alivePeers = Sets.newSetFromMap(new ConcurrentHashMap<>());
    -        AliveListener listener = new AliveListener(alivePeers, latch, acks);
    +        Set<InetAddressAndPort> alivePeers = Collections.newSetFromMap(new ConcurrentHashMap<>());
    +        AliveListener listener = new AliveListener(alivePeers, latchMap, acks, getDatacenter);
             Gossiper.instance.register(listener);
     
             // send out a ping message to open up the non-gossip connections
    -        sendPingMessages(peers, latch, acks);
    +        sendPingMessages(peers, latchMap, acks, getDatacenter);
    --- End diff --
    
    Done.
;07/Nov/18 22:07;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231696993
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -36,13 +36,34 @@
     import org.apache.cassandra.gms.Gossiper;
     import org.apache.cassandra.gms.HeartBeatState;
     import org.apache.cassandra.locator.InetAddressAndPort;
    +import org.apache.cassandra.utils.FBUtilities;
     
     import static org.apache.cassandra.net.async.OutboundConnectionIdentifier.ConnectionType.SMALL_MESSAGE;
     
     public class StartupClusterConnectivityCheckerTest
     {
    -    private StartupClusterConnectivityChecker connectivityChecker;
    +    private StartupClusterConnectivityChecker localQuorumConnectivityChecker;
    +    private StartupClusterConnectivityChecker globalQuorumConnectivityChecker;
    +    private StartupClusterConnectivityChecker noopChecker;
    +    private StartupClusterConnectivityChecker zeroWaitChecker;
    +
    +    private static final int NUM_PER_DC = 6;
         private Set<InetAddressAndPort> peers;
    +    private Set<InetAddressAndPort> peersA;
    +    private Set<InetAddressAndPort> peersAMinusLocal;
    +    private Set<InetAddressAndPort> peersB;
    +    private Set<InetAddressAndPort> peersC;
    +
    +    private String getDatacenter(InetAddressAndPort endpoint)
    +    {
    +        if (peersA.contains(endpoint))
    +            return ""datacenterA"";
    +        if (peersB.contains(endpoint))
    +            return ""datacenterB"";
    +        else if (peersC.contains(endpoint))
    +            return ""datacenterC"";
    +        return ""NA"";
    --- End diff --
    
    Right, so it'll fail fast, done.
;07/Nov/18 22:09;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231697341
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -69,33 +113,102 @@ public void tearDown()
         @Test
         public void execute_HappyPath()
         {
    -        Sink sink = new Sink(true, true);
    +        Sink sink = new Sink(true, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertTrue(connectivityChecker.execute(peers));
    +        Assert.assertTrue(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NotAlive()
         {
    -        Sink sink = new Sink(false, true);
    +        Sink sink = new Sink(false, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NoConnectionsAcks()
         {
    -        Sink sink = new Sink(true, false);
    +        Sink sink = new Sink(true, false, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
    +    }
    +
    +    @Test
    +    public void execute_LocalQuorum()
    +    {
    +        // local peer plus 3 peers from same dc shouldn't pass (4/6)
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 3);
    +        checkAvailable(localQuorumConnectivityChecker, available, false, true);
    +
    +        // local peer plus 4 peers from same dc should pass (5/6)
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        checkAvailable(localQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_GlobalQuorum()
    +    {
    +        // local dc passing shouldn'nt pass globally with two hosts down in datacenterB
    --- End diff --
    
    Ack, fixed.
;07/Nov/18 22:10;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231705256
  
    --- Diff: test/unit/org/apache/cassandra/net/StartupClusterConnectivityCheckerTest.java ---
    @@ -69,33 +113,102 @@ public void tearDown()
         @Test
         public void execute_HappyPath()
         {
    -        Sink sink = new Sink(true, true);
    +        Sink sink = new Sink(true, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertTrue(connectivityChecker.execute(peers));
    +        Assert.assertTrue(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NotAlive()
         {
    -        Sink sink = new Sink(false, true);
    +        Sink sink = new Sink(false, true, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
             checkAllConnectionTypesSeen(sink);
         }
     
         @Test
         public void execute_NoConnectionsAcks()
         {
    -        Sink sink = new Sink(true, false);
    +        Sink sink = new Sink(true, false, peers);
             MessagingService.instance().addMessageSink(sink);
    -        Assert.assertFalse(connectivityChecker.execute(peers));
    +        Assert.assertFalse(localQuorumConnectivityChecker.execute(peers, this::getDatacenter));
    +    }
    +
    +    @Test
    +    public void execute_LocalQuorum()
    +    {
    +        // local peer plus 3 peers from same dc shouldn't pass (4/6)
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 3);
    +        checkAvailable(localQuorumConnectivityChecker, available, false, true);
    +
    +        // local peer plus 4 peers from same dc should pass (5/6)
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        checkAvailable(localQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_GlobalQuorum()
    +    {
    +        // local dc passing shouldn'nt pass globally with two hosts down in datacenterB
    +        Set<InetAddressAndPort> available = new HashSet<>();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC - 2);
    +        copyCount(peersB, available, NUM_PER_DC - 2);
    +        copyCount(peersC, available, NUM_PER_DC - 1);
    +        checkAvailable(globalQuorumConnectivityChecker, available, false, true);
    +
    +        available.clear();
    +        copyCount(peersAMinusLocal, available, NUM_PER_DC -2);
    +        copyCount(peersB, available, NUM_PER_DC - 1);
    +        copyCount(peersC, available, NUM_PER_DC - 1);
    +        checkAvailable(globalQuorumConnectivityChecker, available, true, true);
    +    }
    +
    +    @Test
    +    public void execute_Noop()
    +    {
    +        checkAvailable(noopChecker, new HashSet<>(), true, false);
    +    }
    +
    +    @Test
    +    public void execute_ZeroWait()
    +    {
    +        checkAvailable(zeroWaitChecker, new HashSet<>(), false, false);
    --- End diff --
    
    Makes sense, I refactored this test to look at that instead of just returning immediately.
;07/Nov/18 22:37;githubbot;600","Github user jolynch commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r231711085
  
    --- Diff: src/java/org/apache/cassandra/config/Config.java ---
    @@ -376,9 +376,31 @@
     
         public String full_query_log_dir = null;
     
    -    // parameters to adjust how much to delay startup until a certain amount of the cluster is connect to and marked alive
    -    public int block_for_peers_percentage = 70;
    +    /**
    +     * When a node first starts up it intially thinks all other peers are DOWN, and then as the initial gossip
    +     * broadcast messages comes back nodes transition to UP. These options configure how many nodes can remain in
    +     * DOWN state before we make this node available as a coordinator, as well as an overall timeout on this process
    +     * to ensure that startup is not delayed too much.
    +     *
    +     * The defaults are tuned for LOCAL_ONE consistency levels with RF=3, and have natural settings for other CLs:
    +     *
    +     *     Consistency Level | local_dc     | all_dcs
    +     *     --------------------------------------------------------
    +     *     LOCAL_ONE         | default (2)  | default (any)
    +     *     LOCAL_QUORUM      | 1            | default (any)
    +     *     ONE               | any          | RF - 1
    +     *     QUORUM            | any          | (RF / 2) - 1
    +     *     ALL               | default      | 0
    +     *
    +     * A concrete example with QUORUM would be if you have 3 replicas in 2 datacenters, then you would set
    +     * block_for_peers_all_dcs to (6 / 2) - 1 = 2 because that guarantees that at most 2 hosts in all datacenters
    +     * are down when you start taking client traffic, which should satistfy QUORUM for all RF=6 QUORUM queries.
    +     */
    +    public int block_for_peers_local_dc = 2;
    --- End diff --
    
    We ended up going the direction of a boolean.
;07/Nov/18 23:00;githubbot;600","Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/212#discussion_r232731480
  
    --- Diff: src/java/org/apache/cassandra/net/StartupClusterConnectivityChecker.java ---
    @@ -48,81 +51,133 @@
     {
         private static final Logger logger = LoggerFactory.getLogger(StartupClusterConnectivityChecker.class);
     
    -    private final int targetPercent;
    +    private final boolean blockForRemoteDcs;
         private final long timeoutNanos;
     
    -    public static StartupClusterConnectivityChecker create(int targetPercent, int timeoutSecs)
    +    public static StartupClusterConnectivityChecker create(long timeoutSecs, boolean blockForRemoteDcs)
         {
    -        timeoutSecs = Math.max(1, timeoutSecs);
    +        if (timeoutSecs < 0)
    +            logger.warn(""skipping block-for-peers due to negative timeout. You may encounter errors or timeouts on"" +
    --- End diff --
    
    The reason I thought it was strange was because -1 was a valid config option.
;12/Nov/18 16:43;githubbot;600","Github user jolynch closed the pull request at:

    https://github.com/apache/cassandra/pull/212
;12/Nov/18 23:27;githubbot;600","Github user jolynch commented on the issue:

    https://github.com/apache/cassandra/pull/212
  
    Closing as this was committed in https://github.com/apache/cassandra/commit/801cb70ee811c956e987718a00695638d5bec1b6
;12/Nov/18 23:27;githubbot;600",,0,25200,,,0,25200,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-27 01:01:27.871,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 12 18:45:42 UTC 2018,,,,,,0|i3r0lb:,9223372036854775807,,,,,,,,aweisberg,aweisberg,,,,,,,,,,"27/Mar/18 01:01;githubbot;GitHub user jolynch opened a pull request:

    https://github.com/apache/cassandra/pull/212

    Rework the wait for healthy logic to count down nodes for CASSANDRA-14297

    This improves on the wait for healthy work from CASSANDRA-13993 to
    solve CASSANDRA-14297. It allows the cluster owners to fine tune the
    wait for behaviour to ensure availability of their application during
    rolling restarts. The defaults are to wait for all but one local DC host
    to be alive and up, and not care about remote DC hosts.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jolynch/cassandra CASSANDRA-14297

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/212.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #212
    
----
commit 74373297ae80b44938de5c04078b391f04d63b79
Author: Joseph Lynch <joe.e.lynch@...>
Date:   2018-02-21T05:12:21Z

    Rework the wait for healthy logic to count down nodes
    
    This improves on the wait for healthy work from CASSANDRA-13993 to
    solve CASSANDRA-14297. It allows the cluster owners to fine tune the
    wait for behaviour to ensure availability of their application during
    rolling restarts. The defaults are to wait for all but one local DC host
    to be alive and up, and not care about remote DC hosts.

----
","27/Mar/18 01:05;jolynch;First implementation up on github along with a lot of unit tests. I'll start doing some more e2e testing using ccm just to make sure all the edge cases are covered but if someone ([~aweisberg] or [~jasobrown] perhaps) wants to review that would be excellent.
||trunk||
|[pull request|https://github.com/apache/cassandra/pull/212]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14297.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14297] |

 ",23/May/18 16:01;sumanth.pasupuleti;LGTM. +1,26/May/18 00:17;jolynch;Looks like CASSANDRA-14447 refactoring has given me a nasty merge conflict. I'll work on rebasing the patchset but if a someone has time to give me quick feedback on if this idea is mergeable I'd appreciate it before investing more time into it. I do think this change makes the connectivity checker feature much more useful for operators trying to restart their databases without dropping traffic.,"24/Aug/18 00:46;jolynch;I updated the patch to fix the merge conflicts, and reduced it two just two options to make life easier (the default is tuned to wait for all but 2 local DC nodes and not care about non local DC == local_quorum).

This is ready for review. I hope we get it in before 4.0 because the user interface of a percentage is something users can't reliably set (compared to the count where there are correct answers for each use case).",26/Sep/18 17:18;jolynch;I'm changing this to a bug since I think the current user interface is not possible for users to correctly configure and I hope we don't ship 4.0 with the percentage option instead of a count. If someone thinks that there are plausible settings of the existing configuration options users can use we can change this back to an improvement.,"25/Oct/18 00:16;jolynch;Alright, per the discussion on [IRC|https://wilderness.apache.org/channels/?f=cassandra-dev/2018-10-17#1539793033] with Ariel and Jason, we've decided that instead of counts we should always wait for all but a single local DC node and replace the percentage option with:
{noformat}
block_for_remote_dcs: <boolean, default: false>
{noformat}
The startup connectivity checker will wait for all but a single node in the local datacenter, and if you want to block startup on every datacenter having only a single node down you can set this to true.

The timeout will be the fallback for when multiple nodes are down in a local DC.",26/Oct/18 01:59;jolynch;Ok I've uploaded a patch to my branch that does what was asked in IRC I believe. Let me know if it looks good and I can run dtests and such against it.,01/Nov/18 14:51;aweisberg;I left some comments on the PR. Looks good.,"07/Nov/18 23:01;jolynch;[~aweisberg] Awesome I just rebased and merged in your suggestions:

||trunk||
|[fdd8173f|https://github.com/apache/cassandra/pull/212/commits/fdd8173f]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14297.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14297] |

Dtests are running.","12/Nov/18 17:27;aweisberg;+1 🚢 it.

There is one unused import in [StartupClusterConnectivityCheckerTest.java|https://github.com/apache/cassandra/pull/212/files#diff-c74adeeae072ee4af35c12a157cd7d61L26] I'll fix on commit.","12/Nov/18 17:47;aweisberg;Committed as [801cb70ee811c956e987718a00695638d5bec1b6|https://github.com/apache/cassandra/commit/801cb70ee811c956e987718a00695638d5bec1b6] thanks!

I also added a NEWS.txt and CHANGES.txt entries. I also added Patchy by XYZ; Reviewed by XYZ for CASSANDRA-14297 to the commit message.","12/Nov/18 18:45;jolynch;Sweet, thanks! Yea I was holding off on adding the NEWs/CHANGES entries until you marked it ready to commit. In the future I'll include it with the dtest run.

Thanks for all the great feedback, I think this feature is much more valuable now to users.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress writes even data when n=0,CASSANDRA-13773,13095404,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,eduard.tudenhoefner,eduard.tudenhoefner,eduard.tudenhoefner,17/Aug/17 17:01,12/Mar/19 14:03,13/Mar/19 22:35,22/Aug/17 01:40,3.0.15,3.11.1,4.0,,,Tool/stress,,,,,0,,,,"This is very unintuitive as
{code}
cassandra-stress write n=0 -rate threads=1
{code}
will do inserts even with *n=0*. I guess most people won't ever run with *n=0* but this is a nice shortcut for creating some schema without using *cqlsh*

This is happening because we're writing *50k* rows of warmup data as can be seen below:
{code}
cqlsh> select count(*) from keyspace1.standard1 ;

 count
-------
 50000

(1 rows)
{code}

We can avoid writing warmup data using 
{code}
cassandra-stress write n=0 no-warmup -rate threads=1
{code}

but I would still expect to have *0* rows written when specifying *n=0*.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-21 02:55:04.616,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 22 01:40:04 UTC 2017,,,,,,0|i3ix0f:,9223372036854775807,3.0.14,,,,,,,Stefania,Stefania,,,,,,,,,,"17/Aug/17 17:13;eduard.tudenhoefner;Branch: https://github.com/nastra/cassandra/tree/CASSANDRA-13773-30
Test: https://circleci.com/gh/nastra/cassandra/12","21/Aug/17 02:55;Stefania;The patch LGTM however, whilst it doesn't make sense to perform warmup when n=0, it still changes the behavior seen by the user. Therefore, I am not entirely sure this patch should go into 3.0, any thoughts [~tjake]?

Regarding CI, I think what we care is that dtests using cassandra-stress still work, I don't think cassandra-stress impacts unit tests at all.","21/Aug/17 03:21;Stefania;I've started the dtests on our internal CI, if there is a way to run them on CircleCI please launch them and I haven't set it up yet.

I've tested a bit locally as well, and I think we are better off skipping the command entirely when {{n=0}}, not just the warm-up, otherwise {{cassandra-stress write n=0}} (with no rate specified) will loop over different rates and sleep for no good reason whatsoever. So I suggest that we create the schema and then exit, see [here|https://github.com/apache/cassandra/compare/trunk...stef1927:13773-3.0#diff-fd2f2d2364937fcb1c0d73c8314f1418R57].",21/Aug/17 14:50;eduard.tudenhoefner;Skipping the command entirely sgtm,"22/Aug/17 01:40;Stefania;dtests looked good as well.

I don't think anyone would expect cassandra-stress to write data if n=0, so I've committed it to 3.0 as {{6a1b1f26b7174e8c9bf86a96514ab626ce2a4117}} and merged into 3.11 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Unittest: org.apache.cassandra.db.compaction.BlacklistingCompactionsTest,CASSANDRA-14238,13138972,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,jay.zhuang,jay.zhuang,16/Feb/18 04:54,12/Mar/19 14:03,13/Mar/19 22:35,07/Jun/18 15:46,2.2.13,,,,,Legacy/Testing,,,,,0,testing,,,"The unittest is flaky
{noformat}
    [junit] Testcase: testBlacklistingWithSizeTieredCompactionStrategy(org.apache.cassandra.db.compaction.BlacklistingCompactionsTest): FAILED
    [junit] expected:<8> but was:<25>
    [junit] junit.framework.AssertionFailedError: expected:<8> but was:<25>
    [junit]     at org.apache.cassandra.db.compaction.BlacklistingCompactionsTest.testBlacklisting(BlacklistingCompactionsTest.java:170)
    [junit]     at org.apache.cassandra.db.compaction.BlacklistingCompactionsTest.testBlacklistingWithSizeTieredCompactionStrategy(BlacklistingCompactionsTest.java:71)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-05-31 14:32:26.703,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 07 15:46:00 UTC 2018,,,,,,0|i3q9fb:,9223372036854775807,2.1.x,,,,,,,jay.zhuang,jay.zhuang,,,,,,,,,,"31/May/18 14:32;krummas;this only fails on 2.2, and most often because the DeletionTime is initialized with a negative local deletion time due to the randomness when corrupting the files. Can't reproduce on 3.0+

patch https://github.com/krummas/cassandra/commits/marcuse/14238 which hard codes the random seed to something I've tested works (ran the test in a loop for 10+ minutes)
test run on circle: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14238","07/Jun/18 01:45;jay.zhuang;+1

Thanks [~krummas] for the fix.

Seems the following positions will cause an assertion exception and fail the test:
{{2, 3, 114, 115}}.
[test code|https://github.com/cooldoger/cassandra/commit/32e01cf4d144625eeec207708d90242f195e21b9]. Command to test: 
{noformat}
$ for i in {0..180}; do echo == $i; ant test -Dtest.name=BlacklistingCompactionsTest -DTest.BadPos=$i >> /dev/null 2>&1 && echo ""okay"" || echo ""failed""; done
{noformat}

","07/Jun/18 15:46;krummas;committed as {{fc7a69b65399597a2bf9c6025f035f8fe26724c7}} to 2.2 and merged up with -s ours, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RPM package has too many executable files,CASSANDRA-14181,13132524,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,troels@arvin.dk,troels@arvin.dk,troels@arvin.dk,21/Jan/18 17:18,12/Mar/19 14:03,13/Mar/19 22:35,22/Jan/18 13:39,2.1.20,2.2.12,3.0.16,3.11.2,4.0,Packaging,,,,,0,,,,"When installing using the RPM files:
In /etc/cassandra/conf, the files should not be execuable, as they are either
 * properties-like files
 * readme-like files, or
 * files to be sourced by shell scripts

I'm adding a patch (cassandra-permissions-fix.patch) to the cassandra.spec file which fixes this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21/Jan/18 17:17;troels@arvin.dk;cassandra-permissions-fix.patch;https://issues.apache.org/jira/secure/attachment/12907014/cassandra-permissions-fix.patch,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-01-22 13:39:34.278,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,,,Mon Jan 22 13:39:34 UTC 2018,,,,,,0|i3p5pb:,9223372036854775807,,,,,,,,spodxx@gmail.com,spodxx@gmail.com,,,,,,,,,,"22/Jan/18 13:39;spodxx@gmail.com;Again, thanks for the patch!

I've tested it locally on Fedora 26 and the affected files are now installed and owned by root using 644 permissions, just as expected.

Committed as 5ba9e6da94ed74c11f6ea37199dbe8a501859e7c to 2.1 upwards.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't send new_metadata_id for conditional updates,CASSANDRA-13992,13116066,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ifesdjeen,omichallat,omichallat,03/Nov/17 21:35,12/Mar/19 14:03,13/Mar/19 22:35,21/Nov/17 12:17,4.0,,,,,,,,,,0,,,,"This is a follow-up to CASSANDRA-10786.

Given the table
{code}
CREATE TABLE foo (k int PRIMARY KEY)
{code}
And the prepared statement
{code}
INSERT INTO foo (k) VALUES (?) IF NOT EXISTS
{code}

The result set metadata changes depending on the outcome of the update:
* if the row didn't exist, there is only a single column \[applied] = true
* if it did, the result contains \[applied] = false, plus the current value of column k.

The way this was handled so far is that the PREPARED response contains no result set metadata, and therefore all EXECUTE messages have SKIP_METADATA = false, and the responses always include the full (and correct) metadata.

CASSANDRA-10786 still sends the PREPARED response with no metadata, *but the response to EXECUTE now contains a {{new_metadata_id}}*. The driver thinks it is because of a schema change, and updates its local copy of the prepared statement's result metadata.

The next EXECUTE is sent with SKIP_METADATA = true, but the server appears to ignore that, and still sends the metadata in the response. So each response includes the correct metadata, the driver uses it, and there is no visible issue for client code.

The only drawback is that the driver updates its local copy of the metadata unnecessarily, every time. We can work around that by only updating if we had metadata before, at the cost of an extra volatile read. But I think the best thing to do would be to never send a {{new_metadata_id}} in for a conditional update.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10786,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-07 11:23:38.553,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 08 19:03:46 UTC 2017,,,,,,0|i3mdyn:,9223372036854775807,4.x,,,,,,,snazy,snazy,,,,,,,,,,"07/Nov/17 11:23;KurtG;bq. The next EXECUTE is sent with SKIP_METADATA = true, but the server appears to ignore that
I believe this is because METADATA_CHANGED will take precedence. If C* thinks the metadata changed it will set the METADATA_CHANGED flag and the driver should need to update it's metadata. TBH this isn't super clear from the spec but appears to be what the code achieves [here|https://github.com/apache/cassandra/blob/922dbdb658b1693973926026b213153d05b4077c/src/java/org/apache/cassandra/transport/messages/ExecuteMessage.java#L174].

I may have no idea what I'm talking about but I think the simplest solution to 
bq. never send a new_metadata_id in for a conditional update.
would be to simply always use the same digest for any LWT.
I think the following patch achieves this without breaking anything but I haven't confirmed if it actually fixes the driver issue yet. If someone with more understanding of the protocol and what not could have a glance and let me know if this makes sense or point me in the right direction.
[trunk|https://github.com/apache/cassandra/compare/trunk...kgreav:13992-trunk]","08/Nov/17 18:12;omichallat;bq. If C* thinks the metadata changed it will set the METADATA_CHANGED flag and the driver should need to update it's metadata. TBH this isn't super clear from the spec but appears to be what the code achieves here.
That makes sense, and indeed explains why the server ignores SKIP_METADATA.

I've tested your patch against a driver snapshot. The bound statement executions now always return the same (empty) digest, but the problem is that the initial preparation still returns a non-empty digest. So the driver executes with that initial digest and gets METADATA_CHANGED with the empty digest every time.
The prepare should also return an empty digest, which I think should be done around [here|https://github.com/kgreav/cassandra/blob/fa259fd79ea3e0a0fac8583a54c9c76464a653be/src/java/org/apache/cassandra/cql3/QueryProcessor.java#L444].

cc [~ifesdjeen]","09/Nov/17 10:22;ifesdjeen;If we take this patch, I'd definitely constantize the ""empty"" hash, as very least.

Other than that - I think we should add more tests with LWTs (preferably dtests) and check what happens when we actually alter the table. Since in this case it seems what will happen is when table is ALTER'ed, metadata won't update (I haven't checked it though). In the initial patch we've tried always forcing metadata transfer for LWTs. If the patch has the same effect (metadata is transferred every time). I'm not insisting on any particular solution here though.  ","10/Nov/17 02:10;KurtG;So it seems that prepare does return an empty digest, however {{ResultSet.ResultMetadata.EMPTY}} is created with {{compute}} and is thus a thread local digest, which will get a different hash. From what I can see EMPTY is only ever used in one place worth mentioning, and that's in {{org.apache.cassandra.cql3.ResultSet.ResultMetadata#fromPrepared}} (it's also used in {{org.apache.cassandra.transport.Message.Codec#decode}} but only for protocol versions prior to V1), and will be used for anything that's not a {{SELECT}} statement. 

Now, what I'm wondering is there any significant reason the ID for an ""EMPTY"" metadata ID is created as a thread local digest?
If we can change empty to instead use {{MD5Digest.wrap()}} we solve the prepared problem.

[~omichallat] Before I waste a lot of time figuring out how to test this in the driver, can you point me at how you did it?

bq. Since in this case it seems what will happen is when table is ALTER'ed, metadata won't update (I haven't checked it though)
Yes we'll probably need to do something about this. I'll write up some tests first. 
","13/Nov/17 03:26;omichallat;bq. Before I waste a lot of time figuring out how to test this in the driver, can you point me at how you did it?

The driver-side changes are not merged yet, but you can find them in [this pull request|https://github.com/datastax/java-driver/pull/794]. The test that covers this specific scenario is [PreparedStatementInvalidationTest#should_never_update_statement_id_for_conditional_updates_in_modern_protocol|https://github.com/datastax/java-driver/blob/46825e446ae9d5f57baeb4f5f2c1f5fc4b99d972/driver-core/src/test/java/com/datastax/driver/core/PreparedStatementInvalidationTest.java#L182].

To run the test you'll need to install CCM (see some instructions [here|https://github.com/datastax/java-driver/blob/3.3.x/CONTRIBUTING.md#running-the-tests]). Then because you're not testing a released Cassandra version, you'll need to point the test harness to your local working copy with those system properties: {{-Dcassandra.version=4.0.0 -Dcassandra.directory=/path/to/cassandra}}

If you want to do some debugging, the result metadata is handled in {{ArrayBackedResultSet.java}}. Search for ""CASSANDRA-13992"", there is a comment that explains what should be changed if this ticket is fixed.","13/Nov/17 08:01;ifesdjeen;I've composed a version of the patch, to demonstrate my thinking [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-13992]. It seems that we can solve this problem without patching the driver. In fact, it might be even better if inner doings of metadata hash are transparent for the driver.

In short, we can always force {{METADATA_CHANGED}} for conditional statements and avoid computing their metadata to make sure it's empty. It's a rough equivalent of making metadata hash random, just simpler to reason about. What do you think about it [~KurtG] [~omichallat]","13/Nov/17 17:17;omichallat;[~ifesdjeen] that would work, the driver can treat an empty {{new_metadata_id}} as ""don't update my local copy"". Namely, changing [this line|https://github.com/datastax/java-driver/blob/6eeb8b2193ab5b50b73b0d9a533e775265f11007/driver-core/src/main/java/com/datastax/driver/core/ArrayBackedResultSet.java#L83] to:
{code}
if (newMetadataId != null && newMetadataId.bytes.length > 0) {
{code}
However that feels kind of hacky. Consider how we would have to explain that in the protocol spec:
{quote}
        - <new_metadata_id> is \[short bytes] representing the new, changed resultset
           metadata. The new metadata ID must also be used in subsequent executions of
           the corresponding prepared statement, if any, *except if it is empty*.
{quote}
It would make so much more sense to force {{METADATA_CHANGED}} to *false* for conditional updates, isn't there any way we can do that?","13/Nov/17 19:42;ifesdjeen;[~omichallat] not sure, since {{METADATA_CHANGED}} is just a flag: e.g. if it's set it's {{true}}, otherwise it's {{false}}. Moreover, I think that the default behaviour for LWTs has to be that we _always_ update metadata: there's no way for server to know what was the last metadata on the client (since it depends on the result), the server can't distinguish between the metadata hash inequality caused by {{ALTER}} vs caused by success/non-success LWT result.

Unless I'm missing something, my patch achieves exactly that (also, without any driver changes): it forces the server to _always_ send the metadata. This, combined with the metadata consisting of zeroes can instruct the client that caching metadata is possible, but won't bring anything: new result metadata will just be re-delivered on every call, since it's potentially going to be changing on every request.

I haven't updated spec though. I will, if/when we agree on the behaviour.","13/Nov/17 20:09;omichallat;{{METADATA_CHANGED}} tells the client if it needs to update its local copy of the metadata. For conditional updates, the answer is always no (since the client should never store that information in the first place); that is why I think it's more intuitive to set the flag to false.

To put it another way: if the flag is forced to true, I have to add a condition in the client code ({{newMetadataId.bytes.length > 0}}). My worry is that a client implementation could forget to check that the id is empty, and end up with a sub-optimal behavior (that updates the local metadata unnecessarily each time).

If the flag is absent, conditional updates can be handled like any other statement.

","14/Nov/17 03:51;KurtG;My understanding is that, at the moment, {{METADATA_CHANGED}} will _always_ be set for a conditional update, regardless of whether it's necessary or not. Necessary being defined as the schema has actually changed and the prepared statements need to be updated client side to reflect those schema changes. [~omichallat] is this true? what exactly is ""metadata"" referring to on the driver side, and why is the answer ""always no"" for conditional updates? If there is a change to one of the columns in the update is that going to cause problems if we don't tell the driver that it has changed?

I'm with Olivier that that's a hacky addition to the driver, but if it's not even necessary as per above then simply only passing an empty digest will be sufficient.

I've updated my [branch|https://github.com/apache/cassandra/compare/trunk...kgreav:13992] to reflect this. Note I've changed to using {{MD5Digest#compute}} to calculate an ""empty"" digest. Although it's thread local it will always be the same digest, and this will also solve the initial preparation problem, as it also uses the {{EMPTY}} resultset + metadata.



","14/Nov/17 08:39;ifesdjeen;bq. METADATA_CHANGED tells the client if it needs to update its local copy of the metadata. 

You're right, great point. Sure, I've changed the patch to _always_ send metadata (by avoiding setting {{SKIP_METADATA}} for LWTs) and _never_ send metadata id when statement is LWT (by avoiding setting {{METADATA_CHANGED}} for LWTs)). Technically, {{EMPTY}} part isn't even necessary in that case, but we don't have to calculate it, so why not. 

[~KurtG] to give a bit of context, {{METADATA_CHANGED}} flag is instructing the driver to cache a newly received version of metadata (alongside with a new metadata ID). While {{SKIP_METADATA}} flag is hinting the driver to use already cached metadata from previous responses. If I understood it correctly, what [~omichallat] proposed here was to avoid setting {{METADATA_CHANGED}} flag, so driver wouldn't cache the metadata, _but_ still send a metadata all the time (since it's potentially changing on each request).

bq. I'm with Olivier that that's a hacky addition to the driver

There's no addition to the driver (also, was no addition in the previous version of the patch). The only difference is that we can spare the driver a couple of cycles. Behaviour was right in both cases.

I've pulled in the last version of the driver, added comments and prettified it a bit. If we all agree that this behaviour is correct, can anyone take a short look at it?

The patch can be found:

|[here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-13992]|","15/Nov/17 02:16;KurtG;I don't see why special casing LWT is necessary. It couples LWT, {{ModificationStatement}} and {{BatchStatement}} with {{ExecuteMessage}}, which seems a bit messy. It also introduces a weird edge case into the protocol - i.e, 
1. you can't skip metadata for LWT 
2. metadata will never be changed for LWT. 

2 is fine as it's somewhat the goal of the ticket, but when we already have a mechanism to meet this requirement it seems silly to wrap it in another. 1 just adds complexity to the protocol, which imo, is complex enough. It's not even clear in the protocol that SKIP_METADATA will be ignored if the metadata *did* change (we should update this in the patch). I think this can be better solved simply by providing a consistent MD5 for LWT's as my patch already does [here|https://github.com/apache/cassandra/compare/trunk...kgreav:13992]. This removes the need to introduce new imports, and special casing to {{ExecuteMessage}} while achieving the same behaviour, and I think logically is easier to explain.

AFAICT [~ifesdjeen]'s patch only really works because the driver is already set up for it to work. The _java_ driver already checks that {{new_metadata_id != null}} which is logical, but in no way required. It seems to me we're introducing a metadata_id that could potentially be null where a driver might expect it to have a value. I prefer my patch, or better yet, we create an {{EMPTY_DIGEST}} that's an actual MD5 to use so we don't have to rely on {{compute}} every time we want to use an {{EMPTY_DIGEST}}","15/Nov/17 06:33;omichallat;[~ifesdjeen] yes, your last patch is fine for me from a client's perspective (I can't really comment on the implementation since I'm not that familiar with the Cassandra codebase).

[~KurtG]:

bq. you can't skip metadata for LWT

Nor should you be able to. You can't safely skip metadata since these statements may return different columns depending on the state of the database. You _have_ to return the metadata every time.

bq. metadata will never be changed for LWT

We don't need it since every response includes the correct metadata.

bq. AFAICT Alex Petrov's patch only really works because the driver is already set up for it to work.

Actually no, it allows the driver to treat LWT as any other statement. Here's roughly what we do:
{code}
// When decoding a PREPARED response
if ! NO_METADATA // Note that LWT always have NO_METADATA
  store result metadata in prepared statement cache
end if

// When decoding a ROWS response:
if NO_METADATA
  look up result metadata in prepared statement cache
else
  decode result metadata from response
  if (METADATA_CHANGED)
    update result metadata in prepared statement cache
  end if
end if
{code}
So unsetting {{METADATA_CHANGED}} for LWT allows me to properly skip the last cache update. If we use a special value of {{new_metadata_id}} instead (like the empty array in the initial patch), then I have to change the last test to {{if (METADATA_CHANGED || new_metadata_id == special_value)}}.
HTH",15/Nov/17 09:35;ifesdjeen;[~snazy] could you take a look at the patch as you're most familiar with the previous version and we've discussed the behaviour for LWTs multiple times?,"15/Nov/17 10:10;KurtG;bq. then I have to change the last test to if (METADATA_CHANGED || new_metadata_id == special_value)
In Alex's last version, yes, but if the metadata ID from the prepare and the exec are the same (because they are generated with the same value) this is not the case. You wouldn't have to worry about METADATA_CHANGED being set because a LWT will always have the same ID as the initial preparation. <bikeshedding>","15/Nov/17 10:27;ifesdjeen;I thought we have discussed it and I've tried this idea out and it doesn't solve the problem with actually updating the metadata when performing {{ALTER}}.

This is a problem with many pitfalls, I've done a lot of testing both back when wrote an initial patch and now when we were collaborating on the follow-up and personally believe the proposed patch solves a problem in the best possible way. I'm happy to hear all the alternatives out as long as they're tested out and confirmed to work for all cases and keep consistency with previous versions.","15/Nov/17 15:17;omichallat;[~KurtG]

bq. You wouldn't have to worry about METADATA_CHANGED being set because a LWT will always have the same ID as the initial preparation

But then I would have to compare the ids every time. That costs me a lookup in my client-side prepared statement cache (and there also happens to be an additional volatile read in our current implementation). In contrast, checking METADATA_CHANGED is free since it is already in the response.","16/Nov/17 02:14;KurtG;bq. But then I would have to compare the ids every time. That costs me a lookup in my client-side prepared statement cache (and there also happens to be an additional volatile read in our current implementation). In contrast, checking METADATA_CHANGED is free since it is already in the response.
OK I get it.

bq. I thought we have discussed it and I've tried this idea out and it doesn't solve the problem with actually updating the metadata when performing ALTER.
Just trying to understand things here but from what I can see the patches both provide the same behaviour. Notably, METADATA_CHANGED will never be set for LWT but the metadata will still always be returned to the client. Anyway, you can do it that way if you want seeing as it helps on the java driver side. 

But now that I think about it more and after some testing I don't see how either case works completely for {{ALTER}}. When you say ALTER what are you referring to? AFAICT all you can do is add + drop columns that aren't used in the prepared statement, and atm both patches behave in the exact same way. If you alter any column used in the statement it invalidates the statement and you get an error. What aspects of ALTER are you expecting to work?","16/Nov/17 13:56;snazy;As elaborated above, LWTs definitely need special handling as their result set is (or can be) different for each invocation. Remembering (and evicting) metadata for that result set (which is in the ""ok"" case just one column {{[applied]}} - i.e. not much) is probably not beneficiary. Doing that might be worth another look in a separate ticket at a later point. But I don't expect much from that kind of optimization.

I'm not excited about adding a ""special value"" to indicate that the metadata is empty (neither an empty {{byte[]}} nor the MD5 over an empty {{byte[]}}). Just omitting the metadata flags introduced by CASSANDRA-10786 is sufficient.

Olivier correctly pointed out that (unnecessary) additional processing should be avoided - and I second that. Looking at the {{METADATA_CHANGED}} flag is very cheap - comparing values is more expensive (checking a bit in a CPU register or L1 cache line vs. many dloads). Keeping the performance aspect aside, it also looks cleaner.

Since we do not need those metadata-flags for LWTs, we can just omit those and it ""magically"" works - and that's pretty much what [~ifesdjeen]'s patch does.

I've written a [unit test|https://github.com/snazy/cassandra/commit/fcb221af2dcc74c57e3017b73937365e2226b7d3#diff-d04861816aec1bdaa47b3d6819df1a46R277] that verifies the expected behavior on the protocol level.

+1 on [~ifesdjeen]'s patch. I'd like to see the new unit test being added.
Only change that would be good to have is to move the {{boolean hasConditions()}} function up to {{CQLStatement}} and implement it there as {{public default boolean hasConditions() \{ return false; \} }}. By that you can remove the {{instanceof}} and type casts in the change in {{ExecuteMessage}} and probably also save the {{isLWT}} variable as it would just be a call to {{statement.hasConditions()}}.","21/Nov/17 12:17;ifesdjeen;Thank you for the review! 

Committed to trunk with [7eb915097dc3e34e1bb4ef96e6bd8eb67d574622|https://github.com/apache/cassandra/commit/7eb915097dc3e34e1bb4ef96e6bd8eb67d574622] with added unit test and {{hasConditions}} pulled up to statement.",07/Dec/17 23:37;KurtG;Should we have updated the spec to indicate that those flags will not work with LWT? Seems like it could be quite surprising for new driver developers.,"08/Dec/17 12:10;ifesdjeen;We didn't do any client-side changes. As far as I can understand, metadata changes are server-driven. ","08/Dec/17 19:03;omichallat;Yes, with the algorithm I described in [this comment|https://issues.apache.org/jira/browse/CASSANDRA-13992?focusedCommentId=16253022&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16253022], LWTs are handled exactly the same way as regular statements, so I don't think we need to amend the spec.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool describeclusters shows different snitch info as to what is configured.,CASSANDRA-13528,13071508,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,Lerh Low,superpaul,superpaul,12/May/17 11:17,12/Mar/19 14:03,13/Mar/19 22:35,18/Jan/18 12:37,3.11.2,4.0,,,,Tool/nodetool,,,,,0,lhf,,,"I couldn't find any similar issue as this one so I'm creating one.
I noticed that doing nodetool describecluster shows a different Snitch Information as to what is being set in the configuration file.

My setup is hosted in AWS and I am using Ec2Snitch.

cassandra@cassandra3$ nodetool describecluster
Cluster Information:
	Name: testv3
	Snitch: org.apache.cassandra.locator.DynamicEndpointSnitch
	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
	Schema versions:
		fc6e8656-ee7a-341b-9782-b569d1fd1a51: [10.0.3.61,10.0.3.62,10.0.3.63]

I checked via MX4J and it shows the same, I haven't verified tho using a different Snitch and I am using 2.2.6 above and 3.0.X ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17/Jan/18 22:47;Lerh Low;13528-trunk.txt;https://issues.apache.org/jira/secure/attachment/12906500/13528-trunk.txt,12/May/17 11:17;superpaul;Screen Shot 2017-05-12 at 14.15.04.png;https://issues.apache.org/jira/secure/attachment/12867759/Screen+Shot+2017-05-12+at+14.15.04.png,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-05-14 13:30:05.04,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 12:37:18 UTC 2018,,,,,,0|i3ew73:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,2.2.6,,,,,,,"14/May/17 13:30;rha;Actually {{DynamicEndpointSnitch}} is a wrapper, so it wraps {{EC2Snitch}} here. When DES is disabled, {{nodetool describecluster}} gives you what you expect.  I agree it's not very user friendly nor very useful.

Something like this would be more helpful:
{code}
Snitch: <Snitch name here e.g. org.apache.cassandra.locator.GossipingPropertyFileSnitch>
DynamicEndpointSnitch: (enabled|disabled)
{code}


","18/May/17 22:45;Lerh Low;I like that approach, sounds reasonable. I've attached some patches, one for 2.1 and one for 2.2, 3.0, 3.X, trunk (it applies cleanly). Any thoughts? 

{code}
Cluster Information:
        Name: Test Cluster
        Snitch: org.apache.cassandra.locator.SimpleSnitch
        DynamicEndPointSnitch: enabled
        Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
        Schema versions:
                9cf2fd0e-ab63-348e-b9af-6db0a3da5a29: [127.0.0.1]
{code}

{code}
Cluster Information:
        Name: Test Cluster
        Snitch: org.apache.cassandra.locator.SimpleSnitch
        DynamicEndPointSnitch: disabled
        Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
        Schema versions:
                9cf2fd0e-ab63-348e-b9af-6db0a3da5a29: [127.0.0.1]
{code}","19/May/17 05:35;superpaul;I like the solution! It provides proper information of the snitch and eliminates the confusion.
It would be nice also to have this patch on the forthcoming releases.

Thanks!","16/Jun/17 00:26;jay.zhuang;Not sure if changing the behave of the existing JMX interface is a good idea. I would suggest to use {{[DynamicEndpointSnitchMBean.getSubsnitchClassName()|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java#L362]}}, which will give you the same information.","16/Jun/17 00:30;Lerh Low;Hell. My bad, I didn't know that existed. I'll look into it, if that already exists then definitely we should use it. Thanks for your suggestion. ","05/Dec/17 05:36;Lerh Low;Hello, sorry for taking so long on this, it's ancient by now. I've reuploaded a new patch without changing the MBean interfaces, though the UI is slightly different now (less code changes required): 

{code}
Cluster Information:
	Name: Test Cluster
	Snitch: org.apache.cassandra.locator.SimpleSnitch
	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
	Schema versions:
		59a1610b-0384-337c-a2c5-9c8efaba12be: [127.0.0.1]
{code}

{code}
Cluster Information:
	Name: Test Cluster
	Snitch: org.apache.cassandra.locator.DynamicEndpointSnitch
	SubSnitch: org.apache.cassandra.locator.SimpleSnitch
	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
	Schema versions:
		59a1610b-0384-337c-a2c5-9c8efaba12be: [127.0.0.1]
{code}

Applies cleanly to 2.1/2.2/3.0.X/3.X, and tested as well. ",12/Jan/18 00:12;Lerh Low;Just wondering if anyone would like to review the patch? :),"12/Jan/18 15:35;jjirsa;Marking patch-available so reviewers see it.
",12/Jan/18 19:49;jasobrown;I can review this.,"12/Jan/18 20:07;jasobrown;This patch works and code is fine, but I kinda like the version that printed like this:

{quote}
Cluster Information:
        Snitch: org.apache.cassandra.locator.SimpleSnitch
        DynamicEndPointSnitch: enabled
{quote}

This is better because operators can choose to configure the {{endpoint_snitch}} in the yaml, and to output that as a ""Subsnitch"" from {{nodetool describecluster}} is little confusing as it's not what they configured. Calling it Subsnitch is the correct wrt the src code, it's not operator-friendly.

Which branches should we apply this change to? I'm thinking 3.11 and trunk. Any strong argument for 3.0?","17/Jan/18 22:46;Lerh Low;Thanks for reviewing Jason :)

Agreed. For some reason back then I was thinking the code would be less elegant to support that, but now revisiting it it's still relatively straightforward and doesn't need too many hacks. 

So it's now back to: 
{code:java}
Cluster Information:
Name: Test Cluster
Snitch: org.apache.cassandra.locator.SimpleSnitch
DynamicEndPointSnitch: enabled
Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
Schema versions:
59a1610b-0384-337c-a2c5-9c8efaba12be: [127.0.0.1]{code}

And 
{code:java}
Cluster Information:
Name: Test Cluster
Snitch: org.apache.cassandra.locator.SimpleSnitch
DynamicEndPointSnitch: disabled
Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
Schema versions:
59a1610b-0384-337c-a2c5-9c8efaba12be: [127.0.0.1]{code}

Just finished testing this on trunk and 3.11 (patch applies cleanly). Here's the Github link: [https://github.com/juiceblender/cassandra/commit/e93c219f86469357e01595f3adcea81efd0dec84] if that floats your boat more, otherwise the attached patch has also been updated. 

I believe it actually applies alright all the way down to 2.1 (so we can just apply it to 3.0 if we do wish). But it's up to you, just let me know. ","18/Jan/18 12:37;jasobrown;committed as sha {{fc3357a00e2b6e56d399f07c5b81a82780c1e143}}.

 

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forward slash in role name breaks CassandraAuthorizer,CASSANDRA-14088,13122203,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,KurtG,jhaberku,jhaberku,01/Dec/17 16:09,12/Mar/19 14:03,13/Mar/19 22:35,07/Dec/17 21:22,3.0.16,3.11.2,4.0,,,Feature/Authorization,,,,,0,,,,"The standard system authorizer ({{org.apache.cassandra.auth.CassandraAuthorizer}}) stores the permissions granted to each user for a given resource in {{system_auth.role_permissions}}.

A resource like the {{my_keyspace.items}} table is stored as {{""data/my_keyspace/items""}} (note the {{/}} delimiter).

Similarly, role resources (like the {{joe}} role) are stored as {{""roles/joe""}}.

The problem is that roles can be created with {{/}} in their names, which confuses the authorizer when the table is queried.

For example,

{code}
$ bin/cqlsh -u cassandra -p cassandra
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 4.0-SNAPSHOT | CQL spec 3.4.5 | Native protocol v4]
Use HELP for help.
cassandra@cqlsh> CREATE ROLE emperor;
cassandra@cqlsh> CREATE ROLE ""ki/ng"";
cassandra@cqlsh> GRANT ALTER ON ROLE ""ki/ng"" TO emperor;
cassandra@cqlsh> LIST ROLES;

 role      | super | login | options
-----------+-------+-------+---------
 cassandra |  True |  True |        {}
   emperor | False | False |        {}
     ki/ng | False | False |        {}

(3 rows)
cassandra@cqlsh> SELECT * FROM system_auth.role_permissions;

 role      | resource      | permissions
-----------+---------------+--------------------------------
   emperor |   roles/ki/ng |                      {'ALTER'}
 cassandra | roles/emperor | {'ALTER', 'AUTHORIZE', 'DROP'}
 cassandra |   roles/ki/ng | {'ALTER', 'AUTHORIZE', 'DROP'}

(3 rows)
cassandra@cqlsh> LIST ALL PERMISSIONS OF emperor;
ServerError: java.lang.IllegalArgumentException: roles/ki/ng is not a valid role resource name
{code}

Here's the backtrace from the server process:

{code}
ERROR [Native-Transport-Requests-1] 2017-12-01 11:07:52,811 QueryMessage.java:129 - Unexpected error during query
java.lang.IllegalArgumentException: roles/ki/ng is not a valid role resource name
        at org.apache.cassandra.auth.RoleResource.fromName(RoleResource.java:101) ~[main/:na]
        at org.apache.cassandra.auth.Resources.fromName(Resources.java:56) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.listPermissionsForRole(CassandraAuthorizer.java:283) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.list(CassandraAuthorizer.java:263) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.list(ListPermissionsStatement.java:108) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.execute(ListPermissionsStatement.java:96) ~[main/:na]
        at org.apache.cassandra.cql3.statements.AuthorizationStatement.execute(AuthorizationStatement.java:48) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:207) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:238) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:223) ~[main/:na]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:38) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:353) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_151]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151]
ERROR [Native-Transport-Requests-1] 2017-12-01 11:07:52,812 ErrorMessage.java:389 - Unexpected exception during request
java.lang.IllegalArgumentException: roles/ki/ng is not a valid role resource name
        at org.apache.cassandra.auth.RoleResource.fromName(RoleResource.java:101) ~[main/:na]
        at org.apache.cassandra.auth.Resources.fromName(Resources.java:56) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.listPermissionsForRole(CassandraAuthorizer.java:283) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.list(CassandraAuthorizer.java:263) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.list(ListPermissionsStatement.java:108) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.execute(ListPermissionsStatement.java:96) ~[main/:na]
        at org.apache.cassandra.cql3.statements.AuthorizationStatement.execute(AuthorizationStatement.java:48) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:207) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:238) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:223) ~[main/:na]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:38) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:353) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_151]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151]
{code}",Git commit: 4c80eeece37d79f434078224a0504400ae10a20d ({{HEAD}} of {{trunk}}).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-04 03:38:31.119,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 07 23:52:12 UTC 2017,,,,,,0|i3nfq7:,9223372036854775807,,,,,,,,jjordan,jjordan,,,,,,,,,,"04/Dec/17 03:38;KurtG;[3.0|https://github.com/apache/cassandra/compare/trunk...kgreav:14088-3.0]
No reason for us to split in {{RoleResource::fromName}} more than once, so just specified a max and added some tests to make sure we accept forward slashes and some other names correctly.
Patch is the same for trunk and 3.11. Should merge cleanly.","04/Dec/17 09:13;snazy;Before we continue with this one - what's the reason for the forward slash(es) in the role name?
The delimiter {{/}} was chosen to split components - handling that differently across different resource types would be inconsistent.
I'd be much more in favor of validating the role name in {{CreateRoleStatement}} and restrict role names to a defined set of characters, like we do for keyspaces and tables.","06/Dec/17 23:21;KurtG;Wild guess but I'd say it's probably because they have complicated role/permission domains and break them up by slashes in their environment, and would find it easiest to continue to use the same roles in C*, rather than having to change their delimiter. I've seen similar cases before w.r.t PKI/CN's/DN's.

Seeing as fromName is defined per resource I don't see why we can't have specific implementations for each {{Resource}}. In fact, in {{DataResource}} and {{FunctionResource}} we already handle each name differently as we require 3 {{/}} separators (+different sep's for {{FunctionResource}}.
At the moment any character is allowed in a role name except for slash, because of this issue. We only really care about the first slash, if we ever cared about more than that we'd be creating a new {{Resource}} anyway.","07/Dec/17 02:26;jjordan;Agreed, we should just limit to only splitting the first ""/"".

Patch LGTM +1.","07/Dec/17 20:56;jjirsa;This is marked ready to commit, but there's no mention of utest/dtests at all - if they're clean I'll commit, but could someone confirm that they're clean?
","07/Dec/17 21:11;jjordan;I ran the unit test locally that extensively tests the change.  Feel free to remove the ready to commit if you want a full dtest run for it, but given we don't have any dtests which uses ""/"" in a name I did not feel it was needed.","07/Dec/17 21:22;jjirsa;Works for me. Committed as {{f9de26a79de02e61624994e67e64f2c93fb5a35b}} and merged up to 3.11/trunk. Thanks to all 3 of you [~KurtG], [~jjordan] , [~jhaberku]!
","07/Dec/17 23:52;KurtG;For the record, unit tests passed for me (I just didn't bother checking because I'd recently been flooded by build failures from CircleCI). [unit|https://circleci.com/gh/kgreav/cassandra/45]

Seems that it failed on [3.11|https://circleci.com/gh/kgreav/cassandra/44] but on an unrelated error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SecondaryIndexManagerTest.indexWithfailedInitializationIsNotQueryableAfterPartialRebuild is flaky,CASSANDRA-13963,13110264,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,adelapena,adelapena,adelapena,18/Oct/17 11:04,12/Mar/19 14:03,13/Mar/19 22:35,20/Nov/17 17:17,4.x,,,,,Feature/2i Index,Legacy/Testing,,,,0,,,,"The unit test [SecondaryIndexManagerTest.indexWithfailedInitializationIsNotQueryableAfterPartialRebuild|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/index/SecondaryIndexManagerTest.java#L460-L476] is flaky. Apart from [the CI results showing a 3% flakiness|http://cassci.datastax.com/view/All_Jobs/job/trunk_utest/2430/testReport/org.apache.cassandra.index/SecondaryIndexManagerTest/indexWithfailedInitializationIsNotQueryableAfterPartialRebuild/], the test failure can be locally reproduced just running the test multiple times. In my case, it fails 2-5 times for each 1000 executions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-11-20 14:12:03.47,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 20 17:16:57 UTC 2017,,,,,,0|i3lem7:,9223372036854775807,4.x,,,,,,,snazy,snazy,,,,,,,,,,"18/Oct/17 11:48;adelapena;[Here|https://github.com/apache/cassandra/compare/trunk...adelapena:13963-trunk] is a patch solving the problem.

It seems that the call to [{{TestingIndex.shouldFailCreate = false;}}|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/index/SecondaryIndexManagerTest.java#L466] is done right after creating the index configured to fail, without waiting for the finalization of the build task. If we are not lucky the index initalization task can start after disabling the configured fail. In such case, the two calls to {{assertFalse(cfs.indexManager.isIndexQueryable(index))}} can either succeed because the index build task hasn't started yet (not because it has failed), or fail because the task has successfully finished without the configured initialization task failure.

The unit tests usually use [{{CQLTester.waitForIndex}}|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/cql3/CQLTester.java#L709-L737] method to wait for the finalization of index builds. In that case, since we are making the initialization to fail, we can't rely on this method, so the patch adds a new [{{CQLTester.waitForIndexBuilds}}|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/cql3/CQLTester.java#L709-L737] method to wait for the finalization of index build tasks independently of their results and the marking procedure. This method is in {{CQLTester}} instead of {{SecondaryIndexManagerTest}} because I think it's suitable to be used by other tests. ","20/Nov/17 14:12;snazy;+1 on the patch

Your evaluation is correct and checking the number of builds is fine as the {{CREATE INDEX}} doesn't return before the index build is triggered.
Thanks for the patch!","20/Nov/17 17:16;adelapena;Thank for the review :)

Committed to master as [5792b667ecf461a40cc391bc1496287547179c91|https://github.com/apache/cassandra/commit/5792b667ecf461a40cc391bc1496287547179c91]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add yaml flag for disabling MVs, log warnings on creation",CASSANDRA-13959,13109748,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,16/Oct/17 17:19,12/Mar/19 14:03,13/Mar/19 22:35,26/Oct/17 21:07,3.0.16,3.11.2,4.0,,,Feature/Materialized Views,,,,,0,,,,"As discussed on dev@, we should give operators the option to disable materialized view creation, and log warnings when they're created.

Update - Adding link for posterity: https://lists.apache.org/thread.html/d81a61da48e1b872d7599df4edfa8e244d34cbd591a18539f724796f@%3Cdev.cassandra.apache.org%3E",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-10-17 14:25:23.723,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 02 00:50:32 UTC 2017,,,,,,0|i3lbg7:,9223372036854775807,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"16/Oct/17 22:11;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/13959-3.0] | [utest|https://circleci.com/gh/bdeggleston/cassandra/138]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/13959-3.11]|[utest|https://circleci.com/gh/bdeggleston/cassandra/139]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/13959-trunk]|[utest|https://circleci.com/gh/bdeggleston/cassandra/137]|","17/Oct/17 14:25;iamaleksey;Had a quick conversation about this with [~slebresne]. He raised a point that logging a warning every time we load an MV is unreasonable - and I agree that it is. Can we only keep the warning on creation?

Could add a small dtest (or utest if you so prefer) to show that:
1. Setting the flag to {{false}} really disables creation, and
2. That a client warning is emitted

Aside from this, looks good to me. Maybe give it a week before committing (till next Tue/Wed), to let people with binding -1s have a chance to change their mind since dev@ discussion though.","17/Oct/17 19:50;bdeggleston;removed the warning on schema load and added dtests [here|https://github.com/bdeggleston/cassandra-dtest/tree/13959].

Without thinking about it, I squashed the review fixes. Sorry that makes verifying the changes a bit harder.","19/Oct/17 14:00;iamaleksey;I would add a link to ML archives for that discussion in NEWS.txt (can be done on commit), but +1 (still, give it time till next Tue/Wed though).","19/Oct/17 21:43;KurtG;Interesting. At least you took the advice to keep it enabled in a patch release. I'm still a bit confused at how you can make claims it's experimental without providing any evidence other then some developers having some ""feelings"". Especially when it's quite doubtful that any of said developers 1. run 3.x, and 2. use MV's. 

I'm really still not sure what benefit you see in saying it's experimental rather than just fixing whatever these problems are that you can't actually describe.

bq. I would add a link to ML archives for that discussion in NEWS.txt
Anyone who reads that discussion isn't going to be convinced some well educated decision was made. They are just going to see a thread which contained a lot of disagreement that went nowhere. 

","20/Oct/17 00:33;jjirsa;{quote}
I'm really still not sure what benefit you see in saying it's experimental rather than just fixing whatever these problems are that you can't actually describe.
{quote}

and:

{quote}
Anyone who reads that discussion isn't going to be convinced some well educated decision was made. 
{quote}

Both addressed by [~benedict] in one paragraph:
 [here|https://lists.apache.org/thread.html/de6b92f62eb93e6f424f6a846177f31980e2f1f8ac5e7bde29550a4f@%3Cdev.cassandra.apache.org%3E]:

{quote}
MVs are by far and away the most complicated feature we have ever delivered. We do not fully understand it, even in theory, let alone can we be sure we have the implementation right.
{quote}

The only way to know all of the problems is to formally model it, and then verify the implementation matches the model. That hasn't happened.

The next best way (which is still insufficient) is to run it at scale and actually hunt for flaws. That hasn't happened. I know it hasn't happened, because if that sort of testing did happen, whoever did it would have necessarily stumbled across all sorts bugs like ( CASSANDRA-13595 , CASSANDRA-13911, CASSANDRA-13880, CASSANDRA-12872 , CASSANDRA-13747 ). 

I *strongly encourage* you to spend time and effort to find and fix the bugs you know about, but until someone can provide some level of confidence that it's safe, we shouldn't let users ASSUME it's safe.

",26/Oct/17 21:07;bdeggleston;committed as {{b8697441d7a051e7ff68def6aa9cf14bd92ace9e}},"01/Nov/17 13:17;JoshuaMcKenzie;bq. The only way to know all of the problems is to formally model it, and then verify the implementation matches the model. That hasn't happened.
At the risk of beating a dead horse - by this measure, most of the features in C* should be flagged experimental. I'm all for us having more rigor in terms of proving the correctness of behaviors in a distributed system, but this looks an awful lot like arbitrarily applying that standard to this feature and not to others.","01/Nov/17 13:21;jjirsa;You're cherry-picking sentences [~JoshuaMcKenzie] . That statement is true, but the previous line (quoted from Benedict) and the following paragraph is why it needs to be marked experimental.




","01/Nov/17 13:26;JoshuaMcKenzie;bq. MVs are by far and away the most complicated feature we have ever delivered. We do not fully understand it, even in theory, let alone can we be sure we have the implementation right.
Both claims from Benedict on that email thread are 'citation needed', and I believe what Kurt refers to when he's talking about developer's 'feelings'. To be very clear, I'm not even saying I disagree with Benedict on his *intuition* here, just that we have no precedent for formal analysis whatsoever on things like this and marking a feature experimental that's in use by people in production systems smells knee-jerk. Again, this is largely beating a dead horse from what was on the mailing list so I'm not looking to re-litigate here.

As for the latter paragraph, making the claim that you know nobody is running MV's at scale in the wild is a pretty bold claim, seeing as how we don't have concrete data on the full adoption of the project in the wild as a whole, much less the scale of usage of individual features. So no, my intention wasn't to just cherry-pick sentences, but instead to point out that we're making broadly user-impacting decisions that are not backed by data.","01/Nov/17 13:34;jjirsa;I didn't say nobody was running MVs at scale - I said nobody was doing it AND looking for errors. I'm confident in that assertion because had they done so, they would have found the 5 8099 correctness-impacting bugs I linked, which you would certainly hit as you did range through a table (or view) to check the data matched. Those bugs and ~8 years of intuition are all I need to vote in favor of this.

If you're willing to assert that there exists a competent team running MVs in multiple DCs with multiple racks, writing data into the table with nontrivial volumes and non-trivial patterns (new rows, overwrites, TTLs, range deletes, partition deletes), adding/removing nodes, growing the cluster, shrinking the cluster, running full repair, running incremental repair, killing disks, replacing instances, and then verifying that EVERY SINGLE WRITE *and* EVERY SINGLE DELETE is present with the expected consistency guarantees, by all means, say so and I'll vote to remove the flag in 4.0. 

I'm sure people are running MVs at scale, I just don't believe that they're actively hunting for bugs or trying to prove correctness. But you've got a binding -1 now, if you really believe this is hurting users, you know what to do.
","01/Nov/17 14:41;bdeggleston;bq. this looks an awful lot like arbitrarily applying that standard to this feature and not to others

Do we have other features that, by design, can get themselves into an inconsistent state and can’t be fixed without downtime?",01/Nov/17 14:46;iamaleksey;Are you guys all bored or something?,"02/Nov/17 00:50;KurtG;It seems fair to me that all the people who are adamant about marking MV's experimental should be pushing to fix them by 4.0, instead of trying to absolve themselves of the responsibility by throwing existing users of MV's under the bus. Especially considering these are the people who let MV through in the first place.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.generate.Distribution.average broken on trunk,CASSANDRA-14090,13122298,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,01/Dec/17 22:48,12/Mar/19 14:03,13/Mar/19 22:35,06/Dec/17 00:01,4.0,,,,,,,,,,0,,,,"Looks like the lgtm.com fixes slightly changed the behavior of Distribution.average, which prevents stress from starting up",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-04 22:23:43.952,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 11 13:54:37 UTC 2017,,,,,,0|i3ngb3:,9223372036854775807,,,,,,,,rustyrazorblade,rustyrazorblade,,,,,,,,,,01/Dec/17 22:52;bdeggleston;patch against [trunk|https://github.com/bdeggleston/cassandra/tree/14090],"04/Dec/17 22:23;rustyrazorblade;Verified bug with:

{code}
tools/bin/cassandra-stress user profile=tools/cqlstress-example.yaml 'ops(insert=1,simple=1)' n=1000 cl=ONE -mode native cql3 -rate threads=100 fixed=1000/s
{code}

Fixed by your patch, +1.","06/Dec/17 00:01;bdeggleston;thanks, committed as {{d8f0361228aec146689b8f9402c7aa61d95bdc05}}","11/Dec/17 13:54;jasonstack;[~bdeggleston]  

bq. tools/bin/cassandra-stress user profile=tools/cqlstress-example.yaml ops(insert=1)
should we also change the following line to {{50}} ?

{code}
return (long) (sum / 51);
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw descriptive errors for mixed mode repair attempts,CASSANDRA-13944,13108118,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,09/Oct/17 22:22,12/Mar/19 14:03,13/Mar/19 22:35,24/Oct/17 20:56,4.0,,,,,Consistency/Repair,,,,,0,,,,"We often make breaking changes to streaming and repair between major versions, and don't usually support either in mixed mode clusters. Streaming connections check protocol versions, but repair message handling doesn't, which means cryptic exceptions show up in the logs when operators forget to turn off whatever's scheduling repairs on their cluster. Refusing to send or receive repair messages to/ from incompatible messaging service versions, and throwing a descriptive exception would make it clearer why repair is not working, as well as prevent any potentially unexpected behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-10-24 08:06:37.802,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 24 20:56:39 UTC 2017,,,,,,0|i3l1xr:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"23/Oct/17 21:17;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13944]
[utests|https://circleci.com/gh/bdeggleston/cassandra/135]","24/Oct/17 08:06;krummas;Patch LGTM, but I think the message could be a bit clearer that we don't support mixed version repairs - not sure ""messaging service version"" is something that makes sense to end users. Adding something like ""make sure all nodes involved in the repair are on the same major version"" to the message might help?",24/Oct/17 17:18;bdeggleston;[~krummas] fixed,24/Oct/17 17:21;krummas;+1,24/Oct/17 20:56;bdeggleston;committed as {{2b507c03c5190c744c5e84d7ca5cf7afa2b5c2ae}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check hashed password matches expected bcrypt hash format before checking,CASSANDRA-13626,13081266,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,20/Jun/17 23:47,12/Mar/19 14:03,13/Mar/19 22:35,31/Aug/17 05:06,3.0.15,3.11.1,4.0,,,Feature/Authorization,,,,,0,,,,"We use {{Bcrypt.checkpw}} in the auth subsystem, but do a reasonably poor job of guaranteeing that the hashed password we send to it is really a hashed password, and {{checkpw}} does an even worse job of failing nicely. We should at least sanity check the hash complies with the expected format prior to validating.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-30 09:53:45.21,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 31 05:06:22 UTC 2017,,,,,,0|i3gipz:,9223372036854775807,,,,,,,,beobal,beobal,,,,,,,,,,"29/Aug/17 22:32;jjirsa;|| branch || utest || dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13626] | [3.0 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13626] | [3.0 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/233/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13626] | [3.11 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13626] | [3.11 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/234/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13626] | [trunk circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13626] | [trunk dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/235/] |
","30/Aug/17 09:53;beobal;bq.checkpw does an even worse job of failing nicely

In what way(s)? Checking with a few strings that would be caught by this validation, {{checkpw}} seems to behave as expected. 
This validation would help us detect that we have stored invalid hash values, so that could be useful in diagnosing unexpected auth failures & debugging their causes. That will obviously require logging when the validation fails before throwing the {{AuthenticationException}}, so we should separate it from the actual {{checkPw}} call.  

On the actual validation, the 22 character component is actually the salt, not the cost - the bcrypt format is {{$<id>$<cost>$<salt><digest>}}. Cost, salt and digest are all fixed length (2, 22 & 31 chars repectively), whereas id may be 1 or 2 chars, though we have only ever used a version of jbcrypt that generates the 2 char variant. So we could simplify that check to {{length == 60}}. If {{checkpw}} *is* correctly returning false when the stored hash is invalid though, we only really need to do the validation on failure, in which case we could a more thorough check than simply looking at the length, if that's worthwhile.   

EDIT: I know it's a pretty trivial change, but it would be nice to add some tests to go with it.","30/Aug/17 17:40;jjirsa;Sam and I talked about this a bit offline, a few notes for those following along:
- The length is 60 now, but may be 59 with other bcrypt variants, and may be some other length in the future. The two components of the length that were chosen before (salt+digest) were expected to be fixed length, which is true now but may not be true later). ID is definitely variable length now, though. 
- We don't want a real regex, for a few reasons (combination of futureproofing and the risk of introducing auth weaknesses) - we should let bcrypt handle the hash, we don't need to get involved.
- We can be less invasive here and make it slightly more testable by breaking the hash check into its own static function 

Force pushed branches that simplifies things a bit, makes it less arbitrary.

|| branch || utest || dtest ||
| [3.0|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13626] | [3.0 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.0-13626] | [3.0 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/251/] |
| [3.11|https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13626] | [3.11 circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-3.11-13626] | [3.11 dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/252/] |
| [trunk|https://github.com/jeffjirsa/cassandra/tree/cassandra-13626] | [trunk circle|https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13626] | [trunk dtest|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/253/] |
","30/Aug/17 19:28;beobal;+1 LGTM.

I pushed some *tiny* additions to {{PasswordAuthenticatorTest}} [here|https://github.com/beobal/cassandra/commit/fba4d7fcb3630b4ba67833f9e451cf03aa11aa62]. If you think they're overkill, I won't argue too hard.","31/Aug/17 05:06;jjirsa;Nice. Dtest environment looks pretty messy today, some of the slaves are acting up. I've read through some of the console logs and even on the aborted runs, there's nothing auth related, so I'm proceeding (since it's a fairly trivial patch). Added those tests and committed as {{5e7f60f6bf5da386076faa08cefb3970a6ba5cc0}}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better config validation/documentation,CASSANDRA-13622,13081030,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasonstack,KurtG,KurtG,20/Jun/17 07:31,12/Mar/19 14:03,13/Mar/19 22:35,12/Sep/17 14:35,3.0.x,3.11.x,4.x,,,Local/Config,,,,,0,lhf,,,"There are a number of properties in the yaml that are ""in_mb"", however resolve to bytes when calculated in {{DatabaseDescriptor.java}}, but are stored in int's. This means that their maximum values are 2047, as any higher when converted to bytes overflows the int.

Where possible/reasonable we should convert these to be long's, and stored as long's. If there is no reason for the value to ever be >2047 we should at least document that as the max value, or better yet make it error if set higher than that. Noting that although it's bad practice to increase a lot of them to such high values, there may be cases where it is necessary and in which case we should handle it appropriately rather than overflowing and surprising the user. That is, causing it to break but not in the way the user expected it to :)

Following are functions that currently could be at risk of the above:

{code:java|title=DatabaseDescriptor.java}
getThriftFramedTransportSize()
getMaxValueSize()
getCompactionLargePartitionWarningThreshold()
getCommitLogSegmentSize()
getNativeTransportMaxFrameSize()
# These are in KB so max value of 2096128
getBatchSizeWarnThreshold()
getColumnIndexSize()
getColumnIndexCacheSize()
getMaxMutationSize()
{code}

Note we may not actually need to fix all of these, and there may be more. This was just from a rough scan over the code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13565,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-14 09:04:27.194,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 12 14:49:46 UTC 2017,,,,,,0|i3gh9j:,9223372036854775807,,,,,,,,KurtG,KurtG,,,,,,,,,,"14/Jul/17 09:04;jasonstack;| [trunk| https://github.com/jasonstack/cassandra/commits/CASSANDRA-13622] | [unit|https://circleci.com/gh/jasonstack/cassandra/153] | dtest: except for 1 known error in bootstrap_test |

handled NPE when empty entry in storage_directories and handled overflow when converting to int32 KB from MB","18/Jul/17 08:00;KurtG;Not sure how keen I am on bounding commitlog_segment_size to 2GB. Obviously that's a ridiculous size for a segment, but I have had to increase it to 1.2GB in the past to allow a write through. Granted it was a side effect from MV's (I think it was streaming related), however the ability to do so helped. Obviously that's not great justification but I think it would be better to give it a much higher limit (store it as a long) and simply advise users (in docs+yaml) that they really shouldn't need to set it above the default. 

Same goes for max_value_size. I don't see any compelling reason we should stop people experimenting with that if they want to, as it's really just a safety net that the user configures. ",25/Jul/17 02:50;jasonstack;That's java ByteBuffer restriction on 2GB.. ,"25/Jul/17 03:44;KurtG;Ah yeah didn't think about that. Well, guess that's as good as it will get then.",08/Aug/17 06:23;jasonstack;[~KurtG] could you review it?,14/Aug/17 00:25;KurtG;LGTM. Probably worth also applying the changes to 3.0/3.11. ,"15/Aug/17 08:46;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13622-trunk] |
| [3.11|https://github.com/jasonstack/cassandra/commits/CASSANDRA-13622-3.11] |
| [3.0| https://github.com/jasonstack/cassandra/commits/CASSANDRA-13622-3.0] |

Thanks for reviewing",23/Aug/17 15:47;jasonstack;[~KurtG] could you have a look at the final patch before commiting? thanks,"24/Aug/17 00:44;KurtG;Sorry missed the email for your last comment. CircleCI appears to have failed on [trunk|https://circleci.com/gh/jasonstack/cassandra/424#tests/containers/0]. Might be related as it is testing DatabaseDescriptor. On that note probably wouldn't hurt to have some simple tests for these cases.

Also the merge into trunk has an extra line in {{CHANGES.txt}} before the change. The {{3.0.15}} should be removed.

Might want to rebase as well (sorry for my slowness :/)

Also not a committer so will prod one to commit/review once tests are written/are working.
Thanks.","24/Aug/17 07:53;jasonstack;Thanks, rebased and build passed.",12/Sep/17 14:34;adelapena;Committed as [8fc9275d3020fa0c80ed1852726be0a5a63e487c|https://github.com/apache/cassandra/commit/8fc9275d3020fa0c80ed1852726be0a5a63e487c].,12/Sep/17 14:49;jasonstack;[~KurtG]  [~adelapena] thank you ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LCS ignores compaction thresholds in L0 STCS,CASSANDRA-13861,13101308,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cnlwsu,cnlwsu,cnlwsu,11/Sep/17 16:04,12/Mar/19 14:03,13/Mar/19 22:35,15/Sep/17 20:58,4.0,,,,,Local/Compaction,,,,,0,,,,min max compaction thresholds are hard coded to 4 and 32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-11 16:08:24.013,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 15 20:58:08 UTC 2017,,,,,,0|i3jwqn:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"11/Sep/17 16:08;githubbot;GitHub user clohfink opened a pull request:

    https://github.com/apache/cassandra/pull/150

    Use compaction threshold for STCS in L0 for CASSANDRA-13861

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/clohfink/cassandra 13861

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/150.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #150
    
----
commit ed584ec70a9f1889a317405aa869758145c3c6a5
Author: Chris Lohfink <clohfink@apple.com>
Date:   2017-09-11T16:07:40Z

    Use compaction threshold for STCS in L0 for CASSANDRA-13861

----
","11/Sep/17 16:54;jjirsa;+1 (for trunk only). 

We should be good citizens and run through circleci , just to be sure we don't have any surprises (I know it's a trivial patch, but wouldnt want to break the build or something with missing semicolon). Can you queue that up or do you need me to?
",15/Sep/17 15:37;cnlwsu;https://circleci.com/gh/clohfink/cassandra/39,"15/Sep/17 20:58;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/150
","15/Sep/17 20:58;jjirsa;Committed as {{85b74ca5f53b1285b65a0909168a326ab1ac650a}} , thanks!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add additional unit tests for batch behavior, TTLs, Timestamps",CASSANDRA-13846,13100252,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,06/Sep/17 18:24,12/Mar/19 14:03,13/Mar/19 22:35,11/Sep/17 23:01,4.0,,,,,,,,,,0,,,,"There are some combinations of batch behavior for which there are no unit tests. An example of this is CASSANDRA-13655 , which adds some tests, but not every combination. This ticket will be for additional unit tests around batches / counter batches / batches with TTLs / etc.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-11 21:31:56.016,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 11 23:01:51 UTC 2017,,,,,,0|i3jq6v:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,"11/Sep/17 20:43;jjirsa;All code here is in {{test/}} , so not bothering with dtests 

Branch here: https://github.com/jeffjirsa/cassandra/tree/cassandra-13846
CircleCI: https://circleci.com/gh/jeffjirsa/cassandra/tree/cassandra-13846
",11/Sep/17 21:31;jasobrown;+1. ,"11/Sep/17 23:01;jjirsa;Thanks. Committed to trunk only as {{471835815811d4de42474a3e3899a42cb6d969ce}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
issue with pycharm datastax cassandra driver,CASSANDRA-13779,13096176,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,batchuusa,batchuusa,19/Aug/17 00:25,12/Mar/19 14:03,13/Mar/19 22:35,19/Aug/17 16:27,,,,,,,,,,,0,,,,"[Server error] message=""io.netty.handler.codec.DecoderException: org.apache.cassandra.transport.ProtocolException: Invalid or unsupported protocol version: 4""',)})",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-19 12:46:01.118,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 19 16:27:38 UTC 2017,,,,,,0|i3j1pz:,9223372036854775807,,,,,,,,,,,,,,,,,,,"19/Aug/17 12:46;rha;Hi, this error happens when there is a mismatch between Cassandra version and Driver version. This is not related to PyCharm. Be sure to check [Python DataStax driver compatibility matrix|https://docs.datastax.com/en/developer/driver-matrix/doc/pythonDrivers.html], [Java DataStax driver compatibility matrix|https://docs.datastax.com/en/developer/java-driver/3.3/manual/native_protocol/#compatibility-matrix], etc.

(Note that DataStax Driver has its own bug tracker for [Python|https://datastax-oss.atlassian.net/projects/PYTHON/issues/], [Java|https://datastax-oss.atlassian.net/projects/JAVA/summary], etc.)","19/Aug/17 15:20;batchuusa;thanks i will follow up with Datastax.

Sent from my iPhone


","19/Aug/17 16:25;batchuusa;It was not datastax drive, I downloaded from apache website , cassandra-driver-3.11.0 is same driver what datastax have ?","19/Aug/17 16:27;batchuusa;My apologies, I checked and it is related datastax. Closing the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Return value of CountDownLatch.await() not being checked in Repair,CASSANDRA-13397,13060816,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,szhou,szhou,szhou,31/Mar/17 22:01,12/Mar/19 14:03,13/Mar/19 22:35,21/Apr/17 00:36,3.0.14,3.11.0,4.0,,,,,,,,0,,,,"While looking into repair code, I realize that we should check return value of CountDownLatch.await(). Most of the places that we don't check the return value, nothing bad would happen due to other protection. However, ActiveRepairService#prepareForRepair should have the check. Code to reproduce:
{code}
    public static void testLatch() throws InterruptedException {
        CountDownLatch latch = new CountDownLatch(2);
        latch.countDown();

        new Thread(() -> {
            try {
                Thread.sleep(1200);
            } catch (InterruptedException e) {
                System.err.println(""interrupted"");
            }
            latch.countDown();
            System.out.println(""counted down"");
        }).start();


        latch.await(1, TimeUnit.SECONDS);
        if (latch.getCount() > 0) {
            System.err.println(""failed"");
        } else {
            System.out.println(""success"");
        }
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Apr/17 05:24;szhou;CASSANDRA-13397-v1.patch;https://issues.apache.org/jira/secure/attachment/12861577/CASSANDRA-13397-v1.patch,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-04-17 13:32:00.066,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon May 08 00:13:10 UTC 2017,,,,,,0|i3d2vb:,9223372036854775807,3.0.10,,,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,01/Apr/17 05:24;szhou;The attached patch includes the fix and a minor improvement (bail out early if there is any unavailable neighbor). [~krummas] could you help review this patch?,"17/Apr/17 13:32;pauloricardomg;good catch! lgtm, will merge after CI looks good:

||3.0||
|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...pauloricardomg:3.0-13397]|
|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-13397-testall/lastCompletedBuild/testReport/]|
|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-3.0-13397-dtest/lastCompletedBuild/testReport/]|
","19/Apr/17 13:26;pauloricardomg;Tests look good but there was a minor conflict when merging to trunk so I will submit a new CI round with the trunk patch:

||trunk||
|[branch|https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-13397]|
|[testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-13397-testall/lastCompletedBuild/testReport/]|
|[dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-trunk-13397-dtest/lastCompletedBuild/testReport/]|
",21/Apr/17 00:35;pauloricardomg;Committed to 3.0 and merged up as {{f5b36f12df65a780a52851207c285db7a8b4122f}}. Thanks!,21/Apr/17 01:49;szhou;Thank you [~pauloricardomg]!,"25/Apr/17 23:40;szhou;[~pauloricardomg], in case you haven't done so, are you going to merge the fix to trunk?",25/Apr/17 23:43;pauloricardomg;It's already on [trunk|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/ActiveRepairService.java#L369]..,"08/May/17 00:12;githubbot;GitHub user grom358 opened a pull request:

    https://github.com/apache/cassandra/pull/109

    Backport CASSANDRA-13397

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/instaclustr/cassandra cameron_13397

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/109.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #109
    
----
commit bd49317cf8d406824e8be0b3a7c676a0a6bb95f9
Author: Alwyn Davis <alwyndav@gmail.com>
Date:   2016-09-30T00:44:22Z

    Test commit

commit 2f45b53ee590fbefd3d15382765466fe716675d0
Author: Sylvain Lebresne <sylvain@datastax.com>
Date:   2016-06-23T08:58:14Z

    Fixed conflicts

commit c64c6a80a5a56f517625197fb30154f2ad99c808
Author: Yuki Morishita <yukim@apache.org>
Date:   2016-08-03T14:34:27Z

    Release sstables of failed stream sessions only when outgoing transfers are finished
    
    Patch by Paulo Motta; reviewed by Yuki Morishita for CASSANDRA-11345

commit 7a7c219024128063fee1bac382b68b659f93ea66
Author: Paulo Motta <pauloricardomg@gmail.com>
Date:   2016-08-13T01:06:27Z

    Throw RuntimeException if starting transfer of already completed OutgoingFileMessage

commit 05e9f07723ef53eb4b31ea3543699d6260797e3f
Author: Marcus Eriksson <marcuse@apache.org>
Date:   2016-06-13T13:29:08Z

    Avoid missing sstables when getting the canonical sstables
    
    Patch by marcuse; reviewed by Stefania Alborghetti for CASSANDRA-11996

commit f258bab2954be32c7637c1ba936a11f1d500d52e
Author: Sylvain Lebresne <sylvain@datastax.com>
Date:   2016-07-25T14:35:33Z

    AssertionError with MVs on updating a row that isn't indexed due to a null value
    
    patch by Sylvain Lebresne; reviewed by Carl Yeksigian for CASSANDRA-12247

commit abdb8224a04a56a12a4b8ea6984d68f99234b2c8
Author: Alex Petrov <oleksandr.petrov@gmail.com>
Date:   2016-05-09T09:06:43Z

    Allow updating UDT nested in non-frozen map after ALTERing the UDT
    
    Patch by Alex Petrov; reviewed by jknighton for CASSANDRA-11604

commit 909dfa82a5576a4ff2274511009b569ce9e50cc9
Author: Edward Capriolo <edlinuxguru@gmail.com>
Date:   2016-06-10T15:45:57Z

    StorageService shutdown hook should use a volatile variable
    
    patch by Ed Capriolo; reviewed by Stefania Alborghetti for CASSANDRA-11984

commit 63c6e9b8efaf91f6782f674cf33a8db13dc40f57
Author: Sam Tunnicliffe <sam@beobal.com>
Date:   2016-06-24T10:47:25Z

    Ensure new CFS is initialized before adding to schema
    
    Patch by Sam Tunnicliffe; reviewed by Aleksey Yeschenko for
    CASSANDRA-12083

commit 97d9b149c1189b82f68216be8eeac5f67f92b711
Author: Alwyn <alwyn@instaclustr.com>
Date:   2016-10-06T04:24:00Z

    Fix for incorrect test case in CASSANDRA-11604

commit 34a71bc0ea1fd8f3378a7fa9a286010c09f44956
Author: Alwyn Davis <alwyndav@gmail.com>
Date:   2016-10-15T23:17:01Z

    Bumped version number

commit 9e922d358a6ce8175b6f303b69b3c84a229514db
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T06:57:52Z

    Update README.asc
    
    Changed README to be our FAQ and text. Includes link to actual readme for apache cassandra

commit 5a5d54d2583f6402c67e2ec6cc822e7ac99650cd
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T07:17:49Z

    Merge pull request #1 from benbromhead/patch-1
    
    Update README.asc

commit e36f435a5901a185c196ffb5a69bdebca5540444
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T07:21:43Z

    Update README.asc
    
    Reworded a few things, fixed some typos

commit ca1d94eec089694998d71869246d19516a1ef487
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T07:28:01Z

    Update README.asc
    
    added link to Instaclustr.com

commit 2d1c3f5ae2000899b74af91aaf883bf5690585c9
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T17:38:16Z

    Update README.asc

commit 488d07eaae3489aaf5468aeffde7859957292f3f
Author: benbromhead <ben.bromhead@gmail.com>
Date:   2016-10-19T18:28:45Z

    Update README.asc
    
    a word

commit deb53f468fe27aeb65f9729937233ea8e954f123
Author: Yuki Morishita <yukim@apache.org>
Date:   2016-09-29T20:05:12Z

    Merge branch 'cassandra-3.0' into cassandra-3.7-instaclustr
    Fix merkle tree depth calculation
    
    Patch by Paulo Motta; Reviewed by Yuki Morishita for CASSANDRA-12580

commit e09f1abd7261d3372081b72713fa7e57ccc9d3cd
Author: Yuki Morishita <yukim@apache.org>
Date:   2016-10-20T14:47:36Z

    Fix unreleased resource sockets
    
    patch by Arunkumar M; reviewed by yukim for CASSANDRA-12330

commit 8405b187d5bbd7951fd7de409b9370bfe3f668cd
Author: Yuki Morishita <yukim@apache.org>
Date:   2016-10-20T14:45:28Z

    Fix unreleased resource sockets
    
    patch by Arunkumar M; reviewed by yukim for CASSANDRA-12329

commit 53ebca6850b58f9f8ff8f3e03b50d2c604fdfc3a
Author: Jeff Jirsa <jeff.jirsa@crowdstrike.com>
Date:   2016-10-19T01:11:17Z

    Correct log message for statistics of offheap memtable flush
    
    Patch by Kurt Greaves; Reviewed by Jeff Jirsa for CASSANDRA-12776

commit ab0b88621071ee820699af742eeef12b3dd9e75c
Author: Marcus Eriksson <marcuse@apache.org>
Date:   2016-10-21T07:03:31Z

    Don't skip sstables based on maxLocalDeletionTime
    
    Patch by Cameron Zemek; reviewed by marcuse for CASSANDRA-12765

commit 6e694e76a02fdd30ac73a46d07c55689ba5f93f5
Author: Jeff Jirsa <jeff.jirsa@crowdstrike.com>
Date:   2016-10-22T02:04:04Z

    Split consistent range movement flag correction
    
    Patch by Sankalp Kohli; Reviewed by Jeff Jirsa for CASSANDRA-12786

commit 86e6e4ba1ed414e4ace38c3e3c51a151e74c45bd
Author: Sylvain Lebresne <sylvain@datastax.com>
Date:   2016-07-20T12:29:16Z

    NullPointerExpception when reading/compacting table
    
    patch by Sylvain Lebresne; reviewed by Carl Yeksigian for CASSANDRA-11988

commit 0576db93629790247fcb8cf281e413635b656b1f
Author: Sylvain Lebresne <sylvain@datastax.com>
Date:   2016-07-29T10:36:40Z

    NullPointerException during compaction on table with static columns
    
    patch by Sylvain Lebresne; reviewed by Carl Yeksigian for CASSANDRA-12336

commit 030cc592e8cf1ddc4079fab4570407cb24b058f6
Author: Stefania Alborghetti <stefania.alborghetti@datastax.com>
Date:   2016-09-23T05:52:02Z

    Avoid sstable corrupt exception due to dropped static column
    
    Patch by Stefania Alborghetti; reviewed by Carl Yeksigian for CASSANDRA-12582

commit 2cd54679d0cb1f2e2ef7b8de7a16c13fd374461e
Author: Marcus Eriksson <marcuse@apache.org>
Date:   2016-06-23T07:46:00Z

    Don't try to get sstables for non-repairing column families
    
    Patch by marcuse; reviewed by Paulo Motta for CASSANDRA-12077

commit d7fc4873eed70f8923e87f15af83900598061962
Author: kgreav <kurt201@hotmail.com>
Date:   2016-11-21T01:44:24Z

    Merge pull request #3 from instaclustr/kurt_2
    
    Backports

commit f866272e2e9ba2f39bd480c986e4c0711c689044
Author: Kurt <kurt@instaclustr.com>
Date:   2016-11-21T01:49:11Z

    Bump version to 3.7.2

commit b5587f41e711b07a9392309c179c4d5746db1405
Author: Kurt <kurt@instaclustr.com>
Date:   2016-11-23T00:35:54Z

    Updated list of backports

----
","08/May/17 00:13;githubbot;Github user grom358 closed the pull request at:

    https://github.com/apache/cassandra/pull/109
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SASIIndex and Clustering Key interaction,CASSANDRA-13674,13084968,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,justinhwang,justinhwang,05/Jul/17 21:37,12/Mar/19 14:03,13/Mar/19 22:35,05/Jul/17 23:38,,,,,,Feature/SASI,,,,,0,,,,"Not sure if this is the right place to ask, but it has been a couple days and I haven't been able to figure this out.

The current setup of my table is as such:

{code}
CREATE TABLE test.user_codes (
    user_uuid text,
    code text,
    description text
    PRIMARY KEY (user_uuid, code)
);
CREATE CUSTOM INDEX user_codes_code_idx ON test.user_codes
(code) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS =
{'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 
'case_sensitive': 'false', 'mode': 'CONTAINS', 'analyzed': 'true'};

CREATE CUSTOM INDEX user_codes_description_idx ON test.user_codes
(description) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS =
{'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 
'case_sensitive': 'false', 'mode': 'CONTAINS', 'analyzed': 'true'};
{code}

I can successfully make the following call: 
{code}
SELECT * FROM user_codes WHERE user_uuid='xxxx' and description like 'Test%';
{code}
However, I can't make a similar call unless I allow filtering:
{code}
SELECT * FROM user_codes WHERE user_uuid='xxxx' and code like 'Test%';
{code}
I believe this is because the field `code` is a clustering key, but cannot figure out the proper way to set up the table such that the second call also works.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,2017-07-05 21:37:14.0,,,,,,0|i3h5hz:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counter digests include local data,CASSANDRA-13750,13093241,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,08/Aug/17 17:27,12/Mar/19 14:03,13/Mar/19 22:35,10/Aug/17 22:53,3.0.x,3.11.x,4.0,,,,,,,,0,,,,"In 3.x+, the raw counter value bytes are used when hashing counters for reads and repair, including local shard data, which is removed when streamed. This leads to constant digest mismatches and repair overstreaming.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-09 10:53:01.469,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 10 22:53:53 UTC 2017,,,,,,0|i3ijxb:,9223372036854775807,,,,,,,,iamaleksey,iamaleksey,,,,,,,,,,"08/Aug/17 20:53;bdeggleston;|[trunk|https://github.com/bdeggleston/cassandra/tree/13750-trunk]| [utest|https://circleci.com/gh/bdeggleston/cassandra/81] / [dtest (pending)|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/172/]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/13750-3.11] | [utest|https://circleci.com/gh/bdeggleston/cassandra/80]|
|[3.0|https://github.com/bdeggleston/cassandra/tree/13750-3.0] | [utest|https://circleci.com/gh/bdeggleston/cassandra/79] |","09/Aug/17 10:53;iamaleksey;If you have legacy pre-2.1 data shards lying around in sstables, this bug will hurt. One minor problem is that for people who don't, there will be a short period of digest mismatches during the minor upgrade from 3.0.prev to 3.0.next, but I don't see a way to avoid it.

+1","10/Aug/17 22:53;bdeggleston;Committed as {{eb6f03c8928e913cb6f9eaa7c9ea9f4501039112}}

Opened/reviewed/committed CASSANDRA-13755 to fix only non-flaky test failure",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra 3.10: ClassCastException in ThreadAwareSecurityManager,CASSANDRA-13396,13060689,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ehubert,appodictic,appodictic,31/Mar/17 14:08,12/Mar/19 14:03,13/Mar/19 22:35,26/Mar/18 11:27,3.11.3,4.0,,,,,,,,,9,,,,https://www.mail-archive.com/user@cassandra.apache.org/msg51603.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Mar/18 12:57;ehubert;CASSANDRA-13396_ehubert_1.patch;https://issues.apache.org/jira/secure/attachment/12915316/CASSANDRA-13396_ehubert_1.patch,23/Mar/18 22:30;ehubert;CASSANDRA-13396_ehubert_2.patch;https://issues.apache.org/jira/secure/attachment/12916000/CASSANDRA-13396_ehubert_2.patch,24/Mar/18 17:31;ehubert;CASSANDRA-13396_ehubert_3.patch;https://issues.apache.org/jira/secure/attachment/12916069/CASSANDRA-13396_ehubert_3.patch,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-03-31 14:34:26.328,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 26 15:35:11 UTC 2018,,,,,,0|i3d233:,9223372036854775807,,,,,,,,jasobrown,jasobrown,,,,,,,,,,31/Mar/17 14:19;appodictic;https://github.com/apache/cassandra/compare/trunk...edwardcapriolo:CASSANDRA-13396?expand=1,"31/Mar/17 14:34;snazy;I'm strongly -1 on this change.

This change will cause weird and hard to catch follow-up issues (see the discussions and issues around that piece code), which _cannot_ be caught by neither unit nor dtests because it's an unsupported setup. We do not support embedding C* in a container (i.e. a JVM not controlled ""by us""). IMO, supporting C* in such an environment will cause other issues. Technically, it's not a major bug - changed it to wish.","31/Mar/17 14:50;appodictic;How come everyone in Cassandra's first reaction is to -1 everything? 

The entire model of apache is ""I have an itch to scratch"". This person WANTS to run Cassandra in a container it is an ""itch"". The immediate opposition position should not be ""BUT DON'T SCRATCH THAT ITCH"", because I say so.

","31/Mar/17 14:51;apassiou;@[~snazy]: OK but what if the cassandra daemon is not embedded anywhere but is simply running with a classpath containing several slf4j bindings?
It will still crash, right?

@[~appodictic]: please don't over-react (and don't hijack my question), it's an open discussion ;-)","31/Mar/17 14:54;appodictic;LOL I just posted this tweet yesterday.

https://twitter.com/edwardcapriolo/status/847484593041100800

What comedy cassandra is. No one even bothers to say ""how can we work together?"" or ""how can we wrote the code to make all users happy"" They just instantly drop a -1 on things. lol","31/Mar/17 14:56;appodictic;So funny that i litteraly wake up, go out of my way to fix an issue for someone, and even though everyone is Cassandra is too busy to reply to emails and help people they are Johnny on the spot to jump on Jira and -1 code.","31/Mar/17 15:03;appodictic;""Open discussions"" in cassandra always start with the concept of ""its not my idea so -1"" which is the exact opposite of ""scratch an itch"". 

""We do not support embedding C* in a container""

Really? who says? where is it said? Who is ""we""?","31/Mar/17 15:10;snazy;[~apassiou], right, it's true for any other slf4j binding. 
Reason is stuff like CASSANDRA-12535 and CASSANDRA-13173, which are hard to figure out and even harder to ensure its functionality in unit tests. That's why I'm against such a change. We cannot foresee the consequences, because we have not tested other bindings. Even further, the performance implications of using another logger implementation are not determined. Believe me, it's not blindly shooting something down - I had a hard time to fix this issue and do not like to see it happen again. BTW: It's late in the afternoon over here, so it's not a too quick reaction early in the morning.","31/Mar/17 15:15;appodictic;{quote}
Reason is stuff like CASSANDRA-12535 and CASSANDRA-13173, which are hard to figure out and even harder to ensure its functionality in unit tests. 
{quote}

So because someone made bugs in the past, which are ""hard to figure out"" and you can not ""foresee the consequences"" . Is this back to the future part 4?

Please verify your claim of ""not supporting containers"" before finding other reasons to not like the idea of fixing an obvious problem.


","31/Mar/17 15:21;appodictic;So strange:

No such statement about supporting containers seems to exist.
[edward@jackintosh cassandra]$ find . -type f | xargs grep containers
./src/java/org/apache/cassandra/db/ColumnFamilyStore.java:     * thread safety.  All we do is wipe the sstable containers clean, while leaving the actual
./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:        private final List<TokenTreeBuilder> containers = new ArrayList<>();
./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:                containers.add(keys);
./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:            if (containers.size() > 0)
./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:                for (TokenTreeBuilder tokens : containers)
./src/java/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder.java:            containers.clear();
./conf/jvm.options:# This helps prevent soft faults in containers and makes
Binary file ./build/classes/main/org/apache/cassandra/index/sasi/disk/OnDiskIndexBuilder$MutableDataBlock.class matches
[edward@jackintosh cassandra]$ find . -type f | xargs grep Containers

Its almost as if people just make up things, and then when you corner them on their position being false they just pivot and make up a new reason not to like the idea.","31/Mar/17 15:31;apassiou;Edward, I appreciate that you wanted to help but please stop hijacking my question, or at least try to be constructive...

I don't know what you or Robert have understood when I said ""container"" but in my case it's just an application (a plain Java main()) that instantiates a CassandraDaemon and sometimes other stuff. But one could also do it in a ""unit"" test (which is not really a unit bt more an automatic integration test).

@Robert, I can understand your concerns about not-tested behavior of other bindings, then shouldn't it be stated in the docs that other bindings are not supported, and a more explicit error thrown?
But I don't think the performance impact is a good argument because logback and slf4j are configurable by themselves with configuration files it can have a very strong impact on the performance (log patterns, where you log to) even if one uses logback.","31/Mar/17 15:57;snazy;[~apassiou], C* is meant to run as a standalone application using the dependencies that are in the {{lib/}} folder. Any change to those dependencies and the way C* is started, is basically up to the person who changes the dependencies. We can of course talk about using a different logger implementation instead of logback and discuss the pros and cons. But that is IMO way beyond an {{instanceof}} check.

I'm generally concerned about stability and hidden performance issues and a change to a (logger implementation) library, which is nearly everywhere in the hot code path. Mean, we use logback now for a really long time - but we have no test nor production experience running something else. One example: one thing that may happen is some hidden contention in that logger library causing weird outliers - people would complain that C* is slow but don't realize it's in this case because of that change. That's one reason why we are so careful with library updates especially in minor versions. All I'm saying is, that getting _all_ the consequences of such a change is a lot of work.","31/Mar/17 15:59;snazy;bq. just an application (a plain Java main()) that instantiates a CassandraDaemon

If that's just for testing, why not just use logback?","31/Mar/17 16:25;appodictic;{quote}
people would complain that C* is slow but don't realize it's in this case because of that change. 
{quote}

First, its an obvious bug. The entire point of plug-gable logging implementations is so that you can replace them. 
 
Second, the only person being actually affected would be Anton, because effective no one else is changing logging implementations so no one else is hitting that block.

For Anton (and anyone else) they would have to manually change the files in the lib folder and the configuration. So nothing is 'hidden' to him. He/They make a change and they can report if there actually is a performance issue.  

Because they can ""scratch their itch"" of running Cassandra in a container they might find new problems or they might make new opportunities. For example, they may find that some other implementation is actually better or faster. 

If anyone was actually trying to convince me that this bug is intentional, (which is almost laughable). The proper practice would be:

{code}
if (!logger instanceof XYZ){
  throw new IllegalArgumentException(""we only support XYZ for reasons ABC"");
}
{code}

But instead we are attempting to pretend the opposite, that the bug is intentional and the correct thing to do is throw a ClassCastException. Which is a joke.
","01/Apr/17 04:37;jjirsa;Seems pretty reasonable to me

Certainly logback isn't the only performant slf4j logger available.
","01/Apr/17 04:56;jjirsa;Changes back to bug, because even if the belief is that other loggers shouldn't be encouraged, we surely can do better than throwing a cast exception

Given that log4j2 is likely faster than logback and has been suggested as far back as 2013 CASSANDRA-5883 it seems like artificially forcing logback is a position that would need to be more rigorously defended - I'm +1 on this change conceptually (but this is not a review).
","01/Apr/17 13:52;appodictic;Rigorous defenses are in no short supply around here.

I'm sure someone next will argue that this was intended to 
{code}
    /**
     * The purpose of this class is
     */
{code}
because the purpose code is soooo self documenting it describes itself. Want to fix it? No -1 the comment is perfect and heavily tested!","03/Apr/17 08:09;blerer;The problem that I see with this ticket is the following: ""Once we agree to allow people to use the loggers that they wish we somehow become responsible for the bugs that can show up"".
As [~snazy] point it up, some of those issues might be non trivial to figure out. Simply because when someone will open a bug he might not mention that he changed the logging library and we might end up wasting a lot of time to reproduce the problem.
Due to that, I tend to be in favor of Robert suggestion of not supporting it (years spend debugging crappy issues have made me somehow paranoiac).

Now, I think it also make sense to allow people to use another logging library as long as they know that we do not fully support it.
My proposal woud be to log a warning at startup saying that the logging library that they use is not supported.
","03/Apr/17 08:28;spodxx@gmail.com;I'm leaning to go with the proposed patch with a warning message as suggested by Benjamin. But in any case, this needs to be documented. I'd propose that any patch would also have to add a page [here|http://cassandra.apache.org/doc/latest/configuration/index.html] describing if and how logging can be customized and with a list of all relevant config files.","03/Apr/17 09:11;snazy;bq. add a page here describing if and how logging can be customized and with a list of all relevant config files
bq. it also make sense to allow people to use another logging library

Don't get me wrong, but documenting something that is not supported and has never seen CI does not sound good. It would give people just skimming that page the impression that other logging backends _are_ actually supported. Before we document that, we should have full CI for those backends in place - i.e. all utests and dtests for each backend - or _at least_ document something like ""the combination of C* version X.Y and \[logger backend version Z\] was CI-tested on X/Y/Z using this configuration"". If we allow people to use other logging backends, there must be a way to tell them ""version X of Y should be good, because it has been CI tested"".
Anyway, supporting another logging backend still sounds like a new feature to me.","03/Apr/17 09:25;apassiou;All I was asking for is avoid crashing, and I am concious that I am not in a standard use-case.
I am perfectly fine with a big warning in the log saying that using something other than logback is at my own risk + the the doc that states that nothing but logback is officially supported which makes you not responsible for any bug. 

Of course, as Benjamin says, there is still the risk of you investigating an issue without knowing that a different logger is used, but if you are really paranoid you can also imaging people configuring their logback in a very inefficient or wrong way and create crappy issues while being perfectly ""legal"".","03/Apr/17 09:28;spodxx@gmail.com;I did not suggest to that we should support different loggers, just because we document Cassandra logging:

bq. add a page here describing **if** and **how** logging can be customized and with a list of all relevant config files

We can as well state something like ""Using any logging library not shipped with Cassandra is NOT supported. Use at your own risk."". If that is the consensus of this discussion.
","03/Apr/17 09:40;snazy;bq. I did not suggest to that we should support different loggers

Ah, ok, got it wrong then. Adding a page about how to configure logback (and point to the logback docs) sounds good.
","03/Apr/17 09:40;slebresne;bq. This change will cause weird and hard to catch follow-up issues (see the discussions and issues around that piece code)

So why isn't there *any* comment around the code the patch updates to explain why this exists in the first place and why it's so important that it's here?

",03/Apr/17 10:35;snazy;Ninja'd comments for this code.,03/Apr/17 13:56;appodictic;-1 on ninja fixes. ,"03/Apr/17 14:00;appodictic;{quote}
We can as well state something like ""Using any logging library not shipped with Cassandra is NOT supported. Use at your own risk."". If that is the consensus of this discussion.
{quote}

-1 The point of this Jira is not to create some new policy to avoid committing things. Fake blockers like 'this might cause a bug in the future' are not a valid technical reason to reject something. If that is the case shutdown all development on everything. I appreciate the attempt to compromise, but this is not the right direction. ""Not supported"" puts this in a trigger like situation. It will create a policy that will stop active research. Anyone will be free to reject further development using the ""not supported"" argument regardless of how baseless it is. ","03/Apr/17 14:59;blerer;We do not run any test with another logging library and I do not think that we plan to do it. Which, for me means that we do not officially support any other library. Warning the user about it seems normal to me. As a user I would prefer to know. ","03/Apr/17 16:15;jjirsa;There are really three issues:

1) The existing comments were clearly inadequate, and that's been ninja'd into place. +1 on that. 

2) Throwing a ClassCastException is objectively wrong. The patch fixes that, and should be committed. 

3) As a side effect, the patch allows other loggers, almost all of which are untested. The assertion from [~snazy] is that doing so is dangerous, specifically citing past bugs where other loggers which may do IO and cause sandbox access problems. That's a valid concern, and worth a logged warning in my opinion.

Like [~spodxx@gmail.com] (and I think [~blerer]) suggest above, I think Ed's patch+warning makes sense to me.

If someone wants to ""officially"" support another logger in order to remove the warning, then I think the burden is on them to open a proper ticket and demonstrate that it's sufficiently tested.","03/Apr/17 17:16;appodictic;1) ninja fix 
How does meritocracy work when we spend globs of time striking down patches, while simultaneously 'ninja fixing' stuff? Go make a patch and get it reviewed like everyone else. 

2) Agreed.

3) What a backwards argument. The ""critical past bugs"" sited in CASSANDRA-12535 where caused by the person attempting to drop the -1 on this patch. This directly translates to ""No one can edit the buggy code I introduced because THEY might make bugs.""  Consider throwing a GetOffMyLawn exception
https://github.com/apache/cassandra/commit/8f15eb1b717548816a9ee8314269d4d1e2ee7084


 ","03/Apr/17 18:37;jjirsa;{quote}
How does meritocracy work when we spend globs of time striking down patches, while simultaneously 'ninja fixing' stuff? Go make a patch and get it reviewed like everyone else.
{quote}

The project has always allowed ninja fixing minor (especially non-code) things. Comments here are a net positive. There's no reason to fight about adding comments after the fact. 

{quote}
This directly translates to ""No one can edit the buggy code I introduced because THEY might make bugs.""
{quote}

Logging a warning for users isn't the same as throwing an exception. It's not like we're talking about a system property here that requires explicit operator involvement to even run with another logger, it's logging a single warning message that bugs may happen and we haven't actively tested other configs. I don't think that's unreasonable, and it's not ""get off my lawn"". This isn't an unreasonable compromise - we don't crash, but we give operators a chance to know that they're running an untested config. ","03/Apr/17 18:40;appodictic;A true cassandra special. A patch with a dubious ClassCastException and a half finished comment passed a review, and now the next person who touches the code needs to ""sufficiently test"" to earn the ""officially supported"" designation only granted to committers that make untrue statements like ""We do not support embedding C* in a container "" and ninja fix stuff.","03/Apr/17 19:14;appodictic;{quote}
Logging a warning for users isn't the same as throwing an exception. It's not like we're talking about a system property here that requires explicit operator involvement to even run with another logger, it's logging a single warning message that bugs may happen and we haven't actively tested other configs. I don't think that's unreasonable, and it's not ""get off my lawn"". This isn't an unreasonable compromise - we don't crash, but we give operators a chance to know that they're running an untested config.
{quote}

The ""get off my lawn"" is related to this entire process. It had not even checked who added the code originally. I did not quite understand why it got a -1 so fast. -1s are ""rare"" and kill the proposal dead.

https://www.apache.org/foundation/voting.html
For code-modification votes, +1 votes are in favour of the proposal, but -1 votes are vetos and kill the proposal dead until all vetoers withdraw their -1 votes.

The original reasons given were ""This change will cause weird and hard to catch follow-up issues (see the discussions and issues around that piece code), which cannot be caught by neither unit nor dtests because it's an unsupported setup. We do not support embedding C* in a container (i.e. a JVM not controlled ""by us"")""

Lets break this down:
1) ""This change will cause weird and hard to catch follow-up issues""
Hard to quantify and the statement itself is a hypothesis. Can ""WILL CAUSE"" be proven? No.

2) which cannot be caught by neither unit nor dtests because it's an unsupported setup
Even though we are SURE issues that ""WILL HAPPEN"" they ""CANNOT BE CAUGHT"" . Amazing how that logic works.

3) We do not support embedding C* in a container
Untrue. How does one run the CDC daemon? Not a written rule anyway.

If adding a single if statement to block of code and getting 3 completely ludicrous objections from the person who happened to write said code is not ""get off my lawn"" then I don't know what is.

","03/Apr/17 19:29;appodictic;{quote}
The project has always allowed ninja fixing minor (especially non-code) things. Comments here are a net positive. There's no reason to fight about adding comments after the fact.
{quote}
No. It is important. The -1 er is using his technical insight as a justification for his -1. The incomplete comment shows how much time he really spent working on the given code. No tests, no argument checking, and a half done comment.","03/Apr/17 20:28;jjirsa;Let's focus on the problems and solutions. 

There were missing and incomplete comments around a broken piece of code that fixed a very-hard-to-troubleshoot bug. We've fixed the missing and incomplete comments, we still need to fix the broken code, and we can do so without ignoring the past very-hard-to-troubleshoot-bug. 

We have a patch that fixes the ClassCastException, which should be reviewed. We have a (non-binding) -1 on that review. One of the thing that 3 committers (including myself) seem to have suggested is at least adding a warning. [~snazy] is that agreeable to you?


","04/Apr/17 10:39;snazy;[~jjirsa], can live with that - i.e. logging an explicit warning using a new {{StartupCheck}} that also mentions that UDFs/UDAs might be broken, if a logger that's not logback is used.",04/Apr/17 12:20;apassiou;Happy to see that everybody seem to converge to a reasonable solution ;-),"04/May/17 06:58;eugenefedoto;This is my first ticket. [~jjirsa] helped me get started. Since it hasn't been updated in a while, I've taken the liberty of adapting [~appodictic]'s patch to match the consensus described in this ticket. GitHub links below:

https://github.com/eugenefedoto/cassandra/tree/13396-3.0
https://github.com/eugenefedoto/cassandra/tree/13396-3.11
https://github.com/eugenefedoto/cassandra/tree/13396-trunk","04/May/17 15:56;jjirsa;[~eugenefedoto] - two quick notes

1) In all three versions of your patch, we can't [throw|https://github.com/eugenefedoto/cassandra/commit/5957dd84aaa239d62f40aa4bf5f3159bf7a300d7#diff-30a3dbf7d783cf329b5fb28a8b14332eR110] in {{ThreadAwareSecurityManager.java}} or we'll end up with the same problem we had before.

2) In your [StartupCheck|https://github.com/eugenefedoto/cassandra/commit/5957dd84aaa239d62f40aa4bf5f3159bf7a300d7#diff-a5df240149285ae528cdd3c41aa59360R419] , the second log line (on L419) isn't necessary.

[~snazy] - Eugene is a new contributor, I've talked him through how to contribute offline via email, and probably shaped his approach (notably, the {{instanceof}} check from Ed's patch isn't sufficient, because if logback has been removed from the classpath, we'll throw a {{NoClassDefFoundError}} instead). Given that, do you want to review? ",04/May/17 16:58;eugenefedoto;I made the [suggested changes by Jeff|https://issues.apache.org/jira/browse/CASSANDRA-13396?focusedCommentId=15996968&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15996968].,"29/Jun/17 22:37;gus_heck;Looking forward to the resolution of this issue in any of the following ways:

1) Don't load this security manager and policies if UDF's are configured to be disabled 
2) Handle other possible loggers conditionally (log4j2 being my case)
3) Provide an option to run with insecure UDF's ( by not installing this security manager). Not everyone is exposing UDF's to folks they don't trust. In some use cases it might be a feature to be able to read system properties etc.

Glancing at the discussion it sounds like this is heading towards a ""break UDF's but continue"" strategy, which will also work for me since I don't need UDF's but seems likely to trip folks.

My exact itch is documented here: https://github.com/nsoft/jesterj/issues/89

If option 1 or 3 were available, that would greatly simplify my life, because this security manager installs policies in a class initializer and these policies assume a codePath with a url scheme of ""file"" but in my case the scheme is ""onejar""... which forced me into lots of gyrations to force an early load and then un-set your policies so that the rest of my code could have permissions.
","29/Jun/17 22:43;gus_heck;And yes I might be interested in providing a patch for 3 if folks seem in favor... 1 is probably beyond my knowledge of Cassandra, but a version of 3 dependent on a system property seems tractable.","30/Jun/17 02:56;gus_heck;After some IRC discussion, I've been encouraged to submit a patch. Here's an implementation of my #3 suggestion above: 

https://github.com/nsoft/cassandra/commit/382a44c238b6d4bd7e3d8cc7bbd6710b0a7c5274

Though Github's diff has done a fabulous job of obfuscating it, the patch is very simple all I did was add a constant, and two conditions that read the system property represented by the constant and prevent this security manager and its policies from getting installed via the install() method if the system property has been set. 

Circle CI here: https://circleci.com/gh/nsoft/cassandra/2 (still running as of this comment, but I expect it to pass) Ran tests locally and Installed a version with this patch in JesterJ and everything seems happy there.","05/Jul/17 16:16;gus_heck;FYI, Circle CI did pass. Any commentary or review would be appreciated. I won't be able to release without knowing what direction this issue is going.",30/Aug/17 19:00;gus_heck;Any one have a chance to look at my patch yet?,"04/Sep/17 12:39;ehubert;We faced the same underlying issue after upgrading from Cassandra 3.9 to 3.11.0 when using Cassandra embedded for integration testing using JUnit. 
As our application uses a different logging backend and we did not want to switch it and provide appropriate redundant configuration for logback, we excluded logback dependencies and only provided our implementation to also avoid any warnings about duplicate bindings. This setup worked fine with Cassandra 3.9, but fails with Cassandra >= 3.10; the server does not startup, because of the missing classes. So in this case any patch working with instanceof checks still attempting to load those classes without specific try/catch would obviously also fail. 

In addition to SMAwareReconfigureOnChangeFilter in org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager.install() using multiple logback internals (added with CASSANDRA-12535) I also found the change with CASSANDRA-12509 adding ch.qos.logback.core.hook.DelayingShutdownHook in StorageService#initServer problematic.
Would it be an alternative to handle all access to the underlying logging implementation via reflection? 
E.g. attempting to load logback classes and only if this does not fail, perform implementation specific actions via reflection (otherwise log a warning about missing logback presence, which can be ignored in integration test setups). We are mostly talking about one-time initialization, so the performance impact should be really negligible.
This solution would require users to properly exclude logback logging libs if they want to use other sf4j implementation bindings. Providing multiple logging implementations with sl4fj bindings anyway triggers a warning which should be handled.","06/Sep/17 12:00;ostefano;I had the very same issue when using the {{EmbeddedCassandra}} from {{spark-cassandra-connector}}. Moved to {{logback}} fixes the issue. This was a bit annoying because I had to exclude all other sf4j implementations (there's no way afaik to force one implementation over another in case multiple ones are loaded). Anyway, anything but a ClassCastException is better option imho.","17/Oct/17 12:50;fedefernandez;I have the same problem while attempting to use an embedded Cassandra instance for integration tests in Scala projects. Sadly, I found another strange behavior in SBT that avoids me removing {{log4j-slf4j-impl}} from the classpath (if someone is interested [here is the ticket|https://github.com/sbt/sbt/issues/3645]).

I'm stuck in 3.9 version, so I'm looking forward to a solution on this.","14/Feb/18 15:44;claudenw;It is unclear to me whether or not the proposed patch has been/will be accepted into the code base.  What is the current thinking and if it is to be added when might that be?

I have hit this issue when trying to run Cassandra in a unit test situation where I am testing an implementation of an SPI and am unable to convert product to logback.","14/Feb/18 17:49;jasobrown;OK, looks like this has drifted for long enough. I'm gonna look at the patches and review for commit.

UPDATE: I'll wait until CASSANDRA-14183 is committed before attacking this (as it touches logging, as well)","14/Feb/18 20:41;ehubert;[~jasobrown], maybe you can have a glance on [my comment|#comment-16152569] as well. I have not started working on a patch, because I did not receive any feedback on my suggestion on a conceptual level. I did not want to waste time working on an implementation without knowing of a chance of inclusion. Because we faced some issues in production which are either fixed in 3.11.1 or will be fixed in 3.11.2 we really would like to update, but this still blocks us.","14/Feb/18 20:56;jasobrown;[~ehubert] 3.11.2 is up for vote & release this week, so this patch will not be included in that release. Do you have any test cases you can offer here? Please note that allowing other logging implementation seems to be used seems in scope, but will *not*, in any way, be supported. I'll need to reread through all the commentary again to get the full scope of what people are really asking for, but I'm at least willing to get this to completion.","14/Feb/18 22:18;ehubert;[~jasobrown], fair enough. From what I read/understood the majority of users (if not all, definitely including us) facing this issue, wanted to use Cassandra as an embedded server (mostly for integration testing purposes) utilizing class org.apache.cassandra.service.CassandraDaemon - so no production deployment of a standalone server or cluster.

While running in the same JVM alongside an application using slf4j with any slf4j supported backend != logback after upgrading to Cassandra 3.10 or later you are in trouble and no longer able to start the Cassandra server, although logging/logging performance are none of your (primary) goals/concerns.

I doubt there are many users who want to configure a standalone single or multi-node Cassandra installation using a different logging backend to use this in production and have your support, but many users want to write automated integration/scenario tests of their own application interacting with Cassandra using an embedded Cassandra server in addition to plain unit tests using mocks without being forced to switch the logging backend chosen for their application for similar reasons you have. Therefore I see no conflict at all.

 

An implementation could even somehow enforce the differentiation between embedded and standalone usage similar to ""runManaged"" in CassandraDeamon to only allow/support other logging backends (skip special logging backend specific configuration) when CassandraDeamon is used/configured differently than done from main() used by a standalone server installation, if this is really a concern you want to see addressed.

Even in this case one should exchange nasty runtime exceptions or even JVM errors (ClassCastException or NoClassDefFoundError) with a dedicated error message:
""When using a Cassandra standalone installation the only supported logging backend is logback.""
For ClassCastException add something like ""slf4j is currently bound to a different logging framework. Please ensure your classpath only contains logback implementations!""
For NoClassDefFoundError add something like ""No logback implementation was found. Please ensure your classpath contains the bundled logback implementation!""
You can decide to abort the startup or have the same behavior as for the embedded case, but only providing a detailed error logging regarding the unsupported setup.

For embedded use cases one could advice programmers to activate the CassandraDaemon differently (e.g. some parameter) and here I would propose to simply not execute all logback specific configuration logic - e.g. try loading specific logback classes via reflection, so in this mode logback could be easily replaced by any slf4j logging backend which the application currently uses without further adjustments.

JUnit test cases might be a bit tricky, because I think they involve different classpath setups of the used test runner to simulate/trigger those type of issues.",15/Feb/18 00:53;jasobrown;[~ehubert] Thank you. This use case summary was very helpful.,"15/Feb/18 07:23;claudenw;+1 for [~qb3rt] summary, it touches all the points I was going to make and then some.

 ","19/Feb/18 23:02;gus_heck;Though my embedded usage is not only for unit test, my choice of Cassandra relates more to the fact that you are Apache licensed, pure java and clustered rather than performance concerns. When there's a viable alternative I'll worry about whether or not you're faster... in the mean time, I'm very happy to be responsible for (or take credit for) any performance variation from plugging in my preferred logging framework (log4j2). Please don't use performance worries as an excuse to not fix this. Generally +1 on Eric's summary also. I don't mind doing something extra to enable pluggable logging so that you can default to your supported config, so long as it doesn't impact the command line invocation of my project (i.e. requiring -D or -agentlib, etc).","20/Mar/18 13:17;ehubert;Hi [~jasobrown]! Today, I took some time to prepare a patch against the Cassandra 3.11 branch which basically:
 * bundles all logback implementation specific functionality in one class (required a bit of code reorganization)
 * extracted an interface to be able to a) minimize use of reflection and b) be able to provide alternative implementations (the patch itself only provides a no-op fallback implementation)
 * load and instantiate logging-implementation specific extension according to used slf4j binding via reflection (Cassandra code only works on new interface which has no java class dependencies to specific implementations)

So far there are no new (integration) tests which likely would also require some classpath /  ClassLoader magic.

I tested the change using ""a neutral"" application use case by utilizing [Cassandra Unit|https://github.com/jsevellec/cassandra-unit].

The ""test"" involved adjusting log4j config from Cassandra Unit test resources, changing the used cassandra-all version in parent pom, excluding logback deps from the pom and executing any of the tests.

With stock Cassandra 3.11.2 we see:
{code:java}
2018-03-20 10:51:43,753 [pool-2-thread-1] ERROR cassandra.service.CassandraDaemon - Exception encountered during startup
java.lang.NoClassDefFoundError: ch/qos/logback/classic/Logger
    at org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager.install(ThreadAwareSecurityManager.java:92)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:192)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:602)
    at org.cassandraunit.utils.EmbeddedCassandraServerHelper.lambda$startEmbeddedCassandra$1(EmbeddedCassandraServerHelper.java:144)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: ch.qos.logback.classic.Logger
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 7 more
{code}
Using the patch [^CASSANDRA-13396_ehubert_1.patch] I provided we have a successful server startup with a warning:
{code:java}
2018-03-20 13:47:32,688 [pool-2-thread-1] WARN  utils.logging.LoggingSupportFactory - You are using Cassandra with an unsupported deployment. The intended logging implementation library logback is not used by slf4j. Detected slf4j binding: org.slf4j.impl.Log4jLoggerFactory. You will not be able to dynamically manage log levels via JMX and may have performance or other issues.
{code}

Please consider this as an initial patch suggestion to gather quick feedback on the approach! I'm willing to adjust things according to your requirements or are happy if you like to tweak it to your requirements.

Would be great to see this in Cassandra 3.11.3 if possible.","23/Mar/18 09:25;ehubert;Anyone with some feedback on my submitted patch? I may have some time tomorrow to incorporate feedback/make adjustments etc, but next week might become rather busy.","23/Mar/18 15:34;claudenw;Scanning through the solution it looks good to me.  However, I have not merged it or tried it.  I have no objections to applying it.","23/Mar/18 21:51;jasobrown;[~ehubert] the attached 'patch' file completely fails to apply to cassandra-3.11. Can you either push up a git patch or diff file, or just push your branch up to your github repo and share the link?  Reading the code in the attachment now,,,,","23/Mar/18 22:04;jasobrown;Looks like you are using {{StaticLoggerBinder}} to figure out which logging implementation to use. That seems [deprecated|https://www.slf4j.org/faq.html#changesInVersion18] as of slf4j v1.8 (soon to be released), in lieu of some JIgsaw modularization. I have absolutely zero desire to investigate that modularization, wrt to cassandra as a whole. What are the alternatives here? Remaining on the current version of slf4j is certainly an option

ftr, 3.11 uses slf4j 1.7.7","23/Mar/18 22:44;ehubert;Hi Jason! Sorry about the patch file. I now quickly recreated it from command line using git diff after pulling all updates from the 3.11 branch and attached it as  [^CASSANDRA-13396_ehubert_2.patch].
Regarding your question about alternatives to determine the logging implementation slf4j bound to when using slf4j >= 1.8 I do not think this will be much related to Jigsaw modularization (although I may be wrong). Looks like they make use of the long existing ServiceLoader JDK implementation (AFAIK since JDK6). I happily offer to investigate this. Other alternatives to simply do something similar by attempting to load specific implementation classes via reflection is by far not as elegant if you ask me.
","23/Mar/18 23:34;ehubert;I'm willing to test this out, but I think for slf4j >=1.8 we may need something like the following untested litte utility method to do the job of determining the binding
{code}
private static SLF4JServiceProvider determineSlf4jProvider() 
{
        ServiceLoader<SLF4JServiceProvider> serviceLoader = ServiceLoader.load(SLF4JServiceProvider.class);
        Iterator<SLF4JServiceProvider> serviceIterator = serviceLoader.iterator();
        if (serviceIterator.hasNext()) 
        {
            return serviceIterator.next();
        } else {
            return new NOPServiceProvider()
        }
}
{code}
or via reflection make org.slf4j.LoggerFactory#getProvider() accessible and call it
or provde an own class in package org.slf4j and wrap this static message call to this package-scoped method.

Maybe this is all not necessary and one can directly work with public org.slf4j.LoggerFactory#getILoggerFactory() using the bound provider to return a implementation-specific factory. As said, I'll happily have a closer look at this. I'm pretty sure something which can be addressed rather easily. The latter might even be somehow possible to use for a compatible implementation although returned implementation classes are very likely completely different for those slf4j versions.","24/Mar/18 00:18;jasobrown;hmm, yeah those aren't great options. I think we can stick with the {{StaticLoggerBinder}} while we're still on slf4j < 1.8, to keep it simple. Maybe we can include a note for the future for when we do upgrade slf4j 1.8. wdyt?","24/Mar/18 01:27;jasobrown;[~ehubert] I can successfully apply your patch now. I moved the logback jars out of the way, added slf4j-jdk14-1.7.7.jar into lib, and was able to bring up cassandra without problem with the patch. Got this message as output:

{noformat}
WARNING: You are using Cassandra with an unsupported deployment. The intended logging implementation library logback is not used by slf4j. Detected slf4j binding: org.slf4j.impl.JDK14LoggerFactory. You will not be able to dynamically manage log levels via JMX and may have performance or other issues.
{noformat} 

Thus the code went down the {{else}} block.

Review will continue ...","24/Mar/18 17:42;ehubert;[~jasobrown] thanks for your feedback, which helped me to adjust the logging implementation detection from using an slf4j implementation depended mechanism ({{StaticLoggerBinder}}) instead just using what is available via its public API {{org.slf4j.LoggerFactory#getILoggerFactory()}} which is totally sufficient for this purpose (see updated patch [^CASSANDRA-13396_ehubert_3.patch]).

I now also tested some slf4j backend implemenations with different versions of slf4j:

+slf4j 1.7.12 - JUL:+
 WARNING: You are using Cassandra with an unsupported deployment. The intended logging implementation library logback is not used by slf4j. Detected slf4j logger factory: org.slf4j.*impl*.JDK14LoggerFactory. You will not be able to dynamically manage log levels via JMX and may have performance or other issues.

+slf4j 1.7.12 - Logback 1.1.3 or 1.2.3:+
 No warning - successful Cassandra startup

+slf4j 1.8.0-beta2 - JUL:+
 WARNING: You are using Cassandra with an unsupported deployment. The intended logging implementation library logback is not used by slf4j. Detected slf4j logger factory: org.slf4j.*jul*.JDK14LoggerFactory. You will not be able to dynamically manage log levels via JMX and may have performance or other issues.

+slf4j 1.8.0-beta2 - Logback 1.3.0-alpha4+

_logback depencendy upgrade to 1.3.0 required - slf4j upgrade to 1.8.x should only be done once logback 1.3.x release is availabe_
 No warning - successful Cassandra startup

I did not test whether the logback-implementation specific code (with my patch centralized in {{org.apache.cassandra.utils.logging.LogbackLoggingSupport}}) is still working properly after this library upgrade. I only tested implementation detection does not need to be adjusted when upgrading to the newer slf4j version.","24/Mar/18 21:13;githubbot;GitHub user e-hubert opened a pull request:

    https://github.com/apache/cassandra/pull/210

    CASSANDRA-13396: 

    Centralize logback specific implementation and load/instantiate it via reflection

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/e-hubert/cassandra CASSANDRA-13396

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/210.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #210
    
----

----
","26/Mar/18 11:27;jasobrown;In addition to pulling in the minor fix [~ehubert] added over the weekend, I made some minor editorial changes, and committed as sha {{bd0804065daaa01ba478c0ed97f7411f1180eef9}}. Thanks!","26/Mar/18 15:35;githubbot;Github user e-hubert commented on the issue:

    https://github.com/apache/cassandra/pull/210
  
    Closing this pull request as thanks to @jasobrown those changes were pushed to 3.11 with some editorial changes.
","26/Mar/18 15:35;githubbot;Github user e-hubert closed the pull request at:

    https://github.com/apache/cassandra/pull/210
"
The keyspace repairTime metric is not updated,CASSANDRA-13539,13073170,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cnlwsu,cnlwsu,cnlwsu,18/May/17 15:10,12/Mar/19 14:03,13/Mar/19 22:35,19/May/17 21:09,4.0,,,,,Legacy/Observability,,,,,0,,,,repairTime metric at keyspace metric isnt updated when repairs complete so its always zeros.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-05-18 15:28:46.271,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri May 19 21:09:48 UTC 2017,,,,,,0|i3f6ev:,9223372036854775807,,,,,,,,bdeggleston,bdeggleston,,,,,,,,,,"18/May/17 15:28;githubbot;GitHub user clohfink opened a pull request:

    https://github.com/apache/cassandra/pull/113

    Update repairTime for keyspaces on completion (CASSANDRA-13539)

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/clohfink/cassandra 13539

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/113.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #113
    
----
commit 2132d7b66f05cd9c6e5b4488a5025754456c82cc
Author: Chris Lohfink <clohfink@apple.com>
Date:   2017-05-18T15:27:51Z

    Update repairTime for keyspaces on completion (CASSANDRA-13539)

----
","18/May/17 22:14;bdeggleston;+1, will commit after [utests|https://circleci.com/gh/bdeggleston/cassandra/52] finish","19/May/17 21:09;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/113
",19/May/17 21:09;bdeggleston;Committed to trunk as {{e1f2300a1ae7dab1660c16fc38bcb852fdcd44ef}}. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup ParentRepairSession after repairs,CASSANDRA-13359,13057894,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,21/Mar/17 13:14,12/Mar/19 14:03,13/Mar/19 22:35,21/Mar/17 14:45,4.0,,,,,,,,,,0,,,,"As part of removing anti-compaction code in CASSANDRA-9143, the call the removeParentRepairSession was not moved into the repair complete callback.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-21 14:33:12.948,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 21 14:33:12 UTC 2017,,,,,,0|i3ckun:,9223372036854775807,,,,,,,,,,,,,,,,,,,21/Mar/17 14:31;bdeggleston;patch here: https://github.com/bdeggleston/cassandra/commits/13359,21/Mar/17 14:33;krummas;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race / ref leak in PendingRepairManager,CASSANDRA-13751,13093247,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,08/Aug/17 17:29,12/Mar/19 14:03,13/Mar/19 22:35,10/Aug/17 19:03,4.0,,,,,,,,,,0,,,,"PendingRepairManager#getScanners has an assertion that confirms an sstable is, in fact, marked as pending repair. Since validation compactions don't use the same concurrency controls as proper compactions, they can race with promotion/demotion compactions and end up getting assertion errors when the pending repair id is changed while the scanners are being acquired. Also, error handling in PendingRepairManager and CompactionStrategyManager leaks refs when this happens.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-09 08:02:44.475,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 10 19:03:45 UTC 2017,,,,,,0|i3ijyn:,9223372036854775807,,,,,,,,krummas,krummas,,,,,,,,,,"08/Aug/17 17:38;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/13751]
[utest|https://circleci.com/gh/bdeggleston/cassandra/76]
[dtest (pending)|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/171/]",09/Aug/17 08:02;krummas;+1 if tests succeed (seems the utests failed as well),10/Aug/17 19:03;bdeggleston;Got the utest passing. dtests failures were flaky/succeeding locally. Committed as {{9c3354e3211c6a3f3982e87477e156c29cd9b7ea}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstabledump reports incorrect usage for argument order,CASSANDRA-13532,13072099,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,varuna,ian.ilsley@datastax.com,ian.ilsley@datastax.com,15/May/17 18:08,12/Mar/19 14:02,13/Mar/19 22:35,11/Aug/17 01:16,3.0.15,3.11.1,4.0,,,Legacy/Tools,,,,,0,lhf,,,"sstabledump usage reports 

{{usage: sstabledump <options> <sstable file path>}}

However the actual usage is 

{{sstabledump  <sstable file path> <options>}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Jun/17 06:30;varuna;sstabledump#printUsage.patch;https://issues.apache.org/jira/secure/attachment/12874398/sstabledump%23printUsage.patch,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-06-25 06:30:31.77,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 01 11:32:32 UTC 2017,,,,,,0|i3ezuf:,9223372036854775807,3.0.4,,,,,,,jasonstack,jasonstack,,,,,,,,,,25/Jun/17 06:30;varuna;I'm submitting a patch to fix the `printUsage` function of `SSTableExport.java`. ,25/Jun/17 06:31;varuna;PFA the patch file,26/Jun/17 14:33;jasonstack;LGTM,"11/Aug/17 01:16;jjirsa;Thanks all! Committed as {{fab384560311ec1f3043fbf6137093ea129afa68}}
","01/Sep/17 11:32;micksear;I just came across this bug myself.  I think, though, that it would be preferable to fix the parser so it consumes a single value per argument.  e.g. :

{code:java}
sstabledump -k mykey1 -k mykey2 mysstable
{code}

Don't you think?  I'd have thought this would be more consistent with the way arguments are normally used on the command line.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Illegal unicode character breaks compilation on Chinese env OS,CASSANDRA-13417,13061816,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jjirsa,jjirsa,jjirsa,05/Apr/17 17:23,12/Mar/19 14:02,13/Mar/19 22:35,05/Apr/17 17:28,3.11.0,4.0,,,,,,,,,0,,,,"Creating JIRA for tracking GH issue https://github.com/apache/cassandra/pull/104

Fix is contained within a comment block, so skipping CI.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,https://github.com/apache/cassandra/pull/104,,,,,,,,,9223372036854775807,,,Wed Apr 05 17:28:22 UTC 2017,,,,,,0|i3d91j:,9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Apr/17 17:28;jjirsa;Committed as {{bfdc1e0fdb3e4adad8d044203feaab8350dfdee8}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest failure: batch_test.TestBatch.batchlog_replay_compatibility_?_test,CASSANDRA-13842,13099785,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,krummas,krummas,05/Sep/17 09:32,12/Mar/19 14:00,13/Mar/19 22:35,07/Sep/17 17:08,,,,,,Test/dtest,,,,,0,dtest,,,"batch_test.TestBatch.batchlog_replay_compatibility_1_test and batch_test.TestBatch.batchlog_replay_compatibility_4_test are failing:
http://cassci.datastax.com/view/cassandra-3.11/job/cassandra-3.11_dtest/160/testReport/batch_test/TestBatch/batchlog_replay_compatibility_1_test/
http://cassci.datastax.com/view/cassandra-3.11/job/cassandra-3.11_dtest/160/testReport/batch_test/TestBatch/batchlog_replay_compatibility_4_test/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-09-05 11:21:57.526,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 07 17:08:24 UTC 2017,,,,,,0|i3jnbb:,9223372036854775807,,,,,,,,jjirsa,jjirsa,,,,,,,,,,"05/Sep/17 11:21;iamaleksey;The test passes 100% reliably on MacOS, but it does fail for me in my Ubuntu VM just as it does with Jenkins.

Don't yet know why the difference is there, but I'll deal with it.","07/Sep/17 17:06;iamaleksey;A simple patch [here|https://github.com/iamaleksey/cassandra-dtest/commits/13842].

The problem with the tests is that they don’t take into account the full behaviour of batchlog replay. In particular,
batchlog will *not* replay a batch if it’s younger than 2 times write rpc timeout. The reasoning for this is that
there is a good chance that the batch can still succeed up until that point and be removed by the remote coordinator -
thus avoiding redundant replay. 2 * timeout is there for batched mutations timeout + remove from batchlog mutation
timeout.

One way to fix this would be to introduce a special argument to batchlog replay methods that would ignore the check and
replay everything it has, ignoring that optimisation. But on 3.0 it has correctness implications, since we store last replayed
batch id as the lower bound for the next replay cycle. In 2.2 it’s a bit safer, but we need it to be addressed in both versions -
and I’m not comfortable with making such changes in minor releases.

The other way to fix it is by introducing a deterministic delay to age the batches by 2 * timeout. It adds 20 extra seconds per test,
for a total of 40 seconds, but I don’t see a better way unfortunately. Shouldn’t be flaky however.

P.S. The reason it passes on MacOS and fails on Linux is that the test runs very, very slowly on MacOS. 160+ seconds vs. 30 seconds on Linux.
It naturally has just enough time for the batch to age, so when we force its replay, it is replayed reliably.","07/Sep/17 17:07;jjirsa;+1
","07/Sep/17 17:08;iamaleksey;Committed as [5893020fd9f2ca783be13a5a0974504632529440|https://github.com/apache/cassandra-dtest/commit/5893020fd9f2ca783be13a5a0974504632529440], thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra crashes on startup.  Crash Problematic frame: # C  [sigar-amd64-winnt.dll+0x14ed4] using JRE version: Java(TM) SE Runtime Environment (9.0+11),CASSANDRA-14137,13127251,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,p2pxd,p2pxd,26/Dec/17 18:05,12/Mar/19 14:00,13/Mar/19 22:35,27/Dec/17 02:26,3.0.15,3.0.16,,,,Legacy/Core,Observability/Metrics,,,,0,,,,"Startup of Cassandra crashes in sigar-amd64-winnt.dll+0x14ed4.
Short term work around: change from Java 9 back to Java 8.

#
# A fatal error has been detected by the Java Runtime Environment:
#
#  EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x0000000010014ed4, pid=7112, tid=1764
#
# JRE version: Java(TM) SE Runtime Environment (9.0+11) (build 9.0.1+11)Problematic frame:
# C  [sigar-amd64-winnt.dll+0x14ed4]
# Java VM: Java HotSpot(TM) 64-Bit Server VM (9.0.1+11, mixed mode, tiered, compressed oops, concurrent mark sweep gc, windows-amd64)
# Problematic frame:
# C  [sigar-amd64-winnt.dll+0x14ed4]
#
# No core dump will be written. Minidumps are not enabled by default on client versions of Windows
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
# ","Cassandra 3.0.15
Java 9.0.1+11
Workaround is to use Java 8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-9608,,,,,,,,,26/Dec/17 17:59;p2pxd;hs_err_pid7112.log;https://issues.apache.org/jira/secure/attachment/12903731/hs_err_pid7112.log,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-12-27 02:25:55.687,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 27 02:25:55 UTC 2017,,,,,,0|i3oaon:,9223372036854775807,,,,,,,,,,,,,,,,,,,27/Dec/17 02:25;KurtG;Java 9 isn't officially supported yet and will likely only be supported for 4.0 onwards. See CASSANDRA-9608,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix weightedSize() for row-cache reported by JMX and NodeTool,CASSANDRA-13393,13060301,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,Fuud,Fuud,Fuud,30/Mar/17 08:52,12/Mar/19 14:00,13/Mar/19 22:35,09/Apr/17 09:02,2.2.10,3.0.13,3.11.0,4.0,,Tool/nodetool,,,,,0,lhf,,,"Row Cache size is reported in entries but should be reported in bytes (as KeyCache do).
It happens because incorrect OHCProvider.OHCacheAdapter.weightedSize method. Currently it returns cache size but should return ohCache.memUsed()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-03-31 13:21:31.978,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 13 08:45:54 UTC 2017,,,,,,0|i3czov:,9223372036854775807,,,,,,,,snazy,snazy,,,,,,,,,,"31/Mar/17 13:21;snazy;[~Fuud], mind to provide a patch?","07/Apr/17 07:08;githubbot;GitHub user Fuud opened a pull request:

    https://github.com/apache/cassandra/pull/105

    Fix invalid value for rowCache size (in MB). CASSANDRA-13393

    Row Cache size is reported in entries but should be reported in bytes (as KeyCache do).
    It happens because incorrect OHCProvider.OHCacheAdapter.weightedSize method. Currently it returns cache size but should return ohCache.memUsed()

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Fuud/cassandra 13393

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/105.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #105
    
----
commit 0fa5ca057016c250a4888b8b6d982fcac569d6a5
Author: Fedor Bobin <fuudtorrentsru@gmail.com>
Date:   2017-04-07T07:07:08Z

    Fix invalid value for rowCache size (in MB). CASSANDRA-13393

----
","07/Apr/17 07:08;Fuud;Patch:
https://github.com/apache/cassandra/pull/105","09/Apr/17 09:02;snazy;Thanks for the patch!

Committed as [470f15be652ffb3c471161d6fb10c8893ff59e46|https://github.com/apache/cassandra/commit/470f15be652ffb3c471161d6fb10c8893ff59e46] to [cassandra-2.2|https://github.com/apache/cassandra/tree/cassandra-2.2] and merged up to trunk.
","09/Apr/17 16:20;githubbot;Github user jeffjirsa commented on the issue:

    https://github.com/apache/cassandra/pull/105
  
    This was committed as 470f15be652ffb3c471161d6fb10c8893ff59e46 - mind closing the PR? 

","13/Apr/17 08:45;githubbot;Github user Fuud closed the pull request at:

    https://github.com/apache/cassandra/pull/105
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
