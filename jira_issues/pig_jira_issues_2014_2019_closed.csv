Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Duplicate),Outward issue link (Incorporates),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Regression),Outward issue link (Regression),Outward issue link (Regression),Outward issue link (Required),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (Hadoop Flags),Custom field (Hadoop Flags),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Patch Info),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Severity),Custom field (Severity),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Test and Documentation Plan),Custom field (Testcase included),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
TestGrunt.testStopOnFailure is flaky,PIG-5245,13076113,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,31/May/17 07:21,21/Jun/17 09:15,14/Mar/19 03:07,31/May/17 20:30,,,,,,0.17.0,,,,,0,,,,,,,"  The test is supposed to run two tests in parallel, and one when fails other should be killed when stop on failure is configured. But the test is actually running only job at a time and based on the order in which jobs are run it is passing. This is because of the capacity scheduler configuration of the MiniCluster. It runs only one AM at a time due to resource restrictions. In a 16G node, when the first job runs it takes up 1536 (AM) + 1024 (task) memory.mb. Since only 10% of cluster resource is the default for running AMs and a single AM already takes up memory close to 1.6G, second job AM is not launched.",,,,,,,,,,,,,,,,,,,,31/May/17 07:33;rohini;PIG-5245-1.patch;https://issues.apache.org/jira/secure/attachment/12870536/PIG-5245-1.patch,31/May/17 13:57;szita;PIG-5245-2.addingSparkLibsToMiniCluster.patch;https://issues.apache.org/jira/secure/attachment/12870577/PIG-5245-2.addingSparkLibsToMiniCluster.patch,31/May/17 08:28;rohini;PIG-5245-2.patch;https://issues.apache.org/jira/secure/attachment/12870540/PIG-5245-2.patch,31/May/17 19:28;rohini;PIG-5245-3.patch;https://issues.apache.org/jira/secure/attachment/12870626/PIG-5245-3.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2017-05-31 08:09:54.885,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed May 31 20:30:05 UTC 2017,,,,,,,0|i3fokv:,9223372036854775807,,,,,,,,,,31/May/17 07:33;rohini;Reduced general resource and heap usage of AM and task containers for mapreduce mode and increased number of AM containers to 20% of cluster resource (node RAM).  Will look into doing it in YARNMiniCluster (tez and spark) as well in 0.18.,31/May/17 08:09;szita;+1 for [^PIG-5245-1.patch],31/May/17 08:28;rohini;Skipped this test for Spark as stop on failure is not implemented. Also added these configs to YarnMiniCluster as well.  Running full unit tests for MR and Tez. Will update once done.,"31/May/17 08:46;kellyzly;[~rohini]: stop_on_failure is implemented in spark mode in [JobGraphBuilder.java|https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/spark/JobGraphBuilder.java#L193]
","31/May/17 11:07;szita;[~kellyzly] I think that has a different mechanism compared to MR, it doesn't fail an existing job, merely prevents the new one to be submitted. I think it would be worth to rethink the stopOnFailure feature wrt. Spark mode. Let's do this in PIG-5247","31/May/17 11:15;nkollar;Having this test fixed for MR and for Tez, but not for Spark, can we still proceed with the release? In my opinion this is not a blocker issue for Spark. [~kellyzly] [~szita] [~rohini] what do you think? +1 for PIG-5245-2.patch","31/May/17 13:55;szita;+1 for [^PIG-5245-2.patch] - I verified all Spark unit tests pass (3,338 tests in 2hr 7min). I don't think this is a blocker issue [~nkollar] we will address Spark vs stopOnFailure in Pig 0.18",31/May/17 14:00;szita;[~rohini] I've spotted a missing part in the YarnMinicluster classpath compilation. If we want to run tests in Spark mode with {{SPARK_MASTER=yarn-client}} config (rather than {{local}} which is now default) we will need the Spark jars for the forked JVMs. I've attached [^PIG-5245-2.addingSparkLibsToMiniCluster.patch] which you can apply after [^PIG-5245-2.patch]. It will be helpful in the future.,31/May/17 19:28;rohini;Final patch including [~szita]'s changes for spark. Also reduced the AM percentage to 0.1 as that itself should allow launching 3AMs with a 16G RAM. ,31/May/17 20:11;szita;+1 for [^PIG-5245-3.patch],"31/May/17 20:30;rohini;Patch committed to branch-0.17 and trunk. [~szita], thanks for the review and debugging of the issue.",,,,,,,,,,,,,,,,,
Several unit tests are failing in Tez mode after merging spark branch,PIG-5244,13075765,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,nkollar,nkollar,30/May/17 08:32,21/Jun/17 09:15,14/Mar/19 03:07,30/May/17 09:53,,,,,,0.17.0,0.18.0,tez,,,0,,,,,,,"Looks like during the merge in build.xml PIG-5105 was accidentally partially reverted:
{code}
<sysproperty key=""build.classes"" value=""${build.classes}"" />
<sysproperty key=""test.build.classes"" value=""${test.build.classes}"" />
<sysproperty key=""ivy.lib.dir"" value=""${ivy.lib.dir}"" />
{code}
was removed.",,,,,,,,,,,,,,,,,,,,30/May/17 08:53;nkollar;PIG-5244.patch;https://issues.apache.org/jira/secure/attachment/12870358/PIG-5244.patch,30/May/17 09:18;nkollar;PIG-5244_2.patch;https://issues.apache.org/jira/secure/attachment/12870362/PIG-5244_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-05-30 09:52:41.102,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 30 09:52:41 UTC 2017,,,,,,,0|i3fmfj:,9223372036854775807,,,,,,,,,,"30/May/17 09:52;szita;+1, thanks for fixing this [~nkollar], [^PIG-5244_2.patch] is now committed to trunk and 0.17 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix datetime related test issues after PIG-4748,PIG-5238,13074511,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,szita,szita,szita,24/May/17 12:55,21/Jun/17 09:15,14/Mar/19 03:07,25/May/17 09:39,,,,,,0.17.0,,,,,0,,,,,,,"https://builds.apache.org/job/Pig-trunk-commit/2483/testReport/org.apache.pig.test

2 test failures:
TestDateTime.testDateTimeZoneOnCluster:
-Missed edge case of UTC

TestPackage.testOperator
-Missed calling DateTimeWritable.setupAvailableZoneIds();",,,,,,,,,,,,,,,,,,,,24/May/17 12:57;szita;PIG-5238.0.patch;https://issues.apache.org/jira/secure/attachment/12869645/PIG-5238.0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-05-24 13:17:02.576,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 25 09:39:40 UTC 2017,,,,,,,0|i3feov:,9223372036854775807,,,,,,,,,,"24/May/17 12:58;szita;[^PIG-5238.0.patch] fixes both test cases.
In the case of TestDateTime.testDateTimeZoneOnCluster I've also added one more input record so that we can properly check against local time zone for both DST and non-DST datetime values.",24/May/17 13:17;nkollar;LGTM,24/May/17 13:50;knoguchi;+1,"25/May/17 09:39;szita;[^PIG-5238.0.patch] committed to trunk, thanks for the review [~nkollar], [~knoguchi]",,,,,,,,,,,,,,,,,,,,,,,,
json simple jar not included automatically with piggybank AvroStorage,PIG-5236,13072518,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,satishsaley,satishsaley,satishsaley,16/May/17 16:41,21/Jun/17 09:15,14/Mar/19 03:08,26/May/17 22:17,,,,,,0.17.0,,,,,0,,,,,,,It would be good to include json simple jar by default similar to joda-time or pig.jar,,,,,,,,,,,,,,,,,,,,16/May/17 16:53;satishsaley;PIG-5236-1.patch;https://issues.apache.org/jira/secure/attachment/12868352/PIG-5236-1.patch,19/May/17 16:25;satishsaley;PIG-5236-2.patch;https://issues.apache.org/jira/secure/attachment/12868999/PIG-5236-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-05-17 23:55:16.588,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri May 26 22:17:16 UTC 2017,,,,,,,0|i3f2fb:,9223372036854775807,,,,,,,,,,17/May/17 23:55;rohini;AvroStorage should implement StoreResources and specify the file in getShipFiles(). Refer to HBaseStorage.,26/May/17 22:17;rohini;+1. Committed to trunk. Thanks for the fix Satish.,,,,,,,,,,,,,,,,,,,,,,,,,,
Typecast with as-clause fails for tuple/bag with an empty schema,PIG-5235,13072454,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,16/May/17 13:57,21/Jun/17 09:15,14/Mar/19 03:08,25/May/17 15:14,,,,,,0.17.0,,,,,0,,,,,,,"Following script fails with trunk(0.17).
{code}
a = load 'test.txt' as (mytuple:tuple (), gpa:float);
b = foreach a generate mytuple as (mytuple2:(name:int, age:double));
store b into '/tmp/deleteme';
{code}

2017-05-16 09:52:31,280 \[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. null

(This is a continuation from the as-clause fix at PIG-2315 and follow up jira PIG-4933)
",,,,,,,,,,,,,,,,,,,,16/May/17 14:16;knoguchi;pig-5235-v01.patch;https://issues.apache.org/jira/secure/attachment/12868327/pig-5235-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-05-18 15:30:10.746,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu May 25 15:14:47 UTC 2017,,,,,,,0|i3f213:,9223372036854775807,,,,,,,,,,"16/May/17 14:16;knoguchi;Log file showed
{noformat}
Pig Stack Trace
---------------
ERROR 2999: Unexpected internal error. null

java.lang.NullPointerException
        at org.apache.pig.newplan.logical.relational.LogicalSchema$LogicalFieldSchema.typeMatch(LogicalSchema.java:174)
        at org.apache.pig.newplan.logical.visitor.ForEachUserSchemaVisitor.visit(ForEachUserSchemaVisitor.java:190)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:87)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
        at org.apache.pig.newplan.logical.relational.LogicalPlan.validate(LogicalPlan.java:179)
        at org.apache.pig.PigServer$Graph.compile(PigServer.java:1851)
        at org.apache.pig.PigServer$Graph.access$300(PigServer.java:1527)
        at org.apache.pig.PigServer.execute(PigServer.java:1440)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:488)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:471)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:170)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:233)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:204)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
        at org.apache.pig.Main.run(Main.java:630)
        at org.apache.pig.Main.main(Main.java:175)
{noformat}

First issue is, {{LogicalSchema.typeMatch}} was not properly comparing the null schema.  Fixed.

After that, script failed with
{noformat}
Caused by: org.apache.pig.impl.logicalLayer.validators.TypeCheckerException: ERROR 1052: Cannot cast tuple with schema mytuple2:tuple(name:NULL,age:NULL) to tuple with schema mytuple2:tuple(name:int,age:double)
        at org.apache.pig.newplan.logical.visitor.TypeCheckingExpVisitor.visit(TypeCheckingExpVisitor.java:518)
        at org.apache.pig.newplan.logical.expression.CastExpression.accept(CastExpression.java:44)
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
        at org.apache.pig.newplan.logical.visitor.TypeCheckingRelVisitor.visitExpressionPlan(TypeCheckingRelVisitor.java:191)
        at org.apache.pig.newplan.logical.visitor.TypeCheckingRelVisitor.visit(TypeCheckingRelVisitor.java:157)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:254)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
        at org.apache.pig.newplan.logical.visitor.TypeCheckingRelVisitor.visit(TypeCheckingRelVisitor.java:174)
        ... 15 more
{noformat}

In LOGenearte, we're converting NULL type to BYTEARRAY for simple types but not for nested-types with TUPLE and BAGs.  Fixed. 

Also while debugging, I got confused on why the schema didn't change after ForEachUserSchemaVisitor.  I added extra {{resetSchema}} for consistency purposes although schema was properly reset after this phase already.

","17/May/17 17:26;knoguchi;Another display issue I noticed while debugging, 'describe' now shows the types BEFORE the typecast.   For example, {{describe b}} using the script in description would show 

{noformat}b: {mytuple2: (name: bytearray,age: bytearray)}{noformat}

since as part of PIG-2315, I've reset the original user defined schema and let the next typecast foreach take care of this.  Trying to see if I can fix this but may push to another jira.",18/May/17 15:30;nkollar;+1,25/May/17 06:37;daijy;+1,"25/May/17 15:14;knoguchi;Thanks for the review Daniel, Nandor !!! 

For the ""describe"" issue, created PIG-5243 to follow up.",,,,,,,,,,,,,,,,,,,,,,,
PigStorage with -schema may produce inconsistent outputs with more fields,PIG-5231,13069254,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,04/May/17 19:24,21/Jun/17 09:15,14/Mar/19 03:08,25/May/17 20:20,,,,,,0.17.0,,,,,0,,,,,,,"When multiple directories are passed to PigStorage(',','-schema'), pig will 
{quote}
No attempt to merge conflicting schemas is made during loading. The first schema encountered during a file system scan is used.
{quote}
For two directories input with schema
file1: (f1:chararray, f2:int) and 
file2: (f1:chararray, f2:int, f3:int) 

Pig will pick the first schema from file1 and only allow f1, f2 access. 
However, output would still contain 3 fields for tuples from file2.  This later leads to complete corrupt outputs due to shifted fields resulting in incorrect references. 
(This may also happen when input itself contains the delimiter.)

If file2 schema is picked, this is already handled by filling the missing fields with null.  (PIG-3100)
",,,,,,,,,,,,,,,,,,,,04/May/17 20:19;knoguchi;pig-5231-v01.patch;https://issues.apache.org/jira/secure/attachment/12866487/pig-5231-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-05-25 18:20:15.909,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu May 25 20:20:23 UTC 2017,,,,,,,0|i3eib3:,9223372036854775807,,,,,,,,,,"04/May/17 20:19;knoguchi;I understand that PigStorage is not designed for handling multiple schemas, but it would be nice if we can still handle this limited case where user simply added more fields leaving the rest of schema untouched.  

In general, we ask our users to use AvroStorage or HcatLoader when they expect schema evolution(merging).

A couple of approaches I can think of.
(1) Scan all schemas and fail if any are different
or
(2) Scan all schemas and pick the schema with longest 
or 
(3) Drop any fields not part of the schema

Attaching a patch for approach (3).","25/May/17 18:20;daijy;Vote for 3. We pick the first schema in dirs in all LoadFunc, such as OrcStorage, AvroStorage. I don't think we shall make an exception for PigStorage. +1 for the patch.","25/May/17 20:20;knoguchi;Thanks for the review Daniel!   Committed it to trunk.

Another option I was dreaming about would check all the schema files and dynamically create load statement for each unique schema, then add all of them with Union onschema. Have no idea if this is even feasible or not :)

For now, I think my quick fix is a good step forward since it fixes the issue with one common use case when PigStorage schema evolves by adding more fields.
",,,,,,,,,,,,,,,,,,,,,,,,,
PreprocessorContext.java can deadlock forever with large stderr,PIG-5226,13065971,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,jtolar,jtolar,jtolar,21/Apr/17 18:51,09/Dec/17 01:57,14/Mar/19 03:08,28/Apr/17 14:38,,,,,,0.17.0,,,,,0,,,,,,,"This code can deadlock on *nix (not sure about Windows...) because of how it's using {{Runtime.exec()}} (ie, stderr fills up its buffer and everything hangs). 

Problematic snippet of code:
https://github.com/apache/pig/blob/38c835ed702799f69dc2fa1ad0fbeab25e42c111/src/org/apache/pig/tools/parameters/PreprocessorContext.java#L238-L272

",,,,,,,,,,,,,,,,,,,,26/Apr/17 17:46;jtolar;pig-runtime-exec-1.patch;https://issues.apache.org/jira/secure/attachment/12865188/pig-runtime-exec-1.patch,21/Apr/17 18:52;jtolar;pig-runtime-exec.patch;https://issues.apache.org/jira/secure/attachment/12864546/pig-runtime-exec.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-04-25 17:11:26.811,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Apr 28 14:38:20 UTC 2017,,,,,,,0|i3dy27:,9223372036854775807,,,,,,,,,,"25/Apr/17 17:11;knoguchi;Looks good.  Please add a license text to your new test {{TestPreprocessorContext.java}} file.
For the test path, I tend to put everything on test/org/apache/pig/test but probably your test/org/apache/pig/tools/parameters/TestPreprocessorContext is probably the right way.  Either way is fine.",26/Apr/17 17:46;jtolar;Updated the test to add the license; added a new test for a failing command; moved shutdown to shutdownNow() in finally clause. ,"28/Apr/17 14:38;knoguchi;+1.  Committed to trunk. 

Jacob, thank you for the patch!!!",,,,,,,,,,,,,,,,,,,,,,,,,
TestLimitVariable.testNestedLimitVariable1 and TestSecondarySortMR.testNestedLimitedSort  failing,PIG-5223,13063857,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jins,knoguchi,knoguchi,13/Apr/17 17:35,21/Jun/17 09:15,14/Mar/19 03:08,15/Apr/17 05:31,,,,,,0.17.0,,,,,0,,,,,,,"TestLimitVariable.testNestedLimitVariable1 
{quote}
Comparing actual and expected results.  expected:<\[(1,11), (2,3), (3,10), (6,15)]> but was:<\[(1,11), (2,3), (3,10), (4,11), (5,10), (6,15)]>
{quote}

TestSecondarySortMR.testNestedLimitedSort
{quote}
Error during parsing. <line 1, column 158>  mismatched input 'in' expecting INTO
{quote}

Latter is probably a simple syntax error.  Former looks serious. ",,,,,,,,,,,,,PIG-5211,,,,,,,14/Apr/17 05:50;jins;PIG-5223-1.patch;https://issues.apache.org/jira/secure/attachment/12863426/PIG-5223-1.patch,15/Apr/17 02:17;jins;PIG-5223-2.patch;https://issues.apache.org/jira/secure/attachment/12863532/PIG-5223-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-04-13 22:42:05.629,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Apr 15 05:31:51 UTC 2017,,,,,,,0|i3dlm7:,9223372036854775807,,,,,,,,,,"13/Apr/17 21:40;knoguchi;[~jins], [~daijy], let me know if one of you can take this. 
If not, I'll take a look tomorrow.",13/Apr/17 22:42;jins;I will take a look later today. Probably need some help from you guys later on.,"14/Apr/17 05:36;jins;Hey [~daijy]
The second test failed should be easily fixed by changing ""in"" to ""into"". It is just a syntax mistake.
However I want your opinion on the first one:
In this test case, limit is derived from a variable. Due to lazy evaluation, LOLimit.mLimit is -1 until execution. When we merge LOLimit into LOSort, LOSort will have limit = -1, which later gets converted to POSort with limit = -1. This is how we lost limit and fails this test. I think we should disable NestedLimitOptimizer when we find LOLimit.mLimit is -1. Please correct me if I am wrong.
I just made a patch and seems like tests are passing.
Thanks,
Jin","14/Apr/17 05:37;jins;1. Do not invoked NestedLimitOptimizer if LOLimit has mLimit = -1
2. Add more tests
3. Fix syntax error","14/Apr/17 05:53;daijy;Yes, you can use the condition (mLimit == -1 && mlimitPlan != null), and if true, disable the optimization. It is possible to push the limitPlan to LimitedSortedDataBag, but it is not a one line change and we can do it in followup.","14/Apr/17 14:54;jins;Can you explain what is mlimitplan or LogicalExpressionPlan in general, and why do we want to disable the optimizer when mLimit is -1 and mlimitPlan is not null ? Thanks!","14/Apr/17 16:56;daijy;mlimitplan is an expression to calculate limit variable. That is, the number of limited rows is determined at runtime by evaluation expressionPlan (the physical plan equivalence of mlimitplan). If (mLimit == -1 && mlimitPlan != null), that means limiting by variable not constant. It is basically the same as your (mLimit = -1) condition, but add another condition for assurance.",14/Apr/17 17:06;jins;then should it be mLimit is -1 || mlimitPlan is not null ? Please correct me if I am wrong.,14/Apr/17 17:10;daijy;Both conditions should be met in limit by variable case. It should not happen if mLimit==-1 and mlimitPlan==null.,"14/Apr/17 18:25;knoguchi;Thanks Jin Sun, Daniel!  With the proposed patch, confirmed that it fixed the two failing tests and no news tests failed. ",14/Apr/17 21:16;jins;Thanks Koji. I will make a small fix later today before we merge to trunk.,"15/Apr/17 02:17;jins;Address Daniel's advice, adding mLimitPlan != null
Feel free to leave some comments or merge ","15/Apr/17 05:31;daijy;+1. Both tests pass. Thanks for additional test.

Patch committed to trunk. Thanks Jin!",,,,,,,,,,,,,,,
Merge changes from review board to spark branch,PIG-5215,13062344,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,kellyzly,kellyzly,07/Apr/17 07:49,21/Jun/17 09:18,14/Mar/19 03:08,05/Jun/17 01:20,,,,,,spark-branch,,spark,,,0,,,,,,,"in [review board|https://reviews.apache.org/r/57317/], there are comments from community. After the review board is close, merge these changes to spark branch",,,,,,,,,,,,,,,,,,,,12/Apr/17 02:22;kellyzly;PIG-5215.1.patch;https://issues.apache.org/jira/secure/attachment/12862959/PIG-5215.1.patch,05/May/17 04:40;kellyzly;PIG-5215.3.patch;https://issues.apache.org/jira/secure/attachment/12866543/PIG-5215.3.patch,25/May/17 21:53;szita;PIG-5215.4.TestCombinerFix.patch;https://issues.apache.org/jira/secure/attachment/12869940/PIG-5215.4.TestCombinerFix.patch,25/May/17 15:21;szita;PIG-5215.4.fixes.patch;https://issues.apache.org/jira/secure/attachment/12869887/PIG-5215.4.fixes.patch,25/May/17 08:02;kellyzly;PIG-5215.4.patch;https://issues.apache.org/jira/secure/attachment/12869818/PIG-5215.4.patch,26/May/17 01:58;kellyzly;PIG-5215.5.patch;https://issues.apache.org/jira/secure/attachment/12869975/PIG-5215.5.patch,10/Apr/17 05:44;kellyzly;PIG-5215.patch;https://issues.apache.org/jira/secure/attachment/12862643/PIG-5215.patch,,,,,7.0,,,,,,,,,,,,,,,,,,,2017-04-11 09:29:59.915,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 05 01:20:46 UTC 2017,,,,,,,0|i3dcan:,9223372036854775807,,,,,,,,,,"07/Apr/17 07:51;kellyzly;[~szita] and [~nkollar]:

If have time, please help view this.

1 modification about build.xml in PIG-5215.patch is 
{code:java}
index da93cc0..f1bb010 100644
--- a/build.xml
+++ b/build.xml
@@ -262,6 +262,7 @@
     <property name=""build.ivy.lib.dir"" location=""${build.ivy.dir}/lib"" />
     <property name=""build.ivy.spark.lib.dir"" location=""${build.ivy.dir}/lib/spark"" />
     <property name=""ivy.lib.dir"" location=""${build.ivy.lib.dir}/${ant.project.name}""/>
+    <property name=""ivy.lib.dir.spark"" location=""${ivy.lib.dir}/spark"" />
     <property name=""build.ivy.report.dir"" location=""${build.ivy.dir}/report"" />
     <property name=""build.ivy.maven.dir"" location=""${build.ivy.dir}/maven"" />
     <property name=""pom.xml"" location=""${build.ivy.maven.dir}/pom.xml""/>

{code}
in spark branch, the code is
{code:java}
    <property name=""build.ivy.spark.lib.dir"" location=""${build.ivy.dir}/lib/spark"" />
{code}
I found that it is interesting that if we set {{build.ivy.spark.lib.dir}} as build/ivy/lib/Pig/spark, org.apache.pig.test.TestRegisteredJarVisibility#testRegisterJarOverridePigJarPackages fail at [TestRegisteredJarVisibility#L155|https://github.com/apache/pig/blob/308673a782c86cd4bffa330b51e9b8f26a624536/test/org/apache/pig/test/TestRegisteredJarVisibility.java#L155]. While we set {{build.ivy.spark.lib.dir}} as build/ivy/lib/spark, it will not fail.","07/Apr/17 07:52;kellyzly;[~nkollar] and [~szita]: copy the comment from https://issues-test.apache.org/jira/browse/PIG-5197, will upload PIG-5215.patch soon.",10/Apr/17 05:44;kellyzly;[~szita]:repatch and help review.,"10/Apr/17 08:31;kellyzly;[~szita]:  all unit tests pass in my local jenkins with PIG-5215.patch.
","11/Apr/17 09:29;szita;[~kellyzly]: this looks good to me, one question: shouldn't we rename PigSparkContext.java to *SparkPigContext* ? Since it's a kind of PigContext and we don't want it getting confused with SparkContext which is Spark related terminology.","12/Apr/17 02:22;kellyzly;[~szita]: according to comment, update PIG-5215.1.patch, now commit PIG-5215.1.patch to spark branch, but not resolve the jira because there are 2 remaining pages review from [review board|https://reviews.apache.org/r/57317]. ","21/Apr/17 05:53;kellyzly;[~szita]: PIG-5215.2.patch relates small fix, help review","05/May/17 04:40;kellyzly;[~szita]: help review PIG-5215.3.patch
changes:
1.remove unecessary runtime exception
2. code refactor

all unit tests pass with the patch.
",05/May/17 07:48;szita;[~kellyzly] +1 for [^PIG-5215.3.patch],05/May/17 08:18;kellyzly;[~szita]: thanks for review. Commit to branch.,"25/May/17 07:06;kellyzly;[~szita]: 
update PIG-5215.4.patch, compilation pass while i am testing it in my local jenkins.  
fix all comments on [review board|https://reviews.apache.org/r/57317] except 1:
{noformat}

Testing distinct + orderby + limit serves the same purpose as orderby + limit test. Can you remove orderby from this test? If distinct + limit differs everytime even with spark and a different verify_pig_script runs just ignore the test for now adding a TODO to test num 4.
{noformat}
 I think we can work in parallel( I continue to test the patch while you start to review) as there is not much time before final release. please help review the patch and fix remaining issue as you and [~nkollar] have more knowledge on Limit_4 e2e test.


",25/May/17 08:02;kellyzly;[~sztia]: update latest PIG-5215.4.patch.  ,"25/May/17 15:23;szita;[~kellyzly] I've attached [^PIG-5215.4.fixes.patch] which you can apply after your original [^PIG-5215.4.patch]. It contains some whitespace fixes and a missing ""else"" keyword in TestPigRunner.

Otherwise the patch looks good, unit tests are currently being run, I'll come back with results here once I have them","25/May/17 21:55;szita;[~kellyzly] The result of the unit test (run on spark exec type only) showed only 2 failing tests:
{code}
 org.apache.pig.test.TestCombiner.testOnCluster
 org.apache.pig.test.TestCombiner.testLocal
{code}

This is because of missing ' characters to wrap the expected chararray types into (solves a parsing problem), and a missing type cast (solves the assertEquals problem.)
Please find the fix for these in [^PIG-5215.4.TestCombinerFix.patch]. (To be used after applying my former patch: [^PIG-5215.4.fixes.patch])",26/May/17 01:58;kellyzly;[~szita]: thanks for fix.  Include PIG-5215.4.fixes.patch and PIG-5215.4.TestCombinerFix.patch to PIG-5215.5.patch.  ready to commit to spark branch.,26/May/17 06:49;szita;[~kellyzly] +1 for [^PIG-5215.5.patch] - I can see it's already committed. Is there any other change left to be made before merge can happen?,"26/May/17 06:57;kellyzly;[~szita]: i check the svn history and found that the last commit was success. If i am wrong, please tell me.
{noformat}
[root@bdpe42 pig.on.spark]# svn log|head -n 10
------------------------------------------------------------------------
r1796232 | zly | 2017-05-25 22:01:13 -0400 (Thu, 25 May 2017) | 1 line

PIG-5215:Merge changes from review board to spark branch(Liyun)

{noformat}","26/May/17 07:04;kellyzly;[~szita]: the only possible change is PIG-5167. If [~nkollar] can fix it soon, we may include it in the first release otherwise I suggest we don't do any other change about current code and commit the change of PIG-5167 later.",02/Jun/17 09:20;nkollar;[~kellyzly] should we close this since the merge is done?,"05/Jun/17 01:20;kellyzly;[~nkollar]: thanks for remind, close it",,,,,,,,
Cross product on flatten(map) fails with ClassCastException,PIG-5209,13060854,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,01/Apr/17 04:25,21/Jun/17 09:15,14/Mar/19 03:08,03/Apr/17 16:43,,,,,,0.17.0,,impl,,,0,,,,,,,"This issue started after we added a new flatten on map feature at PIG-5085.

For script like 
{code}
B = foreach A GENERATE FLATTEN(map1), FLATTEN(map2);
{code}

it fails with 
{noformat}
Caused by: java.lang.ClassCastException: java.util.HashMap cannot be cast to org.apache.pig.data.DataBag
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:514)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:233)
    ... 12 more
{noformat}",,,,,,,,,,,,,,,,,,,,01/Apr/17 04:29;knoguchi;pig-5209-v01.patch;https://issues.apache.org/jira/secure/attachment/12861575/pig-5209-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-04-01 04:32:07.27,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Apr 03 16:43:45 UTC 2017,,,,,,,0|i3d33r:,9223372036854775807,,,,,,,,,,"01/Apr/17 04:29;knoguchi;Full stacktrace.
{noformat}
2017-04-01 00:01:38,439 [Thread-40] WARN  org.apache.hadoop.mapred.LocalJobRunner  - job_local1792891293_0002
java.lang.Exception: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Error while executing ForEach at [B[-1,-1]]
    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Error while executing ForEach at [B[-1,-1]]
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:331)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:280)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:275)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:65)
    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
    at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: java.util.HashMap cannot be cast to org.apache.pig.data.DataBag
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:514)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:233)
    ... 12 more
{noformat}

Found the issue while trying to test flatten(null) at PIG-5201.   Attaching a small patch with a testcase. ",01/Apr/17 04:32;rohini;+1,"03/Apr/17 16:43;knoguchi;Thanks for the review Rohini!  
Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,
Create a TestSparkCompiler test class for spark branch,PIG-5202,13059934,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,kellyzly,kellyzly,29/Mar/17 06:15,26/Jun/17 09:03,14/Mar/19 03:08,19/Apr/17 01:52,,,,,,spark-branch,,,,,0,,,,,,,"Create a TestSparkCompiler test class similar to TestMRCompiler/TestTezCompiler which has golden plans.

Detail see the [review board comments|https://reviews.apache.org/r/57317/diff/3/?file=1666919#file1666919line40].
[~szita]: As you are familar with DotSparkPrinter.java, please help fix on it.
",,,,,,,,,,,,,,,,,,,,11/Apr/17 13:06;szita;PIG-5202.0.patch;https://issues.apache.org/jira/secure/attachment/12862849/PIG-5202.0.patch,18/Apr/17 15:21;szita;PIG-5202.disableDOTformat.patch;https://issues.apache.org/jira/secure/attachment/12863833/PIG-5202.disableDOTformat.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-04-11 13:10:19.787,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 19 07:34:28 UTC 2017,,,,,,,0|i3cxfb:,9223372036854775807,,,,,,,,,,11/Apr/17 13:10;szita;[~kellyzly]: I've attached the new test class with one simple test case in it as per Rohini's comment on RB. Please see [^PIG-5202.0.patch],"12/Apr/17 01:44;kellyzly;[~szita]: LGTM, commit to the spark branch. thanks!",12/Apr/17 11:45;szita;Thanks for the review and commit,"17/Apr/17 22:18;kellyzly;in [#392|https://builds.apache.org/job/Pig-spark/lastUnsuccessfulBuild/testReport/org.apache.pig.spark/TestSparkCompiler/testStoreLoad/], it shows that TestSparkCompiler fails.  [~szita]:help investigate it.","18/Apr/17 15:27;szita;[~kellyzly] it looks like the DOT file generation has some randomness involved - it will be difficult to compare expected and actual. I thought DotGraphReader will be of help but it looks like it cannot parse the produced files (although they are correct). For now we should disable this part of the test, we can come back later at it. Please commit patch [^PIG-5202.disableDOTformat.patch]","19/Apr/17 01:51;kellyzly;[~szita]: got it, commit PIG-5202.disableDOTformat.patch. thanks",19/Apr/17 07:34;szita;Thanks [~kellyzly],,,,,,,,,,,,,,,,,,,,,
streaming job stuck with script failure when combined with split,PIG-5198,13059403,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,27/Mar/17 13:57,21/Jun/17 09:15,14/Mar/19 03:08,28/Mar/17 15:04,,,,,,0.17.0,,impl,,,0,,,,,,,"
{code:title=test.pig} 
DEFINE myawk `./test.awk` ship('./test.awk');
DEFINE mypy `python my.py` ship ('./my.py');
A = load 'input.txt';
B =  stream A through myawk ;
BB =  stream A through mypy ;
store B into '$output/abc';
store BB into '$output/bcd';
{code} 
This script would hang when my.py fails with syntax error.
(input.txt has to large)",,,,,,,,,,,,,,,,,,,,27/Mar/17 14:07;knoguchi;pig-5198-v01.patch;https://issues.apache.org/jira/secure/attachment/12860662/pig-5198-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-03-27 16:06:26.576,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Mar 28 15:04:01 UTC 2017,,,,,,,0|i3cu5r:,9223372036854775807,,,,,,,,,,"27/Mar/17 14:07;knoguchi;Hanging itself is same from PIG-4976, but unlike that jira, here POStream is properly returning {{POStatus.STATUS_ERR}}.  It's just that POSplit is converting that to RESULT_EMPTY.

{code:title=POSplit.java}
244         return (res.returnStatus == POStatus.STATUS_OK) ? res : RESULT_EMPTY;
{code}

Attaching a patch that would return STATUS_ERR in such cases.
I couldn't reproduce this on a small local unit test.  
Reason was when input is too small 

{code:title=POSplit.java}
198     public Result getNextTuple() throws ExecException {
199
200         if (this.parentPlan.endOfAllInput) {
201
202             return getStreamCloseResult();
203
204         }
205
{code}
This {{endOfAllInput}} becomes true and code flow chooses a different path and does not hit this bug.  For now, added e2e test that produces the hang.",27/Mar/17 16:06;rohini;+1,"28/Mar/17 15:04;knoguchi;Thanks for the review Rohini! 

Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,
Review pig-index.xml,PIG-5188,13055746,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,daijy,daijy,13/Mar/17 22:06,21/Jun/17 09:15,14/Mar/19 03:08,27/May/17 20:30,,,,,,0.17.0,,documentation,,,0,,,,,,,I am afraid src/docs/src/documentation/content/xdocs/pig-index.xml is out of date. At least I didn't pay attention to it when I do my document update. Will need a review before next major release.,,,,,,,,,,,,,,,,,,,,26/May/17 12:29;szita;PIG-5188.0.patch;https://issues.apache.org/jira/secure/attachment/12870063/PIG-5188.0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-05-26 12:30:46.66,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 27 20:30:46 UTC 2017,,,,,,,0|i3c7l3:,9223372036854775807,,,,,,,,,,"26/May/17 12:30;szita;Hi [~daijy] I took a look on what commits happened since 0.16 release into the docs folder and found 5 items missing from the pig-index page.
You can find them in [^PIG-5188.0.patch]",26/May/17 17:07;rohini;+1,"27/May/17 20:30;szita;[^PIG-5188.0.patch] committed to trunk, thanks for review Rohini",,,,,,,,,,,,,,,,,,,,,,,,,
"Job name show ""DefaultJobName"" when running a Python script",PIG-5185,13050299,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,12/Mar/17 05:11,21/Jun/17 09:15,14/Mar/19 03:06,25/May/17 04:21,,,,,,0.17.0,,impl,,,0,,,,,,,"Run a python script with Pig, Hadoop WebUI show ""DefaultJobName"" instead of script name. We shall use script name, the same semantic for regular Pig script.",,,,,,,,,,,,,,,,,,,,12/Mar/17 05:12;daijy;PIG-5185-1.patch;https://issues.apache.org/jira/secure/attachment/12857508/PIG-5185-1.patch,21/Mar/17 08:00;daijy;PIG-5185-2.patch;https://issues.apache.org/jira/secure/attachment/12859714/PIG-5185-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-03-13 23:14:33.313,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu May 25 04:21:41 UTC 2017,,,,,,,0|i3badr:,9223372036854775807,,,,,,,,,,13/Mar/17 23:14;nkollar;LGTM,14/Mar/17 16:28;rohini;Can we add a single extra assert statement to two existing tests instead of writing a new test for it? One of the tests can be modified to have set for jobName.,21/Mar/17 08:00;daijy;Address Rohini's review comments.,24/May/17 22:37;rohini;+1,25/May/17 04:21;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,
We shall mention NATIVE instead of MAPREDUCE operator in document,PIG-5183,13050294,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,12/Mar/17 03:40,21/Jun/17 09:15,14/Mar/19 03:08,13/Mar/17 22:31,,,,,,0.17.0,,,,,0,,,,,,,"In http://pig.apache.org/docs/r0.16.0/basic.html#mapreduce, we still mention MAPREDUCE operator. We shall encourage user use NATIVE instead as we can run both MAPREDUCE and Tez program.",,,,,,,,,,,,,,,,,,,,12/Mar/17 03:40;daijy;PIG-5183-1.patch;https://issues.apache.org/jira/secure/attachment/12857505/PIG-5183-1.patch,13/Mar/17 21:35;daijy;PIG-5183-2.patch;https://issues.apache.org/jira/secure/attachment/12858550/PIG-5183-2.patch,13/Mar/17 22:04;daijy;PIG-5183-3.patch;https://issues.apache.org/jira/secure/attachment/12858560/PIG-5183-3.patch,13/Mar/17 22:10;daijy;PIG-5183-4.patch;https://issues.apache.org/jira/secure/attachment/12858561/PIG-5183-4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2017-03-13 18:51:17.419,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Mar 15 18:49:36 UTC 2017,,,,,,,0|i3bacn:,9223372036854775807,,,,,,,,,,"13/Mar/17 18:51;knoguchi;After your patch (after PIG-5182), it still has MapReduce showing up as 

{code}
  7707 <p>The input and output locations for the MapReduce/Tez program are conveyed to Pig using the STORE/LOAD clauses.
  7708 Pig, however, does not pass this information (nor require that this information be passed) to the MapReduce program.
  7709 If you want to pass the input and output locations to the MapReduce program you can use the params clause or you can hardcode the locations in the MapReduce program.</p>
{code}
I'm assuming this input/output path not being passed also applies to Tez?
","13/Mar/17 21:35;daijy;Thanks, miss this. Attach new patch.","13/Mar/17 21:50;knoguchi;bq. Thanks, miss this. Attach new patch.

Thanks Daniel.  I see that you now named the section 
{code}<section id=""native"">{code}

Looking at place where it was referencing the original ""mapreduce"" section and I see {{pig-index.xml}}.  I'm afraid of asking here but are we manually updating index till today?

","13/Mar/17 22:04;daijy;Just realize there is a pig-index.xml and I believe it is manually maintained (didn't see an automatic process about this). I am not sure how well it is maintained. Will create a Jira and review it before 0.17 release. For this ticket, yes, we shall change pig-index.xml as well.","13/Mar/17 22:08;knoguchi;Daniel, you also need to manually place the new ""NATIVE"" entry where it belongs in alphabetical order...  This is not maintainable for sure...","13/Mar/17 22:10;daijy;Sure, sorry about one more iteration :(",13/Mar/17 22:15;knoguchi;+1,13/Mar/17 22:31;daijy;Patch committed to trunk. Thanks Koji for review!,"13/Mar/17 22:48;rohini;[~daijy],
   Tez also just runs mapreduce (hadoop jar). Currently code does not run native tez jobs. So the documentation is not correct.","13/Mar/17 23:00;daijy;Actually Pig does't have any restriction on the native job. I believe native tez job should be fine, though pure tez job is very few (maybe only those inside tez examples). The goal for the document change is to encourage user use native instead of mapreduce keyword. I don't mind any rewording if you feel proper.",15/Mar/17 18:49;rohini;[~daijy] pointed out to me that hadoop jar just invokes any jar with a Main-Class defined.  So the change is good.,,,,,,,,,,,,,,,,,
ant docs target is broken by PIG-5110,PIG-5182,13050293,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,12/Mar/17 03:33,13/Apr/18 22:02,14/Mar/19 03:08,13/Mar/17 21:23,,,,,,0.17.0,,documentation,,,0,,,,,,,"Doing ant docs, show the error message below:

{code}
     [exec] /Users/daijy/dev/cs502/pig/src/docs/src/documentation/content/xdocs/basic.xml:5427:11: Element type ""i"" must be declared.
     [exec] /Users/daijy/dev/cs502/pig/src/docs/src/documentation/content/xdocs/basic.xml:5429:5: The content of element type ""p"" must match ""(strong|em|code|sub|sup|br|img|icon|acronym|map|xi:include|a)"".
{code}

This is introduced by PIG-5110.",,,,,,,,,,,,,PIG-5110,,,,,,,12/Mar/17 03:34;daijy;PIG-5182-1.patch;https://issues.apache.org/jira/secure/attachment/12857504/PIG-5182-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-03-13 18:35:52.944,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Mar 13 21:23:08 UTC 2017,,,,,,,0|i3bacf:,9223372036854775807,,,,,,,,,,13/Mar/17 18:35;knoguchi;+1.  (patch seems to be created without {{--no-prefix}} though.),"13/Mar/17 19:31;szita;Ah, that <i> was my mistake, thanks for fixing it. +1 for the patch","13/Mar/17 21:23;daijy;Patch committed to trunk. Thanks Koji, Adam for review!",,,,,,,,,,,,,,,,,,,,,,,,,
Script with multiple splits fails with Invalid dag containing 0 vertices ,PIG-5173,13047495,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,01/Mar/17 22:28,21/Jun/17 09:15,14/Mar/19 03:08,03/Mar/17 20:14,,,,,,0.17.0,,,,,0,,,,,,,PIG-5118 fixed most of the issues with DAG splitting. But still ran into one case when there are multiple levels of splits before the store and load statement.,,,,,,,,,,,,,,,,,,,,01/Mar/17 22:38;rohini;PIG-5173-1.patch;https://issues.apache.org/jira/secure/attachment/12855479/PIG-5173-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-03-03 00:58:07.396,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Mar 03 20:14:21 UTC 2017,,,,,,,0|i3atlb:,9223372036854775807,,,,,,,,,,03/Mar/17 00:58;daijy;+1,03/Mar/17 20:14;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
Include all unit tests for spark branch,PIG-5161,13047151,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,kellyzly,kellyzly,01/Mar/17 01:26,21/Jun/17 09:22,14/Mar/19 03:08,08/Mar/17 02:21,,,,,,spark-branch,,,,,0,,,,,,,"we only test unit tests in test/spark-test in spark  mode. Like Tez mode, it tests all unit tests in test/all-tests. and put irrelevant/problematic tests into test/excluded-tests-tez. 
{code}
    <target name=""test-tez"" depends=""setTezEnv,setWindowsPath,setLinuxPath,compile-test,jar,debugger.check,jackson-pig-3039-test-download"" description=""Run tez unit tests"">
        <macro-test-runner test.file=""${test.all.file}"" tests.failed=""test-tez.failed""/>
        <fail if=""test-tez.failed"">Tests failed!</fail>
    </target>
{code}
",,,,,,,,,,,,PIG-5138,,,,,,,,06/Mar/17 14:39;szita;PIG-5161.0.patch;https://issues.apache.org/jira/secure/attachment/12856289/PIG-5161.0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-03-06 14:44:34.745,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 08 22:25:29 UTC 2017,,,,,,,0|i3argv:,9223372036854775807,,,,,,,,,,"06/Mar/17 14:44;szita;Attached [^PIG-5161.0.patch], changed test-spark target's approach from whitelist to backlist. Anything not intended to be tested using Spark as exec engine should be added to _test/excluded-tests-spark_.
I've also included 3 test cases that are currently failing under Spark, we'll address these in separate Jiras.
[~kellyzly] can take a look please?",06/Mar/17 14:49;nkollar;+1,"07/Mar/17 08:55;kellyzly;[~szita]: exception when i use the patch
{code}
patch -p1<PIG-5161.0.patch 
patching file build.xml
Reversed (or previously applied) patch detected!  Assume -R? [n] y
Hunk #1 succeeded at 921 (offset 42 lines).
can't find file to patch at input line 27
Perhaps you used the wrong -p or --strip option?
The text leading up to this was:
--------------------------
|diff --git a/test/excluded-tests-spark b/test/excluded-tests-spark
|index 8492b2aae7df1bdc05a6798456ef74782de78627..8976ce8ea0ce213279a9499761a2bda3ffe663d2 100644
|--- a/test/excluded-tests-spark
|+++ b/test/excluded-tests-spark
--------------------------
File to patch: n
n: No such file or directory
Skip this patch? [y] y
Skipping patch.

{code}
","07/Mar/17 11:13;szita;[~kellyzly] I can see no errors on my side. You can try with git apply too, or it may be the case that your workspace directory isn't clean. You can check if the three files have any unstaged changes:
{code}
diff build.xml <(curl -s https://raw.githubusercontent.com/apache/pig/spark/build.xml)
diff test/excluded-tests-spark <(curl -s https://raw.githubusercontent.com/apache/pig/spark/test/excluded-tests-spark)
diff test/spark-tests <(curl -s https://raw.githubusercontent.com/apache/pig/spark/test/spark-tests)
{code}
",08/Mar/17 02:21;kellyzly;[~szita]: commit to spark branch.,"08/Mar/17 07:23;kellyzly;[~szita]: it shows exception in org.apache.pig.test.TestPredeployedJar.testPredeployedJar on [jenkins|https://builds.apache.org/job/Pig-spark/373/testReport/junit/org.apache.pig.test/TestPredeployedJar/testPredeployedJar/]
{code}
Error Message

Unable to open iterator for alias a. Backend error : org.apache.pig.backend.executionengine.ExecException: ERROR 2997: Unable to recreate exception from backed error: Exception from container-launch.
Container id: container_1488948129657_0001_01_000003
Exit code: 0
Exception message: Cannot run program ""bash"" (in directory ""/home/jenkins/jenkins-slave/workspace/Pig-spark/target/PigMiniCluster/PigMiniCluster-localDir-nm-0_2/usercache/jenkins/appcache/application_1488948129657_0001/container_1488948129657_0001_01_000003""): error=7, Argument list too long
Stack trace: java.io.IOException: Cannot run program ""bash"" (in directory ""/home/jenkins/jenkins-slave/workspace/Pig-spark/target/PigMiniCluster/PigMiniCluster-localDir-nm-0_2/usercache/jenkins/appcache/application_1488948129657_0001/container_1488948129657_0001_01_000003""): error=7, Argument list too long
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:523)
	at org.apache.hadoop.util.Shell.run(Shell.java:479)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: error=7, Argument list too long
	at java.lang.UNIXProcess.forkAndExec(Native Method)
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:187)
	at java.lang.ProcessImpl.start(ProcessImpl.java:130)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)
	... 10 more
{code}

Maybe this is caused by jenkins env. on my local env, it passes. [~daijy]: in PIG-5161, pig on spark  includes TestPredeployedJar. now it fails on[jenkins|https://builds.apache.org/job/Pig-spark/373/testReport/junit/org.apache.pig.test/TestPredeployedJar/testPredeployedJar/], can you help to view this, whether we need to modify the env variable of jenkins server?","08/Mar/17 07:29;daijy;This should be fixed by PIG-5156, can you port the patch to spark branch and see if fixed?","08/Mar/17 22:25;kellyzly;[~daijy]: thanks, in latest build, all unit tests pass.",,,,,,,,,,,,,,,,,,,,
Fix Pig not saving grunt history,PIG-5159,13047010,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,szita,szita,28/Feb/17 16:11,21/Jun/17 09:15,14/Mar/19 03:08,01/Mar/17 22:21,0.17.0,,,,,0.17.0,,,,,0,,,,,,,"After quitting Pig (grunt) and restarting it I cannot see my last Pig latin statements.
Cause is that pig history file (~/.pig_history) is only being read on Pig startup, but is not being written. This can be traced back to the recent jline upgrade PIG-3851.",,,,,,,,,,,,,,,,,,,,28/Feb/17 16:13;szita;PIG-5159.0.patch;https://issues.apache.org/jira/secure/attachment/12855169/PIG-5159.0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-03-01 22:21:11.352,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Mar 03 15:40:21 UTC 2017,,,,,,,0|i3aqlr:,9223372036854775807,,,,,,,,,,"28/Feb/17 16:17;szita;Looks like the old version of jline took care of history persistence itself (after each line being read from console): https://github.com/jline/jline/blob/master/src/main/java/jline/History.java#L102
The new version doesn't, we have to call flush() explicitly.

[~daijy], can you please take a look at [^PIG-5159.0.patch]",01/Mar/17 22:21;rohini;+1. Thanks Adam for catching this issue and fixing it.,"03/Mar/17 15:40;szita;Thanks for committing, Rohini",,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate jars in CLASSPATH when running test,PIG-5156,13045809,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,23/Feb/17 22:19,21/Jun/17 09:15,14/Mar/19 03:08,08/Mar/17 07:28,,,,,,0.17.0,,,,,0,,,,,,,"When running Pig unit test, there are too many entires added to the classpath, and in some environment, results a bash ""Argument list too long"" exception.
In fact, lib/\*.jar is added twice. One is explicitly in junit task, the other is injected the chain test.classpath->classpath->ivy.lib.dir. We need to remove lib/*.jar.
",,,,,,,,,,,,,PIG-5105,,,,,,,23/Feb/17 22:20;daijy;PIG-5156-1.patch;https://issues.apache.org/jira/secure/attachment/12854305/PIG-5156-1.patch,02/Mar/17 23:59;daijy;PIG-5156-2.patch;https://issues.apache.org/jira/secure/attachment/12855732/PIG-5156-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-02-24 12:27:41.831,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Mar 08 07:28:34 UTC 2017,,,,,,,0|i3ajlb:,9223372036854775807,,,,,,,,,,"24/Feb/17 12:27;nkollar;+1, verified that the patch fixes the failing test",25/Feb/17 18:16;rohini;+1.  This is a better fix than PIG-5105 and will handle MR and Spark as well. ,"02/Mar/17 23:59;daijy;Actually we can remove even more. Jars in the "".ivy2"" can also be removed as it is just a repetition of build/ivy/lib/Pig. ",03/Mar/17 00:04;rohini;+1. Only one copy from build/ivy/lib/Pig will be in classpath now.,06/Mar/17 14:32;szita;+1 for [^PIG-5156-2.patch],"08/Mar/17 07:28;daijy;Patch committed to trunk. Thanks Nandor, Rohini, Adam for review and verify!",,,,,,,,,,,,,,,,,,,,,,
Fix TestPigTest unit test in Spark mode,PIG-5136,13045131,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,22/Feb/17 08:24,21/Jun/17 09:18,14/Mar/19 03:08,07/Apr/17 01:47,,,,,,spark-branch,,spark,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Apr/17 05:04;kellyzly;PIG-5136.patch;https://issues.apache.org/jira/secure/attachment/12862226/PIG-5136.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-04-06 10:56:15.336,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 07 01:47:26 UTC 2017,,,,,,,0|i3affr:,9223372036854775807,,,,,,,,,,06/Apr/17 05:04;kellyzly;[~szita] or [~nkollar]: please help review,06/Apr/17 10:56;nkollar;+1,06/Apr/17 21:35;szita;+1 on [^PIG-5136.patch],07/Apr/17 01:47;kellyzly;[~nkollar] and [~szita]: thanks for review. commit to the spark branch.,,,,,,,,,,,,,,,,,,,,,,,,
HDFS bytes read stats are always 0 in Spark mode,PIG-5135,13045130,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,kellyzly,kellyzly,22/Feb/17 08:23,21/Jun/17 09:18,14/Mar/19 03:08,17/May/17 21:52,,,,,,spark-branch,,spark,,,0,,,,,,,I discovered this while running TestOrcStoragePushdown unit test in Spark mode where the test depends on the value of this stat.,,,,,,,,,,,,,,,,,,,,11/Apr/17 08:54;szita;PIG-5135.0.patch;https://issues.apache.org/jira/secure/attachment/12862801/PIG-5135.0.patch,12/Apr/17 15:51;szita;PIG-5135.1.patch;https://issues.apache.org/jira/secure/attachment/12863073/PIG-5135.1.patch,15/May/17 11:08;szita;PIG-5135.2.patch;https://issues.apache.org/jira/secure/attachment/12868052/PIG-5135.2.patch,25/May/17 13:34;szita;PIG-5135.smallfixes.patch;https://issues.apache.org/jira/secure/attachment/12869871/PIG-5135.smallfixes.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2017-04-10 07:41:56.627,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu May 25 15:20:26 UTC 2017,,,,,,,0|i3affj:,9223372036854775807,,,,,,,,,,08/Apr/17 22:39;kellyzly;duplicate with PIG-5200,10/Apr/17 07:41;szita;Reopening as these unit tests fail for a different reason than PIG-5200...,"11/Apr/17 09:06;szita;The reason these tests failed is that they depend on *hdfsBytesRead stat which is always 0* when using Spark as execution engine. (TestOrcStoragePushdown compares bytes read with and without the optimization and expect a certain difference..)

Spark only counts the bytes read if the type of the split given is of FileSplit. [Here|https://github.com/apache/spark/blob/v1.6.1/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala#L142] you can see that otherwise the {{bytesReadCallback}} is None, and because of that the counter is never incremented. This happens in our case because PigSplit is not a FileSplit.
In my patch [^PIG-5135.0.patch] I have created a wrapper {{SparkPigSplit}} that wraps a PigSplit instance and delegates every method to that. If the original PigSplit contained FileSplits then I create FileSparkPigSplit, otherwise GenericSparkPigSplit. (The former extends FileSplit so Spark will be able to count the bytes being read.)

[~kellyzly] please take a look.","12/Apr/17 02:37;kellyzly;[~szita]: quickly review the patch. revert changes of PigInputFormat.java, if want to refactor the code, please file seperate jira for it.","12/Apr/17 07:56;szita;[~kellyzly] the changes in PigInputFormat are required for this fix to work so we cannot separate those changes out from this patch into another jira.
They are needed to help us create a SparkPigRecordReader instance and this is the cleanest way I've found to do it.
I'm also renaming this ticket to be more descriptive",12/Apr/17 15:51;szita;also attached new patch with one missing method override that came up during e2e test execution,"12/Apr/17 21:04;kellyzly;[~szita]:  I see. please create a jira board and add [~rohini] as reviewer to help review the modification of PigInputFormat.java.

And remove some code in org.apache.pig.test.TestPigRunner#simpleMultiQueryTest3 as now the hdfs bytes read stats are not always 0 in spark mode
{code}
  // For mapreduce, since hdfs bytes read includes replicated tables bytes read is wrong
            // Since Tez does has only one load per job its values are correct
            // By pass the check for spark due to PIG-4788
            if (!Util.isMapredExecType(cluster.getExecType()) && !Util.isSparkExecType(cluster.getExecType())) {
                assertEquals(30, inputStats.get(0).getBytes());
                assertEquals(18, inputStats.get(1).getBytes());
            }
{code}","13/Apr/17 14:44;szita;[~kellyzly] I've checked this, it seems that {{assertEquals(30, inputStats.get(0).getBytes());}} is fine, but {{assertEquals(18, inputStats.get(1).getBytes());}} is not true, Spark returns -1 here. The plan generated for spark consists of 4 jobs, last one being the responsible for replicated join. This latter does 3 loads, and thus SparkPigStats handle this as -1. (Even after adding together all the bytes from all load ops in this job I got different result than 18.) I guess compression is also at work here on the tmp file part generation that further alters the number of bytes being read.

I would say we should leave the exclusion for Spark as is, but update the comment section since we don't get the expected numbers for a different reason. What do you think?",21/Apr/17 03:39;kellyzly;[~szita]: include PIG-5135.1.patch to the total change of [review board|https://reviews.apache.org/r/57317/diff/5-6/].  ,"14/May/17 20:22;rohini;This patch is good to go. Just two minor comments.
   1) isFiledSplits -> isFileSplits
   2) Some cleanup unrelated to this patch but would be good to do as it touches that code - Get rid of the static activeSplit variable and getActiveSplit method in PigInputFormat. Do not seem to be used anywhere and it is not a good idea to have a static state.  Also remove PigInputFormat.sJob which has been deprecated for long.",15/May/17 11:12;szita;[~rohini] Thanks for the review - I've attached [^PIG-5135.2.patch] with the fixes,17/May/17 21:52;rohini;+1. Committed to spark-branch. Thanks [~szita] for fixing this.,17/May/17 21:57;szita;Thanks for review Rohini and Liyun,17/May/17 22:00;kellyzly;[~szita]: will update the review board according to the latest patch of PIG-5135(latest branch code) today.,"18/May/17 06:32;kellyzly;[~rohini]: there is some problem in last checkin causing the [jenkins failure|https://builds.apache.org/job/Pig-spark/402/consoleFull]
* 19463c9 - (HEAD, origin/spark, spark) PIG-5135: HDFS bytes read stats are always 0 in Spark mode (szita via rohini) (8 hours ago) <Rohini Palaniswamy>

recommit and see whether jenkins pass or not.",18/May/17 08:15;szita;[~kellyzly] Thanks for catching this - it was probably missing {{svn add}} calls during the original commit. I can see now it's fixed by committing the missing files,"25/May/17 05:35;kellyzly;[~szita]:
bq.I've checked this, it seems that assertEquals(30, inputStats.get(0).getBytes()); is fine, but assertEquals(18, inputStats.get(1).getBytes()); is not true, Spark returns -1 here. The plan generated for spark consists of 4 jobs, last one being the responsible for replicated join. This latter does 3 loads, and thus SparkPigStats handle this as -1. (Even after adding together all the bytes from all load ops in this job I got different result than 18.) I guess compression is also at work here on the tmp file part generation that further alters the number of bytes being read.
org.apache.pig.test.TestPigRunner#simpleMultiQueryTest3
{code}
#--------------------------------------------------
# Spark Plan                                  
#--------------------------------------------------

Spark node scope-53
Store(hdfs://localhost:58892/tmp/temp-1660154197/tmp1818797386:org.apache.pig.impl.io.InterStorage) - scope-54
|
|---A: New For Each(false,false,false)[bag] - scope-10
    |   |
    |   Cast[int] - scope-2
    |   |
    |   |---Project[bytearray][0] - scope-1
    |   |
    |   Cast[int] - scope-5
    |   |
    |   |---Project[bytearray][1] - scope-4
    |   |
    |   Cast[int] - scope-8
    |   |
    |   |---Project[bytearray][2] - scope-7
    |
    |---A: Load(hdfs://localhost:58892/user/root/input:org.apache.pig.builtin.PigStorage) - scope-0--------

Spark node scope-55
Store(hdfs://localhost:58892/tmp/temp-1660154197/tmp-546700946:org.apache.pig.impl.io.InterStorage) - scope-56
|
|---C: Filter[bag] - scope-14
    |   |
    |   Less Than or Equal[boolean] - scope-17
    |   |
    |   |---Project[int][1] - scope-15
    |   |
    |   |---Constant(5) - scope-16
    |
    |---Load(hdfs://localhost:58892/tmp/temp-1660154197/tmp1818797386:org.apache.pig.impl.io.InterStorage) - scope-10--------

Spark node scope-57
C: Store(hdfs://localhost:58892/user/root/output:org.apache.pig.builtin.PigStorage) - scope-21
|
|---Load(hdfs://localhost:58892/tmp/temp-1660154197/tmp-546700946:org.apache.pig.impl.io.InterStorage) - scope-14--------

Spark node scope-65
D: Store(hdfs://localhost:58892/user/root/output2:org.apache.pig.builtin.PigStorage) - scope-52
|
|---D: FRJoinSpark[tuple] - scope-44
    |   |
    |   Project[int][0] - scope-41
    |   |
    |   Project[int][0] - scope-42
    |   |
    |   Project[int][0] - scope-43
    |
    |---Load(hdfs://localhost:58892/tmp/temp-1660154197/tmp-546700946:org.apache.pig.impl.io.InterStorage) - scope-58
    |
    |---BroadcastSpark - scope-63
    |   |
    |   |---B: Filter[bag] - scope-26
    |       |   |
    |       |   Equal To[boolean] - scope-29
    |       |   |
    |       |   |---Project[int][0] - scope-27
    |       |   |
    |       |   |---Constant(3) - scope-28
    |       |
    |       |---Load(hdfs://localhost:58892/tmp/temp-1660154197/tmp1818797386:org.apache.pig.impl.io.InterStorage) - scope-60
    |
    |---BroadcastSpark - scope-64
        |
        |---A1: New For Each(false,false,false)[bag] - scope-40
            |   |
            |   Cast[int] - scope-32
            |   |
            |   |---Project[bytearray][0] - scope-31
            |   |
            |   Cast[int] - scope-35
            |   |
            |   |---Project[bytearray][1] - scope-34
            |   |
            |   Cast[int] - scope-38
            |   |
            |   |---Project[bytearray][2] - scope-37
            |
            |---A1: Load(hdfs://localhost:58892/user/root/input2:org.apache.pig.builtin.PigStorage) - scope-30--------
{code}
 assertEquals(30, inputStats.get(0).getBytes()) is correct in spark mode,
 assertEquals(18, inputStats.get(1).getBytes()) is wrong in spark mode as the there are 3 loads in {{Spark node scope-65}}.  [{{stats.get(""BytesRead"")}}|https://github.com/apache/pig/blob/spark/src/org/apache/pig/tools/pigstats/spark/SparkJobStats.java#L93] returns 49( guess this is the sum of 
three loads({{input2}},{{tmp1818797386}},{{tmp-546700946}}). But current [{{bytesRead}}|https://github.com/apache/pig/blob/spark/src/org/apache/pig/tools/pigstats/spark/SparkJobStats.java#L91] is -1 because [{{singleInput}}|https://github.com/apache/pig/blob/spark/src/org/apache/pig/tools/pigstats/spark/SparkJobStats.java#L92] is false.


Let's modify the code like
{code}

      // Since Tez does has only one load per job its values are correct
            // the result of inputStats in spark mode is also correct
          if (!Util.isMapredExecType(cluster.getExecType())) {
            assertEquals(30, inputStats.get(0).getBytes());
          }

          //TODO PIG-5240:Fix TestPigRunner#simpleMultiQueryTest3 in spark mode for wrong inputStats
          if (!Util.isMapredExecType(cluster.getExecType()) && !Util.isSparkExecType(cluster.getExecType())) {
            assertEquals(18, inputStats.get(1).getBytes());
          }
{code}
","25/May/17 13:36;szita;[~kellyzly]: fair point. I've done the suggested modifications in [^PIG-5135.smallfixes.patch]. In this patch there is also a fix for a missing ""else"" keyword that currently causes the test to fail in MR mode. We could do it in a separate jira but it's very minimal and could go together with the other change I think.",25/May/17 15:20;szita;[~kellyzly] I just realized you already did this change in one of your patches in PIG-5215. Let's continue the discussion there,,,,,,,,,
Fix TestAvroStorage unit test in Spark mode,PIG-5134,13045129,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,kellyzly,kellyzly,22/Feb/17 08:23,21/Jun/17 09:18,14/Mar/19 03:08,17/May/17 20:45,,,,,,spark-branch,,spark,,,0,,,,,,,"It seems that test fails, because Avro GenericData#Record doesn't implement Serializable interface:
{code}
2017-02-23 09:14:41,887 ERROR [main] spark.JobGraphBuilder (JobGraphBuilder.java:sparkOperToRDD(183)) - throw exception in sparkOperToRDD: 
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 9.0 (TID 9) had a not serializable result: org.apache.avro.generic.GenericData$Record
Serialization stack:
	- object not serializable (class: org.apache.avro.generic.GenericData$Record, value: {""key"": ""stuff in closet"", ""value1"": {""thing"": ""hat"", ""count"": 7}, ""value2"": {""thing"": ""coat"", ""count"": 2}})
	- field (class: org.apache.pig.impl.util.avro.AvroTupleWrapper, name: avroObject, type: interface org.apache.avro.generic.IndexedRecord)
	- object (class org.apache.pig.impl.util.avro.AvroTupleWrapper, org.apache.pig.impl.util.avro.AvroTupleWrapper@3d3a58c1)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
{code}
The failing tests is a new test introduced with merging trunk to spark branch, that's why we didn't see this error before.",,,,,,,,,,,,,,,,,,,,23/Feb/17 12:35;nkollar;PIG-5134.patch;https://issues.apache.org/jira/secure/attachment/12854201/PIG-5134.patch,24/Mar/17 16:51;nkollar;PIG-5134_2.patch;https://issues.apache.org/jira/secure/attachment/12860390/PIG-5134_2.patch,15/May/17 08:51;nkollar;PIG-5134_3.patch;https://issues.apache.org/jira/secure/attachment/12868032/PIG-5134_3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-02-23 11:12:50.568,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed May 17 20:45:16 UTC 2017,,,,,,,0|i3affb:,9223372036854775807,,,,,,,,,,23/Feb/17 08:22;kellyzly;[~nkollar]: TestAvroStorage is not included in test/spark-tests. before we did not test it.  are you sure this pass before we merge trunk changes?,"23/Feb/17 11:12;nkollar;[~kellyzly] yes, I executed this test on spark branch without your merge commit, and all tests in TestAvroStorage passed. After the merge, only one failed, and it was a new test in trunk since the last rebase. 
It looks like we have two options to fix this, I'll attach one soon:
- use Kryo serialization, instead of Spark 1.6.1 default Java serialization
- upgrade to Spark 2.0
- ask Avro to make this class Serializable (least preferred option)

I'd vote for the first option now, and later on once we make spark branch stable, we should upgrade to Spark 2.0, which uses Kryo serialization by default. It is also told, that Kryo is 10x faster than default serialization, I guess that's why Spark moved to Kryo from Java serialization.","23/Feb/17 12:41;nkollar;Attached PIG-5134.patch with these changes:
- user Kryo serialization for Spark, we had to upgrade to latest Kryo and the dependency name changed
- KryoSerializer class in Pig codebase was not used, deleted it
- Avro String records are serialized to Utf8 class, due to this problem, I had to fix a ClassCastException in AvroTupleWrapper

[~kellyzly], [~szita] could you please take a look at the patch?","23/Feb/17 18:32;nkollar;Looks like this will break TestCurrentTime test case, Kryo doesn't work well with Joda DateTime. I'll search for a workaround.","24/Mar/17 16:51;nkollar;[~kellyzly] an update: I managed to solve this without using Kryo, but don't really like the solution I came up with. Using Kryo would be better choice I think. In my solution, I implemented readObject and writeObject methods, these methods read/write the Avro schema as well as the data from/to the OutputStream/InputStream. This is done only for AvroTupleWrapper, but I'm afraid we'll have to implement the same logic for the other Avro wrapper classes too. I noticed, that a similar issue related to Spark and Avro compatibility was already resolved: AVRO-1502. It seems that this was only fixed for SpecificRecords but not for GenericRecords, which we use in Pig. [~rohini] do you have any recommendation, which option should we follow?","24/Mar/17 19:15;rohini;bq. Using Kryo would be better choice I think. 
  Agree with this. Changing code of a Load or StoreFunc to get it working with Spark is not a good idea. We might fix the builtin ones, but custom ones from user will still break. My only concern with kryo is about incompatibility. This and guava are the most problematic ones and people generally resort to shading. For eg: HIVE-5915, https://issues.cloudera.org/browse/LIVY-109. But spark does not seem to use a shaded kryo version. Could you investigate if that could lead to any issues?

bq. name=""minlog"" rev=""1.3""
  Add minlog.version to libraries.properties instead of hardcoding","27/Mar/17 08:11;nkollar;[~rohini] yes, unfortunately using Kryo would lead to issues: Hive UDF test cases are failing. Like you said, Spark doesn't use shaded Kryo, but Hive does, so Hive UDTF test cases fail:
{code}
Caused by: java.lang.ClassNotFoundException: com.esotericsoftware.shaded.org.objenesis.strategy.InstantiatorStrategy
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
{code}
I'll try to find a solution for this. Also, it seems Joda date is not compatible with Kryo, but I think that's a simpler problem.","27/Mar/17 09:58;nkollar;Actually this issue came up before, Kryo was downgraded on spark branch to 2.21 (but to support Hive UDFs and ORC we need 2.22): PIG-4693","07/Apr/17 03:01;kellyzly;[~nkollar]:  here is my understanding for the jira.
You provide 2 options to solve the issue
1. PIG-5134.patch:use Kryo 4.0.0 serialization. but if use PIG-5134.patch,cases with HiveUDF and ORC will fail because the kryo version need to be 2.22
2. PIG-5134.1.patch: implemented readObject and writeObject methods in AvroTupleWrapper and not to use Kryo. But customized Wrapper class like AvroTupleWrapper will still be broken.

If my understanding is right, I suggest to exclude TestAvroStorage from unittest and not fixed in first release of pig on spark. [~rohini], can you give us some suggestion? ","14/May/17 05:46;rohini;bq. I suggest to exclude TestAvroStorage from unittest and not fixed in first release of pig on spark
   Lets go with Nandor's second patch. Folks using custom tuple are a minority, but many use AvroStorage. So it would be good to have it fixed.

[~nkollar],
    Can you revert the wild card imports? They are generally not recommended in PIG-5134_2.patch. After fixing that, we can have this patch committed.  Please do create a new jira for tracking the current issue and exploring kryo serialization which we can look at a later release.

","15/May/17 09:45;nkollar;[~rohini]
Attached PIG-5134_3.patch. Fixed the wildcard imports and removed the test from the excluded tests. I'm wondering if this we should do the same patch for the other two Avro wrapper classes (bag and map wrapper), what do you think?","15/May/17 15:43;rohini;bq. I'm wondering if this we should do the same patch for the other two Avro wrapper classes (bag and map wrapper), what do you think?
  Are there no avro tests that test bag and map? If they have the same issue, yes then we should. Do you want to address in this jira or a new jira?","16/May/17 07:42;nkollar;Let's address it in a different Jira, I'll have to investigate what the kind of Avro test we have now.",17/May/17 20:45;rohini;+1. Committed to spark-branch. Thanks for fixing this [~nkollar],,,,,,,,,,,,,,
Merge from trunk (5) [Spark Branch],PIG-5132,13044741,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,21/Feb/17 08:02,21/Jun/17 09:18,14/Mar/19 03:08,01/Mar/17 13:49,,,,,,spark-branch,,spark,,,0,,,,,,,"merge changes from trunk to branch.
the latest commit in trunk is
 92df45d - (origin/trunk, origin/HEAD, trunk) PIG-5085: Support FLATTEN of maps (szita via rohini) 

",,,,,,,,,,,,,,,,,,,,21/Feb/17 15:37;szita;PIG-5132.1.patch;https://issues.apache.org/jira/secure/attachment/12853767/PIG-5132.1.patch,21/Feb/17 15:38;szita;PIG-5132.1_fixes.patch;https://issues.apache.org/jira/secure/attachment/12853768/PIG-5132.1_fixes.patch,23/Feb/17 02:41;kellyzly;PIG-5132.2.fix.patch;https://issues.apache.org/jira/secure/attachment/12854122/PIG-5132.2.fix.patch,22/Feb/17 07:39;kellyzly;PIG-5132.2.zip;https://issues.apache.org/jira/secure/attachment/12853904/PIG-5132.2.zip,24/Feb/17 08:57;szita;PIG-5132.aftermerge.0.patch;https://issues.apache.org/jira/secure/attachment/12854410/PIG-5132.aftermerge.0.patch,28/Feb/17 07:59;szita;PIG-5132.aftermerge.1.patch;https://issues.apache.org/jira/secure/attachment/12855082/PIG-5132.aftermerge.1.patch,21/Feb/17 08:46;kellyzly;PIG-5132.patch;https://issues.apache.org/jira/secure/attachment/12853690/PIG-5132.patch,27/Feb/17 21:53;kellyzly;buid.xml.from.rb.PNG;https://issues.apache.org/jira/secure/attachment/12854986/buid.xml.from.rb.PNG,23/Feb/17 09:42;szita;diffOfPatches.png;https://issues.apache.org/jira/secure/attachment/12854169/diffOfPatches.png,24/Feb/17 08:59;szita;hadoop-streaming.jar;https://issues.apache.org/jira/secure/attachment/12854411/hadoop-streaming.jar,23/Feb/17 01:48;kellyzly;jenkins.5132.2.fix.PNG;https://issues.apache.org/jira/secure/attachment/12854113/jenkins.5132.2.fix.PNG,11.0,,,,,,,,,,,,,,,,,,,2017-02-21 16:42:55.554,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 01 12:31:58 UTC 2017,,,,,,,0|i3ad13:,9223372036854775807,,,,,,,,,,"21/Feb/17 08:05;kellyzly;initial patch, need more time to test in my local env",21/Feb/17 08:46;kellyzly;[~nkollar] and [~szita]: please help review.,"21/Feb/17 16:42;szita;[~kellyzly]: [~nkollar] and I have created the merge patch too, see [^PIG-5132.1.patch].
It is less in size because file movements are handled as moves and not as deletion/additions. It also includes binary file changes (e.g. ant-contrib.jar).
Basically diff was generated as
{code}
git diff HEAD~ --full-index --binary
{code}
After resolving the merge conflicts we saw some build problems. These along with some e2e testing fixes can be found in a subsequent patch [^PIG-5132.1_fixes.patch]
We're running unit and e2e tests right now.","22/Feb/17 06:46;kellyzly;[~nkollar] and [~szita]: thanks your patch. but in my env, there are some problems(missing files like pig/test/org/apache/pig/test/TezMiniCluster.javaTezMiniCluster.java). now upload PIG-5132.2.patch.  I verify it in my env successfully according to following steps. 
1. download latest trunk to get ivy/ant-contrib-1.0b3.jar, here i can not include this in the patch
2. switch to spark branch and git latest code.
   unzip PIG-5132.2.zip
   patch -p1<PIG-5132.2.patch
3. ant -Dhadoopversion=2 jar","22/Feb/17 07:47;nkollar;[~kellyzly] I think you don't need '-Dhadoopversion=2'. I see you applied our PIG-5132.1_fixes.patch, that's fine, but I can't find in PIG-5132.2.patch the changes we made on META-INF/services/org.apache.pig.ExecType: re-enable tez exec types. Were these intentionally omitted? With your patch, Tez is disabled now.","22/Feb/17 08:05;szita;Test runs are now finished with a lot of failures:
E2E (spark mode with MR as benchmark):
[exec] Final results ,    PASSED: 515  FAILED: 28   SKIPPED: 95   ABORTED: 104  FAILED DEPENDENCY: 0

Unit tests (-Dtest.exec.type=spark):
{code}
./TEST-org.apache.pig.builtin.TestAvroStorage.txt:Tests run: 41, Failures: 1, Errors: 0, Time elapsed: 23.737 sec
./TEST-org.apache.pig.builtin.TestOrcStoragePushdown.txt:Tests run: 16, Failures: 9, Errors: 0, Time elapsed: 32.895 sec
./TEST-org.apache.pig.test.pigunit.TestPigTest.txt:Tests run: 21, Failures: 0, Errors: 1, Time elapsed: 31.092 sec
./TEST-org.apache.pig.test.TestBuiltin.txt:Tests run: 75, Failures: 1, Errors: 0, Time elapsed: 34.719 sec
./TEST-org.apache.pig.test.TestCounters.txt:Tests run: 13, Failures: 1, Errors: 8, Time elapsed: 19.508 sec
./TEST-org.apache.pig.test.TestCustomPartitioner.txt:Tests run: 4, Failures: 0, Errors: 1, Time elapsed: 25.945 sec
./TEST-org.apache.pig.test.TestEmptyInputDir.txt:Tests run: 9, Failures: 4, Errors: 0, Time elapsed: 9.081 sec
./TEST-org.apache.pig.test.TestEvalPipeline2.txt:Tests run: 55, Failures: 0, Errors: 1, Time elapsed: 30.533 sec
./TEST-org.apache.pig.test.TestFRJoinNullValue.txt:Tests run: 4, Failures: 4, Errors: 0, Time elapsed: 8.468 sec
./TEST-org.apache.pig.test.TestFRJoin.txt:Tests run: 18, Failures: 11, Errors: 0, Time elapsed: 18.057 sec
./TEST-org.apache.pig.test.TestJoinSmoke.txt:Tests run: 3, Failures: 1, Errors: 0, Time elapsed: 9.582 sec
./TEST-org.apache.pig.test.TestLineageFindRelVisitor.txt:Tests run: 6, Failures: 1, Errors: 0, Time elapsed: 6.655 sec
./TEST-org.apache.pig.test.TestMultiQuery.txt:Tests run: 16, Failures: 2, Errors: 1, Time elapsed: 12.556 sec
./TEST-org.apache.pig.test.TestPigRunner.txt:Tests run: 32, Failures: 1, Errors: 0, Time elapsed: 35.882 sec
./TEST-org.apache.pig.test.TestPredeployedJar.txt:Tests run: 2, Failures: 0, Errors: 1, Time elapsed: 32.77 sec
./TEST-org.apache.pig.test.TestPruneColumn.txt:Tests run: 71, Failures: 2, Errors: 2, Time elapsed: 22.66 sec
{code}","22/Feb/17 14:07;szita;[~kellyzly] I see you have committed your patch (PIG-5132.2.patch). I think we'll be better off reverting this commit as the patch contains a lot of errors.
Some examples:
1. XPathTest#testExecTupleWithDontIgnoreNamespace is present 3 times
2. XPathTest#testFunctionInXPath is present 2 times
3. Random whitespaces came into NonFSLoadFunc:21 and :22
..etc
It seems like tests can not even be built (*ant clean jar test*) because shims/test/hadoop2 folder doesn't exist.
This is because some missing SVN add/mv/rm commands have to be run as well.

I think the following steps should be taken after reverting the last commit:
{code}
#my patch was generated using git diff HEAD~ --full-index --binary so it has every piece, no need to download anything else, just use git apply instead of patch
git apply PIG-5132.1.patch
git apply PIG-5132.1_fixes.patch
#let's check if everything is okay:
ant clean jar pigunit-jar
ant clean jar -f contrib/piggybank/java/build.xml
#delete build files before committing to svn
ant clean
ant clean -f contrib/piggybank/java/build.xml
#svn rm files that were deleted
svn rm contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HadoopJobHistoryLoader.java
svn rm contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestHadoopJobHistoryLoader.java
svn rm shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java
svn rm shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java
svn rm shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
svn rm shims/src/hadoop20/org/apache/pig/backend/hadoop20/PigJobControl.java
svn rm shims/src/hadoop23/org/apache/hadoop/mapred/DowngradeHelper.java
svn rm shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java
svn rm shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java
svn rm shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
svn rm shims/src/hadoop23/org/apache/pig/backend/hadoop23/PigJobControl.java
svn rm shims/test/hadoop20/org/apache/pig/test/MiniCluster.java
svn rm shims/test/hadoop20/org/apache/pig/test/SparkMiniCluster.java
svn rm shims/test/hadoop20/org/apache/pig/test/TezMiniCluster.java
svn rm shims/test/hadoop23/org/apache/pig/test/MiniCluster.java
svn rm shims/test/hadoop23/org/apache/pig/test/SparkMiniCluster.java
svn rm shims/test/hadoop23/org/apache/pig/test/TezMiniCluster.java
svn rm src/META-INF/services/org.apache.hadoop.mapreduce.protocol.ClientProtocolProvider
svn rm src/docs/jdiff/pig_0.15.0.xml
svn rm src/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaConstantBooleanObjectInspector.java
svn rm src/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaConstantDoubleObjectInspector.java
svn rm src/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaConstantFloatObjectInspector.java
svn rm src/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaConstantIntObjectInspector.java
svn rm src/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaConstantLongObjectInspector.java
svn rm src/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaConstantStringObjectInspector.java
svn rm test/e2e/pig/lib/hadoop-0.23.0-streaming.jar
svn rm test/excluded-tests-20
svn rm test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-LoadStore-2-JDK7.gld
#recursively add every new file/folder (this is why we have to ant clean some steps above)
svn add .
#finally commit
svn commit
{code}


","23/Feb/17 01:27;kellyzly;[~szita]:  sorry for the mistake i made in PIG-5132.2.patch.  The reason why i not use PIG-5132.1.patch is because i need to delete directories like shims/src/hadoop23 and others after i applying the patch. So i create PIG-5132.2.patch.  The problems in PIG-5132.2.patch is 
{code}
1. XPathTest#testExecTupleWithDontIgnoreNamespace is present 3 times
2. XPathTest#testFunctionInXPath is present 2 times
3. Random whitespaces came into NonFSLoadFunc:21 and :22
4. not create directory shims/test/hadoop2
{code}
in PIG-5132.2.fix.patch, i fixed above 4 points and test them in my local jenkins(see attached jenkins.5132.2.fix.png). Can you and [~nkollar] help verify PIG-5132.2.fix.patch by following way?  if anyone of you +1, i will commit PIG-5132.2.fix.patch.
{code}
1. download latest code
   the latest version is  1a2b4f3 - (origin/spark) PIG-5132:Merge from trunk (5) [Spark Branch](Adam via Liyun) (15 hours ago) 
2. download PIG-5132.2.fix.patch; git apply<PIG-5132.2.fix.patch #here please use git apply to patch. if use patch -p1<xxx.patch, the directory shims/test/hadoop2 will not be created.
3. ant -Dhadoopversion=2 jar
4. ant -Dhadoopversion=2 test-spark
{code}

After applying the patch , i found the unit test about TestPredeployedJar passed. so ready to close the unit test failure.","23/Feb/17 09:55;szita;[~kellyzly]: there are still a lot of differences that your patch doesn't have. I only listed some examples before, not the whole list, see [^diffOfPatches.png].
Here:
-blue means diff in the file
-black denotes missing files from your side
-green means files that should be deleted but are still present on your side
You can see that throughout ivy, documentation, hadoop-streaming and other locations there is still differences to be pulled from trunk into your patch.

So your recent patch (PIG-5132.2.fix.patch) only fixes 4 problems and I don't think it is worth to fix every little issue one-by-one manually.
I still think you should revert the committed patch, and *git apply PIG-5132.1.patch*. After this you have to *git apply PIG-5132.1_fixes.patch* as I wrote in the steps above.
The reason why I placed two diffs is:
PIG-5132.1.patch: merge_from_trunk diff (but this is not necessarily results in a build-able Pig code base)
PIG-5132.1_fixes.patch: takes care of necessary changes on spark branch side to follow trunk properly (e.g. removing SparkMiniCluster.java from hadoop20, moving SparkMiniCluster from hadoop23 under pig/test directory

If you apply these patches after each other (using git apply) hadoop20 and hadoop23 dirs get removed from shims because git doesn't keep empty directory structures.
","23/Feb/17 12:59;nkollar;+1 for PIG-5132.1.patch and PIG-5132.1.patch and PIG-5132.1_fixes.patch. Like Adam told, there are still differences, these can cause problems we should fix one-by-one later, we should avoid fixing these manually.","24/Feb/17 07:56;kellyzly;[~szita]: can not use git apply PIG-5132.1.patch, error is like following
{code}

[root@bdpe42 pig.on.spark]# git apply --whitespace=warn PIG-5132.1.patch 
PIG-5132.1.patch:31: trailing whitespace.
To compile with Hadoop 2.x 
PIG-5132.1.patch:37: trailing whitespace.
    
PIG-5132.1.patch:55: trailing whitespace.
A very valid way of working is by having your favourite IDE that has the project 
PIG-5132.1.patch:65: trailing whitespace.
    
PIG-5132.1.patch:76: trailing whitespace.
tree from where you started. 
error: patch failed: src/org/apache/pig/backend/hadoop/datastorage/HDataStorage.java:18
error: src/org/apache/pig/backend/hadoop/datastorage/HDataStorage.java: patch does not apply
error: patch failed: test/e2e/pig/tests/multiquery.conf:728
error: test/e2e/pig/tests/multiquery.conf: patch does not apply
{code}","24/Feb/17 08:04;kellyzly;[~szita]:  PIG-5132.1.patch is generated by ""git diff”？
the changes of NonFSLoadFunc.java is 
{code}
diff --git a/shims/test/hadoop20/org/apache/pig/test/TezMiniCluster.java b/src/org/apache/pig/NonFSLoadFunc.java
similarity index 54%
rename from shims/test/hadoop20/org/apache/pig/test/TezMiniCluster.java
rename to src/org/apache/pig/NonFSLoadFunc.java
index 505c4727caaf849ff20e301222fa67a20655768d..605b693c5ab8b0dd8243b23398e52f38898b8be3 100644
{code}

I'm afraid that this can not be applied  if i use ""git apply PIG-5132.1.patch"" in pig svn directory.

I redo the patch and commit to svn. If have any problem, please tell me and i will also verify the report from jenkins.
","24/Feb/17 08:50;szita;Thanks [~kellyzly]! I see your new commit, it looks much much better now.
I can see very few things that we still have to take care of in:
- libraries.properties
- POMergeJoin.java
- hadoop-streaming.jar
- build.xml
- ivy.xml

I'll put together a quick patch for these shortly and it should be good then",24/Feb/17 09:00;szita;Attached [^PIG-5132.aftermerge.0.patch] with the small fixes and hadoop-streaming.jar. Please apply the patch and replace hadoop-streaming.jar under test/e2e/pig/lib,"27/Feb/17 08:56;kellyzly;[~szita]: in PIG-5132.aftermerge.0.patch. most accept except  modification in build.xml
{code}
git diff build.xml
diff --git a/build.xml b/build.xml
index 2ecf7b8..d8e88f1 100644
--- a/build.xml
+++ b/build.xml
@@ -394,6 +394,21 @@
             <include name=""joda-time-${joda-time.version}.jar""/>
             <include name=""automaton-${automaton.version}.jar""/>
             <include name=""jansi-${jansi.version}.jar""/>
+            <include name=""scala*.jar""/>
+            <include name=""akka*.jar""/>
+            <include name=""jcl-over-slf4j*.jar""/>
+            <include name=""jul-to-slf4j*.jar""/>
+            <include name=""slf4j*.jar""/>
+            <include name=""commons*.jar""/>
+            <include name=""config*.jar""/>
+            <include name=""netty*.jar""/>
+            <include name=""jetty*.jar""/>
+            <include name=""metrics-core*.jar""/>
+            <include name=""jackson*.jar""/>
+            <include name=""metrics-json-*.jar""/>
+            <include name=""json4s-*.jar""/>
+            <include name=""javax.servlet-*.jar""/>
+            <include name=""reflectasm*.jar""/>
         </patternset>
     </fileset>
 
{code}

actually we need not put these jars to  build/ivy/lib/Pig/, this folder includes jars which is common use. All jar related to spark branch is in  build/ivy/lib/spark.","27/Feb/17 09:09;szita;[~kellyzly] what you are talking about (ivy/lib/Pig) is build related.
The list above is in *runtime.dependencies-withouthadoop.jar* and is runtime related. The list of these jars were there before the merge and I think we should to keep them there.

To be more exact this list decides what jars to include classes from into legacy/pig-0.16.0-SNAPSHOT-withouthadoop-h2.jar. If you remove this, a lot of classes will be missing from it, you can see as the filesize gets cut less than half:
{code}
szita@szitamac:~/shadow/CDH/pig$ ls -al with*/legacy
withBuildXmlFix/legacy:
total 27568
drwxrwxr-x  3 szita  staff       102 27 Feb 16:55 .
drwxr-xr-x  3 szita  staff       102 27 Feb 16:55 ..
-rw-rw-r--  1 szita  staff  14111349 27 Feb 16:55 pig-0.16.0-SNAPSHOT-withouthadoop-h2.jar

withoutBuildXmlFix/legacy:
total 11888
drwxrwxr-x  3 szita  staff      102 27 Feb 16:49 .
drwxr-xr-x  3 szita  staff      102 27 Feb 16:53 ..
-rw-rw-r--  1 szita  staff  6085577 27 Feb 16:49 pig-0.16.0-SNAPSHOT-withouthadoop-h2.jar
{code}","27/Feb/17 21:52;kellyzly;[~szita]:  [~rohini] suggested to remove all these jars in last round of [review|https://reviews.apache.org/r/45667/](more detailed see build.xml.from.rb.png).  After test i found that  in local or yarn mode, simple tests passed. so you find some exception or errors now?
",28/Feb/17 08:04;szita;[~kellyzly]: Okay I see. I uploaded the patch without this part: [^PIG-5132.aftermerge.1.patch],"01/Mar/17 08:18;kellyzly;[~szita]: commit PIG-5132-aftermerge.1.patch, please verify whether test/e2e/pig/lib/hadoop-streaming.jar update or not. If have any other issue, tell me.","01/Mar/17 12:31;szita;[~kellyzly] commit looks good, hadoop-streaming.jar checks out as well. This should conclude our merge from trunk. Thanks!",,,,,,,,
Test fail when running test-core-mrtez,PIG-5127,13041603,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,09/Feb/17 06:46,21/Jun/17 09:15,14/Mar/19 03:08,10/Feb/17 21:14,,,,,,0.17.0,,,,,0,,,,,,,"For example, the following command fail:

ant -Dtestcase=TestPredeployedJar test-core-mrtez

The reason is mr test left hadoop-site.xml and interfere with tez test. MiniMRCluster and MiniTezCluster use a different set of config files (hadoop-site.xml vs core-site.xml+hdfs-site.xml) and will only clear it's own config file when starting.",,,,,,,,,,,,,,,,,,,,09/Feb/17 06:47;daijy;PIG-5127-1.patch;https://issues.apache.org/jira/secure/attachment/12851807/PIG-5127-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-02-09 09:02:31.824,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Feb 10 21:14:58 UTC 2017,,,,,,,0|i39tvz:,9223372036854775807,,,,,,,,,,"09/Feb/17 09:02;szita;+1, looks good, test passes",09/Feb/17 19:41;rohini;+1,"10/Feb/17 21:14;daijy;Patch committed to trunk. Thanks Rohini, Adam for review!",,,,,,,,,,,,,,,,,,,,,,,,,
SkewedJoin_15 is unstable,PIG-5119,13039255,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,31/Jan/17 18:26,21/Jun/17 09:15,14/Mar/19 03:08,31/Jan/17 21:11,,,,,,0.16.1,0.17.0,,,,0,,,,,,,See intermittent failure as the order of tuple inside a bag is different.,,,,,,,,,,,,,,,,,,,,31/Jan/17 18:26;daijy;PIG-5119-1.patch;https://issues.apache.org/jira/secure/attachment/12850279/PIG-5119-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-01-31 19:44:27.896,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jan 31 21:11:50 UTC 2017,,,,,,,0|i39fgv:,9223372036854775807,,,,,,,,,,"31/Jan/17 19:44;nkollar;LGTM, thanks Daniel for fixing my mistake!",31/Jan/17 20:58;rohini;+1,"31/Jan/17 21:11;daijy;Patch committed to both trunk and 0.16 branch. Thanks Rohini, Nandor for review!",,,,,,,,,,,,,,,,,,,,,,,,,
Script fails with Invalid dag containing 0 vertices,PIG-5118,13038565,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,27/Jan/17 22:37,21/Jun/17 09:15,14/Mar/19 03:08,30/Jan/17 19:29,,,,,,0.16.1,0.17.0,tez,,,0,,,,,,,"  The current code of splitting DAG does not handle cases where there is split and a clean segmentation is not possible. It also does not handle cases where there is multiple segments and they are later used together. Both cases it ends up with ""Invalid dag containing 0 vertices"" error as it creates one empty DAG and and a new DAG which contains the full original plan as it was not able to segment.

Usually can ask user to get rid of unnecessary store and then load if in same script as it is usually a result of bad programming and inefficient. But in this case they were part of different large scripts and run as below.

{code:title=script.pig}
run -param output=output1 script1.pig  --stores to output1
run -param input=output1 script2.pig  -- load from output1
{code}",,,,,,,,,,,,,,,,,,,,27/Jan/17 22:45;rohini;PIG-5118-1.patch;https://issues.apache.org/jira/secure/attachment/12849766/PIG-5118-1.patch,30/Jan/17 01:36;rohini;PIG-5118-2.patch;https://issues.apache.org/jira/secure/attachment/12849898/PIG-5118-2.patch,30/Jan/17 18:39;rohini;PIG-5118-3.patch;https://issues.apache.org/jira/secure/attachment/12850022/PIG-5118-3.patch,31/Jan/17 12:27;szita;PIG-5118.fixTest.patch;https://issues.apache.org/jira/secure/attachment/12850188/PIG-5118.fixTest.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2017-01-30 18:44:54.522,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jan 31 16:39:30 UTC 2017,,,,,,,0|i39bkv:,9223372036854775807,,,,,,,,,,27/Jan/17 22:46;rohini;Patch uploaded to review board as well - https://reviews.apache.org/r/56043/,30/Jan/17 18:39;rohini;Daniel pointed out that StoredLoad_1 actually had 3 DAGs. Fixed that and added a test case for it.,30/Jan/17 18:44;daijy;+1 for PIG-5118-3.patch.,30/Jan/17 19:29;rohini;Committed to branch-0.16 and trunk. Thanks for the review Daniel.,"31/Jan/17 12:28;szita;It seems like Pig-trunk-commit job became unstable after this commit.
See: https://builds.apache.org/job/Pig-trunk-commit/2422/testReport/org.apache.pig.tez/TestTezCompiler/testStoreLoadSplit/

Seems like you forgot to regenerate the golden files after started using _FileLocalizer.setR(new Random(1331L))_ :
{code}
file:/tmp/temp-994194982/tmp570752215:org.apache.pig.impl.io.InterStorage
{code}
vs
{code}
file:/tmp/temp-1456742965/tmp-26634357:org.apache.pig.impl.io.InterStorage
{code}

Attached [^PIG-5118.fixTest.patch] to fix it
","31/Jan/17 16:39;rohini;Thanks [~szita] for fixing this. It did fail in our internal jenkins as well. Committed the PIG-5118.fixTest.patch. 

bq. Seems like you forgot to regenerate the golden files after started using FileLocalizer.setR(new Random(1331L))
 Actually I did regenerate and it was passing fine in my laptop due to https://github.com/apache/pig/blob/trunk/src/org/apache/pig/impl/io/FileLocalizer.java#L497-L499. I should have cleaned up  /tmp/temp* files before generating the golden file.",,,,,,,,,,,,,,,,,,,,,,
Cleanup pig-template.xml,PIG-5112,13037584,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,25/Jan/17 06:00,21/Jun/17 09:15,14/Mar/19 03:08,26/Jan/17 05:04,,,,,,0.16.1,0.17.0,build,,,0,,,,,,,"Several entries in pig-template.xml are outdated. Attach a patch to remove or update those entries. Later we shall use ivy:makepom to generate pig.pom and lib dir, I will open a separate ticket for that.",,,,,,,,,,,,,,,,,,,,25/Jan/17 06:12;daijy;PIG-5112-1.patch;https://issues.apache.org/jira/secure/attachment/12849227/PIG-5112-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-01-25 17:18:47.505,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Jan 26 05:04:15 UTC 2017,,,,,,,0|i3961r:,9223372036854775807,,,,,,,,,,25/Jan/17 17:18;thejas;+1,26/Jan/17 05:04;daijy;Patch committed to both 0.16 branch and trunk. Thanks Thejas for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e Utf8Test fails in local mode,PIG-5111,13036640,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,20/Jan/17 18:36,21/Jun/17 09:15,14/Mar/19 03:08,26/Jan/17 05:56,,,,,,0.16.1,0.17.0,,,,0,,,,,,,The test data required is not setup during deploy in local mode (test-e2e-deploy-local),,,,,,,,,,,,,,,,,,,,20/Jan/17 18:42;rohini;PIG-5111-1.patch;https://issues.apache.org/jira/secure/attachment/12848600/PIG-5111-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-01-26 01:31:12.391,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Jan 26 05:56:30 UTC 2017,,,,,,,0|i39087:,9223372036854775807,,,,,,,,,,26/Jan/17 01:31;daijy;+1,26/Jan/17 05:56;rohini;Committed to branch-0.16 and trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage on Tez with exception on nested records,PIG-5108,13035581,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,sgeller@teamaol.com,sgeller@teamaol.com,17/Jan/17 15:51,21/Jun/17 09:15,14/Mar/19 03:08,23/Jan/17 17:31,0.16.0,,,,,0.16.1,0.17.0,tez,,,0,,,,,,,"Hi,

While migrating to the latest Pig version we have seen a general issue when using nested Avro records on Tez:

{code}
Caused by: java.io.IOException: class org.apache.pig.impl.util.avro.AvroTupleWrapper.write called, but not implemented yet
	at org.apache.pig.impl.util.avro.AvroTupleWrapper.write(AvroTupleWrapper.java:68)
	at org.apache.pig.impl.io.PigNullableWritable.write(PigNullableWritable.java:139)
...
{code}

The setup is
schema
{code}
{
    ""fields"": [
        {
            ""name"": ""id"",
            ""type"": ""int""
        },
        {
            ""name"": ""property"",
            ""type"": {
                ""fields"": [
                    {
                        ""name"": ""id"",
                        ""type"": ""int""
                    }
                ],
                ""name"": ""Property"",
                ""type"": ""record""
            }
        }
    ],
    ""name"": ""Person"",
    ""namespace"": ""com.github.ouyi.avro"",
    ""type"": ""record""
}
{code}

Pig script group_person.pig
{code}
loaded_person =
    LOAD '$input'
    USING AvroStorage();

grouped_records =
    GROUP
        loaded_person BY (property.id);

STORE grouped_records
    INTO '$output'
    USING AvroStorage();
{code}

sample data
{code}
{""id"":1,""property"":{""id"":1}}
{code}

Execution on Tez
{code}
pig -x tez_local -p input=file:///usr/lib/pig/pig-0.16.0/person-prop.avro -p output=file:///output group_person.pig
...
Caused by: java.io.IOException: class org.apache.pig.impl.util.avro.AvroTupleWrapper.write called, but not implemented yet
	at org.apache.pig.impl.util.avro.AvroTupleWrapper.write(AvroTupleWrapper.java:68)
	at org.apache.pig.impl.io.PigNullableWritable.write(PigNullableWritable.java:139)
...
{code}

Execution on mapred
{code}
pig -x local -p input=file:///usr/lib/pig/pig-0.16.0/person-prop.avro -p output=file:///output7 group_person.pig
...
Output(s):
Successfully stored 1 records in: ""file:///output7""
...
{code}

I am going to attach the complete log files of both runs.

I assume that the Pig script should work regardless of Tez or mapreduce? Is there any underlying change when migrating to Tez which makes the schema invalid?

Thanks,
Sebastian","HadoopVersion: 2.6.0-cdh5.8.0
PigVersion: 0.16.0
TezVersion: 0.7.0",,,,,,,,,,,,,,,,,,,18/Jan/17 22:43;daijy;PIG-5108-1.patch;https://issues.apache.org/jira/secure/attachment/12848154/PIG-5108-1.patch,19/Jan/17 23:27;rohini;PIG-5108-2-addendum.patch;https://issues.apache.org/jira/secure/attachment/12848414/PIG-5108-2-addendum.patch,18/Jan/17 07:48;sgeller@teamaol.com;person-prop.avro;https://issues.apache.org/jira/secure/attachment/12848008/person-prop.avro,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-01-17 21:31:24.131,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed May 24 15:59:06 UTC 2017,,,,,,,0|i38tov:,9223372036854775807,,,,,,,,,,17/Jan/17 21:31;rohini;Deleted the original PIG-5103 which was faulty having been created during maintenance.,"17/Jan/17 23:44;daijy;[~sgeller], can you upload person-prop.avro to make our reproduce easier?",18/Jan/17 07:49;sgeller@teamaol.com;Attached person-prop.,"18/Jan/17 10:08;ouyi;The issue was also reproduced on AWS EMR 5.2.1, with the following Pig, Tez, and Hadoop versions:

tez.noarch                    0.8.4-1.amzn1                            @Bigtop
pig.noarch                    0.16.0.amzn.0-1.amzn1                    @Bigtop
hadoop.x86_64                 2.7.3.amzn.1-1.amzn1                     @Bigtop",18/Jan/17 22:43;daijy;AvroTupleWrapper.write needs implementation to cross vertex boundary. The script success in MR because all tuple will be converted to BinSedesTuple first (PigGenericMapBase:271).,18/Jan/17 22:46;rohini;+1,18/Jan/17 23:51;daijy;Patch committed to both trunk and 0.16 branch. Thanks Rohini for super fast review!,"19/Jan/17 07:56;sgeller@teamaol.com;[~daijy]
Thank you very much!",19/Jan/17 09:04;ouyi;Thanks guys!,"19/Jan/17 23:21;rohini;Reopening issue as it only fixes the problem for this tuple type. Came across another one which extended DefaultTuple and it errored out with ""Unexpected datatype 110 while reading tuplefrom binary file""",19/Jan/17 23:27;rohini;Attaching patch which does a tuple copy in POSimpleTezLoad similar to PigGenericMapReduce which Daniel pointed out.,23/Jan/17 09:33;daijy;+1 for PIG-5108-2-addendum.patch.,23/Jan/17 17:31;rohini;Committed to branch-0.16 and trunk. Thanks for the review Daniel.,24/May/17 15:59;rohini;Deleted PIG-5090 to PIG-5099 which are duplicates of this jira created during a jira outage.,,,,,,,,,,,,,,
Union_15 e2e test failing on Spark,PIG-5104,13034778,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,nkollar,nkollar,13/Jan/17 18:30,21/Jun/17 09:18,14/Mar/19 03:08,14/Mar/17 22:07,,,,,,spark-branch,,spark,,,0,,,,,,,"While working on PIG-4891 I noticed that Union_15 e2e test is failing on Spark mode with this exception:

Caused by: java.lang.RuntimeException: org.apache.pig.backend.executionengine.ExecException: ERROR 2078: Caught error from UDF: org.apache.pig.impl.builtin.GFCross [Unable to get parallelism hint from job conf]
	at org.apache.pig.backend.hadoop.executionengine.spark.converter.OutputConsumerIterator.readNext(OutputConsumerIterator.java:89)
	at org.apache.pig.backend.hadoop.executionengine.spark.converter.OutputConsumerIterator.hasNext(OutputConsumerIterator.java:96)
	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:369)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2078: Caught error from UDF: org.apache.pig.impl.builtin.GFCross [Unable to get parallelism hint from job conf]
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:358)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNextDataBag(POUserFunc.java:374)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:335)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:404)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:321)
	at org.apache.pig.backend.hadoop.executionengine.spark.converter.ForEachConverter$ForEachFunction$1$1.getNextResult(ForEachConverter.java:87)
	at org.apache.pig.backend.hadoop.executionengine.spark.converter.OutputConsumerIterator.readNext(OutputConsumerIterator.java:69)
	... 11 more
Caused by: java.io.IOException: Unable to get parallelism hint from job conf
	at org.apache.pig.impl.builtin.GFCross.exec(GFCross.java:66)
	at org.apache.pig.impl.builtin.GFCross.exec(GFCross.java:37)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:330)",,,,,,,,,,,,,,,,,,,,17/Jan/17 16:54;nkollar;PIG-5104.patch;https://issues.apache.org/jira/secure/attachment/12847864/PIG-5104.patch,10/Feb/17 08:17;kellyzly;PIG-5104.zly.patch;https://issues.apache.org/jira/secure/attachment/12852015/PIG-5104.zly.patch,25/Jan/17 10:51;nkollar;TestUnion_15.java;https://issues.apache.org/jira/secure/attachment/12849257/TestUnion_15.java,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-01-17 15:45:14.768,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 14 22:07:35 UTC 2017,,,,,,,0|i38or3:,9223372036854775807,,,,,,,,,,"17/Jan/17 15:45;rohini;[~nkollar],
   Deleted the duplicate jiras PIG-5100 and PIG-5101 that were created during maintenance and which were in a inconsistent state as per your request.",25/Jan/17 10:51;nkollar;Thanks Rohini!,"25/Jan/17 10:53;nkollar;[~kellyzly] attached a unit test to show the issue. It is not included into the diff, because we already have similar e2e tests for this scenario, but executing the unit test might be easier in local mode.","10/Feb/17 08:16;kellyzly;[~nkollar]: thanks for your patience. sorry to late reply.
here my understanding to this jira is
1. ParallelismSetter#visitSparkOp fails to execute ""jobConf.set(PigImplConstants.PIG_CROSS_PARALLELISM + ""."" + key, Integer.toString(96)))"" because sparkOperator missed some crossKey
2. the reason why sparkOperator missed some crossKey is because we don't add crossKey of previous sparkOperator in SparkCompiler#merge
more detail see PIG-5104.zly.patch. Not have time to full test the patch. 

","10/Feb/17 08:17;kellyzly;[~nkollar]: after using PIG-5104.zly.patch, the given unit test pass.","10/Feb/17 09:12;nkollar;Ok, your patch is more compact. I'll apply it and re-run the e2e test again to make sure it fixes the related test case.","14/Feb/17 10:43;nkollar;+1 for PIG-5104.zly.patch, Union_15 is green after applying the patch, however I noticed that Union_3 is still failing, while it passes on MR mode in spark branch. I think this failure is not related to this issue.","14/Feb/17 11:25;nkollar;It seems that Union_3 failure is not related to the parallelism estimation, created a separate Jira for it: PIG-5130",14/Feb/17 16:58;rohini;Deleted PIG-5102 as per [~nkollar]'s request. It was a duplicate jira created during jira maintenance which was in a unusable state.,20/Feb/17 02:39;kellyzly;[~nkollar] : all unit tests pass in local mode after using PIG-5104_zly.patch in my local jenkins. whether there is any problem in yarn-client or yarn-cluster mode after this patch in e2e tests?,"28/Feb/17 10:59;nkollar;Union e2e tests were fine, only Union_3 was failing, but I think it is not related to your patch.",13/Mar/17 09:26;szita;+1 on [^PIG-5104.zly.patch] [~kellyzly],"14/Mar/17 21:23;kellyzly;[~xuefuz]: please help check in PIG-5104.zly.patch, thanks",14/Mar/17 21:26;rohini;+1. You can go ahead and commit it.,"14/Mar/17 22:07;xuefuz;Patch committed to Spark branch. Thanks, Liyun!",,,,,,,,,,,,,
HashValuePartitioner has skew when there is only map fields,PIG-5088,13033992,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,11/Jan/17 22:28,21/Jun/17 09:15,14/Mar/19 03:08,12/Jan/17 00:09,,,,,,0.16.1,0.17.0,,,,0,,,,,,,"  One user who just did union of map fields, got a skew with everything going to one input file as hashcode for map was calculated on the size. [~knoguchi] pointed out that my assumption of map hashcode being non-deterministic and can change with jdk versions is wrong as it just does a sum of hashcode of its entries.",,,,,,,,,,,,,,,,,,,,11/Jan/17 22:31;rohini;PIG-5088-1.patch;https://issues.apache.org/jira/secure/attachment/12847091/PIG-5088-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-01-11 22:33:15.916,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Jan 12 00:09:55 UTC 2017,,,,,,,0|i38kxb:,9223372036854775807,,,,,,,,,,11/Jan/17 22:33;knoguchi;+1,12/Jan/17 00:09;rohini;Committed to branch-0.16 and trunk. Thanks for the review Koji.,,,,,,,,,,,,,,,,,,,,,,,,,,
CombinerPackager and LitePackager should not materialize bags,PIG-5083,13032687,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,06/Jan/17 21:34,21/Jun/17 09:15,14/Mar/19 03:08,18/Jan/17 13:38,,,,,,0.17.0,,,,,0,,,,,,,"Before PIG-3591 and creation of CombinerPackager, POCombinerPackage directly read from the combiner/reducer input instead of materializing the bag.

https://github.com/apache/pig/blob/branch-0.12/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCombinerPackage.java#L140-L161

The unnecessary materialization leads to lot of spills and OOMs in some cases.",,,,,,,,,,,,,,,,,,,,06/Jan/17 21:58;rohini;PIG-5083-1.patch;https://issues.apache.org/jira/secure/attachment/12846099/PIG-5083-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-01-14 08:15:42.904,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Jan 18 13:38:21 UTC 2017,,,,,,,0|i38dmv:,9223372036854775807,,,,,,,,,,06/Jan/17 21:46;rohini;Found that https://github.com/apache/pig/blob/branch-0.12/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackageLite.java#L170-L180 also was not materializing bags earlier.,"06/Jan/17 21:58;rohini;While analyzing the heapdump during OOM for the Combiner, found that while deserializing next value in NullableTuple 
{code}
mValue = bis.readTuple(in);
{code}

the previous value of mValue was still a strong reference and could not be garbage collected. In case of DISTINCT inside nested foreach and map.exec.PartAgg=true that could be a really big bag taking up lot of memory and can lead to OOM while the next tuple is being deserialized in bis.readTuple. That is also fixed in this patch.

Just noticed that it could be applied to LitePackager as well and added that to the patch. So rerunning the full unit and e2e tests now. ",11/Jan/17 17:39;rohini;Unit and e2e tests are good.,"14/Jan/17 08:15;daijy;I didn't get what you mean by not materializing the bag in https://github.com/apache/pig/blob/branch-0.12/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCombinerPackage.java#L140-L161. InternalCachedBag is created and tuples are taken from iterator and added to the bag in that code. On the other hand, I can see LitePackager is not given a ReadOnceBag in Tez and you do fix it in the patch.","14/Jan/17 18:43;rohini;If you look at https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/Packager.java#L113-L129 , in case of mapreduce where readOnce is true records are read from the PeekedBag (extends ReadOnceBag)  and put in a InternalCachedBag before being handed off to the CombinerPackager.getNext() which then creates different bags for the rest of the plan to work with. Since the bag is only iterated once, there is no need to materialize it into a InternalCachedBag. Iteration can be done on the ReadOnceBag.

 What this patch does is pass the PeekedBag directly to the CombinerPackager in case of mapreduce and pass TezReadOnceBag with tez.  Tez was always constructing a InternalCachedBag before and did not have concept of ReadOnceBag (readOnce was always false). It saves one copy and a lot of memory+GC.","18/Jan/17 08:53;daijy;I see. CombinerPackager.attachInput prevent materializing bag on the combiner. TezReadOnceBag prevent materializing bag on the vertex input if the vertex has combine or order by. Seems streaming the last input in join case will use TezReadOnceBag as well in the follow up fix.

+1 for commit.",18/Jan/17 13:38;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,
Tez UnionOptimizer creates vertex group with one member,PIG-5082,13029932,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rohini,tmwoodruff,tmwoodruff,21/Dec/16 22:36,21/Jun/17 09:15,14/Mar/19 03:08,17/Jan/17 15:47,0.16.0,,,,,0.16.1,0.17.0,tez,,,0,,,,,,,"This script results in a vertex group with one member:

{code}
a = LOAD '/tmp/empty.txt' USING PigStorage('\t') AS (x:chararray);
b = LOAD '/tmp/empty.txt' USING PigStorage('\t') AS (x:chararray);
c = LOAD '/tmp/empty.txt' USING PigStorage('\t') AS (y:chararray);

u1 = UNION ONSCHEMA a, b;
SPLIT u1 INTO r IF x != '', s OTHERWISE;
d = JOIN r BY x LEFT, c BY y;
u2 = UNION ONSCHEMA d, s;
e = FILTER u2 BY x == '';
f = FILTER u2 BY x == 'm';
u3 = UNION ONSCHEMA e, f;
DUMP u3;
{code}

Which results in:
{code}
java.lang.IllegalArgumentException: VertexGroup must have at least 2 members
	at org.apache.tez.dag.api.VertexGroup.<init>(VertexGroup.java:77)
	at org.apache.tez.dag.api.DAG.createVertexGroup(DAG.java:202)
	at org.apache.pig.backend.hadoop.executionengine.tez.TezDagBuilder.visitTezOp(TezDagBuilder.java:396)
	at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezOperator.visit(TezOperator.java:255)
...
{code}

This seems to be happening because {{UnionOptimizer}} is replacing a union with a vertex group and then optimizing away a predecessor union thus removing a node and resulting in a vertex group with one member.
",,,,,,,,,,,,,,,,,,,,21/Dec/16 22:45;tmwoodruff;PIG-5082.patch;https://issues.apache.org/jira/secure/attachment/12844308/PIG-5082.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-01-04 00:56:02.58,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 17 15:47:56 UTC 2017,,,,,,,0|i37wnj:,9223372036854775807,,,,,,,,,,"21/Dec/16 22:45;tmwoodruff;Here's a patch that works around the issue. A better fix would probably be to detect the bad vertex group and remove it from the plan, but that's above my skill level.","04/Jan/17 00:56;daijy;The patch works for me. Can you add the test case to patch.

[~rohini], can you check if you want fix in a different way?","05/Jan/17 23:53;rohini;Disabling is not a good thing. It should have removed that vertex group while merging into split. [~tmwoodruff], is it ok if I assign this jira to myself or you want to work on it?",06/Jan/17 00:02;daijy;Actually I assigned the issue to Travis. I assume it is fine.,06/Jan/17 00:14;rohini;Actually the patch I had done for PIG-5078 fixes this as well. Let me add this test case also to it and upload patch.,"06/Jan/17 00:43;tmwoodruff;Feel free. I haven't had a chance to add a test for this yet, so let me know if you still want me to do so (should have time tomorrow).","06/Jan/17 17:23;rohini;[~tmwoodruff],
   No. I got that covered. I only need to update the e2e test I had for PIG-5078 with your case. I will do that shortly. Meanwhile, can you test the https://issues.apache.org/jira/secure/attachment/12846040/PIG-5078-1.patch and let me know if that works for you?","16/Jan/17 14:49;tmwoodruff;Bad news, [~rohini]. The patch for PIG-5078 (I used PIG-5078-3.patch) fixed my test above, but it didn't fix my actual script that was failing. The following still fails with the same error (added an additional split and join to the end):

{code}
a = LOAD '/tmp/empty.txt' USING PigStorage('\t') AS (x:chararray);
b = LOAD '/tmp/empty.txt' USING PigStorage('\t') AS (x:chararray);
c = LOAD '/tmp/empty.txt' USING PigStorage('\t') AS (y:chararray);

u1 = UNION ONSCHEMA a, b;
SPLIT u1 INTO r IF x != '', s OTHERWISE;
d = JOIN r BY x LEFT, c BY y;
u2 = UNION ONSCHEMA d, s;
e = FILTER u2 BY x == '';
f = FILTER u2 BY x == 'm';
u3 = UNION ONSCHEMA e, f;
SPLIT u3 INTO t if x != '', u OTHERWISE;
v = JOIN t BY x LEFT, c BY y;
DUMP v;
{code}

This also fails (same as previous but with limit instead of final join):

{code}
a = LOAD '/tmp/empty.txt' USING PigStorage('\t') AS (x:chararray);
b = LOAD '/tmp/empty.txt' USING PigStorage('\t') AS (x:chararray);
c = LOAD '/tmp/empty.txt' USING PigStorage('\t') AS (y:chararray);

u1 = UNION ONSCHEMA a, b;
SPLIT u1 INTO r IF x != '', s OTHERWISE;
d = JOIN r BY x LEFT, c BY y;
u2 = UNION ONSCHEMA d, s;
e = FILTER u2 BY x == '';
f = FILTER u2 BY x == 'm';
u3 = UNION ONSCHEMA e, f;
SPLIT u3 INTO t if x != '', u OTHERWISE;
v = LIMIT t 10;
DUMP t;
{code}",16/Jan/17 19:41;rohini;That is a case of non-store vertex at the end. The created plan has two separate vertex groups going into the final limit vertex. Looking into it now. Will fix it as well. ,"16/Jan/17 20:50;rohini;[~tmwoodruff],
     Can you try https://issues.apache.org/jira/secure/attachment/12847673/PIG-5078-4.patch? Issue was same as with store. I had to put the same fix for non-store vertex groups as well. Added both of your cases to the test.  Plan also looks neat now. It only has 5 vertices - 3 Load vertices and 2 join vertices for your first case and 3 Load vertices, 1 join vertex and 1 limit vertex for your second case.  This should perform really better compared to turning of the union optimizer in Tez. It should also perform lot better than mapreduce which has 6 jobs - 4 map only jobs and 2 map-reduce jobs.",17/Jan/17 13:53;tmwoodruff;Looks good after applying that patch. Thanks. Should I close this?,"17/Jan/17 15:47;rohini;Closing this jira.  Patch for this jira is part of PIG-5078. Not closing as duplicate because, the second issue was different than one in PIG-5078.",,,,,,,,,,,,,,,,
Script fails with error - POStoreTez only accepts MROutput,PIG-5078,13028762,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,16/Dec/16 20:09,21/Jun/17 09:15,14/Mar/19 03:08,17/Jan/17 17:46,,,,,,0.16.1,0.17.0,,,,0,,,,,,,"Script with following pattern

{code}
a = load 'file:///tmp/input' as (x:int, y:chararray);
b = load 'file:///tmp/input1' as (y:chararray, x:int);
c = union onschema a, b;"" +
split c into d if x <= 5, e if x <= 10, f if x >10, g if y == '6';
h = union onschema d, e, f, g;
store h into 'output';
{code}

fails with the below exception

{code}
Error: Failure while running task:org.apache.pig.backend.executionengine.ExecException: ERROR 0: POStoreTez only accepts MROutput. key = scope-57, outputs = {scope-45=org.apache.tez.mapreduce.output.MROutput@154c3ee2, scope-46=org.apache.tez.mapreduce.output.MROutput@1e986ab0, scope-44=org.apache.tez.mapreduce.output.MROutput@360aaa8a, scope-47=org.apache.tez.mapreduce.output.MROutput@75a0d6b1}
at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POStoreTez.attachOutputs(POStoreTez.java:95)
at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.initializeOutputs(PigProcessor.java:372)
at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:230)
{code}",,,,,,,,,,,,PIG-5082,,,,,,,,06/Jan/17 17:20;rohini;PIG-5078-1.patch;https://issues.apache.org/jira/secure/attachment/12846040/PIG-5078-1.patch,06/Jan/17 20:18;rohini;PIG-5078-2.patch;https://issues.apache.org/jira/secure/attachment/12846074/PIG-5078-2.patch,14/Jan/17 19:17;rohini;PIG-5078-3.patch;https://issues.apache.org/jira/secure/attachment/12847505/PIG-5078-3.patch,16/Jan/17 20:43;rohini;PIG-5078-4.patch;https://issues.apache.org/jira/secure/attachment/12847673/PIG-5078-4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2017-01-14 08:24:47.552,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jan 17 17:46:14 UTC 2017,,,,,,,0|i37pfr:,9223372036854775807,,,,,,,,,,06/Jan/17 17:20;rohini;Will attach another patch with e2e test shortly,14/Jan/17 08:24;daijy;+1,14/Jan/17 19:17;rohini;While running tests before commit found that TestTezCompiler.testUnionSplit failed because of golden file mismatch. Not sure how I missed that earlier. Fixed that now.,16/Jan/17 20:43;rohini;Attaching patch that fixes the same case for non-store vertex groups and additional tests for it based on [~tmwoodruff]'s comments in https://issues.apache.org/jira/browse/PIG-5082?focusedCommentId=15824102&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15824102,17/Jan/17 17:30;daijy;+1 for PIG-5078-4.patch.,17/Jan/17 17:46;rohini;Committed to branch-0.16 and trunk. Thanks Daniel for the review.  Thanks [~tmwoodruff] for pointing out additional issues and verifying the fixes.,,,,,,,,,,,,,,,,,,,,,,
Build broken when hadoopversion=20 in branch 0.16,PIG-5074,13026637,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,szita,szita,08/Dec/16 10:48,21/Jun/17 09:15,14/Mar/19 03:08,08/Dec/16 18:14,0.16.1,,,,,0.16.1,0.17.0,build,,,0,,,,,,,"Came across this issue:
{code}
ant -Dhadoopversion=20 jar pigunit-jar
{code}
{code}
compile-test:
     [echo] *** Building Test Sources ***
     [echo] *** To compile with all warnings enabled, supply -Dall.warnings=1 on command line ***
     [echo] *** Else, you will only be warned about deprecations ***
    [javac] Compiling 47 source files to /Users/szita/shadow/CDH/pig/build/test/classes
    [javac] warning: [options] bootstrap class path not set in conjunction with -source 1.7
    [javac] /Users/szita/shadow/CDH/pig/test/org/apache/pig/impl/builtin/TestHiveUDTF.java:20: error: package org.apache.commons.collections4 does not exist
    [javac] import org.apache.commons.collections4.IteratorUtils;
    [javac]                                       ^
    [javac] /Users/szita/shadow/CDH/pig/test/org/apache/pig/impl/builtin/TestHiveUDTF.java:66: error: cannot find symbol
    [javac]         List<Tuple> out = IteratorUtils.toList(result);
    [javac]                           ^
    [javac]   symbol:   variable IteratorUtils
    [javac]   location: class TestHiveUDTF
    [javac] /Users/szita/shadow/CDH/pig/test/org/apache/pig/impl/builtin/TestHiveUDTF.java:88: error: cannot find symbol
    [javac]         List<Tuple> out = IteratorUtils.toList(result);
    [javac]                           ^
    [javac]   symbol:   variable IteratorUtils
    [javac]   location: class TestHiveUDTF
    [javac] /Users/szita/shadow/CDH/pig/test/org/apache/pig/impl/builtin/TestHiveUDTF.java:110: error: cannot find symbol
    [javac]         List<Tuple> out = IteratorUtils.toList(result);
    [javac]                           ^
    [javac]   symbol:   variable IteratorUtils
    [javac]   location: class TestHiveUDTF
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 4 errors
    [javac] 1 warning

BUILD FAILED
{code}",,,,,,,,,,,,,,,,,,,,08/Dec/16 10:53;szita;PIG-5074.patch;https://issues.apache.org/jira/secure/attachment/12842325/PIG-5074.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-12-08 18:14:22.345,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Dec 08 18:16:10 UTC 2016,,,,,,,0|i37cbr:,9223372036854775807,,,,,,,,,,"08/Dec/16 11:03;szita;Build was broken because org.apache.commons/commons-collections4 is only used in hadoop23 ivy config.
I believe the least intrusive fix is to use the Guava library for the same purpose (we already depend on Guava in all configs).

The patch ([^PIG-5074.patch]) is also cherry-pickable to trunk (although hadoop20 will be removed there) if we want to deviate less between branches.

[~daijy] can you please take a quick look?",08/Dec/16 18:14;daijy;+1. Patch committed to both trunk and 0.16 branch. Thanks Adam!,08/Dec/16 18:16;szita;Thanks for reviewing!,,,,,,,,,,,,,,,,,,,,,,,,,
Revisit union on numeric type and chararray to bytearray,PIG-5067,13021851,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,18/Nov/16 20:50,21/Jun/17 09:15,14/Mar/19 03:08,02/Dec/16 15:59,,,,,,0.17.0,,,,,0,,,,,,,"In PIG-2071, we changed the behavior of union on numeric and chararray to bytearray.

This itself was always failing at runtime until we changed to skip the bytearray typecast for union-onschema in PIG-3270.
(For union, it still fails with typecast to bytearray error. )

Now, seeing users getting inconsistent results due to this union-ed bytearray.
",,,,,,,,,,,,,,,,,,,,01/Dec/16 20:04;knoguchi;pig-5067-v01.patch;https://issues.apache.org/jira/secure/attachment/12841342/pig-5067-v01.patch,02/Dec/16 15:41;knoguchi;pig-5067-v02.patch;https://issues.apache.org/jira/secure/attachment/12841518/pig-5067-v02.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-11-18 22:14:02.524,,,no_permission,,,,,,,,,,,,,9223372036854775807,Incompatible change,Reviewed,,,Fri Dec 02 15:59:29 UTC 2016,,,,,,,0|i36is7:,9223372036854775807,"This change now disallows union-onschema of incompatible types,  (for example, numeric types and chararray). It makes it consistent with plain union.
",,,,,,,,,"18/Nov/16 20:54;knoguchi;Discussed with [~rohini] offline and she pointed out 
{quote}
Either chararray should be converted to numeric or other way round. Doesn't make sense to consider them as bytearray. All key based operations (order by, join, group by) will give wrong output
{quote}

[~daijy], asking for your input.","18/Nov/16 22:14;daijy;bytearray is a wrong name, it means unknown type in Pig. Union chararray and numeric result an unknown type is reasonable. Order by mixed type makes nonsense. join/""group by"" should deal with unknown type, any particular issue did you see?","18/Nov/16 22:40;knoguchi;bq.  any particular issue did you see?

The one I looked at was from user unknowingly mixing two incompatible types in union-onschema and order-by showing completely off results.  

Also, I prefer to avoid mixed type inside bytearray when possible.  
Following result is counter-intuitive to me (although I understand why it's like that).

{code:title=test.pig}
 A = load '1.txt' as (a1:int);
 B = load '2.txt' as (a1:chararray);
 C = UNION onschema A, B; '
 describe C;
 D = GROUP C by a1;
 dump D;
{code}
{noformat}
% cat 1.txt 
1
% cat 2.txt 
1
% pig test.pig 
C: {a1: bytearray}
(1,{(1)})
(1,{(1)})
{noformat}","18/Nov/16 22:48;daijy;It produce theoretically right result, but I agree that's misleading. How about throw a frontend exception if mixing incompatible types?","18/Nov/16 23:01;knoguchi;bq. How about throw a frontend exception if mixing incompatible types?

+1.  With just  'union', it already fail at compile time since I only took out the (bytearray) typecast for union-onschema and not union in PIG-3270.","21/Nov/16 22:19;knoguchi;While looking at the current testcase, saw 

{code:title=TestUnionOnSchema.testUnionOnSchemaIncompatibleTypes}
          query =
              ""  l1 = load '"" + INP_FILE_2NUMS + ""' as (x : long, y : chararray);""
              + ""l2 = load '"" + INP_FILE_2NUMS + ""' as (x : map[ ], y : chararray);""
              + ""u = union onschema l1, l2;""
          ;
          checkSchemaEquals(query, ""x : bytearray, y : chararray"");
{code}

""Union"" itself fails while trying to typecast to bytearray but Union-onschema currently passes by skipping bytearray typecast...
Shall we fail this use-case as well?   Not sure how many users depend on these incompatible type union.
",28/Nov/16 20:03;daijy;We shall mark it a backward incompatible change and document how to fix existing script. But I think this is a reasonable change and we shall go ahead.,"01/Dec/16 20:04;knoguchi;Attaching {{pig-5067-v01.patch}} that would 
* Fail for incompatible union with ""onschema"". Regular union has been failing already.
* union with different internal column types for tuples or bags are still allowed (with empty schema)
* Add line-number for ""union"" failures.",01/Dec/16 23:53;daijy;+1 pending test.,"02/Dec/16 15:41;knoguchi;Thanks for the review Daniel.

Tests passed, committing shortly.  
Found one typo in my error message.  

{noformat}
< String msg = ""Union of incompatible types now allowed. ""
> String msg = ""Union of incompatible types not allowed. ""
{noformat}
Obviously, it should be ""NOT"" allowed.  Updating the patch and committing this one.","02/Dec/16 15:59;knoguchi;Thanks again for the review Daniel!  

Committed to trunk.",,,,,,,,,,,,,,,,,
NPE in TestScriptUDF#testPythonBuiltinModuleImport1 when JAVA_HOME is not set,PIG-5064,13021705,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,water,water,water,18/Nov/16 08:31,21/Jun/17 09:15,14/Mar/19 03:08,21/Nov/16 18:37,0.16.0,,,,,0.16.1,0.17.0,,,,0,UT,,,,,,"When JAVA_HOME is not set (my env is Red Hat 7, alternatives is used and JAVA_HOME could be not set)
line 250
{code}
Assert.assertTrue(t.get(0).toString().equals(System.getenv(input[1])));
{code}
t.get(0) is null, as a result, the following NPE is thrown:
{code}
java.lang.NullPointerException
        at org.apache.pig.test.TestScriptUDF.testPythonBuiltinModuleImport1(TestScriptUDF.java:250)
{code}",,,,,,,,,,,,,,,,,,,,18/Nov/16 08:44;water;PIG-5064-trunk-0.patch;https://issues.apache.org/jira/secure/attachment/12839526/PIG-5064-trunk-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-11-18 22:34:01.816,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Nov 22 07:07:49 UTC 2016,,,,,,,0|i36hvr:,9223372036854775807,,,,,,,,,,"18/Nov/16 09:17;water;The fix checks if JAVA_HOME is set firstly
(1) If it is set, it is safe to call t.get(0).toString(). 
(2) If it is not, verify if t.get(0) is null.",18/Nov/16 09:18;water;Patch 0 can be applied into branch-0.16 as well.,18/Nov/16 22:34;daijy;Fine for me. Do you really want it in 16.1? Or trunk is ok?,"19/Nov/16 02:02;water;Hi Daniel, it would be good if it can be applied into both trunk and 0.16.1. Thanks!",21/Nov/16 18:37;daijy;Patch committed to both trunk and 0.16 branch. Thanks Xiang!,22/Nov/16 07:07;water;Thanks Daniel!,,,,,,,,,,,,,,,,,,,,,,
Fix AvroStorage writing enums,PIG-5056,13019527,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,szita,szita,09/Nov/16 12:58,21/Jun/17 09:15,14/Mar/19 03:08,10/Nov/16 23:15,0.16.0,,,,,0.17.0,,,,,0,avro,,,,,,"Issue is observable with latest (1.8.1) Avro since it has an extra check for enum types that the current 1.7.5 does not care about (see here: https://github.com/apache/avro/blob/release-1.8.1/lang/java/avro/src/main/java/org/apache/avro/generic/GenericDatumWriter.java#L163)

This results in TestAvroStorage#testLoadRecordsWithEnums failing: Pig reads an Avro file with a schema containing (string,int,enum) this is then represented in Pig as (chararray,int,chararray) and then Pig writes this back to an Avro file with given schema (string,int,enum). 

{code}
java.lang.Exception: java.io.IOException: org.apache.avro.file.DataFileWriter$AppendWriteException: org.apache.avro.AvroTypeException: Not an enum: GOOD
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.io.IOException: org.apache.avro.file.DataFileWriter$AppendWriteException: org.apache.avro.AvroTypeException: Not an enum: GOOD
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.StoreFuncDecorator.putNext(StoreFuncDecorator.java:83)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:144)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:97)
	at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:655)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:282)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:275)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:65)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.avro.file.DataFileWriter$AppendWriteException: org.apache.avro.AvroTypeException: Not an enum: GOOD
	at org.apache.avro.file.DataFileWriter.append(DataFileWriter.java:308)
	at org.apache.pig.impl.util.avro.AvroRecordWriter.write(AvroRecordWriter.java:115)
	at org.apache.pig.impl.util.avro.AvroRecordWriter.write(AvroRecordWriter.java:51)
	at org.apache.pig.builtin.AvroStorage.putNext(AvroStorage.java:520)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.StoreFuncDecorator.putNext(StoreFuncDecorator.java:75)
	... 18 more
Caused by: org.apache.avro.AvroTypeException: Not an enum: GOOD
	at org.apache.avro.generic.GenericDatumWriter.writeEnum(GenericDatumWriter.java:164)
	at org.apache.avro.generic.GenericDatumWriter.writeWithoutConversion(GenericDatumWriter.java:106)
	at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:73)
	at org.apache.avro.generic.GenericDatumWriter.writeField(GenericDatumWriter.java:153)
	at org.apache.avro.generic.GenericDatumWriter.writeRecord(GenericDatumWriter.java:143)
	at org.apache.avro.generic.GenericDatumWriter.writeWithoutConversion(GenericDatumWriter.java:105)
	at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:73)
	at org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:60)
	at org.apache.avro.file.DataFileWriter.append(DataFileWriter.java:302)
	... 22 more
{code}",,,,,,,,,,,,,AVRO-997,,,,,,,09/Nov/16 13:24;szita;PIG-5056.patch;https://issues.apache.org/jira/secure/attachment/12838174/PIG-5056.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-11-09 13:58:02.061,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Nov 11 11:49:10 UTC 2016,,,,,,,0|i364fr:,9223372036854775807,,,,,,,,,,"09/Nov/16 13:58;nkollar;Looks good to me, doesn't break Avro 1.7.5, and fixes problem with 1.8.1.",09/Nov/16 14:29;szita;[~daijy] I think we could also think about upgrading the avro version to 1.8.1 - we might want a separate jira for that,"09/Nov/16 18:24;daijy;I see current stable release of Hadoop/Hive is still using 1.7. Unless there is a compelling reason, stay in line with Hadoop/Hive will reduce the chance of conflicting.","10/Nov/16 09:55;szita;That makes sense, I guess we'll talk about upgrading to 1.8.1 in the future then.

Meanwhile however, we still might want to have this small fix in ([^PIG-5056.patch]) - in case anyone wants to use this with 1.8 Avro in their cluster.","10/Nov/16 23:15;daijy;+1. Patch committed to trunk. Thanks Nandor, Adam!",11/Nov/16 11:49;szita;Thank you all!,,,,,,,,,,,,,,,,,,,,,,
Infinite loop with join by fixed index,PIG-5055,13019400,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,08/Nov/16 22:57,21/Jun/17 09:15,14/Mar/19 03:08,10/Nov/16 18:06,,,,,,0.17.0,,,,,0,,,,,,,"Following pig script runs forever unless {{ColumnMapKeyPrune}} is disabled.
{code}
A = load 'input1.txt' as (a0:int, a1:int, a2:int);
B = load 'input2.txt' as (b0:int, b1:int, b2:int);
B2 = FILTER B by b0 == 0;
C = join A by (1), B2 by (1) ;
D = FOREACH C GENERATE A::a1, A::a2;
store D into '/tmp/deleteme';
{code}",,,,,,,,,,,,,,,,,,,,08/Nov/16 23:17;knoguchi;pig-5055-v01.patch;https://issues.apache.org/jira/secure/attachment/12838075/pig-5055-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-11-09 00:28:17.051,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Nov 10 18:06:39 UTC 2016,,,,,,,0|i363nr:,9223372036854775807,,,,,,,,,,"08/Nov/16 23:07;knoguchi;This seems to be due to having EMPTY Foreach being added in the logicalplan.

{noformat}
        |
        |---B2: (Name: LOForEach Schema: null)
            |   |
            |   (Name: LOGenerate[] Schema: null)
            |
            |---B2: (Name: LOFilter Schema: b0#1:int)ColumnPrune:OutputUids=[]ColumnPrune:InputUids=[1]
                |   |
                |   (Name: Equal Type: boolean Uid: 12)
                |   |
                |   |---b0:(Name: Project Type: int Uid: 1 Input: 0 Column: 0)
                |   |
                |   |---(Name: Constant Type: int Uid: 11)
                |
                |---B: (Name: LOForEach Schema: b0#1:int)ColumnPrune:OutputUids=[1]ColumnPrune:InputUids=[1]
                    |   |
                    |   (Name: LOGenerate[false] Schema: b0#1:int)ColumnPrune:OutputUids=[1]ColumnPrune:InputUids=[1]
                    |   |   |
                    |   |   (Name: Cast Type: int Uid: 1)
                    |   |   |
                    |   |   |---b0:(Name: Project Type: bytearray Uid: 1 Input: 0 Column: (*))
                    |   |
                    |   |---(Name: LOInnerLoad[0] Schema: b0#1:bytearray)
                    |
                    |---B: (Name: LOLoad Schema: b0#1:bytearray)ColumnPrune:OutputUids=[1]ColumnPrune:InputUids=[1]ColumnPrune:RequiredColumns=[0]RequiredFields:[0]
{noformat}","08/Nov/16 23:17;knoguchi;Attaching a patch that adds check in AddForEach optimization to skip when {{outputUids}} is empty.

Running tests.",09/Nov/16 00:28;daijy;+1 pending test.,"10/Nov/16 18:06;knoguchi;Thanks for the review Daniel! 
Committed to trunk.  ",,,,,,,,,,,,,,,,,,,,,,,,
Cleanup e2e tests turing_jython.conf,PIG-5049,13014993,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,25/Oct/16 06:38,21/Jun/17 09:15,14/Mar/19 03:08,03/Nov/16 21:43,,,,,,0.17.0,,e2e harness,,,0,,,,,,,"turing_jython is quite messy. In this ticket, I clean up the test suits and fix several issues:
1. The simple load/store with no param test repeated 3 times in different context
2. bring back Jython_Properties
3. Jython_Error_[4,5,7] are not test the right negative condition
4. comments does not match test",,,,,,,,,,,,,,,,,,,,25/Oct/16 06:40;daijy;PIG-5049-1.patch;https://issues.apache.org/jira/secure/attachment/12835076/PIG-5049-1.patch,02/Nov/16 23:54;daijy;PIG-5049-2.patch;https://issues.apache.org/jira/secure/attachment/12836692/PIG-5049-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-11-02 22:47:26.513,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Nov 03 21:43:09 UTC 2016,,,,,,,0|i35chb:,9223372036854775807,,,,,,,,,,"02/Nov/16 22:47;rohini;1) Could you also move Jython_Command_1 to hcat.conf? Always have a failure on that one when I don't have hcat setup.
2) Jython_Error_7 
      - results = P.bind().run() does not have out1 and out2 bound. It will fail there instead of failing on iter = results[0].result(""E"").iterator(). 
      -  result.isSuccessful() should be results[0].isSuccessful(). It is not going to be executed as iterator call will fail. But still for sake of correctness.","02/Nov/16 23:54;daijy;Good idea to move Jython_Command_1 to hcat.conf.

Jython_Error_7 have out1 and out2 bound and it fails with results[0].result(""E"").iterator(). Fixed result.isSuccessful() -> results[0].isSuccessful().",03/Nov/16 20:15;rohini;+1,03/Nov/16 21:43;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
HiveUDTF fail if it is the first expression in projection,PIG-5048,13014401,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,daijy,daijy,21/Oct/16 20:29,21/Jun/17 09:15,14/Mar/19 03:08,09/Nov/16 18:02,,,,,,0.16.1,0.17.0,impl,,,0,,,,,,,"The following script fail:
{code}
define explode HiveUDTF('explode');
A = load 'bag.txt' as (a0:{(b0:chararray)});
B = foreach A generate explode(a0);
dump B;
{code}
Message: Unimplemented at org.apache.pig.data.UnlimitedNullTuple.size(UnlimitedNullTuple.java:31)

If it is not the first projection, the script pass:
{code}
define explode HiveUDTF('explode');
A = load 'bag.txt' as (a0:{(b0:chararray)});
B = foreach A generate a0, explode(a0);
dump B;
{code}

Thanks [~nkollar] reporting it!",,,,,,,,,,,,,,,,,,,,27/Oct/16 15:53;nkollar;PIG-5048-1.patch;https://issues.apache.org/jira/secure/attachment/12835614/PIG-5048-1.patch,03/Nov/16 13:05;nkollar;PIG-5048-2.patch;https://issues.apache.org/jira/secure/attachment/12836788/PIG-5048-2.patch,08/Nov/16 13:20;nkollar;PIG-5048-3.patch;https://issues.apache.org/jira/secure/attachment/12837974/PIG-5048-3.patch,09/Nov/16 09:04;nkollar;PIG-5048-4.patch;https://issues.apache.org/jira/secure/attachment/12838137/PIG-5048-4.patch,25/Oct/16 08:01;nkollar;PIG-5048.patch;https://issues.apache.org/jira/secure/attachment/12835084/PIG-5048.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2016-10-24 19:25:49.41,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Nov 09 19:36:22 UTC 2016,,,,,,,0|i358tr:,9223372036854775807,,,,,,,,,,"24/Oct/16 19:25;nkollar;It seems that with this trivial modification in UnlimitedNullTuple
{code}
public int size() {
    return -1;
}
{code}
the UDAF gives the (almost) correct result, though I'm afraid this is not the best solution. I say almost, because I also noticed, that there is an extra empty tuple at the end (no idea why), but this is present even when the projection includes both a0 and explode(a0). Also, it would be nice to implement the other methods in this class, [~daijy] if you don't mind, can attach a patch.","24/Oct/16 21:22;daijy;Yes, it's on my plate.","25/Oct/16 07:30;nkollar;I just realized that I wanted to say I can attach a patch, but missed 'I'.","26/Oct/16 22:54;daijy;Thanks for the patch. Several comments:
1. I'd like to return Integer.MAX_VALUE in size(), as the name UnlimitedNullTuple suggests
2. I'd rather not change other method if not causing problem
3. Can you add test case?","27/Oct/16 15:26;nkollar;Do we need this UnlimitedNullTuple class at all? The only place where it is used is in POForEach:
{code}
                if (inp.returnStatus == POStatus.STATUS_EOP) {
                    if (parentPlan!=null && parentPlan.endOfAllInput && !endOfAllInputProcessed && endOfAllInputProcessing) {
                        // continue pull one more output
                        inp = new Result(POStatus.STATUS_OK, new UnlimitedNullTuple());
                    } else {
                        return inp;
                    }
                }
{code}
As far as I understood, this is used to allow UDF to produce a last record in close, does close here mean the cleanup phase of map tasks? What if we use RESULT_EMPTY from PhysicalOperator instead of UnlimitedNullTuple with OK status? The description of STATUS_NULL tells 'This is represented as 'null' with STATUS_OK', and it seems this is what we need instead of UnlimitedNullTuple. [~daijy] could you please review my second patch, and help me understand why UnlimitedNullTuple was required? I'd like to add a test case where the UDF produces a last record in close to ensure that my patch doesn't brake it, but I don't know when this happens.","28/Oct/16 22:14;daijy;The following script will not generate correct result:
{code}
define COUNT2 HiveUDTF('org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount2');
a = load 'studenttab10k' as (name, age, gpa);
b = foreach a generate flatten(COUNT2(name));
dump b;
{code}
Expected:
(10000)
(10000)

The UDTF invokes forward inside close, UnlimitedNullTuple plays the trick here.

This is a regression of Pig 0.16 (broken by PIG-4862). We need to fix it in 0.16.1 and add a test case for this type of UDTF.","31/Oct/16 18:41;knoguchi;{quote}
This is a regression of Pig 0.16 (broken by PIG-4862). We need to fix it in 0.16.1 and add a test case for this type of UDTF.
{quote}
Ouch. I wasn't aware that I introduced a regression.  Sorry about that.","03/Nov/16 13:21;nkollar;Attached a new version of my patch, it includes these changes:
- test cases for Hive UDFs UDTFs and UDAFs
- extracted the UnlimitedNullTuple to a constant in POForEach
- added Hive contrib package to the dependencies to be able to use GenericUDTFCount2 in the tests
- UnlimitedNullTuple's size method doesn't throw an exception, but returns Integer.MAX_VALUE
- In HiveUDTF class, the collector is reused in close and in normal process case, thus if init doesn't create a new bag, but just clears the current, the close() will erase the result of normal process.

One thing I still don't really like is that it seems that if close doesn't produce any new tuple(s) because close is not implemented at all in the UDF, an empty tuple is still appended to the end of the output. I don't know how to handle this case, since we don't know if Hive UDF actually did something in close, but the result was empty (in this case I think we have to append the empty result to the output), or close was not even implemented (here I think it doesn't make sense to append an empty tuple). [~daijy] what do you think? Could you please help with the review?","08/Nov/16 00:08;daijy;Thanks for the patch. A few comments:
* The only test case missing is UDTF with forward inside close (GenericUDTFCount2), all other test cases are in e2e tests HiveUDF
* hive-contrib can be a test dependency rather than a compile dependency
* Didn't get your last change, collector need to be cleared for every record, no matter normal process or endOfAllInput processing, clear is in favor as newDefaultBag is more costly

The empty tuple only append to the result if the UDF require endOfAllInput. Only HiveUDTF use this flag.","08/Nov/16 09:54;nkollar;Thanks Daniel for the comments. I added these tests to the Junit test suite, because it was easier to execute and verify, should I delete those which are already covered in e2e tests, or it is fine? I'll change hive-contrib to test dependency.
As for the collector, the UDTF exec was called twice: first from the normal process, then for close, and I noticed that in the test, the output for the process case was cleared in the close call. I just realized that this is probably a test issue with mock Storage I used in the tests, putNext doesn't make a deep copy from the tuples:
{code}
  @Override
  public void putNext(Tuple t) throws IOException {
      mockRecordWriter.dataBeingWritten.add(TF.newTuple(t.getAll()));
  }
{code}
Since in tests the tuple contains a bag, both the output bag of process and close will point to the same bag instance. Will figure out how to test this properly.","08/Nov/16 13:22;nkollar;Attached patch version 3, decided not to use mock Store in the tests and changed hive-contrib to test dependency.","08/Nov/16 20:59;daijy;Looks good. But let's remove duplicate tests, we are in the mission to cut down the total test runtime.","09/Nov/16 09:09;nkollar;Ok, deleted the UDF and UDAF tests, since those are covered in the e2e test suite. I think all UDTF tests are needed, there is one for the simple bag projection (this was case where the issue was discovered), one for two projections, and one for the GenericUDTFCount2 to verify UDTF close() is not broken.","09/Nov/16 18:02;daijy;+1.

Patch committed to both trunk and branch-0.16. Thanks Nandor!",09/Nov/16 19:36;nkollar;Thank you Daniel for the review and for helping me learn more about how HiveUDFs work!,,,,,,,,,,,,,
Skewed join with auto parallelism hangs when right input also has autoparallelism,PIG-5046,13013330,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,18/Oct/16 22:03,21/Jun/17 09:15,14/Mar/19 03:08,19/Jan/17 13:45,,,,,,0.16.1,0.17.0,,,,0,,,,,,,The problem is due to PartitionerDefinedVertexManager scheduling tasks before the source vertices are configured. ,,,,,,,,,,,,,,,,,,,,13/Jan/17 13:55;rohini;PIG-5046-1.patch;https://issues.apache.org/jira/secure/attachment/12847334/PIG-5046-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-01-18 23:48:20.476,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Jan 19 13:45:56 UTC 2017,,,,,,,0|i3527z:,9223372036854775807,,,,,,,,,,18/Jan/17 23:48;daijy;+1,19/Jan/17 13:45;rohini;Committed to both branch-0.16 and trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
"CSVExcelStorage Load: A Quoted Field with a Single Escaped Quote """""""" Becomes """" This should become "" instead.",PIG-5045,13012695,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,szita,Absolutesantaja,Absolutesantaja,16/Oct/16 16:25,26/Sep/18 17:50,14/Mar/19 03:08,22/Oct/16 00:28,0.16.0,,,,,0.17.0,,piggybank,,,0,,,,,,,"CSVExcelStorage incorrectly loads """""""" as """" instead of "" this bug sounds related to PIG-2557",CentOS 7 Hortonworks HDP 2.5 Release,,,,,,,,,,,,,,,,,,,16/Oct/16 16:26;Absolutesantaja;Excel Output.png;https://issues.apache.org/jira/secure/attachment/12833644/Excel+Output.png,17/Oct/16 13:21;szita;PIG-5045.patch;https://issues.apache.org/jira/secure/attachment/12833728/PIG-5045.patch,16/Oct/16 16:25;Absolutesantaja;pig output.txt;https://issues.apache.org/jira/secure/attachment/12833642/pig+output.txt,16/Oct/16 16:25;Absolutesantaja;test.csv;https://issues.apache.org/jira/secure/attachment/12833643/test.csv,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2016-10-17 13:22:40.347,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Oct 22 17:45:46 UTC 2016,,,,,,,0|i34yfb:,9223372036854775807,,,,,,,,,,16/Oct/16 16:25;Absolutesantaja;Example File and Results,16/Oct/16 16:26;Absolutesantaja;Screenshot from Excel,17/Oct/16 13:22;szita;It looks like CSVExcelStorage doesn't handle this corner case properly. I put a patch together to fix this: PIG-5045.patch,"22/Oct/16 00:28;daijy;+1. Patch committed to trunk. I don't feel it has to go to 16.1, let me know if you think otherwise.","22/Oct/16 17:45;szita;Thanks for the review, Daniel. Committing to 0.17 only is fine by me.",,,,,,,,,,,,,,,,,,,,,,,
Slowstart not applied in Tez with PARALLEL clause,PIG-5043,13012128,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,13/Oct/16 20:08,13/Sep/17 18:58,14/Mar/19 03:08,11/Jan/17 15:43,,,,,,0.16.1,0.17.0,,,,0,,,,,,,"We enable auto parallelism only if there is no custom partiioner, but in case of UnorderedPartitioned it always had RoundRobinPartitioner.

VertexManager plugin configuration is not overridden if there is no auto parallelism. So slow start settings are not applied.",,,,,,,,,,,,,,,,,,,,17/Oct/16 20:57;rohini;PIG-5043-1.patch;https://issues.apache.org/jira/secure/attachment/12833803/PIG-5043-1.patch,06/Jan/17 20:05;rohini;PIG-5043-2.patch;https://issues.apache.org/jira/secure/attachment/12846071/PIG-5043-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-10-20 20:08:51.088,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Jan 11 15:43:33 UTC 2017,,,,,,,0|i34uwv:,9223372036854775807,,,,,,,,,,"17/Oct/16 18:43;rohini;Changes done:
   - Config is set on  ShuffleVertexManager even if PARALLEL is specified so that slow start applies.
   - Reduced the amount of config set on ShuffleVertexManager/PigGraceShuffleVertexManager by only setting applicable settings. Reduced payload size is good for performance.

Also fixed two other issues
   - mapreduce.reduce.env was not being picked up and mapreduce.map.env was used
  - mapreduce.job.running.map.limit might be picked up instead of mapreduce.job.running.reduce.limit",20/Oct/16 20:08;daijy;Do you mind adding a test case for union (HashValuePartitioner) with auto parallelism?,06/Jan/17 20:05;rohini;Not able to get the test case working. Somehow auto parallelism is not being applied by Tez with unordered partitioned and it seems to be a Tez problem. I will look into that later and enable again in a separate jira. Taking that part out in this jira.,10/Jan/17 23:05;daijy;+1 for PIG-5043-2.patch.,11/Jan/17 15:43;rohini;Committed to branch-0.16 and trunk. Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,
RoundRobinPartitioner is not deterministic when order of input records change,PIG-5041,13011732,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,rohini,rohini,rohini,12/Oct/16 17:19,21/Jun/17 09:15,14/Mar/19 03:08,17/Oct/16 17:08,,,,,,0.16.1,0.17.0,,,,0,,,,,,,"Maps can be rerun due to shuffle fetch failures. Half of the reducers can end up successfully pulling partitions from first run of the map while other half could pull from the rerun after shuffle fetch failures. If the data is not partitioned by the Partitioner exactly the same way every time then it could lead to incorrect results (loss of records and duplicated records). 

There is a good probability of order of input records changing
    - With OrderedGroupedMergedKVInput (shuffle input), they keys are sorted but values can be in any order as the shuffle and merge depends on the order in which inputs are fetched. Anything involving FLATTEN can produce different order of output records.
    - With UnorderedKVInput, the records could be in any order depending on order of shuffle fetch. 

RoundRobinPartitioner can partition records differently everytime as order of input records change which is very bad. We need to get rid of RoundRobinPartitioner. Since the key is empty whenever we use  RoundRobinPartitioner we need to partitioning based on hashcode of values to produce consistent partitioning.

Partitioning based on hashcode is required for correctness, but disadvantage is that it
    - adds a lot of performance overhead with hashcode computation
    - with the random distribution due to hashcode (as opposed to batched round robin) input records sorted on some column could get distributed to different reducers and if union is followed by a store, the output can have bad compression.
",,,,,,,,,,,,,,,,,,,,13/Oct/16 17:58;rohini;PIG-5041-1.patch;https://issues.apache.org/jira/secure/attachment/12833167/PIG-5041-1.patch,13/Oct/16 22:31;rohini;PIG-5041-2.patch;https://issues.apache.org/jira/secure/attachment/12833226/PIG-5041-2.patch,17/Oct/16 19:12;rohini;PIG-5041-branch0.16.patch;https://issues.apache.org/jira/secure/attachment/12833789/PIG-5041-branch0.16.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-10-13 20:29:41.076,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Oct 17 17:08:26 UTC 2016,,,,,,,0|i34sgv:,9223372036854775807,,,,,,,,,,"12/Oct/16 17:33;rohini;Currently UnorderedPartitionedKVOutput+UnorderedKVInput+RoundRobinPartitioner only comes into play only when union optimizer is turned off or union optimizer could not be applied due to some condition.   We hit the issue with a script that had two levels of union followed by replicate join which does not have union optimization with vertex groups due to  PIG-3856.

{code}
a = load 'file:///tmp/input' as (x:int, y:chararray);
b = load 'file:///tmp/input' as (y:chararray, x:int);
c = union onschema a, b;"" +
d = load 'file:///tmp/input1' as (x:int, z:chararray);
e = join c by x, d by x using 'replicated';
f = load 'file:///tmp/input' as (y:chararray, x:int);
g = union onschema e, f;"" +
h = join g by y, d by y using 'replicated';
i = group h by x;
store i into 'file:///tmp/pigoutput';
{code}

Vertex processing c,e had unorderedinput and produced unorderedoutput with RoundRobinPartitioner. Rerun of one of the tasks in this vertex due to shuffle fetch failure caused incorrect results to be sent to vertex processing g,h. This is because first run processed in the order of a,b and second run processed in the order of b,a based on whichever shuffle file was fetched first. Map output of vertices processing a and b could also run into the non-deterministic partitioning issue if they had any operation that changed order of records but in this case since they read records from input file and output in same order without any intermediate processing those will not have issue on rerun even with RoundRobinPartitioner.  
","13/Oct/16 20:29;knoguchi;Rohini, you seem to have one debug statement in the patch.

As for using hashcode, I'm not sure if it'll work for Tuple with a bag.
Looking at DefaultAbstractBag, I see that hashcode is taken without sorting.
If same logic of non-determistic order of bag applies here, I'm afraid hashcode may differ by retries.

Outside of this jira, I'm worried with the implementation of DefaultAbstractBag where taking hashcode does NOT sort but compareTo() does.",13/Oct/16 22:31;rohini;Thanks [~knoguchi] for catching it. Have changed it to compute hashcode for each field of the tuple ignoring bags and only taking size into account for maps. Some implementation of bags iterate through whole bag to get size and so did not add the bag size in hashcode.,"14/Oct/16 00:31;daijy;Bag can also be a key. This is a problem more than HashValuePartitioner. I think that only happens when client/server running different jvm. But if that happens, I believe there will be more headache than non-deterministic partitioner.","14/Oct/16 03:17;rohini;bq. Bag can also be a key. 
  In distinct? That definitely is a possibility and I think we will have to try fix for that case. I don't see folks using it as a key in group by, order by or join. Will open a separate jira for that as that will involve changing hashcode implementation for bags. That is not a concern for this issue or partitioner implementation as we don't care where each records go as long as they go to same partitioner every time. Skipping bags and maps in hashcode will also provide speedup. In case of distinct, the identical bags have to go to same reducer and so hashcode of the bags are important in that case.

bq. I think that only happens when client/server running different jvm
   Mapreduce/Tez shuffle has no guarantee on the order of the values in a Key,List<Values> input to the reducer. It all depends on which map's shuffle output is fetched first during merge process. So the values in the bag can be of any order and it does not depend on the jvm.","14/Oct/16 04:22;daijy;I don't mean the fetch, I mean the order of bag.iterator","14/Oct/16 04:24;daijy;Anyway, the bag as the key is a different issue. Need to address separately. Maybe we can write a hashcode which is order independent.

I am fine with the HashValuePartitioner fix for this Jira. +1 for PIG-5041-2.patch.",17/Oct/16 17:08;rohini;Committed to branch-0.16 and trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,
Order by and CROSS partitioning is not deterministic due to usage of Random,PIG-5040,13011724,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,rohini,rohini,rohini,12/Oct/16 17:05,21/Jun/17 09:15,14/Mar/19 03:08,17/Oct/16 15:20,,,,,,0.16.1,0.17.0,,,,0,,,,,,,"Maps can be rerun due to shuffle fetch failures. Half of the reducers can end up successfully pulling partitions from first run of the map while other half could pull from the rerun after shuffle fetch failures. If the data is not partitioned by the Partitioner exactly the same way every time then it could lead to incorrect results (loss of records and duplicated records). Even though issue has existed for 8 years now with order by and affects mapreduce as well found this with Tez where the frequency of rerun due to shuffle fetch failures is high (Order by partitioner gets its data from a 1-1 edge, so there are no retries and shuffle fetch failures trigger a rerun immediately).",,,,,,,,,,,,,,,,,,,,12/Oct/16 17:10;rohini;PIG-5040-1-nowhitespacechanges.patch;https://issues.apache.org/jira/secure/attachment/12832935/PIG-5040-1-nowhitespacechanges.patch,12/Oct/16 17:10;rohini;PIG-5040-1.patch;https://issues.apache.org/jira/secure/attachment/12832936/PIG-5040-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-10-14 00:01:52.893,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Oct 17 15:20:38 UTC 2016,,,,,,,0|i34sf3:,9223372036854775807,,,,,,,,,,14/Oct/16 00:01;daijy;Is it possible to reuse PigConstants.TASK_INDEX?,"14/Oct/16 03:29;rohini;bq. Is it possible to reuse PigConstants.TASK_INDEX?
 Why? TASK_ID is also available in jobconf and provides better hashcode being a String than task index which is just a integer whose hashcode is same as the integer.  

Changes to LocalMapReduceSimulator.java were done for TestExampleGenerator which tests PigServer.getExamples().","14/Oct/16 04:41;daijy;Oh, I see, I confused because I saw you do set TASK_ID, but turns out it is in LocalMapReduceSimulator. But if we just use the existing value, I am fine. +1.",14/Oct/16 23:12;daijy;Also RandomSampleLoader in MR has the same issue.,"14/Oct/16 23:18;rohini;RandomSampleLoader and POReservoirSample even though they use new Random() and rerun will produce different samples, it is not a problem because the output is always sent to a single reducer (sample aggregator). If there were more than one reducer, then it is a problem.","14/Oct/16 23:32;daijy;You are right, single reducer should be fine.",17/Oct/16 15:20;rohini;Committed to branch-0.16 and trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,
TestTypeCheckingValidatorNewLP.TestTypeCheckingValidatorNewLP is failing,PIG-5039,13011324,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,nkollar,nkollar,nkollar,11/Oct/16 12:22,21/Jun/17 09:15,14/Mar/19 03:08,11/Oct/16 14:08,,,,,,0.17.0,,,,,0,,,,,,,"The test asserts for ""Cannot resolve load function to use for casting from bytearray to double at"" but the casting does chararray casting",,,,,,,,,,,,,,,,,,,,11/Oct/16 12:31;nkollar;PIG-5039.patch;https://issues.apache.org/jira/secure/attachment/12832656/PIG-5039.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-10-11 14:08:41.705,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Oct 11 14:17:46 UTC 2016,,,,,,,0|i34pyv:,9223372036854775807,,,,,,,,,,"11/Oct/16 14:08;knoguchi;Thanks Nandor for fixing my mistake! 
",11/Oct/16 14:17;nkollar;Thanks for the review! :),,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Limit_2 e2e test failed with sort check,PIG-5038,13010945,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,Konstantin_Harasov,Konstantin_Harasov,Konstantin_Harasov,10/Oct/16 08:10,21/Jun/17 09:15,14/Mar/19 03:08,12/Oct/16 21:47,,,,,,0.17.0,,,,,1,,,,,,,"{noformat}
error: Going to run sort check command: sort -cs -t     -k 1,3 ./out/pigtest/../..-1475241304-nightly.conf/Limit_2.out/out_original
/bin/sort: ./out/pigtest/../..-1475241304-nightly.conf/Limit_2.out/out_original:27: disorder:       18
Sort check failed
INFO: TestDriver::runTestGroup() at 706:Test Limit_2 FAILED at 1475241624
Ending test Limit_2 at 1475241624
{noformat}


The test failed because of difference in sorting in Pig {{(ORDER BY $0,$1,$2)}} and {{sort -t  $'\t'-k 1,3}} in bash.
The problem is that empty fields are sorted/processed differently 
in Pig using {{ORDER BY}} and bash using {{sort}}.

See example for file studentnulltab10k.

*Pig*:

{code:linenumbers=true}
		
		
		
		0.12
		1.04
		1.15
		1.25
		1.27
		1.31
		1.59
		1.61
		1.62
		1.76
		1.95
		2.09
		2.35
		2.66
		3.04
		3.23
		3.31
		3.39
		3.46
		3.54
		3.65
		3.75
		3.97
	18	
	18	0.41
{code}

*bash: sort -t  $'\t'-k 1,3*

{code:linenumbers=true}
		
		
		
		0.12
		1.04
		1.15
		1.25
		1.27
		1.31
		1.59
		1.61
		1.62
		1.76
	18	
	18	0.41
	18	0.54
	18	1.78
	18	2.46
	18	2.54
	19	0.07
	19	0.27
	19	0.39
	19	2.27
	19	2.50
	19	2.60
	19	2.89
	19	3.87
		1.95
{code}


*bash: sort -t  $'\t'-k 1,2*

{code:linenumbers=true}
		
		
		
		0.12
		1.04
		1.15
		1.25
		1.27
		1.31
		1.59
		1.61
		1.62
		1.76
		1.95
		2.09
		2.35
		2.66
		3.04
		3.23
		3.31
		3.39
		3.46
		3.54
		3.65
		3.75
		3.97
	18	
	18	0.41
{code}",,,,,,,,,,,,,,,,,,,,10/Oct/16 08:24;Konstantin_Harasov;PIG-5038.patch;https://issues.apache.org/jira/secure/attachment/12832431/PIG-5038.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-10-12 21:47:40.787,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Oct 12 21:47:40 UTC 2016,,,,,,,0|i34nn3:,9223372036854775807,,,,,,,,,,"12/Oct/16 21:47;rohini;+1. Committed to trunk.

Thanks for finding a solution/workaround for this. This was something in my todo list to look into for a long time. As per definition of the sort option -k1,3 should work fine and is what we should be doing as order by is done on three columns. The test passes fine in Mac with -k1,3 with sort command working as expected. Not sure why the Linux implementation was doing a wrong sort. For -k1,3 it actually gives result of -k1,1.

",,,,,,,,,,,,,,,,,,,,,,,,,,,
killJob API does not work in Tez,PIG-5035,13008776,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjffdu,zjffdu,zjffdu,30/Sep/16 00:56,21/Jun/17 09:15,14/Mar/19 03:08,30/Sep/16 16:26,0.16.0,,,,,0.16.1,0.17.0,,,,0,,,,,,,"In TezLauncher, appId is not compared correctly which cause the kill api doesn't work. ",,,,,,,,,,,,,,,,,,,,30/Sep/16 00:59;zjffdu;PIG_5035_1.patch;https://issues.apache.org/jira/secure/attachment/12831035/PIG_5035_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-09-30 16:26:44.059,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Sep 30 16:26:44 UTC 2016,,,,,,,0|i34a9z:,9223372036854775807,,,,,,,,,,30/Sep/16 01:00;zjffdu;\cc [~daijy] [~rohini],30/Sep/16 16:26;rohini;+1. Committed to branch-0.16 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,
"MultiQueryOptimizerTez creates bad plan with union, split and FRJoin",PIG-5033,13006303,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,tmwoodruff,tmwoodruff,20/Sep/16 20:39,21/Jun/17 09:15,14/Mar/19 03:08,31/Oct/16 15:57,0.16.0,,,,,0.16.1,0.17.0,tez,,,0,,,,,,,"This script produces incorrect results:

{code}
a = load 'file:///tmp/input1' as (x:int, y:int);
b = load 'file:///tmp/input2' as (x:int, y:int);
u = union a,b;
c = load 'file:///tmp/input3' as (x:int, y:int);
e = filter c by y > 3;
f = filter c by y < 2;
g = join u by x left, e by x using 'replicated';
h = join g by u::x left, f by x using 'replicated';
store h into 'file:///tmp/pigoutput';
{code}

Without the union, or with opt.multiquery=false, or with non-replicated joins, it works as expected.

",,,,,,,,,,,,,,,,,,,,17/Oct/16 15:43;rohini;PIG-5033-2.patch;https://issues.apache.org/jira/secure/attachment/12833753/PIG-5033-2.patch,20/Sep/16 21:30;tmwoodruff;PIG-5033.patch;https://issues.apache.org/jira/secure/attachment/12829456/PIG-5033.patch,20/Sep/16 21:29;tmwoodruff;input1;https://issues.apache.org/jira/secure/attachment/12829453/input1,20/Sep/16 21:29;tmwoodruff;input2;https://issues.apache.org/jira/secure/attachment/12829454/input2,20/Sep/16 21:29;tmwoodruff;input3;https://issues.apache.org/jira/secure/attachment/12829455/input3,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2016-10-03 21:05:36.521,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Oct 31 15:57:37 UTC 2016,,,,,,,0|i33v1j:,9223372036854775807,,,,,,,,,,"20/Sep/16 20:55;tmwoodruff;Here's the DAG plan. I've only started testing Tez today, so I'm not very familiar with how these should look, but the fact that both the joins use scope-61 seems a bit suspicious.

{code}
Tez vertex scope-55	->	Tez vertex scope-57,
Tez vertex scope-56	->	Tez vertex scope-57,
Tez vertex scope-61	->	Tez vertex scope-57,
Tez vertex scope-57

Tez vertex scope-55
# Plan on vertex
POValueOutputTez - scope-59	->	 [scope-57]
|
|---a: New For Each(false,false)[bag] - scope-7
    |   |
    |   Cast[int] - scope-2
    |   |
    |   |---Project[bytearray][0] - scope-1
    |   |
    |   Cast[int] - scope-5
    |   |
    |   |---Project[bytearray][1] - scope-4
    |
    |---a: Load(file:///tmp/input1:org.apache.pig.builtin.PigStorage) - scope-0
Tez vertex scope-56
# Plan on vertex
POValueOutputTez - scope-60	->	 [scope-57]
|
|---b: New For Each(false,false)[bag] - scope-15
    |   |
    |   Cast[int] - scope-10
    |   |
    |   |---Project[bytearray][0] - scope-9
    |   |
    |   Cast[int] - scope-13
    |   |
    |   |---Project[bytearray][1] - scope-12
    |
    |---b: Load(file:///tmp/input2:org.apache.pig.builtin.PigStorage) - scope-8
Tez vertex scope-61
# Plan on vertex
c: Split - scope-67
|   |
|   Local Rearrange[tuple]{int}(false) - scope-37	->	 scope-57
|   |   |
|   |   Project[int][0] - scope-33
|   |
|   |---e: Filter[bag] - scope-28
|       |   |
|       |   Greater Than[boolean] - scope-31
|       |   |
|       |   |---Project[int][1] - scope-29
|       |   |
|       |   |---Constant(3) - scope-30
|   |
|   Local Rearrange[tuple]{int}(false) - scope-51	->	 scope-57
|   |   |
|   |   Project[int][0] - scope-47
|   |
|   |---f: Filter[bag] - scope-42
|       |   |
|       |   Less Than[boolean] - scope-45
|       |   |
|       |   |---Project[int][1] - scope-43
|       |   |
|       |   |---Constant(2) - scope-44
|
|---c: New For Each(false,false)[bag] - scope-24
    |   |
    |   Cast[int] - scope-19
    |   |
    |   |---Project[bytearray][0] - scope-18
    |   |
    |   Cast[int] - scope-22
    |   |
    |   |---Project[bytearray][1] - scope-21
    |
    |---c: Load(file:///tmp/input3:org.apache.pig.builtin.PigStorage) - scope-17
Tez vertex scope-57
# Plan on vertex
h: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-54
|
|---h: FRJoin[tuple] - scope-48	<-	 scope-61
    |   |
    |   Project[int][0] - scope-46
    |   |
    |   Project[int][0] - scope-47
    |
    |---g: FRJoin[tuple] - scope-34	<-	 scope-61
        |   |
        |   Project[int][0] - scope-32
        |   |
        |   Project[int][0] - scope-33
        |
        |---POShuffledValueInputTez - scope-58	<-	 [scope-55, scope-56]
{code}","20/Sep/16 20:58;tmwoodruff;And here's the plan when I remove the union (which gives correct results). The difference seems to be that it leaves one of the join right-side inputs in a separate vertex.

{code}
Tez vertex scope-47	->	Tez vertex scope-46,Tez vertex scope-51,
Tez vertex scope-51	->	Tez vertex scope-46,
Tez vertex scope-46

Tez vertex scope-47
# Plan on vertex
c: Split - scope-53
|   |
|   Local Rearrange[tuple]{int}(false) - scope-28	->	 scope-46
|   |   |
|   |   Project[int][0] - scope-24
|   |
|   |---e: Filter[bag] - scope-19
|       |   |
|       |   Greater Than[boolean] - scope-22
|       |   |
|       |   |---Project[int][1] - scope-20
|       |   |
|       |   |---Constant(3) - scope-21
|   |
|   POValueOutputTez - scope-48	->	 [scope-51]
|
|---c: New For Each(false,false)[bag] - scope-15
    |   |
    |   Cast[int] - scope-10
    |   |
    |   |---Project[bytearray][0] - scope-9
    |   |
    |   Cast[int] - scope-13
    |   |
    |   |---Project[bytearray][1] - scope-12
    |
    |---c: Load(file:///tmp/input3:org.apache.pig.builtin.PigStorage) - scope-8
Tez vertex scope-51
# Plan on vertex
Local Rearrange[tuple]{int}(false) - scope-42	->	 scope-46
|   |
|   Project[int][0] - scope-38
|
|---f: Filter[bag] - scope-33
    |   |
    |   Less Than[boolean] - scope-36
    |   |
    |   |---Project[int][1] - scope-34
    |   |
    |   |---Constant(2) - scope-35
    |
    |---POValueInputTez - scope-52	<-	 scope-47
Tez vertex scope-46
# Plan on vertex
h: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-45
|
|---h: FRJoin[tuple] - scope-39	<-	 scope-51
    |   |
    |   Project[int][0] - scope-37
    |   |
    |   Project[int][0] - scope-38
    |
    |---g: FRJoin[tuple] - scope-25	<-	 scope-47
        |   |
        |   Project[int][0] - scope-23
        |   |
        |   Project[int][0] - scope-24
        |
        |---a: New For Each(false,false)[bag] - scope-7
            |   |
            |   Cast[int] - scope-2
            |   |
            |   |---Project[bytearray][0] - scope-1
            |   |
            |   Cast[int] - scope-5
            |   |
            |   |---Project[bytearray][1] - scope-4
            |
            |---a: Load(file:///tmp/input1:org.apache.pig.builtin.PigStorage) - scope-0
{code}",20/Sep/16 21:32;tmwoodruff;Here's an attempt at a fix. I have no confidence that this is the best (or even right) way to fix.,"03/Oct/16 21:05;daijy;scope-61 definitely cause the issue as we cannot have parallel edges between a pair of vertex. The patch prevent multiquery optimization once detecting replicated join. However, I think the proper fix should detect parallel edges. [~rohini], would you like to take a look?","17/Oct/16 15:43;rohini;[~tmwoodruff],
   Sorry. Had to take over the patch as it required little more work. Hope that is fine with you.

[~daijy],
Changes done
    - Inside of the union block, was not checking if the output from predecessor was to a scalar or replicate join. It always assumed the input went to POShuffleValueInputTez.
   - Changed TezCompilerUtil.isNonPackageInput to return true only for POFRJoinTez.  We only care about scalars and replicate join and it was returning true for POShuffleValueInputTez (union) as well.
","19/Oct/16 18:56;daijy;Didn't reason about the code change but the plan looks right, and test cases are proper. +1.",31/Oct/16 15:57;rohini;Committed to branch-0.16 and trunk. Thanks [~tmwoodruff] for tracking down the issue and the patch. Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,
Output record stats in Tez is wrong when there is split followed by union,PIG-5032,13004709,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,13/Sep/16 20:25,21/Jun/17 09:15,14/Mar/19 03:08,14/Sep/16 20:29,,,,,,0.16.1,0.17.0,,,,0,,,,,,,"    The logic to include unique stores, took into account record counters of only the first store if there are multi store counters.",,,,,,,,,,,,,,,,,,,,13/Sep/16 20:30;rohini;PIG-5032-1-nowhitespacechanges.patch;https://issues.apache.org/jira/secure/attachment/12828321/PIG-5032-1-nowhitespacechanges.patch,13/Sep/16 20:30;rohini;PIG-5032-1.patch;https://issues.apache.org/jira/secure/attachment/12828322/PIG-5032-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-09-14 19:54:41.373,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Sep 14 20:29:22 UTC 2016,,,,,,,0|i33l7z:,9223372036854775807,,,,,,,,,,"13/Sep/16 20:30;rohini;Found that TestCounters.java was not ported to Tez. Fixed that as well.

Patch also adds Multi store counters for all stores which makes it very easy for users to view in the DAG counters page and helps in debugging.",14/Sep/16 19:54;daijy;Now MULTI_STORE_COUNTER_GROUP is actually STORE_COUNTER_GROUP. Is that easy to change?,"14/Sep/16 19:58;rohini;Should be easy to change. But many people have existing validation jobs, that look at that counter group to validate their output from mapreduce days. So will prefer not changing it to keep counter names same as mapreduce.","14/Sep/16 20:25;rohini;Also MULTI_STORE_COUNTER_GROUP was meant to show records of different stores if there is more than one. So I guess naming still makes sense. Only thing is mapreduce showed them only if there was more than one store in a single mapreduce job. In Tez case, one job usually has more stores, but we were only showing them if one vertex had more stores. Now we show all the time.","14/Sep/16 20:26;daijy;Discussed offline, there is a minor behavior change: In Tez, we might get one extra MULTI_STORE_RECORD_COUNTER equals to OUTPUT_RECORDS if the vertex contains only one store. That won't affect user experience in general. +1.",14/Sep/16 20:29;rohini;Committed to branch-0.16 and trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,
Tez failing to compile when replicate join is done with a limit vertex on left,PIG-5031,13004706,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,13/Sep/16 20:06,21/Jun/17 09:15,14/Mar/19 03:08,13/Sep/16 21:33,,,,,,0.17.0,,tez,,,0,,,,,,,"{code}
a = load 'file:///tmp/input' as (x:int, y:int);
b = load 'file:///tmp/input' as (x:int, y:int);
c = limit a 1;
d = join c by x, b by x using 'replicated';
store a into 'file:///tmp/pigoutput/a';
store d into 'file:///tmp/pigoutput/d';
{code} 

This fails with
{noformat}
ERROR 2022: The current operator is closed. This is unexpected while compiling.

org.apache.pig.backend.hadoop.executionengine.tez.plan.TezCompilerException: ERROR 2034: Error compiling operator POFRJoin
        at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezCompiler.visitFRJoin(TezCompiler.java:756)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.visit(POFRJoin.java:213)
        at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezCompiler.compile(TezCompiler.java:409)
        at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezCompiler.compile(TezCompiler.java:388)
        at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezCompiler.compile(TezCompiler.java:264)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher.compile(TezLauncher.java:423)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher.launchPig(TezLauncher.java:171)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:290)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1474)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1459)
        at org.apache.pig.PigServer.execute(PigServer.java:1448)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:488)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:471)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:172)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:235)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:206)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
        at org.apache.pig.Main.run(Main.java:630)
        at org.apache.pig.Main.main(Main.java:176)
Caused by: org.apache.pig.backend.hadoop.executionengine.tez.plan.TezCompilerException: ERROR 2022: The current operator is closed. This is unexpected while compiling.
        at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezCompiler.visitFRJoin(TezCompiler.java:739)
        ... 18 more
{noformat}",,,,,,,,,,,,,,,,,,,,13/Sep/16 20:12;knoguchi;pig-5031-v01.patch;https://issues.apache.org/jira/secure/attachment/12828319/pig-5031-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-09-13 21:13:04.1,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Sep 13 21:33:44 UTC 2016,,,,,,,0|i33l7b:,9223372036854775807,,,,,,,,,,"13/Sep/16 20:12;knoguchi;It seems to be failing when POFRJoin incorrectly references {{POValueOutputTez}} instead of {{POValueInputTez}}.  Attaching a 1-line patch.

Thanks [~rohini] for showing me how to write  TezCompiler.java testcase with golden file.  Running unit/e2e tests. ",13/Sep/16 21:13;rohini;+1,"13/Sep/16 21:33;knoguchi;Thanks for the review Rohini! 

Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,
Error in TOKENIZE Example ,PIG-5022,13002377,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,icook,icook,icook,02/Sep/16 15:09,21/Jun/17 09:15,14/Mar/19 03:08,02/Sep/16 20:11,,,,,,0.17.0,,documentation,,,0,,,,,,,"The second TOKENIZE example in the Built In Functions documentation omits {{GENERATE}}. It reads:
{code}B = FOREACH A TOKENIZE (f1,'||');{code}
It should read:
{code}B = FOREACH A GENERATE TOKENIZE (f1,'||');{code}
It looks like this error was introduced when this second example was added in r0.10.0.",,,,,,,,,,,,,,,,,,,,02/Sep/16 19:26;icook;PIG-5022.patch;https://issues.apache.org/jira/secure/attachment/12826914/PIG-5022.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-09-02 19:12:16.208,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Sep 02 20:11:42 UTC 2016,,,,,,,0|i336u7:,9223372036854775807,,,,,,,,,,"02/Sep/16 19:12;knoguchi;Thanks Ian!

Do you mind creating a patch for this? 

(1) Downloading the source from http://svn.apache.org/repos/asf/pig/trunk/ or git://git.apache.org/pig.git 
(2) Update src/docs/src/documentation/content/xdocs/func.xml
(3) Save the patch by 'svn diff' or 'git diff --no-prefix' 
(4) Attach the patch from (3) here.  
",02/Sep/16 19:26;icook;Git patch attached,"02/Sep/16 20:11;knoguchi;Patch committed to trunk.
Thanks Ian for your first patch to Pig!",,,,,,,,,,,,,,,,,,,,,,,,,
Pig generates tons of warnings for udf with enabled warnings aggregation,PIG-5019,13002015,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,murshyd,murshyd,murshyd,01/Sep/16 11:08,21/Jun/17 09:15,14/Mar/19 03:08,12/Sep/16 17:27,0.14.0,,,,,0.16.1,0.17.0,internal-udfs,,,1,,,,,,,"For data set containing 9 lines the aggregated warning message is displayed 
{code}
2016-09-01 19:40:33,664 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Encountered Warning UDF_WARNING_1 6 time(s).
{code}

but in contained logs I see a separate log message ""Cannot
extract group for input"" for every not matching value
{code}
2016-09-01 19:40:28,115 INFO [main] org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map: Aliases being processed per job phase (AliasName[line,offset]): M
: b[10,4],b[-1,-1],extract_fields[17,17] C:  R: 
2016-09-01 19:40:28,122 WARN [main] org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger: org.apache.pig.builtin.REGEX_EXTRACT(UDF_WARNING_1): RegexExtrac
t : Cannot extract group for input /v1=1&v3=9
2016-09-01 19:40:28,124 WARN [main] org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger: org.apache.pig.builtin.REGEX_EXTRACT(UDF_WARNING_1): RegexExtrac
t : Cannot extract group for input /v2=3&v3=7
2016-09-01 19:40:28,124 WARN [main] org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger: org.apache.pig.builtin.REGEX_EXTRACT(UDF_WARNING_1): RegexExtract : Cannot extract group for input /v1=4&v3=6
2016-09-01 19:40:28,125 WARN [main] org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger: org.apache.pig.builtin.REGEX_EXTRACT(UDF_WARNING_1): RegexExtract : Cannot extract group for input /v2=5&v3=5
2016-09-01 19:40:28,125 WARN [main] org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger: org.apache.pig.builtin.REGEX_EXTRACT(UDF_WARNING_1): RegexExtract : Cannot extract group for input /v1=8&v3=2
2016-09-01 19:40:28,125 WARN [main] org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger: org.apache.pig.builtin.REGEX_EXTRACT(UDF_WARNING_1): RegexExtract : Cannot extract group for input /v3=9&v2=1
{code}

It does not log the warning messages in the task logs.

The patch for PIG-2207 was committed to
Pig 0.13+

In 0.12 we had a single counter for all UDF warnings, but in  0.13+ we have
separate counter and message for every unique warning log line. 

Two lines below are unique
/v2=3&v3=7
/v1=4&v3=6

That's why Pig print both of them to the console.

Printing a separate log message for every data line slows down the overall performance as well.",,,,,,,,,,,,,PIG-2207,,,,,,,01/Sep/16 11:11;murshyd;PIG-5019.patch;https://issues.apache.org/jira/secure/attachment/12826604/PIG-5019.patch,12/Sep/16 09:27;murshyd;PIG-5019_2.patch;https://issues.apache.org/jira/secure/attachment/12828004/PIG-5019_2.patch,12/Sep/16 17:26;rohini;PIG-5019_3.patch;https://issues.apache.org/jira/secure/attachment/12828077/PIG-5019_3.patch,01/Sep/16 11:09;murshyd;input_example.gz;https://issues.apache.org/jira/secure/attachment/12826602/input_example.gz,01/Sep/16 11:09;murshyd;test_pig14_udf .pig;https://issues.apache.org/jira/secure/attachment/12826603/test_pig14_udf+.pig,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2016-09-09 20:22:03.844,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Sep 12 17:26:23 UTC 2016,,,,,,,0|i334m7:,9223372036854775807,,,,,,,,,,"01/Sep/16 11:12;murshyd;[~aniket486] , could you please check the patch. It's related to commit for PIG-2207","09/Sep/16 20:22;rohini;[~murshyd],
  Thanks for reporting this. We recently encountered this as well.
   
{code}
// log at least once
                if (msgMap.get(o) == null || !msgMap.get(o).equals(displayMessage)) {
                    log.warn(displayMessage);
                    msgMap.put(o, displayMessage);
                }
{code}

Can you get rid of the whole block of logging once above instead of just logging classname and warning name? It is not useful as that can be anyways seen from the counters and the pig client log message.  Having to do a map.get() and equals check just for the message will affect performance when there are huge number of records. 

Documented behavior has always been to turn off aggregation with -w/-warning to see the actual warnings and was the case with older releases. So we are not losing anything by removing the code.  ","12/Sep/16 09:29;murshyd;[~rohini] , thank you for review.
Attached PIG-5019_2.patch ","12/Sep/16 17:26;rohini;+1. Committed to branch-0.16 and trunk. Thanks [~murshyd] for the patch. 

I removed the unused msgMap variable before checkin. Attached PIG-5019_3.patch which has that change.",,,,,,,,,,,,,,,,,,,,,,,,
streaming job with store clause stuck if the script fail,PIG-4976,13001859,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,31/Aug/16 23:57,21/Jun/17 09:15,14/Mar/19 03:08,04/Oct/16 17:54,,,,,,0.17.0,,impl,,,0,,,,,,,"When investigating PIG-4972, I also notice Pig job stuck when the perl script have syntax error. This happens if we have output clause in stream specification (means use a file as staging). The bug exist in both Tez and MR, and it is not a regression.

Here is an example:
{code}
define CMD `perl kk.pl` output('foo') ship('kk.pl');

A = load 'studenttab10k' as (name, age, gpa);
B = foreach A generate name;
C = stream B through CMD;
store C into 'ooo';
{code}

kk.pl is any perl script contain a syntax error.",,,,,,,,,,,,,,,,,,,,01/Sep/16 06:43;daijy;PIG-4976-1.patch;https://issues.apache.org/jira/secure/attachment/12826564/PIG-4976-1.patch,15/Sep/16 07:58;daijy;PIG-4976-2.patch;https://issues.apache.org/jira/secure/attachment/12828599/PIG-4976-2.patch,15/Sep/16 18:28;knoguchi;PIG-4976-3.patch;https://issues.apache.org/jira/secure/attachment/12828698/PIG-4976-3.patch,20/Sep/16 12:59;nkollar;PIG-4976-4.patch;https://issues.apache.org/jira/secure/attachment/12829383/PIG-4976-4.patch,20/Sep/16 21:48;knoguchi;PIG-4976-5-knoguchi.patch;https://issues.apache.org/jira/secure/attachment/12829457/PIG-4976-5-knoguchi.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2016-09-08 22:13:33.094,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 04 17:54:56 UTC 2016,,,,,,,0|i333nj:,9223372036854775807,,,,,,,,,,"01/Sep/16 06:41;daijy;The reason is, when the process exit, the input/output stream closed. Pig continue write to/flush the process input stream and raise exception. The exception is not handled properly so that the script process is killed, but Pig is still waiting for binaryInputQueue.

If the streaming command does not have store clause, Pig will use ProcessOutputThread to handle the process output, which can detect process exit and send signal to Pig's data pipeline. If there is store clause, Pig will not use ProcessOutputThread.","08/Sep/16 22:13;knoguchi;I don't think it's safe to ignore IOException from inputHandler.close(). 
I'm afraid of the possibility of task finishing with partial input for some cases.","09/Sep/16 14:33;knoguchi;{code:title=ExecutableManager.java}
324                         try {
325                             t = (Tuple) inp.result;
326                             inputHandler.putNext(t);
327                         } catch (IOException e) {
328                             // if input type is synchronous then it could
329                             // be related to the process terminating
330                             if(inputHandler.getInputType() == InputType.SYNCHRONOUS) {
331                                 LOG.warn(""Exception while trying to write to stream binary's input"", e);
332                                 // could be because the process
333                                 // died OR closed the input stream
334                                 // we will only call close() here and not
335                                 // worry about deducing whether the process died
336                                 // normally or abnormally - if there was any real
337                                 // issue the ProcessOutputThread should see
338                                 // a non zero exit code from the process and send
339                                 // a POStatus.STATUS_ERR back - what if we got
340                                 // an IOException because there was only an issue with
341                                 // writing to input of the binary - hmm..hope that means
342                                 // the process died abnormally!!
343                                 close();
344                                 return;
345                             } else {
346                                 // asynchronous case - then this is a real exception
347                                 LOG.error(""Exception while trying to write to stream binary's input"", e);
348                                 // send POStatus.STATUS_ERR to POStream to signal the error
349                                 // Generally the ProcessOutputThread would do this but now
350                                 // we should do it here since neither the process nor the
351                                 // ProcessOutputThread will ever be spawned
352                                 Result res = new Result(POStatus.STATUS_ERR,
353                                         ""Exception while trying to write to stream binary's input"" + e.getMessage());
354                                 sendOutput(poStream.getBinaryOutputQueue(), res);
355                                 throw e;
356                             }
357                         }
...
{code}

Can we wrap ""close()"" with try-block at line 343 and send {{sendOutput(poStream.getBinaryOutputQueue(), res);}} when catching any Throwable?

btw,

bq. The bug exist in both Tez and MR, and it is not a regression.

This bug does exist on both Tez and MR, but I believe it only hangs longer than default 10 mins timeout on Tez. 
[~rohini], [~jeagles] taught me that this is because tez.task.progress.stuck.interval-ms is not being set in Pig and we need TEZ-3317 fixed first for that.","15/Sep/16 07:58;daijy;Yes, that's doable. At least we can signal error to the pipeline.","15/Sep/16 18:28;knoguchi;Thanks Daniel, but test still hung.  

Looking back, I see that I was wrong.
My test reliably fail immediately only when the input was huge, but for small input, it threw the exception inside 

{code:title=ExecutableManager.java}
308                     if (inp != null && inp.returnStatus == POStatus.STATUS_EOP) {
309                         // signal cleanup in ExecutableManager
310                         close();  //***
311                         return;
312                     }
{code}
and the test still hung.

At this point, wondering if we can signal an error at the outside catch block irrespective of how the ProcessOutputThread may or may not handle the error... 

Would something like this work?  Attaching a slightly different version on where to call sendOutput. {{PIG-4976-3.patch}}
",16/Sep/16 00:21;daijy;TestStreamingLocal pass for me with PIG-4976-2.patch. You are using another test case?,"16/Sep/16 12:03;nkollar;When I applied PIG-4976-2.patch I had the same problem as [~knoguchi], the test case hung. I noticed, that the test passed when I apply first PIG-4976-1.patch then PIG-4976-3.patch. I also noticed that when I apply just the first patch, the test hung, but not because of syntax error in Perl script, but because of NullPointerException while trying to close the OutputHandler:
Exception in thread ""Thread-31"" java.lang.NullPointerException
	at org.apache.pig.impl.streaming.OutputHandler.close(OutputHandler.java:178)
	at org.apache.pig.impl.streaming.ExecutableManager.killProcess(ExecutableManager.java:188)
	at org.apache.pig.impl.streaming.ExecutableManager.access$200(ExecutableManager.java:52)
	at org.apache.pig.impl.streaming.ExecutableManager$ProcessInputThread.run(ExecutableManager.java:372)
2016-09-16 13:53:03,784 ERROR [Thread-31] streaming.ExecutableManager (ExecutableManager.java:run(369)) - Error while reading from POStream and passing it to the streaming process
java.io.FileNotFoundException: foo (No such file or directory)

It looks like 'foo' should exist before executing the test? For me, it looks like we have two similar issues here (two test case): syntax error in script and writing to a file that doesn't exist.","19/Sep/16 21:20;knoguchi;bq. TestStreamingLocal pass for me with PIG-4976-2.patch. You are using another test case?

Sorry for my late response.  I was using TestStreamingLocal.testNegativeScriptSyntaxError with PIG-4976-2.patch. 
As [~nkollar] pointed out, maybe the difference is you had ""foo"" file present from previous testing. (Thanks Nandor!) 

bq. It looks like 'foo' should exist before executing the test?

It's the opposite.  'foo' should _not_ exist since we're testing a streaming script that is supposed to write to 'foo' but fails with syntax error.","20/Sep/16 12:58;nkollar;Ok, so in this case it seems that the file is not getting created in FileOutputHandler:
        File file = new File(this.fileName);
        BufferedPositionedInputStream fileInStream = 
            new BufferedPositionedInputStream(new FileInputStream(file)); 
This won't create the file, I added a test to show this problem, created a patch based on PIG-4976-3.patch (PIG-4976-4.patch) with an additional test for this case and changes FileOutputHandler to create the output file. I'm not sure what we should do if the output file exist, should we just append to it, or we should throw an exception instead?","20/Sep/16 21:19;knoguchi;bq. Ok, so in this case it seems that the file is not getting created in FileOutputHandler:

File not created is expected.  Streaming process failed so I'd rather keep it that way instead of risking getting a false positive by having an empty output file.


Trying to sort out the issue.  So far, I've only tested on my macbook.  

Code path for the error I saw were two types.
(1) With small input (including the test from Daniel's patch), 
It fails in close() at  
{code}
308                     if (inp != null && inp.returnStatus == POStatus.STATUS_EOP) {
309                         // signal cleanup in ExecutableManager
310                         close();
311                         return;
312                     }
{code}
which then calls 
-> ExecutableManager.close():{{114         inputHandler.close(process);}} somehow pass then
->ExecutableManager.close():{{160             outputHandler.bindTo("""", null, 0, -1);}} fails with 
{noformat}
2016-09-20 16:59:42,425 [Thread-30] ERROR org.apache.pig.impl.streaming.ExecutableManager - Error while reading from POStream and passing it to thes
java.io.FileNotFoundException: foo (No such file or directory)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(FileInputStream.java:195)
        at java.io.FileInputStream.<init>(FileInputStream.java:138)
        at org.apache.pig.impl.streaming.FileOutputHandler.bindTo(FileOutputHandler.java:57)
        at org.apache.pig.impl.streaming.ExecutableManager.close(ExecutableManager.java:160)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.close(HadoopExecutableManager.java:131)
        at org.apache.pig.impl.streaming.ExecutableManager$ProcessInputThread.run(ExecutableManager.java:310)
{noformat}
then outer catch block call killprocess and also fails with 
{noformat}
Exception in thread ""Thread-30"" java.lang.NullPointerException
        at org.apache.pig.impl.streaming.OutputHandler.close(OutputHandler.java:178)
        at org.apache.pig.impl.streaming.ExecutableManager.killProcess(ExecutableManager.java:184)
        at org.apache.pig.impl.streaming.ExecutableManager.access$200(ExecutableManager.java:52)
        at org.apache.pig.impl.streaming.ExecutableManager$ProcessInputThread.run(ExecutableManager.java:368)
{noformat}

(2) When input is large, {{326                             inputHandler.putNext(t);}} threw Exception
{code} 
324                         try {
325                             t = (Tuple) inp.result;
326                             inputHandler.putNext(t);
327                         } catch (IOException e) {
328                             // if input type is synchronous then it could
329                             // be related to the process terminating
330                             if(inputHandler.getInputType() == InputType.SYNCHRONOUS) {
331                                 LOG.warn(""Exception while trying to write to stream binary's input"", e);
...
343                                 close();
344                                 return;
{code}

{noformat}
2016-09-20 17:07:12,362 [Thread-30] WARN  org.apache.pig.impl.streaming.ExecutableManager - Exception while trying to write to stream binary's input
java.io.IOException: Stream closed
        at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:433)
        at java.io.OutputStream.write(OutputStream.java:116)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
        at java.io.DataOutputStream.write(DataOutputStream.java:107)
        at org.apache.pig.impl.streaming.InputHandler.putNext(InputHandler.java:72)
        at org.apache.pig.impl.streaming.ExecutableManager$ProcessInputThread.run(ExecutableManager.java:326)
{noformat} 

then {{343                                 close();}} call inside the catch block failed at {{:{{114         inputHandler.close(process);}}}} with 

{noformat}
2016-09-20 17:07:12,365 [Thread-30] ERROR org.apache.pig.impl.streaming.ExecutableManager - Error while reading from POStream and passing it to thes
java.io.IOException: Stream closed
        at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:433)
        at java.io.OutputStream.write(OutputStream.java:116)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
        at java.io.DataOutputStream.flush(DataOutputStream.java:123)
        at org.apache.pig.impl.streaming.InputHandler.close(InputHandler.java:93)
        at org.apache.pig.impl.streaming.DefaultInputHandler.close(DefaultInputHandler.java:50)
        at org.apache.pig.impl.streaming.ExecutableManager.close(ExecutableManager.java:114)
        at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.close(HadoopExecutableManager.java:131)
        at org.apache.pig.impl.streaming.ExecutableManager$ProcessInputThread.run(ExecutableManager.java:343)
{noformat}

then you would also see killprocess fails with NullPointerException just like in (1). 


Daniel's {{PIG-4976-1.patch}} and {{PIG-4976-2.patch}} both handles issue (2) at different level.  I am not sure why the provided testcase is hitting (1) in my environment but (2) in Daniel's environment. ","20/Sep/16 21:48;knoguchi;(I) My previous patch {{PIG-4976-3.patch}} handled both cases by relying on the exceptions thrown inside the close().  However, as Nandor pointed out, exception from (1) 
""Error while reading from POStream and passing it to thes
java.io.FileNotFoundException: foo (No such file or directory)""
was misleading in that it's checking the output file ""foo"" AFTER it saw exitcode of 255.  We probably shouldn't be checking the output file in the first place.

Here, I've added a line to signal error when exitcode != 0 inside close() and skipped the checking of output file.
I still need the outer catch block to signal error for the case in (2). 

(II) As for NullPointerException from killProcess, we probably should get rid of it. 

Added extra null check for OutputHandler.java and also made killprocess to ignore any Exceptions for inputhandler.close() and outputhandler.close(). 

(III) Modified the test case to handle small and big inputs.  In my environment, they went through the two different paths, (1) and (2). 

For (1), it'll show 
{noformat}
2016-09-20 17:45:38,630 WARN  [Thread-263] mapred.LocalJobRunner (LocalJobRunner.java:run(560)) - job_local2000154943_0012
java.lang.Exception: org.apache.pig.backend.executionengine.ExecException: ERROR 2055: Received Error while processing the map plan: 'perl script212
937190666196237pl (stdin-org.apache.pig.builtin.PigStreaming/foo-org.apache.pig.builtin.PigStreaming())' failed with exit status: 255
    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
{noformat}
whereas for (2), it'll only show
{noformat}
2016-09-20 17:45:39,304 WARN  [Thread-320] streaming.ExecutableManager (ExecutableManager.java:run(340)) - Exception while trying to write to stream
 binary's input
java.io.IOException: Stream closed
    at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:433)
    at java.io.OutputStream.write(OutputStream.java:116)
    at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
    at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
    at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
    at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
    at java.io.DataOutputStream.write(DataOutputStream.java:107)
    at org.apache.pig.impl.streaming.InputHandler.putNext(InputHandler.java:72)
    at org.apache.pig.impl.streaming.ExecutableManager$ProcessInputThread.run(ExecutableManager.java:335)
2016-09-20 17:45:39,305 ERROR [Thread-320] streaming.ExecutableManager (ExecutableManager.java:run(369)) - Error while reading from POStream and pas
sing it to the streaming process:
java.io.IOException: Stream closed
    at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:433)
    at java.io.OutputStream.write(OutputStream.java:116)
    at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
    at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
    at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
    at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
    at java.io.DataOutputStream.flush(DataOutputStream.java:123)
    at org.apache.pig.impl.streaming.InputHandler.close(InputHandler.java:93)
    at org.apache.pig.impl.streaming.DefaultInputHandler.close(DefaultInputHandler.java:50)
    at org.apache.pig.impl.streaming.ExecutableManager.close(ExecutableManager.java:114)
    at org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager.close(HadoopExecutableManager.java:131)
    at org.apache.pig.impl.streaming.ExecutableManager$ProcessInputThread.run(ExecutableManager.java:352)
2016-09-20 17:45:39,306 INFO  [Thread-293] mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - map task executor complete.
2016-09-20 17:45:39,309 WARN  [Thread-293] mapred.LocalJobRunner (LocalJobRunner.java:run(560)) - job_local331634717_0013
java.lang.Exception: org.apache.pig.backend.executionengine.ExecException: ERROR 2055: Received Error while processing the map plan: Error while rea
ding from POStream and passing it to the streaming process:Stream closed
    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
{noformat}

In both cases, stderr will show the syntax errors.   
{panel}
Can't locate object method ""syntax"" via package ""error"" (perhaps you forgot to load ""error""?) at script4635494469399418013pl line 2.
{panel}","21/Sep/16 11:20;nkollar;""File not created is expected."" [~knoguchi], does this mean, that the file should already exist? If I'd like to use '/tmp/foo' as an output (define CMD `perl kk.pl` output('/tmp/foo') ship('kk.pl')), then I have to create the file before I execute the Pig script? Otherwise, it will fail I guess.","21/Sep/16 14:29;knoguchi;Nandor, granted I've never used this feature myself, but from 
http://pig.apache.org/docs/r0.16.0/basic.html#define-udfs
I'm guessing {{define CMD `perl kk.pl` output('foo')}} means the streaming command (here, it would be perl) would write all its output to file 'foo'.  Then PigStreaming would 'deserialize' them into Tuple form and pass them to next call.

It is responsibility of the streaming process to create the file.  I don't want the framework creating an empty output file and risk getting false positives.  (From the current code, it should still fail but why risk it.)","22/Sep/16 13:07;nkollar;Ok, thanks for the explanation, I agree, no need to create a new file in Pig like I would have done in patch #3. The only think I'm wondering if we need two separate test cases: one for non-existing file and one for existing file but syntactically incorrect script, or the one we have is enough. What do you think?","22/Sep/16 17:09;knoguchi;bq.  if we need two separate test cases: one for non-existing file and one for existing file but syntactically incorrect script, or the one we have is enough

When the script has syntax error, there won't be file created so I don't see the need for 
""existing file but syntactically incorrect script"" case.

Note that last patch updated the test case with two input sizes which did test both (1) and (2) that I mentioned above.  (at least on my mac)

We can create a new jira and discuss the meaning of non-existent or empty output file.
I haven't looked if we already have test cases for this. 
To me, process successful but no-output file should fail the task whereas process successful with empty output file should proceed with empty input.
","27/Sep/16 10:46;nkollar;Ok, PIG-4976-5-knoguchi.patch LGTM, no test is hanging in TestStreamingLocal on my mac.","03/Oct/16 20:09;daijy;I do have a ""foo"" file in my test directory, that's why I hit (2) not (1) in my test. Patch looks good to me, and test include both (1) and (2). +1.","04/Oct/16 17:54;knoguchi;Committed to trunk.

Leaving the assignment to Daniel since I didn't understand the issue until I saw Daniel's initial patch and explanation.

Thanks Daniel!

Also, thank you [~nkollar] for all your feedback!  
I understood the issue better by answering your questions.",,,,,,,,,,
"Map schema shows ""Type: null Uid: null"" in explain",PIG-4975,13001835,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,knoguchi,knoguchi,knoguchi,31/Aug/16 22:04,21/Jun/17 09:15,14/Mar/19 03:08,01/Sep/16 17:03,,,,,,0.17.0,,,,,0,,,,,,,"From PIG-4974, saw ""explain"" somehow showing 

{noformat}
(Name: Map Type: null Uid: null Key: a)
{noformat}

It's not affecting execution but we probably want to fix this.",,,,,,,,,,,,,,,,,,,,31/Aug/16 22:11;knoguchi;pig-4975-v01.patch;https://issues.apache.org/jira/secure/attachment/12826508/pig-4975-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-09-01 07:37:21.906,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Sep 01 17:03:11 UTC 2016,,,,,,,0|i333i7:,9223372036854775807,,,,,,,,,,"31/Aug/16 22:11;knoguchi;A simple patch that keeps the fieldSchema.
I think this is safe but running full tests.
",01/Sep/16 07:37;daijy;+1 pending tests.,"01/Sep/16 17:03;knoguchi;Thanks for the review Daniel.
Tests passed.  Committed to trunk.
",,,,,,,,,,,,,,,,,,,,,,,,,
Bigdecimal divison fails,PIG-4973,13001391,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,szita,szita,30/Aug/16 16:50,21/Jun/17 09:15,14/Mar/19 03:08,01/Sep/16 07:54,,,,,,0.17.0,,impl,,,0,,,,,,,"Division of BigDecimals doesn't work because we're not passing scale and rounding information in divide() method. In cases like 10/3 we'll get ArithmeticException:

Pig script:
grunt> A = LOAD 'decimaltest/f1' USING PigStorage(',') AS (id,col1:bigdecimal,col2:bigdecimal);
grunt> B = foreach A generate col1, col2, col1/col2;
grunt> dump B

Input file content:
1,10.0,3
2,51651351.13153143512,10.00
3,252525.252525,123.456

Output with bigdecimal type in the schema:

java.lang.Exception: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [Divide (Name: Divide[bigdecimal] - scope-34 Operator Key: scope-34) children: [[POProject (Name: Project[bigdecimal][0] - scope-32 Operator Key: scope-32) children: null at []], [POProject (Name: Project[bigdecimal][1] - scope-33 Operator Key: scope-33) children: null at []]] at []]: java.lang.ArithmeticException: Non-terminating decimal expansion; no exact representable decimal result.
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [Divide (Name: Divide[bigdecimal] - scope-34 Operator Key: scope-34) children: [[POProject (Name: Project[bigdecimal][0] - scope-32 Operator Key: scope-32) children: null at []], [POProject (Name: Project[bigdecimal][1] - scope-33 Operator Key: scope-33) children: null at []]] at []]: java.lang.ArithmeticException: Non-terminating decimal expansion; no exact representable decimal result.
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:364)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:404)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:321)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:280)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:275)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:65)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArithmeticException: Non-terminating decimal expansion; no exact representable decimal result.
	at java.math.BigDecimal.divide(BigDecimal.java:1616)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Divide.divide(Divide.java:75)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Divide.genericGetNext(Divide.java:133)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Divide.getNextBigDecimal(Divide.java:166)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:353)
	... 14 more

Output with double in the schema:
(10.0,3.0,3.3333333333333335)
(5.165135113153143E7,10.0,5165135.113153143)
(252525.252525,123.456,2045.467636445373)
",,,,,,,,,,,,,,,,,,,,30/Aug/16 16:54;szita;PIG-4973.patch;https://issues.apache.org/jira/secure/attachment/12826207/PIG-4973.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-09-01 07:54:50.319,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Sep 01 18:25:41 UTC 2016,,,,,,,0|i330rb:,9223372036854775807,,,,,,,,,,31/Aug/16 20:29;szita;[~daijy] can you please take a look?,"01/Sep/16 07:54;daijy;+1, that's useful fix. Patch committed to trunk. Thanks Adam!",01/Sep/16 18:25;szita;thanks for the review Daniel,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingIO_1 fail on perl 5.22,PIG-4972,13001150,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,29/Aug/16 23:45,21/Jun/17 09:15,14/Mar/19 03:08,30/Aug/16 20:23,,,,,,0.17.0,,e2e harness,,,0,,,,,,,"Saw StreamingIO_1 on particular perl version due to a warning in PigStreaming.pl. You can see the warning in any version of perl using ""perl -w"":
{code}
defined(%hash) is deprecated at streaming/PigStreaming.pl line 76.
	(Maybe you should just omit the defined()?)
{code}

In some particular version of perl, warning check is mandatory and the perl script just fail.",,,,,,,,,,,,,,,,,,,,29/Aug/16 23:47;daijy;PIG-4972-1.patch;https://issues.apache.org/jira/secure/attachment/12826107/PIG-4972-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-08-30 20:15:03.507,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Aug 30 20:23:16 UTC 2016,,,,,,,0|i32z9z:,9223372036854775807,,,,,,,,,,30/Aug/16 20:15;thejas;+1,"30/Aug/16 20:18;knoguchi;+1 (Just in case, tested the patch with e2e and they ran fine.)","30/Aug/16 20:23;daijy;Patch committed to trunk. Thanks for review Thejas, Koji!",,,,,,,,,,,,,,,,,,,,,,,,,
Fix Pig compatibility with Hive 2.1.0,PIG-4966,12995349,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,zyork,zyork,zyork,05/Aug/16 21:21,21/Jun/17 09:15,14/Mar/19 03:08,06/Aug/16 06:00,0.15.0,,,,,0.17.0,,build,,,0,,,,,,,"Hive refactored their RecordReader code in Hive 2.1. This patch maintains compatibility with Hive 2.1 and previous versions.
",,,,,,,,,,,,,,,,,,,,05/Aug/16 21:25;zyork;PIG-4966.patch;https://issues.apache.org/jira/secure/attachment/12822378/PIG-4966.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-08-06 06:00:17.353,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Aug 08 06:38:08 UTC 2016,,,,,,,0|i31zi7:,9223372036854775807,,,,,,,,,,"06/Aug/16 06:00;daijy;Looks good for me. Patch committed to trunk. Thanks Zach!

Note this patch is only meaningful if we bump up hive dependency to 2.1. And when doing that, we will need PIG-4764 as well.","07/Aug/16 22:25;zyork;Daniel, what is the status of PIG-4764? Hive 2.0 has been out for a while now and it would be great to get Pig to work with the latest versions of Hive.",08/Aug/16 06:38;daijy;There are incompatible changes between Hive 2 and Hive 1. Currently Hive 2 is experimental and Hive 1 is the workhorse. Thus Hive 1 is still the main target for Pig and those who want to use Hive 2 may apply PIG-4764. I am hesitate to add another shims layer for Hive as we already have shims for hadoop and hbase.,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor test/perf/pigmix/bin/runpigmix.pl to delete the output of single test case if we enable cleanup_after_test,PIG-4965,12995097,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,05/Aug/16 04:52,21/Jun/17 09:15,14/Mar/19 03:08,08/Aug/16 17:38,,,,,,0.17.0,spark-branch,,,,0,,,,,,,"in  test/perf/pigmix/bin/runpigmix.pl#cleanup
{code}
sub cleanup {
    my $suffix = shift;
    my $cmd;
    $cmd = ""$pigbin -e rmf L"".$suffix.""out"";
    print STDERR `$cmd 2>&1`;
    $cmd = ""$pigbin -e rmf highest_value_page_per_user"";
    print STDERR `$cmd 2>&1`;
    $cmd = ""$pigbin -e rmf total_timespent_per_term"";
    print STDERR `$cmd 2>&1`;
    $cmd = ""$pigbin -e rmf queries_per_action"";
    print STDERR `$cmd 2>&1`;
    $cmd = ""$pigbin -e rmf tmp"";
    print STDERR `$cmd 2>&1`;
    if ($cleanup_after_test) {
        $cmd = ""$hadoopbin fs -rmr $pigmixoutput"";
        print STDERR `$cmd 2>&1`;
    }   
}
{code}

In PIG-200, it deletes the output of the single script. But the output of a single script is not [L.$suffix.out|https://github.com/apache/pig/blob/spark/test/perf/pigmix/bin/runpigmix.pl#L113] so the output is not deleted. The correct output of a single script is PIGMIX_OUTPUT/L.$suffix.out. In PIG-4753(Pigmix should have option to delete outputs after completing the tests), we can delete the output if we enable cleanup_after_test.  So i think we can refactor the code as following:
{code}
sub cleanup {
    if ($cleanup_after_test) {
        my $suffix = shift;
        my $cmd;
        $cmd = ""$pigbin -e rmf $pigmixoutput/L"".$suffix.""out"";
        print STDERR `$cmd 2>&1`;
        $cmd = ""$pigbin -e rmf $pigmixoutput/highest_value_page_per_user"";
        print STDERR `$cmd 2>&1`;
        $cmd = ""$pigbin -e rmf $pigmixoutput/total_timespent_per_term"";
        print STDERR `$cmd 2>&1`;
        $cmd = ""$pigbin -e rmf $pigmixoutput/queries_per_action"";
        print STDERR `$cmd 2>&1`;
        $cmd = ""$pigbin -e rmf tmp"";
        print STDERR `$cmd 2>&1`;
    }
}
{code}
",,,,,,,,,,,,,,,,,,,,08/Aug/16 07:40;kellyzly;PIG-4965.patch;https://issues.apache.org/jira/secure/attachment/12822523/PIG-4965.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-08-05 22:19:52.344,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Aug 08 17:38:17 UTC 2016,,,,,,,0|i31xy7:,9223372036854775807,,,,,,,,,,"05/Aug/16 05:16;kellyzly;[~daijy] and [~rohini]:  As you are familiar with pigmix benchmark, can you help review this?",05/Aug/16 22:19;daijy;Remove a file does not exist clearly a bug. I think you can just add $pigmixoutput prefix to each file intend to remove.,"08/Aug/16 04:51;kellyzly;[~daijy]:  In PIG-200, we really want to delete output $pigmixoutput/L.$suffix.out?  If it is, i think we need not add cleanup_after_test option(PIG-4753: Pigmix should have option to delete outputs after completing the tests). If we still want users to view the output, we need retain output($pigmixoutput/L.$suffix.out),  so i think we can delete the output $pigmixoutput/L.$suffix.out only when ""cleanup_after_test"" is enabled like following, if you think the following code is ok, i will upload the patch.
{code}
sub cleanup {
    if ($cleanup_after_test) {
        my $suffix = shift;
        my $cmd;
        $cmd = ""$pigbin -e rmf $pigmixoutput/L"".$suffix.""out"";
        print STDERR `$cmd 2>&1`;
        $cmd = ""$pigbin -e rmf $pigmixoutput/highest_value_page_per_user"";
        print STDERR `$cmd 2>&1`;
        $cmd = ""$pigbin -e rmf $pigmixoutput/total_timespent_per_term"";
        print STDERR `$cmd 2>&1`;
        $cmd = ""$pigbin -e rmf $pigmixoutput/queries_per_action"";
        print STDERR `$cmd 2>&1`;
        $cmd = ""$pigbin -e rmf tmp"";
        print STDERR `$cmd 2>&1`;
    }
}
{code}
",08/Aug/16 06:31;daijy;The suggested code makes sense to me. Remove a non-exist file is certainly a bug and this might be overlooked by PIG-4753. ,"08/Aug/16 07:39;kellyzly;[~daijy]:  After investigating the code, i found that in pig mode, the output is $pigmixoutput/pig/L.$suffix.out while in mr mode the output is $pigmixoutput/mapreduce/L.$suffix.out.   Upload PIG-4965.patch, changes:
1. add $pigmixoutput prefix to each file intend to remove in runpigmix.pl#cleanup when enable ""cleanup_after_test"" option.",08/Aug/16 17:38;daijy;Patch committed to both trunk and spark-branch. Thanks Liyun!,,,,,,,,,,,,,,,,,,,,,,
CROSS followed by LIMIT inside nested foreach drop data from result,PIG-4961,12992268,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,vaifer,vaifer,25/Jul/16 13:10,21/Jun/17 09:15,14/Mar/19 03:08,26/Jul/16 12:30,0.10.0,,,,,0.16.1,0.17.0,,,,0,,,,,,,"After changing in https://issues.apache.org/jira/browse/PIG-4259 , LIMIT operation can drop some data from result.
",,,,,,,,,,,,,,,,,,,,26/Jul/16 04:09;rohini;PIG-4961-1.patch;https://issues.apache.org/jira/secure/attachment/12820089/PIG-4961-1.patch,25/Jul/16 13:12;vaifer;input1.zip;https://issues.apache.org/jira/secure/attachment/12819921/input1.zip,25/Jul/16 13:12;vaifer;script.pig;https://issues.apache.org/jira/secure/attachment/12819922/script.pig,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-07-25 19:31:11.509,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jul 26 12:30:15 UTC 2016,,,,,,,0|i31ghr:,9223372036854775807,,,,,,,,,,"25/Jul/16 13:12;vaifer;Attached script and sample data for reproduce it.

Expected result:
(1,1,1,3.1,1,3.1)
(2,1,2,3.7,2,3.7)
(3,1,3,3.2,3,3.2)

Actual result:
(1,1,1,3.1,1,3.1)
(3,1,3,3.2,3,3.2)","25/Jul/16 14:13;vaifer;If revert changes from src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLimit.java (that was added in PIG-4259 ), expected and actual results will be the same","25/Jul/16 19:31;daijy;Yes, I can reproduce. [~rohini], I don't feel the POLimit change is needed, can you check?",25/Jul/16 21:27;rohini;  It was just an optimization. Will check. Need to determine what cases are impacted.,"26/Jul/16 03:06;rohini;It is not a problem with the limit change. There is some issue with POCross not resetting input. If I just add another bag to input, I am getting same wrong result even with POLimit changes reverted.

{code}
1       {(1,1,3.1),(8,1,2.2),(1,1,3.1)} {(1,3.1)}
2       {(8,2,3.5),(1,2,3.7)}   {(2,3.7)}
3       {(7,3,0.5),(1,3,3.2)}   {(3,3.2)}
{code}",26/Jul/16 05:49;daijy;+1,26/Jul/16 07:44;vaifer;+1. All scripts return correct results after this patch.,26/Jul/16 12:30;rohini;Committed to branch-0.16 and trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,
Split followed by order by/skewed join is skewed in Tez,PIG-4960,12992261,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,25/Jul/16 12:56,21/Jun/17 09:15,14/Mar/19 03:08,26/Jul/16 12:23,,,,,,0.16.1,0.17.0,,,,0,,,,,,,"Sampling is not done right. Split is a special case as EOP is returned after each record is processed. We did fixes for that before (PIG-4480, etc), but still it is not done right.  

   In case of skewed join, skipInterval is applied for each record instead of all the records. So except for the first record all the other records are mostly skipped. Sampling is slightly better than worse if there is a FLATTEN of bag on the input record to Split as there are multiple records to process.  

  In case of order by, samples were being returned even as they were being updated with new data. So samples mostly contained records from the first few hundreds of rows.",,,,,,,,,,,,,,,,,,,,25/Jul/16 13:35;rohini;PIG-4960-1.patch;https://issues.apache.org/jira/secure/attachment/12819927/PIG-4960-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-07-26 05:54:31.767,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 12:23:08 UTC 2016,,,,,,,0|i31gg7:,9223372036854775807,,,,,,,,,,26/Jul/16 01:57;rohini;int rand = randGen.nextInt(rowProcessed + 1);  - This change wasn't necessary. But just put it there to be same as RandomSampleLoader code which did rowNum = numSamples+1,26/Jul/16 05:54;daijy;+1,26/Jul/16 12:23;rohini;Patch committed to branch-0.16 and trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,
"See ""Received kill signal"" message for a normal run after PIG-4921",PIG-4957,12991959,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,daijy,daijy,22/Jul/16 21:42,21/Jun/17 09:15,14/Mar/19 03:08,26/Jul/16 01:59,,,,,,0.16.1,0.17.0,,,,0,,,,,,,"See message in almost every job:
{code}
INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Received kill signal
{code}

This needs to be fixed.",,,,,,,,,,,,,,,,,,,,24/Jul/16 03:36;rohini;PIG-4957-1.patch;https://issues.apache.org/jira/secure/attachment/12819812/PIG-4957-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-07-23 00:16:51.018,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jul 26 01:59:32 UTC 2016,,,,,,,0|i31el3:,9223372036854775807,,,,,,,,,,23/Jul/16 00:16;rohini;Dupe of PIG-4928 ?,"23/Jul/16 00:21;daijy;It happens in MR job, does not sound like PIG-4928.","23/Jul/16 00:21;rohini;Realized that in mapreduce mode, it is always going to log even if there are no running jobs. We can use this jira to log the message if jc!=null && !jc.getRunningJobs().isEmpty() . PIG-4928 can be used to fix the MRExecutionEngine instantiation problem in Tez.",25/Jul/16 19:21;daijy;+1,26/Jul/16 01:59;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,
Predicate push-down will not run filters for single unary expressions,PIG-4953,12990814,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rdblue,rdblue,rdblue,19/Jul/16 15:45,21/Jun/17 09:15,14/Mar/19 03:08,11/Feb/17 06:14,0.15.0,,,,,0.17.0,,impl,,,0,,,,,,,"While testing PIG-4940, I noticed that a ""is not null"" predicate was not pushed down, but equality predicates were. The problem is that the {{FilterExtractor#visit}} method ignores anything that isn't a {{BinaryExpression}}. Pushing down {{UnaryExpression}} filters works if the expression is linked in by a binary expression, like ""b is not null and c == 'val'"".",,,,,,,,,,,,,,,,,,,,19/Jul/16 18:03;rdblue;PIG-4953.1.patch;https://issues.apache.org/jira/secure/attachment/12818874/PIG-4953.1.patch,20/Jul/16 04:40;rdblue;PIG-4953.2.patch;https://issues.apache.org/jira/secure/attachment/12819017/PIG-4953.2.patch,22/Jul/16 05:14;daijy;PIG-4953.3.patch;https://issues.apache.org/jira/secure/attachment/12819557/PIG-4953.3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-07-20 04:21:41.047,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Jul 22 16:30:05 UTC 2016,,,,,,,0|i317iv:,9223372036854775807,,,,,,,,,,"19/Jul/16 15:49;rdblue;A comment in the code states, ""if the leaf is a unary operator it should be a FilterFunc in which case we don't try to extract partition filter conditions"". This appears to be a stale comment because it refers to partition filter conditions (one of the subclasses), but this is being used for push-down filtering as well. Partition filtering will check {{isSupportedOpType}} that returns false for all unary expressions, so it is perfectly safe to call {{checkPushDown}} in the partition filtering case.",19/Jul/16 18:03;rdblue;Attaching a patch with the fix.,"19/Jul/16 18:07;rdblue;[~daijy], can you take a look at this patch?

This fixes the visit method and adds tests for predicate push-down. I noticed that the tests added with PIG-4940 were for partition push-down, not predicate. I copied the test file and modified it to use {{PredicatePushDownFilterExtractor}}. I also added a test for PIG-4940 and for this. The successful case caught an additional problem: the fix for PIG-4940 didn't remove the residual filters that accumulate on the filter plan when the entire unary expression is going to be added to the filter plan isntead. I fixed this case and the tests are passing. I'd appreciate it if the reviewers could check that I got the logic right here. I'm not very familiar with how residuals are handled and want to make sure this is correct. Thanks!","20/Jul/16 04:21;daijy;Seems you miss something in the test, I only see you change getTestExpression from private to public. Is that right?",20/Jul/16 04:40;rdblue;Forgot to add the new test file. I'm attaching a new patch.,"22/Jul/16 05:14;daijy;The import statement in TestNewPartitionFilterPushDown is unnecessary. Also added several more tests.

Patch committed to trunk. Thanks Ryan!",22/Jul/16 16:30;rdblue;Thanks for reviewing so quickly!,,,,,,,,,,,,,,,,,,,,,
Fix minor issues with running scripts in non-local FileSystems,PIG-4950,12990018,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,petersla,petersla,petersla,16/Jul/16 06:01,21/Jun/17 09:15,14/Mar/19 03:08,17/Jul/16 08:27,0.15.0,0.16.0,,,,0.16.1,0.17.0,,,,0,,,,,,,"There are two similar minor issues regarding running Pig scripts located in non-local FileSystems such as hdfs and s3.
 
# The first occurs when the script path is passed using the ‘-f’ option. In this case, the script contents are not set in ScriptState. Instead a WARN message is logged due to an IOException being thrown. This is because the ‘remote’ path is treated as a local one. Instead, the path of the downloaded script should be passed over to ScriptState#setScript. As a result of this bug, an empty string is set for the “pig.script” property when the Pig job runs on a Hadoop cluster. Also, if Tez is being used, then the Dag info does not include the script contents as it normally does when a local script is passed.
# The second issue is more minor, but #validateLogFile in the Main class is set to use the path given by the user rather than using the downloaded local file path. Again, #validateLogFile method treats the given path as a local one, but this would not be the case if the user specifies a remote path. i.e. one with a scheme such as hdfs or s3. This occurs in both cases: when the script is specified using the ‘-f’ option or when the script is passed as the last/remaining argument.
 
Both fixes to these issues are to just pass in the local downloaded path instead. If the script path specified is a local one, then the local downloaded path would just be that path specified.",,,,,,,,,,,,,,,,,,,,16/Jul/16 06:04;petersla;PIG-4950.1.patch;https://issues.apache.org/jira/secure/attachment/12818312/PIG-4950.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-07-17 08:27:12.196,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sun Jul 17 08:27:12 UTC 2016,,,,,,,0|i312m7:,9223372036854775807,,,,,,,,,,16/Jul/16 06:05;petersla;Attached is the fix for these two issues including unit tests to cover the affected code paths.,"17/Jul/16 08:27;daijy;+1.

Patch committed to both 0.16 branch and trunk. Thanks Peter!",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix registering jar in S3 which was broken by PIG-4417 in Pig 0.16,PIG-4949,12989967,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yangyishan0901m,yangyishan0901m,yangyishan0901m,15/Jul/16 22:35,21/Jun/17 09:15,14/Mar/19 03:08,16/Jul/16 05:07,0.16.0,,,,,0.16.1,0.17.0,parser,,,0,patch,,,,,,"From PIG-4417, PIG introduced a uri validation for registering which can only allow hdfs, ivy. For example, ""REGISTER s3://template/abc.jar"" will fail, because scheme s3 is filtered out. This patch fixes a bug in RegisterResolver.java which introduced by PIG-4417. author of PIG-4417 added one check to detect file system scheme, but it can only accept hdfs, ivy and blocked every else possible valid filesystems, like s3, ftp, etc. ",,,,,,,,,,,,,,,,,,,,15/Jul/16 22:38;yangyishan0901m;PIG-4949.patch;https://issues.apache.org/jira/secure/attachment/12818254/PIG-4949.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-07-16 05:07:06.251,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,Reviewed,,,,Sat Jul 16 05:07:06 UTC 2016,,,Patch Available,,,,0|i312av:,9223372036854775807,,,,,,,,,,"16/Jul/16 05:07;daijy;+1.

Patch committed to both trunk and 0.16 branch. Thanks Yishan!",,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig on Tez AM use too much memory on a small cluster,PIG-4948,12989600,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Jul/16 22:31,21/Jun/17 09:15,14/Mar/19 03:08,19/Jul/16 01:29,,,,,,0.16.1,0.17.0,tez,,,0,,,,,,,"Pig will use 1024M for AM memory even if ""tez.am.resource.memory.mb"" is smaller. On a small cluster such as sandbox, the total memory is very small (eg, 2g) and this will make the capacity full and not able to run any job.",,,,,,,,,,,,,,,,,,,,15/Jul/16 18:13;daijy;PIG-4948-1.patch;https://issues.apache.org/jira/secure/attachment/12818207/PIG-4948-1.patch,19/Jul/16 01:23;daijy;PIG-4948-2.patch;https://issues.apache.org/jira/secure/attachment/12818723/PIG-4948-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-07-19 01:02:42.352,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jul 26 12:21:48 UTC 2016,,,,,,,0|i3101b:,9223372036854775807,,,,,,,,,,"14/Jul/16 23:16;daijy;I am going to introduce an option to respect tez.am.resource.memory.mb, [~rohini], do you have any different opinion?","19/Jul/16 01:02;rohini;Could you rename it to pig.tez.configure.am.memory or pig.tez.autoconfigure.am.memory ? Also could you make the constant name also same so that we keep naming convention consistent like in TezConfiguration. i.e PIG_TEZ_CONFIGURE_AM_MEMORY . Unrelated, I noticed that ENABLE_ATS does not confirm to that. Could you change that as well to PIG_ATS_ENABLED? If you want to retain existing one for backward compatibility with 0.15, you can move it to deprecated section below.","19/Jul/16 01:23;daijy;Rename pig.tez.override.am.memory to pig.tez.configure.am.memory as suggested. For PIG_ATS_ENABLED change, created PIG-4951 for that.",19/Jul/16 01:24;rohini;+1,19/Jul/16 01:29;daijy;Patch committed to both 0.16 branch and trunk. Thanks Rohini for review!,26/Jul/16 12:21;rohini;Created PIG-4962 to estimate smaller heap for small jobs with fewer tasks.,,,,,,,,,,,,,,,,,,,,,,
LOAD with HBaseStorage using a mix of pure wildcards and prefixed wildcards results in empty maps for the pure wildcarded column families.,PIG-4947,12989381,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,promiseu,promiseu,14/Jul/16 08:03,21/Jun/17 09:15,14/Mar/19 03:08,21/Jul/16 20:38,0.15.0,,,,,0.16.1,0.17.0,grunt,,,0,,,,,,,"LOAD with HBaseStorage using a mix of pure wildcards and prefixed wildcards results in empty maps for the pure wildcarded column families.

Here is my test scenario.
~~ Create test HBASE table.~~
create 'test_sha1', '3', 'i', 'd' 
put 'test_sha1', '1', '3:name', 'youngjin' 
put 'test_sha1', '1', 'i:whatever', 'true' 
put 'test_sha1', '1', 'd:forgemeta_1_whatever', 'true' 
--- 
1. If use pure wildcarded column families then it works. 
sha1_contents_succeed = load 'hbase://test_sha1' using org.apache.pig.backend.hadoop.hbase.HBaseStorage('3:* i:* d:*', '-loadKey true') as (row_key:chararray, unpacker:map[], stats_i:map[], forgemeta:map[]); 
dump sha1_contents_succeed; 
(1,[name#youngjin],[whatever#true],[forgemeta_1_whatever#true])

2. If use a mix of pure wildcards and prefixed wildcards - When only one column family has prefixed wildcards - then result but only shows a prefixed wildcards column family. 
sha1_contents_no_result = load 'hbase://test_sha1' using org.apache.pig.backend.hadoop.hbase.HBaseStorage('3:* i:* d:forgemeta_1_*', '-loadKey true') as (row_key:chararray, unpacker:map[], stats_i:map[], forgemeta:map[]); 
dump sha1_contents_no_result; 
===> (1,[],[],[forgemeta_1_whatever#true]) 


If use a mix of pure wildcards and prefixed wildcards - When only one column family has prefixed wildcards - then result but only shows a prefixed wildcards column family. 
sha1_contents_no_result = load 'hbase://test_sha1' using org.apache.pig.backend.hadoop.hbase.HBaseStorage('3:na* i:* d:*', '-loadKey true') as (row_key:chararray, unpacker:map[], stats_i:map[], forgemeta:map[]); 
(1,[name#youngjin],[],[]) 

If use a mix of pure wildcards and prefixed wildcards - When only one column family has prefixed wildcards - then result but only shows a prefixed wildcards column family. 
sha1_contents_no_result = load 'hbase://test_sha1' using org.apache.pig.backend.hadoop.hbase.HBaseStorage('3:* i:wh* d:*', '-loadKey true') as (row_key:chararray, unpacker:map[], stats_i:map[], forgemeta:map[]); 
(1,[],[whatever#true],[])","HDP 2.4.0
Pig 0.15.0",,,,,,,,,,,,,,,,,,,14/Jul/16 19:32;daijy;PIG-4947-1.patch;https://issues.apache.org/jira/secure/attachment/12818003/PIG-4947-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-07-21 04:52:09.84,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Jul 21 20:38:49 UTC 2016,,,,,,,0|i30yon:,9223372036854775807,,,,,,,,,,21/Jul/16 04:52;rohini;+1,21/Jul/16 20:38;daijy;Patch committed to both 0.16 branch and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove redudant code of bin/pig in spark mode after PIG-4903 check in,PIG-4946,12987670,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,08/Jul/16 02:19,21/Jun/17 09:18,14/Mar/19 03:08,11/Jul/16 14:57,,,,,,spark-branch,,,,,0,,,,,,,"After PIG-4903 checkin, some redudant code of bin/pig in spark branch is generated.",,,,,,,,,,,,,,,,,,,,08/Jul/16 02:41;kellyzly;PIG-4946.patch;https://issues.apache.org/jira/secure/attachment/12816747/PIG-4946.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-07-11 07:17:08.947,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 11 14:57:26 UTC 2016,,,,,,,0|i30onj:,9223372036854775807,,,,,,,,,,"08/Jul/16 02:41;kellyzly;[~kexianda]:
After PIG-4903 check in, the process of starting a pig script changes(before we add all jars under $PIG_HOME/lib and $PIG_HOME/lib/spark/* to the classpath and export them to SPARK_YARN_DIST_FILES and SPARK_DIST_CLASSPATH) now we dynamically load libs(PIG-4893)

I found that some redudant code in PIG-4903's check in. So checkout latest code and patch -p1<PIG-4946.patch to verify this jira.

  Can you verify the process as following both in spark local and spark yarn-client mode?

*export SPARK_MASTER=""yarn-client”*
1. not export SPARK_HOME  to run:
{code}
     error message like ""Error: SPARK_HOME is not set!"" is thrown out
{code}
2. export SPARK_HOME but not export SPARK_JAR to run:
{code}
    error message like ""Error: SPARK_JAR is not set, SPARK_JAR stands for the hdfs location of spark-assembly*.jar. This allows YARN to cache spark-assembly*.jar on nodes so that it     doesn't need to be distributed each time an application runs.""
{code}
3. export SPARK_HOME and SPARK_JAR  to run:
{code}
export SPARK_SOURCE=/home/zly/prj/oss/spark_source/
export SPARK_JAR=hdfs://zly1.sh.intel.com:8020/user/root/spark-assembly-1.6.0-hadoop2.6.0.jar
./pig -x spark xx.pig
{code}

*export SPARK_MASTER=""local""
1. we need not export SPARK_HOME, SPARK_JAR in local mode, so even we don't export these two variables, the script should pass.
     ","11/Jul/16 07:17;kexianda;It is OK to remove the  redundant code.
Verified.  
LGTM. 
+1","11/Jul/16 07:18;kellyzly;[~xuefuz]: please commit it to branch, thanks!","11/Jul/16 14:57;xuefuz;Committed to Spark branch. Thanks, Liyun!",,,,,,,,,,,,,,,,,,,,,,,,
Update document for conflicting macro params,PIG-4945,12987509,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,knoguchi,knoguchi,knoguchi,07/Jul/16 15:29,21/Jun/17 09:15,14/Mar/19 03:08,16/Jul/16 20:55,,,,,,0.16.1,0.17.0,documentation,,,0,,,,,,,Update document to reflect PIG-4880 which now allows overlapping of top-level param names and macro params. ,,,,,,,,,,,,,,,,,,,,07/Jul/16 15:36;knoguchi;pig-4945-doc-v01.patch;https://issues.apache.org/jira/secure/attachment/12816652/pig-4945-doc-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-07-16 20:55:55.782,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Jul 16 20:55:55 UTC 2016,,,,,,,0|i30nnr:,9223372036854775807,,,,,,,,,,16/Jul/16 20:55;daijy;Patch committed to both 0.16 branch and trunk. Thanks Koji!,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reset UDFContext#jobConf in spark mode,PIG-4944,12986728,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,05/Jul/16 06:43,21/Jun/17 09:18,14/Mar/19 03:08,06/Jul/16 14:31,,,,,,spark-branch,,,,,0,,,,,,,"Community gave some comments about TestEvalPipelineLocal unit test:
https://reviews.apache.org/r/45667/#comment199056

We can reset ""UDFContext.getUDFContext().addJobConf(null)"" in other place not  in TestEvalPipelineLocal#testSetLocationCalledInFE


",,,,,,,,,,,,,,,,,,,,05/Jul/16 06:49;kellyzly;PIG-4944.patch;https://issues.apache.org/jira/secure/attachment/12816142/PIG-4944.patch,06/Jul/16 08:33;kellyzly;PIG-4944_2.patch;https://issues.apache.org/jira/secure/attachment/12816386/PIG-4944_2.patch,05/Jul/16 06:51;kellyzly;TestEvalPipelineLocal.mr;https://issues.apache.org/jira/secure/attachment/12816143/TestEvalPipelineLocal.mr,05/Jul/16 06:51;kellyzly;TestEvalPipelineLocal.spark;https://issues.apache.org/jira/secure/attachment/12816144/TestEvalPipelineLocal.spark,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2016-07-06 05:25:54.691,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 06 14:31:28 UTC 2016,,,,,,,0|i30iu7:,9223372036854775807,,,,,,,,,,"05/Jul/16 06:49;kellyzly;Let's explain why we need reset (UDFContext.getUDFContext().addJobConf(null)) in SparkLauncher#resetUDFContext:

If we don't reset the value, following error will be thrown:
{code:title=org.apache.pig.test.TestEvalPipelineLocal.SetLocationTestLoadFunc}
 public static class SetLocationTestLoadFunc extends PigStorage {
        String suffix = ""test"";
        public SetLocationTestLoadFunc() {
        }
        @Override
        public void setLocation(String location, Job job) throws IOException {
            super.setLocation(location, job);
            Properties p = UDFContext.getUDFContext().getUDFProperties(this.getClass());
            if (UDFContext.getUDFContext().isFrontend()) {
                p.setProperty(""t_""+signature, ""test"");
            } else {
                if (p.getProperty(""t_""+signature)==null)
                    throw new IOException(""property expected""); //Throw this exception
            }
        }
    }
{code}


It is interesting that we need not reset(UDFContext.getUDFContext().addJobConf(null)) in mr  but we need do this in spark or tez mode([org.apache.pig.backend.hadoop.executionengine.tez.TezDagBuilder#addCombiner|https://github.com/apache/pig/blob/7cf1a945772f49ff620d7eab75bf2c7e635ab2ae/src/org/apache/pig/backend/hadoop/executionengine/tez/TezDagBuilder.java#L1008]). The reason is because UDFContext.getUDFContext is a threadlocal variable.  These variables differ in different thread.  In mr mode, UDFContext.getUDFContext().setJobConf(null) in main thread and UDFContext.getUDFContext().getJobConf() is null when TestEvalPipelineLocal.SetLocationTestLoadFunc is called in main thread.  The behavior differs in spark mode:
UDFContext.getUDFContext().setJobConf(null) and UDFContext.getUDFContext().setJobConf(not null) in main thread, so when org.apache.pig.test.TestEvalPipelineLocal.SetLocationTestLoadFunc, exception is thrown out.  

I add some log info in the code to verify above conclusion:
in attached TestEvalPipelineLocal.spark:
It shows that in spark mode
in Line 24548  UDFContext#addJobConf(null) is called, then in Line 24621  UDFContext#addJobConf(not null) is called in main thread and in Line 26138 SetLocationTestLoadFunc#setLocation is called in main thread.

in attached TestEvalPipelineLocal.mr,
It shows that in mr mode in Line 5734 UDFContext.getUDFContext().addJobConf(null) in main thread and in Line 6097 SetLocationTestLoadFunc#setLocation is called in main thread. Between Line 5734 and Line 6097, UDFContext.getUDFContext().addJobConf(not null) is not called in main thread.

","05/Jul/16 06:54;kellyzly;[~Pratyy] and [~pallavi.rao]: Before  in PIG-4807, you worked on TestEvalPipeLineLocal, can any one of you help review this jira, thanks!","06/Jul/16 05:25;kexianda;In MR mode.  LocalJobRunner create a new thread for different UT cases. Thread_local jobConf is set in this new created thread.
In spark mode(Local mode), all the ut cases share the same main thread.  UDFContext's JobConf is reset in PigInputFormat.passLoadSignature()

LGTM.  It is the right place to clean JobConf in end of SparkLauncher.launchPig()
+1

[~xuefuz], please help review and commit

{code}
patch -p1 < PIG-4944.patch
{code}","06/Jul/16 08:35;kellyzly;[~praveenr019] and [~xuefuz]:  If anyone of you see this comment, please first commit PIG-4797_5.patch then PIG-4944_2.patch.(PIG-4944_2.patch is based on PIG-4797_5.patch).","06/Jul/16 14:31;xuefuz;Committed to Spark branch. Thanks, Liyun!",,,,,,,,,,,,,,,,,,,,,,,
Predicate push-down filtering unary expressions can be pushed.,PIG-4940,12984474,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rdblue,rdblue,rdblue,28/Jun/16 19:16,21/Jun/17 09:15,14/Mar/19 03:08,19/Jul/16 00:49,,,,,,0.17.0,,,,,0,,,,,,,"While testing predicate push-down, I ran into the following error:

{code:title=Pig Exception}
ERROR 0: Unsupported conversion of LogicalExpression to Expression: Map
        at org.apache.pig.newplan.FilterExtractor.getExpression(FilterExtractor.java:389)
        at org.apache.pig.newplan.FilterExtractor.getExpression(FilterExtractor.java:401)
        at org.apache.pig.newplan.FilterExtractor.getExpression(FilterExtractor.java:378)
        at org.apache.pig.newplan.FilterExtractor.getExpression(FilterExtractor.java:401)
        at org.apache.pig.newplan.FilterExtractor.getExpression(FilterExtractor.java:380)
        at org.apache.pig.newplan.FilterExtractor.visit(FilterExtractor.java:109)
        at org.apache.pig.newplan.PredicatePushDownFilterExtractor.visit(PredicatePushDownFilterExtractor.java:70)
        at org.apache.pig.newplan.logical.rules.PredicatePushdownOptimizer$PredicatePushDownTransformer.transform(PredicatePushdownOptimizer.java:146)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:110)
        ... 19 more
{code}

The problem is that the code is trying to push a map access operation, that isn't supported. The cause appears to be the logic in {{checkPushDown(UnaryExpression)}} that separates expressions that can be pushed from expressions that must be run by Pig. This function assumes that any expression under {{IsNullExpression}} or {{NotExpression}} can be pushed and adds the unary node's child expression to the pushdown expression without calling {{checkPushDown}} on it.",,,,,,,,,,,,,PIG-4953,,,,,,,11/Jul/16 20:16;rdblue;PIG-4940.1.patch;https://issues.apache.org/jira/secure/attachment/12817231/PIG-4940.1.patch,11/Jul/16 23:40;rdblue;PIG-4940.2.patch;https://issues.apache.org/jira/secure/attachment/12817292/PIG-4940.2.patch,18/Jul/16 23:29;daijy;PIG-4940.3.patch;https://issues.apache.org/jira/secure/attachment/12818696/PIG-4940.3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-06-28 19:21:21.955,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jul 19 00:51:32 UTC 2016,,,,,,,0|i309wn:,9223372036854775807,,,,,,,,,,"28/Jun/16 19:21;githubbot;GitHub user rdblue opened a pull request:

    https://github.com/apache/pig/pull/25

    PIG-4940: Fix predicate push-down Unary expressions.

    Rather than assuming any child expression of `IsNullExpression` and
    `NotExpression` can be pushed down, this calls `checkPushDown` on the child
    and will push the entire expression if and only if the entire child can
    be pushed.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rdblue/pig PIG-4940-unary-expression-push-down

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/pig/pull/25.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #25
    
----
commit 0b23e3af0d6432a3736e7101f6fdea20b03382cc
Author: Ryan Blue <blue@apache.org>
Date:   2016-06-28T19:02:28Z

    PIG-4940: Fix predicate push-down Unary expressions.
    
    Rather than assuming any child expression of IsNullExpression and
    NotExpression can be pushed down, this calls checkPushDown on the child
    and will push the entire expression if and only if the entire child can
    be pushed.

----
",28/Jun/16 19:21;rdblue;Fixed by PR #25.,"28/Jun/16 20:02;rdblue;[~cheolsoo], you may be interested in this.","28/Jun/16 21:24;cheolsoo;Sorry, but I'll let someone who is still active in the community review your patch...",01/Jul/16 19:16;daijy;Can you attach patch instead? Would you mind a test case?,"11/Jul/16 21:17;rdblue;I attached a patch with my changes. The case that we hit is a ""not map_column#'prop' is null"" expression. The map access can't be pushed down, but the code assumes that anything under a unary expression can be.

I looked at writing a test, but I don't see a test suite for predicate push-down in the FilterExpression code already there so it will take me a while to wrap my head around what needs to be done. I'm not very familiar with the Pig internals necessary to cleanly test this. If someone can help with the setup, I can get a test case written pretty quickly. Otherwise, it may take me some time so I'm posting the patch now so you can get started looking at it in the mean time.",11/Jul/16 21:34;rohini;You can add the test to TestOrcStoragePushdown.java,"11/Jul/16 23:40;rdblue;Adding a test for this, testUnSupportedFieldsWithUnaryExpression, in the Orc push-down suite. The case demonstrates that ""not browser#'type' is null"" is not pushed down. The testNot and testAndOr cases already verify that the ""is null""  predicate (and the negated form) works when the column is not a map access and can be pushed.",11/Jul/16 23:41;rdblue;Thanks for your help! I added a test to the Orc suite.,"17/Jul/16 08:18;daijy;The fix looks good. However, the test case fits better in TestNewPartitionFilterPushDown since it is not Orc specific. Can you move it?",17/Jul/16 22:53;rdblue;That's where we get back to my lack of knowledge of Pig. The tests in TestNewPartitionFilterPushDown require a lot of knowledge of Pig internals that I don't have. I can try over the next few months if someone can help me out with where to start.,"17/Jul/16 23:54;daijy;No worry, I can move it.",18/Jul/16 01:40;rdblue;Thank you!,18/Jul/16 23:29;daijy;Move the test to TestNewPartitionFilterPushDown.,19/Jul/16 00:48;rdblue;+1. Thanks for your help!,19/Jul/16 00:49;daijy;Patch committed to trunk. Thanks Ryan!,"19/Jul/16 00:50;rdblue;I also just noticed that the {{visit}} method in FilterExtractor assumes that the root expression is going to be a binary expression. If you try to filter with just ""is null"" or ""is not null"" then predicate push-down isn't triggered. There's a simple patch. Should I attach it here based on [~daijy]'s latest changes, or should I open a new issue?","19/Jul/16 00:51;rdblue;Nevermind, this was committed. I'll fix the second problme in a new JIRA issue. From looking at Daniel's test, I think I can replicate it this time.",,,,,,,,,,
[PiggyBank] XPath returns empty values when using aggregation method,PIG-4938,12983932,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,nkollar,ivo.lenting@gmail.com,ivo.lenting@gmail.com,27/Jun/16 20:34,21/Jun/17 09:15,14/Mar/19 03:08,16/Jul/16 20:48,0.15.0,,,,,0.17.0,,piggybank,,,0,,,,,,,"I have a xml file which I want to parse using the piggybank XPath udf.

The xml is:
<Aa name=""test1"">	
	<Bb Cc=""1""/>
	<Bb Cc=""1""/>
	<Bb Cc=""1""/>
	<Bb Cc=""1""/>
	<Dd>test2</Dd>
</Aa>

The xpath contains a sum aggregate to sum all Cc values. 
The complete pig script:

REGISTER piggybank.jar
DEFINE XPath org.apache.pig.piggybank.evaluation.xml.XPath();
DEFINE XPathAll org.apache.pig.piggybank.evaluation.xml.XPathAll();
XMLFile = LOAD '/demo/test.xml' using org.apache.pig.piggybank.storage.XMLLoader('Aa') as (xmlContents:chararray);
MyOutput = FOREACH XMLFile GENERATE XPathAll(xmlContents,'Aa/@name',true,false).$0 AS Aa:chararray,XPath(xmlContents,'sum(Aa/Bb/@Cc)') AS Cc:Double, XPath(xmlContents,'Aa/Dd') AS Dd:chararray;
STORE MyOutput INTO 'Output/MyOutput' USING PigStorage('|');

MyOutput:
test1||test2

So i'm missing the aggregate 4 in column 2.",,,,,,,,,,,,,,,,,,,,06/Jul/16 12:43;nkollar;PIG-4938.patch;https://issues.apache.org/jira/secure/attachment/12816423/PIG-4938.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-07-01 12:21:04.614,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Jul 16 20:48:17 UTC 2016,,,,,,,0|i307mf:,9223372036854775807,,,,,,,,,,"01/Jul/16 12:21;githubbot;GitHub user nandorKollar opened a pull request:

    https://github.com/apache/pig/pull/26

    Fix for piggybank XPath function related bugs: PIG-4938 and PIG-4938

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/nandorKollar/pig trunk

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/pig/pull/26.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #26
    
----
commit ff613c0a9ebf3d7d211aa02bfe79e1d340ecb906
Author: Nandor Kollar <nkollar@cloudera.com>
Date:   2016-06-30T11:35:39Z

    PIG-4751 : XPath/XPathAll - ignoreNamspace breaks searching for XML attributes
    PIG-4938 : [PiggyBank] XPath returns empty values when using aggregation method

commit 06dcc49e3e12e40c5dc2bb196546570dd16cec72
Author: Nandor Kollar <nkollar@cloudera.com>
Date:   2016-06-30T11:47:22Z

    add change history

----
","01/Jul/16 12:28;githubbot;Github user nandorKollar closed the pull request at:

    https://github.com/apache/pig/pull/26
","01/Jul/16 18:56;daijy;Can you attach patch instead? Is your patch address both PIG-4751 and PIG-4938, or PIG-4938 alone?","06/Jul/16 12:52;nkollar;Hi Daniel,
Attached the patch, and created a review on review board too: https://reviews.apache.org/r/49704
The patch addresses PIG-4752 too.",14/Jul/16 08:26;nkollar;[~daijy] could you please take a look at the the patch?,"16/Jul/16 20:48;daijy;+1.

Patch committed to trunk. Thanks Nandor!",,,,,,,,,,,,,,,,,,,,,,
TEZ_USE_CLUSTER_HADOOP_LIBS is always set to true,PIG-4935,12982511,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,23/Jun/16 23:23,21/Jun/17 09:15,14/Mar/19 03:08,26/Jul/16 22:14,,,,,,0.16.1,0.17.0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Jun/16 23:25;rohini;PIG-4935-1.patch;https://issues.apache.org/jira/secure/attachment/12812958/PIG-4935-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-07-26 21:30:35.564,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jul 26 22:14:26 UTC 2016,,,,,,,0|i3005j:,9223372036854775807,,,,,,,,,,26/Jul/16 21:30;daijy;+1,26/Jul/16 22:14;rohini;Committed to branch-0.16 and trunk. Thanks for the review Daniel,,,,,,,,,,,,,,,,,,,,,,,,,,
SET command does not work well with deprecated settings,PIG-4934,12982506,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,rohini,rohini,23/Jun/16 23:06,21/Jun/17 09:15,14/Mar/19 03:08,27/Oct/16 20:25,,,,,,0.17.0,,,,,0,,,,,,,"For eg: If mapred.job.map.memory.mb was specified in the script using set command and mapreduce.map.memory.mb was present in mapred-site.xml, that takes effect.  This is because of the use of Properties and not Configuration.

GruntParser.processSet() calls HExecutionEngine.setProperty which just updates pigContext.getProperties()

{code}
public void setProperty(String property, String value) {
        Properties properties = pigContext.getProperties();
        properties.put(property, value);
    }
{code}",,,,,,,,,,,,,,,,,,,,25/Oct/16 14:21;szita;PIG-4934.2.patch;https://issues.apache.org/jira/secure/attachment/12835124/PIG-4934.2.patch,27/Oct/16 14:19;szita;PIG-4934.3.patch;https://issues.apache.org/jira/secure/attachment/12835601/PIG-4934.3.patch,23/Oct/16 20:20;szita;PIG-4934.patch;https://issues.apache.org/jira/secure/attachment/12834854/PIG-4934.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-10-23 20:27:19.078,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Oct 28 10:32:44 UTC 2016,,,,,,,0|i3004f:,9223372036854775807,,,,,,,,,,"23/Jun/16 23:13;rohini;Fix would be to do 

{code}
   @Override
    public void setProperty(String property, String value) {
        Properties properties = pigContext.getProperties();
        Configuration conf = ConfigurationUtil.toConfiguration(properties);
        conf.set(property, value);
        properties.clear();
        Iterator<Map.Entry<String, String>> iter = conf.iterator();
        while (iter.hasNext()) {
            Map.Entry<String, String> entry = iter.next();
            properties.put(entry.getKey(), entry.getValue());
        }
        
    }
{code}

but that is a overkill when there are lot of set commands. Configuration.getAlternateNames() gives all possible deprecated and new keys and would be easy to check if the value if properties contains any of those names and overwrite or add it if none of the names are present. 
Unfortunately it is a private method.  Wondering if I should access that method using reflection as it would be lot simpler in terms of processing. But it will be hacky as we are accessing a private method. Thoughts or any other ideas?",23/Oct/16 20:27;szita;We could create a util function for this that acts as getAlternateNames and returns an enriched set of properties from a single key-value pair. See [^PIG-4934.patch] as example (without error-handling and testing just yet),"24/Oct/16 18:40;daijy;For optimization, you can check isDeprecated before invoking costly expandForAlternativeNames.","25/Oct/16 14:22;szita;That'd be a good idea, but we can't expect that only non-deprecated properties are set at the time when SET command comes in.
For example:
-we specify something using the old key: _mapred.job.map.memory.mb_=1 in a config xml
-then we'll have _mapred.job.map.memory.mb_=1 and *mapreduce.map.memory.mb*=1 at startup already.
-if a user now uses the current name and gives: SET *mapreduce.map.memory.mb* 2, we would only be setting the new name and leave the old one (_mapred.job.map.memory.mb_=1) 
... so I don't think we can use the isDeprecated() check unfortunately.

Anyhow I've uploaded a second revision including a test case: [^PIG-4934.2.patch]","26/Oct/16 22:38;daijy;If you have both old and new key set to different value, engine will take the new key, why it matters?","27/Oct/16 14:23;szita;I see, in this case we can indeed do the optimization. Revised [^PIG-4934.3.patch]",27/Oct/16 20:25;daijy;+1. Patch committed to trunk. Thanks Adam!,"28/Oct/16 10:32;szita;Thanks for the review, [~daijy]",,,,,,,,,,,,,,,,,,,,
TestDataBagAccess.testBagConstantFlatten1/TestLogicalPlanBuilder.testQuery90  broken after PIG-2315,PIG-4933,12981975,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,22/Jun/16 17:12,21/Jun/17 09:15,14/Mar/19 03:08,11/Aug/16 15:29,,,,,,0.17.0,,,,,0,,,,,,,"{code:title=test.pig}
A = load '1line.txt';
B = foreach A generate {(('p1-t1-e1', 'p1-t1-e2'),('p1-t2-e1', 'p1-t2-e2')),(('p2-t1-e1', 'p2-t1-e2'), ('p2-t2-e1', 'p2-t2-e2'))};
C = foreach B generate $0 as pairbag : { pair: ( t1: (e1, e2), t2: (e1, e2) ) };
DUMP C;
{code}

Above results in empty '()'",,,,,,,,,,,,,,,,,,,,02/Aug/16 13:26;knoguchi;pig-4933-v01.patch;https://issues.apache.org/jira/secure/attachment/12821605/pig-4933-v01.patch,02/Aug/16 16:57;knoguchi;pig-4933-v02.patch;https://issues.apache.org/jira/secure/attachment/12821651/pig-4933-v02.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-06-24 22:47:19.213,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Aug 11 15:29:04 UTC 2016,,,,,,,0|i2zx3b:,9223372036854775807,,,,,,,,,,"24/Jun/16 22:31;knoguchi;Found one more test, TestLogicalPlanBuilder.testQuery90, failing.

{code}
a = load 'test3.txt' as (name:chararray, age:int, gpa:float);
b = group a by (name, age);
c = foreach b generate group as mygroup:(myname, myage)
{code} 

Schema for mygroup is becoming (bytearray, bytearray).  

Basically a same pattern from the description when tuple/bag(and probably map) is renamed, it still inserts the casting corrupting the schema.

Two separate issues.
* casts should not be inserted in the first place
* Even if casts are inserted, they shouldn't be casting to ""bytearray"". 

I'll see if I can work on it next week, but until then trunk is broken for these cases.  If quick fix is needed, we can skip this feature for tuple/map/bag and bring back the previous behavior.    Cc: [~daijy]

Sorry again for not catching these failures before committing the patch.","24/Jun/16 22:47;daijy;The statement doesn't mean to cast to bytearray, it means don't cast. We shall remove the bytearray cast. Also if the cast is unnecessary (the original type is the same as the cast), we shall remove the cast.","01/Aug/16 22:20;knoguchi;Attaching my initial patch (pig-4933-v01.patch).

bq. The statement doesn't mean to cast to bytearray, it means don't cast. We shall remove the bytearray cast. 

Updated POCast to make toByteArray cast as no-op.
(Before, it was returning empty fields.)
Also added testAsWithByteArrayCast to test this condition.

bq. Also if the cast is unnecessary (the original type is the same as the cast), we shall remove the cast.

Updated ForEachUserSchemaVisitor to 
* Check if userDefinedSchema do not have type specified in {{hasOnlyNullOrByteArraySchema}}.
* Merge expectedschema and userschema so that we don't overwrite field schema as bytearray when only alias is provided. {{replaceNullByteArraySchema}}.
* When resetting the schema to null (for the original foreach), made sure to reset the type for complicate types (bag/tuple/map). {{resetTypeToNull}}.

And in TestLogicalPlanBuilder.java, uncommented part of the testQuery90 that were being skipped for some reasons.

[~daijy], appreciate if you can take a look when you have time.   ",02/Aug/16 13:26;knoguchi;I forgot to use {{--no-prefix}} when taking a diff.  Re-attaching.,"02/Aug/16 16:57;knoguchi;Some tests were failing with NPE with the previous patch.
Added one null check.

With this patch, at least the tests seem to be going through.
","11/Aug/16 03:36;daijy;Thanks for dealing with nested schema. Though the code is lengthy, I cannot find a way to make simpler. +1.","11/Aug/16 15:29;knoguchi;Thanks for the review Daniel!  

Committed to trunk.",,,,,,,,,,,,,,,,,,,,,
Skewed Join Breaks On Empty Sampled Input When Key is From Map,PIG-4930,12979925,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,butlerw,butlerw,16/Jun/16 18:38,21/Jun/17 09:15,14/Mar/19 03:08,07/Jan/17 20:20,0.16.0,0.9.2,,,,0.16.1,0.17.0,,,,0,,,,,,,"When using a skewed join, if the left relation gets its key from a map and said relation is empty, then the skewed join fails during the sampling phase with:

org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing (Name: Local Rearrange[tuple]{tuple}(false) - scope-27 Operator Key: scope-27): org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [POMapLookUp (Name: POMapLookUp[bytearray] - scope-14 Operator Key: scope-14) children: null at [null[3,17]]]: java.lang.ClassCastException: java.lang.String cannot be cast to java.util.Map
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:314)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNextTuple(POLocalRearrange.java:287)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:280)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:275)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:65)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)


I think the problem is more fundamental to Pig's skewed join implementation than maps, but it is easily demonstrable with them. I have written an additional test in TestSkewedJoin that demonstrates the problem. The join works correctly if we remove ""using 'skewed'""",,,,,,,,,,,,,,,,,,,,07/Jan/17 20:21;rohini;PIG-4930-2.patch;https://issues.apache.org/jira/secure/attachment/12846197/PIG-4930-2.patch,17/Dec/16 21:15;nkollar;PIG-4930.patch;https://issues.apache.org/jira/secure/attachment/12843729/PIG-4930.patch,16/Jun/16 18:41;butlerw;empty_skew.diff;https://issues.apache.org/jira/secure/attachment/12811176/empty_skew.diff,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-12-16 12:35:46.988,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Jan 07 20:20:58 UTC 2017,,,,,,,0|i2zkzz:,9223372036854775807,,,,,,,,,,"16/Jun/16 18:42;butlerw;Attached empty_skew.diff
This contains a test that errors as is on Pig 0.16 but passes if we remove ""using 'skewed'""",16/Dec/16 12:35;nkollar;[~daijy] would you mind if I would work on this item?,"18/Dec/16 04:45;daijy;Sure, go ahead.","18/Dec/16 22:03;nkollar;When there is no data, PoissonSampleLoader still emits one tuple: (NUMROWS_TUPLE_MARKER, 0) and then later in the physical plan processing, the cast fails (in this case it fails because the join key is expected to be a map, but would fail for tuple key too). I think PoissonSampleLoader shouldn't emit any tuple if the dataset is empty. [~daijy] what do you think? Should I add an e2e test to cover the empty dataset case, or the test what William provided is enough?","20/Dec/16 17:40;rohini;Is this only a problem with MR and not Tez?

bq. I think PoissonSampleLoader shouldn't emit any tuple if the dataset is empty. 
  Need to check if there are other cases where it might become a problem. Will get back on this.

","20/Dec/16 21:29;nkollar;No, it looks like the test (testSkewedJoinMapLeftEmpty) fails only in MR mode.",05/Jan/17 23:17;rohini;Code is ok and I was going to commit it. But saw that TestSkewedJoin is hanging in testSkewedJoinMapKey. Even without the patch it seems to be hanging. Can you look into it?,"06/Jan/17 08:53;nkollar;Sure, I can, could you please assign the Jira to me?",06/Jan/17 18:01;rohini;There is no jira. But I was wondering if it was result of PIG-3417 as there has been no other recent changes. ,"07/Jan/17 20:20;rohini;I accidentally committed this also as part of PIG-4923 and https://builds.apache.org/job/Pig-trunk-commit/lastCompletedBuild/testReport/org.apache.pig.test/TestSkewedJoin/ was fine. So must be something with my laptop.

Just made a minor change of adding PARALLEL 2 to the testcase. With Parallelism of 1, the Partitioner is not exercised. So it is always best to have paralellism of > 1 for order by and skewed join tests which use custom partitioner.

Committed to branch-0.16 as well. Thanks [~nkollar] for fixing this. ",,,,,,,,,,,,,,,,,,
Kill running jobs on InterruptedException,PIG-4921,12975608,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,03/Jun/16 19:50,21/Jun/17 09:15,14/Mar/19 03:08,08/Jun/16 09:26,,,,,,0.16.1,0.17.0,,,,0,,,,,,,"Shutdown hook kills running jobs, but when running in Oozie launcher sometimes NodeManager can issue a SIGKILL after AM unregisters and before shutdown hook gets to execute causing orphaned jobs that continue to run. So it is better to kill when we see the InterruptedException.

Added additional System.err logging to code in shutdown hooks as LogManager.shutdown() is called by mapreduce AM (Oozie launcher job is mapreduce) and log4j logging does not appear anymore.  ",,,,,,,,,,,,,,,,,,,,03/Jun/16 19:53;rohini;PIG-4921-1.patch;https://issues.apache.org/jira/secure/attachment/12808045/PIG-4921-1.patch,04/Jun/16 21:29;rohini;PIG-4921-2.patch;https://issues.apache.org/jira/secure/attachment/12808188/PIG-4921-2.patch,06/Jun/16 23:43;rohini;PIG-4921-3.patch;https://issues.apache.org/jira/secure/attachment/12808506/PIG-4921-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-06-03 23:48:23.345,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Jul 22 21:43:24 UTC 2016,,,,,,,0|i2yyrr:,9223372036854775807,,,,,,,,,,03/Jun/16 19:53;rohini;Also removed HangingJobKiller shutdown hook for Tez. It was redundant and duplicating effort as TezSessionManager shutdown hook already takes care of shutting down all applications.,"03/Jun/16 23:48;daijy;TezLaucher.kill still kill individual job, is that redundant as well?

For MR, kill will be invoked in both exception handling and shutdown hook, will this an issue?","04/Jun/16 21:29;rohini;bq. TezLaucher.kill still kill individual job, is that redundant as well?
   TezLaucher.kill is now not called as part of HangingJobKiller shutdown hook. Only called on interrupted exception. TezSessionManager shutdown hook will shutdown all sessions.

bq. For MR, kill will be invoked in both exception handling and shutdown hook, will this an issue?
   No. If already killed in exception handling then shutdown hook will be a no-op as it kills jc.getRunningJobs. Actually found that PIG-3524 broke HangingJobKiller for mapreduce as it was setting jc = null and it skipped killing jobs because of that. That was a unnecessary change. Reverted that to fix the issue and also added System.err logging statement to mapreduce as well.",06/Jun/16 23:43;rohini;Updated patch to have shutdown hooks ordered taking into account issue in PIG-4916.  One side effect of this is that shutdown hook all run in a single thread now. But it should be fine as only when running via Oozie launcher time is of concern and in that case we kill the running jobs before shutdown hook. So shutdown hook will only have to cleanup tmp files and close TimelineClient which have to be done before filesystem is closed.,"07/Jun/16 06:20;daijy;Thanks for taking care of shutdown hook with priority. In case of InterruptedException of a Tez job, do you actually want to call TezSessionManager.shutdown()? TezLauncher.kill only kill the running job and may leave idle AM behind.",07/Jun/16 14:41;rohini;No. Killing the running job does both dagClient.tryKillDAG(); and tezClient.stop(); . So we are good.,"07/Jun/16 16:59;daijy;It kills active session but not idle sessions. Idle session will go away when expire, so probably no big deal. But when you say HangingJobKiller duplicates TezSessionManager.shutdown, it's not true, TezSessionManager.shutdown does more than HangingJobKiller.","07/Jun/16 17:24;rohini;bq. It kills active session but not idle sessions. Idle session will go away when expire, so probably no big deal.
  Yes. Also they will not be running any code as there is no DAG submitted to them. So idle sessions are not a problem. The problem was active sessions as many times the orphaned jobs interfered with job reruns.

bq. But when you say HangingJobKiller duplicates TezSessionManager.shutdown, it's not true, TezSessionManager.shutdown does more than HangingJobKiller.
  Yes. TezSessionManager.shutdown does all that HangingJobKiller does and more. That is why getting rid of HangingJobKiller.","07/Jun/16 17:36;daijy;Then in case of Oozie interruption, Pig call HangingJobKiller; In case of other interruption, Pig call TezSessionManager.shutdown. Can we make it consistent?","07/Jun/16 21:01;rohini;bq.  in case of Oozie interruption, Pig call HangingJobKiller; In case of other interruption, Pig call TezSessionManager.shutdown
   In case of Oozie interruption, Pig will call kill directly and HangingJobKiller(mapreduce)/TezSessionManager.shutdown will also be invoked as part of shutdown hook. In commandline, only HangingJobKiller(mapreduce)/TezSessionManager.shutdown is called. This is required because with launcher is killed, enough time is not given for the shutdown hook to execute before the container is shot down with SIGTERM (kill -9).  In case of command line, when user does Ctrl+C shutdown hook has all the time to execute. Also there is no InterruptedException as no one interrupts the Thread like in launcher.","08/Jun/16 07:37;daijy;You mentioned TezSessionManager.shutdown is not effective in Oozie. If that happen, idle AM will left behind. But I agree compare to dangling job, it is less a worry. +1 for PIG-4921-3.patch.",08/Jun/16 09:26;rohini;Committed to trunk. Thanks for the review Daniel.,"22/Jul/16 21:43;daijy;See lots of ""Received kill signal"" message after the patch. Created PIG-4957 for that.",,,,,,,,,,,,,,,
Pig on Tez cannot switch pig.temp.dir to another fs,PIG-4918,12974977,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,01/Jun/16 23:35,21/Jun/17 09:15,14/Mar/19 03:08,18/Jan/17 00:12,,,,,,0.16.1,0.17.0,,,,0,,,,,,,"If pig.temp.dir points to another fs, Pig fails. One such case is the defaultFS is set to s3, but use hdfs as temp dir. Error message:
{code}
org.apache.pig.backend.hadoop.executionengine.JobCreationException: ERROR 2017: Internal error creating job configuration.
        at org.apache.pig.backend.hadoop.executionengine.tez.TezJobCompiler.getJob(TezJobCompiler.java:141)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezJobCompiler.compile(TezJobCompiler.java:79)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher.launchPig(TezLauncher.java:194)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:304)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1431)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1416)
        at org.apache.pig.PigServer.execute(PigServer.java:1405)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:456)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:439)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:171)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:234)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
        at org.apache.pig.Main.run(Main.java:631)
        at org.apache.pig.Main.main(Main.java:177)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.IllegalArgumentException: Wrong FS: hdfs://pig-aws-devenv-5.openstacklocal:8020/tmp/daijy/temp-265134702/automaton-1.11-8.jar, expected: s3a://pig-aws-devenv
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:658)
        at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:478)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezResourceManager.addTezResource(TezResourceManager.java:82)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezResourceManager.addTezResources(TezResourceManager.java:106)
        at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezPlanContainer.getLocalResources(TezPlanContainer.java:107)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezJobCompiler.getJob(TezJobCompiler.java:95)
        ... 20 more
{code}",,,,,,,,,,,,,,,,,,,,01/Jun/16 23:36;daijy;PIG-4918-1.patch;https://issues.apache.org/jira/secure/attachment/12807558/PIG-4918-1.patch,14/Jan/17 08:22;daijy;PIG-4918-2.patch;https://issues.apache.org/jira/secure/attachment/12847488/PIG-4918-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-06-02 20:48:34.327,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Jan 18 00:12:05 UTC 2017,,,,,,,0|i2yuw7:,9223372036854775807,,,,,,,,,,01/Jun/16 23:37;daijy;Don't find an easy way to write a test since we need different fs to reproduce.,02/Jun/16 20:48;rohini;Can we still initialize once? i.e this.remoteFs = resourcesDir.getFileSystem(conf);  It is an unnecessary call to initialize everytime and if FileSystem cache is disabled for some reason can be quite expensive.,"13/Jan/17 14:03;rohini;[~daijy],
  Can you change this to
{code}
             this.resourcesDir = FileLocalizer.getTemporaryResourcePath(pigContext);
             this.remoteFs = resourcesDir.getFileSystem(conf);
{code}",14/Jan/17 18:44;rohini;+1,18/Jan/17 00:12;daijy;Patch committed to both trunk and 0.16 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,
Pig on Tez fail to remove temporary HDFS files in some cases,PIG-4916,12974972,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,01/Jun/16 23:27,21/Jun/17 09:15,14/Mar/19 03:08,06/Jun/16 22:14,,,,,,0.16.1,0.17.0,,,,0,,,,,,,"We saw the following stack trace when running Pig on S3:
{code}
2016-06-01 22:04:22,714 [Thread-19] INFO  org.apache.hadoop.service.AbstractService - Service org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl failed in state STOPPED; cause: java.io.IOException: Filesystem closed
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:808)
	at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:2034)
	at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1980)
	at org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:130)
	at org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFD.flush(FileSystemTimelineWriter.java:370)
	at org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFDsCache.flush(FileSystemTimelineWriter.java:485)
	at org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter.close(FileSystemTimelineWriter.java:271)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceStop(TimelineClientImpl.java:326)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
	at org.apache.tez.dag.history.ats.acls.ATSV15HistoryACLPolicyManager.close(ATSV15HistoryACLPolicyManager.java:259)
	at org.apache.tez.client.TezClient.stop(TezClient.java:582)
	at org.apache.pig.backend.hadoop.executionengine.tez.TezSessionManager.shutdown(TezSessionManager.java:308)
	at org.apache.pig.backend.hadoop.executionengine.tez.TezSessionManager$1.run(TezSessionManager.java:53)
2016-06-01 22:04:22,718 [Thread-19] ERROR org.apache.pig.backend.hadoop.executionengine.tez.TezSessionManager - Error shutting down Tez session org.apache.tez.client.TezClient@48bf833a
org.apache.hadoop.service.ServiceStateException: java.io.IOException: Filesystem closed
	at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:225)
	at org.apache.tez.dag.history.ats.acls.ATSV15HistoryACLPolicyManager.close(ATSV15HistoryACLPolicyManager.java:259)
	at org.apache.tez.client.TezClient.stop(TezClient.java:582)
	at org.apache.pig.backend.hadoop.executionengine.tez.TezSessionManager.shutdown(TezSessionManager.java:308)
	at org.apache.pig.backend.hadoop.executionengine.tez.TezSessionManager$1.run(TezSessionManager.java:53)
Caused by: java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:808)
	at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:2034)
	at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1980)
	at org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:130)
	at org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFD.flush(FileSystemTimelineWriter.java:370)
	at org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFDsCache.flush(FileSystemTimelineWriter.java:485)
	at org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter.close(FileSystemTimelineWriter.java:271)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceStop(TimelineClientImpl.java:326)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
	... 4 more
{code}
The job run successfully, but the temporary hdfs files are not removed.

[~cnauroth] points out FileSystem also use shutdown hook to close FileSystem instances and it might run before Pig's shutdown hook in Main. By switching to Hadoop's ShutdownHookManager, we can put an order on shutdown hook.

This has been verified by testing the following code in Main:
{code}
        ShutdownHookManager.get().addShutdownHook(new Runnable() {
            @Override
            public void run() {
                FileLocalizer.deleteTempResourceFiles();
            }
        }, priority);
{code}

Notice FileSystem.SHUTDOWN_HOOK_PRIORITY=10. When priority=9, Pig fail. When priority=11, Pig success.
",,,,,,,,,,,,,,,,,,,,01/Jun/16 23:29;daijy;PIG-4916-1.patch;https://issues.apache.org/jira/secure/attachment/12807555/PIG-4916-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-06-01 23:43:11.258,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Jun 06 22:14:01 UTC 2016,,,,,,,0|i2yuv3:,9223372036854775807,,,,,,,,,,01/Jun/16 23:30;daijy;I don't see a way to write a test case since this is non-deterministic in nature.,01/Jun/16 23:43;cnauroth;Hello [~daijy].  Thank you for the patch.  +1 (non-binding) from me.  I agree that it isn't feasible to write a unit test for this.  We have confirmation from your manual testing that it worked though.,"06/Jun/16 19:56;rohini;Does it compile on Hadoop 1.x? I don't see any ShutdownHooManager class or FileSystem.SHUTDOWN_HOOK_PRIORITY in https://github.com/apache/hadoop/blob/branch-1.2/src/core/org/apache/hadoop/fs/FileSystem.java. 

Can you please create a HadoopShims method addShutdownHook(Runnable runnable, int priorityIncrease) ? Hadoop 1.x will have the older implementation. Where priorityIncrease is the increase from FileSystem.SHUTDOWN_HOOK_PRIORITY  and will be 1. Can you change all of our other shutdown hooks to use the same? Based on the stacktrace TezSessionManager.shutdown also failed and would have left orphaned apps.","06/Jun/16 21:01;daijy;Good catch on Hadoop 1 compilation failure. In that sense, I'd only put the fix on 0.17 where we drop Hadoop 1.x support. I don't want to put a short lived code and make the process complex.

TezSessionManager.shutdown failure is due to the FileSystem closure and should be fixed. What's the benefit to switch all shutdown hooks to use the same? Code consistency?","06/Jun/16 21:04;rohini;bq. TezSessionManager.shutdown failure is due to the FileSystem closure and should be fixed. 
  It will not be fixed as only resource files deletion shutdown hook is run after FileSystem closure in the patch.",06/Jun/16 22:01;rohini;I can take care of that in PIG-4921 if you want. We can just go ahead and commit this.,06/Jun/16 22:12;daijy;Thanks Rohini. I will commit it shortly.,06/Jun/16 22:14;daijy;Patch committed to trunk. Thanks Rohini and Chris for review!,,,,,,,,,,,,,,,,,,,,
PigStorage incompatible with commons-cli-1.3 ,PIG-4909,12972191,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,knoguchi,knoguchi,knoguchi,23/May/16 20:59,08/Jun/16 20:48,14/Mar/19 03:08,24/May/16 13:21,,,,,,0.16.0,,impl,,,0,,,,,,,"Pig uses commons-cli-1.2.jar but when user overwrite the classpath with commons-cli-1.3.jar, job failed with 

{noformat}
Caused by: java.lang.IllegalArgumentException: Illegal option name ' '
at org.apache.commons.cli.OptionValidator.validateOption(OptionValidator.java:60)
{noformat}",,,,,,,,,,,,,,,,,,,,23/May/16 21:20;knoguchi;pig-4909-v01-notest.patch;https://issues.apache.org/jira/secure/attachment/12805750/pig-4909-v01-notest.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-05-23 22:28:21.152,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 24 13:21:47 UTC 2016,,,,,,,0|i2ye33:,9223372036854775807,,,,,,,,,,"23/May/16 21:20;knoguchi;This is coming from

{code:title=PigStorage.java}
174         Option overwrite = new Option("" "", ""Overwrites the destination."");
{code}

Reason for passing space over null was mentioned in
https://issues.apache.org/jira/browse/PIG-3988?focusedCommentId=14028212&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14028212
""Looks like commons-cli 1.0 passes "" "" instead of null to the Option constructor in OptionBuilder.""

This makes it a bit interesting such that for providing only long-option,

|| || "" ""(space) || null ||
| commons-cli-1.0.jar | O | X |
| commons-cli-1.1.jar | O | O |
| commons-cli-1.2.jar | O | O |
| commons-cli-1.3.1.jar | X | O |

At this point, I was stuck.  But stepping a bit back, with current source code, testing
\-overwrite"" (short option) as well as ""\--overwrite""(long option).

|| || -overwrite || --overwrite ||
| commons-cli-1.0.jar | X | O |
| commons-cli-1.1.jar | O | O |
| commons-cli-1.2.jar | O | O |

So ""\-overwrite"" and ""\--overwrite"" are both somehow supported.
If that's the case,  I think there's no harm in passing that as a param for short-option name.
Also reading original PIG-259, I don’t think there was any reason not to support short-option.

Uploading a simple patch.
Manually tested it with commons-cli-\{1.0,1.1,1.2,1.3.1\}.jar","23/May/16 22:28;rohini;bq. I don’t think there was any reason not to support short-option.
   Good fix instead of complicating things. Just one suggestion. Can we add -o as short option instead of -overwrite?",24/May/16 02:05;rohini;+1. [~knoguchi] pointed out that using -overwrite will keep it similar to rest of the options like -schema and -noschema which do not use the long form now.,24/May/16 13:21;knoguchi;Thanks for the review Rohini.  Committed to 0.16 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,
JythonFunction refers to Oozie launcher script absolute path,PIG-4908,12972104,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,23/May/16 16:22,14/Jun/16 20:19,14/Mar/19 03:08,23/May/16 22:39,,,,,,0.16.0,,,,,0,,,,,,,"  We had a scenario where a user had multiple udfs all named udfs.py. JythonFunction was referring to the absolute localized path of udfs.py in the Oozie launcher. Tasks which ran on a node and which had a different version of udfs.py localized to same path (hashcode conflict) as the Oozie launcher failed.  We should be referring to relative path of the files. The current code check is the canonical path starts with cwd, but that does not work as the files are downloaded to a different location and symlinked to the current working directory of the task attempt.",,,,,,,,,,,,,,,,,,,,23/May/16 16:29;rohini;PIG-4908-1.patch;https://issues.apache.org/jira/secure/attachment/12805661/PIG-4908-1.patch,24/May/16 18:57;rohini;PIG-4908-2-fixtest.patch;https://issues.apache.org/jira/secure/attachment/12805942/PIG-4908-2-fixtest.patch,31/May/16 17:36;rohini;PIG-4908-3-fixtest.patch;https://issues.apache.org/jira/secure/attachment/12807211/PIG-4908-3-fixtest.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-05-23 21:05:01.443,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jun 14 20:19:00 UTC 2016,,,,,,,0|i2ydjr:,9223372036854775807,,,,,,,,,,"23/May/16 21:05;daijy;+1. The ""starts with cwd"" check is very buggy. There are other situations such as ""xxx/.././"" which will fail the match.",23/May/16 22:39;rohini;Committed to branch 0.16 and trunk. Thanks for the review Daniel.,24/May/16 18:57;rohini;The ./ and ../ paths actually have issue with this patch and Scripting e2e tests failed. Attached PIG-4908-2-fixtest.patch to revert back to the old behavior if we have those.,"25/May/16 15:01;knoguchi;{quote}
The ./ and ../ paths actually have issue with this patch and Scripting e2e tests failed. Attached PIG-4908-2-fixtest.patch to revert back to the old behavior if we have those.
{quote}
Confirmed this fixes the e2e failures I was seeing. +1.

But for this jira, can you share the steps on how the conflict happened?  
Without a testcase, I'm not seeing how the incorrect udfs.py was picked.","25/May/16 18:22;rohini;bq. Without a testcase, I'm not seeing how the incorrect udfs.py was picked.
   Explanation is in description. We put the absolute path in the JythonFunction in the pig client (Oozie launcher). If a different file with same name exists on that backend task node in that path it runs into errors. It is hard to write a testcase for it as it you can't have two different files in the local filesytem on the same machine.

I am going to look a little bit more into ScriptEngine.getScriptAsStream() to see if there is a better way to fix this.","31/May/16 17:36;rohini;  When absolute path is used, the nameInJar finally has the leading / removed and added to jar. For eg:  /tmp/sleep.py will be added to jar as tmp/sleep.py so that it is accessible through ScriptEngine.class.getResourceAsStream(scriptPath) when the jar is in classpath. 

Problem was we were looking for the absolute path first before trying the jar. So additionally fixed that code to look for local file first if frontend else look at the last in PIG-4908-3-fixtest.patch.

Saw that during compilation we were initializing the function too many times with the different visitors. Will file a separate jira to cut down on that.","31/May/16 19:44;daijy;+1.

Further, I don't see a reason to make relative path different. We shall remove the relative path handling code in a separate ticket, which is already very fragile, eg:
path.indexOf(""."" + File.separator)
Why path contain . make a difference.","31/May/16 19:52;rohini;bq. Why path contain . make a difference.
  It does not work with paths in jar and getResourceAsStream.","31/May/16 20:03;rohini;bq. We shall remove the relative path handling code in a separate ticket, 
  I think we should be ok to do that. It will have longer path names in the jar which should not be a problem. Relative path names allow picking up from local directory when the scripts are part of distributed cache files though. Cannot think of anything else now, but to be on the safer side will do after this release.

Committed PIG-4908-3-fixtest.patch to trunk and branch-0.16. Thanks for the review [~daijy] and [~knoguchi].","14/Jun/16 20:14;knoguchi;bq. but to be on the safer side will do after this release.

[~rohini], should we have a new jira so that we won't forget about it? ",14/Jun/16 20:19;rohini;Already made a note to take care of that in PIG-4908. ,,,,,,,,,,,,,,,,,
"Fix UT failures on 0.16 branch: TestTezGraceParallelism, TestPigScriptParser",PIG-4902,12971852,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,22/May/16 06:57,15/Jun/16 20:23,14/Mar/19 03:08,24/May/16 18:12,,,,,,0.16.0,0.17.0,,,,0,,,,,,,"Saw UT failures in TestPigScriptParser and TestTezGraceParallelism on 0.16 branch and trunk. 

TestTezGraceParallelism failures are introduced by PIG-4884. The vertex output data size is larger since DistinctCombiner does not do column projection. Also due to the plan change, the vertex name also changed.",,,,,,,,,,,,,,,,,,,,22/May/16 07:01;daijy;PIG-4902-1.patch;https://issues.apache.org/jira/secure/attachment/12805508/PIG-4902-1.patch,24/May/16 06:05;daijy;PIG-4902-2.patch;https://issues.apache.org/jira/secure/attachment/12805819/PIG-4902-2.patch,24/May/16 18:12;daijy;PIG-4902-3.patch;https://issues.apache.org/jira/secure/attachment/12805937/PIG-4902-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-05-23 05:42:57.972,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 24 18:12:16 UTC 2016,,,,,,,0|i2ybzr:,9223372036854775807,,,,,,,,,,"23/May/16 05:42;rohini;bq. The vertex output data size is larger since DistinctCombiner does not do column projection.
   Did not understand why the data size is larger. Distinct is on the whole alias right? What column projection are we talking about?. I can revisit  PIG-4884 if there is data size increase and try fixing that.","24/May/16 06:05;daijy;You are right, it is not the runtime data size change. It is due to a missing change in TezOperDependencyParallelismEstimator for DistinctCombiner.","24/May/16 06:17;rohini;Thanks for checking it. Was worried that I had missed something.

+1.  Can you format the line if (!edge.combinePlan.isEmpty()||edge.needsDistinctCombiner())  before checkin.",24/May/16 18:12;daijy;Committed to both 0.16 branch and trunk with a format change. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
Scope of param substitution for run/exec commands,PIG-4897,12970170,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,16/May/16 22:09,21/Jun/17 09:15,14/Mar/19 03:08,10/Jan/17 21:15,,,,,,0.17.0,,,,,0,,,,,,,"After PIG-3359, pig param substitution became global in that parameter declared in the pig script called from {{run}} or {{exec}} would live after that script finishes.  

This created an interesting situation.
{code:title=test1.pig}
exec -param output=/tmp/deleteme111 test1_1.pig
exec -param output=/tmp/deleteme222 test1_1.pig
{code}

{code:title=test1_1.pig}
%default myout '$output.out';
A = load 'input.txt' as (a0:int);
store A into '$myout';
{code}

Running {{test1.pig}} would try to run two jobs that both tries to write to /tmp/deleteme111 and fail.  (Second param output=/tmp/deleteme222 is ignored.)",,,,,,,,,,,,,PIG-5028,,,,,,,18/May/16 22:10;knoguchi;pig-4897-v01-notestyet.patch;https://issues.apache.org/jira/secure/attachment/12804804/pig-4897-v01-notestyet.patch,20/May/16 21:15;knoguchi;pig-4897-v02.patch;https://issues.apache.org/jira/secure/attachment/12805307/pig-4897-v02.patch,23/May/16 18:56;knoguchi;pig-4897-v03.patch;https://issues.apache.org/jira/secure/attachment/12805716/pig-4897-v03.patch,21/Jun/16 19:03;knoguchi;pig-4897-v04.patch;https://issues.apache.org/jira/secure/attachment/12812272/pig-4897-v04.patch,29/Jul/16 17:18;knoguchi;pig-4897-v05.patch;https://issues.apache.org/jira/secure/attachment/12821027/pig-4897-v05.patch,09/Sep/16 20:36;knoguchi;pig-4897-v06.patch;https://issues.apache.org/jira/secure/attachment/12827826/pig-4897-v06.patch,12/Sep/16 17:45;knoguchi;pig-4897-v07.patch;https://issues.apache.org/jira/secure/attachment/12828078/pig-4897-v07.patch,13/Sep/16 19:32;knoguchi;pig-4897-v08.patch;https://issues.apache.org/jira/secure/attachment/12828317/pig-4897-v08.patch,29/Sep/16 17:24;knoguchi;pig-4897-v09.patch;https://issues.apache.org/jira/secure/attachment/12830931/pig-4897-v09.patch,,,9.0,,,,,,,,,,,,,,,,,,,2016-06-17 17:19:47.445,,,no_permission,,,,,,,,,,,,,9223372036854775807,Incompatible change,Reviewed,,,Tue Jan 10 21:15:50 UTC 2017,,,,,,,0|i2y1mf:,9223372036854775807,"With run/exec command, parameters are now scoped and caller would not see the parameters declared within the callee's scripts.",,,,,,,,,"18/May/16 21:06;knoguchi;A better example.

{code:title=test10.pig}
exec HADOOPPF-12103/test10_1.pig
exec HADOOPPF-12103/test10_2.pig
{code}
{code:title=test10_1.pig}
%default output '/tmp/deleteme111';
A = load 'input.txt' as (a0:int);
store A into '$output'; --writes to /tmp/deleteme111
{code}
{code:title=test10_2.pig}
%default output '/tmp/deleteme222';
A = load 'input.txt' as (a0:int);
store A into '$output'; --this tries to write to /tmp/deleteme111 unexpectedly.
{code}","18/May/16 22:10;knoguchi;Attaching a patch that reverts part of PIG-3359 and make 'run/exec command' NOT to share the param substitution variables outside.

This would be considered a ""Incompatible change"" if any users were depending on this param sharing among run/exec-ed scripts.

I'll add a testcase tomorrow but I'm still not sure if this is a right approach.",20/May/16 21:15;knoguchi;Attaching {{pig-4897-v02.patch}} with a test case.,"23/May/16 18:56;knoguchi;{code}
assertEquals(""Two jobs should have run."", 2, stats.getNumberJobs());
{code}

This was clearly wrong for local-tez mode since it only run one job. 
Took out the line since we already have necessary check for making sure we have two unique outputs. ","23/May/16 21:34;knoguchi;Setting ""incompatible change"" flag.  
Please let me know your thought.

If we have any users started using 'run/exec' for setting some params after PIG-3359, this patch will break it.

For example, 

{code:title=test.pig}
run setparams.pig
A = load '${useparamfrompreviousrun}' using PigStorage(); 
...
{code}
","25/May/16 21:04;knoguchi;Rohini brought up another (more valid) case that would break with my patch.

This works.
{noformat}
knoguchi > pig -param output=/tmp/deleteme test12-1.pig
{noformat}
{code:title=test12-1.pig}
import 'test12-2.macro';
A = load 'input.txt';
mymacro(A);
{code}
{code:title=test12-2.macro}
DEFINE mymacro (A) RETURNS void {
store A into '$output';
};
{code}

But calling the above pig script with run/exec would now break even though it is still passing output param.
{noformat}
knoguchi > pig test12.pig
{noformat}
{code:title=test12.pig}
run -param output=/tmp/deleteme111 test12-1.pig
{code}
""Macro inline failed for macro 'mymacro'. Reason: Undefined parameter : output""

This is happening since only the param set at the root script (test12.pig) is propagated to the macro with my patch.

One approach I can think of to satisfy this test case as well as my [previous case|https://issues.apache.org/jira/browse/PIG-4897?focusedCommentId=15289823#comment-15289823] would be to introduce a ""scope"" so that param is propagated to called script but then dropped when the corresponding script exits.

But that would still leave 'exec setparams.pig' like usage.  Do we need to support this usecase? ","17/Jun/16 17:19;rohini;Makes sense to have scope for run/exec. Anything defined in the main script is always applicable, but those introduced in the run/exec scripts are scoped and only apply or override the main settings during its execution.","21/Jun/16 19:03;knoguchi;{quote}
Makes sense to have scope for run/exec. Anything defined in the main script is always applicable, but those introduced in the run/exec scripts are scoped and only apply or override the main settings during its execution.
{quote}
Thanks Rohini.  Attaching {{pig-4897-v04.patch}} that would add the scope for run/exec calls.
Now the patch matches with what the description says, ""Scope of param substitution for run/exec commands"".

In addition to this scope change, I also took out one public constructor which seemed not used anywhere.
{code}
public PreprocessorContext(Map<String, String> paramVal)
{code}","13/Jul/16 23:11;rohini;Few minor comments
   - We would always want to deleteOnExit. So not sure if that needs to be a method argument
   - Can we have one testcase with run -param output=xxx which will override the %default output value in the script?
  - Can we have one testcase for the macro case?","29/Jul/16 17:18;knoguchi;{quote}
* We would always want to deleteOnExit. So not sure if that needs to be a method argument
* Can we have one testcase with run -param output=xxx which will override the %default output value in the script?
{quote}
Attaching a new patch that does the above.  (pig-4897-v05.patch)
However, 

bq. Can we have one testcase for the macro case?
This turns out to be a major feedback.

After writing a test for this, realized that PigMacro has almost nothing to do with the changes I'm making here. 
General param saving and substitution happens from PigScriptParser.jj but Macro expansion is done through QueryParser.g.  I've seen it mentioned elsewhere but never hit me how this would affect my patch here.

I wish we didn't enable "" global param substitutions in macros."" in PIG-3359.  It's pretty confusing even without this jira.

For example, 

{code:title=example1.pig}
DEFINE mymacro (A) RETURNS void {
  store $A into '${output}';
}

A = load 'input.txt'
%declare output '/tmp/abc';
mymacro(A);
%declare output '/tmp/def';
{code}

would save the output to /tmp/def.  
Irrespective of where the macro is defined or called, it'll use the values defined after all the scripts are read.
","29/Jul/16 17:32;knoguchi;[~daijy] can correct me if I'm mistaken, but to correct the PigMacro behavior, we probably need PIG-2597 where we want to move out of javacc (PigScriptParser.jj) to ANTLR(QueryParser.g). 

And coming back to non-macro, general param substitution issue with run/exec command, we have three options so far.

(1) All global.  (current behavior since 0.12 from PIG-3359)
(2) All private.  (pig-4897-v03.patch).  Only way to transfer params from one script to another would be to pass it through commandline params.   run/exec -param k=v abc.pig 
(3) Scoped. (pig-4897-v04/05.patch).   

I don't like (1) given the example I've shown in the description and the [first comment|https://issues.apache.org/jira/browse/PIG-4897?focusedCommentId=15289823#comment-15289823].

I like (2) the best with no sharing but I'm afraid it may break too many scripts that started to depend on (1) behavior.  
(I don't know how many)

(3) is a compromise and hopefully won't break too many scripts who depend on (1) behavior but still fixes the two example cases from this jira.

As for fixing the PigMacro, I prefer to create a new jira and revisit once we have PIG-2597.","06/Aug/16 06:15;daijy;bq. can correct me if I'm mistaken, but to correct the PigMacro behavior, we probably need PIG-2597 where we want to move out of javacc (PigScriptParser.jj) to ANTLR(QueryParser.g)
Yes, make macro use all commands/statements is one of the major goal of PIG-2597.","06/Sep/16 22:19;rohini;I think we should only scope for exec and not run. For run we should overwrite the passed parameter as declare inside run should be available outside it as per definition from Pig book. 

{code}
exec [-param param_name = param_value] [-param_file filename] [script] Execute the Pig Latin script script. Aliases defined in script are not imported into Grunt. This command is useful for testing your Pig Latin scripts while inside a Grunt session. You can also run exec without paramter to only run the Pig statements before exec. The difference is Pig will not combine them with the rest of the script in execution.
run [-param param_name = param_value] [-param_file filename] script Execute the Pig Latin script script in the current Grunt shell. Thus all aliases referenced in script are available to Grunt, and the commands in script are accessible via the shell history. This is another option for testing Pig Latin scripts while inside a Grunt session.
{code}","07/Sep/16 14:11;knoguchi;bq. I think we should only scope for exec and not run. For run we should overwrite the passed parameter as declare inside run should be available outside it as per definition from Pig book.

Definition only talks about ""aliases referenced in script"" but not on params or macros.

Also, it's not black and white in that, what would you do when we have both.
{noformat}
script1.pig 
      -> exec script2.pig 
               -> run script3.pig 
                         -> exec script4.pig
{noformat}
Params defined inside script3.pig would be visible to script2.pig but not to script1.pig ? Or should it be global and script1.pig should be able to see it?",08/Sep/16 20:36;rohini;Makes sense. We can just go with the current patch which keeps things simple and clear. testScopeOfParamWithNestedRunCommand is in v04 patch but missing in v05 though. Can you also create a new jira for the macro case and update the comment for the testScopeOfParamWithMacro with that jira number.,"08/Sep/16 21:01;knoguchi;Thanks Rohini!  I'll create a new jira for the macro and patch shortly. 

But given this is an incompatible change if we have user depending on the global param behavior, can I have one more +1?  [~daijy], appreciate if you could take a brief look.  (Not for the patch itself but for the idea of making the parameters scoped for exec&run.)",08/Sep/16 21:32;rohini;Sure. I don't see it as a incompatible change though :). See it as a bug introduced in Pig 0.12 as it broke old scripts and doing something no user would expect. ,"09/Sep/16 20:36;knoguchi;{quote}
Makes sense. We can just go with the current patch which keeps things simple and clear. testScopeOfParamWithNestedRunCommand is in v04 patch but missing in v05 though. Can you also create a new jira for the macro case and update the comment for the testScopeOfParamWithMacro with that jira number.
{quote}
Thanks Rohini.
Added back testScopeOfParamWithNestedRunCommand, created PIG-5028 for tracking the macro issue and updated the comment in testScopeOfParamWithMacro.","09/Sep/16 22:36;rohini;Just realized that the testCommandLineParamOverwritingDefault is not what I intended. It calls PigRunner.run with -param output=<path>. What I wanted was a mainscript.pig that had ""run runscript1.pig -param output=<path>"" similar to testScopeOfParamWithRunCommand. Could you just update that test with a additional run command which takes a parameter as below and asserts for it?

{code}
 +        File mainscript = Util.createFile(""mainscript.pig"",                 
  +                  new String[] { ""run "" + script1.getAbsolutePath() + "";"",  
  +                                 ""run "" + script2.getAbsolutePath() + "";"" 
 +                                 ""run "" + script1.getAbsolutePath() + -param output output3.getAbsolutePath()"";"" 
});
{code} ","12/Sep/16 17:45;knoguchi;bq. Just realized that the testCommandLineParamOverwritingDefault is not what I intended.

Sorry, misunderstood your initial comment of 
  ""Can we have one testcase with run -param output=xxx which will override the %default output value in the script?""
as to, we should be testing '-param' in general. 

Leaving that test case but also updated {{testScopeOfParamWithNestedRunCommand}} with run -param call for script2.pig so that your case is covered.
",13/Sep/16 19:32;knoguchi;Rohini wanted one specific test with run+param overwriting '%default'.  Added {{testRunWithParamOverwritingDefault}}.,"13/Sep/16 20:02;rohini;+1. [~daijy], can you review once?","15/Sep/16 07:37;daijy;+1, the new behavior is more useful. We shall also document it.","16/Sep/16 16:47;rohini;bq. We shall also document it.
  Thanks for reminding. We missed that in the patch.

[~knoguchi],
   There are some test failures with TestMacroExpansion. Can you look into them? ","29/Sep/16 17:24;knoguchi;bq. We shall also document it.

Added. Please check my English to see if it makes sense.

bq. There are some test failures with TestMacroExpansion. Can you look into them?

Worked around the issue by skipping scoping when the 'loadscript' method is called outside of run/exec.

But more I look, I cannot wait to drop javacc in PIG-2597...",03/Oct/16 20:14;daijy;+1 for doc addition.,12/Dec/16 23:40;rohini;+1.,"10/Jan/17 21:15;knoguchi;Thanks for the review Rohini, Daniel!

Committed to trunk."
Param substitution ignored when redefined ,PIG-4896,12970153,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,16/May/16 21:22,21/Jun/17 09:15,14/Mar/19 03:08,15/Jun/16 19:21,,,,,,0.17.0,,parser,,,0,,,,,,,"After PIG-3359, the way we cache parameter substitution results changed a bit.

{code:title=test.pig}
A = load 'input.txt' ;
%declare output '/tmp/abc';
%declare actualoutput '$output.out';
store A into '$actualoutput';

%declare output '/tmp/def';
%declare actualoutput '$output.out';
store A into '$actualoutput';

%declare number '1d';
%declare shellout `bash -c ""date -v-$number +'%Y%m%d_%H:%S_%s'; sleep 1"" `
store A into '$shellout';

%declare shellout `bash -c ""date -v-$number +'%Y%m%d_%H:%S_%s'; sleep 1"" `
store A into '$shellout';

%declare number '2d';
%declare shellout `bash -c ""date -v-$number +'%Y%m%d_%H:%S_%s'; sleep 1"" `
store A into '$shellout';
{code}

Result from pig 0.11 (almost correct)
{panel}
A = load 'input.txt' ;
store A into '/tmp/abc.out';
store A into '/tmp/def.out';
store A into '20160515_17:32_1463346752';
store A into '20160515_17:33_1463346753'; {color:red}NO{color}
store A into '20160514_17:34_1463260354';
{panel}
Result from trunk 
{panel}
A = load 'input.txt' ;
store A into '/tmp/abc.out';
store A into '/tmp/abc.out';  {color:red} NO{color}
store A into '20160515_15:10_1463338810';
store A into '20160515_15:10_1463338810';
store A into '20160515_15:10_1463338810'; {color:red}NO{color}
{panel}",,,,,,,,,,,,,,,,,,,,16/May/16 21:54;knoguchi;pig-4896-v01.patch;https://issues.apache.org/jira/secure/attachment/12804267/pig-4896-v01.patch,17/May/16 18:42;knoguchi;pig-4896-v02.patch;https://issues.apache.org/jira/secure/attachment/12804485/pig-4896-v02.patch,17/May/16 19:20;knoguchi;pig-4896-v03.patch;https://issues.apache.org/jira/secure/attachment/12804499/pig-4896-v03.patch,25/May/16 19:27;knoguchi;pig-4896-v04.patch;https://issues.apache.org/jira/secure/attachment/12806195/pig-4896-v04.patch,25/May/16 20:12;knoguchi;pig-4896-v05.patch;https://issues.apache.org/jira/secure/attachment/12806202/pig-4896-v05.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2016-05-24 22:11:01.196,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Jun 15 19:21:12 UTC 2016,,,,,,,0|i2y1in:,9223372036854775807,,,,,,,,,,"16/May/16 21:35;knoguchi;For pig 0.11 shell call, it doesn't reuse the previous result because PreprocessorContext simply compare the result of the shell call {{20160515_17:32_1463346752'}} with the shell command line {{bash \-c ""date \-v\-$number +'%Y%m%d_%H:%S_%s'; sleep 1""}} and simply decides they are different. 

They should be comparing the command themselves. 

For trunk, non-shell parameter substitution is comparing {{$output.out}} and {{$output.out}} and deciding they represent the same value even though{{$output}} was different for each.  Same for its shell calls. ","16/May/16 21:54;knoguchi;Original issue was from PIG-3359 where we added 
{quote}
There's a slight complication with the handling of the warnings in that ""val"" passed to those functions in PreprocessorContext is the raw string, which might not be the same as the val put into param_val. I added a separate hashtable ""param_source"" to keep track of this original argument, and if it's seen again to skip it instead of logging the warning.
{quote}

For non-shell {{processOrdLine}}, I've simply took out this ""param_source"" and compare the result from ""param_value"".
For shell {{processShellCmd}}, I've kept the ""param_source' but using the value after the param replacement so that we only use the cache result when command-line matches,

Running test.",17/May/16 18:21;knoguchi;Testcase in my patch doesn't work on linux.  Need to think of one that works with my mac laptop and linux.,17/May/16 18:42;knoguchi;Changed the test case so that it would run on both platforms. (mac/linux),17/May/16 19:20;knoguchi;Sorry for the spam.  Took out two logging I've added for my debugging.,"24/May/16 22:11;rohini;bq. store A into '20160515_17:33_1463346753'; NO
   I would say that we should go with Pig 0.11 on this behavior and it looks correct to me. User most likely will not be redefining the same shell cmd if value is expected to be same.","25/May/16 19:27;knoguchi;{quote}
bq. store A into '20160515_17:33_1463346753'; NO
I would say that we should go with Pig 0.11 on this behavior and it looks correct to me. User most likely will not be redefining the same shell cmd if value is expected to be same.
{quote}
Got it.  
Uploading {{pig-4896-v04.patch}} which now 
* Re-executes even when command-line is identical as long as ovewrite is set
* Updating a testcase to make sure this re-execution happen
* Updating the warning message of ovewrites to show the previous and new values.",25/May/16 20:12;knoguchi;Rohini pointed out that testcase is testing equal instead of not-equal which makes a big difference.  Updated. ,"25/May/16 20:37;rohini;Shouldn't this be not equals?
{code}
 assertEquals(""Identical shell param should be reexecuted."",
+                     filenames[0],
+                     filenames[1]);
{code}","14/Jun/16 19:32;knoguchi;bq. Shouldn't this be not equals?
Thanks [~rohini].
This is fixed with my last {{pig-4896-v05.patch}}.  When you have time, can you take a (hopefully final) look? ",14/Jun/16 20:16;rohini;+1,"15/Jun/16 19:21;knoguchi;Thanks for the review Rohini.
Committed to trunk(0.17).",,,,,,,,,,,,,,,,
User UDFs relying on mapreduce.job.maps broken in Tez,PIG-4895,12969700,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,13/May/16 23:49,08/Jun/16 20:48,14/Mar/19 03:08,16/May/16 18:56,,,,,,0.16.0,,,,,0,,,,,,,"  Found one of the user UDFs relying on mapreduce.job.maps to perform batching is broken.

",,,,,,,,,,,,,,,,,,,,13/May/16 23:51;rohini;PIG-4895-1.patch;https://issues.apache.org/jira/secure/attachment/12803978/PIG-4895-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-05-15 21:42:21.294,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon May 16 18:56:20 UTC 2016,,,,,,,0|i2xypz:,9223372036854775807,,,,,,,,,,15/May/16 21:42;daijy;+1,16/May/16 18:56;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
removing /tmp/output before UT,PIG-4892,12967604,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,11/May/16 18:05,08/Jun/16 20:48,14/Mar/19 03:08,11/May/16 19:52,,,,,,0.16.0,,build,,,0,,,,,,,"Quite frequently, we saw folder /tmp/output exists on test machine and a couple of tests fail with the message:

Output Location Validation Failed for: 'file:///tmp/output More info to follow:
Output directory file:/tmp/output already exists

I'd like to remove /tmp/output before the UT.",,,,,,,,,,,,,,,,,,,,11/May/16 18:07;daijy;PIG-4892-1.patch;https://issues.apache.org/jira/secure/attachment/12803468/PIG-4892-1.patch,11/May/16 19:47;daijy;PIG-4892-2.patch;https://issues.apache.org/jira/secure/attachment/12803497/PIG-4892-2.patch,11/May/16 19:51;daijy;PIG-4892-3.patch;https://issues.apache.org/jira/secure/attachment/12803499/PIG-4892-3.patch,24/May/16 19:23;daijy;PIG-4892-TestLoad.patch;https://issues.apache.org/jira/secure/attachment/12805949/PIG-4892-TestLoad.patch,19/May/16 00:37;rohini;PIG-4892-fixtest.patch;https://issues.apache.org/jira/secure/attachment/12804841/PIG-4892-fixtest.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2016-05-11 19:25:04.48,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 24 19:28:54 UTC 2016,,,,,,,0|i2xltz:,9223372036854775807,,,,,,,,,,11/May/16 19:25;rohini;Checked our code. We use '/tmp/output' only in TestTezJobControlCompiler and TestTezCompiler which only build plans and never create the directory. So /tmp/output should be coming from some other project running on the jenkins host at the same time. Deleting it might make their tests fail. We should change the output path in those tests to /tmp/pigoutput or something to avoid the issue.,11/May/16 19:47;daijy;That's doable. Also remove the /tmp/pigoutput before the test suit to make sure it doesn't affect by the last run.,"11/May/16 19:48;rohini;You forgot to revert this back after regenerating

+    private boolean generate = true;","11/May/16 19:51;daijy;Yes, bring it back.",11/May/16 19:52;daijy;Patch committed to trunk. Thanks Rohini for review!,19/May/16 00:37;rohini;testStoreLoadMultiple had two different golden files - one for jdk 7 and one for jdk8 and the jdk8 one was not updated. Fixed that in PIG-4892-fixtest.patch,19/May/16 22:16;daijy;+1 for PIG-4892-fixtest.patch.,21/May/16 23:23;rohini;Committed PIG-4892-fixtest.patch to trunk and branch-0.16.,24/May/16 19:23;daijy;Another case in TestLoad.,24/May/16 19:24;rohini;+1,24/May/16 19:28;daijy;PIG-4892-TestLoad.patch committed to both trunk and 0.16 branch.,,,,,,,,,,,,,,,,,
Replacing backslash fails as lexical error ,PIG-4889,12966576,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,10/May/16 01:30,08/Jun/16 20:48,14/Mar/19 03:08,12/May/16 21:48,0.11.2,0.12.1,0.13.0,0.14.0,0.15.0,0.16.0,,parser,,,0,,,,,,,"{code:title:test.pig}
A = load 'input.txt' as (url:chararray);
B = foreach A generate REPLACE(url, '\\\\', '') as url2:chararray;
DUMP B;
{code}

fails with 
{panel}
2016-05-09 21:27:09,670 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Lexical error at line 4, column 0.  Encountered: <EOF> after : """"
{panel}",,,,,,,,,,,,,,,,,,,,10/May/16 03:07;knoguchi;pig-4889-v01_notestcase.patch;https://issues.apache.org/jira/secure/attachment/12803142/pig-4889-v01_notestcase.patch,10/May/16 18:51;knoguchi;pig-4889-v02.patch;https://issues.apache.org/jira/secure/attachment/12803290/pig-4889-v02.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-05-12 21:13:35.858,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu May 12 21:48:01 UTC 2016,,,,,,,0|i2xfhz:,9223372036854775807,,,,,,,,,,"10/May/16 01:35;knoguchi;This started failing after PIG-2507 when it _correctly_ recognized ""IN_STRING"" state except that this string state is missing the handling of quoted backslash.

","10/May/16 03:07;knoguchi;Uploading a patch that would match 
{noformat}
\\
{noformat} 
over 
{noformat}
\'
{noformat}

I'll add some tests tomorrow.","10/May/16 18:51;knoguchi;Uploading {{pig-4889-v02.patch}}.
I've got so confused with how the escaping work in java and pig, decided to have three tests with input&output check.

* testBackSlashSingleQuote has been working fine.
* testBackSlashOnly has been failing for years
* testBackSlashReplace started to fail after PIG-2507

",12/May/16 21:13;rohini;+1,12/May/16 21:48;knoguchi;Thanks for the review Rohini!  Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,
Line number off when reporting syntax error inside a macro,PIG-4888,12965434,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,knoguchi,knoguchi,knoguchi,05/May/16 22:27,08/Jun/16 20:48,14/Mar/19 03:08,12/May/16 16:39,,,,,,0.16.0,,parser,,,0,,,,,,,"{panel}
  1 /*
  2  * extra lines to offset the line number for the macro
  3  *
  4  *
  5  */
  6
  7
  8 define mymacro() returns void {
  9 A = load 'x' as ( u:int, v:long, w:bytearray);
 10 B = limit A 100;
 11 C = {color:red}filter_typo{color} B by 2 > 1;
 12 D = load 'y' as (d1, d2);
 13 E = join C by ( $0, $1 ), D by ( d1, d2 ) using 'replicated' parallel 16;
 14 F = store E into 'output';
 15 };
 16 mymacro();
{panel}

This fails with 
{panel}
""2016-05-05 22:25:28,390 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <file test.pig, line 16> Failed to parse macro 'mymacro'. Reason: <file /homes/knoguchi/test.pig, line 4, column 0>  Syntax error, unexpected symbol at or near 'C'""
{panel}
{{test.pig, line 4,}} should have been line 11.
",,,,,,,,,,,,,,,,,,,,05/May/16 22:29;knoguchi;pig-4888-v01.patch;https://issues.apache.org/jira/secure/attachment/12802550/pig-4888-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-05-11 00:14:37.215,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu May 12 16:39:35 UTC 2016,,,,,,,0|i2x8if:,9223372036854775807,,,,,,,,,,05/May/16 22:29;knoguchi;Antlr's {{RecognitionException}} doesn't seem to consider the line offset.  Adding this offset when showing the error message.,11/May/16 00:14;daijy;+1,12/May/16 16:39;knoguchi;Thanks for the review Daniel!   Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,
Parameter substitution skipped with glob on register,PIG-4887,12965340,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,05/May/16 18:07,08/Jun/16 20:48,14/Mar/19 03:08,13/May/16 01:46,,,,,,0.16.0,,,,,0,,,,,,,"For a script with globbing on register followed by multi-line comments, parameter substitution doesn't work.
{code:title=test.pig}
register /Users/knoguchi/mydir/*.jar; 

A = load '$input' as (a1:int);
/* comment here */
store A into '$output';

{code}

Trying to run this would result in 
{panel}
org.apache.pig.backend.executionengine.ExecException: ERROR 2118: Input path does not exist: file:/Users/knoguchi/git/pig/$input
{panel}

Running with {{-dryrun}}, we can see that parameter between the globbing and the multi-line style comment is ignored. 

{noformat}
% pig -dryrun  -param input=input.txt -param output=output.txt  test.pig
...
2016-05-05 14:04:34,613 [main] INFO  org.apache.pig.Main - Dry run completed. Substituted pig script is at test.pig.substituted
...
% cat test.pig.substituted
register /Users/knoguchi/mydir/*.jar;

A = load '$input' as (a1:int);
/* comment here */
store A into 'output.txt';

%
{noformat}",,,,,,,,,,,,,,,,,,,,05/May/16 18:26;knoguchi;pig-4887-v01.patch;https://issues.apache.org/jira/secure/attachment/12802483/pig-4887-v01.patch,12/May/16 21:41;knoguchi;pig-4887-v02.patch;https://issues.apache.org/jira/secure/attachment/12803753/pig-4887-v02.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-05-12 21:16:51.947,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri May 13 01:46:20 UTC 2016,,,,,,,0|i2x7xj:,9223372036854775807,,,,,,,,,,"05/May/16 18:26;knoguchi;This is yet another parsing issue which is based on javacc.
In PIG-2597, we're planning to move away from javacc completely and move to ANTLR but that hasn't happened for years...

With limited understanding of javacc, here's my patch.
I believe this is happening when PigFileParser's tokenizer matches the longest match as comment. Starting from {{/*.jar}} to {{here */}}.

{panel}
register /Users/knoguchi/mydir{color:red}/*.jar;

A = load '$input' as (a1:int);
/* comment here */{color}
store A into '$output';
{panel}

In the patch, treating register as a special case and avoids next word to be treated as a comment.",12/May/16 21:16;rohini;Load also can have /* . What happens in that case?,"12/May/16 21:41;knoguchi;bq. Load also can have /* . What happens in that case?

Didn't think of that, but looking at the PigFileParser, this case is handled since the load path is quoted.  

In any cases, it doesn't hurt to have a test for this in case it breaks in the future.  Uploading {{pig-4887-v02.patch}}.  (Also, fixed one typo when printing the test name inside log.info.)",12/May/16 22:50;rohini;+1,13/May/16 01:46;knoguchi;Thanks for the review Rohini!  Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,
TestBuiltin.testUniqueID failing on hadoop-1.x,PIG-4881,12963518,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,28/Apr/16 15:21,08/Jun/16 20:48,14/Mar/19 03:08,12/May/16 16:42,,,,,,0.16.0,,,,,0,,,,,,,"-Patch from PIG-4819 that changed RANDOM() broke TestBuiltin.testURIWithCurlyBrace which I fixed in PIG-4833 which broke TestBuiltin.testUniqueID in hadoop1.x mode...-

-Hopefully I can fix this without breaking another test...-

TestBuiltin.testUniqueID has been failing on hadoop-1.x since the test was _fixed_ in PIG-4819.
(It used to ignore the results and always return success.)",,,,,,,,,,,,,,,,,,,,28/Apr/16 22:09;knoguchi;pig-4881-v01.patch;https://issues.apache.org/jira/secure/attachment/12801330/pig-4881-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-05-11 00:14:18.884,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu May 12 16:42:59 UTC 2016,,,,,,,0|i2wwov:,9223372036854775807,,,,,,,,,,"28/Apr/16 15:27;knoguchi;{panel}
Testcase: testUniqueID took 22.598 sec
    FAILED
expected:<\[0-5]> but was:<\[1-0]>                                                                                                                    junit.framework.AssertionFailedError: expected:<\[0-5]> but was:<\[1-0]>
    at org.apache.pig.test.TestBuiltin.testUniqueID(TestBuiltin.java:3225)
{panel}

Error message is opposite in that expected was ""[1-0]"" and result was ""[0-5]"".  
In any cases, this is showing the job didn't run with 2 mappers.

","28/Apr/16 20:18;knoguchi;k. Looking back on the code change, I now remember that I didn't _break_ the test but I've _fixed_ the test so that it checks the results properly.
Changing the description accordingly. 

Now, I don't need to feel guilty.  ","28/Apr/16 22:09;knoguchi;This was due to difference in behavior for ""mapred.max.split.size"" in hadoop 1.x and 2.x.  Former assigned one extra record for the first mapper.  

Instead of trying to adjust, changed the test to use 2 input files with 5 records so that we have 2 mappers.",11/May/16 00:14;daijy;+1,12/May/16 16:42;knoguchi;Thanks for the review Daniel!  Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,
Overlapping of parameter substitution names inside&outside a macro fails with NPE,PIG-4880,12962406,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,25/Apr/16 21:55,08/Jun/16 20:48,14/Mar/19 03:08,12/May/16 16:49,0.12.0,,,,,0.16.0,,parser,,,0,,,,,,,"With 
{code:title=macro.pig}
DEFINE mygroupby(REL, key) RETURNS G {
   %declare number 333;
   $G = GROUP $REL by $key parallel $number;
};
{code}
and
{code:title=test.pig}
-- equivalent of -param number=111
%declare number 111;

IMPORT 'macro.pig';
data = LOAD '1234.txt' USING PigStorage() AS (i: int);
result = mygroupby(data, i);
STORE result INTO 'test.out' USING PigStorage();
{code}

Fails with 
{{error msg: <file myscript.pig, line 4> Macro inline failed for macro 'mygroupby'. Reason: null}}

Similarly, when macro param and command-line param overlap, it fails with 
{{Macro inline failed for macro 'mygroupby'. Reason: Macro contains argument or return value number which conflicts with a Pig parameter of the same name.}}",,,,,,,,,,,,,,,,,,,,25/Apr/16 22:14;knoguchi;pig-4880-v01.patch;https://issues.apache.org/jira/secure/attachment/12800650/pig-4880-v01.patch,23/May/16 19:16;knoguchi;pig-4880-v01_documentudpate.patch;https://issues.apache.org/jira/secure/attachment/12805720/pig-4880-v01_documentudpate.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-05-11 17:51:17.129,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon May 23 19:16:12 UTC 2016,,,,,,,0|i2wpvb:,9223372036854775807,"Now, macro argument/return names can overlap with parameter substitution names.  Former value is taken for these cases.",,,,,,,,,"25/Apr/16 22:07;knoguchi;bq. error msg: <file myscript.pig, line 4> Macro inline failed for macro 'mygroupby'. Reason: null

By making it dump the stack trace in a testcase, it showed 

{noformat}
java.lang.NullPointerException
    at org.apache.pig.tools.parameters.PreprocessorContext.processOrdLine(PreprocessorContext.java:179)
    at org.apache.pig.tools.parameters.PigFileParser.param_value(PigFileParser.java:117)
    at org.apache.pig.tools.parameters.PigFileParser.input(PigFileParser.java:57)
    at org.apache.pig.tools.parameters.PigFileParser.Parse(PigFileParser.java:43)
    at org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor.parsePigFile(ParameterSubstitutionPreprocessor.java:95)
    at org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor.genSubstitutedFile(ParameterSubstitutionPreprocessor.java:76)
    at org.apache.pig.parser.PigMacro.substituteParams(PigMacro.java:182)
    at org.apache.pig.parser.PigMacro.inline(PigMacro.java:96)
    at org.apache.pig.parser.PigMacro.macroInline(PigMacro.java:489)
    at org.apache.pig.parser.QueryParserDriver.inlineMacro(QueryParserDriver.java:301)
    at org.apache.pig.parser.QueryParserDriver.expandMacro(QueryParserDriver.java:290)
    at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:183)
    at org.apache.pig.PigServer$Graph.validateQuery(PigServer.java:1729)
    at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1702)
    at org.apache.pig.PigServer.registerQuery(PigServer.java:645)
    at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:1075)
    at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:505)
    at org.apache.pig.tools.grunt.GruntParser.loadScript(GruntParser.java:557)
    at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:370)
    at org.apache.pig.tools.grunt.Grunt.checkScript(Grunt.java:95)
    at org.apache.pig.Main.run(Main.java:629)
    at org.apache.pig.PigRunner.run(PigRunner.java:49)
    at org.apache.pig.test.TestMacroExpansion.verify(TestMacroExpansion.java:2410)
    at org.apache.pig.test.TestMacroExpansion.testParamOverLap4(TestMacroExpansion.java:2357)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
    at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:535)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1182)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1033)
{noformat}

Corresponding code at 
{code:title=PreprocessorContext.java}
176     public  void processOrdLine(String key, String val, Boolean overwrite)  throws ParameterSubstitutionException {
177
178         if (param_val.containsKey(key)) {
179             if (param_source.get(key).equals(val) || !overwrite) {
{code}

and this showed the case when {{param_val}} and {{param_source}} became inconsistent in terms of list of keys. As a result, {{param_source.get(key)}} returned null.  

Tracing the code, inconsistency was introduced at 
{code:title=PigMacro.java}
165         try {
166             PreprocessorContext pc = new PreprocessorContext(50);
167             pc.loadParamVal(Arrays.asList(args), null);
168
169             Map<String, String> paramVal = pc.getParamVal();
170             for (Map.Entry<String, String> e : pigContext.getParamVal().entrySet()) {
171                 if (paramVal.containsKey(e.getKey())) {
172                     throw new ParserException(
173                         ""Macro contains argument or return value "" + e.getKey() + "" which conflicts "" +
174                         ""with a Pig parameter of the same name.""
175                     );
176                 } else {
177                     paramVal.put(e.getKey(), e.getValue());
178                 }
179             }
{code}

where PigMacro updates the {{param_val}} but not {{param_source}} for  global param substitutions.","25/Apr/16 22:09;knoguchi;{quote}
Similarly, when macro param and command-line param overlap, it fails with 
{{Macro inline failed for macro 'mygroupby'. Reason: Macro contains argument or return value number which conflicts with a Pig parameter of the same name.}}
{quote}

Sample code.

{code:title=macro.pig}
DEFINE mygroupby(REL, key, number) RETURNS G {
   $G = GROUP $REL by $key parallel $number;
};
{code}

{code:title=test.pig}
-- equivalent of -param number=111
%declare number 111;

IMPORT 'macro.pig';
data = LOAD '1234.txt' USING PigStorage() AS (i: int);
result = mygroupby(data, i, 222);
STORE result INTO 'test.out' USING PigStorage();
{code}

I think we can allow this by simply taking the macro parameter value.","25/Apr/16 22:14;knoguchi;Attaching a patch that would allow 
* Overlapping of macro-param and command-line param names by taking the macro-param for this case.
and 
* Overlapping of param substitution on both inside and outside the macro by following the default/declare definition. ",11/May/16 17:51;daijy;+1,12/May/16 16:49;knoguchi;Thanks for the review Daniel!  Committed to trunk.,"23/May/16 19:16;knoguchi;Just realized I should have updated the document for the changes I've made for this jira.  

Please let me know if I should open a new jira. 
Also, if I can commit the change to 0.16 branch (as well as trunk).  Thanks.",,,,,,,,,,,,,,,,,,,,,,
Fix issues from PIG-4847,PIG-4878,12961906,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,22/Apr/16 20:30,08/Jun/16 20:48,14/Mar/19 03:08,25/Apr/16 01:00,,,,,,0.16.0,,,,,0,,,,,,,"PIG-4847 created two issues.

  1) Caused compilation failure with Hadoop 1.x
https://builds.apache.org/job/Pig-trunk-commit/2316/console

{code}
  [javac]         double memoryThresholdFraction = conf.getDouble(PigConfiguration.PIG_SPILL_MEMORY_USAGE_THRESHOLD_FRACTION, MEMORY_THRESHOLD_FRACTION_DEFAULT);
    [javac]                                              ^
    [javac]   symbol:   method getDouble(String,double)
    [javac]   location: variable conf of type Configuration
{code}

conf.getDouble() is not present in Hadoop 1.x

2) Interchanged use of memory and collection thresholds",,,,,,,,,,,,,,,,,,,,22/Apr/16 20:32;rohini;PIG-4878-1.patch;https://issues.apache.org/jira/secure/attachment/12800296/PIG-4878-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-04-22 21:02:34.246,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Apr 25 01:00:39 UTC 2016,,,,,,,0|i2wmsf:,9223372036854775807,,,,,,,,,,"22/Apr/16 21:02;knoguchi;{quote}
-            toFree = info.getUsage().getUsed() - collectionThresholdSize + (long)(collectionThresholdSize * 0.5);
+            toFree = info.getUsage().getUsed() - memoryThresholdSize + (long)(memoryThresholdSize * 0.5);
{quote}

Are you fixing another bug in this patch?  Probably better to have a separate jira for that.",22/Apr/16 21:06;rohini;Updated title and description to reflect both issues.,"22/Apr/16 21:11;knoguchi;bq. Updated title and description to reflect both issues.

Thanks.  +1",25/Apr/16 01:00;rohini;Committed to trunk. Thanks for the review Koji.,,,,,,,,,,,,,,,,,,,,,,,,
LogFormat parser fails test,PIG-4877,12960767,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nielsbasjes,nielsbasjes,nielsbasjes,21/Apr/16 14:29,08/Jun/16 20:48,14/Mar/19 03:08,22/Apr/16 22:47,,,,,,0.16.0,,piggybank,,,0,,,,,,,Test failure caused my tiny change in the way a date field is extracted in the upstream library.,,,,,,,,,,,,,,,,,,,,21/Apr/16 14:37;nielsbasjes;PIG-4877.patch;https://issues.apache.org/jira/secure/attachment/12799973/PIG-4877.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-04-22 22:47:31.19,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Apr 22 22:47:31 UTC 2016,,,,,,,0|i2wgb3:,9223372036854775807,,,,,,,,,,"21/Apr/16 14:37;nielsbasjes;I broke it, I fix it. Sorry.",22/Apr/16 22:47;daijy;Patch committed to trunk. Thanks Niels!,,,,,,,,,,,,,,,,,,,,,,,,,,
InputSplit.getLocations return null and result a NPE in Pig,PIG-4873,12958996,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Apr/16 22:34,08/Jun/16 20:48,14/Mar/19 03:08,17/May/16 19:49,,,,,,0.16.0,,,,,0,,,,,,,"A customer report a NPE stack below:
Caused by: java.lang.NullPointerException
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit.toString(PigSplit.java:403)
at java.lang.String.valueOf(String.java:2854)
at java.lang.StringBuilder.append(StringBuilder.java:128)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:753)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)

It is be a MapReduce issue to return null in InputSplit.getLocations. But it is unnecessary to fail the job just because Pig needs to print out the information for the split. The job run normally after capture the null.",,,,,,,,,,,,,,,,,,,,14/Apr/16 22:35;daijy;PIG-4873-1.patch;https://issues.apache.org/jira/secure/attachment/12798845/PIG-4873-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-04-16 01:37:43.257,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 17 19:49:09 UTC 2016,,,,,,,0|i2w5dr:,9223372036854775807,,,,,,,,,,16/Apr/16 01:37;rohini;+1,17/May/16 19:49;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Removing unwanted configuration in Tez broke ConfiguredFailoverProxyProvider,PIG-4869,12958216,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,12/Apr/16 19:09,08/Jun/16 20:48,14/Mar/19 03:08,12/Apr/16 22:15,,,,,,0.16.0,,,,,0,,,,,,,"There are some client side dfs.namenode and dfs.datanode settings which should not be removed

https://issues.apache.org/jira/browse/TEZ-3187?focusedCommentId=15224856&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15224856",,,,,,,,,,,,,TEZ-3187,,,,,,,12/Apr/16 20:35;rohini;PIG-4869-1.patch;https://issues.apache.org/jira/secure/attachment/12798361/PIG-4869-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-04-12 22:09:00.16,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Apr 12 22:15:53 UTC 2016,,,,,,,0|i2w0kn:,9223372036854775807,,,,,,,,,,"12/Apr/16 20:35;rohini;Found lot more settings starting with dfs.namenode and dfs.datanode in https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java. Can ensure those settings are not unset, but there are too many to worry about. So removed namenode and datanode from the list, keeping only nodemanager. ","12/Apr/16 22:09;knoguchi;+1. 

bq. So removed namenode and datanode from the list, keeping only nodemanager.

I was initially confused with this comment but I take it that we're taking out namenode&datanode out of removeUnwantedSettings which means we're now keeping them inside the conf with this patch :) ",12/Apr/16 22:15;rohini;Committed to trunk. Thanks for the review Koji.,,,,,,,,,,,,,,,,,,,,,,,,,
Low values for bytes.per.reducer configured by user not honored in Tez for inputs,PIG-4868,12958177,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,12/Apr/16 17:47,08/Jun/16 20:48,14/Mar/19 03:08,12/Apr/16 23:38,,,,,,0.16.0,,,,,0,,,,,,,"  We don't use that in parallelism estimation and only use that in ShuffleVertexManager. Usually it is fine, but in cases when user has set a really low bytes.per.reducer it does not work as our estimation is low. ShuffleVertexManager only decreases parallelism and not increases parallelism.",,,,,,,,,,,,,,,,,,,,12/Apr/16 19:54;rohini;PIG-4868-1.patch;https://issues.apache.org/jira/secure/attachment/12798350/PIG-4868-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-04-12 23:14:35.457,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Apr 12 23:38:50 UTC 2016,,,,,,,0|i2w0bz:,9223372036854775807,,,,,,,,,,"12/Apr/16 23:14;daijy;This only applicable for the vertex with input, right? If so, +1.",12/Apr/16 23:37;rohini;Yes.,12/Apr/16 23:38;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,
-stop_on_failure does not work with Tez,PIG-4867,12958142,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,12/Apr/16 15:35,08/Jun/16 20:48,14/Mar/19 03:08,12/Apr/16 19:32,,,,,,0.15.1,0.16.0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Apr/16 15:36;rohini;PIG-4867-1.patch;https://issues.apache.org/jira/secure/attachment/12798293/PIG-4867-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-04-12 16:24:39.586,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Apr 12 19:32:51 UTC 2016,,,,,,,0|i2w047:,9223372036854775807,,,,,,,,,,12/Apr/16 16:24;knoguchi;+1.  I wish we had more pig specific config name than {{stop.on.failure}} but I guess this has been used since the beginning.,12/Apr/16 19:32;rohini;Committed to branch-0.15 and trunk. Thanks for the review Koji.,,,,,,,,,,,,,,,,,,,,,,,,,,
Loading data using OrcStorage() accepts only default FileSystem path,PIG-4860,12955509,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,beriaanirudh,beriaanirudh,beriaanirudh,02/Apr/16 14:20,08/Jun/16 20:48,14/Mar/19 03:08,18/Apr/16 17:17,,,,,,0.15.1,0.16.0,,,,0,,,,,,,"If the default file system is HDFS, but one wants to load data from S3 (or some other storage) using OrcStorage(), the query fails since the path expected is of format `hdfs://` but gets `s3://`",,,,,,,,,,,,,,,,,,,,03/Apr/16 14:37;beriaanirudh;PIG-4860.patch;https://issues.apache.org/jira/secure/attachment/12796722/PIG-4860.patch,18/Apr/16 07:22;beriaanirudh;PIG-4860_2.patch;https://issues.apache.org/jira/secure/attachment/12799207/PIG-4860_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-04-11 20:28:21.061,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Apr 18 17:17:24 UTC 2016,,,,,,,0|i2vjw7:,9223372036854775807,,,,,,,,,,"07/Apr/16 13:13;beriaanirudh;Hi. I am not sure of the next steps for resolving this. I have attached the fix patch, but I don't see any Jenkins Job +1 as suggested in wiki https://cwiki.apache.org/confluence/display/PIG/HowToContribute#HowToContribute-Contributingyourwork . Please help.",07/Apr/16 14:21;beriaanirudh;cc [~daijy],"11/Apr/16 20:28;daijy;Patch looks good to me. For the testing, it might not easy to write a unit test for that, but can you test it manually and write down how you test it? Forget about Jenkins +1 message, it has been broken for a long time.","13/Apr/16 13:56;beriaanirudh;Thanks for taking a look Daniel. Here's how I tested it:
{noformat}
X = LOAD 's3://<my_bucket>/data/excite-small.log' USING PigStorage(' ') AS (user, time, query);
store X into 'hdfs://ec2-54-205-241-48.compute-1.amazonaws.com:9000/orc_file' using OrcStorage();
store X into 's3://<my_bucket>/beria/orc_file' using OrcStorage();
Y = LOAD 's3://<my_bucket>/beria/orc_file' using OrcStorage();
Z = LOAD 'hdfs://ec2-54-205-241-48.compute-1.amazonaws.com:9000/orc_file' using OrcStorage();
dump Y;
dump Z;
{noformat}
I also downloaded data files ( {{*/orc_file/part-m-00000}} ) and checked they are indeed orc format files.","18/Apr/16 04:17;rohini;Can you please do new Path instead of new URI?

FileSystem fs = FileSystem.get(new Path(location), job.getConfiguration());

","18/Apr/16 07:22;beriaanirudh;Thanks for the suggestion Rohini. Changed to {{FileSystem fs = FileSystem.get(new Path(location).toUri(), job.getConfiguration());}} taking clue from https://github.com/apache/pig/blob/trunk/src/org/apache/pig/builtin/JsonMetadata.java#L295 . This has reduced the patch to a minimum. Attached it.
Did the same test as commented above, and got same correct results.",18/Apr/16 17:17;rohini;Committed to branch-0.15 and trunk. Thanks for the patch [~beriaanirudh],,,,,,,,,,,,,,,,,,,,,
Null not padded when input has less fields than declared schema for some loader,PIG-4851,12953677,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,knoguchi,knoguchi,25/Mar/16 22:13,08/Jun/16 20:48,14/Mar/19 03:08,28/Mar/16 18:55,0.12.1,0.13.0,0.14.0,0.15.0,,0.15.1,0.16.0,,,,0,,,,,,,"{code:title=test.pig}
A = load 'input.txt' using org.apache.pig.piggybank.storage.CSVLoader() as (field1, field2);
dump  A;
{code}

{code:title=input.txt}
a
b,
c,d
,e
f
{code}

{code:title=expected output by pig-0.11}
(a,)
(b,)
(c,d)
(,e)
(f,)
{code}

{code:title=incorrect output by trunk and probably from 0.12}
(a)
(b)
(c,d)
(,e)
(f)
{code}

",,,,,,,,,,,,,,,,,,,,28/Mar/16 15:03;rohini;PIG-4851-1.patch;https://issues.apache.org/jira/secure/attachment/12795633/PIG-4851-1.patch,28/Mar/16 15:45;rohini;PIG-4851-2-nowhitespacechanges.patch;https://issues.apache.org/jira/secure/attachment/12795639/PIG-4851-2-nowhitespacechanges.patch,28/Mar/16 15:45;rohini;PIG-4851-2.patch;https://issues.apache.org/jira/secure/attachment/12795640/PIG-4851-2.patch,28/Mar/16 16:21;rohini;PIG-4851-3-nowhitespacechanges.patch;https://issues.apache.org/jira/secure/attachment/12795644/PIG-4851-3-nowhitespacechanges.patch,28/Mar/16 16:21;rohini;PIG-4851-3.patch;https://issues.apache.org/jira/secure/attachment/12795645/PIG-4851-3.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2016-03-28 16:21:13.056,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Mar 28 18:55:15 UTC 2016,,,,,,,0|i2v8lj:,9223372036854775807,,,,,,,,,,"25/Mar/16 22:27;knoguchi;Noticed the bug when reviewing a different issue with our user's custom loader that extends PigStorage and appended hardcoded value to the tuple.

In 0.12, PIG-3123 added a feature that skips identity projection when possible and moved the Null padding logic to inside PigStorage getNext(). 

This itself is debatable since it broke our user's custom loader (that extended PigStorage), but more serious issue exists with  _other_ Loaders that lack this padding logic, like CSVLoader from description.

Reviewing the code, I think the issue exists for Loader that implements LoadPushDown but LoLoad.getDeterminedSchema() returns null.


","28/Mar/16 15:26;knoguchi;Thanks [~rohini], looking good.  Most of the test changes are reverting pig-3123.
I see one new test case for covering one user's case of having custom loader extending PigStorage.

Can you also add a test case I described on this jira using {{org.apache.pig.piggybank.storage.CSVLoader}} or alike? 
As I mentioned, breaking user's custom loader that was extending PigStorage was one but more critical issue was null not padded for non-PigStorage loaders.",28/Mar/16 16:21;rohini;Updated patch with testcases that I missed reverting. Thanks [~knoguchi] for checking those and pointing out.,"28/Mar/16 16:26;knoguchi;Thanks for adding extra test for CSVLoader.  Since this is a bug in pig core, I preferred this test to be in core and not piggybank, but at the same time I hate copy-and-pasting .   Given we perform full tests including piggybank before the release, I'm fine.

+1 on the patch.",28/Mar/16 18:55;rohini;Committed to branch-0.15 and trunk. Thanks [~knoguchi] for identifying all the different issues and reviewing the patch.,,,,,,,,,,,,,,,,,,,,,,,
Registered jars do not use submit replication,PIG-4850,12953676,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rdblue,rdblue,rdblue,25/Mar/16 22:10,08/Jun/16 20:48,14/Mar/19 03:08,26/Mar/16 05:55,,,,,,0.16.0,,impl,,,0,,,,,,,"PIG-4074 added support for mapred.submit.replication, which sets the replication factor for files added to the distributed cache. The purpose is to avoid a huge number of task attempts downloading the same file in HDFS at once during localization and slowing down because of contention over few replicas. The replication factor for files was set correctly, but registered jars are added to HDFS through a different code path and weren't using the submit replication factor. This causes localization time for jobs to increase by as much as 10 minutes (at which point the tasks are killed).",,,,,,,,,,,,,,,,,,,,25/Mar/16 22:14;rdblue;PIG-4850.1.patch;https://issues.apache.org/jira/secure/attachment/12795474/PIG-4850.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-03-25 23:06:27.329,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 28 15:43:30 UTC 2016,,,,,,,0|i2v8lb:,9223372036854775807,,,,,,,,,,25/Mar/16 22:14;rdblue;Attaching a simple patch that fixes the problem.,"25/Mar/16 22:15;rdblue;[~cheolsoo], since you implemented PIG-4074, would you like to have a look at this? Thanks!","25/Mar/16 23:06;cheolsoo;Oh so PIG-4074 missed a code path for registered jars. LGTM.

Thanks for fixing this! I'll commit it later today.

+1.",26/Mar/16 05:55;cheolsoo;Committed into trunk.,"28/Mar/16 15:43;rdblue;Thanks for the quick response, Cheolsoo!",,,,,,,,,,,,,,,,,,,,,,,
Parallel instantiation of classes in Tez cause tasks to fail,PIG-4845,12952233,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,21/Mar/16 22:45,08/Jun/16 20:48,14/Mar/19 03:08,22/Mar/16 10:45,,,,,,0.16.0,,,,,0,,,,,,,"OptionBuilder is not thread-safe

https://commons.apache.org/proper/commons-cli/javadocs/api-release/org/apache/commons/cli/OptionBuilder.html
{code}
This class is NOT thread safe. See CLI-209
{code}


HBaseStorage, elephantbird SequenceFileConfig, etc use it in their constructor. This leads to NoSuchMethodException, UnrecognizedOptionException etc when processor, inputs and outputs are initialized in parallel in Tez making the task fail. Retry attempts mostly go through and job might succeed. Need to synchronize the initialization. synchronize would make it similar to mapreduce though at a slight performance cost. But that should be lot better than cost of relaunching containers after hitting failures.

Following two kinds of exception where seen when HBaseStorage and SequenceFileStorage of elephantbird were used together.

org.apache.commons.cli.UnrecognizedOptionException: Unrecognized option: -loadKey

{code}
Caused by: java.lang.RuntimeException: Failed to create WritableConverter instance
        at com.twitter.elephantbird.pig.util.SequenceFileConfig.getWritableConverter(SequenceFileConfig.java:225)
        at com.twitter.elephantbird.pig.util.SequenceFileConfig.<init>(SequenceFileConfig.java:101)
        at com.twitter.elephantbird.pig.util.SequenceFileConfig.<init>(SequenceFileConfig.java:115)
        ... 11 more
Caused by: java.lang.NoSuchMethodException: com.twitter.elephantbird.pig.util.TextConverter.<init>(java.lang.String)
        at java.lang.Class.getConstructor0(Class.java:3082)
        at java.lang.Class.getConstructor(Class.java:1825)
        at com.twitter.elephantbird.pig.util.SequenceFileConfig.getWritableConverter(SequenceFileConfig.java:213)
        ... 13 more
{code}",,,,,,,,,,,,,,,,,,,,21/Mar/16 22:47;rohini;PIG-4845-1.patch;https://issues.apache.org/jira/secure/attachment/12794626/PIG-4845-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-03-22 00:47:31.377,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Mar 22 10:45:57 UTC 2016,,,,,,,0|i2uzon:,9223372036854775807,,,,,,,,,,22/Mar/16 00:47;daijy;+1,22/Mar/16 10:45;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
Inline-op with schema declaration fails with syntax error,PIG-4841,12949367,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,11/Mar/16 22:59,08/Jun/16 20:48,14/Mar/19 03:08,21/Mar/16 21:46,0.11.1,0.15.0,,,,0.16.0,,parser,,,0,,,,,,,"Inline-op+foreach+schema(""as"") fails. 

{code}
knoguchi@truelisten-lm pig> cat test6.pig
A = load 'input.txt' as (a1:int, a2:int);
B = FOREACH (FOREACH A GENERATE a1 as b1) GENERATE b1;

dump B;
{code}

{panel}
2016-03-11 17:57:31,364 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Lexical error at line 5, column 0.  Encountered: <EOF> after : """"
{panel}",,,,,,,,,,,,,,,,,,,,13/Mar/16 03:12;knoguchi;pig-4841-v01.patch;https://issues.apache.org/jira/secure/attachment/12793179/pig-4841-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-03-21 21:16:55.408,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Mar 21 21:46:23 UTC 2016,,,,,,,0|i2ujxj:,9223372036854775807,,,,,,,,,,"13/Mar/16 02:49;knoguchi;Rohini pointed out to me that ""inline"" is the terminology I should be using.
Changing summary.

I took ""nested"" from {{QueryParser.g}}'s ""nested_op_clause"".  
Maybe we should update that later.","13/Mar/16 03:12;knoguchi;In the patch I have two new tests, one from the description and another that I decided not to fix as part of this jira and commented it out.

{code}
+    @Test
+    public void testWithInlineOpWithNestedForeach() throws Throwable {
+        // This one currently fails because ""{}"" is treated as
+        // IN_BLOCK in PigScriptParser.jj which jumps to PIG_END and ignore
+        // "") generate *; "" part of the code.
+        // In order to support this test, we need to add parenthesis matching
+        // everywhere in PigScriptParser.jj (or stop using it)
+        //
+        String query = ""a = load 'i1' as (f1:chararray);"" +
+               ""b = group a ALL; "" +
+               ""c = foreach ( foreach b {b1 = limit a 3; generate 1, b1;} ) generate *; "" +
+               ""dump c;"";
+        ArrayList<String> msgs = new ArrayList<String>();                //
+        validate(query, true, msgs.toArray(new String[0]));
+    }
{code}

I had a patch that would cover this test case but it required me to add parenthesis checking everywhere and it still gave me 10-20 more test failures with mismatching "")"".  At that point, decided to step back and fix the original case only and defer inlineop+nested-foreach case to PIG-2597 where we're trying to move Grunt from javacc to ANTLR.",21/Mar/16 21:16;daijy;+1,"21/Mar/16 21:46;knoguchi;Thanks for the review Daniel! 

Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,
TestBuiltin.testURIWithCurlyBrace in TEZ failing after PIG-4819,PIG-4833,12948091,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,08/Mar/16 17:52,08/Jun/16 20:48,14/Mar/19 03:08,09/Mar/16 18:30,,,,,,0.16.0,,,,,0,,,,,,,"From https://issues.apache.org/jira/browse/PIG-4819?focusedCommentId=15179448#comment-15179448

{quote}
TestBuiltin.testURIWithCurlyBrace is failing after addition of testRandomJob with -Dhadoopversion=23 -Dexectype=tez. Possible to take a look at it? 
{quote}",,,,,,,,,,,,,,,,,,,,08/Mar/16 18:24;knoguchi;pig-4833-v01.patch;https://issues.apache.org/jira/secure/attachment/12792051/pig-4833-v01.patch,27/Apr/16 22:04;knoguchi;pig-4833-v01_fixregression_1.patch;https://issues.apache.org/jira/secure/attachment/12801130/pig-4833-v01_fixregression_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-03-08 22:30:26.601,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Apr 28 15:23:48 UTC 2016,,,,,,,0|i2ucjj:,9223372036854775807,,,,,,,,,,"08/Mar/16 18:24;knoguchi;This was weird to me since my patch in PIG-4819 didn't touch {{TestBuiltin.testURIWithCurlyBrace}} at all.

Looking further, it seems like an issue with tez test environment when local mode and mapreduce/tez mode are intermixed.  Test code does call {{Util. resetStateForExecModeSwitch()}}, which states 

{code:title=Util.java}
  1337     /**
  1338      * Called to reset ThreadLocal or static states that PigServer depends on
  1339      * when a test suite has testcases switching between LOCAL and MAPREDUCE/TEZ
  1340      * execution modes
  1341      */
  1342     public static void resetStateForExecModeSwitch() {
{code}

but {{TezResourceManager}} caches an instance from PIG-3978 and would not re-initialize when we move from tez mode to local mode.  Uploading a patch, pig-4833-v01.patch,  that explicitly drops this instance for every Util.resetStateForExecModeSwitch call.
Not sure if adding a public method just for testing is justified or not.  

Besides from this issue, made minor changes in {{testUniqueID}} and {{testRANDOMWithJob}} in TestBuiltin.java to 
* use its own copy for setting PigServer properties.
* delete the input file when test is done.

These minor changes were things I've tried before finding out the issue above.
",08/Mar/16 22:30;daijy;+1,09/Mar/16 18:30;knoguchi;Thanks for the review Daniel!  Committed to trunk.,"25/Apr/16 20:18;rohini;This patch breaks tests in Hadoop 1.x mode.

ant clean test -Dtestcase=TestBatchAliases -logfile /tmp/pig.log

{code}
   [javac] /apache/pig/trunk/test/org/apache/pig/test/Util.java:78: error: package org.apache.pig.backend.hadoop.executionengine.tez does not exist
    [javac] import org.apache.pig.backend.hadoop.executionengine.tez.TezResourceManager;
    [javac]                                                         ^
    [javac] /apache/pig/trunk/test/org/apache/pig/test/Util.java:1347: error: cannot find symbol
    [javac]         TezResourceManager.dropInstance();
    [javac]         ^
    [javac]   symbol:   variable TezResourceManager
    [javac]   location: class Util

{code}","27/Apr/16 22:04;knoguchi;bq. This patch breaks tests in Hadoop 1.x mode.

Ouch.  Sorry about that.  Uploading a patch that does 
* Reflection on Tez class.  Rohini pointed out that using shim for testcode is too much.
* Also added @VisibleForTesting annotation suggested by Rohini.

Running tests with this patch.

Besides from this, I see that I somehow set ""Incompatible change"" flag on this jira.  Taking it out.",27/Apr/16 22:06;rohini;+1,28/Apr/16 14:55;knoguchi;Thanks Rohini.  Now committed {{pig-4833-v01_fixregression_1.patch}} to trunk (0.16).,"28/Apr/16 15:23;knoguchi;With the last patch, test now compiles with hadoop 1.x.
However, I see that TestBuiltin.testUniqueID is somehow failing (from the original patch of pig-4833-v01.patch).  It runs fine with hadoop 2.x.  

Created PIG-4881.  Hopefully I can stop breaking things and actually start fixing some issues... :(",,,,,,,,,,,,,,,,,,,,
Fix TestPrumeColumn NPE failure,PIG-4832,12947936,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,08/Mar/16 08:04,08/Jun/16 20:48,14/Mar/19 03:08,21/Mar/16 21:11,,,,,,0.16.0,spark-branch,,,,0,,,,,,," ant clean -Dtestcase=TestPruneColumn  -Dexectype=mr  -Dhadoopversion=23 test

following tests fail:
testSharedSchemaObject 
testMapKey4

Error message:
{code}
Testcase: testSharedSchemaObject took 0.133 sec
        Caused an ERROR 
Unable to open iterator for alias C
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias C
        at org.apache.pig.PigServer.openIterator(PigServer.java:957)
        at org.apache.pig.test.TestPruneColumn.testSharedSchemaObject(TestPruneColumn.java:1660)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias C
        at org.apache.pig.PigServer.storeEx(PigServer.java:1060)
        at org.apache.pig.PigServer.store(PigServer.java:1019)
        at org.apache.pig.PigServer.openIterator(PigServer.java:932)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:310)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1412)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1397)
        at org.apache.pig.PigServer.storeEx(PigServer.java:1056)
Caused by: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getNextTuple(POStore.java:164)
        at org.apache.pig.backend.hadoop.executionengine.fetch.FetchLauncher.runPipeline(FetchLauncher.java:157)
        at org.apache.pig.backend.hadoop.executionengine.fetch.FetchLauncher.launchPig(FetchLauncher.java:81)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:302)

Testcase: testMapKey4 took 0.127 sec
        Caused an ERROR
Unable to open iterator for alias C
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias C
        at org.apache.pig.PigServer.openIterator(PigServer.java:957)
        at org.apache.pig.test.TestPruneColumn.testMapKey4(TestPruneColumn.java:1242)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias C
        at org.apache.pig.PigServer.storeEx(PigServer.java:1060)
        at org.apache.pig.PigServer.store(PigServer.java:1019)
        at org.apache.pig.PigServer.openIterator(PigServer.java:932)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:310)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1412)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1397)
        at org.apache.pig.PigServer.storeEx(PigServer.java:1056)
Caused by: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore.getNextTuple(POStore.java:164)
        at org.apache.pig.backend.hadoop.executionengine.fetch.FetchLauncher.runPipeline(FetchLauncher.java:157)
        at org.apache.pig.backend.hadoop.executionengine.fetch.FetchLauncher.launchPig(FetchLauncher.java:81)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:302)
{code}

The reason why they fail is because POStore#sDecorator is null in some cases.",,,,,,,,,,,PIG-4831,,,,,,,,,08/Mar/16 08:08;kellyzly;PIG-4832.patch;https://issues.apache.org/jira/secure/attachment/12791956/PIG-4832.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-03-21 21:11:12.208,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Mar 21 21:11:12 UTC 2016,,,,,,,0|i2ubl3:,9223372036854775807,,,,,,,,,,"08/Mar/16 08:08;kellyzly;[~rohini] , [~xuefuz] and [~pallavi.rao]:

Fix the NPE problem. please help review.",16/Mar/16 09:01;kellyzly;[~daijy] and [~rohini]: Can you help reivew this? this blocks the unit test of TestPrumeCplumn in mr and spark mode,21/Mar/16 21:11;daijy;This addresses PIG-4731. Committed to both trunk and spark branch. Thanks Liyun!,,,,,,,,,,,,,,,,,,,,,,,,,
Add excluded-tests-spark,PIG-4826,12947446,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,pallavi.rao,pallavi.rao,pallavi.rao,05/Mar/16 12:37,21/Jun/17 09:18,14/Mar/19 03:08,06/Mar/16 15:23,,,,,,spark-branch,,spark,,,0,spork,,,,,,,,,,,,,,,,,,,,,,,,,,06/Mar/16 15:34;xuefuz;PIG-4826-amend.patch;https://issues.apache.org/jira/secure/attachment/12791672/PIG-4826-amend.patch,05/Mar/16 12:42;pallavi.rao;PIG-4826.patch;https://issues.apache.org/jira/secure/attachment/12791624/PIG-4826.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-03-06 15:23:05.151,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Sun Mar 06 15:34:57 UTC 2016,,,,,,,0|i2u8kf:,9223372036854775807,,,,,,,,,,05/Mar/16 12:39;pallavi.rao;https://builds.apache.org/job/Pig-spark/310/ - Build is failing because it is missing the test exclusion file.,"05/Mar/16 12:42;pallavi.rao;[~xuefuz], please commit this patch and it should fix the build.","06/Mar/16 15:23;xuefuz;+1. Patch committed to Spark branch. Thanks, Pallavi.",06/Mar/16 15:34;xuefuz;It looks like additional change is needed. Trivial patch is attached and committed to fix the build.,,,,,,,,,,,,,,,,,,,,,,,,
Pig chararray field with special UTF-8 chars as part of tuple join key produces wrong results in Tez,PIG-4821,12946311,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,02/Mar/16 22:40,08/Jun/16 20:48,14/Mar/19 03:08,31/May/16 23:01,,,,,,0.15.1,0.16.0,,,,0,,,,,,,"SedesHelper.writeChararray does writeUTF, but we do str1 = new String(bb1.array(), bb1.position(), casz1, BinInterSedes.UTF8); when reading it in the BinInterSedesTupleRawComparator https://github.com/apache/pig/blob/e0c5f265c68491395d8303c86195445be3d8aecf/src/org/apache/pig/data/BinInterSedes.java#L959-L964. For some reason, this works fine in my MAC (both jdk7 and jdk8) but not in Linux. Not sure about the actual cause and have not dug into it. Suspecting either charset environment or the specific update of jdk 8 (different in my MAC and Linux).",,,,,,,,,,,,,PIG-4914,,,,,,,31/May/16 22:27;rohini;PIG-4821-1.patch;https://issues.apache.org/jira/secure/attachment/12807274/PIG-4821-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-05-31 22:34:42.45,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 31 23:01:45 UTC 2016,,,,,,,0|i2u23r:,9223372036854775807,,,,,,,,,,"31/May/16 22:27;rohini;  Attaching patch.  I have not been able to come up with a test case for it and had postponed uploading the patch for long. Patch is simple. Been struggling with test case due to two issues
    - Not easily reproducible in my laptop. Tried playing around with encodings - sun.io.unicode.encoding, jdk and different subset of data and it did not work. Issue is reproducible sometimes, but is rare and not repeatable.
   - Narrow down to a smaller dataset for the test. Issue occurs during sorting and happens only when specific order of data go through comparison. Not able to exactly narrow down the minimal set of records from the larger data. 
   
 This patch is important and needs to go into the release. Will create a separate jira to add testcase later. ",31/May/16 22:34;daijy;+1,"31/May/16 23:01;rohini;Committed to branch-0.15, branch-0.16 and trunk. Thanks for the review Daniel.",,,,,,,,,,,,,,,,,,,,,,,,,
RANDOM() udf can lead to missing or redundant records,PIG-4819,12945934,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,01/Mar/16 20:18,08/Jun/16 20:48,14/Mar/19 03:08,03/Mar/16 20:58,,,,,,0.16.0,,,,,0,,,,,,,"When RANDOM() value is used for grouping/distinct/etc, it breaks the mapreduce rule and can lead to redundant or missing records. 

Some discussion can be found in 
https://issues.apache.org/jira/browse/PIG-3257?focusedCommentId=13669195#comment-13669195

We should make RANDOM less random so that it'll produce the same sequence of random values from the task retries.",,,,,,,,,,,,,,,,,,,,01/Mar/16 20:24;knoguchi;pig-4819-v01.patch;https://issues.apache.org/jira/secure/attachment/12790781/pig-4819-v01.patch,02/Mar/16 15:23;knoguchi;pig-4819-v02.patch;https://issues.apache.org/jira/secure/attachment/12790947/pig-4819-v02.patch,03/Mar/16 05:38;knoguchi;pig-4819-v02_fix_v01.patch;https://issues.apache.org/jira/secure/attachment/12791101/pig-4819-v02_fix_v01.patch,03/Mar/16 18:32;knoguchi;pig-4819-v02_fix_v02.patch;https://issues.apache.org/jira/secure/attachment/12791214/pig-4819-v02_fix_v02.patch,03/Mar/16 19:13;knoguchi;pig-4819-v02_fix_v03.patch;https://issues.apache.org/jira/secure/attachment/12791228/pig-4819-v02_fix_v03.patch,03/Mar/16 19:51;knoguchi;pig-4819-v02_fix_v04.patch;https://issues.apache.org/jira/secure/attachment/12791241/pig-4819-v02_fix_v04.patch,03/Mar/16 20:23;knoguchi;pig-4819-v02_fix_v05.patch;https://issues.apache.org/jira/secure/attachment/12791247/pig-4819-v02_fix_v05.patch,03/Mar/16 20:49;knoguchi;pig-4819-v02_fix_v06.patch;https://issues.apache.org/jira/secure/attachment/12791250/pig-4819-v02_fix_v06.patch,,,,8.0,,,,,,,,,,,,,,,,,,,2016-03-01 21:44:54.796,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Mar 09 18:34:01 UTC 2016,,,,,,,0|i2tzrz:,9223372036854775807,,,,,,,,,,"01/Mar/16 20:24;knoguchi;Attaching a patch that 
* Use jobid and taskid as the seed. 
* A test case to make sure attempts from the same task would produce a same random sequence 
* (Unrelated, but) fixing a minor bug in testUniqueID where results weren't checked properly.

I only changed {{./src/org/apache/pig/builtin/RANDOM.java}} but not {{./contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/math/RANDOM.java}}.  Let me know if I should make the same change.","01/Mar/16 21:44;rohini;bq. Let me know if I should make the same change.
  Yes. That would be good.

Other comments on the patch
 -  Tab spacing should be 4 spaces and not two in exec() method.
  - Can we remove System.err.println(tmpresult[i]);  or use debug logging?","02/Mar/16 15:23;knoguchi;{quote}
bq. Let me know if I should make the same change.
Yes. That would be good.
{quote}
Made the same change there.
But should I simply extend {{org.apache.pig.builtin.RANDOM}} from 
{{org.apache.pig.piggybank.evaluation.math.RANDOM}} ?

bq. Tab spacing should be 4 spaces and not two in exec() method.

I did have 4 spaces for the lines I touched.  Assuming you're talking about the existing code having actual tab in the code, replaced them with 4 spaces.  

bq. Can we remove System.err.println(tmpresult[i]); or use debug logging?
Forgot to delete that.  Thanks for catching it.
Since the output gave me assurance that I am getting some random values, I replaced it with logging(info).
","02/Mar/16 17:58;rohini;+1

bq. But should I simply extend org.apache.pig.builtin.RANDOM from  org.apache.pig.piggybank.evaluation.math.RANDOM
   Would be ideal, but if they use newer piggybank jar with older version of pig it will break. So I think duplicating code is better for now.
",02/Mar/16 19:37;knoguchi;Thanks for the review Rohini!  Committed to trunk.,"03/Mar/16 03:45;knoguchi;Sorry for reopening but my unit test {{testRANDOMWithJob}} was failing indicating that random isn't random enough.

Trying with a real cluster with small inputs, saw a very similar random values produced with high probability.  Obviously long ""seed"" was too close. Changing how I'm setting the seed.","03/Mar/16 05:38;knoguchi;Wasn't sure if I should rollback the change and attach a complete new patch or attach a diff from the last commit. 
This one does the latter.

Two major changes.
* Given two consecutive seeds seem to result in close random values (at least for some initial ones), now creating a seed(type:long) with task_number and job_id.hascode() connected instead of other ways.  Given only the first 48 bits is used from the passed seed, leaving 28 bits for the task id.

* To add more randomness across jobs, adding submit time with XOR.  (Ideally it would be better if this was nano-seconds, but I think this is good enough with milli-seconds.)

I'll do some more testing tomorrow.","03/Mar/16 18:32;knoguchi;Discussing with Rohini, simplified a call by creating a long by connecting two jobid-hash and xor-ing with a task number.  Also, added a logic so that if RANDOM is called for more than once in the script, they would return a different value.

{code}
B = FOREACH A generate RANDOM(), RANDOM();
{code}

bq. To add more randomness across jobs, adding submit time with XOR.

This didn't work with Tez.  It wasn't transferring ""pig.job.submitted.timestamp"".  For now, taking it out but it would be nice to have this.  (Even better with nanosecond).

{quote}
bq. But should I simply extend org.apache.pig.builtin.RANDOM from org.apache.pig.piggybank.evaluation.math.RANDOM
Would be ideal, but if they use newer piggybank jar with older version of pig it will break. So I think duplicating code is better for now.
{quote}
Given the not so obvious changes I've made to original RANDOM, I wasn't comfortable with copy and pasting.  I simply went with extending option.
My understanding is, worst case would be piggybank.RANDOM referencing the original builtin.RANDOM without my changes but it won't fail.","03/Mar/16 19:13;knoguchi;Rohini pointed out that I would need to re-initialize the static variables for Tez jvm reuse.

{quote}
http://pig.apache.org/docs/r0.15.0/udf.html
_Clean up static variable in Tez_
{quote}

Added.  But still trying to see if I can get rid of the static variable dependency.",03/Mar/16 19:51;knoguchi;{{import org.apache.pig.StaticDataCleanup;}} was missed when adding the cleanup method.,"03/Mar/16 20:23;knoguchi;Hopefully this is close to the final one.
* Added comment on why I came up with that seed formula.
* Renaming {{staticDataCleanup}} to {{resetSeedUniquifier}} and using that to initialize from test.
",03/Mar/16 20:24;rohini;+1,"03/Mar/16 20:49;knoguchi;bq. +1

Thanks Rohini.
Uploading another for a minor typo in my comment. 
""48 bytes"" should be ""48 bits"".  

Also took out unnecessary lines from TestBuiltin.java that were no longer necessary after calling {{RANDOM.resetSeedUniquifier}} directly.",03/Mar/16 20:51;rohini;+1,"03/Mar/16 20:58;knoguchi;In addition to previous pig-4819-v02.patch, applied the fix with pig-4819-v02_fix_v06.patch.","04/Mar/16 06:46;rohini;[~knoguchi],
    TestBuiltin.testURIWithCurlyBrace is failing after addition of testRandomJob with -Dhadoopversion=23 -Dexectype=tez. Possible to take a look at it? Also would be good to put this in Pig 0.15.1 as well.","09/Mar/16 18:34;knoguchi;bq. TestBuiltin.testURIWithCurlyBrace is failing after addition of testRandomJob with -Dhadoopversion=23 -Dexectype=tez. 

Fixed in PIG-4833.

bq. Also would be good to put this in Pig 0.15.1 as well.

Not sure.   I do like my change but still afraid of how it'll perform for our users.  
For now, I prefer to keep it only in trunk.
",,,,,,,,,,,
Single quote inside comment in GENERATE is not being ignored,PIG-4818,12945590,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,29/Feb/16 20:10,08/Jun/16 20:48,14/Mar/19 03:08,01/Mar/16 15:52,0.12.0,0.12.1,0.13.0,0.14.0,0.15.0,0.16.0,,,,,0,,,,,,,"{code}
A = load '1.txt' as (a1:int, a2:int);
B = FOREACH A GENERATE a1,
 -- testing ' here with single quote
              a2;
dump B;
{code}

This fails with 
{panel}
2016-02-29 20:09:05,507 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Lexical error at line 6, column 0.  Encountered: <EOF> after : """"
{panel}",,,,,,,,,,,,,,,,,,,,29/Feb/16 20:15;knoguchi;pig-4818-v01.patch;https://issues.apache.org/jira/secure/attachment/12790552/pig-4818-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-02-29 22:52:47.446,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 01 15:52:41 UTC 2016,,,,,,,0|i2txnr:,9223372036854775807,,,,,,,,,,"29/Feb/16 20:15;knoguchi;It seems like a minor change in PIG-2507 caused this unexpected behavior.  Not understanding javacc much, but pasting some changes that seem to work.  Appreciate if someone can review the change.

","29/Feb/16 22:52;daijy;+1.

And this happens only when the comment is in GENERATE clause.","01/Mar/16 15:52;knoguchi;Thanks for the review [~daijy] !

Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,
Read a null scalar causing a Tez failure,PIG-4816,12945118,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,26/Feb/16 23:47,08/Jun/16 20:48,14/Mar/19 03:08,01/Mar/16 22:48,,,,,,0.16.0,,,,,0,,,,,,,"The following script fail:
{code}
a = load 'studenttab10k' as (name:chararray, age:int, gpa:double);
b = filter a by age > 100;
c = group b all;
d = foreach c generate COUNT(b.gpa) as count;
e = foreach a generate (d.count IS NOT NULL? d.count : 0l);
dump e;
{code}

Error stack:
{code}
Failure while running task:org.apache.pig.backend.executionengine.ExecException: ERROR 0: java.io.IOException: Please check if you are invoking next() even after it returned false. For usage, please refer to KeyValueReader javadocs
	at org.apache.pig.backend.hadoop.executionengine.tez.plan.udf.ReadScalarsTez.attachInputs(ReadScalarsTez.java:96)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.initializeInputs(PigProcessor.java:295)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:183)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:337)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Please check if you are invoking next() even after it returned false. For usage, please refer to KeyValueReader javadocs
	at org.apache.tez.runtime.library.api.KeyValueReader.hasCompletedProcessing(KeyValueReader.java:77)
	at org.apache.tez.runtime.library.common.readers.UnorderedKVReader.moveToNextInput(UnorderedKVReader.java:190)
	at org.apache.tez.runtime.library.common.readers.UnorderedKVReader.next(UnorderedKVReader.java:118)
	at org.apache.pig.backend.hadoop.executionengine.tez.plan.udf.ReadScalarsTez.attachInputs(ReadScalarsTez.java:81)
	... 15 more
{code}
",,,,,,,,,,,,,,,,,,,,27/Feb/16 00:39;daijy;PIG-4816-1.patch;https://issues.apache.org/jira/secure/attachment/12790264/PIG-4816-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-03-01 22:19:49.022,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Mar 01 22:48:21 UTC 2016,,,,,,,0|i2tur3:,9223372036854775807,,,,,,,,,,"26/Feb/16 23:48;daijy;This happens while the scalar is null. The null scalar is a result of PIG-4724. As we don't have a good solution for PIG-4724 yet, we shall allow null scalar value.",01/Mar/16 22:19;rohini;+1,01/Mar/16 22:48;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage does not take namenode HA as part of schema file url,PIG-4814,12940678,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,19/Feb/16 22:27,08/Jun/16 20:48,14/Mar/19 03:08,20/Feb/16 00:40,,,,,,0.16.0,,,,,0,,,,,,,"The following script fail:
{code}
in = LOAD 'recordsOfStringArrays.avro' USING AvroStorage('', '-d -f hdfs://ml-ha/user/aeckstein/notower-rw/streamRunwaysInputSchema.avsc');
{code}

Where ml-ha is the configured HA name for namenode.

The reason is AvroStorage creates a new Configuration object in the backend, which missing the namenode HA configuration in hdfs-site.xml:
{code}
<property>
  <name>dfs.client.failover.proxy.provider.ml-ha</name>
  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
{code}

It should use the configuration passed from frontend rather than construct a new one.",,,,,,,,,,,,,,,,,,,,19/Feb/16 22:28;daijy;PIG-4814-1.patch;https://issues.apache.org/jira/secure/attachment/12788769/PIG-4814-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-02-19 23:49:46.587,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Feb 20 00:40:20 UTC 2016,,,,,,,0|i2t3cn:,9223372036854775807,,,,,,,,,,19/Feb/16 22:28;daijy;It might be hard to come out a test cuz the reproduction involve namenode HA setting.,19/Feb/16 23:49;rohini;UDFContext.getUDFContext().getJobConf() is usually null on the frontend. How does it work when schema is parsed in the front-end?,"19/Feb/16 23:51;daijy;Frontend still using new Configuration(), which is the same as before. The issue happens in the backend.",20/Feb/16 00:02;rohini;+1. Got it. new Configuration() will load the defaults in the frontend and that will include hdfs-site.xml.,20/Feb/16 00:40;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,
Register Groovy UDF with relative path does not work,PIG-4812,12940083,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/Feb/16 07:34,08/Jun/16 20:48,14/Mar/19 03:08,20/Feb/16 00:37,,,,,,0.15.1,0.16.0,,,,0,,,,,,,"The following script does not work:
register 'scriptingudf.groovy' using groovy as myfuncs;

Stack:
{code}
org.apache.pig.backend.executionengine.ExecException: ERROR 2997: Unable to recreate exception from backed error: Error: java.io.IOException: Deserialization error: could not instantiate 'org.apache.pig.scripting.groovy.GroovyEvalFuncObject' with arguments '[scriptingudf.groovy, myfuncs, square]'
	at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:62)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.setup(PigGenericMapBase.java:181)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.RuntimeException: could not instantiate 'org.apache.pig.scripting.groovy.GroovyEvalFuncObject' with arguments '[scriptingudf.groovy, myfuncs, square]'
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:766)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.instantiateFunc(POUserFunc.java:124)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.readObject(POUserFunc.java:576)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.ArrayList.readObject(ArrayList.java:771)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.HashMap.readObject(HashMap.java:1180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.ArrayList.readObject(ArrayList.java:771)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.HashMap.readObject(HashMap.java:1180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:60)
	... 9 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:734)
	... 75 more
Caused by: java.io.IOException: groovy.util.ResourceException: Cannot open URL: file:/private/tmp/hadoop-daijy/nm-local-dir/usercache/daijy/appcache/application_1455767198940_0038/container_1455767198940_0038_01_000005/scriptingudf.groovy
	at org.apache.pig.scripting.groovy.GroovyEvalFunc.<init>(GroovyEvalFunc.java:73)
	at org.apache.pig.scripting.groovy.GroovyEvalFunc.<init>(GroovyEvalFunc.java:59)
	at org.apache.pig.scripting.groovy.GroovyEvalFuncObject.<init>(GroovyEvalFuncObject.java:29)
	... 80 more
Caused by: groovy.util.ResourceException: Cannot open URL: file:/private/tmp/hadoop-daijy/nm-local-dir/usercache/daijy/appcache/application_1455767198940_0038/container_1455767198940_0038_01_000005/scriptingudf.groovy
	at groovy.util.GroovyScriptEngine.getResourceConnection(GroovyScriptEngine.java:316)
	at groovy.util.GroovyScriptEngine.loadScriptByName(GroovyScriptEngine.java:461)
	at org.apache.pig.scripting.groovy.GroovyEvalFunc.<init>(GroovyEvalFunc.java:69)
	... 82 more
{code}",,,,,,,,,,,,,,,,,,,,18/Feb/16 07:36;daijy;PIG-4812-1.patch;https://issues.apache.org/jira/secure/attachment/12788406/PIG-4812-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-02-19 23:46:59.669,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Feb 20 00:37:43 UTC 2016,,,,,,,0|i2szof:,9223372036854775807,,,,,,,,,,18/Feb/16 07:36;daijy;The attached e2e test case fail in multi-node cluster. Also find we don't have e2e test for javascript udf. Add one for sanity check.,19/Feb/16 23:46;rohini;+1,20/Feb/16 00:37;daijy;Patch committed to both trunk and 0.15 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade groovy library to address MethodClosure vulnerability,PIG-4811,12939889,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,17/Feb/16 18:25,08/Jun/16 20:48,14/Mar/19 03:08,20/Feb/16 00:39,,,,,,0.16.0,,,,,0,,,,,,,"Please see https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-3253
As Groovy versions 1.7.0 through 2.4.3 are affected by the above CVE, groovy library should be upgraded.

I'd like to upgrade to the latest version, which is 2.4.5.",,,,,,,,,,,,,,,,,,,,17/Feb/16 18:25;daijy;PIG-4811.patch;https://issues.apache.org/jira/secure/attachment/12788277/PIG-4811.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-02-19 23:44:02.514,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Feb 20 00:39:02 UTC 2016,,,,,,,0|i2syhb:,9223372036854775807,,,,,,,,,,19/Feb/16 23:44;rohini;+1,20/Feb/16 00:39;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
PluckTuple overwrites regex if used more than once in the same script,PIG-4808,12938929,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,eyal,eyal,eyal,12/Feb/16 21:03,08/Jun/16 20:48,14/Mar/19 03:08,15/Feb/16 06:29,0.15.0,,,,,0.15.1,0.16.0,,,,0,,,,,,,"If you define two PluckTuples with different regexes, and use them together, they will overwrite the regex field of one another's (this will not affect cases where non-regex prefixes are used).

Example (from testOutput in TestPluckTuple.java, modified)

a = load 'a' using mock.Storage();
b = load 'b' using mock.Storage();
c = join a by x, b by x;
define pluck1 PluckTuple('a::.*');
define pluck2 PluckTuple('b::.*');
d = foreach c generate pluck1(*) AS pluck1, pluck2(*) AS pluck2;

Expected:

((1,hey,2),(1,sasf,5))
((2,woah,3),(2,woah,6))

Actual:

((1,sasf,5),(1,sasf,5))
((2,woah,6),(2,woah,6))",,,,,,,,,,,,,,,,,,,,12/Feb/16 21:48;eyal;PIG-4808.patch;https://issues.apache.org/jira/secure/attachment/12787739/PIG-4808.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-02-15 06:29:09.464,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Feb 15 06:29:09 UTC 2016,,,,,,,0|i2sslj:,9223372036854775807,,,,,,,,,,"12/Feb/16 21:48;eyal;This patch fixes the bug by removing the static modifier, and adds a new test to check for this case. I also changed the order of the main conditional in the exec method, because checking the boolean (which is fast) should come before the pattern matching (which is slower).","15/Feb/16 06:29;daijy;+1.

Committed to both trunk and 0.15 branch since this leads to wrong result.

Thanks Eyal!",,,,,,,,,,,,,,,,,,,,,,,,,,
UDFContext can be reset in the middle during Tez input and output initialization,PIG-4806,12938369,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,10/Feb/16 22:51,08/Jun/16 20:48,14/Mar/19 03:08,16/Feb/16 18:19,,,,,,0.16.0,,,,,0,,,,,,,"We reinitialize UDFContext ThreadLocal itself in PigProcessor.initialize(). PigProcessor.initialize() is run in parallel with threads that do MRInput.initialize() and MROutput.initialize(). It can overwrite the initialized values in the input and output half way through. This can lead to exceptions if the property stored in UDFContext is mandatory and task will be retried which is ok. If it is not mandatory and it ends up getting null when it actually had a value, wrong data can be produced silently.",,,,,,,,,,,,,,,,,,,,11/Feb/16 01:28;rohini;PIG-4806-1.patch;https://issues.apache.org/jira/secure/attachment/12787401/PIG-4806-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-02-16 06:39:13.12,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Feb 16 18:19:44 UTC 2016,,,,,,,0|i2sp5b:,9223372036854775807,,,,,,,,,,11/Feb/16 01:28;rohini;Not possible to write a reproducible test case as it depends on thread race condition. Verified by running a large job which used to hit the issue couple of times. Just added a test for StoreFunc UDF checking as there was only test for LoadFunc before.,16/Feb/16 06:39;daijy;+1,16/Feb/16 18:19;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,
Provide backward compatibility with mapreduce mapred.task settings,PIG-4801,12938186,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,10/Feb/16 13:52,08/Jun/16 20:48,14/Mar/19 03:08,10/Feb/16 21:37,,,,,,0.16.0,,,,,0,,,,,,,  Some users use settings like mapred.task.id in their UDFs. It is not available in Tez and the job breaks.,,,,,,,,,,,,,,,,,,,,10/Feb/16 13:56;rohini;PIG-4801-1.patch;https://issues.apache.org/jira/secure/attachment/12787261/PIG-4801-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-02-10 18:52:38.455,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Feb 10 21:37:47 UTC 2016,,,,,,,0|i2so0n:,9223372036854775807,,,,,,,,,,10/Feb/16 18:52;daijy;+1,10/Feb/16 21:37;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
EvalFunc.getCacheFiles() fails for different namenode,PIG-4800,12938144,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,10/Feb/16 10:34,08/Jun/16 20:48,14/Mar/19 03:08,10/Feb/16 18:18,,,,,,0.16.0,,,,,0,,,,,,,"{code}
Caused by: java.io.FileNotFoundException: File does not exist: /tmp/input.txt
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1309)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)
	at org.apache.pig.backend.hadoop.executionengine.tez.TezResourceManager.getTezResources(TezResourceManager.java:133)
{code}",,,,,,,,,,,,,,,,,,,,10/Feb/16 11:10;rohini;PIG-4800-1.patch;https://issues.apache.org/jira/secure/attachment/12787244/PIG-4800-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-02-10 18:12:00.881,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Feb 10 18:18:15 UTC 2016,,,,,,,0|i2snrb:,9223372036854775807,,,,,,,,,,10/Feb/16 18:12;daijy;+1,10/Feb/16 18:18;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
big integer and big decimal literals fail to parse,PIG-4798,12937405,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,ssavvides,ssavvides,08/Feb/16 04:02,21/Jun/17 09:15,14/Mar/19 03:08,27/Oct/16 20:18,0.15.0,0.16.0,,,,0.17.0,,,,,0,,,,,,,"For example:

x < 12345678901234567890

with x being a biginteger tries to parse the literal as Integer. 

x < 12345678901234567890BI

""BI"" is not recognized",,,,,,,,,,,,,,,,,,,,27/Oct/16 13:12;szita;PIG-4798.2.patch;https://issues.apache.org/jira/secure/attachment/12835576/PIG-4798.2.patch,25/Oct/16 11:28;szita;PIG-4798.patch;https://issues.apache.org/jira/secure/attachment/12835104/PIG-4798.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-10-25 11:49:48.283,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Oct 28 10:32:58 UTC 2016,,,,,,,0|i2sjfb:,9223372036854775807,,,,,,,,,,"25/Oct/16 11:49;szita;Looks like neither BigIntegers nor BigDecimals can be parsed like this, altough their lexer definition matches correctly to the notation used above ( https://github.com/apache/pig/blob/trunk/src/org/apache/pig/parser/QueryLexer.g#L380 )

The problem is that these token types are not listed under 'scalar' in QueryParser.g . Because of this the generated QueryParser.java/scalar() method will not recognise these types and also no valid transitions are generated in the appropriate state machine (QuerParser.java/DFA141) from BI or BD types.
On top of this LogicalPlanBuilder.java/parseBigInteger only cuts one character from ""12346BI""-like values - need to adjust this to 2 chars.

[^PIG-4798.patch] will fix this and adds some tests too.

[~daijy], [~jcoveney] can you please take a look?","26/Oct/16 22:16;daijy;This is not just a bug fix, it enables a feature never work before. Can you also update document? (src/docs/src/documentation/content/xdocs/basic.xml).","27/Oct/16 13:13;szita;Good point, attached revised patch [^PIG-4798.2.patch]",27/Oct/16 20:18;daijy;+1. Patch committed to trunk. Thanks Adam!,"28/Oct/16 10:32;szita;Thanks for the review, [~daijy]",,,,,,,,,,,,,,,,,,,,,,,
Flushing ObjectOutputStream before calling toByteArray on the underlying ByteArrayOutputStream,PIG-4795,12936613,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,emopers,emopers,emopers,04/Feb/16 05:29,08/Jun/16 20:48,14/Mar/19 03:08,05/Feb/16 06:00,,,,,,0.16.0,,,,,0,easyfix,patch,,,,,"In PigSplit.java
{code}
    private void writeObject(Serializable obj, DataOutput os)
            throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        ObjectOutputStream oos = new ObjectOutputStream(baos);
        oos.writeObject(obj);
        byte[] bytes = baos.toByteArray();
        os.writeInt(bytes.length);
        os.write(bytes);
    }
{code}
When an ObjectOutputStream instance wraps an underlying ByteArrayOutputStream instance,
it is recommended to flush or close the ObjectOutputStream before invoking the underlying instances's toByteArray(). Also, it is a good practice to call flush/close explicitly as mentioned for example at http://stackoverflow.com/questions/2984538/how-to-use-bytearrayoutputstream-and-dataoutputstream-simultaneously-java.
The patch adds a flush method before calling toByteArray().",,,,,,,,,,,,,,,,,,,,04/Feb/16 05:39;emopers;PIG-4795-0.patch;https://issues.apache.org/jira/secure/attachment/12786194/PIG-4795-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-02-05 01:06:11.561,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,Reviewed,,,,Fri Feb 05 06:00:48 UTC 2016,,,Patch Available,,,,0|i2sejj:,9223372036854775807,,,,,,,,,,05/Feb/16 01:06;daijy;Do you see a specific error in your environment?,"05/Feb/16 01:28;emopers;No, I don't see any specific error but as I said it is just a good practice to add flush(), to be on safe side.
","05/Feb/16 06:00;daijy;I don't see any downside.

Patch committed to trunk. Thanks [~emopers]",,,,,,,,,,,,,,,,,,,,,,,,,
PORelationToExprProject filters records instead of returning emptybag in nested foreach after union,PIG-4791,12935203,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,29/Jan/16 18:24,08/Jun/16 20:48,14/Mar/19 03:08,29/Jan/16 22:05,0.14.0,,,,,0.16.0,,tez,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Jan/16 21:20;rohini;PIG-4791-1.patch;https://issues.apache.org/jira/secure/attachment/12785253/PIG-4791-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-01-29 22:00:26.783,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Jan 29 22:05:48 UTC 2016,,,,,,,0|i2s5vb:,9223372036854775807,,,,,,,,,,29/Jan/16 21:20;rohini;POForeach OpsToReset is being set incorrectly on clone and so PORelationToExprProject.reset() which sets sendEmptyBagOnEOP = true is not called causing records to be filtered instead of sending empty bag.,29/Jan/16 22:00;daijy;+1,29/Jan/16 22:05;rohini;Committed to trunk. Thanks for the review Daniel,,,,,,,,,,,,,,,,,,,,,,,,,
Join after union fail due to UnionOptimizer,PIG-4790,12934979,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,daijy,daijy,29/Jan/16 01:44,08/Jun/16 20:48,14/Mar/19 03:08,10/Feb/16 18:12,,,,,,0.16.0,,tez,,,0,,,,,,,"The following script fail to run:
{code}
rmf ooo

a = load 'student.txt' as (name:chararray, age:int, gpa:double);
b = filter a by age > 65;
c = filter a by age <=10;
d = union b, c;
e = join a by name left, d by name;
store e into 'ooo';
{code}

Exception stack:
{code}
Caused by: java.lang.IllegalArgumentException: Edge [scope-43 : org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor] -> [scope-55 : org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor] ({ SCATTER_GATHER : org.apache.tez.runtime.library.input.OrderedGroupedKVInput >> PERSISTED >> org.apache.tez.runtime.library.output.OrderedPartitionedKVOutput >> NullEdgeManager }) already defined!
        at org.apache.tez.dag.api.DAG.addEdge(DAG.java:272)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezDagBuilder.visitTezOp(TezDagBuilder.java:311)
        at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezOperator.visit(TezOperator.java:252)
        at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezOperator.visit(TezOperator.java:56)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:87)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:46)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezJobCompiler.buildDAG(TezJobCompiler.java:65)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezJobCompiler.getJob(TezJobCompiler.java:111)
        ... 20 more
{code}

Disable pig.tez.opt.union the script runs fine.

Seems we shall detect this patten and disallow merge vertex group into a pair already has an edge.",,,,,,,,,,,,,,,,,,,,31/Jan/16 01:55;rohini;PIG-4790-1.patch;https://issues.apache.org/jira/secure/attachment/12785388/PIG-4790-1.patch,10/Feb/16 01:20;daijy;PIG-4790-2.patch;https://issues.apache.org/jira/secure/attachment/12787196/PIG-4790-2.patch,10/Feb/16 10:31;rohini;PIG-4790-3.patch;https://issues.apache.org/jira/secure/attachment/12787238/PIG-4790-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-01-31 01:55:12.853,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Feb 10 18:12:47 UTC 2016,,,,,,,0|i2s4hr:,9223372036854775807,,,,,,,,,,31/Jan/16 01:55;rohini;Additionally modified TezCompiler.testCrossScalarSplit added in PIG-4690 as the golden file generated for JDK 7 and 8 were different due to ordering and the test was failing.,"10/Feb/16 01:20;daijy;The patch fix the script above. However, a more complex script fail for the same stack:
{code}
a = LOAD 'studenttab10k' AS (name:chararray, age:int, gpa:double);

SPLIT a INTO b IF age > 40,
             c IF age <= 40;

d = FOREACH c GENERATE name, age, gpa;

e = FILTER d BY gpa > 3;
f = FILTER d BY gpa <= 3;

g = JOIN e BY name LEFT, f BY name;
h = FOREACH g GENERATE e::name as name, e::age as age, e::gpa as gpa;

i = DISTINCT h;

j = FILTER b BY gpa > 3;
k = FILTER b by gpa <= 3;

l = JOIN j BY name LEFT, k BY name;
m = FOREACH l generate j::name as name, j::age as age, j::gpa as gpa;
n = DISTINCT m;

m = UNION e, i, j, n;

n = JOIN a BY name, m BY name;

STORE n INTO 'ooo';
{code}

Attach another patch to fix both.",10/Feb/16 09:53;rohini;The difference in the complex script is that one of the edges is a vertex group. The fix has a problem though. It turns of UnionOptimizer for the simple case as well where the edges are normal which the previous patch handled. We should avoid turning off UnionOptimizer as much as possible because the performance of UnorderedPartitionedKVOutput is currently very bad and is not fixed yet. Would be good to add the script to TestTezCompiler as well. ,10/Feb/16 10:31;rohini;Attached patch that takes care of it. It was just addition of simple check on uniqueMembers to your existing condition and putting back my initial checks.,"10/Feb/16 17:50;daijy;+1 for PIG-4790-3.patch.
",10/Feb/16 18:12;rohini;Committed to trunk. Thanks for the review and the patch Daniel.,,,,,,,,,,,,,,,,,,,,,,
CROSS will not work correctly with Grace Parallelism,PIG-4786,12933966,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,rohini,rohini,25/Jan/16 22:49,08/Jun/16 20:48,14/Mar/19 03:08,31/May/16 19:47,,,,,,0.16.0,0.17.0,tez,,,0,,,,,,,PigImplConstants.PIG_CROSS_PARALLELISM accessed in GFCross UDF will refer to the old parallelism.,,,,,,,,,,,,,,,,,,,,31/May/16 06:32;daijy;PIG-4786-1.patch;https://issues.apache.org/jira/secure/attachment/12807051/PIG-4786-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-05-31 19:47:46.517,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 31 19:47:46 UTC 2016,,,,,,,0|i2ry8n:,9223372036854775807,,,,,,,,,,31/May/16 17:16;rohini;+1,31/May/16 19:47;daijy;Patch committed to both trunk and 0.16 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfMemoryError: GC overhead limit exceeded with POPartialAgg,PIG-4782,12931721,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,17/Jan/16 02:13,08/Jun/16 20:48,14/Mar/19 03:08,17/Jan/16 20:49,,,,,,0.16.0,,,,,0,,,,,,,"   In some cases, even though spill is triggered the main thread is still executing some user UDF which constructs a DataBag which requires lot of memory. Since we block on spill in POPartialAgg, there is kind of a deadlock as there is no memory for the user DataBag to grow and spill does not happen and job finally failing with OutOfMemoryError: GC overhead limit exceeded. So need to make POPartialAgg non-blocking so that the user DataBag can be spilled first for the code to reach POPartialAgg and spill.",,,,,,,,,,,,,,,,,,,,17/Jan/16 02:23;rohini;PIG-4782-1.patch;https://issues.apache.org/jira/secure/attachment/12782747/PIG-4782-1.patch,30/Jan/16 17:53;rohini;PIG-4782-2-fixslowness.patch;https://issues.apache.org/jira/secure/attachment/12785363/PIG-4782-2-fixslowness.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-01-17 20:34:21.366,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Feb 01 16:26:02 UTC 2016,,,,,,,0|i2rkfj:,9223372036854775807,,,,,,,,,,17/Jan/16 20:34;daijy;+1,17/Jan/16 20:49;rohini;Committed to trunk. Thanks for the review Daniel.,"30/Jan/16 17:53;rohini;Though the patch avoided OOM, it introduced slowness with the sleep of 5 sec in cases when spill is called multiple times before the code reaches POPartialAgg and it is aggregated. It gets slow by 5 sec * number of times the spill was called. The uploaded PIG-4782-2-fixslowness.patch skips the sleep second time if spill is not processed by POPartialAgg yet.",01/Feb/16 08:51;daijy;+1 for PIG-4782-2-fixslowness.patch.,01/Feb/16 16:26;rohini;Committed PIG-4782-2-fixslowness.patch to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,
testBZ2Concatenation[pig.bzip.use.hadoop.inputformat = true] failing due to successful read,PIG-4779,12929912,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,13/Jan/16 18:57,08/Jun/16 20:48,14/Mar/19 03:08,20/Jan/16 18:04,,,,,,0.16.0,,,,,0,,,,,,,"From [Pig-3251|https://issues.apache.org/jira/browse/PIG-3251?focusedCommentId=15096780#comment-15096780], {{testBZ2Concatenation [pig.bzip.use.hadoop.inputformat = true\]}} is failing .

{quote}
Koji Noguchi,
https://builds.apache.org/job/Pig-trunk-commit/2278/testReport/org.apache.pig.test/TestBZip/testBZ2Concatenation_pig_bzip_use_hadoop_inputformat___true__/ tests are failing. This should because concatenated bzip works with hadoop's TextInputFormat. Can you fix the testcase? I think it would be good to keep the original one which throws the exception for Pig's bzipinputformat and another one for hadoop's which passes and also verifies the output.
{quote}
",,,,,,,,,,,,,,,,,,,,14/Jan/16 21:34;knoguchi;pig-4779-v01.patch;https://issues.apache.org/jira/secure/attachment/12782370/pig-4779-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-01-19 22:27:50.924,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Jan 20 18:04:01 UTC 2016,,,,,,,0|i2r99r:,9223372036854775807,,,,,,,,,,"14/Jan/16 21:34;knoguchi;Sorry for the delay.

In my test environment (on both mac and linux), test was incorrectly passing by throwing
{panel}
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: java.io.IOException: unexpected end of stream
{panel}
even when hadoop's TextInputFormat was used.  

Looking at the test, found that concatenated bzip file was corrupt.
I don't think we can use character-line based BufferedWriter/Reader for creating a binary(bzip) file.  
Though, I don't know how the build on the description was correctly failing...

Fixing test's {{catInto}} method and adjusting the testBZ2Concatenation to follow [~rohini]'s suggestion.

{quote}
Can you fix the testcase? I think it would be good to keep the original one which throws the exception for Pig's bzipinputformat and another one for hadoop's which passes and also verifies the output.
{quote}
",19/Jan/16 22:27;rohini;+1,20/Jan/16 18:04;knoguchi;Thanks for the review Rohini.  Committed to trunk(0.16).,,,,,,,,,,,,,,,,,,,,,,,,,
Better default values for shuffle bytes per reducer,PIG-4775,12929030,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,10/Jan/16 19:56,08/Jun/16 20:48,14/Mar/19 03:08,11/Jan/16 02:55,,,,,,0.16.0,,,,,0,,,,,,,"Currently the code does not set TEZ_SHUFFLE_VERTEX_MANAGER_DESIRED_TASK_INPUT_SIZE if BYTES_PER_REDUCER_PARAM is not set or equal to DEFAULT_BYTES_PER_REDUCER (1G). Which makes it default to TEZ_SHUFFLE_VERTEX_MANAGER_DESIRED_TASK_INPUT_SIZE_DEFAULT = 1024*1024*100L (100MB) which is low and can cause to produce more output files than usual. Removing that check and defaulting to 1G would be bad for performance as in case of mapreduce that was based as map input size, but in Tez it is taken as map output size. So setting 384MB as default for group by as they usually reduce size of data output and keeping 256MB for joins as they increase size of output data.

Did not touch order by and skewed join as DEFAULT_BYTES_PER_REDUCER of 1G is honored there. Using 1G for them would be similar to mapreduce, as map input and output would be same for those cases. ",,,,,,,,,,,,,,,,,,,,10/Jan/16 19:57;rohini;PIG-4775-1.patch;https://issues.apache.org/jira/secure/attachment/12781467/PIG-4775-1.patch,11/Jan/16 02:44;rohini;PIG-4775-2.patch;https://issues.apache.org/jira/secure/attachment/12781485/PIG-4775-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-01-11 02:35:25.845,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Jan 11 02:55:53 UTC 2016,,,,,,,0|i2r3tz:,9223372036854775807,,,,,,,,,,"11/Jan/16 02:35;daijy;384 * 1024 * 1024L and 256 * 1024 * 1024L should be predefined constants (intermediateTaskInputSize too, but that's not part of the patch). Otherwise +1.","11/Jan/16 02:44;rohini;Added the constants. intermediateTaskInputSize is not a constant, but determined in the constructor based on filesystem default block size.","11/Jan/16 02:47;daijy;I mean the default 128MB. Anyway, this is not related to the patch.

+1","11/Jan/16 02:55;rohini;Got it. Will make that as a constant in the next patch that involves TezDAGBuilder.java

Committed to trunk. Thanks for the review Daniel.",,,,,,,,,,,,,,,,,,,,,,,,
"Fix NPE in SUM,AVG,MIN,MAX UDFs for null bag input",PIG-4774,12928930,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,10/Jan/16 01:36,08/Jun/16 20:48,14/Mar/19 03:08,11/Jan/16 00:19,,,,,,0.16.0,,,,,0,,,,,,,"For UDF backward compatibility issue after POStatus.STATUS_NULL refactory issue, PIG-4184 fixed the udfs to handle null by adding input.get(0) == null check in all the UDFs. UDFs extending AlgebraicMathBase, AVG, MIN, MAX, etc was not fixed.

Script to reproduce NPE. It is an odd usage doing aggregation after join instead of group by which one user was doing and rewrite moving aggregation after group by fixed the NPE. Might be rare, but there might be other cases where user call those functions with a bag directly without group by which might cause nulls to be passed to it.

A = LOAD '/tmp/data' as (f1:int, f2:int, f3:int);
B = LOAD '/tmp/data1' as (f1:int, f2:int, f3:int);
A1 = GROUP A by f1;
A2 = FOREACH A1 GENERATE group as f1, $1;
C = JOIN B by f1 LEFT, A2 by f1;
D = FOREACH C GENERATE B::f1, (double)SUM(A2::A.f3)/SUM(A2::A.f2);
STORE D into '/tmp/out';",,,,,,,,,,,,,,,,,,,,10/Jan/16 01:54;rohini;PIG-4774-1-withoutwhitespacechanges.patch;https://issues.apache.org/jira/secure/attachment/12781430/PIG-4774-1-withoutwhitespacechanges.patch,10/Jan/16 01:54;rohini;PIG-4774-1.patch;https://issues.apache.org/jira/secure/attachment/12781431/PIG-4774-1.patch,10/Jan/16 18:41;rohini;PIG-4774-2.patch;https://issues.apache.org/jira/secure/attachment/12781461/PIG-4774-2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-01-10 07:35:41.197,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Jan 11 00:19:32 UTC 2016,,,,,,,0|i2r37r:,9223372036854775807,,,,,,,,,,10/Jan/16 07:35;daijy;Do you mind adding the script as test case?,10/Jan/16 18:28;rohini;  Added null check in TestBuiltin.java instead of a script as that covers all the different UDFs.,10/Jan/16 22:30;daijy;+1,11/Jan/16 00:19;rohini;Committed patch to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,
[Pig on Tez] Secondary key descending sort in nested foreach after union does ascending instead,PIG-4773,12928736,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,08/Jan/16 21:11,08/Jun/16 20:48,14/Mar/19 03:08,11/Jan/16 06:23,,,,,,0.16.0,,tez,,,0,,,,,,,"  PigSecondaryKeyComparator does not implement a compare(WritableComparable a, WritableComparable b) method. In case of  OrderedGroupedMergedKVInput with vertex groups, object comparison instead of byte comparison is done when merging the multiple inputs into one. Since the compare API is not overridden, WritableComparator compare API is called and  the value types are directly compared which means it is always ascending.",,,,,,,,,,,,,,,,,,,,08/Jan/16 21:17;rohini;PIG-4773-1.patch;https://issues.apache.org/jira/secure/attachment/12781303/PIG-4773-1.patch,10/Jan/16 19:44;rohini;PIG-4773-2.patch;https://issues.apache.org/jira/secure/attachment/12781466/PIG-4773-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-01-11 05:41:35.682,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Jan 11 06:23:50 UTC 2016,,,,,,,0|i2r20n:,9223372036854775807,,,,,,,,,,10/Jan/16 19:44;rohini;Fixed the wrong assert in BinInterSedes (compare of byte arrays had the correct assert) which was causing junit tests which had jvm assertions enabled to fail. ,"11/Jan/16 05:41;daijy;Do we need to set ""pig.secondarySortOrder"" again at vertex level? We already set it at edge level.","11/Jan/16 06:12;rohini;bq. Do we need to set ""pig.secondarySortOrder"" again at vertex level? We already set it at edge level.
  Main reason for setting it at vertex level is for comparator intialization in POShuffleTezLoad. POShuffleTezLoad makes use of comparator for key comparison for multiple inputs. Even though we don't have the case of multiple inputs (cogroup) with SecondaryKeyComparison now which is the code path in which it will be exercised, added it now itself to avoid missing that in future and added a TODO. 

","11/Jan/16 06:17;daijy;I see. Once we have two inputs for secondary sort, this will be tricky, since sorting should be an edge property.

+1 for now.",11/Jan/16 06:23;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,
OOM with POPartialAgg in some cases,PIG-4770,12927111,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,04/Jan/16 22:09,08/Jun/16 20:48,14/Mar/19 03:08,12/Jan/16 06:53,,,,,,0.16.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Jan/16 19:00;rohini;PIG-4770-1.patch;https://issues.apache.org/jira/secure/attachment/12781620/PIG-4770-1.patch,11/Jan/16 22:24;rohini;PIG-4770-2.patch;https://issues.apache.org/jira/secure/attachment/12781662/PIG-4770-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-01-11 19:18:27.613,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jan 12 06:53:01 UTC 2016,,,,,,,0|i2qs07:,9223372036854775807,,,,,,,,,,"11/Jan/16 19:00;rohini;Changes done:
     1) Avoid hashmap resizing and arraylist (value of hashmap) resizing as much as possible to reduce the memory pressure
          -   Optimize group all by initializing the values List to same size as maxListSize.
          - Changed default minReduction to 7 as secondary hashmap will grow to 4096 with default reduction of 10 or 7. 7x reduction is also good for performance. So lowered the default.
      2) Fixed an NPE in SpillableMemoryManager which was due to not using precalculated memory size. It was a mistake in my fix for PIG-4012.",11/Jan/16 19:18;daijy;+1,"11/Jan/16 22:24;rohini;[~daijy], 
   Addressed your concern of checking for key equals ""all"" sounding a bit hacky. The patch also would encounter issues if the user had a key ""all"" and it was the first key.
 Now determining if it is GROUP ALL in CombinerOptimizer by looking at the plan and setting it in POPartialAgg. This is more fool proof.",11/Jan/16 22:27;daijy;+1,12/Jan/16 06:53;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,
UnionOptimizer hits errors when merging vertex group into split,PIG-4769,12926898,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,04/Jan/16 07:28,08/Jun/16 20:48,14/Mar/19 03:08,05/Jan/16 08:31,,,,,,0.16.0,,,,,0,,,,,,," 1) For a script that needs store vertex group removed and merged into split, it gives error

Caused by: java.lang.IllegalArgumentException: VertexGroup must have at least 2 members

as we leave the vertexgroup with just one member in it (POSplit).

2)  For a script that needs non-store vertex group removed, it gives error

Input from vertex scope-xxx is missing

In this case, vertex group is removed but successor inputs are not replaced correctly.",,,,,,,,,,,,,,,,,,,,04/Jan/16 07:35;rohini;PIG-4769-1.patch;https://issues.apache.org/jira/secure/attachment/12780272/PIG-4769-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-01-05 01:51:30.619,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jan 05 08:31:09 UTC 2016,,,,,,,0|i2qqov:,9223372036854775807,,,,,,,,,,05/Jan/16 01:51;daijy;+1,05/Jan/16 08:31;rohini;Thanks for the review Daniel. Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,
EvalFunc reporter is null in Tez,PIG-4768,12926864,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,04/Jan/16 03:12,08/Jun/16 20:48,14/Mar/19 03:08,05/Jan/16 08:29,,,,,,0.16.0,,tez,,,0,,,,,,," One user hit NPE in his UDF because of that. Only in Tez 0.8, support for progress notification is added (TEZ-808). For now will set an empty ProgressableReporter.",,,,,,,,,,,,,,,,,,,,04/Jan/16 03:15;rohini;PIG-4768-1.patch;https://issues.apache.org/jira/secure/attachment/12780261/PIG-4768-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-01-05 01:48:50.994,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jan 05 08:29:29 UTC 2016,,,,,,,0|i2qqhb:,9223372036854775807,,,,,,,,,,05/Jan/16 01:48;daijy;+1,05/Jan/16 08:29;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
Insufficient check for the number of arguments in runpigmix.pl,PIG-4763,12922927,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,sekikn,sekikn,sekikn,18/Dec/15 04:37,08/Jun/16 20:48,14/Mar/19 03:08,19/Jan/16 01:18,,,,,,0.16.0,,,,,0,,,,,,,"runpigmix.pl first checks the number of arguments as follows:

{code}
  3 if(scalar(@ARGV) < 6 )
  4 {
  5     print STDERR ""Usage: $0 <pig_home> <pig_bin> <pigmix_jar> <hadoop_home> <hadoop_bin> <pig mix scripts dir> <hdfs_root> <pigmix_output> [parallel] [numruns] [runmapreduce] [cleanup_after_test]\n"";
  6     exit(-1);
  7 }
{code}

but the number of the required parameters is 8, so this check seems insufficient.",,,,,,,,,,,,,,,,,,,,18/Dec/15 04:49;sekikn;PIG-4763.1.patch;https://issues.apache.org/jira/secure/attachment/12778413/PIG-4763.1.patch,24/Dec/15 00:16;sekikn;PIG-4763.2.patch;https://issues.apache.org/jira/secure/attachment/12779345/PIG-4763.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-12-21 18:45:44.574,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jan 19 01:18:25 UTC 2016,,,,,,,0|i2q2nz:,9223372036854775807,,,,,,,,,,21/Dec/15 18:45;rohini;Last two parameters are optional.,"22/Dec/15 06:37;sekikn;Thanks for the comment, [~rohini]. I understood that the root directory is used as the input and output directory if the 7th (hdfs_root) and 8th (pigmix_output) parameters are not specified. But if they are optional, wouldn't it be better to set default values just like ""parallel"" or other optional parameters to suppress warnings? In addition, the following code seems to assume that pigmixoutput is always set. Shouldn't it be skipped?

{code}
 41 print STDERR ""Removing output dir $pigmixoutput \n"";
 42 $cmd = ""$hadoopbin fs -rmr $pigmixoutput"";
 43 print STDERR ""Going to run $cmd\n"";
 44 print STDERR `$cmd 2>&1`;
{code}

Sorry if I misunderstand something.","22/Dec/15 17:03;rohini;bq. But if they are optional, wouldn't it be better to set default values just like ""parallel"" or other optional parameters to suppress warnings?
  Agree with that. Looks like it will cause warnings or errors if the 7th and 8th one are not set. Can you change the patch to add default values based on the ones we usually specify in pigmix build.xml ?  We can change to say 8 arguments are mandatory too, but it is easier for the users if they have to specify less arguments as most of the time defaults are good.",24/Dec/15 00:16;sekikn;Patch updated. Thanks for the advice!,19/Jan/16 01:18;rohini;Committed to trunk. Thanks for the patch [~sekikn],,,,,,,,,,,,,,,,,,,,,,,
"TezDAGStats.convertToHadoopCounters is not used, but impose MR counter limit",PIG-4760,12920924,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,10/Dec/15 23:55,08/Jun/16 20:48,14/Mar/19 03:08,22/Dec/15 19:23,,,,,,0.15.1,0.16.0,tez,,,0,,,,,,,"PIG-4529 reveal a MR counter limit issue imposed to Pig. The original fix won't work in Oozie since Oozie launcher already imposed counter limit and Pig cannot change it (there is an internal flag in Limits to make sure it does not init twice).

Actually the MR counter is completely redundant and should be removed.",,,,,,,,,,,,,,,,,,,,10/Dec/15 23:57;daijy;PIG-4760-1.patch;https://issues.apache.org/jira/secure/attachment/12776924/PIG-4760-1.patch,11/Dec/15 02:03;daijy;PIG-4760-2.patch;https://issues.apache.org/jira/secure/attachment/12776943/PIG-4760-2.patch,11/Dec/15 23:57;daijy;PIG-4760-3.patch;https://issues.apache.org/jira/secure/attachment/12777205/PIG-4760-3.patch,15/Dec/15 22:12;daijy;PIG-4760-4.patch;https://issues.apache.org/jira/secure/attachment/12777837/PIG-4760-4.patch,22/Dec/15 19:22;daijy;PIG-4760-5.patch;https://issues.apache.org/jira/secure/attachment/12779106/PIG-4760-5.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2015-12-11 00:26:26.508,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Dec 22 19:23:14 UTC 2015,,,,,,,0|i2pquf:,9223372036854775807,,,,,,,,,,"11/Dec/15 00:26;rohini;bq. Actually the MR counter is completely redundant and should be removed.
  It is used by Oozie and cannot remove it.

https://git1-us-west.apache.org/repos/asf?p=oozie.git;a=blob;f=sharelib/pig/src/main/java/org/apache/oozie/action/hadoop/OoziePigStats.java;h=c3a31b596e7914709f017eee21a498985055c897;hb=refs/heads/master#l99

  jobStatsGroup.put(""HADOOP_COUNTERS"", toJSONFromCounters(jobStats.getHadoopCounters()));",11/Dec/15 02:01;daijy;Seems the only solution is to truncate the counter. Attach 2nd patch.,"11/Dec/15 23:57;daijy;Discussed with [~rohini], we don't want to truncate the counters for Oozie. In the new patch, I use reflection to change the accessibility of the Limits field. This is hacky but seems the only viable way.

Also fix a typo in PIG-4529.",15/Dec/15 18:32;rohini;Can you put the reflection in a try catch Throwable block and log a warning if there is any failure. That way it does not break Pig if there is some change to Hadoop.,15/Dec/15 22:12;daijy;Added try catch block.,15/Dec/15 23:05;rohini;+1. Can you change Exception to Throwable before checking in as reflection will cause Error like NoSuchFieldError. ,22/Dec/15 19:22;daijy;Attach the final patch to commit.,22/Dec/15 19:23;daijy;Patch committed to both trunk and 0.16 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,
Fix Classresolution_1 e2e failure,PIG-4759,12920825,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,10/Dec/15 17:43,08/Jun/16 20:48,14/Mar/19 03:08,10/Feb/16 21:20,,,,,,0.16.0,,,,,0,,,,,,,"  We had left it as a known issue to be fixed later as that was a very odd and uncommon usage put in just for the particular testcase - store into a file with one StoreFunc, but read back with a different reader in the same script. But came across one of our user doing that same case. Storing bags using PigStorage and reading back with TextLoader and processing them as plain strings later on.

",,,,,,,,,,,,,,,,,,,,05/Feb/16 23:07;rohini;PIG-4759-1.patch;https://issues.apache.org/jira/secure/attachment/12786587/PIG-4759-1.patch,10/Feb/16 21:03;rohini;PIG-4759-2.patch;https://issues.apache.org/jira/secure/attachment/12787338/PIG-4759-2.patch,11/Feb/16 14:57;rohini;PIG-4759-fix-testfailure.patch;https://issues.apache.org/jira/secure/attachment/12787481/PIG-4759-fix-testfailure.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-02-10 19:37:19.396,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Feb 12 15:38:49 UTC 2016,,,,,,,0|i2pq8f:,9223372036854775807,,,,,,,,,,10/Feb/16 19:37;daijy;I am hesitate to introduce a separate DAG generally for that. Can we just detect such use case and only cut a DAG in that case?,"10/Feb/16 19:39;rohini;We need to that as that is the expected functionality. Else it is breaking scripts and is backward incompatible with mapreduce. 

Do not want to special case for same loader and try to match field names. [~knoguchi] had a case where there was a static variable issue with one of the native libraries and he had used load and store to separate it into multiple jobs. Also these cases are rare enough it is not worth optimizing. Even if multiple DAG, it reuses same application and containers. So not too much of a cost.",10/Feb/16 21:03;rohini;Added documentation to explicitly call that out as per our offline discussion.,10/Feb/16 21:16;daijy;+1,10/Feb/16 21:20;rohini;Committed to trunk. Thanks for the review Daniel.,11/Feb/16 14:48;rohini;TestMultiQueryLocal.testStoreOrder failed. There were issues with the way split of the DAGs were done. Fixed that in PIG-4759-fix-testfailure.patch. ,11/Feb/16 18:42;daijy;Can you elaborate what has been changed in DAG split?,11/Feb/16 20:26;rohini;Put up the changes in review board - https://reviews.apache.org/r/43498/ as it will be easier to read and make sense of the comments in the code. Also added comments in the review board description on why I had to do that.,12/Feb/16 06:07;daijy;+1,12/Feb/16 15:38;rohini;Committed PIG-4759-fix-testfailure.patch to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,
Job stats on successfully read/output records wrong with multiple inputs/outputs,PIG-4757,12919555,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,07/Dec/15 02:40,08/Jun/16 20:48,14/Mar/19 03:08,06/Jan/16 22:26,,,,,,0.16.0,,tez,,,0,,,,,,,"TezVertexStats uses TaskCounter.INPUT_RECORDS_PROCESSED to display records read from MRInput. But in cases of replicate join or scalar it also includes replicate join input.  Need to have a pig specific counter (MULTI_INPUTS_RECORD_COUNTER) in POSimpleTezLoad.

TezVertexStats uses TaskCounter.OUTPUT_RECORDS to display records stored to MROutput if there is single store. If there are multiple stores it uses MULTI_STORE_RECORD_COUNTER and there are no issues. If there is a single store with another output, then value from OUTPUT_RECORDS is wrong. Need to use MULTI_STORE_RECORD_COUNTER for all cases even if there is no multiple store.",,,,,,,,,,,,,,,,,,,,04/Jan/16 21:53;rohini;PIG-4757-1.patch;https://issues.apache.org/jira/secure/attachment/12780391/PIG-4757-1.patch,05/Jan/16 08:17;rohini;PIG-4757-2.patch;https://issues.apache.org/jira/secure/attachment/12780493/PIG-4757-2.patch,06/Jan/16 18:47;rohini;PIG-4757-3.patch;https://issues.apache.org/jira/secure/attachment/12780809/PIG-4757-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-01-05 01:48:00.504,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Jan 06 22:26:11 UTC 2016,,,,,,,0|i2pief:,9223372036854775807,,,,,,,,,,"07/Dec/15 02:44;rohini;Another thing to fix is for hbase inputs, there is no bytes read with Tez, but it is displayed in MR.","04/Jan/16 21:53;rohini;bq. Another thing to fix is for hbase inputs, there is no bytes read with Tez, but it is displayed in MR.
  This is invalid. Pig displays HDFS_BYTES_WRITTEN in MR for hbase inputs which is incorrect. Not displaying anything is better in Tez.

HBase Counters
BYTES_IN_REMOTE_RESULTS 	166228275 	0 	166228275
BYTES_IN_RESULTS 	166228275 	0 	166228275

Displaying bytes in above counter would make sense, but that also could be incorrect if any UDF also accessed hbase in the plan. So leaving it as is for now.",05/Jan/16 01:48;daijy;Posted several comments on RB: https://reviews.apache.org/r/41913,05/Jan/16 08:17;rohini;Fixed comment,05/Jan/16 20:27;daijy;+1,"06/Jan/16 18:47;rohini;  Realized that the patch missed the testcase file. Added that. [~daijy], could you review that?",06/Jan/16 19:34;daijy;+1,06/Jan/16 22:26;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,
Typo in runpigmix script,PIG-4755,12919128,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mitdesai,mitdesai,mitdesai,05/Dec/15 00:05,08/Jun/16 20:48,14/Mar/19 03:08,05/Dec/15 00:50,,,,,,0.16.0,,,,,0,,,,,,,PIG-4753 created a new variable in the script to delete the output after each test ends. There is a typo in the variable deceleration.,,,,,,,,,,,,,,,,,,,,05/Dec/15 00:07;mitdesai;PIG-4755.1.patch;https://issues.apache.org/jira/secure/attachment/12775885/PIG-4755.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-12-05 00:50:14.378,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Dec 05 00:50:14 UTC 2015,,,,,,,0|i2pfrr:,9223372036854775807,,,,,,,,,,05/Dec/15 00:07;mitdesai;Attached the patch to fix the typo,05/Dec/15 00:50;daijy;Patch committed to trunk. Thanks Mit!,,,,,,,,,,,,,,,,,,,,,,,,,,
DateTimeWritable forgets Chronology,PIG-4748,12916806,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,mju,mju,29/Nov/15 12:59,21/Jun/17 09:15,14/Mar/19 03:08,22/May/17 19:53,0.16.0,,,,,0.17.0,,impl,,,0,,,,,,,"The following test fails:

{code}
@Test
public void foo() throws IOException {
    DateTime nowIn = DateTime.now();
    DateTimeWritable in = new DateTimeWritable(nowIn);

    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
    DataOutputStream dataOut = new DataOutputStream(outputStream);
    in.write(dataOut);
    dataOut.flush();

    // read from byte[]
    DateTimeWritable out = new DateTimeWritable();
    ByteArrayInputStream inputStream = new ByteArrayInputStream(
      outputStream.toByteArray());
    DataInputStream dataIn = new DataInputStream(inputStream);
    out.readFields(dataIn);

    assertEquals(in.get(), out.get());
}
{code}

In equals(), the original instance has
{code}
ISOChronology[Europe/Berlin]
{code}
while the deserialized instance has
{code}
ISOChronology[+01:00]
{code}

",,,,,,,,,,,,,,,,,,,,02/Feb/17 16:32;szita;PIG-4748.2.patch;https://issues.apache.org/jira/secure/attachment/12850653/PIG-4748.2.patch,17/Mar/17 13:59;szita;PIG-4748.3.patch;https://issues.apache.org/jira/secure/attachment/12859294/PIG-4748.3.patch,13/Dec/16 13:37;szita;PIG-4748.patch;https://issues.apache.org/jira/secure/attachment/12842998/PIG-4748.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-12-13 13:39:51.963,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 22 19:53:19 UTC 2017,,,,,,,0|i2p1g7:,9223372036854775807,,,,,,,,,,"13/Dec/16 13:39;szita;Relying on timezone ID rather then offset millis solves this. Attached fix in [^PIG-4748.patch]
[~rohini] can take a look please?",16/Dec/16 15:00;nkollar;Using zone ID instead of offset looks good to me.,"04/Jan/17 00:43;daijy;It might bloat the output too much. Is there some enum, or we have to rely on compression?",04/Jan/17 09:43;szita;I think we'll have to rely on compression. Using enums would make it hard to follow ever-updating timezones: http://joda-interest.219941.n2.nabble.com/Are-there-enums-of-the-timezone-id-strings-somewhere-td6274470.html,"31/Jan/17 19:24;szita;[~daijy], what's your opinion?","31/Jan/17 19:42;daijy;Some customer might not compress intermediate file, so that might be a problem. Can we do getAvailableIDs() on the frontend and create enum on the fly?","31/Jan/17 21:10;rohini;bq. Can we do getAvailableIDs() on the frontend and create enum on the fly?
  It is a good idea. Can sort the DateTimeZone.getAvailableIDs()  in a list and use the index of that list in the serialization. One problem I see with it though is if more than two versions of jodatime is in classpath then DateTimeZone.getAvailableIDs() might differ and might produce different results in different tasks. Somehow have to ensure we get same result in both frontend and all tasks in backend for DateTimeZone.getAvailableIDs(). ","31/Jan/17 21:15;daijy;You may pass the mapping to conf. It is around 9k, guess it is ok.","02/Feb/17 16:38;szita;I upgraded the patch for this, see [^PIG-4748.2.patch].
DateTimeWritables now write (long,int,int) corresponding to the DateTime instant (millis), offsetInMillis, position in the zone ID list respectively.
Zone list itself is carried along to backends via JobConf. For some inputs we can only rely on offsetInMillis (e.g. +01:00), on others we rely on ZoneID (e.g. America/Los_Angeles).","08/Feb/17 16:05;szita;Would [^PIG-4748.2.patch] be something you had in mind [~rohini], [~daijy]?","03/Mar/17 07:54;daijy;Looks good to me. [~rohini], do you mind adding the mapping to configuration?","06/Mar/17 20:23;rohini;[~szita],

Couple of questions:
  1) When will zone id be -1 ? When user specifies just offset?
  2) Why not do writeShort for offset as before?
   
Comments:
1) Can you move the config from PigConfiguration to PigImplConstants which is for configs used internally for implementation purposes.
2) availableZoneIDs.size() > 0 check is redundant in availableZoneIDs != null && availableZoneIDs.size() > 0
3) Does it work with Tez e2e? Currently it is only set in the vertex conf which will only be available to PigProcessor and not available to the PigInputFormat or PigOutputFormat which will actually be deserializing the input or serializing it as output. I would suggest adding it to UDFContext instead like a UDF does. That will take care of compressing the setting as well so that it occupies less space.
4) testDateTimeZoneOnCluster - Please validate the value of DateTime objects as well. You can use Util.checkQueryOutputsAfterSort for comparing exact values with a expected list.
5) Can you move testDateTimeWritables from TestDataModel.java to TestDateTime.java?
5) Can you move tests from TestDefaultDateTimeZone.java into TestDateTime.java and delete TestDefaultDateTimeZone.java? Each new test class is fork of a new jvm which makes the tests take more time. So wherever possible would be good to avoid creating a new class.

","17/Mar/17 14:10;szita;Thanks for the comments [~rohini], I've uploaded a new patch to address them: [^PIG-4748.3.patch]

Questions:
1, Yes, if we only supply the zone as offset (e.g +01:00) we won't find this in the zone list, so -1 will be the position in the zone list.
This is why we have to send the offset as millis as well.
2, Joda time returns offsetAsMillis in integer. If we want to keep a general approach we should leave this as is. We could argue that a +00:00:00.001 zone offset doesn't make sense and we could programatically save some memory by transforming it to short by dividing it and loosing some precision. However I would rather go with the general approach.

Comments:
1, Done
2, Done
3, Done, I think the previous solution would have worked as well but in the new patch I made this cleaner and made use of UDFContext()
4, Done, one of the tuples has a DateTime object where the zone information is deliberately left to be default. So I had to apply a little trick at result comparation.
5, Done, test classes are merged","17/May/17 21:35;rohini;bq. one of the tuples has a DateTime object where the zone information is deliberately left to be default. 
  What was the output before this patch? Ideally simple load+store should produce same data as original input. Currently it is adding the system zone to it which we should try to avoid. 

Everything else is perfect.","22/May/17 13:02;szita;[~rohini] I think this behaviour is consistent with what we have in trunk now:
{code}
grunt> cat fruits.csv
apple   1       red     1971-01-01T00:00:00.000+00:00
orange  2       orange  1972-01-01T00:00:00.000+00:00
kiwi    3       green   1973-01-01T00:00:00.000+01:00
orange  4       orange  1974-01-01T00:00:00.000
grunt> A = LOAD 'fruits.csv' as (name:chararray, id:int, fruit:chararray, fut:datetime);
grunt> dump A;
(apple,1,red,1971-01-01T00:00:00.000Z)
(orange,2,orange,1972-01-01T00:00:00.000Z)
(kiwi,3,green,1973-01-01T00:00:00.000+01:00)
(orange,4,orange,1974-01-01T00:00:00.000+01:00)
{code}
(On my US-based cluster I get (-07:00) for the last record using the same data and query)",22/May/17 17:45;rohini;+1,"22/May/17 19:53;szita;This is now committed to trunk, thanks for the review [~rohini]",,,,,,,,,,,
Check and fix clone implementation for all classes extending PhysicalOperator,PIG-4737,12913487,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,16/Nov/15 22:58,08/Jun/16 20:48,14/Mar/19 03:08,12/Jan/16 23:23,,,,,,0.16.0,,,,,0,,,,,,,"    PhysicalOperator.clone() eventually calls Object.clone() which only does a shallow copy (javadoc wrongly says deep copy) and this causes issues with UnionOptimizer in Tez. Most of the clone is already fixed due to issues found earlier, but recently ran into an issue with POStream where after clone same reference was retained to binaryOutputQueue and binaryInputQueue and caused the script to hang. 

Mostly cloned operators in Union go to different tez vertex plans and the issue would not have occurred, but in the particular case due to replicated join and with the combination of multi-query and union optimization, both the cloned plans of union ended up in the same vertex(one that loads C). That single vertex will handle both the replicated joins and streaming in two sub-plans of split and store the final result in g.

{code}
A = LOAD 'a';
B = LOAD 'b';
C = LOAD 'c';
D = JOIN C by $0, A by $0 using 'replicated';
E = JOIN C by $0, B by $0 using 'replicated';
F = UNION D, E;
G = STREAM F through ....
STORE G into 'g';
{code}

It is good to go through all classes extending PhysicalOperator and check if it deep clones objects that are not primitive types.",,,,,,,,,,,,,,,,,,,,12/Jan/16 18:20;rohini;PIG-4737-1.patch;https://issues.apache.org/jira/secure/attachment/12781875/PIG-4737-1.patch,12/Jan/16 23:23;rohini;PIG-4737-2.patch;https://issues.apache.org/jira/secure/attachment/12781940/PIG-4737-2.patch,13/Jan/16 17:11;rohini;PIG-4737-fixtestfailures.patch;https://issues.apache.org/jira/secure/attachment/12782090/PIG-4737-fixtestfailures.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-01-12 23:20:48.458,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Jan 13 18:46:17 UTC 2016,,,,,,,0|i2oh0f:,9223372036854775807,,,,,,,,,,"12/Jan/16 23:20;daijy;+1

",12/Jan/16 23:23;rohini;Committed to trunk. Thanks for the review Daniel.,"13/Jan/16 17:11;rohini;Had replaced new Result(POStatus.STATUS_EOP, null); with the RESULT_EOP constant which was used in some places to all places in the initial patch. POSort.java (nested foreach without secondary key optimization) was modifying RESULT_EOP 's returnStatus to STATUS_OK leading to OOM errors because of infinite iteration and adding same value to bag. Patch also has some other minor changes which I added when investigating the failures.

https://builds.apache.org/job/Pig-trunk-commit/2278/testReport/ - 
TestLimitVariable.java
TestExampleGenerator.java
TestGrunt.java
TestSecondarySortMR.java
TestSecondarySortTez.java
TestPruneColumn.java

Other failures existed before this patch
TestEvalPipeline.testLimit
TestBZip.java
TestLoad.java
TestPigTest.java
  Except for TestBZip.java, others are due to the newly introduced StoreFuncDecorator not working with FetchLauncher. Will deal with them in a separate jira.","13/Jan/16 18:36;daijy;+1.

There is a ticket for NPE in StoreFuncDecorator: PIG-4731","13/Jan/16 18:46;rohini;Thanks Daniel. Will use PIG-4731 to address all the other test failures.

Committed https://issues.apache.org/jira/secure/attachment/12782090/PIG-4737-fixtestfailures.patch to trunk. ",,,,,,,,,,,,,,,,,,,,,,,
Removing empty keys in UDFContext broke one LoadFunc,PIG-4736,12913034,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,14/Nov/15 00:48,08/Jun/16 20:48,14/Mar/19 03:08,14/Nov/15 23:32,,,,,,0.16.0,,tez,,,0,,,,,,,PIG-4697 separated UDFContext specific to each vertex. But it also had an optimization to remove empty keys. That broke one user LoadFunc which was keeping the UDFContext Properties as a local variable and serializing in setLocation only if getSchema() was called. Even if UDFContext.isFrontend() logic was used it would have worked.,,,,,,,,,,,,,,,,,,,,14/Nov/15 00:54;rohini;PIG-4736-1.patch;https://issues.apache.org/jira/secure/attachment/12772333/PIG-4736-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-11-14 05:38:32.887,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Nov 14 23:32:07 UTC 2015,,,,,,,0|i2oe87:,9223372036854775807,,,,,,,,,,14/Nov/15 00:54;rohini;The patch only removes during the final serialization instead of removing it in UDFContext itself which was causing the problem when called from LoaderProcessor.,14/Nov/15 05:38;daijy;+1,14/Nov/15 23:32;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,
TOMAP schema inferring breaks some scripts in type checking for bincond,PIG-4734,12912950,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,rohini,rohini,13/Nov/15 20:05,08/Jun/16 20:48,14/Mar/19 03:08,31/May/16 19:49,,,,,,0.16.0,0.17.0,,,,0,,,,,,,"PIG-4674 added schema inferring for TOMAP.

{code}
FOREACH A GENERATE (val == 'x' ? TOMAP('key', floatfield1) : (val == 'y' ? GenerateFloatMap('key', floatfield2) : NULL)) as floatmap:map[float],
{code}

The following line fails with
{code}
Two inputs of BinCond must have compatible schemas. left hand side: #675:map(#676:float) right hand side: #801:map
	at org.apache.pig.newplan.logical.visitor.TypeCheckingExpVisitor.visit(TypeCheckingExpVisitor.java:616)
	... 45 more
{code}

GenerateFloatMap is a UDF that returns new HashMap<String, Float>(), but does not have outputSchema() defined. It worked earlier because TOMAP also did not have outputSchema() defined.",,,,,,,,,,,,,,,,,,,,13/Nov/15 20:54;daijy;PIG-4734-1.patch;https://issues.apache.org/jira/secure/attachment/12772289/PIG-4734-1.patch,28/May/16 18:52;daijy;PIG-4734-2.patch;https://issues.apache.org/jira/secure/attachment/12806824/PIG-4734-2.patch,31/May/16 06:31;daijy;PIG-4734-3.patch;https://issues.apache.org/jira/secure/attachment/12807050/PIG-4734-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2016-05-28 18:52:16.618,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 31 19:49:31 UTC 2016,,,,,,,0|i2odpj:,9223372036854775807,,,,,,,,,,"13/Nov/15 21:09;rohini;Doesn't it relax the type checking too much?

{code}
public static LogicalFieldSchema merge(LogicalFieldSchema fs1, LogicalFieldSchema fs2, MergeMode mode) {
.....
            /else {
                // Union schema
                if (fs1.type==DataType.BYTEARRAY) {
                    mergedType=fs2.type;
                } else if (fs2.type==DataType.BYTEARRAY) {
                    mergedType = fs1.type;
                }
                else {
                    // Take the more specific type
                    mergedType = DataType.mergeType(fs1.type, fs2.type);
                    if (mergedType == DataType.ERROR) {
                        // True incompatible, set to bytearray
                        mergedType = DataType.BYTEARRAY;
                    }
                }
            }
....
}
{code}","13/Nov/15 21:35;rohini;Any udf not implementing outputSchema() method is good for reproducing. But the UDF that hit the issue had another slight property in addition to not implementing outputSchema() method.  It does GenerateMap extends EvalFunc<Map<String, String>> making returnType of EvalFunc to be Map<String, String>, but can also return Map<String, Float>.  It would be good to have that in the testcase as well as EvalFunc.getReturnType() is used when outputSchema() returns null.

DEFINE GenerateMapFloat GenerateMap('FLOAT', 'xyz');

{code}
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;

public class GenerateMap extends EvalFunc<Map<String, String>>{
    private ArrayList<String> map_keys = null;
    private String mapType = null;
    
    public GenerateMap(String... args ) throws Exception {
        // TODO Auto-generated constructor stub
        for (int i = 0; i < args.length; i++)
        {
            if (args[i] == null || args[i].length() <= 0)
            {
                System.out.println(i);
                throw new Exception(""Cannot have null or empty string map key"");
            } else if(i > 0) {
                map_keys.add(args[i]);
            } else {
                map_keys = new ArrayList<String>();
                mapType = args[i];
            }
        }
    }

    @Override
    public Map exec(Tuple input) throws IOException {
        if (input.size() != map_keys.size())
        {
            throw new IOException(""tuple size of: "" + input.size() + "" must match number of arguments: "" + map_keys.size() + "" in constructor"");
        }
        Map pig_map;
        
        if(mapType.equals(""STRING"")) {
            pig_map = new HashMap<String, String>();
        }
        else  {
            pig_map = new HashMap<String, Float>();
        }
        
        for (int i = 0; i < input.size(); i++)
        {
            if (input.get(i) != null)
            {
                String value = input.get(i).toString();
                if (value.length() != 0)
                {
                    if(mapType.equals(""STRING"")) {
                        pig_map.put(map_keys.get(i), value);
                    } else {
                        pig_map.put(map_keys.get(i), Float.parseFloat(value));
                    }
                
                }
            }
            
        }
        
        return pig_map;
    }

}
{code}

",28/May/16 18:52;daijy;Use a restrictive way to do the check. GenerateMapFloat is no special in terms of type checking. I manually verified it works. I tend not to include it in the case since most of the code there is not related to the case. Is that Ok>,29/May/16 22:28;rohini;Should we relax the type checking for all types? Can we just do it for map? I am not sure what errors relaxing for everything might introduce.,31/May/16 06:31;daijy;The issue is not limited to map. Tuple/bag has similar issue. Attach a patch to include other test cases.,31/May/16 17:18;rohini;+1,31/May/16 19:49;daijy;Patch committed to both trunk and 0.16 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,
Avoid NullPointerException in JVMReuseImpl for builtin classes,PIG-4733,12912664,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,12/Nov/15 23:54,08/Jun/16 20:48,14/Mar/19 03:08,13/Nov/15 19:45,,,,,,0.16.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Nov/15 18:15;rohini;PIG-4733-1.patch;https://issues.apache.org/jira/secure/attachment/12772259/PIG-4733-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-11-13 19:27:26.886,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Nov 13 19:45:46 UTC 2015,,,,,,,0|i2oby7:,9223372036854775807,,,,,,,,,,"13/Nov/15 18:15;rohini;Details in https://issues.apache.org/jira/browse/PIG-4418?focusedCommentId=15003293&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15003293. 

Still do not the root cause of the NPE happening. Just avoiding it for builtin classes by calling directly in this patch as that forms the majority scenario.",13/Nov/15 19:27;daijy;+1,13/Nov/15 19:45;rohini;Committed to trunk. Thanks for the review [~daijy],,,,,,,,,,,,,,,,,,,,,,,,,
[Pig on Tez] Total parallelism estimation does not account load parallelism,PIG-4730,12911660,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,09/Nov/15 20:47,08/Jun/16 20:48,14/Mar/19 03:08,10/Nov/15 00:04,,,,,,0.16.0,,,,,0,,,,,,,Total parallelism estimation in ParallelismSetter which is used to auto increment AM size does not include parallelism of load vertices causing the estimation to be wrong and not incrementing AM memory in cases where it is needed leading to OOM.,,,,,,,,,,,,,,,,,,,,09/Nov/15 21:46;rohini;PIG-4730-1.patch;https://issues.apache.org/jira/secure/attachment/12771441/PIG-4730-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-11-09 23:23:59.988,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Nov 10 00:04:10 UTC 2015,,,,,,,0|i2o5sf:,9223372036854775807,,,,,,,,,,09/Nov/15 23:23;daijy;+1,10/Nov/15 00:04;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect types table for AVG in docs,PIG-4727,12910367,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,nsmith,nsmith,nsmith,04/Nov/15 15:17,08/Jun/16 20:48,14/Mar/19 03:08,04/Nov/15 18:18,0.15.0,,,,,0.16.0,,documentation,,,0,,,,,,,"The docs for AVG state that AVG for int & long produces a long, but the source for those classes actually produce a double.

* https://github.com/apache/pig/blob/trunk/src/org/apache/pig/builtin/AVG.java
* https://github.com/apache/pig/blob/trunk/src/org/apache/pig/builtin/LongAvg.java
* https://github.com/apache/pig/blob/trunk/src/org/apache/pig/builtin/IntAvg.java ",,,,,,,,,,,,,,,,,,,,04/Nov/15 15:18;nsmith;PIG-4727.patch;https://issues.apache.org/jira/secure/attachment/12770596/PIG-4727.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-11-04 18:18:44.033,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,Reviewed,,,,Wed Nov 04 18:18:44 UTC 2015,,,Patch Available,,,,0|i2nxyv:,9223372036854775807,,,,,,,,,,04/Nov/15 18:18;daijy;You are right. LongAvg produces double. Patch committed. Thanks Nathan!,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Typo in FrontendException messages ""Incompatable""",PIG-4725,12910240,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,nsmith,nsmith,nsmith,04/Nov/15 04:12,08/Jun/16 20:48,14/Mar/19 03:08,04/Nov/15 07:20,0.15.0,,,,,0.16.0,,,,,0,,,,,,,"There is a typo in some ""FrontendException"" error messages where ""Incompatible"" is misspelled as ""Incompatable"".",,,,,,,,,,,,,,,,,,,,04/Nov/15 04:14;nsmith;PIG-4725.patch;https://issues.apache.org/jira/secure/attachment/12770513/PIG-4725.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-11-04 07:20:01.13,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,Reviewed,,,,Wed Nov 04 07:20:01 UTC 2015,,,Patch Available,,,,0|i2nx6n:,9223372036854775807,,,,,,,,,,04/Nov/15 07:20;daijy;Committed to trunk. Thanks Nathan!,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Pig on Tez] NPE while running Combiner,PIG-4722,12909375,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,31/Oct/15 02:00,08/Jun/16 20:48,14/Mar/19 03:08,13/Nov/15 19:28,,,,,,0.16.0,,,,,0,,,,,,,"DefaultSorter in Tez calls Combiner from two different threads - during spill in SpillThread and flush in the main thread. If both run the combiner, one ends up with NPE as Reporter is set on only one thread in initialization. 

{code}
java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:366)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNextTuple(POLocalRearrange.java:332)
        at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POLocalRearrangeTez.getNextTuple(POLocalRearrangeTez.java:128)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.processOnePackageOutput(PigCombiner.java:197)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:175)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:50)
        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
        at org.apache.tez.mapreduce.combine.MRCombiner.runNewCombiner(MRCombiner.java:191)
        at org.apache.tez.mapreduce.combine.MRCombiner.combine(MRCombiner.java:115)
        at org.apache.tez.runtime.library.common.sort.impl.ExternalSorter.runCombineProcessor(ExternalSorter.java:279)
        at org.apache.tez.runtime.library.common.sort.impl.dflt.DefaultSorter.spill(DefaultSorter.java:854)
        at org.apache.tez.runtime.library.common.sort.impl.dflt.DefaultSorter.sortAndSpill(DefaultSorter.java:780)
        at org.apache.tez.runtime.library.common.sort.impl.dflt.DefaultSorter$SpillThread.run(DefaultSorter.java:708)
Caused by: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:303)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNextTuple(POProject.java:403)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:361)
        ... 12 more
{code}",,,,,,,,,,,,,,,,,,TEZ-2937,,13/Nov/15 17:09;rohini;PIG-4722-1.patch;https://issues.apache.org/jira/secure/attachment/12772239/PIG-4722-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-11-13 19:24:10.39,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Nov 13 19:28:26 UTC 2015,,,,,,,0|i2nrsf:,9223372036854775807,,,,,,,,,,"10/Nov/15 18:33;rohini;Actually my initial analysis was incorrect. Combiner  is called multiple times from SpillThread and also called from ExternalSorter.flush() which is a different thread. But setup(),reduce() and cleanup() is always called as a batch everytime, so there is no cross thread issue there with initialization or cleanup. The actual problem was PigProcessor.close() was being called when spill thread was still running combiner. So even though PhysicalOperator.java had a getReporter() != null check, it became null when calling getReporter().progress() as PigProcessor.close() was resetting it.

{code}
            if (getReporter() != null) {
                getReporter().progress();
            }
{code}

Need to ensure that PigProcessor.close does not cleanup static data when Combiner is running.","11/Nov/15 23:39;rohini;[~daijy] as discussed, can you check with the tez folks - [~bikassaha] or [~hitesh], whether they can call processor.close() in LogicalIOProcessRunTimeTask.java at the very end after the inputs and outputs are done and closed.

Meanwhile will go with a hacky patch to not do anything in Processor.close() if combiner is running at the time and do the cleanup in the combiner cleanup(). If we don't have Tez APIs to deal with cleanup at the very end, we need to think of cleaner solution and rethink the whole way we use ThreadLocal's to work with local mode.",13/Nov/15 17:09;rohini;Attached patch just fixes the NPE and avoids additional threadlocal call.  This is assuming that moving of PigProcessor.close() in TEZ-2937 will avoid cleanup being done when SpillThread is running combiner.  If TEZ-2937 introduces a new API then will have another patch depending on that.,13/Nov/15 19:24;daijy;+1,13/Nov/15 19:28;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,
IsEmpty documentation error,PIG-4721,12909298,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,nsmith,nsmith,nsmith,30/Oct/15 19:17,08/Jun/16 20:48,14/Mar/19 03:08,30/Oct/15 19:51,0.15.0,,,,,0.16.0,,documentation,,,0,,,,,,,"http://pig.apache.org/docs/r0.15.0/func.html#isempty

The documentation example uses a left outer join, but this produces a flat tuple, which is invalid for IsEmpty. I believe the example in the docs should be:

{code}
SSN = load 'ssn.txt' using PigStorage() as (ssn:long);

SSN_NAME = load 'students.txt' using PigStorage() as (ssn:long, name:chararray);

/* do a cogroup of SSN with SSN_Name */
X = COGROUP SSN by ssn, SSN_NAME by ssn;

/* only keep those ssn's for which there is no name */
Y = filter X by IsEmpty(SSN_NAME);
{code}",,,,,,,,,,,,,,,,,,,,30/Oct/15 19:49;daijy;PIG-4721.patch;https://issues.apache.org/jira/secure/attachment/12769849/PIG-4721.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-10-30 19:51:30.899,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 30 19:51:30 UTC 2015,,,,,,,0|i2nrbb:,9223372036854775807,,,,,,,,,,30/Oct/15 19:51;daijy;Fixed. Thanks Nathan!,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Pig on Tez] NPE in Bloom UDF after Union ,PIG-4712,12907846,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,26/Oct/15 08:06,08/Jun/16 20:48,14/Mar/19 03:08,26/Oct/15 18:36,,,,,,0.15.1,0.16.0,,,,0,,,,,,,   POUserFunc clone does not take care of cloning shipFiles and cacheFiles. So Bloom UDF hits a NPE as the bloom file (cacheFile) is not shipped to the task and available to the UDF.  Issue will happen with any udf that has shipFiles or cacheFiles.,,,,,,,,,,,,,,,,,,,,26/Oct/15 09:21;rohini;PIG-4712-1.patch;https://issues.apache.org/jira/secure/attachment/12768680/PIG-4712-1.patch,26/Oct/15 18:29;rohini;PIG-4712-2.patch;https://issues.apache.org/jira/secure/attachment/12768779/PIG-4712-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-10-26 18:10:14.953,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Nov 02 20:07:30 UTC 2015,,,,,,,0|i2nien:,9223372036854775807,,,,,,,,,,"26/Oct/15 18:10;daijy;I am not sure if changing Bloom_1 is better or just adding a new test, seems the original test is the prevailing use case and should be tested.",26/Oct/15 18:29;rohini;Uploaded new patch which adds a new test instead of updating old one.,26/Oct/15 18:31;daijy;+1,26/Oct/15 18:36;rohini;Committed to trunk. Thanks for the review Daniel.,02/Nov/15 20:07;daijy;Backport to 0.15 branch.,,,,,,,,,,,,,,,,,,,,,,,
[Pig on Tez] Streaming job hangs with pig.exec.mapPartAgg=true,PIG-4707,12906117,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,20/Oct/15 01:17,08/Jun/16 20:48,14/Mar/19 03:08,25/Oct/15 12:56,,,,,,0.15.1,0.16.0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Oct/15 01:46;rohini;PIG-4707-1-withoutwhitespacechanges.patch;https://issues.apache.org/jira/secure/attachment/12767511/PIG-4707-1-withoutwhitespacechanges.patch,20/Oct/15 01:46;rohini;PIG-4707-1.patch;https://issues.apache.org/jira/secure/attachment/12767512/PIG-4707-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-10-20 22:00:52.254,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Nov 02 19:55:37 UTC 2015,,,,,,,0|i2n7sn:,9223372036854775807,,,,,,,,,,"20/Oct/15 01:46;rohini;Unlike MR, parentPlan.endOfAllInput can become true in different conditions (https://issues.apache.org/jira/browse/PIG-4020?focusedCommentId=14036346&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14036346)

So need to set allOutputFromBinaryProcessed to true in all the branch conditions we encounter STATUS_EOS.","20/Oct/15 22:00;knoguchi;I haven't had time to understand the patch but at least the new e2e testing from this patch needs to be fixed.

{noformat}
ERROR 1200: _file ./out/pigtest/gtrain/gtrain-1445373986-nightly.conf/MapPartialAgg_6_benchmark.pig, line 6, column 29_  mismatched input 'otherwise' expecting IF

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. _file ./out/pigtest/gtrain/gtrain-1445373986-nightly.conf/MapPartialAgg_6_benchmark.pig, line 6, column 29_  mismatched input 'otherwise' expecting IF
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1646)
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1589)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:589)
	at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:969)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:386)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:189)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
	at org.apache.pig.Main.run(Main.java:561)
	at org.apache.pig.Main.main(Main.java:111)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: Failed to parse: _file ./out/pigtest/gtrain/gtrain-1445373986-nightly.conf/MapPartialAgg_6_benchmark.pig, line 6, column 29_  mismatched input 'otherwise' expecting IF
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:222)
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:164)
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1638)
	... 15 more
{noformat}",20/Oct/15 23:32;rohini;Actually the e2e runs fine for me. Can you paste the contents of ./out/pigtest/gtrain/gtrain-1445373986-nightly.conf/MapPartialAgg_6_benchmark.pig ?,"21/Oct/15 20:36;daijy;The test pass for me. However, the new test pass even before I apply the patch. Is there any condition to reproduce it?","21/Oct/15 20:40;knoguchi;Test failure was due to my e2e test trying with pig-0.9.  
Rohini pointed out my mistake.  Thank you and sorry~.","22/Oct/15 19:24;rohini;[~daijy],
   The test does hang for me without the fix to POStream.java. Below is the stacktrace of the hung thread. Can you recheck?

{code}
""TezChild"" daemon prio=5 tid=0x00007fd92c237000 nid=0x4d07 waiting on condition [0x000000010c5ce000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000007fbc47b68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
        at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream.getNextTuple(POStream.java:157)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNextTuple(POFilter.java:91)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getStreamCloseResult(POSplit.java:277)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getNextTuple(POSplit.java:213)
        at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:334)
        at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:199)
{code}","22/Oct/15 20:24;daijy;Yes, it hangs without fix. I probably run with MR. I will review shortly.","24/Oct/15 08:20;daijy;This fix should be quite safe. Once Pig see a EOS (stream process finishes), Pig shall remember and return EOP without doing anything else. So +1 on patch.

I still need to think through the interaction between split and stream. We've already get several fixes around it (PIG-4480, PIG-4493). But that should not block this Jira.",25/Oct/15 12:56;rohini;Committed to trunk. Thanks for the review [~daijy],02/Nov/15 19:55;daijy;Backport to 0.15 branch.,,,,,,,,,,,,,,,,,,
TezOperator.stores shall not ship to backend,PIG-4703,12905276,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,15/Oct/15 17:55,08/Jun/16 20:48,14/Mar/19 03:08,16/Oct/15 20:49,,,,,,0.15.1,0.16.0,tez,,,0,,,,,,,"We end up of shipping all physical plan to the backend. One direct result is UDFContext is not set in OutputCommitter. Here is how:
1. PigGraceShuffleVertexManager deserialize ""pig.tez.plan"". TezOperator.stores reference physical plan so physical plan get deserialize.
2. When deserialize POUserFunc, POUserFunc.instantiateFunc is invoked. Inside it, POUserFunc.setFuncInputSchema would use UDFContext and leave an entry there (though it is empty map).
3. MapRedUtil.setupUDFContext would only deserialize if UDFContext is empty, since we already touch UDFContext with some maps, UDFContext is not deserialized",,,,,,,,,,,,,,,,,,,,15/Oct/15 17:56;daijy;PIG-4703-1.patch;https://issues.apache.org/jira/secure/attachment/12766832/PIG-4703-1.patch,16/Oct/15 07:50;daijy;PIG-4703-2.patch;https://issues.apache.org/jira/secure/attachment/12767010/PIG-4703-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-10-15 21:51:32.851,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 16 20:49:05 UTC 2015,,,,,,,0|i2n2pz:,9223372036854775807,,,,,,,,,,"15/Oct/15 21:51;rohini;[~daijy],
   Can we actually get rid of stores variable in TezOperator instead of making it transient and also get rid of TezCompilerUtil.isIntermediateReducer method and inline it into TezOperator.isIntermediateReducer method. Can get rid of the setter as well. That will clean up some code.",15/Oct/15 21:55;rohini;Should we also deserialize UDFContext in PigGraceShuffleVertexManager before plan is deserialized to be on the safer side in future?,16/Oct/15 07:50;daijy;Agree TezOperator.stores is not needed. UDFContext is not a concern now since we don't invoke any UDF hook in PigGraceShuffleVertexManager. Attach PIG-4703-2.patch.,16/Oct/15 17:50;rohini;+1,16/Oct/15 20:49;daijy;Patch committed to both trunk and 0.15 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,
Empty map returned by a streaming_python udf wrongly contains a null key,PIG-4696,12903077,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,cheolsoo,cheolsoo,cheolsoo,07/Oct/15 18:30,08/Jun/16 20:48,14/Mar/19 03:08,08/Oct/15 16:09,0.15.0,,,,,0.15.1,0.16.0,impl,,,0,,,,,,,"To reproduce, please run the following query-
{code}
b = FOREACH a GENERATE (map[])udfs.empty_dict();
DUMP b;
{code}
where empty_dict() is a Python udf-
{code}
@outputSchema(""map_out: []"")
def empty_dict():
    return {}
{code}
This returns {{([])}} in jython while {{(\[null#\])}} in streaming_python.",,,,,,,,,,,,,,,,,,,,07/Oct/15 18:33;cheolsoo;PIG-4696.1.patch;https://issues.apache.org/jira/secure/attachment/12765430/PIG-4696.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-10-07 19:59:05.609,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 02 20:10:25 UTC 2015,,,,,,,0|i2mpcf:,9223372036854775807,,,,,,,,,,07/Oct/15 19:59;rohini;+1,08/Oct/15 16:09;cheolsoo;Thanks Rohini for the review. Committed to trunk.,02/Nov/15 20:10;daijy;Backport to 0.15 branch.,,,,,,,,,,,,,,,,,,,,,,,,,
[Pig on Tez] Support for whitelisting storefuncs for union optimization,PIG-4691,12902532,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,05/Oct/15 22:40,08/Jun/16 20:48,14/Mar/19 03:08,06/Oct/15 21:23,,,,,,0.16.0,,,,,0,,,,,,,PIG-4649 added support for blacklisting some storefuncs when applying union+store vertex group optimization as HCatStorer was not honoring mapreduce.output.basename and hardcoding part file names. Found that some of our user StoreFuncs also do that and ended up with partial results. So would be good to have a whitelist option as well where you can put StoreFuncs that do not mess with mapreduce.output.basename.,,,,,,,,,,,,,,,,,,,,06/Oct/15 19:59;rohini;PIG-4691-1.patch;https://issues.apache.org/jira/secure/attachment/12765236/PIG-4691-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-10-06 21:08:36.672,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Oct 06 21:23:37 UTC 2015,,,,,,,0|i2mlzr:,9223372036854775807,"Union optimization (pig.tez.opt.union=true) in tez uses vertex groups to store output from different vertices into one final output location. If a StoreFunc's OutputCommitter does not honor mapreduce.output.basename or has other issues with multiple vertices writing to the destination location at the same time, then you can disable union optimization just for that StoreFunc. Refer PIG-4649. You can also specify a whitelist of StoreFuncs that are known to work with multiple vertices writing to same location instead of a blacklist.

#pig.tez.opt.union.unsupported.storefuncs=org.apache.hcatalog.pig.HCatStorer,org.apache.hive.hcatalog.pig.HCatStorer
#pig.tez.opt.union.supported.storefuncs=",,,,,,,,,06/Oct/15 21:08;daijy;+1,06/Oct/15 21:23;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
Union with self replicate join will fail in Tez,PIG-4690,12902526,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,05/Oct/15 22:11,01/Jul/16 14:15,14/Mar/19 03:08,10/Feb/16 18:33,,,,,,0.16.0,,,,,0,,,,,,,"Found this while trying to get a reproducible script for a different issue. Not a user reported one, but a possibility nonetheless. 

A = LOAD 'x';
B = LOAD 'y';
C = UNION A, B;
D = JOIN C, A using 'repl';
DUMP D;",,,,,,,,,,,,PIG-4789,,,,,,,,25/Jan/16 06:45;rohini;PIG-4690-1.patch;https://issues.apache.org/jira/secure/attachment/12784105/PIG-4690-1.patch,25/Jan/16 16:47;rohini;PIG-4690-2.patch;https://issues.apache.org/jira/secure/attachment/12784188/PIG-4690-2.patch,29/Jan/16 21:51;rohini;PIG-4690-3.patch;https://issues.apache.org/jira/secure/attachment/12785259/PIG-4690-3.patch,04/Mar/16 21:42;daijy;PIG-4690-branch-15.patch;https://issues.apache.org/jira/secure/attachment/12791538/PIG-4690-branch-15.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2016-03-04 21:42:25.399,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Mar 04 21:42:25 UTC 2016,,,,,,,0|i2mlyf:,9223372036854775807,,,,,,,,,,"25/Jan/16 06:34;rohini;  Had another user reported issue where user had cross + scalar from same Split. The script was actually wrong as it was using . instead of :: operator.  But since both inputs had 1 record, the script worked producing the right results. it The script failed to run in Tez with ""java.lang.IllegalArgumentException: bound must be positive""  as parallelism of the cross vertex was set to 0.  Main problem was not parallelism estimation,  but the planning where the shuffle vertex for Cross was overwritten with the broadcast vertex for scalar.

Multi-query planning should differentiate between POPackage inputs and non-POPackage inputs. Splittee should be merged if it has either POPackage or a non-POPackage input and not both.  

",10/Feb/16 18:33;rohini;Committed to trunk a week back. Just closing now. +1 is in review board - https://reviews.apache.org/r/42709/. Thanks for the review Daniel.,04/Mar/16 21:42;daijy;Rebase the patch to branch 0.15.,,,,,,,,,,,,,,,,,,,,,,,,,
CSV Writes incorrect header if two CSV files are created in one script,PIG-4689,12902403,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nielsbasjes,nielsbasjes,nielsbasjes,05/Oct/15 12:00,08/Jun/16 20:48,14/Mar/19 03:08,06/Nov/15 20:04,0.14.0,0.15.0,,,,0.16.0,,,,,0,,,,,,,"From a single Pig script I write two completely different and unrelated CSV files; both with the flag 'WRITE_OUTPUT_HEADER'.

The bug is that both files get the SAME header at the top of the output file even though the data is different.

*Reproduction:*
{code:title=foo.txt}
1
{code}

{code:title=bar.txt (Tab separated)}
1	a
{code}

{code:title=WriteTwoCSV.pig}
FOO =
    LOAD 'foo.txt'
    USING PigStorage('\t')
    AS (a:chararray);

BAR =
    LOAD 'bar.txt'
    USING PigStorage('\t')
    AS (b:chararray, c:chararray);

STORE FOO into 'Foo'
USING org.apache.pig.piggybank.storage.CSVExcelStorage('\t','NO_MULTILINE', 'UNIX', 'WRITE_OUTPUT_HEADER');

STORE BAR into 'Bar'
USING org.apache.pig.piggybank.storage.CSVExcelStorage('\t','NO_MULTILINE', 'UNIX', 'WRITE_OUTPUT_HEADER');
{code}

*Command:*
{quote}pig -x local WriteTwoCSV.pig{quote}

*Result:*
{quote}cat Bar/part-*{quote}
{code}
b	c
1	a
{code}
{quote}cat Foo/part-*{quote}
{code}
b	c
1
{code}

*The error is that the {{Foo}} output has a the two column header from the {{Bar}} output.*
*One of the effects is that parsing the {{Foo}} data will probably fail due to the varying number of columns*


",,,,,,,,,,,,,,,,,,,,06/Oct/15 09:47;nielsbasjes;PIG-4689-2015-10-06.patch;https://issues.apache.org/jira/secure/attachment/12765152/PIG-4689-2015-10-06.patch,16/Oct/15 10:07;nielsbasjes;PIG-4689-20151016.patch;https://issues.apache.org/jira/secure/attachment/12767040/PIG-4689-20151016.patch,05/Nov/15 12:43;nielsbasjes;PIG-4689-20151105.patch;https://issues.apache.org/jira/secure/attachment/12770798/PIG-4689-20151105.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2015-10-14 17:13:46.423,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Nov 06 20:04:53 UTC 2015,,,,,,,0|hzzzyn:,9223372036854775807,,,,,,,,,,06/Oct/15 09:04;nielsbasjes;I double checked and the effect is identical to running on a cluster.,"06/Oct/15 09:42;nielsbasjes;Quick first analysis of the problem:
In the [checkSchema method the schema is stored with the class and the udfContextSignature as the 'key' for this class ([Link|https://github.com/apache/pig/blob/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/CSVExcelStorage.java#L245]).
Because in both instances we have the same class AND the same the argumentlist they apparently get the same udfContextSignature : Thus one of them overwrites the schema for the other.

At this point my best guess is that the actual values that Pig calls the 'setUDFContextSignature' with is not unique enough.
","06/Oct/15 09:47;nielsbasjes;*Found it*
Turns out the CSVExcelStorage implemented the WRONG setter for receiving the unique UDFContextSignature. 
Hence no unique value was ever set and all instances used 'null' as their 'unique value'.

{code:title=Foo/part-m-00000}
a
1
{code}

{code:title=Bar/part-m-00000}
b	c
1	a
{code}","06/Oct/15 09:49;nielsbasjes;Please verify the patch I wrote (""It works in my machine"").","06/Oct/15 13:00;nielsbasjes;I am uncertain if the rename is correct.
Possibly adding the new method is better (setting the same value).",14/Oct/15 15:43;nielsbasjes;Needs rework,"14/Oct/15 17:13;rohini;bq. Possibly adding the new method is better (setting the same value).
   Yes. You need both. setUDFContextSignature is for LoadFunc and setStoreFuncUDFContextSignature is for StoreFunc. ","16/Oct/15 10:07;nielsbasjes;Revised patch that adds the new function setStoreFuncUDFContextSignature.
This passes the TestCSVExcelStorage and also solves the reported reproduction path.",26/Oct/15 21:30;daijy;Can you add your test case to TestCSVExcelStorage?,"27/Oct/15 16:27;nielsbasjes;Yes, will do.","05/Nov/15 12:43;nielsbasjes;I created a unit test to reproduce the problem.
I validated this test in a separate project because after fighting 'ant' for 2 hours I gave up. (Please let's switch to something different from ant)

Please validate if I did it correctly because running and testing code in piggybank is near impossible for me.
","06/Nov/15 20:04;daijy;+1, verified the test case works. Patch committed to trunk. Thanks Niels!",,,,,,,,,,,,,,,,
Limit followed by POPartialAgg can give empty or partial results in Tez,PIG-4688,12902360,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,05/Oct/15 06:00,08/Jun/16 20:48,14/Mar/19 03:08,05/Oct/15 20:57,,,,,,0.15.1,0.16.0,,,,0,,,,,,, This is due to endOfAllInput flag not being set.,,,,,,,,,,,,,,,,,,,,05/Oct/15 06:04;rohini;PIG-4688-1.patch;https://issues.apache.org/jira/secure/attachment/12764969/PIG-4688-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-10-05 18:01:35.147,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Oct 05 20:57:11 UTC 2015,,,,,,,0|i2mkxj:,9223372036854775807,,,,,,,,,,05/Oct/15 06:04;rohini;Setting endOfAllInput in PigProcessor and calling runPipeline(leaf) once again like in PigGenericMapBase and PigGenericMapReduce. Did not remove the existing ones in classes implementing TezInput as they will cut short one iteration of pipeline traversal.,05/Oct/15 18:01;daijy;+1,05/Oct/15 20:57;rohini;Committed to branch-0.15 and trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,
Nested order is broken after PIG-3591 in some cases,PIG-4683,12895228,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,21/Sep/15 19:13,31/Aug/16 22:47,14/Mar/19 03:08,24/Sep/15 05:46,,,,,,0.15.1,0.16.0,impl,,,0,,,,,,,"The following script fail after PIG-3591.
{code}
a = load '1.txt' using PigStorage(',') as (a0:chararray, a1:chararray);
b = load '2.txt' as (b0:chararray);
c = cogroup b by b0, a by a0;
d = foreach c {
    a_sorted = order a by a1 desc;
    generate group, a_sorted, b;
}
dump d;
{code}
1.txt
a,0
a,2
a,1

2.txt
a

Expected:
{code}
(a,{(a,1),(a,2),(a,0)},{(a)})
{code}
Actual:
{code}
(a,{(a,2),(a,1),(a,0),(a)},{})
{code}",,,,,,,,,,,,,,,,,,,,21/Sep/15 20:08;daijy;PIG-4683-1.patch;https://issues.apache.org/jira/secure/attachment/12761484/PIG-4683-1.patch,23/Sep/15 18:38;daijy;PIG-4683-2.patch;https://issues.apache.org/jira/secure/attachment/12761950/PIG-4683-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-09-21 22:57:44.428,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Sep 24 05:46:53 UTC 2015,,,,,,,0|i2ld3r:,9223372036854775807,,,,,,,,,,"21/Sep/15 19:19;daijy;The reason is PigSecondaryKeyComparator doesn't sort on index, but POPackage assumes iterator is sorted by index after PIG-3591. Before that, if there are multiple inputs, POPackage will create multiple bag and accumulate individually. Now once POPackage sees an index of the last input, it stop accumulating and create a PeekedBag immediately. However, the iterator may have records of other inputs not processed.","21/Sep/15 22:57;rohini;Few questions:
  1) This is not an issue with Tez right?
  2)  If we are removing null checks in tuple, won't output be incorrect if cogroup is on multiple keys and they contain null? Most likely there are no test cases covering that scenario. I realized that with self joins for PIG-4627 and had to add/modify tests to use studentnulltab10k.","21/Sep/15 23:50;daijy;[~rohini]
1) No, because currently SecondaryKeyOptimizerTez does not optimize cogroup. But if we enable that in Tez, we will face the same issue.
2) PigSecondaryKeyComparator should only do sorting. Whether tuple containing null should collapse together is the responsibility of PigSecondaryKeyGroupComparator, which I didn't touch. Actually in the patch, I only changed the code path which key doesn't contain null.","22/Sep/15 22:09;rohini;bq. No, because currently SecondaryKeyOptimizerTez does not optimize cogroup. But if we enable that in Tez, we will face the same issue.
  We should have a jira to fix SecondaryKeyOptimizerTez to support it. Even if it was supported it should not be an issue as POShuffleTezLoad does not use PeekedBag like POPackage",23/Sep/15 18:38;daijy;New patch fixed unit test failures.,23/Sep/15 19:41;rohini;+1. This fix is better.,24/Sep/15 05:46;daijy;Patch committed to both trunk and 0.15 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,
Performance degradation due to InputSizeReducerEstimator since PIG-3754,PIG-4679,12864440,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Sep/15 04:12,08/Jun/16 20:48,14/Mar/19 03:08,17/Sep/15 02:39,,,,,,0.15.1,0.16.0,impl,,,0,,,,,,,"On encountering a non-HDFS location in the input (for example a JOIN involving both HBase tables and intermediate temp files), Pig 0.14 ReducerEstimator is returning total input size as -1 (unknown) where as in Pig 0.12.1 it was returning the sum of temp file sizes as the total size. Since -1 is returned as the input size, Pig end up using only one reducer for the job.

STEPS TO REPRODUCE:
1.	Create an HBase table with enough data.  Using PerformanceEvaluation tool to generate data
{code:java}
hbase org.apache.hadoop.hbase.PerformanceEvaluation --presplit=20 --rows=1000000 sequentialWrite 10
{code}

2.	Dump the table data into a file which we can then use in a Pig JOIN.  Following Pig script generates the data file
{code:java}
$ pig
A = LOAD 'hbase://TestTable' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('info:data', '-loadKey') AS (row_key: chararray, data: chararray);
STORE A INTO 'hdfs:///tmp/re_test/test_table_data' USING PigStorage('|');
{code}

3.	Check file size to make sure that it is more than 1,000,000,000 which is the default bytes per reducer Pig configuration
{code:java}
$ hdfs dfs -count hdfs:///tmp/re_test/test_table_data
QA:           1           41        10280000000 hdfs:///tmp/re_test/test_table_data
PROD:         1           57        10280000000 hdfs:///tmp/re_test/test_table_data
{code}

4.	Run a Pig script that joins the HBase table with the data file.  QA and PROD will use different number of reducers.  QA (176243) should run 1 reducer and PROD (176258) should run 11 reducers (10,280,000,000 / 1,000,000,000)
{code:java}
$ pig
A = LOAD 'hbase://TestTable' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('info:data', '-loadKey') AS (row_key: chararray, data: chararray);
B = LOAD 'hdfs:///tmp/re_test/test_table_data' USING PigStorage('|') AS (row_key: chararray, data: chararray);
C = JOIN A BY row_key, B BY row_key;
STORE C INTO 'hdfs:///tmp/re_test/test_table_data_join' USING PigStorage('|');
{code}

Pig 0.12.1 ran 11 reduce, Pig 0.13+ run only 1 reduce.",,,,,,,,,,,,,,,,,,,,16/Sep/15 04:14;daijy;PIG-4679-0.patch;https://issues.apache.org/jira/secure/attachment/12756156/PIG-4679-0.patch,16/Sep/15 18:11;daijy;PIG-4679-1.patch;https://issues.apache.org/jira/secure/attachment/12756314/PIG-4679-1.patch,23/Sep/15 05:35;daijy;PIG-4679-fixtest.patch;https://issues.apache.org/jira/secure/attachment/12761809/PIG-4679-fixtest.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2015-09-16 20:56:33.337,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Nov 02 19:52:24 UTC 2015,,,,,,,0|i2k8cf:,9223372036854775807,,,,,,,,,,"16/Sep/15 04:14;daijy;We don't estimate size for non-hdfs inputs before 0.12. However, we will use the size of the other input. Though it is still wrong, but it is not -1, which leads to 1 reduce. Attach a draft patch.",16/Sep/15 20:56;rohini;+1,17/Sep/15 02:39;daijy;Patch committed to trunk. Thanks Rohini for review!,23/Sep/15 05:35;daijy;Fix a unit test failure.,23/Sep/15 06:13;prkommireddi;+1,02/Nov/15 19:52;daijy;Backport to 0.15 branch.,,,,,,,,,,,,,,,,,,,,,,
Display failure information on stop on failure,PIG-4677,12864037,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,mitdesai,mitdesai,14/Sep/15 21:27,21/Jun/17 09:15,14/Mar/19 03:08,30/Mar/17 14:38,0.11.1,,,,,0.17.0,,,,,0,,,,,,,"When stop on failure option is specified, pig abruptly exits without displaying any job stats or failed job information which it usually does in case of failures.

{code}
2015-06-04 20:35:38,170 [uber-SubtaskRunner] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - 9% complete
2015-06-04 20:35:38,171 [uber-SubtaskRunner] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - Running jobs are [job_1428329756093_3741748,job_1428329756093_3741752,job_1428329756093_3741753,job_1428329756093_3741754,job_1428329756093_3741756]
2015-06-04 20:35:40,201 [uber-SubtaskRunner] ERROR org.apache.pig.tools.grunt.Grunt  - ERROR 6017: Job failed!
Hadoop Job IDs executed by Pig: job_1428329756093_3739816,job_1428329756093_3741752,job_1428329756093_3739814,job_1428329756093_3741748,job_1428329756093_3741756,job_1428329756093_3741753,job_1428329756093_3741754

<<< Invocation of Main class completed <<<
{code}
",,,,,,,,,,,,,,,,,,,,28/Mar/17 21:28;rohini;PIG-4677-5.patch;https://issues.apache.org/jira/secure/attachment/12860941/PIG-4677-5.patch,31/Mar/17 21:52;rohini;PIG-4677-fixflakytest.patch;https://issues.apache.org/jira/secure/attachment/12861520/PIG-4677-fixflakytest.patch,18/Sep/15 21:07;mitdesai;PIG-4677.2.patch;https://issues.apache.org/jira/secure/attachment/12761195/PIG-4677.2.patch,21/Sep/15 13:42;mitdesai;PIG-4677.3.patch;https://issues.apache.org/jira/secure/attachment/12761432/PIG-4677.3.patch,29/Jan/16 18:03;mitdesai;PIG-4677.4.patch;https://issues.apache.org/jira/secure/attachment/12785202/PIG-4677.4.patch,14/Sep/15 21:29;mitdesai;PIG-4677.patch;https://issues.apache.org/jira/secure/attachment/12755842/PIG-4677.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2015-09-16 23:05:55.153,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Mar 31 22:29:29 UTC 2017,,,,,,,0|i2k5xb:,9223372036854775807,,,,,,,,,,14/Sep/15 21:29;mitdesai;Attaching the patch,"16/Sep/15 23:05;rohini;[~mitdesai],
   checkStopOnFailure currently throws an exception which makes it exit early. So it ends up skipping cleanupOnFailure, printing ""Job Stats"" information, etc. i.e all code starting from MRScriptState.get().emitProgressUpdatedNotification(100);. 

  So instead of throwing exception and exiting early and relying on system shutdown hook to kill the remaining jobs, you will have to call failJob(message) on JobControl.getReadyJobsList() and  JobControl.getRunningJobList() in checkStopOnFailure. That will kill all jobs and will then follow the regular code path.","17/Sep/15 21:42;mitdesai;Oh.
I will update the patch.",18/Sep/15 21:07;mitdesai;Updated the patch.,"18/Sep/15 23:06;rohini;It should still be assertFalse(server.existsFile(""done"")); . With this change it is still not stopping execution of the script when it is compiled in two phases due to fs statements. Will have to make checkStopOnFailure return true instead of void, and throw new ExecException(msg.toString(), errCode, PigException.REMOTE_ENVIRONMENT); instead of return pigStats at the end in launchPig if checkStopOnFailure returned true. ","21/Sep/15 13:42;mitdesai;Thanks for the Review [~rohini]. Updated the patch.

In the ExecException, the message used to be ""job_id Job Failed!"". I have changed it to ""Some jobs failed. stop.on.failure set true. Stopping execution"". What job failed can now be easily found out by looking at the Script Status which will look something like this:
{noformat}
Error: org.apache.pig.backend.executionengine.ExecException: ERROR 2055: Received Error while processing the map plan: 'false (stdin-org.apache.pig.builtin.PigStreaming/stdout-org.apache.pig.builtin.PigStreaming)' failed with exit status: 1
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:307)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:274)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)

2015-09-20 16:44:56,874 [main] ERROR org.apache.pig.tools.pigstats.PigStats  - ERROR 0: org.apache.pig.backend.executionengine.ExecException: ERROR 2055: Received Error while processing the map plan: 'false (stdin-org.apache.pig.builtin.PigStreaming/stdout-org.apache.pig.builtin.PigStreaming)' failed with exit status: 1
2015-09-20 16:44:56,874 [main] ERROR org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil  - 1 map reduce job(s) failed!
2015-09-20 16:44:56,875 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.SimplePigStats  - Script Statistics: 

HadoopVersion	PigVersion	UserId	StartedAt	FinishedAt	Features
2.6.0	0.16.0-SNAPSHOT	mitdesai	2015-09-20 16:44:38	2015-09-20 16:44:56	STREAMING

Failed!

Failed Jobs:
JobId	Alias	Feature	Message	Outputs
job_1442785471589_0001	A,B	STREAMING,MAP_ONLY	Message: Job failed!	hdfs://localhost:63266/user/mitdesai/bar,

Input(s):

Output(s):
Failed to produce result in ""hdfs://localhost:63266/user/mitdesai/bar""

Counters:
Total records written : 0
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_1442785471589_0001	->	null,
null


2015-09-20 16:44:56,875 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher  - Failed!
2015-09-20 16:44:56,876 [IPC Server handler 8 on 63266] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=mitdesai (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/user/mitdesai/bar	dst=null	perm=null	proto=rpc
2015-09-20 16:44:56,878 [IPC Server handler 9 on 63266] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=mitdesai (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/user/mitdesai/bar	dst=null	perm=null	proto=rpc
2015-09-20 16:44:56,879 [main] ERROR org.apache.pig.tools.grunt.Grunt  - ERROR 6017: stop.on.failure set true. Some jobs failed. Stopping Execution
2015-09-20 16:44:56,879 [main] WARN  org.apache.pig.tools.grunt.Grunt  - There is no log file to write to.
2015-09-20 16:44:56,879 [main] ERROR org.apache.pig.tools.grunt.Grunt  - org.apache.pig.backend.executionengine.ExecException: ERROR 6017: stop.on.failure set true. Some jobs failed. Stopping Execution
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:546)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:304)
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1390)
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1375){noformat}",21/Sep/15 13:44;mitdesai;The ExecException message is different in the log here. I forgot to rerun the test after modifying the error message.,29/Jan/16 18:03;mitdesai;Updated the patch.,"28/Mar/17 21:28;rohini;With the attached patch, it display job stats when stop on failure is set as below in mapreduce.

{code}
2017-03-28 14:15:04,580 [main] ERROR org.apache.pig.tools.pigstats.PigStats - ERROR 0: org.apache.pig.backend.executionengine.ExecException: ERROR 2055: Received Error while processing the map plan: 'false (stdin-org.apache.pig.builtin.PigStreaming/stdout-org.apache.pig.builtin.PigStreaming)' failed with exit status: 1
2017-03-28 14:15:04,580 [main] ERROR org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil - 2 map reduce job(s) failed!
2017-03-28 14:15:04,580 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.SimplePigStats - Script Statistics:

HadoopVersion   PigVersion      UserId  StartedAt       FinishedAt      Features
2.7.3   0.17.0-SNAPSHOT rohinip 2017-03-28 14:14:41     2017-03-28 14:15:04     STREAMING

Failed!

Failed Jobs:
JobId   Alias   Feature Message Outputs
job_1490735309298_0017  A,B     STREAMING,MAP_ONLY      Message: Job failed!    hdfs://localhost:56480/bar,
job_1490735309298_0018  A1      MAP_ONLY        Message: Failing running job for -stop_on_failure: job_1490735309298_0018       hdfs://localhost:56480/bar1,

Input(s):

Output(s):
Failed to produce result in ""hdfs://localhost:56480/bar""
Failed to produce result in ""hdfs://localhost:56480/bar1""

Counters:
Total records written : 0
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0

Job DAG:
job_1490735309298_0017  ->      null,
null
job_1490735309298_0018
{code}",30/Mar/17 06:12;daijy;+1,30/Mar/17 14:38;rohini;Committed to trunk. Thanks for the review Daniel.,31/Mar/17 21:52;rohini;The test was flaky based on the order of jobs launched. Fixed the assert statements for that in PIG-4677-fixflakytest.patch.,31/Mar/17 22:08;daijy;+1 on PIG-4677-fixflakytest.patch.,31/Mar/17 22:29;rohini;Committed  PIG-4677-fixflakytest.patch to trunk. Thanks Daniel for the review.,,,,,,,,,,,,,,
TOMAP should infer schema,PIG-4674,12863300,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,11/Sep/15 06:31,08/Jun/16 20:48,14/Mar/19 03:08,17/Sep/15 02:41,,,,,,0.16.0,,impl,,,0,,,,,,,TOMAP schema is map only without map value schema. This should be inferred if available.,,,,,,,,,,,,,,,PIG-4734,,,,,11/Sep/15 06:32;daijy;PIG-4674-1.patch;https://issues.apache.org/jira/secure/attachment/12755338/PIG-4674-1.patch,15/Sep/15 22:18;daijy;PIG-4674-2.patch;https://issues.apache.org/jira/secure/attachment/12756106/PIG-4674-2.patch,16/Sep/15 21:35;daijy;PIG-4674-3.patch;https://issues.apache.org/jira/secure/attachment/12756355/PIG-4674-3.patch,17/Sep/15 18:18;daijy;PIG-4674-fixtest.patch;https://issues.apache.org/jira/secure/attachment/12757130/PIG-4674-fixtest.patch,23/Sep/15 05:32;daijy;PIG-4674-fixtest2.patch;https://issues.apache.org/jira/secure/attachment/12761807/PIG-4674-fixtest2.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2015-09-14 21:12:13.173,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Sep 23 06:14:33 UTC 2015,,,,,,,0|i2k1e7:,9223372036854775807,,,,,,,,,,"14/Sep/15 21:12;rohini;TOMAP also accepts a bag with 'pair' tuples (i.e. tuples with a 'key' and a 'value'). That is not handled. Can also simplify code to

{code}
+        Byte valueType = null;
+        if (input != null && input.getFields()!=null) {
            if (input.size() == 1) {
                   //Code for bag of tuple pairs case. Get valueType of the second field in tuple
            } else {
                   valueType = input.getFields().get(1).type;
             }
+        }
+        Schema s = new Schema(new Schema.FieldSchema(null, DataType.MAP));
+        if (valueType != null && valueType != DataType.BYTEARRAY) {
+            s.getFields().get(0).schema = new Schema(new Schema.FieldSchema(null, valueType));
+            return s;
+        }
{code} ","15/Sep/15 22:18;daijy;Yes, that is missing. Attach second patch.","16/Sep/15 20:51;rohini;{code}
+        byte valueType = DataType.BYTEARRAY;
.....
+                    if (valueType == DataType.BYTEARRAY || valueType == input.getFields().get(i).type) {
+                        valueType = input.getFields().get(i).type;
+                    } else {
+                        valueType = DataType.BYTEARRAY;
+                        break;
+                    }
{code}

should be

{code}
+        Byte valueType = null;
+                    if (valueType == null)) {
+                        valueType = input.getFields().get(i).type;
+                    } else if (valueType != input.getFields().get(i).type) {
+                        valueType = DataType.BYTEARRAY;
+                        break;
+                    }
{code}

Without that if we had a0,a1,a2,a3 with a1 as bytearray and a3 as int, it will become map[int] instead of map[bytearray]. ","16/Sep/15 21:35;daijy;Yes, there is a hole. Apply suggested change.",16/Sep/15 22:44;rohini;+1,17/Sep/15 02:41;daijy;Patch committed to trunk. Thanks Rohini for review!,17/Sep/15 18:18;daijy;Commit PIG-4674-fixtest.patch to fix unit test failure.,17/Sep/15 18:26;rohini;+1 for the https://issues.apache.org/jira/secure/attachment/12757130/PIG-4674-fixtest.patch.,23/Sep/15 05:32;daijy;Another unit test failure patch.,23/Sep/15 06:14;prkommireddi;+1,,,,,,,,,,,,,,,,,,
Embedded Python scripts still parse line by line,PIG-4670,12861881,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,04/Sep/15 16:24,08/Jun/16 20:48,14/Mar/19 03:08,04/Oct/15 18:23,,,,,,0.16.0,,,,,0,,,,,,,"PIG-3204 fixed pig script parsing to parse in batches instead of line by line. But the fix in BoundScript is not right and it is still parsing line by line. That makes parsing take long time for very large pig scripts using PigStorage when there is no schema file stored and without -noschema as it tries to find the schema file lots of times.

It should be grunt.parseStopOnError(false); instead of grunt.parseStopOnError(true); to make it parse statements in batch.",,,,,,,,,,,,,PIG-3204,,,,,,,04/Sep/15 16:28;rohini;PIG-4670-1.patch;https://issues.apache.org/jira/secure/attachment/12754221/PIG-4670-1.patch,02/Oct/15 20:11;rohini;PIG-4670-2.patch;https://issues.apache.org/jira/secure/attachment/12764839/PIG-4670-2.patch,08/Oct/15 23:29;rohini;PIG-4670-fix-e2e-failures-nowhitespacechange.patch;https://issues.apache.org/jira/secure/attachment/12765710/PIG-4670-fix-e2e-failures-nowhitespacechange.patch,08/Oct/15 23:29;rohini;PIG-4670-fix-e2e-failures.patch;https://issues.apache.org/jira/secure/attachment/12765711/PIG-4670-fix-e2e-failures.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2015-09-30 23:12:21.783,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Oct 13 03:34:29 UTC 2015,,,,,,,0|i2jss7:,9223372036854775807,,,,,,,,,,30/Sep/15 23:12;daijy;+1. Can you put an explanation why no test added?,"30/Sep/15 23:27;rohini;bq. Can you put an explanation why no test added?
   It just basically changes how a script is parsed and executed - in batch instead of line by line. So existing script tests for unit (TestScript*) and e2e would cover it.",02/Oct/15 20:11;rohini;Actually it is better to have a test case added to verify if it is actually properly working. Added to the existing testcase which was testing the number of initiations when script was parsed. ,02/Oct/15 22:15;daijy;+1,04/Oct/15 18:23;rohini;Committed to trunk. Thanks Daniel for the review.,"08/Oct/15 23:29;rohini;Tests Jython_Diagnostics 1,2,3 were failing. PIG-4670-fix-e2e-failures.patch fixes that. Attached a version without whitespace change as well for easy reviewing. Illustrate still fails for tez mode complaining of 

{code}
Caused by: java.lang.ClassCastException: org.apache.pig.tools.pigstats.tez.TezScriptState cannot be cast to org.apache.pig.tools.pigstats.mapreduce.MRScriptState
{code}

in LocalMapreduceSimulator. Apparently it was succeeding with mapreduce before. Anyways skipped it for tez as illustrate is not implemented. 
",09/Oct/15 00:29;daijy;I see you disable illustrate tests for tez. What are other changes about?,"09/Oct/15 11:09;rohini;I ended up changing code for describe and explain as well earlier which caused the query to execute. So reverted changes in that case back to what it was. Basically it turns on the batch, but called parseStopOnError(true) so that executeBatch() is not done in parseStopOnError. 

{code}
if (!sameBatch) {
                executeBatch();
            }
{code}

Additionally added pigServer.setSkipParseInRegisterForBatch(true); to skip parsing while registering query as dumpSchema() was calling parseQuery again.",13/Oct/15 00:11;daijy;+1,13/Oct/15 03:34;rohini;Committed https://issues.apache.org/jira/secure/attachment/12765711/PIG-4670-fix-e2e-failures.patch. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,
ant mvn-deploy target is broken,PIG-4650,12852343,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,05/Aug/15 18:54,08/Jun/16 20:48,14/Mar/19 03:08,05/Aug/15 23:32,,,,,,0.15.1,0.16.0,build,,,0,,,,,,,"ant mvn-deploy without ""-Dstaging"" is broken. Message:
[artifact:deploy] Error deploying artifact 'org.apache.pig:pig:jar': Error deploying artifact: File /home/jenkins/jenkins-slave/workspace/Pig-trunk-commit/trunk/build/pig-0.16.0-SNAPSHOT-h1.jar does not exist

This causes Pig night build fail: https://builds.apache.org/job/Pig-trunk-commit/2215/",,,,,,,,,,,,,,,,,,,,05/Aug/15 18:54;daijy;PIG-4650-1.patch;https://issues.apache.org/jira/secure/attachment/12748896/PIG-4650-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-08-05 23:25:23.242,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Aug 05 23:32:45 UTC 2015,,,,,,,0|i2idpz:,9223372036854775807,,,,,,,,,,05/Aug/15 23:25;rohini;+1,05/Aug/15 23:32;daijy;Patch committed to trunk and 0.15 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
[Pig on Tez] Union followed by HCatStorer misses some data,PIG-4649,12851445,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,04/Aug/15 17:58,08/Jun/16 20:48,14/Mar/19 03:08,05/Aug/15 20:22,,,,,,0.15.1,0.16.0,,,,0,,,,,,,"Script to reproduce:
{code}
A = LOAD 'data01.txt' USING PigStorage() as (id:chararray, message:chararray);
B = LOAD 'data02.txt' USING PigStorage() as (id:chararray, message:chararray);
C = UNION A, B;
STORE C INTO 'db.table1' USING org.apache.hive.hcatalog.pig.HCatStorer();
{code}",,,,,,,,,,,,,PIG-4691,PIG-4694,,,,,,04/Aug/15 19:06;rohini;PIG-4649-1.patch;https://issues.apache.org/jira/secure/attachment/12748711/PIG-4649-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-08-05 17:55:51.874,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Aug 05 20:22:30 UTC 2015,,,,,,,0|i2ibnz:,9223372036854775807,,,,,,,,,,"04/Aug/15 17:59;rohini;Union directly followed by HCatStorer retains only half of the data. This is due to HCatStorer not honoring mapreduce.output.basename and always using part. Tez sets mapreduce.output.basename to part-v000-o000 (vertex id followed by output id). With union optimizer, Pig uses vertex groups to write directly from both the vertices to the final output directory. Since hcat ignores the mapreduce.output.basename, both the vertices produce part-r-0000<n> and when the files are moved from the temp location to the final directory, they just overwrite each other. There is no failure and only one of the files with that name makes it into the final directory.","04/Aug/15 18:06;rohini;Filed HIVE-11456 to fix HCatStorer. But also adding support in Pig to not optimize unions if certain StoreFunc is present. Initially, by default will turn union optimization off for HCatStorer using a setting.
pig.tez.opt.union.unsupported.storefuncs=org.apache.hcatalog.pig.HCatStorer,org.apache.hive.hcatalog.pig.HCatStorer

Folks with a hive release with HIVE-11456 fixed can override that setting.","05/Aug/15 17:55;daijy;+1.

We can remove this once HIVE-11456 is fully fixed, right?","05/Aug/15 19:48;rohini;bq. We can remove this once HIVE-11456 is fully fixed, right?
    We can keep the setting, but change default to none. If there are other custom storefuncs which do not handle it in OutputCommitter well, it will be an option to turn of for that StoreFunc instead of turning off the optimizer all together.",05/Aug/15 20:22;rohini;Committed to branch-0.15 (0.15.1) and trunk (0.16). Thanks for the review Daniel. ,,,,,,,,,,,,,,,,,,,,,,,
PORelationToExprProject.clone() is broken,PIG-4644,12849368,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,erwaman,rdsr,rdsr,27/Jul/15 16:48,08/Jun/16 20:48,14/Mar/19 03:08,21/Sep/15 23:06,,,,,,0.15.1,0.16.0,,,,0,,,,,,,"We are receiving the following exception when using Pig
{noformat}
Caused by: java.lang.ClassCastException: org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject cannot be cast to org.apache.pig.backend.hadoop.executionen\
gine.physicalLayer.expressionOperators.PORelationToExprProject
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORelationToExprProject.clone(PORelationToExprProject.java:144)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORelationToExprProject.clone(PORelationToExprProject.java:50)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan.clone(PhysicalPlan.java:227)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.clone(POForEach.java:639)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.clone(POForEach.java:53)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan.clone(PhysicalPlan.java:227)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer.mergeDiamondMROper(MultiQueryOptimizer.java:298)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer.visitMROp(MultiQueryOptimizer.java:219)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:273)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:46)
        at org.apache.pig.impl.plan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:71)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:46)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer.visit(MultiQueryOptimizer.java:94)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.compile(MapReduceLauncher.java:629)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:148)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1264)
{noformat}

On further investigation it seems that POProject's clone method is implemented as 

{noformat}
    @Override
    public POProject clone() throws CloneNotSupportedException {
        ArrayList<Integer> cols = new ArrayList<Integer>(columns.size());
        // Can resuse the same Integer objects, as they are immutable
        for (Integer i : columns) {
            cols.add(i);
        }
        POProject clone = new POProject(new OperatorKey(mKey.scope,
            NodeIdGenerator.getGenerator().getNextNodeId(mKey.scope)),
            requestedParallelism, cols);
        clone.cloneHelper(this);
        clone.overloaded = overloaded;
        clone.startCol = startCol;
        clone.isProjectToEnd = isProjectToEnd;
        clone.resultType = resultType;
        return clone;
    }
{noformat}

It uses a constructor to clone POProject (which break the weak rule of object cloning)
In the subclass , PORelationToExprProject implements cloneable as

{noformat}
@Override
    public PORelationToExprProject clone() throws CloneNotSupportedException {
        return (PORelationToExprProject) super.clone();
    }
{noformat}

As seen from the POProject's implementation of cloneable, super.clone will never be of type PORelationToExprProject,",,,,,,,,,,,,,,,,,,,,05/Aug/15 04:12;erwaman;PIG-4644.1.patch;https://issues.apache.org/jira/secure/attachment/12748783/PIG-4644.1.patch,06/Aug/15 01:13;erwaman;PIG-4644.2.patch;https://issues.apache.org/jira/secure/attachment/12748973/PIG-4644.2.patch,06/Aug/15 18:19;erwaman;PIG-4644.3.patch;https://issues.apache.org/jira/secure/attachment/12749109/PIG-4644.3.patch,06/Aug/15 20:08;rohini;PIG-4644.4.patch;https://issues.apache.org/jira/secure/attachment/12749119/PIG-4644.4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2015-07-27 18:46:25.331,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Nov 02 19:12:26 UTC 2015,,,,,,,0|i2hz5b:,9223372036854775807,,,,,,,,,,"27/Jul/15 18:46;erwaman;I believe the original implementation of PORelationToExprProject.clone() from PIG-514 was correct:
{code}
+    @Override
+    public PORelationToExprProject clone() throws CloneNotSupportedException {
+        ArrayList<Integer> cols = new ArrayList<Integer>(columns.size());
+        // Can resuse the same Integer objects, as they are immutable
+        for (Integer i : columns) {
+            cols.add(i);
+        }
+        PORelationToExprProject clone = new PORelationToExprProject(new OperatorKey(mKey.scope, 
+            NodeIdGenerator.getGenerator().getNextNodeId(mKey.scope)),
+            requestedParallelism, cols);
+        clone.cloneHelper(this);
+        clone.star = star;
+        clone.overloaded = overloaded;
+        return clone;
+    }
{code}

but it seems to have been changed to its current state in PIG-1693:
{code}
     @Override
     public PORelationToExprProject clone() throws CloneNotSupportedException {
-        ArrayList<Integer> cols = new ArrayList<Integer>(columns.size());
-        // Can resuse the same Integer objects, as they are immutable
-        for (Integer i : columns) {
-            cols.add(i);
-        }
-        PORelationToExprProject clone = new PORelationToExprProject(new OperatorKey(mKey.scope, 
-            NodeIdGenerator.getGenerator().getNextNodeId(mKey.scope)),
-            requestedParallelism, cols);
-        clone.cloneHelper(this);
-        clone.star = star;
-        clone.overloaded = overloaded;
-        return clone;
+        return (PORelationToExprProject) super.clone();
     }
{code}","28/Jul/15 23:50;rohini;Surprising that we have not run into this in so many years.  [~rdsr], are you planning to provide the fix for this? If not could you just post the script to reproduce the issue and me or Daniel can pick this up?",30/Jul/15 16:40;rdsr;[~rohini] One of our user's script failed. I'll work through the script to come up with a minimal example which breaks the code. ,"04/Aug/15 18:48;erwaman;We're using a custom loader. The (simplified) user script looks something like this
{code}
a = LOAD 'data' USING CustomLoader();

a = foreach a {
  b = foreach foo generate c.d#'e';
  generate b;
};

b = filter a by foo is not null;
c = filter a by foo is null;

d = UNION b,c;
dump d;
{code}

The physical plan looks like this:
{code}
#-----------------------------------------------
# Physical Plan:
#-----------------------------------------------
d: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-22
|
|---d: Union[bag] - scope-21
    |
    |---b: Filter[bag] - scope-12
    |   |   |
    |   |   Not[boolean] - scope-15
    |   |   |
    |   |   |---POIsNull[boolean] - scope-14
    |   |       |
    |   |       |---Project[bag][0] - scope-13
    |   |
    |   |---a: Filter[bag] - scope-10
    |       |   |
    |       |   Constant(true) - scope-11
    |       |
    |       |---a: Split - scope-9
    |           |
    |           |---a: New For Each(false)[bag] - scope-8
    |               |   |
    |               |   RelationToExpressionProject[bag][*] - scope-1
    |               |   |
    |               |   |---foo: New For Each(false)[bag] - scope-7
    |               |       |   |
    |               |       |   POMapLookUp[chararray] - scope-5
    |               |       |   |
    |               |       |   |---Project[map][0] - scope-4
    |               |       |       |
    |               |       |       |---Project[tuple][4] - scope-3
    |               |       |
    |               |       |---Project[bag][0] - scope-2
    |               |
    |               |---a: Load(data:CustomLoader) - scope-0
    |
    |---c: Filter[bag] - scope-18
        |   |
        |   POIsNull[boolean] - scope-20
        |   |
        |   |---Project[bag][0] - scope-19
        |
        |---a: Filter[bag] - scope-16
            |   |
            |   Constant(true) - scope-17
            |
            |---a: Split - scope-9
                |
                |---a: New For Each(false)[bag] - scope-8
                    |   |
                    |   RelationToExpressionProject[bag][*] - scope-1
                    |   |
                    |   |---foo: New For Each(false)[bag] - scope-7
                    |       |   |
                    |       |   POMapLookUp[chararray] - scope-5
                    |       |   |
                    |       |   |---Project[map][0] - scope-4
                    |       |       |
                    |       |       |---Project[tuple][4] - scope-3
                    |       |
                    |       |---Project[bag][0] - scope-2
                    |
                    |---a: Load(data:CustomLoader) - scope-0
{code}
and the map reduce plan looks like:
{code}
#--------------------------------------------------
# Map Reduce Plan
#--------------------------------------------------
MapReduce node scope-29
Map Plan
d: Store(fakefile:org.apache.pig.builtin.PigStorage) - scope-22
|
|---d: Union[bag] - scope-21
    |
    |---b: Filter[bag] - scope-12
    |   |   |
    |   |   Not[boolean] - scope-15
    |   |   |
    |   |   |---POIsNull[boolean] - scope-14
    |   |       |
    |   |       |---Project[bag][0] - scope-13
    |   |
    |   |---a: New For Each(false)[bag] - scope-44
    |       |   |
    |       |   RelationToExpressionProject[bag][*] - scope-42
    |       |   |
    |       |   |---foo: New For Each(false)[bag] - scope-41
    |       |       |   |
    |       |       |   POMapLookUp[chararray] - scope-38
    |       |       |   |
    |       |       |   |---Project[map][0] - scope-40
    |       |       |       |
    |       |       |       |---Project[tuple][4] - scope-39
    |       |       |
    |       |       |---Project[bag][0] - scope-43
    |       |
    |       |---a: Load(data:CustomLoader) - scope-45
    |
    |---c: Filter[bag] - scope-18
        |   |
        |   POIsNull[boolean] - scope-20
        |   |
        |   |---Project[bag][0] - scope-19
        |
        |---a: New For Each(false)[bag] - scope-36
            |   |
            |   RelationToExpressionProject[bag][*] - scope-34
            |   |
            |   |---foo: New For Each(false)[bag] - scope-33
            |       |   |
            |       |   POMapLookUp[chararray] - scope-30
            |       |   |
            |       |   |---Project[map][0] - scope-32
            |       |       |
            |       |       |---Project[tuple][4] - scope-31
            |       |
            |       |---Project[bag][0] - scope-35
            |
            |---a: Load(data:CustomLoader) - scope-37--------
Global sort: false
----------------
{code}

I haven't been able to reproduce this issue using PigStorage and some sample data. When I try, though the physical plan looks the same, the MR plan ends up having two MR jobs instead of one and the issue doesn't surface.",05/Aug/15 04:11;erwaman;RB: https://reviews.apache.org/r/37115/,"05/Aug/15 20:29;rohini;{code}
if (this.isProjectToEnd()) {
+          clone.setProjectToEnd(this.getStartCol());
+        }
{code}

Please change to

{code}
clone.startCol = startCol;
clone.isProjectToEnd = isProjectToEnd;
{code}

clone.setProjectToEnd(int) reinitializes columns to empty arraylist
",05/Aug/15 20:30;rohini;Also you can skip the test in TestPORelationToExprProject.java unless you can find a pig script to reproduce the issue.,"06/Aug/15 01:10;erwaman;Thanks for the review, Rohini. I had to do it this way because {{startCol}} and {{isProjectToEnd}} are private variables in {{POProject.java}}

{code:title=POProject.java}
    private boolean isProjectToEnd = false;
    private int startCol;
{code}

and thus cannot be accessed directly from {{PORelationToExprProject.java}}. Do you think we should change the accessibility to protected?","06/Aug/15 01:14;erwaman;I've removed the test, updated my [RB|https://reviews.apache.org/r/37115/], and uploaded a [new version of my patch|^PIG-4644.2.patch].","06/Aug/15 15:33;rohini;bq. Do you think we should change the accessibility to protected?
   Yes. Can you also add protected to overloaded when doing that? Currently it does not have any modifier.","06/Aug/15 18:21;erwaman;I changed the accessibility of {{isProjectToEnd}} and {{startCol}} to {{protected}} and updated {{PORelationToExprProject.clone()}}.

Where else do you want me to add {{protected}}?  I did not understand your second comment.","06/Aug/15 18:21;erwaman;I changed the accessibility of {{isProjectToEnd}} and {{startCol}} to {{protected}} and updated {{PORelationToExprProject.clone()}}.

Where else do you want me to add {{protected}}?  I did not understand your second comment.","06/Aug/15 20:08;rohini; +1.  Committed to trunk. Thanks [~erwaman] for fixing this.

Wanted the overloaded variable to be changed to protected which did not have any modifier. There were two others as well. Minor change. Did it before committing. PIG-4644.4.patch is the committed one. 

","07/Aug/15 18:14;erwaman;Thanks, Rohini!",21/Sep/15 23:06;rohini;Had missed marking the jira resolved earlier. Doing that now.,02/Nov/15 19:12;daijy;Backport the patch to 0.15 branch.,,,,,,,,,,,,
Print the instance of Object without using toString(),PIG-4641,12849176,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,sandyridgeracer,songwang,songwang,26/Jul/15 20:06,08/Jun/16 20:48,14/Mar/19 03:08,25/Apr/16 20:23,0.13.0,0.14.0,0.15.0,,,0.16.0,,,,,0,,,,,,,"In  Class: pig-0.15.0\src\org\apache\pig\scripting\js\JsFunction.java

when printing the instance of Object ""value"", should use ""toString()"" to convert it to a String instance

private void debugConvertPigToJS(int depth, String pigType, Object value, Schema schema) {
        if (LOG.isDebugEnabled()) {
            LOG.debug(indent(depth)+""converting from Pig "" + pigType + "" "" + value + "" using "" + stringify(schema));
        }
    }

In the same Class, all others use toString(value) to print the instance of Object ""value"".",,600,600,,0%,600,600,,,,,,,,,,,,,19/Sep/15 07:23;sandyridgeracer;PIG-4641.patch;https://issues.apache.org/jira/secure/attachment/12761262/PIG-4641.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-07-29 00:10:52.595,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Apr 25 20:23:10 UTC 2016,,,,,,,0|i2hxzr:,9223372036854775807,,,,,,,toString,,,29/Jul/15 00:10;rohini;It is just a debug statement and it should not matter. ,"30/Jul/15 02:39;songwang;yes, it is trivial but still a bug, I came cross this bug with my bug detection tool.","19/Sep/15 07:23;sandyridgeracer;Attaching the patch.
Used {{toString}} to print the value.",25/Apr/16 20:23;rohini;+1. Committed to trunk. Thanks for the contribution Sandeep.,,,,,,,,,,,,,,,,,,,,,,,,
Compiling Pig with JDK8 or JDK7 Update 85 breaks Ruby UDFs,PIG-4640,12848938,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,deltreey,deltreey,24/Jul/15 16:51,21/Jun/17 09:15,14/Mar/19 03:08,01/Apr/17 04:59,,,,,,0.17.0,,,,,0,,,,,,,"If you build pig against java patch version 79, it works, but pig built against 85 does not.  Here is the code to reproduce.

{code}
$ java -version
java version ""1.7.0_85""
OpenJDK Runtime Environment (rhel-2.6.1.3.el6_6-x86_64 u85-b01)
OpenJDK 64-Bit Server VM (build 24.85-b03, mixed mode)
{code}

Then add this code to `query.pig`. Notice that we're registering a ruby UDF and that's all.

{code}
SET job.priority 'normal'; REGISTER ./utility_udfs.rb using jruby AS utilities;
{code}

Make sure you build pig against this java version.

{code}
$ant
{code}

Then run the job

{code}
$pig -x local -F query.pig
{code}

You will get this error

{code}
2015-07-24 11:32:25,215 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. Native Exception: 'class java.lang.ClassFormatError'; Message: Duplicate method name&signature in class file org/apache/pig/scripting/jruby/RubyDataBag$i$initialize; StackTrace: java.lang.ClassFormatError: Duplicate method name&signature in class file org/apache/pig/scripting/jruby/RubyDataBag$i$initialize
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
    at org.jruby.util.JRubyClassLoader.defineClass(JRubyClassLoader.java:76)
    at org.jruby.internal.runtime.methods.InvocationMethodFactory.endClass(InvocationMethodFactory.java:1378)
    at org.jruby.internal.runtime.methods.InvocationMethodFactory.getAnnotatedMethodClass(InvocationMethodFactory.java:792)
    at org.jruby.internal.runtime.methods.InvocationMethodFactory.getAnnotatedMethod(InvocationMethodFactory.java:702)
    at org.jruby.RubyModule.defineAnnotatedMethod(RubyModule.java:716)
    at org.jruby.anno.TypePopulator$DefaultTypePopulator.populate(TypePopulator.java:75)
    at org.jruby.RubyModule.defineAnnotatedMethodsIndividually(RubyModule.java:708)
    at org.jruby.RubyModule.defineAnnotatedMethods(RubyModule.java:596)
    at org.apache.pig.scripting.jruby.RubyDataBag.define(RubyDataBag.java:112)
    at org.apache.pig.scripting.jruby.PigJrubyLibrary.load(PigJrubyLibrary.java:76)
{code}

If you're on this Java version, everything runs fine

{code}
$ java -version
java version ""1.7.0_79""
OpenJDK Runtime Environment (rhel-2.5.5.3.el6_6-x86_64 u79-b14)
OpenJDK 64-Bit Server VM (build 24.79-b02, mixed mode)
{code}

java 1.7.0_85 also breaks with pig 0.15","CentOS 6.5
Pig 0.14.3
Java 1.7.0 - see below",,,,,,,,,,,,PIG-5175,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-07-28 23:57:34.767,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 01 04:59:01 UTC 2017,,,,,,,0|i2hwjb:,9223372036854775807,,,,,,,,,,"28/Jul/15 23:57;rohini;Thanks for creating the issue. I had encountered the same issue earlier, but hit it when compiling pig with JDK 8. The cause of the issue is http://mail.openjdk.java.net/pipermail/compiler-dev/2013-November/008151.html.  Could not find any workaround. Need to try not changing the return type of the initialize methods and leave them at RubyObject and see if that works for pig scripts.","29/Jul/15 00:20;bridiver;It seems to me that there is a lot of unnecessary translation going on with the jruby UDFs converting Java classes to jruby objects. I think Pig could benefit from a more efficient implementation that passed in Java Tuples and DataBags instead of RubyDataBags, hashes and arrays. If we worked on a second implementation (so as not to break backwards compatibility), would that be something that you would be interested in making a part of pig?","30/Jul/15 02:58;rohini;[~bridiver],
     More efficient and performant implementation and which is totally backward compatible too - I doubt you will hear any NO's for that :) .","01/Aug/15 00:28;bridiver;Well, backwards compatible in that it will be a parallel implementation. So instead of 'using jruby' maybe it would be 'using jruby-native' or something like that","03/Aug/15 23:23;daijy;I definitely prefer a more efficient implementation. We can leave parallel implementations for one or two transitive releases, and deprecate the old version in the future release.",04/Aug/15 00:58;bridiver;https://issues.apache.org/jira/browse/PIG-4648,08/Mar/17 19:21;daijy;Upgrade to a newer version of JRuby would fix the issue.,01/Apr/17 04:59;rohini;[~daijy] has fixed this as part of PIG-5175,,,,,,,,,,,,,,,,,,,,
Occurred spelled incorrectly in error message for Launcher and POMergeCogroup,PIG-4636,12848387,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,stevenmz,stevenmz,stevenmz,22/Jul/15 21:24,08/Jun/16 20:48,14/Mar/19 03:08,04/Aug/15 05:36,0.15.0,,,,,0.16.0,,,,22/Jul/15 00:00,0,easyfix,newbie,spelling,,,,Two minor spelling mistakes in the error messages of two classes.,,60,60,,0%,60,60,,,,,,,,,,,,,22/Jul/15 21:27;stevenmz;spelling_patch.diff;https://issues.apache.org/jira/secure/attachment/12746647/spelling_patch.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-08-04 05:36:05.693,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Aug 04 05:36:05 UTC 2015,,,Patch Available,,,,0|i2ht6v:,9223372036854775807,,,,,,,,,,22/Jul/15 21:27;stevenmz;A patch to fix two spelling mistakes.,22/Jul/15 21:28;stevenmz;A patch to fix two spelling mistakes has been attached to this JIRA issue.,04/Aug/15 05:36;daijy;Patch committed to trunk. Thanks [~stevenmz]!,,,,,,,,,,,,,,,,,,,,,,,,,
NPE while running pig script in tez mode( pig 0.15 with tez 0.7),PIG-4635,12846448,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,sachinsabbarwal,sachinsabbarwal,21/Jul/15 07:22,08/Jun/16 20:48,14/Mar/19 03:08,25/Sep/15 17:45,0.15.0,,,,,0.15.1,0.16.0,,,,0,,,,,,,"I was trying to run (attached) pig script on pig 0.15 in tez mode(with tez 0.7).
I got following NullPointerException. I tried to send a mail to tez user mailing list and was asked to file a jira for this.
Here is the NPE:

 5 org.apache.pig.impl.plan.VisitorException: ERROR 0: java.lang.NullPointerException
  6   at org.apache.pig.backend.hadoop.executionengine.tez.plan.optimizer.ParallelismSetter.visitTezOp(ParallelismSetter.java:201)
  7   at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezOperator.visit(TezOperator.java:246)
  8   at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezOperator.visit(TezOperator.java:53)
  9   at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:87)
 10   at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:46)
 11   at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher.processLoadAndParallelism(TezLauncher.java:449)
 12   at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher.launchPig(TezLauncher.java:163)
 13   at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:304)
 14   at org.apache.pig.PigServer.launchPlan(PigServer.java:1390)
 15   at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1375)
 16   at org.apache.pig.PigServer.execute(PigServer.java:1364)
 17   at org.apache.pig.PigServer.executeBatch(PigServer.java:415)
 18   at org.apache.pig.PigServer.executeBatch(PigServer.java:398)
 19   at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:171)
 20   at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:234)
 21   at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)
 22   at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
 23   at org.apache.pig.Main.run(Main.java:502)
 24   at org.apache.pig.Main.main(Main.java:177)
 25   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 26   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
 27   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 28   at java.lang.reflect.Method.invoke(Method.java:606)
 29   at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
 30 Caused by: java.lang.NullPointerException
 31   at org.apache.pig.backend.hadoop.executionengine.tez.plan.optimizer.TezOperDependencyParallelismEstimator.estimateParallelism(TezOperDependencyParallelismEstimator.java:114)
 32   at org.apache.pig.backend.hadoop.executionengine.tez.plan.optimizer.ParallelismSetter.visitTezOp(ParallelismSetter.java:138)
 33   ... 23 more","Pig: 0.15
Tez: 0.7",,,,,,,,,,,,,,,,,,,24/Sep/15 05:42;daijy;PIG-4635-1.patch;https://issues.apache.org/jira/secure/attachment/12762060/PIG-4635-1.patch,24/Sep/15 22:59;daijy;PIG-4635-2.patch;https://issues.apache.org/jira/secure/attachment/12762264/PIG-4635-2.patch,23/Jul/15 21:28;rohini;debug.patch;https://issues.apache.org/jira/secure/attachment/12746882/debug.patch,21/Jul/15 07:24;sachinsabbarwal;main.pig;https://issues.apache.org/jira/secure/attachment/12746290/main.pig,21/Jul/15 07:24;sachinsabbarwal;properties;https://issues.apache.org/jira/secure/attachment/12746291/properties,21/Jul/15 07:24;sachinsabbarwal;tez-site.xml;https://issues.apache.org/jira/secure/attachment/12746292/tez-site.xml,,,,,,6.0,,,,,,,,,,,,,,,,,,,2015-07-23 21:28:13.094,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Sep 25 17:45:10 UTC 2015,,,,,,,0|i2hhb3:,9223372036854775807,,,,,,,,,,21/Jul/15 07:24;sachinsabbarwal;Attached pig script and other files.,"23/Jul/15 21:28;rohini;[~sachinsabbarwal],
    Can you compile pig with the attached patch and try run it? This has more debug messages to help identify the problem.

Can you also try running your script with -Dpig.tez.opt.union=false and see if that works?","03/Aug/15 11:58;sachinsabbarwal;Hi Rohini, Just saw this comment.
I'll try this patch and share the results with you guys.

Thanks","02/Sep/15 21:58;rohini;[~sachinsabbarwal],
   Is it possible for you try your script with code compiled from recent pig trunk? PIG-4315 might fix your issue.","24/Sep/15 05:42;daijy;Finally get a reproduction. The issue is caused by transient variable TezOperator.vertexGroupInfo. This variable will not be deserialized in PigGraceShuffleVertexManager. Change the way we doing isVertexGroup, which will be used in PigGraceShuffleVertexManager.","24/Sep/15 21:41;rohini;Daniel,

{code}
     public boolean isVertexGroup() {
-        return vertexGroupInfo != null;
+        return vertexGroupMembers != null;
     }
{code}

 vertexGroupMembers is always set for union operators. This change would not be right as vertexGroupMembers will not be null even if it was not optimized as a vertexgroup in UnionOptimizer or if UnionOptimizer was turned off. You should not require the changes in TezPrinter.java as well. If needed for PigGraceShuffleVertexManager we can maintain another boolean variable isVertexGroup when setVertexGroupInfo is called which will be serialized.  

- Can you also add assert for contents of the output in the test case? Would be good to validate that considering there are very few test cases which will hit PigGraceShuffleVertexManager code.","24/Sep/15 22:59;daijy;Yes, maintaining another variable is safer, I can do that. I do want the TezPrinter.java change since tezOper.getVertexGroupInfo() is null in the backend and I cannot peek the plan.","25/Sep/15 00:19;rohini;bq. I do want the TezPrinter.java change since tezOper.getVertexGroupInfo() is null in the backend and I cannot peek the plan.
  You mean for debugging and looking at variables in Eclipse or Intellij ?

+1","25/Sep/15 02:27;daijy;Yes, that what I mean.","25/Sep/15 17:45;daijy;Patch committed to both trunk and 0.15 branch. Thanks Rohini, Sachin!",,,,,,,,,,,,,,,,,,
[Pig on Tez] Self join does not handle null values correctly,PIG-4627,12843350,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,07/Jul/15 23:39,08/Jun/16 20:48,14/Mar/19 03:08,12/Aug/15 16:34,,,,,,0.15.1,0.16.0,,,,0,,,,,,,  Self join does not produce right results in case of null after PIG-4495 which writes multiple inputs into same tez input. Need the https://issues.apache.org/jira/secure/attachment/12628162/PIG-3761-1.patch fix of  PIG-3761 to handle that by comparing indexes in the raw comparators.,,,,,,,,,,,,,,,,,,,,09/Aug/15 03:25;rohini;PIG-4627-1.patch;https://issues.apache.org/jira/secure/attachment/12749444/PIG-4627-1.patch,02/Sep/15 05:08;rohini;PIG-4627-fix-testfailures.patch;https://issues.apache.org/jira/secure/attachment/12753679/PIG-4627-fix-testfailures.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-08-11 22:05:24.578,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Sep 02 21:48:28 UTC 2015,,,,,,,0|i2gyrj:,9223372036854775807,,,,,,,,,,11/Aug/15 21:47;rohini;Tez has not been using PigWritableComparator classes like Mapreduce and instead using SortComparators and that cannot be changed without some additions to Tez. Details in PIG-4652.  So the SortComparators had to be fixed to handle null correctly.,"11/Aug/15 22:05;daijy;The best solution is to use PigWritableComparator for join/group, since PigWritableComparator is much faster than PigRawComparator.

For now, +1 for the patch since it provides a simpler fix to the correctness issue.",12/Aug/15 16:34;rohini;Committed to trunk and branch-0.15. Thanks for the review Daniel.,"02/Sep/15 05:08;rohini;Fixing unit test failures in https://builds.apache.org/job/Pig-trunk-commit/2232/testReport/

TestEvalPipelineLocal.testBytesRawComparatorDesc
TestEvalPipelineLocal.testBytesRawComparatorDesc

PigBytesRawComparator.java needed to return return code immediately after comparison without applying sort order for PIG-4298 which I had missed earlier not reading the comment.",02/Sep/15 21:31;daijy;+1 for PIG-4627-fix-testfailures.patch.,02/Sep/15 21:48;rohini;Test failure fix patch is now checked into trunk and branch-0.15. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,
Error on ORC empty file without schema,PIG-4624,12843050,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,thejas,thejas,07/Jul/15 00:17,08/Jun/16 20:48,14/Mar/19 03:08,04/Aug/15 00:17,0.15.0,,,,,0.15.1,0.16.0,,,,0,,,,,,,"If ORC produces an empty file without schema (which ideally, it is not supposed to), then pig query reading the data gives the following error - ""org.apache.pig.tools.grunt.Grunt - ERROR 1031: Incompatable schema"" ",,,,,,,,,,,,,,,,,,,,07/Jul/15 01:03;thejas;PIG-4624.1.patch;https://issues.apache.org/jira/secure/attachment/12743860/PIG-4624.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-07-07 16:59:17.304,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Aug 04 00:17:14 UTC 2015,,,,,,,0|i2gwx3:,9223372036854775807,,,,,,,,,,"07/Jul/15 00:27;thejas;The ORC issue should be separately addressed in ORC/Hive, however, it would be good if pig can handle this case with already generated files.

Attaching patch from [~daijy].
",07/Jul/15 16:59;rohini;+1,27/Jul/15 03:39;water;+1,"04/Aug/15 00:17;daijy;Patch committed to trunk. Thanks [~thejas], [~rohini], [~water]",,,,,,,,,,,,,,,,,,,,,,,,
"When use tez as the engine , set pig.user.cache.enabled=true  do  not take effect  ",PIG-4618,12841955,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,wisgood,wisgood,wisgood,01/Jul/15 10:33,08/Jun/16 20:48,14/Mar/19 03:08,15/Jul/15 21:13,0.14.0,,,,,0.16.0,,tez,,,0,,,,,,,,"hadoop2.5
tez0.6",,,,,,,,,,,,,,,,,,,01/Jul/15 10:36;wisgood;PIG-4618.0.patch.txt;https://issues.apache.org/jira/secure/attachment/12743041/PIG-4618.0.patch.txt,11/Jul/15 01:55;wisgood;PIG-4618.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12744863/PIG-4618.1.patch.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-07-11 00:51:30.763,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Jul 15 21:13:39 UTC 2015,,,,,,,0|i2gqcf:,9223372036854775807,,,,,,,,,,"01/Jul/15 10:44;wisgood;I need a review ，thanks 
[~rohini.u]  [~daro] ",01/Jul/15 10:46;wisgood;[~rohini],08/Jul/15 03:27;wisgood;[~rohini] [~daro] ,08/Jul/15 13:56;wisgood;[~rohini] [~daro]  Could you confirm that is this a bug ?Thanks very much !,11/Jul/15 00:51;rohini;Can you change the static methods in JobControlCompiler to public and call them instead of duplicating them in TezResourceManager?,"11/Jul/15 01:11;wisgood;Ok,I will have a try!",11/Jul/15 02:00;wisgood;I pick up a new patch .[~rohini],14/Jul/15 00:26;wisgood;[~rohini],15/Jul/15 21:13;rohini;+1. Committed to trunk (0.16). Thanks [~wisgood] for the contribution.,,,,,,,,,,,,,,,,,,,
Applying isFirstReduceOfKey for Skewed left outer join skips records,PIG-4587,12835245,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,rohini,rohini,03/Jun/15 23:54,08/Jun/16 20:48,14/Mar/19 03:08,18/Jan/16 19:48,0.15.0,,,,,0.15.1,0.16.0,,,,0,,,,,,,"PIG-4377 introduced isFirstReduceOfKey to avoid extra records in case of over sampling. But the issue can occur only in the case of right outer join. But it is added to the plan in MRCompiler and TezCompiler (PIG-4580) for both left and right outer joins. We need to remove that extra check for right outer join. It is unnecessary performance penalty.
",,,,,,,,,,,,,,,,,,,,21/Jan/16 22:41;rohini;PIG-4587-1-branch-0.15.patch;https://issues.apache.org/jira/secure/attachment/12783692/PIG-4587-1-branch-0.15.patch,18/Jan/16 18:01;rohini;PIG-4587-1.patch;https://issues.apache.org/jira/secure/attachment/12782909/PIG-4587-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-01-18 19:42:37.137,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Jan 21 22:40:50 UTC 2016,,,,,,,0|i2flvr:,9223372036854775807,,,,,,,,,,18/Jan/16 17:27;rohini;Found that PIG-4377 fix for right outer join produces wrong results with left outer join and skips records if it is not the first reducer which is very bad.,"18/Jan/16 18:01;rohini;Changes done:
    - Now adding isFirstReduceOfKey only for right outer join
    - Additional enhancements to stats display which helped debug the issue faster instead of having to go to the UI
            -Added REDUCE_INPUT_RECORDS counter in the stats output which contains the shuffle records. INPUT_RECORDS_PROCESSED only contains broadcast and MR input.
            -InputRecords and OutputRecords were showing 0 if there was no load or store. ",18/Jan/16 19:42;daijy;+1,18/Jan/16 19:48;rohini;Committed to trunk. Thanks for the review Daniel.,21/Jan/16 22:40;rohini;Committing the patch to branch-0.15 as well. PIG-4587-1-branch-0.15.patch is the patch committed to branch 0.15 which does not contain the TezVertexStats and TezDAGStats changes due to lot of conflict. Those are unrelated enhancements anyways.,,,,,,,,,,,,,,,,,,,,,,,
thread safe issue in NodeIdGenerator,PIG-4581,12834174,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,rcatherinot,rcatherinot,rcatherinot,01/Jun/15 08:24,08/Jun/16 20:48,14/Mar/19 03:08,25/Apr/16 21:04,0.14.0,,,,,0.16.0,,impl,,,0,ThreadSafe,,,,,,"NodeIdGenerator use an HashMap without any synchronization. In multithreaded environment it may lead to concurrent access to the HashMap which, in some case, triggers an infinite loop inside the HashMap implementation => the parsing of the pig request never ends so the job will never run + it eats a threads in the JVM that just keep burning CPU cycles.

since it is hard to reproduce and it is not a deterministic bug it is impossible to provide a viable unit tests for it to demonstrate both the bug and the fix.","any computer with multiple cores, does not always happen since it is not a determinitic bug. The more threads and real cpu, the more chances it will happen.",,,,,,,,,,,,,,,,,,,01/Jun/15 14:24;rcatherinot;PIG-4581.patch;https://issues.apache.org/jira/secure/attachment/12736568/PIG-4581.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-04-25 21:04:22.669,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Apr 25 21:04:22 UTC 2016,,,,,,,0|i2fg2n:,9223372036854775807,ThreadSafe version of NodeIdGenerator,,,,,,,,,01/Jun/15 14:24;rcatherinot;Isolate HashMap accesses by using ThreadLocal + added javadoc to state the class is thread-safe. No unit tests because it's not possible to reproduce it on demand.,"01/Jun/15 16:48;rcatherinot;cannot be easily reproduced, need a real multi-core machine within a JVM with lots of threads using parsing several queries at the same time to have a chance to encounter the bug.",25/Apr/16 21:04;rohini;+1. Verified that TestMRCompiler and TestTezCompiler are good. Committed to trunk. Thanks for the contribution Remi.,,,,,,,,,,,,,,,,,,,,,,,,,
Fix TestTezAutoParallelism.testSkewedJoinIncreaseParallelism test failure,PIG-4580,12834126,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,01/Jun/15 02:22,07/Jun/15 03:47,14/Mar/19 03:08,01/Jun/15 17:43,,,,,,0.15.0,,tez,,,0,,,,,,,"If a vertex need to increase parallelism dynamically, it cannot has additional broadcast input. In the test case, join vertex requires a broadcast edge to acquire sample file (used in IsFirstReduceOfKeyTez), and the join vertex use PartitionerDefinedVertexManager which can increase parallelism dynamically. If that happens, we will see exception:
{code}
Fetch failed:java.lang.IndexOutOfBoundsException
	at java.nio.Buffer.checkIndex(Buffer.java:532)
	at java.nio.ByteBufferAsLongBufferB.get(ByteBufferAsLongBufferB.java:115)
	at org.apache.tez.runtime.library.common.sort.impl.TezSpillRecord.getIndex(TezSpillRecord.java:101)
	at org.apache.tez.runtime.library.common.shuffle.Fetcher.getTezIndexRecord(Fetcher.java:596)
	at org.apache.tez.runtime.library.common.shuffle.Fetcher.doLocalDiskFetch(Fetcher.java:537)
	at org.apache.tez.runtime.library.common.shuffle.Fetcher.setupLocalDiskFetch(Fetcher.java:518)
	at org.apache.tez.runtime.library.common.shuffle.Fetcher.callInternal(Fetcher.java:191)
	at org.apache.tez.runtime.library.common.shuffle.Fetcher.callInternal(Fetcher.java:70)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

The fix disable dynamic parallelism if there is a broadcast edge (including sample file in skewed outer join, scalar followed by skewed join/order by). Also in inner skewed join, the sample file is not needed in join vertex, so it is not necessarily to broadcast it thus disable dynamic parallelism.",,,,,,,,,,,,,,,,,,,,01/Jun/15 02:23;daijy;PIG-4580-1.patch;https://issues.apache.org/jira/secure/attachment/12736486/PIG-4580-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-06-01 16:49:53.991,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Jun 01 17:43:33 UTC 2015,,,,,,,0|i2ffs7:,9223372036854775807,,,,,,,,,,"01/Jun/15 16:49;rohini;So with this patch only in case of skewed outer join, the sample is also broadcast to the skewed join vertex. After 0.15, we need to revisit the fix for PIG-4377 to see if we can do it in a better way so that we still do dynamic parallelism for skewed outer join as well.

+1. ",01/Jun/15 17:43;daijy;Patch committed to 0.15 branch and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Nightly test HCat_DDL_2 fails with TDE ON,PIG-4576,12833516,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nmaheshwari,nmaheshwari,nmaheshwari,28/May/15 18:25,08/Jun/16 20:48,14/Mar/19 03:08,17/May/16 19:56,0.15.0,,,,,0.16.0,,,,,0,,,,,,,"Nightly test HCat_DDL_2 fails with TDE ON

{code}
2015-05-28 08:57:27,095 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /home/hrt_qa/.pigbootup not found
2015-05-28 08:57:27,814 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://ip-172-31-7-30.ec2.internal:8020
2015-05-28 08:57:32,145 [main] INFO  org.apache.pig.tools.grunt.GruntParser - Going to run hcat command: drop table if exists pig_hcat_ddl_1;
WARNING: Use ""yarn jar"" to launch YARN applications.
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Unable to drop table because it is in an encryption zone and trash is enabled.  Use PURGE option to skip trash.)
2015-05-28 08:57:47,608 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2997: Encountered IOException. sql command 'drop table if exists pig_hcat_ddl_1;' failed. 
Details at logfile: /grid/0/hadoopqe/pig/test/e2e/pig/testdist/out/pigtest/hrtqa/hrtqa-1432803362-hcat.conf-HCatDDL/HCatDDL_2.log
2015-05-28 08:57:47,725 [main] INFO  org.apache.pig.Main - Pig script completed in 24 seconds and 528 milliseconds (24528 ms)
ERROR TestDriver::runTestGroup at : 729 Failed to run test HCatDDL_2 <Failed running ./out/pigtest/hrtqa/hrtqa-1432803362-hcat.conf-HCatDDL/HCatDDL_2.pig
{code}",,,,,,,,,,,,,,,,,,,,28/May/15 18:30;nmaheshwari;PIG-4576.patch;https://issues.apache.org/jira/secure/attachment/12735931/PIG-4576.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-05-17 19:56:44.721,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 17 19:56:44 UTC 2016,,,,,,,0|i2fc33:,9223372036854775807,,,,,,,,,,28/May/15 18:32;nmaheshwari;[~daijy] - Please find the attached patch,17/May/16 19:56;daijy;Patch committed to trunk. Thanks Namit!,,,,,,,,,,,,,,,,,,,,,,,,,,
TestPigRunner.testGetHadoopCounters fail on Windows,PIG-4571,12832588,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,26/May/15 00:17,07/Jun/15 03:47,14/Mar/19 03:08,26/May/15 22:49,,,,,,0.15.0,,,,,0,,,,,,,"Error message:
junit.framework.AssertionFailedError: expected:<30> but was:<35>
	at org.apache.pig.test.TestPigRunner.testGetHadoopCounters(TestPigRunner.java:917)",,,,,,,,,,,,,,,,,,,,26/May/15 00:19;daijy;PIG-4571-1.patch;https://issues.apache.org/jira/secure/attachment/12735228/PIG-4571-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-26 02:00:35.402,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 26 22:49:10 UTC 2015,,,,,,,0|i2f6fr:,9223372036854775807,,,,,,,,,,26/May/15 00:19;daijy;The generated input file size is different on Windows since it uses two characters for new line. Attach fix.,26/May/15 02:00;rohini;+1,26/May/15 22:49;daijy;Patch committed to both 0.15 branch and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
Fix e2e test Rank_1 failure,PIG-4569,12832060,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,22/May/15 06:25,07/Jun/15 03:47,14/Mar/19 03:08,23/May/15 06:47,,,,,,0.15.0,,,,,0,,,,,,,"It fails on Hadoop 1, but the issue could exist on Hadoop 2 as well.

Error message:
{code}
org.apache.pig.backend.executionengine.ExecException: ERROR 0: Error while executing ForEach at [C[6,4]]
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:325)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:279)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:274)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.RuntimeException: Unable to read counter pig.counters.counter_6929257954538808410_2
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank.getRankCounterOffset(PORank.java:185)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank.addRank(PORank.java:160)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank.getNextTuple(PORank.java:141)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:252)
	... 11 more
{code}",,,,,,,,,,,,,,,,,,,,22/May/15 06:26;daijy;PIG-4569-1.patch;https://issues.apache.org/jira/secure/attachment/12734751/PIG-4569-1.patch,22/May/15 22:45;daijy;PIG-4569-2.patch;https://issues.apache.org/jira/secure/attachment/12734955/PIG-4569-2.patch,23/May/15 00:21;daijy;PIG-4569-3.patch;https://issues.apache.org/jira/secure/attachment/12734971/PIG-4569-3.patch,23/May/15 06:22;rohini;PIG-4569-4.patch;https://issues.apache.org/jira/secure/attachment/12734989/PIG-4569-4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2015-05-22 22:22:42.363,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat May 23 06:47:08 UTC 2015,,,,,,,0|i2f38n:,9223372036854775807,,,,,,,,,,"22/May/15 06:26;daijy;Attach patch. JobConf.getNumMapTasks() cannot get the right #maps, it returns the default value 2. Change to use counters to get #maps.",22/May/15 22:22;rohini;Counter.TOTAL_LAUNCHED_MAPS includes failed/killed and speculative attempts. We need to find some other way.,22/May/15 22:45;daijy;Seems the only option is from taskreport. Attach PIG-4569-2.patch.,"22/May/15 23:25;rohini;Taskreports are huge and can cause OOM. Have seen that many times. So a previously running rank job might fail and require more memory on the pig client or oozie launcher with this patch.

I see that we switched to getNumReduceTasks because of https://issues.apache.org/jira/browse/PIG-4392 .  Can we go back to the code before PIG-4392 on iterating on the counters, but if the counterList size is less than job.getJob().getNumReduceTasks() in case of !isRowNumber, then initialize 0 for the missing counters ? This way can avoid getting the number of map tasks, but still have PIG-4392 fixed.","23/May/15 00:21;daijy;Totally agree. I don't like task report approach either. Though we can refactor the code not pull task report twice, but task report is not reliable and the execution code should not rely on it. 

We can go back to PIG-4392 and find an alternative fix. Attach PIG-4569-3.patch which rollback part of PIG-4392 and fix differently.","23/May/15 06:22;rohini;[~daijy],
    Attached a patch that solves the issue in JobControlCompiler itself by creating the counter offsets correctly skipping the empty tasks.  Instead of using counterSize index as the taskId in the counterPair keys, it uses the actual taskIds in ascending order. Makes it easier to understand.","23/May/15 06:40;daijy;+1, that's even better.",23/May/15 06:47;rohini;Committed to branch-0.15 and trunk. Thanks Daniel.,,,,,,,,,,,,,,,,,,,,
Pig can deadlock in POPartialAgg if there is a bag,PIG-4564,12831551,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,20/May/15 23:40,07/Jun/15 03:47,14/Mar/19 03:08,23/May/15 23:00,,,,,,0.15.0,,,,,0,,,,,,,"PIG-3979 made spill of POPartialAgg synchronous, but if there is a bag in the data being grouped then it can deadlock when that tries to register the Spillable with SpillableMemoryManager.",,,,,,,,,,,,PIG-4012,,,,,,,,23/May/15 19:25;rohini;PIG-4564-1.patch;https://issues.apache.org/jira/secure/attachment/12735031/PIG-4564-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-23 22:55:45.692,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat May 23 23:00:48 UTC 2015,,,,,,,0|i2f0nj:,9223372036854775807,,,,,,,,,,"20/May/15 23:41;rohini;{code}
""TezChild"" daemon prio=10 tid=0xaa753400 nid=0x5a58 waiting for monitor entry [0xaabfe000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.pig.impl.util.SpillableMemoryManager.registerSpillable(SpillableMemoryManager.java:324)
        - waiting to lock <0xb49c1e08> (a java.util.LinkedList)
        at org.apache.pig.data.DefaultAbstractBag.markSpillableIfNecessary(DefaultAbstractBag.java:129)
        at org.apache.pig.data.DefaultAbstractBag.add(DefaultAbstractBag.java:118)
        at org.apache.hive.hcatalog.pig.PigHCatUtil.transformToBag(PigHCatUtil.java:491)
        at org.apache.hive.hcatalog.pig.PigHCatUtil.extractPigObject(PigHCatUtil.java:403)
        at org.apache.hive.hcatalog.pig.PigHCatUtil.transformToTuple(PigHCatUtil.java:458)
        at org.apache.hive.hcatalog.pig.PigHCatUtil.transformToTuple(PigHCatUtil.java:376)
        at org.apache.hive.hcatalog.pig.HCatBaseLoader.getNext(HCatBaseLoader.java:64)
        at org.apache.hive.hcatalog.pig.HCatLoader.getNext(HCatLoader.java:59)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:204)
        at org.apache.tez.mapreduce.lib.MRReaderMapReduce.next(MRReaderMapReduce.java:116)
        at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POSimpleTezLoad.getNextTuple(POSimpleTezLoad.java:106)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getNextTuple(POSplit.java:210)
        at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:317)
        at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:196)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)

""Service Thread"" daemon prio=10 tid=0xacbfbc00 nid=0x59ff waiting on condition [0xac7fc000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg.spill(POPartialAgg.java:602)
        - locked <0xd89759d8> (a java.lang.Object)
        at org.apache.pig.impl.util.SpillableMemoryManager.handleNotification(SpillableMemoryManager.java:274)
        - locked <0xb49c1e08> (a java.util.LinkedList)
        at sun.management.NotificationEmitterSupport.sendNotification(NotificationEmitterSupport.java:156)
        at sun.management.MemoryImpl.createNotification(MemoryImpl.java:168)
        at sun.management.MemoryPoolImpl$PoolSensor.triggerAction(MemoryPoolImpl.java:301)
        at sun.management.Sensor.trigger(Sensor.java:137)
{code}","23/May/15 19:25;rohini;This patch also incorporates fix for PIG-4012. 

The problem is we need to have POPartialAgg to be synchronous spill so that its memory consumption is reduced before we invoke System.gc() in SpillableMemoryManager. But for POPartialAgg to do the aggregation, it needs to read the next record which can block on registerSpillable if there is a bag in the record causing deadlock. This patch allows registering to the spillables if doing POPartialAgg. For other normal spills it just blocks as before. registerSpillable is still synchronized on spillables to avoid adding to spillables when the spillablesSR is being constructed. But it mainly relies on blockRegisterOnSpill variable to block from adding to spillables when the actual spill is in progress.",23/May/15 19:26;rohini;Patch in review board - https://reviews.apache.org/r/34635/,23/May/15 22:55;daijy;+1,23/May/15 23:00;daijy;Patch committed to both 0.15 branch and trunk. Thanks Rohini!,,,,,,,,,,,,,,,,,,,,,,,
Typo in DataType.toDateTime,PIG-4562,12831260,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,20/May/15 02:43,07/Jun/15 03:47,14/Mar/19 03:08,20/May/15 21:12,,,,,,0.15.0,,,,,0,,,,,,,Which make the error message misleading.,,,,,,,,,,,,,,,,,,,,20/May/15 02:45;daijy;PIG-4562-1.patch;https://issues.apache.org/jira/secure/attachment/12734046/PIG-4562-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-20 21:05:57.674,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed May 20 21:12:50 UTC 2015,,,,,,,0|i2eyvj:,9223372036854775807,,,,,,,,,,20/May/15 21:05;rohini;+1,20/May/15 21:12;daijy;Patch committed to both 0.15 branch and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix several new tez e2e test failures,PIG-4559,12830569,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/May/15 04:24,07/Jun/15 03:47,14/Mar/19 03:08,19/May/15 23:49,,,,,,0.15.0,,tez,,,0,,,,,,,"Make the tests stable. The test results are good, however, MR and Tez produce different results due to the rounding/sort order differences. HiveUDF_7 is one exception, it fails due to a stock counter is removed. Change the way to test MapredContext in HiveUDF.",,,,,,,,,,,,,,,,,,,,18/May/15 04:26;daijy;PIG-4559-1.patch;https://issues.apache.org/jira/secure/attachment/12733441/PIG-4559-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-19 22:02:09.924,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 19 23:49:32 UTC 2015,,,,,,,0|i2eunj:,9223372036854775807,,,,,,,,,,19/May/15 22:02;rohini;+1,19/May/15 23:49;daijy;Patch committed to both branch 0.15 and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Local mode is broken in some case by PIG-4247,PIG-4556,12830390,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/May/15 07:55,07/Jun/15 03:47,14/Mar/19 03:08,21/May/15 22:37,,,,,,0.15.0,,,,,0,,,,,,,"HExecutionEngine.getS3Conf is wrong. It should only return s3 config. Currently it will return all the properties, including *-site.xml even in local mode. In one particular case, mapred-site.xml contains ""mapreduce.application.framework.path"", this will going to the local mode config, thus we see the exception:
{code}
Message: java.io.FileNotFoundException: File file:/hdp/apps/2.2.0.0-2041/mapreduce/mapreduce.tar.gz does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:111)
	at org.apache.hadoop.fs.AbstractFileSystem.resolvePath(AbstractFileSystem.java:460)
	at org.apache.hadoop.fs.FilterFs.resolvePath(FilterFs.java:157)
	at org.apache.hadoop.fs.FileContext$24.next(FileContext.java:2137)
	at org.apache.hadoop.fs.FileContext$24.next(FileContext.java:2133)
	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
	at org.apache.hadoop.fs.FileContext.resolve(FileContext.java:2133)
	at org.apache.hadoop.fs.FileContext.resolvePath(FileContext.java:595)
	at org.apache.hadoop.mapreduce.JobSubmitter.addMRFrameworkToDistributedCache(JobSubmitter.java:753)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:435)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)
	at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.submit(ControlledJob.java:335)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.pig.backend.hadoop23.PigJobControl.submit(PigJobControl.java:128)
	at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:194)
	at java.lang.Thread.run(Thread.java:745)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:276)
{code}",,,,,,,,,,,,,,,,,,,,16/May/15 07:57;daijy;PIG-4556-1.patch;https://issues.apache.org/jira/secure/attachment/12733312/PIG-4556-1.patch,21/May/15 22:11;daijy;PIG-4556-2.patch;https://issues.apache.org/jira/secure/attachment/12734659/PIG-4556-2.patch,21/May/15 22:34;daijy;PIG-4556-3.patch;https://issues.apache.org/jira/secure/attachment/12734664/PIG-4556-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2015-05-19 21:54:25.463,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu May 21 22:37:55 UTC 2015,,,,,,,0|i2etkn:,9223372036854775807,,,,,,,,,,19/May/15 21:54;rohini;+1,19/May/15 23:42;daijy;Patch committed to both 0.15 branch and trunk. Thanks Rohini for review!,"21/May/15 22:11;daijy;This fix break MR mode. In particular, Pig fails on secure cluster. Need one more fix.","21/May/15 22:24;rohini;Do we need the s3 conf in non local mode? It can be simplified to below.

{code}
JobConf jc;
        if (!this.pigContext.getExecType().isLocal()) {
            jc = getExecConf(properties);

            // Trick to .......
        } else {
            // If we are running in local mode we dont read the hadoop conf file
            .........
            properties.setProperty(ALTERNATIVE_FILE_SYSTEM_LOCATION, ""file:///"");

            jc = getLocalConf();
            // Pick s3 properties from core-site.xml and add to local conf
            JobConf s3Jc = getS3Conf();
            ConfigurationUtil.mergeConf(jc, s3Jc);
        }
{code}","21/May/15 22:29;thejas;I believe s3 is used in non-local modes as well.
Change looks good to me. (for my reference, since it took me few mins to figure out, the real change is in order in which params are passed to  ConfigurationUtil.mergeConf).
","21/May/15 22:32;rohini;bq. I believe s3 is used in non-local modes as well.
   Yes. But the additional code getS3Conf() is only needed in local mode as core-site.xml is not loaded there. Non-local modes load core-site.xml and so will have the s3 properties from that already.","21/May/15 22:34;daijy;Look at PIG-4247, it only need to add s3 properties in local mode, so yes, we can simplify. Attach another patch as Rohini suggested.",21/May/15 22:36;rohini;+1,21/May/15 22:37;daijy;PIG-4556-3.patch committed to both 0.15 branch and trunk. Thanks Thejas/Rohini for quick review!,,,,,,,,,,,,,,,,,,,
Fix Unit test TestJobSubmissionMR,PIG-4545,12828932,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,11/May/15 19:35,07/Jun/15 03:47,14/Mar/19 03:08,13/May/15 22:45,,,,,,0.15.0,,,,,0,,,,,,,"It seems to be an HBase bug (HBASE-10029). I'd like to fix it by upgrading HBase to a newer version, which anyway we'd like to do.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 13 22:45:19 UTC 2015,,,,,,,0|i2eks7:,9223372036854775807,,,,,,,,,,13/May/15 22:45;daijy;PIG-4544 is fixed. Close the issue as fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestEvalPipelineLocal.testRankWithEmptyReduce fail on Hadoop 1,PIG-4543,12828922,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,11/May/15 19:23,07/Jun/15 03:48,14/Mar/19 03:08,13/May/15 22:47,,,,,,0.15.0,,,,,0,,,,,,,Hadoop 1 local mode does not support multiple reducers. So testRankWithEmptyReduce should test with Minicluster.,,,,,,,,,,,,,,,,,,,,11/May/15 19:24;daijy;PIG-4543-1.patch;https://issues.apache.org/jira/secure/attachment/12731983/PIG-4543-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-13 22:30:33.341,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed May 13 22:47:23 UTC 2015,,,,,,,0|i2ekpz:,9223372036854775807,,,,,,,,,,13/May/15 22:30;rohini;+1,13/May/15 22:47;daijy;Patch committed to both branch 0.15 and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Skewed full outer join does not return records if any relation is empty. Outer join does not return any record if left relation is empty,PIG-4541,12828473,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,dghosal,dghosal,08/May/15 20:00,07/Jun/15 03:48,14/Mar/19 03:08,26/May/15 22:54,0.14.0,,,,,0.15.0,,build,,08/May/15 00:00,0,,,,,,,"Test1:
Perform full join on two relation with left relation being blank and right containing records
empty_relation = FILTER a_relation by (join_column=='eliminate everything');
Test_output = JOIN empty_relation by (join_column) FULL , non_empty_relation by (join_column);
Result : Zero records returned.

Test2:
Perform full join on two relation with left relation being blank and right containing records using skewed
Test_output = JOIN empty_relation by (join_column) FULL , non_empty_relation by (join_column) using ‘skewed’;

Result : Zero records returned.

Test3:
Perform full join on two relation with left relation being blank and right containing records using parallel
Test_output = JOIN empty_relation by (join_column) FULL , non_empty_relation by (join_column) PARALLEL 10;

Result : Zero records returned.

Test4:
Perform full join on two relation with left relation being non empty  and right not containing records using parallel
Test_output = JOIN , non_empty_relation by (join_column) FULL , empty_relation by (join_column) PARALLEL 10;

Result : valid records  returned.

Observation:
1) If the either relation is blank , skewed full outer join does not return anything
2) If the non empty relation is kept on left, everything works except skewed
3) FULL OUTER will only work if the left relation is not empty
4) Skewed will only work if both relation is non empty.
",HDP 2.2.4,,,,,,,,,,,,,,,,,,,26/May/15 21:25;daijy;PIG-4541-1.patch;https://issues.apache.org/jira/secure/attachment/12735413/PIG-4541-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-26 21:18:03.346,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 26 22:54:40 UTC 2015,,,,,,,0|i2ehz3:,9223372036854775807,,,,,,,,,,"26/May/15 21:18;daijy;[~dghosal], I tried Pig 0.14.0 with some of your queries, I cannot reproduce the issue:
1.txt:
1,2
{code}
a = load '1.txt' using PigStorage(',') as (a0, a1);
b = filter a by a0==100;
c = join b by a0 full, a by a0 using 'skewed' parallel 10;
dump c;
{code}
{code}
a = load '1.txt' using PigStorage(',') as (a0, a1);
b = filter a by a0==100;
c = join b by a0 full, a by a0 parallel 2;
dump c;
{code}

Both cases I get the right result.

There is a regression in trunk I will fix shortly.","26/May/15 21:55;dghosal;I rechecked again and things are not working here.
The pig version is 
0.14.0.2.2.4.2-2 (rexported) and the distro is HDP.","26/May/15 22:08;daijy;[~dghosal], I can see the skewed join issue when left relation is empty, but I cannot reproduce the regular join issue in any version. Do you have a specific script?",26/May/15 22:41;rohini;+1,"26/May/15 22:54;daijy;Patch committed to both 0.15 branch and trunk. Thanks Rohini for review!

To make it simple, restrict the ticket to skew join issue. [~dghosal], if you do find some issue in regular join, please open a seperate ticket. ",,,,,,,,,,,,,,,,,,,,,,,
Pig script fail with CNF in follow up MR job,PIG-4538,12827834,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,06/May/15 21:58,07/Jun/15 03:48,14/Mar/19 03:08,08/May/15 17:08,,,,,,0.15.0,,tez,,,0,,,,,,,"The following script fail:
{code}
in1 = LOAD 'data.txt' AS (ident:chararray);
in2 = LOAD 'data.txt' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',') AS (ident:chararray);
in3 = LOAD 'data.txt';
joined = JOIN in1 BY ident LEFT OUTER, in2 BY ident;
crossed = CROSS joined, in2;
DUMP crossed;
{code}
The second MR job fail with message:

ERROR 1070: Could not resolve org.apache.pig.piggybank.storage.CSVExcelStorage using imports: \[, java.lang., org.apache.pig.builtin., org.apache.pig.impl.builtin.\]

This is a regression from Pig 0.13, possibly introduced by PIG-3591.",,,,,,,,,,,,,,,,,,,,08/May/15 05:09;daijy;PIG-4538-1.patch;https://issues.apache.org/jira/secure/attachment/12731362/PIG-4538-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-08 14:46:04.163,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri May 08 17:08:14 UTC 2015,,,,,,,0|i2ee3j:,9223372036854775807,,,,,,,,,,"07/May/15 22:50;daijy;This is actually introduced by FetchOptimizer (PIG-3642). Even if we are using MR engine eventually, Pig still went through FetchOptimizer. And FetchOptimizer:179 set the whole physical plan to all operators, so the LoadFunc get serialized into the conf. And since Pig doesn't ship LoadFunc to backend in the followup MR job, Pig fails with CNF exception.",08/May/15 14:46;rohini;+1,08/May/15 17:08;daijy;Patch committed to both 0.15 branch and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix unit test failure introduced by TEZ-2392: TestCollectedGroup, TestLimitVariable, TestMapSideCogroup, etc",PIG-4537,12827832,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,06/May/15 21:52,07/Jun/15 03:48,14/Mar/19 03:08,07/May/15 23:00,,,,,,0.15.0,,tez,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,07/May/15 04:24;daijy;PIG-4537-1.patch;https://issues.apache.org/jira/secure/attachment/12731068/PIG-4537-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-07 19:10:52.222,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu May 07 23:00:17 UTC 2015,,,,,,,0|i2ee33:,9223372036854775807,,,,,,,,,,"06/May/15 21:53;daijy;Error message:
{code}
Failure while running task:org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing (Name: a: Split - scope-80 Operator Key: scope-80): org.apache.pig.backend.executionengine.ExecException: ERROR 0: java.io.IOException: Please check if you are invoking next() even after it returned false. For usage, please refer to KeyValueReader javadocs
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:316)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getStreamCloseResult(POSplit.java:277)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getNextTuple(POSplit.java:213)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:319)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:196)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:331)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: java.io.IOException: Please check if you are invoking next() even after it returned false. For usage, please refer to KeyValueReader javadocs
	at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POSimpleTezLoad.getNextTuple(POSimpleTezLoad.java:122)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
	... 17 more
Caused by: java.io.IOException: Please check if you are invoking next() even after it returned false. For usage, please refer to KeyValueReader javadocs
	at org.apache.tez.runtime.library.api.KeyValueReader.hasCompletedProcessing(KeyValueReader.java:77)
	at org.apache.tez.mapreduce.lib.MRReaderMapReduce.next(MRReaderMapReduce.java:124)
	at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POSimpleTezLoad.getNextTuple(POSimpleTezLoad.java:106)
	... 18 more
{code}",07/May/15 04:24;daijy;Add check to avoid pull a finished input.,07/May/15 19:10;rohini;+1,07/May/15 23:00;daijy;Patch committed to both 0.15 branch and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
Document error: Pig does support concatenated gz file,PIG-4533,12827607,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,xhudik,xhudik,06/May/15 10:16,08/Jun/16 20:48,14/Mar/19 03:08,16/Jun/15 21:29,,,,,,0.16.0,,documentation,parser,,0,,,,,,,"Documentation (since 0.11.1 at least) says :
http://pig.apache.org/docs/r0.11.1/func.html#handling-compression
_""Note: PigStorage and TextLoader correctly read compressed files as long as they are NOT CONCATENATED FILES generated in this manner: ...""_

This is not true for gz, since
* I did a test - concatenated&compress some files and processed them. The same was done with the raw files (no compression). The results were identical

Jira's https://issues.apache.org/jira/i#browse/HADOOP-4012 and 
https://issues.apache.org/jira/i#browse/HADOOP-6835 says the concatenation problems were fixed in Hadoop 0.22, Hadoop 0.20 respectively for both: bz2 and gz. That said Hadoop (1 and 2) are supporting concatenated archives bz2, gz  already. 

Pig deals with bz2 on its own(historical reasons) which is redundant to hadoop-common. Therefore this activity should be left to hadoop-common (there is no need to be handled by Pig anymore). 

The documentation needs to be updated accordingly (concatenated gz, bz2 are processing correctly with hadoop-commons). Also a remark that tar.gz and tar.bz2 are not supported would be helpful since many users are using tar.gz or tar.bz2 automatically.



",,,,,,,,,,,,,PIG-3251,,,,,,,12/Jun/15 18:06;daijy;PIG-4533-1.patch;https://issues.apache.org/jira/secure/attachment/12739296/PIG-4533-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-06-12 18:06:41.062,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Oct 29 21:53:01 UTC 2015,,,,,,,0|i2ecpr:,9223372036854775807,,,,,,,,,,"19/May/15 08:31;xhudik;Based on the discussion in the mailing list http://mail-archives.apache.org/mod_mbox/pig-user/201505.mbox/browser (click on: ""concatenated gzip/bzip in Pig 0.11 and higher"") seems that gzip files are ok since they are processed by hadoop which can handle concatenated gzip (https://issues.apache.org/jira/browse/HADOOP-6835). 
The problem is with bzip2 which are processed by Pig. Since Hadoop is able to process also concatenated bzip2 files as well (https://issues.apache.org/jira/browse/HADOOP-4012 or http://stackoverflow.com/a/25888475/1408096 ) I'd let Hadoop do this job instead of Pig. If so the documentation (http://pig.apache.org/docs/r0.11.1/func.html#handling-compression) needs to be updated accordingly",12/Jun/15 18:06;daijy;Fix document first. Will open a separate Jira for switching bzip processing.,12/Jun/15 19:38;rohini;+1,"12/Jun/15 19:48;rohini;[~xhudik],
   I am confused. In PIG-4599 you mention that tar.gz does not work. But in this one you say it does. Am I missing something?","13/Jun/15 14:26;xhudik;My understanding is that these two issues are not related. This one is about concatenated archives(gz/bz2), while that one is about a possible bug in tar.gz ",14/Jun/15 20:10;rohini;That should be fine. Could you correct the description of this jira then? It seems to imply that tar.gz works with Pig while it does not. I assume you meant to say concatenated .gz works and not tar.gz.,"15/Jun/15 09:31;xhudik;done, now it should make more sense","16/Jun/15 21:29;daijy;Patch committed to trunk. Thanks Tomas, Rohini!","29/Oct/15 21:51;rohini;[~daijy],
  Pig has been supporting concatenated bzip2 files for a while. [~knoguchi] pointed out that we have a test for it - TestBzip.testBZ2Concatenation. So we need to correct the documentation for bzip as well. ",29/Oct/15 21:53;rohini;Ignore my previous comment. Missed the @Test (expected=IOException.class) on top of that test.,,,,,,,,,,,,,,,,,,
Pig Documentation contains typo for AvroStorage,PIG-4532,12827577,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,fredericschmaljohann,fredericschmaljohann,fredericschmaljohann,06/May/15 07:35,07/Jun/15 03:47,14/Mar/19 03:08,11/May/15 21:50,0.14.0,,,,,0.15.0,,documentation,,,0,,,,,,,"The Pig documentation about AvroStorage (http://pig.apache.org/docs/r0.14.0/func.html#AvroStorage) contains a typo. It says:
{quote}
Avrostorage(['schema|record name'], ['options'])
{quote}

where it should be
{quote}
AvroStorage(['schema|record name'], ['options'])
{quote}",,,,,,,,,,,,,,,,,,,,06/May/15 07:40;fredericschmaljohann;PIG-4532.patch;https://issues.apache.org/jira/secure/attachment/12730763/PIG-4532.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-11 21:50:34.002,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon May 11 21:50:34 UTC 2015,,,,,,,0|i2ecif:,9223372036854775807,,,,,,,,,,11/May/15 21:50;daijy;Patch committed to both 0.15 branch and trunk. Thanks Frederic!,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflow in TestMultiQueryLocal running under hadoop20,PIG-4530,12826756,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nielsbasjes,nielsbasjes,nielsbasjes,02/May/15 21:09,07/Jun/15 03:48,14/Mar/19 03:08,04/May/15 18:17,,,,,,0.15.0,,,,,0,,,,,,,"If you run the command {{ant -Djavac.args=""-Xlint -Xmaxwarns 1000"" clean jar test-commit}}
as indicated here https://cwiki.apache.org/confluence/display/PIG/HowToContribute

The result is a failed build caused by Stack overflow due to an unintended infinite recursion in setting up the MiniCluster.

It looks like the cause is that  {{shims/test/hadoop20/org/apache/pig/test/MiniCluster.java}} does not override the {{static public Launcher getLauncher()}} method.
",,,,,,,,,,,,,,,,,,,,02/May/15 21:18;nielsbasjes;PIG-4530-2015-05-02-2309.patch;https://issues.apache.org/jira/secure/attachment/12729993/PIG-4530-2015-05-02-2309.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-04 18:17:05.049,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 04 18:17:05 UTC 2015,,,,,,,0|i2e7tz:,9223372036854775807,,,,,,,,,,"02/May/15 21:11;nielsbasjes;Now I get ""BUILD SUCCESSFUL""",04/May/15 18:17;rohini;+1. Committed to branch-0.15 and trunk. Thanks Niels for fixing this.,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig on tez hit counter limit imposed by MR,PIG-4529,12826650,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,01/May/15 21:32,10/Dec/15 23:56,14/Mar/19 03:08,01/May/15 22:43,,,,,,0.15.0,,tez,,,0,,,,,,,"See the following stack in test:
{code}
org.apache.hadoop.mapreduce.counters.LimitExceededException: Too many counters: 131 max=130
	at org.apache.hadoop.mapreduce.counters.Limits.checkCounters(Limits.java:101)
	at org.apache.hadoop.mapreduce.counters.Limits.incrCounters(Limits.java:108)
	at org.apache.hadoop.mapreduce.counters.AbstractCounterGroup.addCounter(AbstractCounterGroup.java:78)
	at org.apache.hadoop.mapreduce.counters.AbstractCounterGroup.addCounterImpl(AbstractCounterGroup.java:95)
	at org.apache.hadoop.mapreduce.counters.AbstractCounterGroup.addCounter(AbstractCounterGroup.java:87)
	at org.apache.hadoop.mapred.Counters$Group.addCounter(Counters.java:354)
	at org.apache.pig.tools.pigstats.tez.TezDAGStats.covertToHadoopCounters(TezDAGStats.java:276)
	at org.apache.pig.tools.pigstats.tez.TezDAGStats.accumulateStats(TezDAGStats.java:192)
	at org.apache.pig.tools.pigstats.tez.TezPigScriptStats.accumulateStats(TezPigScriptStats.java:181)
	at org.apache.pig.backend.hadoop.executionengine.tez.TezJob.run(TezJob.java:198)
	at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher$1.run(TezLauncher.java:174)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}
Actually I've set tez.counters.max to a much higher value, TezLauncher should not be limited by MR limits.",,,,,,,,,,,,,,,,,,,,01/May/15 21:33;daijy;PIG-4529-1.patch;https://issues.apache.org/jira/secure/attachment/12729852/PIG-4529-1.patch,01/May/15 22:37;daijy;PIG-4529-2.patch;https://issues.apache.org/jira/secure/attachment/12729880/PIG-4529-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-05-01 22:14:46.589,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Dec 10 23:56:04 UTC 2015,,,,,,,0|i2e76n:,9223372036854775807,,,,,,,,,,01/May/15 22:14;rohini;Math.max compares same keys,01/May/15 22:30;daijy;That's a mistake. Fixed.,01/May/15 22:35;rohini;org.apache.hadoop.mapreduce.MRJobConfig.COUNTER_GROUPS_MAX_KEY and org.apache.tez.mapreduce.hadoop.MRJobConfig.COUNTER_GROUPS_MAX_KEY are both equal to mapreduce.job.counters.groups.max. Shouldn't it be TezConfiguration.TEZ_COUNTERS_MAX?,"01/May/15 22:38;daijy;Sure, updated.",01/May/15 22:40;rohini;+1,01/May/15 22:43;daijy;Patch committed to both branch 0.15 and trunk. Thanks Rohini for review!,10/Dec/15 23:56;daijy;This won't work in Oozie since Limits is imposed already in Oozie job. Pig cannot change it. Actually the MR counter code is not needed. Open PIG-4760 to fix that.,,,,,,,,,,,,,,,,,,,,,
Fix a typo in src/docs/src/documentation/content/xdocs/basic.xml,PIG-4528,12826649,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,namusyaka,namusyaka,namusyaka,01/May/15 21:31,07/Jun/15 03:47,14/Mar/19 03:08,11/May/15 21:55,,,,,,0.15.0,,documentation,,,0,,,,,,,"Hi there.
I've just found a minor typo in documentation, so I'd like to send a patch.
Thanks.",,,,,,,,,,,,,,,,,,,,01/May/15 21:33;namusyaka;fix-a-typo.patch;https://issues.apache.org/jira/secure/attachment/12729853/fix-a-typo.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-11 21:55:39.977,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,Reviewed,,,,Mon May 11 21:55:39 UTC 2015,,,Patch Available,,,,0|i2e76f:,9223372036854775807,,,,,,,,,,11/May/15 21:55;daijy;Patch committed to both 0.15 branch and trunk. Thanks Namusyaka!,,,,,,,,,,,,,,,,,,,,,,,,,,,
NON-ASCII Characters in Javadoc break 'ant docs',PIG-4527,12826323,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nielsbasjes,nielsbasjes,nielsbasjes,30/Apr/15 14:57,07/Jun/15 03:48,14/Mar/19 03:08,30/Apr/15 20:58,,,,,,0.15.0,,,,,0,,,,,,,"Found while testing PIG-4526
{code}contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/IsInt.java {code}
contains
{code}- * Note this function checks for Integer range <E2><88><92>2,147,483,648 to 2,147,483,647.
{code}",,,,,,,,,,,,,,,,,,,,30/Apr/15 14:58;nielsbasjes;PIG-4527-2015-04-30-1657.patch;https://issues.apache.org/jira/secure/attachment/12729502/PIG-4527-2015-04-30-1657.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-04-30 20:58:47.64,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Apr 30 20:58:47 UTC 2015,,,,,,,0|i2e56n:,9223372036854775807,,,,,,,,,,30/Apr/15 20:58;daijy;Patch committed to both 0.15 branch and trunk. Thanks Niels!,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Minicluster unit tests broken by TEZ-2333,PIG-4524,12826166,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,30/Apr/15 00:17,07/Jun/15 03:48,14/Mar/19 03:08,01/May/15 22:28,,,,,,0.15.0,,tez,,,0,,,,,,,"Filed TEZ-2366. Before Tez fix the issue, we can fix it temporarily in Pig side first.",,,,,,,,,,,,,,,,,,,,30/Apr/15 00:18;daijy;PIG-4524-1.patch;https://issues.apache.org/jira/secure/attachment/12729356/PIG-4524-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-01 22:04:49.786,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri May 01 22:28:10 UTC 2015,,,,,,,0|i2e48f:,9223372036854775807,,,,,,,,,,01/May/15 22:04;rohini;+1,01/May/15 22:28;daijy;Patch committed to both 0.15 branch and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Tez engine should use tez config rather than mr config whenever possible,PIG-4523,12826053,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,29/Apr/15 18:47,07/Jun/15 03:48,14/Mar/19 03:08,13/May/15 23:34,,,,,,0.15.0,,tez,,,0,,,,,,,"Currently, Pig on tez use memory/heap setting from MR not tez, but respect other tez settings. In the environment which MR and tez settings are not aligned, this could be a problem. For a specific scenario, user increase both tez both memory settings and io.sort.mb to tune tez performance, but MR settings retains. Pig on Tez get increased io.sort.mb but same vertex memory, end up with OOM exception.

I think tez engine shall respect Tez settings first, unless user really want to use mr settings for backward compatibility. It can be:
1. a flag which turn on the mr settings for tez
2. If the mr memory settings are from Pig script not the config file",,,,,,,,,,,,,,,,,,,,13/May/15 17:01;daijy;PIG-4523-1.patch;https://issues.apache.org/jira/secure/attachment/12732610/PIG-4523-1.patch,13/May/15 23:28;daijy;PIG-4523-2.patch;https://issues.apache.org/jira/secure/attachment/12732705/PIG-4523-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-05-13 23:08:00.067,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed May 13 23:34:15 UTC 2015,,,,,,,0|i2e3jz:,9223372036854775807,,,,,,,,,,13/May/15 23:08;rohini;TEZ_AM_LAUNCH_CMD_OPTS  should be  TEZ_TASK_LAUNCH_CMD_OPTS,13/May/15 23:28;daijy;Good catch. Corrected.,13/May/15 23:29;rohini;+1,13/May/15 23:34;daijy;Patch committed to branch 0.15 and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
Correct link to Contribute page,PIG-4519,12823842,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,gliptak,gliptak,gliptak,24/Apr/15 18:45,07/Jun/15 03:47,14/Mar/19 03:08,24/Apr/15 19:58,,,,,,0.15.0,,documentation,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Apr/15 18:48;gliptak;PIG-4519.patch;https://issues.apache.org/jira/secure/attachment/12727987/PIG-4519.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-04-24 19:58:40.77,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Apr 24 19:58:40 UTC 2015,,,,,,,0|i2dqdr:,9223372036854775807,Correct link to Contribute page,,,,,,,,,24/Apr/15 19:58;daijy;Patch committed to trunk. Thanks Gabor!,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig trunk compilation is broken - VertexManagerPluginContext.reconfigureVertex change,PIG-4514,12823250,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,23/Apr/15 00:41,07/Jun/15 03:48,14/Mar/19 03:08,23/Apr/15 18:23,0.15.0,,,,,0.15.0,,,,,0,,,,,,,"
{code}
src/org/apache/pig/backend/hadoop/executionengine/tez/runtime/PigGraceShuffleVertexManager.java:173: error: exception TezException is never thrown in body of corresponding try statement
    [javac]             } catch (TezException e) {
    [javac]               ^
{code}
",,,,,,,,,,,,,,,,,,,,23/Apr/15 00:42;thejas;PIG-4514.1.patch;https://issues.apache.org/jira/secure/attachment/12727485/PIG-4514.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-04-23 18:23:00.118,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Apr 23 18:23:00 UTC 2015,,,,,,,0|i2dmsn:,9223372036854775807,,,,,,,,,,23/Apr/15 18:23;daijy;Patch committed to trunk. Thanks Thejas!,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Pig on Tez] Unassigned applications not killed on shutdown,PIG-4509,12821333,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,15/Apr/15 22:00,07/Jun/15 03:48,14/Mar/19 03:08,16/Apr/15 14:51,0.14.0,,,,,0.15.0,,,,,0,,,,,,, tezclient.stop() should be called when tezClient.waitTillReady() is interrupted on shutdown.,,,,,,,,,,,,,,,,,,,,15/Apr/15 23:52;rohini;PIG-4509-1.patch;https://issues.apache.org/jira/secure/attachment/12725739/PIG-4509-1.patch,16/Apr/15 21:51;rohini;PIG-4509-FixCompileError.patch;https://issues.apache.org/jira/secure/attachment/12725992/PIG-4509-FixCompileError.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-04-16 07:42:36.27,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Apr 16 22:00:58 UTC 2015,,,,,,,0|i2db9z:,9223372036854775807,,,,,,,,,,16/Apr/15 07:42;daijy;+1,16/Apr/15 14:51;rohini;Committed to trunk. Thanks for the review Daniel.,"16/Apr/15 17:16;thejas;[~rohini] This results in a compilation failure. 

{code}
src/org/apache/pig/backend/hadoop/executionengine/tez/TezSessionManager.java:105: error: unreported exception Throwable; must be caught or declared to be thrown
    [javac]             throw e;
    [javac]             ^
{code}",16/Apr/15 17:36;rohini;I see the problem with the code. But it is odd that it builds fine for me with ant clean jar -Dhadoopversion=23 and eclipse also does not show any errors. Let me put up a patch to fix that.,"16/Apr/15 17:51;rohini;[~knoguchi] tried and it compiles fine for him too. I have jdk8 in eclipse and jdk7 for ant. But it failed for me when I compiled with jdk6 on a Linux box. [~thejas], what is the jdk version you are using and Mac or Linux?","16/Apr/15 21:44;thejas;It builds fine on my mac as well with jdk 7. However, it is failing with jdk7 in our internal build environment as well (probably linux).

The fact that it passes in some setups is certainly very strange. I think we should still go ahead and fix this, as far as i know this should result in a syntax error.
","16/Apr/15 21:51;rohini;Attached fix. 

Was surprised that I did not get a compilation error. Just wanted to be sure that I don't have a screwed up build environment. Interesting to know that MAC jdk does not fail compilation for this.","16/Apr/15 21:57;thejas;+1
The change looks good to me.
Thanks Rohini!

",16/Apr/15 22:00;rohini;Committed to trunk. Thanks for reporting this and the review [~thejas].,,,,,,,,,,,,,,,,,,,
[Pig on Tez] PigProcessor check for commit only on MROutput,PIG-4508,12821215,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,15/Apr/15 15:57,07/Jun/15 03:48,14/Mar/19 03:08,16/Apr/15 14:49,,,,,,0.15.0,,,,,0,,,,,,,,,,,,,,,,,,,,TEZ-2317,,,,,,,15/Apr/15 16:00;rohini;PIG-4508-1.patch;https://issues.apache.org/jira/secure/attachment/12725612/PIG-4508-1.patch,15/Apr/15 19:11;rohini;PIG-4508-2.patch;https://issues.apache.org/jira/secure/attachment/12725652/PIG-4508-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-04-16 07:58:48.586,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Apr 16 14:49:34 UTC 2015,,,,,,,0|i2dajz:,9223372036854775807,,,,,,,,,,16/Apr/15 07:58;daijy;+1,16/Apr/15 14:49;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
binstorage fails to write biginteger,PIG-4506,12820772,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ssavvides,ssavvides,ssavvides,14/Apr/15 09:54,07/Jun/15 03:48,14/Mar/19 03:08,19/May/15 23:47,,,,,,0.15.0,,data,impl,,0,,,,,,,"When trying to store a biginteger using binstorage the following error is issued (The error might manifest elsewhere too):
java.lang.RuntimeException: Unexpected data type -1 found in stream

This is caused by a bug in the writeDatum method of the DataReaderWriter.java class. When writeDatum is called with a BigInteger as a argument, the BigInteger is converted to a byte[] and the writeDatum method is recursively called on the byte[]. writeDatum cannon handle byte[] objects but instead expects DataByteArray objects.

Suggested fix - wrap byte[] to DataByteArray:
change this line:
_writeDatum(out, ((BigInteger)val).toByteArray());_
to this:
_writeDatum(out, new DataByteArray(((BigInteger)val).toByteArray()));_",,300,300,,0%,300,300,,,,,,,,,,,,,29/Apr/15 19:28;daijy;PIG-4506-1.patch;https://issues.apache.org/jira/secure/attachment/12729269/PIG-4506-1.patch,18/May/15 00:58;daijy;PIG-4506-2.patch;https://issues.apache.org/jira/secure/attachment/12733426/PIG-4506-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-04-29 19:28:30.883,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue May 19 23:47:04 UTC 2015,,,,,,,0|i2d7xz:,9223372036854775807,,,,,,,,,,29/Apr/15 19:28;daijy;The fix looks good. Adding a test case.,"13/May/15 21:55;rohini;  The fix might be easy, but it is bad as it wastes one byte per biginteger and bigdecimal.

{code}
case DataType.BIGINTEGER:
                out.writeByte(DataType.BIGINTEGER);
                writeDatum(out, ((BigInteger)val).toByteArray());
                break;
case DataType.BIGDECIMAL:
                out.writeByte(DataType.BIGDECIMAL);
                writeDatum(out, ((BigDecimal)val).toString());
{code}

Instead of actually writing DataType.BIGINTEGER + length of bytearray + byte array, the code is writing DataType.BIGINTEGER + DATATYPE.BYTEARRAY + length of bytearray + byte array. In case of BigDecimal it is DataType.BIGDECIMAL + DataType.CHARARRAY/DataType.BIGCHARARRAY + short/int length of bytearray + byte array.   We should get rid of the DATATYPE.BYTEARRAY  and DataType.CHARARRAY/DataType.BIGCHARARRAY. Though it makes for easy coding it is inefficient.

Can we extract out the writing and reading of bytearray and its length and reuse that code instead of calling writeDatum(databytearray).  For BigDecimal, we can always do out.writeShort(length) as the length of the length of the BigDecimal String should not be > 65535. ","13/May/15 22:04;rohini;For the test, could you just add biginteger and bigdecimal to TestEvalPipeline2.testBinStorageByteArrayCastsSimple instead of adding a new one in TestStore. That is a more logical fit place for the test as it already tests other datatypes for BinStorage there.",18/May/15 00:58;daijy;Update the patch according to Rohini's comments.,19/May/15 21:58;rohini;+1,"19/May/15 23:47;daijy;Patch committed to both 0.15 branch and trunk. Thanks Savvas, Rohini!",,,,,,,,,,,,,,,,,,,,,,
[Pig on Tez] Auto adjust AM memory can hit OOM with 3.5GXmx,PIG-4505,12820664,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,13/Apr/15 23:15,07/Jun/15 03:47,14/Mar/19 03:08,16/Apr/15 14:47,,,,,,0.15.0,,,,,0,,,,,,,"  If the cluster is big and can launch many containers, Tez can try to create more threads to talk to the nodes and that can cause java.lang.OutOfMemoryError: unable to create new native thread as there is only 512MB of native memory (4GB limit due to 32-bit jvm) to create threads. ",,,,,,,,,,,,,,,,,,,,16/Apr/15 00:32;rohini;PIG-4505-1.patch;https://issues.apache.org/jira/secure/attachment/12725745/PIG-4505-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-04-16 07:31:25.202,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Apr 16 14:47:14 UTC 2015,,,,,,,0|i2d7av:,9223372036854775807,,,,,,,,,,"16/Apr/15 00:32;rohini;AM creates one thread for each nodemanager it needs to talk to and can be high in a big cluster with more nodes when number of tasks is high requiring more native memory (thread stack size). More threads will not only use up pmem, but also will allocate lot more vmem most of which mostly go unused. In fact jdk8 and 64bit jvms use lot more virtual address space. Since pmem-vmem ratio is usually being turned off or set to a higher value in Hadoop 2.x when jdk8 is being used, virtual address space usage exceeding container size or pmem-vmem ratio is not an issue for 64 bit jvms. But with 32-bit jvm (irrespective of value of pmem-vmem ratio), jvm cannot go beyond 4G limit for allocating virtual address space and that is where it hits the problem. So need to keep the heap size small giving room for it to not hit 4G limit.",16/Apr/15 07:31;daijy;+1,16/Apr/15 14:47;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,
[Pig on Tez] NPE in UnionOptimizer with multiple levels of union,PIG-4503,12819646,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,09/Apr/15 12:10,07/Jun/15 03:48,14/Mar/19 03:08,17/Apr/15 16:19,0.14.0,,,,,0.15.0,,,,,0,,,,,,,"   When there are multiple levels of union, with the last union having both store and an output (group by,join, etc) following it then there is a NPE in 

{code}
if (succ.isVertexGroup()
                        && succ.getVertexGroupInfo().getOutput()
                                .equals(succOp.getOperatorKey().toString())) {
                    succOpVertexGroup = succ;
                    break;
                }
{code}

It should check for getOutput() != null as it now has a store vertexgroup",,,,,,,,,,,,,,,,,,,,16/Apr/15 14:43;rohini;PIG-4503-1.patch;https://issues.apache.org/jira/secure/attachment/12725875/PIG-4503-1.patch,14/May/15 19:13;rohini;PIG-4503-additionalfix.patch;https://issues.apache.org/jira/secure/attachment/12732930/PIG-4503-additionalfix.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-04-17 04:37:12.555,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu May 14 19:17:40 UTC 2015,,,,,,,0|i2d113:,9223372036854775807,,,,,,,,,,"16/Apr/15 14:43;rohini;Changes done:
   - Reversed the equal checks in UnionOptimizer to avoid NPE
   - Added an additional store to TestTezCompiler.testUnionUnion to simulate this issue.
   - testUnionScalar is a new test. Not related to the issue, but included it as I wrote the test while initially debugging the big script in this issue and is a good one to have to cover the missing case of union and scalar. ",17/Apr/15 04:37;daijy;+1,17/Apr/15 16:19;rohini;Committed to trunk. Thanks for the review Daniel.,"14/May/15 19:13;rohini;   Noticed that the vertex group members are not set right for a store vertex group in case of multiple levels of union and store (TestTezCompiler.testUnionUnionStore). Currently there are no issues as TezDAGBuilder only accesses the first member from the vertex group, but put up an additional patch that fixes it for correctness in case all members are accessed sometime in the future. ",14/May/15 19:14;daijy;+1,14/May/15 19:17;rohini;Committed PIG-4503-additionalfix.patch to branch-0.15 and trunk. Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,
E2E tests build fail with udfs compile,PIG-4502,12819425,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nmaheshwari,nmaheshwari,nmaheshwari,08/Apr/15 19:01,07/Jun/15 03:47,14/Mar/19 03:08,08/Apr/15 19:05,0.15.0,,,,,0.15.0,,,,,0,,,,,,,"While running e2e pig system tests for Dal, the system tests fail with udfs compile
{code}
2015-04-08 05:31:29,757|beaver.machine|INFO|9471|140392870434560|MainThread|RUNNING: /grid/0/hadoopqe/tools/apache-ant-1.8.4/bin/ant  -Dpig.dir=""/usr/hdp/current/pig-client"" -Dtests.suites=""tests/nightly.conf"" -Dhadoop.common.lib.dir=""/usr/hdp/current/hadoop-client/lib"" -DPH_BENCHMARK_CACHE_PATH=""/grid/0/hadoopqe/pig-benchmarks/benchmarks"" -Dfork.factor.conf.file=""3"" -Djython.jar=""/usr/hdp/current/pig-client/lib/jython-standalone-2.5.3.jar"" -Dpig.jar.dir=""/usr/hdp/current/pig-client"" -Dhadoop.common.dir=""/usr/hdp/current/hadoop-client"" -Dhcat.bin=""/usr/hdp/current/hive-webhcat/bin/hcat"" -Dharness.cluster.conf=""/etc/hadoop/conf"" -Dhadoop.mapreduce.dir=""/usr/hdp/current/hadoop-mapreduce-client"" -Djruby.jar=""/usr/hdp/current/pig-client/lib/jruby-complete-1.6.7.jar"" -Dharness.conf.file=""conf/tez_rpm.conf"" -Dhadoopversion=""23"" -Dharness.hadoop.home=""/usr/hdp/current/hadoop-client"" -Dharness.old.pig=""/usr/hdp/current/pig-client"" -Dfork.factor.group=""3"" -Dharness.cluster.bin=""/usr/hdp/current/hadoop-client/bin/hadoop"" test-tez
2015-04-08 05:31:30,013|beaver.machine|INFO|9471|140392870434560|MainThread|Buildfile: /grid/0/hadoopqe/pig/test/e2e/pig/build.xml
2015-04-08 05:31:30,768|beaver.machine|INFO|9471|140392870434560|MainThread|
2015-04-08 05:31:30,768|beaver.machine|INFO|9471|140392870434560|MainThread|test-tez:
2015-04-08 05:31:30,901|beaver.machine|INFO|9471|140392870434560|MainThread|
2015-04-08 05:31:30,902|beaver.machine|INFO|9471|140392870434560|MainThread|property-check:
2015-04-08 05:31:30,915|beaver.machine|INFO|9471|140392870434560|MainThread|
2015-04-08 05:31:30,915|beaver.machine|INFO|9471|140392870434560|MainThread|udfs:
2015-04-08 05:31:30,995|beaver.machine|INFO|9471|140392870434560|MainThread|
2015-04-08 05:31:30,996|beaver.machine|INFO|9471|140392870434560|MainThread|init:
2015-04-08 05:31:31,000|beaver.machine|INFO|9471|140392870434560|MainThread|[mkdir] Created dir: /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/build
2015-04-08 05:31:31,001|beaver.machine|INFO|9471|140392870434560|MainThread|
2015-04-08 05:31:31,001|beaver.machine|INFO|9471|140392870434560|MainThread|udf-compile:
2015-04-08 05:31:31,021|beaver.machine|INFO|9471|140392870434560|MainThread|[echo] *** Compiling UDFs ***
2015-04-08 05:31:31,035|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/build.xml:57: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
2015-04-08 05:31:31,122|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] Compiling 44 source files to /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/build
2015-04-08 05:31:32,789|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:21: error: package org.apache.hadoop.hive.ql.exec does not exist
2015-04-08 05:31:32,790|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] import org.apache.hadoop.hive.ql.exec.Description;
2015-04-08 05:31:32,791|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                                      ^
2015-04-08 05:31:32,792|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:22: error: package org.apache.hadoop.hive.ql.exec does not exist
2015-04-08 05:31:32,792|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] import org.apache.hadoop.hive.ql.exec.MapredContext;
2015-04-08 05:31:32,793|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                                      ^
2015-04-08 05:31:32,794|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:23: error: package org.apache.hadoop.hive.ql.exec does not exist
2015-04-08 05:31:32,794|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
2015-04-08 05:31:32,795|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                                      ^
2015-04-08 05:31:32,796|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:24: error: package org.apache.hadoop.hive.ql.metadata does not exist
2015-04-08 05:31:32,796|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] import org.apache.hadoop.hive.ql.metadata.HiveException;
2015-04-08 05:31:32,797|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                                          ^
2015-04-08 05:31:32,799|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:25: error: package org.apache.hadoop.hive.ql.udf.generic does not exist
2015-04-08 05:31:32,799|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
2015-04-08 05:31:32,800|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                                             ^
2015-04-08 05:31:32,801|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:26: error: package org.apache.hadoop.hive.serde2.objectinspector does not exist
2015-04-08 05:31:32,802|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
2015-04-08 05:31:32,803|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                                                     ^
2015-04-08 05:31:32,804|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:27: error: package org.apache.hadoop.hive.serde2.objectinspector.primitive does not exist
2015-04-08 05:31:32,805|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
2015-04-08 05:31:32,805|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                                                               ^
2015-04-08 05:31:32,861|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:35: error: cannot find symbol
2015-04-08 05:31:32,862|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] public class DummyContextUDF extends GenericUDF {
2015-04-08 05:31:32,862|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                                      ^
2015-04-08 05:31:32,862|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   symbol: class GenericUDF
2015-04-08 05:31:32,863|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:33: error: cannot find symbol
2015-04-08 05:31:32,863|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] @Description(name = ""dummycontextudf"",
2015-04-08 05:31:32,864|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]  ^
2015-04-08 05:31:32,864|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   symbol: class Description
2015-04-08 05:31:32,865|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:37: error: cannot find symbol
2015-04-08 05:31:32,865|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   private MapredContext context;
2015-04-08 05:31:32,866|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]           ^
2015-04-08 05:31:32,866|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   symbol:   class MapredContext
2015-04-08 05:31:32,866|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   location: class DummyContextUDF
2015-04-08 05:31:32,867|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:40: error: cannot find symbol
2015-04-08 05:31:32,867|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
2015-04-08 05:31:32,867|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                                     ^
2015-04-08 05:31:32,868|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   symbol:   class ObjectInspector
2015-04-08 05:31:32,868|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   location: class DummyContextUDF
2015-04-08 05:31:32,869|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:40: error: cannot find symbol
2015-04-08 05:31:32,869|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
2015-04-08 05:31:32,869|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]          ^
2015-04-08 05:31:32,870|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   symbol:   class ObjectInspector
2015-04-08 05:31:32,870|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   location: class DummyContextUDF
2015-04-08 05:31:32,871|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:40: error: cannot find symbol
2015-04-08 05:31:32,872|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
2015-04-08 05:31:32,873|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                                                                         ^
2015-04-08 05:31:32,873|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   symbol:   class UDFArgumentException
2015-04-08 05:31:32,874|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   location: class DummyContextUDF
2015-04-08 05:31:32,875|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:44: error: cannot find symbol
2015-04-08 05:31:32,876|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   public Object evaluate(DeferredObject[] arguments) throws HiveException {
2015-04-08 05:31:32,876|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                          ^
2015-04-08 05:31:32,877|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   symbol:   class DeferredObject
2015-04-08 05:31:32,878|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   location: class DummyContextUDF
2015-04-08 05:31:32,880|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:44: error: cannot find symbol
2015-04-08 05:31:32,880|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   public Object evaluate(DeferredObject[] arguments) throws HiveException {
2015-04-08 05:31:32,880|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                                                             ^
2015-04-08 05:31:32,881|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   symbol:   class HiveException
2015-04-08 05:31:32,882|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   location: class DummyContextUDF
2015-04-08 05:31:32,887|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:61: error: cannot find symbol
2015-04-08 05:31:32,887|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]     public void configure(MapredContext context) {
2015-04-08 05:31:32,888|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]                           ^
2015-04-08 05:31:32,888|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   symbol:   class MapredContext
2015-04-08 05:31:32,889|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   location: class DummyContextUDF
2015-04-08 05:31:33,124|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /usr/hdp/current/hadoop-client/hadoop-common-2.7.0.2.3.0.0-1517.jar(org/apache/hadoop/fs/FileSystem.class): warning: Cannot find annotation method 'value()' in type 'LimitedPrivate': class file for org.apache.hadoop.classification.InterfaceAudience not found
2015-04-08 05:31:33,125|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /usr/hdp/current/hadoop-client/hadoop-common-2.7.0.2.3.0.0-1517.jar(org/apache/hadoop/fs/FileSystem.class): warning: Cannot find annotation method 'value()' in type 'LimitedPrivate'
2015-04-08 05:31:33,126|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /usr/hdp/current/hadoop-client/hadoop-common-2.7.0.2.3.0.0-1517.jar(org/apache/hadoop/fs/FileSystem.class): warning: Cannot find annotation method 'value()' in type 'LimitedPrivate'
2015-04-08 05:31:33,127|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /usr/hdp/current/hadoop-client/hadoop-common-2.7.0.2.3.0.0-1517.jar(org/apache/hadoop/fs/FileSystem.class): warning: Cannot find annotation method 'value()' in type 'LimitedPrivate'
2015-04-08 05:31:33,128|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /usr/hdp/current/hadoop-client/hadoop-common-2.7.0.2.3.0.0-1517.jar(org/apache/hadoop/fs/FSDataInputStream.class): warning: Cannot find annotation method 'value()' in type 'LimitedPrivate'
2015-04-08 05:31:33,128|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /usr/hdp/current/hadoop-client/hadoop-common-2.7.0.2.3.0.0-1517.jar(org/apache/hadoop/fs/Path.class): warning: Cannot find annotation method 'value()' in type 'LimitedPrivate'
2015-04-08 05:31:33,312|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:41: error: cannot find symbol
2015-04-08 05:31:33,312|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]     return PrimitiveObjectInspectorFactory.writableLongObjectInspector;
2015-04-08 05:31:33,312|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]            ^
2015-04-08 05:31:33,313|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   symbol:   variable PrimitiveObjectInspectorFactory
2015-04-08 05:31:33,313|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   location: class DummyContextUDF
2015-04-08 05:31:33,322|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] /grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/org/apache/pig/test/udf/evalfunc/DummyContextUDF.java:60: error: method does not override or implement a method from a supertype
2015-04-08 05:31:33,323|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   @Override
2015-04-08 05:31:33,323|beaver.machine|INFO|9471|140392870434560|MainThread|[javac]   ^
2015-04-08 05:31:33,575|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] Note: Some input files use or override a deprecated API.
2015-04-08 05:31:33,575|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] Note: Recompile with -Xlint:deprecation for details.
2015-04-08 05:31:33,576|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] Note: Some input files use unchecked or unsafe operations.
2015-04-08 05:31:33,576|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] Note: Recompile with -Xlint:unchecked for details.
2015-04-08 05:31:33,576|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] 18 errors
2015-04-08 05:31:33,576|beaver.machine|INFO|9471|140392870434560|MainThread|[javac] 6 warnings
2015-04-08 05:31:33,590|beaver.machine|INFO|9471|140392870434560|MainThread|
2015-04-08 05:31:33,590|beaver.machine|INFO|9471|140392870434560|MainThread|BUILD FAILED
2015-04-08 05:31:33,591|beaver.machine|INFO|9471|140392870434560|MainThread|/grid/0/hadoopqe/pig/test/e2e/pig/build.xml:337: The following error occurred while executing this line:
2015-04-08 05:31:33,591|beaver.machine|INFO|9471|140392870434560|MainThread|/grid/0/hadoopqe/pig/test/e2e/pig/build.xml:156: The following error occurred while executing this line:
2015-04-08 05:31:33,591|beaver.machine|INFO|9471|140392870434560|MainThread|/grid/0/hadoopqe/pig/test/e2e/pig/udfs/java/build.xml:57: Compile failed; see the compiler error output for details.
2015-04-08 05:31:33,591|beaver.machine|INFO|9471|140392870434560|MainThread|
2015-04-08 05:31:33,592|beaver.machine|INFO|9471|140392870434560|MainThread|Total time: 3 seconds
{code}",,,,,,,,,,,,,,,,,,,,08/Apr/15 19:02;nmaheshwari;PIG-4502.patch;https://issues.apache.org/jira/secure/attachment/12723986/PIG-4502.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-04-08 19:05:23.451,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Apr 08 19:05:23 UTC 2015,,,,,,,0|i2czon:,9223372036854775807,,,,,,,,,,08/Apr/15 19:02;nmaheshwari;Please find the attached patch. This can resolve the issue.,08/Apr/15 19:05;daijy;+1. Patch committed to trunk. Thanks Namit!,,,,,,,,,,,,,,,,,,,,,,,,,,
mvn-build miss tez classes in pig-h2.jar,PIG-4499,12788258,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,04/Apr/15 06:58,07/Jun/15 03:48,14/Mar/19 03:08,05/Apr/15 03:27,,,,,,0.15.0,,build,,,0,,,,,,,"Do mvn-build, pig-h2.jar does not contains tez classes. This result the published pig-h2.jar wrong.",,,,,,,,,,,,,,,,,,,,04/Apr/15 06:58;daijy;PIG-4499-1.patch;https://issues.apache.org/jira/secure/attachment/12709400/PIG-4499-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-04-04 18:22:05.761,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sun Apr 05 03:27:52 UTC 2015,,,,,,,0|i27s9z:,9223372036854775807,,,,,,,,,,04/Apr/15 18:22;rohini;Why should pig-h2.jar contain tez classes? Shouldn't it be just a pom dependency so that people can download them separate?,"05/Apr/15 00:50;daijy;Those are Pig tez classes, not tez jars. All Pig classes inside tez package is missing.",05/Apr/15 00:58;rohini;Thanks for the clarification. jar-h12 does takes care of that. +1,05/Apr/15 03:27;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage in Piggbank does not handle bad records and fails,PIG-4498,12788245,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,viraj,viraj,viraj,04/Apr/15 01:52,07/Jun/15 03:48,14/Mar/19 03:08,06/Apr/15 23:56,0.11.1,0.12.0,0.13.1,0.14.1,,0.15.0,,piggybank,,,0,piggybank,,,,,,"The following Pig script fails if the records within the file are corrupted.

{code}
DEFINE AvroLoader org.apache.pig.piggybank.storage.avro.AvroStorage('ignore_bad_files');
 DH_RAW = LOAD 'bad_data*' USING AvroLoader();
STORE DH_RAW INTO 'output' USING PigStorage();
{code}

Here is the stack trace:
{quote}
java.lang.ArrayIndexOutOfBoundsException: -49 at org.apache.pig.piggybank.storage.avro.PigAvroRecordReader.getCurrentValue(PigAvroRecordReader.java:230) at org.apache.pig.piggybank.storage.avro.AvroStorage.getNext(AvroStorage.java:407) ... 12 more Caused by: java.lang.ArrayIndexOutOfBoundsException: -49 at org.apache.avro.io.parsing.Symbol$Alternative.getSymbol(Symbol.java:364) at org.apache.avro.io.ResolvingDecoder.doAction(ResolvingDecoder.java:229) at org.apache.avro.io.parsing.Parser.advance(Parser.java:88) at org.apache.avro.io.ResolvingDecoder.readIndex(ResolvingDecoder.java:206) at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:152) at org.apache.pig.piggybank.storage.avro.PigAvroDatumReader.readMap(PigAvroDatumReader.java:89) at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:151) at org.apache.pig.piggybank.storage.avro.PigAvroDatumReader.readRecord(PigAvroDatumReader.java:73) at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:148) at org.apache.pig.piggybank.storage.avro.PigAvroDatumReader.readRecord(PigAvroDatumReader.java:73) at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:148) at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:139) at org.apache.avro.file.DataFileStream.next(DataFileStream.java:233) at org.apache.avro.file.DataFileStream.next(DataFileStream.java:220) at org.apache.pig.piggybank.storage.avro.PigAvroRecordReader.getCurrentValue(PigAvroRecordReader.java:198) ..
{quote}",,,,,,,,,,,,,,,,,,,,06/Apr/15 21:43;viraj;PIG-4498.patch;https://issues.apache.org/jira/secure/attachment/12723452/PIG-4498.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-04-06 23:56:44.582,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Apr 06 23:56:44 UTC 2015,,,,,,,0|i27s73:,9223372036854775807,,,,,,,,,,06/Apr/15 23:56;rohini;Committed to trunk. Thanks for the patch Viraj.,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Pig on Tez] NPE for null scalar,PIG-4497,12787970,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,03/Apr/15 11:58,07/Jun/15 03:47,14/Mar/19 03:08,03/Apr/15 20:19,0.14.0,,,,,0.15.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,03/Apr/15 11:59;rohini;PIG-4497-1.patch;https://issues.apache.org/jira/secure/attachment/12709216/PIG-4497-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-04-03 17:15:34.967,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Apr 03 20:19:36 UTC 2015,,,,,,,0|i27r2v:,9223372036854775807,,,,,,,,,,03/Apr/15 17:15;daijy;+1,03/Apr/15 20:19;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig's htrace version conflicts with that of hadoop 2.6.0,PIG-4494,12787573,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,zjffdu,zjffdu,02/Apr/15 05:52,07/Jun/15 03:48,14/Mar/19 03:08,28/Apr/15 22:45,,,,,,0.15.0,,,,,0,,,,,,,"Pig is using htrace-2.0 while hadoop-2.6.0 is using 3.0.4. htrace 3.04 has refactor its package layout which is total incompatible with htrace 2.0
{code}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/htrace/Trace
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:214)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:752)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1988)
	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1118)
	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1114)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1400)
	at org.apache.pig.backend.hadoop.datastorage.HDataStorage.isContainer(HDataStorage.java:200)
	at org.apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:128)
	at org.apache.pig.backend.hadoop.datastorage.HDataStorage.asElement(HDataStorage.java:138)
	at org.apache.pig.parser.QueryParserUtils.getCurrentDir(QueryParserUtils.java:91)
	at org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:910)
	at org.apache.pig.parser.LogicalPlanGenerator.load_clause(LogicalPlanGenerator.java:3569)
	at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1626)
	at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1103)
	at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:561)
	at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:422)
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:191)
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1735)
	at org.apache.pig.PigServer$Graph.access$0(PigServer.java:1722)
	at org.apache.pig.PigServer.parseAndBuild(PigServer.java:387)
	at org.apache.pig.PigServer.executeBatch(PigServer.java:412)
	at org.apache.pig.PigServer.executeBatch(PigServer.java:398)
	at pig.PigExample.main(PigExample.java:59)
Caused by: java.lang.ClassNotFoundException: org.htrace.Trace
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 33 more

{code}",,,,,,,,,,,,,,,,,,,,03/Apr/15 23:14;daijy;PIG-4494-1.patch;https://issues.apache.org/jira/secure/attachment/12709339/PIG-4494-1.patch,05/Apr/15 03:26;daijy;PIG-4494-2.patch;https://issues.apache.org/jira/secure/attachment/12709450/PIG-4494-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-04-03 23:14:00.346,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Apr 28 22:45:54 UTC 2015,,,,,,,0|i27onr:,9223372036854775807,,,,,,,,,,03/Apr/15 23:14;daijy;Attach patch. Pig miss a few of other jars for Hadoop 2.6.0. Include in this patch as well.,"04/Apr/15 18:27;rohini;<dependency org=""org.htrace"" name=""htrace-core"" rev=""3.0.4"" conf=""hadoop23->master""/>
<dependency org=""org.cloudera.htrace"" name=""htrace-core"" rev=""2.00"" conf=""hadoop23->master"">
<dependency org=""org.apache.htrace"" name=""htrace-core"" rev=""${htrace.version}"" conf=""hbase95->master"">

If the first one is required by hadoop 2.6 and third one by hbase, what is the second one required for? If it is not used, we should remove it even though all three package names are different and will not conflict.  ","05/Apr/15 03:26;daijy;Yes, only 2 htrace is needed. Update patch.",05/Apr/15 23:26;rohini;+1,28/Apr/15 22:45;daijy;Patch committed to both 0.15 branch and trunk.. Thanks Jeff reporting and Rohini reviewing!,,,,,,,,,,,,,,,,,,,,,,,
Pig on Tez gives wrong results if Union is followed by Split,PIG-4493,12787524,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,01/Apr/15 23:40,07/Jun/15 03:48,14/Mar/19 03:08,03/Apr/15 20:16,0.14.0,,,,,0.15.0,,,,,0,,,,,,,POSplit subplans were not being cloned when the plans were cloned for pushing up in UnionOptimizer. This caused input plans being detached for one of them and they processed and produced 0 records.,,,,,,,,,,,,,,,,,,,,02/Apr/15 00:05;rohini;PIG-4493-1.patch;https://issues.apache.org/jira/secure/attachment/12708862/PIG-4493-1.patch,02/Apr/15 20:11;rohini;PIG-4493-2.patch;https://issues.apache.org/jira/secure/attachment/12709054/PIG-4493-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-04-03 17:19:13.424,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Apr 03 20:16:27 UTC 2015,,,,,,,0|i27oaf:,9223372036854775807,,,,,,,,,,02/Apr/15 20:11;rohini;Updated patch to fix TestTezCompiler test failure. Also added clone to POFilter just in case even though there were no test failures as same operators in expression were being reused in the different plans. ,03/Apr/15 17:19;daijy;+1,03/Apr/15 20:16;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming Python Bytearray Bugs,PIG-4491,12787412,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jeremykarn,jeremykarn,jeremykarn,01/Apr/15 17:38,07/Jun/15 03:48,14/Mar/19 03:08,03/Apr/15 18:51,0.12.1,0.13.1,0.14.1,,,0.15.0,,,,,0,,,,,,,"While using a streaming python udf that returned a byte array we hit a couple of bugs.

The first was: 

{panel}
org.apache.pig.impl.streaming.StreamingUDFException: LINE : UnicodeDecodeError: 'ascii' codec can't decode byte 0xe6 in position 0: ordinal not in range(128)
{panel}

and the second (after fixing the first) was a null pointer exception.

I traced the problem to two issues:

1. In the python controller the output from the udf was being logged as a unicode string which can fail for bytearrays.

2. Newlines in the data at the start of a response weren't being handled properly on the Java side.

I'm attaching a patch w/ tests that fix these two issues.",,,,,,,,,,,,,,,,,,,,01/Apr/15 17:40;jeremykarn;PIG-4491.patch;https://issues.apache.org/jira/secure/attachment/12708749/PIG-4491.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-04-03 18:51:37.571,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Apr 03 21:15:59 UTC 2015,,,,,,,0|i27nlr:,9223372036854775807,,,,,,,,,,03/Apr/15 18:51;daijy;+1. Patch committed to trunk. Thanks Jeremy!,"03/Apr/15 19:49;rohini;{code}
String data = ""\na|_\n"";
....
Assert.assertEquals(tf.newTuple(""\na""), t);
{code}

Shouldn't the output be two rows? i.e an empty row  tf.newTuple("""")  and then tf.newTuple(""a|_"") ","03/Apr/15 20:18;jeremykarn;The streaming UDF code uses a multi-character delimiter for newlines to handle cases where the data itself contains newlines.  

In this case a newline is indicated by ""|_\n"".  And so the actual data is a single tuple of ""\na"".","03/Apr/15 21:15;rohini;Got it. Looked more closely at the OutputHandler code. It seems to rely on the delimiter always ending in ""\n"". If it was anything else it would break. We will have to fix that sometime. ",,,,,,,,,,,,,,,,,,,,,,,,
MIN/MAX builtin UDFs return wrong results when accumulating for strings,PIG-4490,12787294,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,opensource@xplenty.com,opensource@xplenty.com,opensource@xplenty.com,01/Apr/15 08:35,07/Jun/15 03:47,14/Mar/19 03:08,22/May/15 22:14,0.12.0,0.13.0,0.14.0,,,0.15.0,,internal-udfs,,,1,,,,,,,"When using MIN/MAX UDFs with strings in a job that uses the accumulator interface the results are wrong - The UDF won't return the correct MIN/MAX value.

this is caused by a reverse 'GreaterThan/SmallerThan"" (<>) sign in the accumulate() function of both StringMin/StringMax UDFs.",,,,,,,,,,,,,,,,,,,,22/May/15 21:47;rohini;PIG-4490-1.patch;https://issues.apache.org/jira/secure/attachment/12734945/PIG-4490-1.patch,29/Apr/15 12:40;opensource@xplenty.com;fix-min-max-test.patch;https://issues.apache.org/jira/secure/attachment/12729156/fix-min-max-test.patch,01/Apr/15 08:37;opensource@xplenty.com;fix-min-max.patch;https://issues.apache.org/jira/secure/attachment/12708642/fix-min-max.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2015-04-02 17:33:28.735,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri May 22 22:14:13 UTC 2015,,,Patch Available,,,,0|i27mxb:,9223372036854775807,,,,,,,,,,02/Apr/15 17:33;rohini;This is bad. Can you please add one testcase in TestBuiltin including both two UDFs for the case of accumulator?,06/Apr/15 08:54;opensource@xplenty.com;I attached a new patch with unit tests but I can't test that it works as I can't compile the code right now. ,"22/Apr/15 22:43;rohini;[~opensource@xplenty.com],
    What problems do you have with compiling?  Please ensure that the testcases you have added fail without the fix and pass after the fix. 

Let me know if you need any help or clarifications as I would like to get this patch into Pig 0.15. ",29/Apr/15 12:40;opensource@xplenty.com;fixes previous test patch,"29/Apr/15 12:41;opensource@xplenty.com;I fixed the testcases, everything compiles cleanly now. It fails before the fix and passes with it.","22/May/15 21:47;rohini;Thanks [~opensource@xplenty.com]. The new tests are good.

Did some additional changes on [~opensource@xplenty.com]'s patch when I saw the duplication of code with asserts. So
     - Extracted out the repeated asserts in the multiple min and max tests into a method. 
   - Also added some javadoc to make the new Util method more clear. It is a good one could really be useful when writing future tests. 

[~daijy], can you review the changes.",22/May/15 21:55;daijy;+1,22/May/15 22:14;rohini;Committed to branch-0.15 and trunk. Thanks [~opensource@xplenty.com] for fixing this important issue.  Thanks Daniel for the additional review.,,,,,,,,,,,,,,,,,,,,
Pig on tez mask tez.queue.name,PIG-4488,12787131,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,31/Mar/15 19:50,07/Jun/15 03:47,14/Mar/19 03:08,05/Apr/15 00:55,,,,,,0.15.0,,,,,0,,,,,,,"Set ""tez.queue.name"" does not work, however, set ""mapreduce.job.queuename"" works on tez. The reason is we mask tez.queue.name with mapreduce.job.queuename in MRToTezHelper:
{code}
        String queueName = tezConf.get(JobContext.QUEUE_NAME,
                YarnConfiguration.DEFAULT_QUEUE_NAME);
        dagAMConf.setIfUnset(TezConfiguration.TEZ_QUEUE_NAME, queueName);
{code}
",,,,,,,,,,,,,,,,,,,,04/Apr/15 03:45;daijy;PIG-4488-1.patch;https://issues.apache.org/jira/secure/attachment/12709383/PIG-4488-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-31 20:03:35.421,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sun Apr 05 00:55:56 UTC 2015,,,,,,,0|i27lxr:,9223372036854775807,,,,,,,,,,31/Mar/15 20:03;rohini;Doesn't setIfUnset handle that?. i.e if tez.queue.name is specified it should take that over the mapreduce setting.,"31/Mar/15 20:27;daijy;In my test, ""tezConf.get(JobContext.QUEUE_NAME, YarnConfiguration.DEFAULT_QUEUE_NAME)"" always return ""default"", even if I don't set ""mapreduce.job.queue"".","31/Mar/15 20:33;rohini;Yes. But dagAMConf.setIfUnset(TezConfiguration.TEZ_QUEUE_NAME, queueName); should only set tez.queue.name to default if it is not already set in dagAMConf either from tez-site.xml or pig command line.","31/Mar/15 21:42;daijy;Yes, you are right. The actual code cause the problem is:
{code}
        for (Entry<String, String> entry : mrParamToDAGParamMap.entrySet()) {
            if (dagAMConf.get(entry.getKey()) != null) {
                dagAMConf.set(entry.getValue(), dagAMConf.get(entry.getKey()));
                dagAMConf.unset(entry.getKey());
                if (LOG.isDebugEnabled()) {
                    LOG.debug(""MR->DAG Translating MR key: "" + entry.getKey()
                            + "" to Tez key: "" + entry.getValue()
                            + "" with value "" + dagAMConf.get(entry.getValue()));
                }
            }
        }
{code}
dagAMConf contains ""mapreduce.job.queuename=default"" when mapreduce.job.queuename is not set , thus override ""tez.queue.name"".","31/Mar/15 23:04;rohini;Ah. So that should be dagAMConf.setIfUnset(entry.getValue(), dagAMConf.get(entry.getKey()));",04/Apr/15 10:20;rohini;+1,05/Apr/15 00:55;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,
Pig on Tez gives wrong success message on failure in case of multiple outputs,PIG-4487,12786863,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,30/Mar/15 22:53,24/May/16 22:29,14/Mar/19 03:08,01/Apr/15 23:26,,,,,,0.15.0,,,,,0,,,,,,,"  If there are multiple output vertices, even though commit is it at DAG level and no records are stored till the whole DAG is successful, pig stats says successfully stored records for successful vertices. It should say that only if vertex level commit is set to true.

For eg:
Input(s):
Successfully read 301 records (7930 bytes) from: ""aaa""
Successfully read 301 records (7136 bytes) from: ""bbb""

Output(s):
Failed to produce result in ""xxx""
Successfully stored 10 records (738 bytes) in: ""yyy""",,,,,,,,,,,,PIG-4446,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 01 23:26:15 UTC 2015,,,,,,,0|i27kbj:,9223372036854775807,,,,,,,,,,01/Apr/15 23:26;rohini;Fixed this as part of patch for PIG-4483. Both modified OutputStats so combined into one patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ant pull jetty-6.1.26.zip on some platform,PIG-4484,12785932,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,26/Mar/15 16:58,07/Jun/15 03:47,14/Mar/19 03:08,26/Mar/15 19:09,,,,,,0.15.0,,build,,,0,,,,,,,"On some platform, Pig build fail with the message:
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		::              FAILED DOWNLOADS            ::
[ivy:resolve] 		:: ^ see resolution messages for details  ^ ::
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		:: org.mortbay.jetty#jetty;6.1.26!jetty.zip
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::

For some reason ivy is trying to pull jetty.zip instead of jetty.jar, and fail with the zip file is not there.

When I examining the ivy.xml, I also find we put jetty twice. Fix that as well.",,,,,,,,,,,,,,,,,,,,26/Mar/15 16:58;daijy;PIG-4484-1.patch;https://issues.apache.org/jira/secure/attachment/12707538/PIG-4484-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-26 18:38:06.539,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Mar 26 19:09:05 UTC 2015,,,,,,,0|i27emv:,9223372036854775807,,,,,,,,,,26/Mar/15 18:38;rohini;+1,26/Mar/15 19:09;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig script failure on Tez with split and order by due to missing sample collection,PIG-4480,12784988,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,23/Mar/15 20:54,07/Jun/15 03:48,14/Mar/19 03:08,30/Mar/15 13:20,0.14.0,,,,,0.15.0,,,,,0,,,,,,,"{code}
Error: Failure while running task:java.lang.NullPointerException at java.util.Arrays.binarySearch(Arrays.java:1523) at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:82) at org.apache.pig.backend.hadoop.executionengine.tez.runtime.WeightedRangePartitionerTez.getPartition(WeightedRangePartitionerTez.java:46) at org.apache.pig.backend.hadoop.executionengine.tez.runtime.WeightedRangePartitionerTez.getPartition(WeightedRangePartitionerTez.java:35) at org.apache.tez.mapreduce.partition.MRPartitioner.getPartition(MRPartitioner.java:92) at org.apache.tez.runtime.library.common.sort.impl.dflt.DefaultSorter.write(DefaultSorter.java:185) at org.apache.tez.runtime.library.output.OrderedPartitionedKVOutput$1.write(OrderedPartitionedKVOutput.java:127) at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POIdentityInOutTez.getNextTuple(POIdentityInOutTez.java:150) at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:317) at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:196) at 
{code}",,,,,,,,,,,,,,,,,,,,30/Mar/15 06:02;rohini;PIG-4480-1.patch;https://issues.apache.org/jira/secure/attachment/12708088/PIG-4480-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-30 06:54:58.579,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Mar 30 13:20:01 UTC 2015,,,,,,,0|i278yf:,9223372036854775807,,,,,,,,,,30/Mar/15 06:02;rohini;Sample was not being collected in case of Split if the first record did not match condition as lastSample was being set to POStatus.STATUS_EOP and that was always returned as sample even if further records matched. ,30/Mar/15 06:54;daijy;+1 once tests pass.,30/Mar/15 13:20;rohini;Committed to trunk. Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,,
Pig script with union within nested splits followed by join failed on Tez,PIG-4479,12784985,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,23/Mar/15 20:46,07/Jun/15 03:48,14/Mar/19 03:08,25/Mar/15 17:42,,,,,,0.15.0,,,,,0,,,,,,,"
{code}
ERROR 2087: Unexpected problem during optimization. Found index:0 in multiple LocalRearrange operators.

org.apache.pig.impl.plan.optimizer.OptimizerException: ERROR 2087: Unexpected problem during optimization. Found index:0 in multiple LocalRearrange operators.
    at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezPOPackageAnnotator$LoRearrangeDiscoverer.visitLocalRearrange(TezPOPackageAnnotator.java:170)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.visit(POLocalRearrange.java:185)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.visit(POLocalRearrange.java:52)
    at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
    at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
    at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
    at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:52)
    at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:46)
    at org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor.visitSplit(PhyPlanVisitor.java:168)

{code}",,,,,,,,,,,,,,,,,,,,25/Mar/15 15:29;rohini;PIG-4479-1.patch;https://issues.apache.org/jira/secure/attachment/12707221/PIG-4479-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-25 16:38:47.646,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Mar 25 17:42:51 UTC 2015,,,,,,,0|i278xr:,9223372036854775807,,,,,,,,,,"25/Mar/15 15:29;rohini;  This is a special case where multiple LocalRearrange in POSplit point to same vertex group. We usually avoid keeping them in POSplit, but the LocalRearranges are in different nested splits. Not changing MultiQueryOptimizer to avoid this as well as vertex group handles multiple edges from same vertex unlike regular edge.",25/Mar/15 16:38;daijy;+1,25/Mar/15 17:42;rohini;Committed to trunk. Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,,
TestCSVExcelStorage fails with jdk8,PIG-4478,12784943,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,23/Mar/15 18:45,13/Nov/15 19:25,14/Mar/19 03:08,23/Mar/15 20:40,,,,,,0.15.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Mar/15 18:50;rohini;PIG-4478-1.patch;https://issues.apache.org/jira/secure/attachment/12706681/PIG-4478-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-23 18:51:39.063,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Mar 23 20:40:27 UTC 2015,,,,,,,0|i278on:,9223372036854775807,,,,,,,,,,"23/Mar/15 18:50;rohini; This is the only jdk8 failure in trunk right now due to hashmap ordering. 

 Just comparing for jdk7 or jdk8 output. Was too much of work to write a new parser to sort within that line output and handle multiple maps in nested bags correctly. QueryParser could not be reused based on the way it was dumped by csv storage.",23/Mar/15 18:51;daijy;+1,23/Mar/15 20:40;rohini;Committed to trunk. Thanks for the review Daniel,,,,,,,,,,,,,,,,,,,,,,,,,
Fix logging in AvroStorage* classes and SchemaTuple class,PIG-4476,12784758,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rdsr,rdsr,rdsr,23/Mar/15 07:53,07/Jun/15 03:47,14/Mar/19 03:08,25/Mar/15 18:23,,,,,,0.15.0,,,,,0,,,,,,,"AvroStorageSchemaConversionUtilities and SchemaTuple use org.mortbay.log as their logger.

I've removed that with common logging like the rest of the Pig code",,,,,,,,,,,,,,,,,,,,23/Mar/15 07:55;rdsr;PIG-4476.patch;https://issues.apache.org/jira/secure/attachment/12706470/PIG-4476.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-23 19:40:13.961,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Mar 25 18:23:43 UTC 2015,,,,,,,0|i277rj:,9223372036854775807,,,,,,,,,,23/Mar/15 07:55;rdsr;Simple fix,23/Mar/15 19:40;erwaman;LGTM,25/Mar/15 18:23;rohini;+1. Committed to trunk. Thanks RD.,,,,,,,,,,,,,,,,,,,,,,,,,
Keys in AvroMapWrapper are not proper Pig types,PIG-4475,12783829,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rdsr,rdsr,rdsr,21/Mar/15 05:19,07/Jun/15 03:48,14/Mar/19 03:08,24/Mar/15 16:47,,,,,,0.15.0,,,,,0,,,,,,,"AvroMapWrapper could contain utf8 keys, which are not supported by Pig. Pig expects keys to be of type String.",,,,,,,,,,,,,,,,,,,,21/Mar/15 05:47;rdsr;PIG-4475.patch;https://issues.apache.org/jira/secure/attachment/12706127/PIG-4475.patch,24/Mar/15 13:58;rdsr;PIG-4475_1.patch;https://issues.apache.org/jira/secure/attachment/12706912/PIG-4475_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-03-23 18:36:59.127,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Mar 24 16:47:05 UTC 2015,,,,,,,0|i272vr:,9223372036854775807,,,,,,,,,,21/Mar/15 05:47;rdsr;Attaching fix with testcase modification,"23/Mar/15 18:36;erwaman;Looks good. One small comment: The indentation in this block looks inconsistent:
{code}
@@ -107,6 +112,13 @@ public final class AvroMapWrapper implements Map<CharSequence, Object> {
 
   @Override
   public Set<CharSequence> keySet() {
+    if (isUtf8key) {
+        final Set<CharSequence> keySet = new HashSet<CharSequence>();
+        for (CharSequence cs : innerMap.keySet()) {
+            keySet.add(cs.toString());
+        }
+        return keySet;
+    }
     return innerMap.keySet();
   }
{code}
I see both 2 and 4 space indentation. I think we should stick with 2, since the rest of the files seems to use that.",24/Mar/15 13:58;rdsr;Addressing Anthony's comments,"24/Mar/15 16:47;daijy;+1. Patch committed to trunk. Thanks Ratandeep, Anthony!",,,,,,,,,,,,,,,,,,,,,,,,
Increasing intermediate parallelism has issue with default parallelism,PIG-4474,12783664,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,20/Mar/15 16:05,07/Jun/15 03:47,14/Mar/19 03:08,23/Mar/15 20:37,0.14.0,,,,,0.15.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Mar/15 06:37;rohini;PIG-4474-1.patch;https://issues.apache.org/jira/secure/attachment/12706460/PIG-4474-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-23 16:31:52.062,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Mar 23 20:37:51 UTC 2015,,,,,,,0|i271vj:,9223372036854775807,,,,,,,,,,"23/Mar/15 06:37;rohini;The script fails in the scenarios like Load(s) + Join + GroupBy or Load(s) + GroupBy+ GroupBy if default_parallel is defined while estimating parallelism for the second group by..

Some operators like Skewed Join/Order by set value of requested parallelism to default parallelism and because of that don't encounter this error. i.e Load(s) + Skewed Join + GroupBy or Load(s) + GroupBy + Order by does not fail.",23/Mar/15 16:31;daijy;+1,23/Mar/15 20:37;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,
Pig's jackson version conflicts with that of hadoop 2.6.0 or newer,PIG-4468,12783112,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjffdu,zjffdu,zjffdu,19/Mar/15 03:54,08/Jun/16 20:48,14/Mar/19 03:08,26/Oct/15 21:37,,,,,,0.16.0,,,,,0,,,,,,,"Pig use jackson of 1.8.8 while hadoop 2.6.0 use 1.9.13. And hadoop 2.6.0 use one of ObjectMapper's new method setSerializationInclusion which is not existed in jackson 1.8.8. It would cause the following issue

{code}
Caused by: java.lang.NoSuchMethodError: org.codehaus.jackson.map.ObjectMapper.setSerializationInclusion(Lorg/codehaus/jackson/map/annotate/JsonSerialize$Inclusion;)Lorg/codehaus/jackson/map/ObjectMapper;
    at org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider.configObjectMapper(YarnJacksonJaxbJsonProvider.java:59)
    at org.apache.hadoop.yarn.util.timeline.TimelineUtils.<clinit>(TimelineUtils.java:47)
    at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:166)
    at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
    at org.apache.tez.client.TezYarnClient.init(TezYarnClient.java:45)
    at org.apache.tez.client.TezClient.start(TezClient.java:299)
    at org.apache.pig.backend.hadoop.executionengine.tez.TezSessionManager.createSession(TezSessionManager.java:95)
    at org.apache.pig.backend.hadoop.executionengine.tez.TezSessionManager.getClient(TezSessionManager.java:195)
    at org.apache.pig.backend.hadoop.executionengine.tez.TezJob.run(TezJob.java:158)
    at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher$1.run(TezLauncher.java:174)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
{code}",,,,,,,,,,,,,,,,,,,,26/Oct/15 21:19;daijy;PIG-4468-2.patch;https://issues.apache.org/jira/secure/attachment/12768827/PIG-4468-2.patch,31/Mar/15 05:18;zjffdu;PIG_4468_1.patch;https://issues.apache.org/jira/secure/attachment/12708329/PIG_4468_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-03-31 06:51:55.873,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Oct 26 21:37:53 UTC 2015,,,,,,,0|i26yi7:,9223372036854775807,,,,,,,,,,31/Mar/15 05:18;zjffdu;Upload the patch (upgrade jackson to 1.9.13),"31/Mar/15 06:51;rohini;Few comments:
   - Can you check if hadoop 1.x compiles and runs fine?
  - Can you update TestRegisteredJarVisibility.testRegisterJarOverridePigJarPackages test and  jackson-pig-3039-test.version=1.9.9 to use a newer version of jackson (2.x) ?  ","16/Oct/15 04:07;tsato;I found this issue on HDP 2.3(HDP-2.3.2.0-2950).

My workaround is to remove the followings from pig classpath so that relevant newer jars are loaded from hadoop classpaths.

$ ls -l /usr/hdp/current/pig-client/lib/jack*
-rw-r--r-- 1 root root   227500 Jul 14 23:11 jackson-core-asl-1.8.8.jar
-rw-r--r-- 1 root root   668564 Jul 14 23:11 jackson-mapper-asl-1.8.8.jar

---------------
ERROR 2998: Unhandled internal error. org.codehaus.jackson.map.ObjectMapper.setSerializationInclusion(Lorg/codehaus/jackson/map/annotate/JsonSerialize$Inclusion;)Lorg/codehaus/jackson/map/ObjectMapper;

java.lang.NoSuchMethodError: org.codehaus.jackson.map.ObjectMapper.setSerializationInclusion(Lorg/codehaus/jackson/map/annotate/JsonSerialize$Inclusion;)Lorg/codehaus/jackson/map/ObjectMapper;
        at org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider.configObjectMapper(YarnJacksonJaxbJsonProvider.java:59)
        at org.apache.hadoop.yarn.util.timeline.TimelineUtils.<clinit>(TimelineUtils.java:50)
        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:172)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.mapred.ResourceMgrDelegate.serviceInit(ResourceMgrDelegate.java:103)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.mapred.ResourceMgrDelegate.<init>(ResourceMgrDelegate.java:97)
        at org.apache.hadoop.mapred.YARNRunner.<init>(YARNRunner.java:112)
        at org.apache.hadoop.mapred.YarnClientProtocolProvider.create(YarnClientProtocolProvider.java:34)
        at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:95)
        at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:82)
        at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:75)
        at org.apache.hadoop.mapred.JobClient.init(JobClient.java:475)
        at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:454)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:163)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:304)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1390)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1375)
        at org.apache.pig.PigServer.storeEx(PigServer.java:1034)
        at org.apache.pig.PigServer.store(PigServer.java:997)
        at org.apache.pig.PigServer.openIterator(PigServer.java:910)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:754)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:376)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:66)
        at org.apache.pig.Main.run(Main.java:565)
        at org.apache.pig.Main.main(Main.java:177)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
================================================================================
",26/Oct/15 21:19;daijy;Fix TestRegisteredJarVisibility test failure. Also tested on Hadoop 1 and works.,26/Oct/15 21:22;rohini;+1,"26/Oct/15 21:37;daijy;Patch committed to trunk. Thanks Jeff, Rohini!",,,,,,,,,,,,,,,,,,,,,,
Pig streaming ship fails for relative paths on Tez,PIG-4465,12783056,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,18/Mar/15 23:22,07/Jun/15 03:47,14/Mar/19 03:08,20/Mar/15 19:23,0.14.0,,,,,0.15.0,,,,,0,,,,,,,"DEFINE MyScript `python script.py` ship('script.py'); causes

{code}
Caused by: java.io.FileNotFoundException: File
/home/username/pig/ script.py does not exist
    at
org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
    at
org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
    at
org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
    at
org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:416)
    at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)
    at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1934)
    at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1902)
    at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1867)
    at
org.apache.pig.backend.hadoop.executionengine.tez.TezResourceManager.addTezResource(TezResourceManager.java:83)
    at
org.apache.pig.backend.hadoop.executionengine.tez.TezJobCompiler.getJob(TezJobCompiler.java:95)
{code}",,,,,,,,,,,,,,,,,,,,20/Mar/15 15:10;rohini;PIG-4465-1.patch;https://issues.apache.org/jira/secure/attachment/12705944/PIG-4465-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-20 17:14:56.374,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Mar 20 19:23:24 UTC 2015,,,,,,,0|i26y67:,9223372036854775807,,,,,,,,,,20/Mar/15 17:14;daijy;+1,20/Mar/15 19:23;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroMapWrapper still leaks Avro data types and AvroStorageDataConversionUtilities do not handle Pig maps,PIG-4463,12782611,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rdsr,rdsr,rdsr,17/Mar/15 16:39,07/Jun/15 03:47,14/Mar/19 03:08,18/Mar/15 17:04,,,,,,0.15.0,,,,,0,,,,,,,"I did not address all the ways in which values in AvroMapWrapper could be access in the ticket PIG-4448

I've fixed AvroMapWrapper now . Wrote some exhaustive tests around it and also discovered that AvroStorageDataConversionUtilities do not handle maps. 

I'll update with a patch once the commit tests pass",,,,,,,,,,,,,,,,,,,,17/Mar/15 16:44;rdsr;PIG-4463.patch;https://issues.apache.org/jira/secure/attachment/12705109/PIG-4463.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-18 17:04:20.814,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Mar 18 17:04:20 UTC 2015,,,,,,,0|i26vi7:,9223372036854775807,,,,,,,"avro, map",,,"17/Mar/15 16:44;rdsr;Attaching patch

Deleted the old test around AvroMapWrapper. More exhaustive test is written for it now in TestAvroStorage

Changes in AvroStorageDataConversionUtilities are tested by testcases
testLoadRecordsWithMapsOfArrayOfRecords and 
testLoadRecordsWithMapsOfRecords",18/Mar/15 17:04;daijy;Patch committed to trunk. Thanks Ratandeep!,,,,,,,,,,,,,,,,,,,,,,,,,,
Use benchmarks for Windows Pig e2e tests,PIG-4461,12781892,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,nmaheshwari,nmaheshwari,nmaheshwari,13/Mar/15 18:39,07/Jun/15 03:48,14/Mar/19 03:08,19/Mar/15 00:14,0.14.1,,,,,0.15.0,,,,,0,,,,,,,"Use benchmarks for Windows Pig e2e tests. Currently in the Pig system test for windows, we do not use benchmarks. Due to this, each test run twice to verify if its a pass. Using benchmark will save a lot of time, in the windows run.
",,,,,,,,,,,,,,,,,,,,18/Mar/15 23:49;nmaheshwari;PIG-4461-new.patch;https://issues.apache.org/jira/secure/attachment/12705475/PIG-4461-new.patch,13/Mar/15 18:56;nmaheshwari;PIG-4461.patch;https://issues.apache.org/jira/secure/attachment/12704484/PIG-4461.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-03-18 17:10:42.505,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Mar 19 00:14:31 UTC 2015,,,,,,,0|i26r5j:,9223372036854775807,,,,,,,,,,13/Mar/15 18:56;nmaheshwari;[~daijy] - Can you please review the attached patch for using benchmarks.,"18/Mar/15 17:10;daijy;LGTM. [~rohini], this patch will do a sort on the cached benchmark. This is to address the behavior difference between sort on Unix/Windows. Windows sort file differently than Linux, so we cannot compare Windoes test result with benchmark file collected under Unix directly. This will increase the total runtime by a small margin. Are you Ok with it or you want a flag to enable by demand?",18/Mar/15 18:27;rohini;Flag by demand would be better.,18/Mar/15 23:49;nmaheshwari;[~daijy]- Please find the latest patch. It uses a flag for sorting,19/Mar/15 00:14;daijy;Patch committed to trunk. Thanks Namit!,,,,,,,,,,,,,,,,,,,,,,,
TestBuiltIn testValueListOutputSchemaComplexType and testValueSetOutputSchemaComplexType tests create bags whose inner schema is not a tuple,PIG-4460,12781868,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,erwaman,erwaman,erwaman,13/Mar/15 17:34,07/Jun/15 03:47,14/Mar/19 03:08,13/Mar/15 18:00,0.15.0,,,,,0.15.0,,,,,0,,,,,,,"These are bugs in the tests added in PIG-4432/PIG-4445.

TestBuiltin.testValueSetOutputSchemaComplexType and TestBuiltin.testValueListOutputSchemaComplexType create bag schemas like this:
{code}
Schema bagSchema = Schema.generateNestedSchema(DataType.BAG, DataType.CHARARRAY);
{code}
This is incorrect because the inner schema for bags should always be a TUPLE.",,,,,,,,,,,,,,,,,,,,13/Mar/15 17:46;erwaman;PIG-4460.1.patch;https://issues.apache.org/jira/secure/attachment/12704463/PIG-4460.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-13 18:00:21.299,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Mar 13 18:00:21 UTC 2015,,,,,,,0|i26r07:,9223372036854775807,,,,,,,,,,13/Mar/15 17:46;erwaman;Uploaded patch.,"13/Mar/15 18:00;daijy;+1

Patch committed to trunk. Thanks Anthony!",,,,,,,,,,,,,,,,,,,,,,,,,,
Error is thrown by JobStats.getOutputSize() when storing to a MySql table,PIG-4457,12781136,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,kunal_kr,kunal_kr,11/Mar/15 10:18,07/Jun/15 03:48,14/Mar/19 03:06,24/Mar/15 16:52,0.14.0,,,,,0.15.0,,,,,0,,,,,,,"Here is an example of stack trace printed to console output. Actually, this is a warning message and does not make the job fail. The data is getting stored to mysql table, but i have no idea why pig is looking to store output on hdfs. I am using PIg along with Tez.

using output size reader: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
unable to find the output file
java.io.FileNotFoundException: File hdfs://pts0021.persistent.co.in:9000/user/shareinsights/filtered_stock_data does not exist.
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:647)
        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:101)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:705)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:701)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:701)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:81)
        at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:351)
        at org.apache.pig.tools.pigstats.tez.TezVertexStats.addOutputStatistics(TezVertexStats.java:270)
        at org.apache.pig.tools.pigstats.tez.TezVertexStats.accumulateStats(TezVertexStats.java:188)
        at org.apache.pig.tools.pigstats.tez.TezDAGStats.accumulateStats(TezDAGStats.java:209)
        at org.apache.pig.tools.pigstats.tez.TezPigScriptStats.accumulateStats(TezPigScriptStats.java:180)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezJob.run(TezJob.java:194)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher$1.run(TezLauncher.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)",,,,,,,,,,,,,,,,,,,,24/Mar/15 09:38;rohini;PIG-4457-1.patch;https://issues.apache.org/jira/secure/attachment/12706869/PIG-4457-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-11 20:17:41.774,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Mar 24 16:52:48 UTC 2015,,,,,,,0|i26ml3:,9223372036854775807,,,,,,,,,,"11/Mar/15 20:17;rohini;You can set pig.stats.output.size.reader.unsupported to your StoreFunc to avoid getting this warning.

For eg:
pig.stats.output.size.reader.unsupported=org.apache.hcatalog.pig.HCatStorer,org.apache.hive.hcatalog.pig.HCatStorer","11/Mar/15 20:18;rohini;[~kunal_kr],
    Can you paste the snippet of your pig script that does STORE?","24/Mar/15 06:57;kunal_kr;Thanks Rohini. Setting pig.stats.output.size.reader.unsupported=org.apache.pig.piggybank.storage.DBStorage worked.
","24/Mar/15 09:33;rohini;Reopening to have pig.stats.output.size.reader.unsupported=org.apache.hcatalog.pig.HCatStorer,org.apache.hive.hcatalog.pig.HCatStorer,org.apache.pig.piggybank.storage.DBStorage

added to pig-default.properties.",24/Mar/15 16:36;daijy;+1,24/Mar/15 16:52;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,
"Embedded SQL using ""SQL"" instead of ""sql"" fails with string index out of range: -1 error",PIG-4452,12780132,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,bkessler,bkessler,06/Mar/15 18:17,07/Jun/15 03:48,14/Mar/19 03:08,13/May/15 23:32,0.12.0,,,,,0.15.0,,grunt,parser,,0,,,,,,,"The embedded SQL command is case sensitive and fails with an string index out of range error that is not helpful for determining the cause of the failure.

grunt> SQL CREATE TABLE bk_test(col1 STRING);
2015-03-06 17:47:19,416 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. String index out of range: -1

grunt> sql CREATE TABLE bk_test(col1 STRING);
2015-03-06 17:49:47,219 [main] INFO  org.apache.pig.tools.grunt.GruntParser - Going to run hcat command: create table bk_test(col1 STRING);

The issue is possibly in this section of code implementing embedded SQL as part of PIG-2482 looking for indexOf(""sql"") which would return -1 on ""SQL"":

+    public static int runSQLCommand(String hcatBin, String cmd, boolean mInteractive) throws IOException {
+        String[] tokens = new String[3];
+        tokens[0] = hcatBin;
+        tokens[1] = ""-e"";
+        tokens[2] = cmd.substring(cmd.indexOf(""sql"")).substring(4);

",,,,,,,,,,,,,,,,,,,,06/Mar/15 20:32;daijy;PIG-4452-1.patch;https://issues.apache.org/jira/secure/attachment/12703126/PIG-4452-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-06 20:32:34.508,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed May 13 23:32:04 UTC 2015,,,,,,,0|i26gk7:,9223372036854775807,,,,,,,,,,06/Mar/15 20:32;daijy;Thanks for reporting. Patch attached.,13/May/15 23:09;rohini;+1,13/May/15 23:32;daijy;Patch committed to both branch 0.15 and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
AvroMapWrapper leaks Avro data types when the map values are complex avro records,PIG-4448,12779857,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rdsr,rdsr,rdsr,05/Mar/15 18:29,07/Jun/15 03:47,14/Mar/19 03:08,11/Mar/15 17:16,0.13.0,,,,,0.15.0,,,,,0,,,,,,,"AvroMapWrapper does not convert value objects in it's inner map to Pig types when queried. This could lead to problems when the value object is a complex Avro type. 

Attaching a preliminary patch. Please let me know if this is not the best way to fix it.",,,,,,,,,,,,,,,,,,,,05/Mar/15 18:30;rdsr;avro_map_wrapper_fix.patch;https://issues.apache.org/jira/secure/attachment/12702865/avro_map_wrapper_fix.patch,06/Mar/15 16:31;rdsr;avro_map_wrapper_fix2.patch;https://issues.apache.org/jira/secure/attachment/12703075/avro_map_wrapper_fix2.patch,07/Mar/15 07:53;rdsr;avro_map_wrapper_fix3.patch;https://issues.apache.org/jira/secure/attachment/12703228/avro_map_wrapper_fix3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2015-03-06 20:43:17.861,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Mar 11 17:16:52 UTC 2015,,,Patch Available,,,,0|i26ew7:,9223372036854775807,,,,,,,,,,05/Mar/15 18:30;rdsr;Fix with a simple testcase,05/Mar/15 18:54;rdsr;I'm realizing that this is not the right way to fix this. This also breaks the AvroStorage tests. I'll look into it a little more deeply tomorrow,"06/Mar/15 16:30;rdsr;For instance, below is the Avro schema and Pig schema for an Avro record which contains a map which contain a record.
{code}
Avro schema
{
  ""type"": ""record"",
  ""name"": ""recordInMap"",
  ""namespace"": ""org.apache.pig.test.builtin"",
  ""fields"": [
    {
      ""name"": ""key"",
      ""type"": ""string""
    },
    {
      ""name"": ""value"",
      ""type"": ""int""
    },
    {
      ""name"": ""parameters"",
      ""type"": {
        ""type"": ""map"",
        ""values"": {
          ""type"": ""record"",
          ""name"": ""A"",
          ""fields"": [
            {
              ""name"": ""id"",
              ""type"": [
                ""null"",
                ""int""
              ]
            }
          ]
        }
      }
    }
  ]
}

Pig schema
key:chararray,value:int,parameters:[A:(id:int)]
{code}

According to this, the Pig representation should contain a tuple for the the map's value record, but instead it currently contains an Avro GenericRecord

Updating the patch. Now I'm using the logic from org.apache.pig.impl.util.avro.AvroBagWrapper on how it is converting each element of the Bag to an equivalent Pig type.

My only gripe now is that the name of the method which helps to convert types from avro to Pig in AvroTupleWrapper is ""resolveUnion"" . Ideally I'd love to name it avroToPig or something similar but since its scope is  public  I'm refraining to do so.",06/Mar/15 16:31;rdsr;Attaching updated patch,"06/Mar/15 20:43;daijy;If ""resolveUnion"" is not the right name, can you change it? Also a minor comment, testRecordAsMapValue does not seem to justify a separate test suite, can you merge into TestAvroStorage or TestAvroStorageSchemaConversionUtilities?",07/Mar/15 03:14;rdsr;Thanks [~daijy] . I' ll move the test under TestAvroStorageSchemaConversionUtilities and fix the name,"07/Mar/15 07:53;rdsr;Attaching the updated patch.
I hope I don't break anything with the name change of a public method. ",10/Mar/15 17:06;rdsr;Hi [~daijy]  I've updated the patch. Do let me know if it looks ok or it needs modifications or even if there's a better approach to this problem.,11/Mar/15 17:16;daijy;Patch committed to trunk. Thanks Ratandeep!,,,,,,,,,,,,,,,,,,,
Pig Cannot handle nullable values (arrays and records) in avro records,PIG-4447,12779837,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rdsr,rdsr,rdsr,05/Mar/15 16:58,07/Jun/15 03:48,14/Mar/19 03:08,05/Mar/15 17:46,0.13.0,,,,,0.15.0,,,,,0,,,,,,,"Here's an example of an avro schema containing nullable values in a map
{noformat}
{
    ""name"" : ""nullableRecordInMap"",
    ""namespace"" : ""org.apache.pig.test.builtin"",
    ""type"" : ""record"",
    ""fields"" : [
        {""name"" : ""key"", ""type"" : ""string""},
        {""name"" : ""value"", ""type"" : ""int""},
        {
            ""name"" : ""parameters"",
            ""type"": [
                ""null"",
                {
                    ""type"": ""map"",
                    ""values"": [
                        ""null"",
                        {
                            ""type"": ""record"",
                            ""name"": ""nullable_record"",
                            ""fields"": [
                                {
                                    ""name"": ""id"",
                                    ""type"": [
                                        ""null"",
                                        ""string""
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        }
    ]
}
{noformat}

Here's the corresponding Pig resource schema on running it through org.apache.pig.impl.util.avro.AvroStorageSchemaConversionUtilities
{noformat}
key:chararray,value:int,parameters:[nullable_record:(union:(id:chararray))]
{noformat}

Note that Pig should unpack the underlying schema from the nullable union and the Pig schema should be
{noformat}
key:chararray,value:int,parameters:[nullable_record:(id:chararray)]
{noformat}

There's similar behavior if the nullal map value is of type array

I've created a patch with a few testcases written.",,,,,,,,,,,,,,,,,,,HADOOP-9365,05/Mar/15 17:01;rdsr;pig.patch;https://issues.apache.org/jira/secure/attachment/12702852/pig.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-05 17:46:25.837,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Mar 11 11:16:41 UTC 2015,,,Patch Available,,,,0|i26erz:,9223372036854775807,,,,,,,,,,05/Mar/15 17:01;rdsr;Attaching patch with testcases for nullable values (records and arrays) in a map,05/Mar/15 17:46;daijy;Patch committed to trunk. Thanks Ratandeep!,07/Mar/15 04:05;rdsr;Hi [~daijy] It seems that the file TestAvroStorageSchemaConversionUtilities.java is missing from the commit.  I wanted to add the test written for PIG-4448 in this test file. ,"07/Mar/15 06:21;daijy;Sorry, done.",11/Mar/15 11:16;rdsr;Sorry for the last comment it was by mistake. (I can't seem to delete that though),,,,,,,,,,,,,,,,,,,,,,,
VALUELIST and VALUESET outputSchema does not match actual schema of data returned when map value schema is complex,PIG-4445,12779303,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,erwaman,erwaman,erwaman,04/Mar/15 05:40,07/Jun/15 03:48,14/Mar/19 03:08,04/Mar/15 06:19,0.11.1,,,,,0.15.0,,,,,0,,,,,,,"This was caused by a bug in the fix for PIG-4432.

To reproduce:

{code:title=map_complex.txt}
[a#(foo),b#(bar)]
{code}

{code:title=testValueListComplex.pig}
a = load 'map_complex.txt' as (map[(chararray)]);
b = foreach a generate VALUELIST($0);
describe b;
{code}

Expected:
{code}
b: {{((val_0: chararray))}}
{code}

Actual:
{code}
b: {{(val_0: chararray)}}
{code}",,,,,,,,,,,,,PIG-4460,,,,,,,04/Mar/15 05:42;erwaman;PIG-4445.1.patch;https://issues.apache.org/jira/secure/attachment/12702376/PIG-4445.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-04 06:19:28.975,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Mar 04 06:19:28 UTC 2015,,,,,,,0|i26byf:,9223372036854775807,,,,,,,,,,04/Mar/15 05:43;erwaman;Uploaded a patch: [^PIG-4445.1.patch],"04/Mar/15 06:19;daijy;Patch committed to trunk. Thanks for the fix, Anthony!",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unit test failure TestTezAutoParallelism,PIG-4444,12779230,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,03/Mar/15 23:30,07/Jun/15 03:48,14/Mar/19 03:08,04/Mar/15 22:23,,,,,,0.15.0,,tez,,,0,,,,,,,"Introduced by PIG-4437. No big deal, update the test due to size estimation algorithm change.",,,,,,,,,,,,,,,,,,,,03/Mar/15 23:30;daijy;PIG-4444-1.patch;https://issues.apache.org/jira/secure/attachment/12702297/PIG-4444-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-04 01:42:13.107,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Mar 04 22:23:50 UTC 2015,,,,,,,0|i26bif:,9223372036854775807,,,,,,,,,,"04/Mar/15 01:42;rohini;How much was the special tuple size? If 40K is 5 reducers and 80K 3 reducers, 60K will be 4 reducers. Difference of 20K bytes? Worried about having lesser number of reducers.","04/Mar/15 06:12;daijy;The size of the special mark is 144 bytes. However, there are only 3 sample tuples in this test case, so that could make a huge difference in the average tuple size estimation.",04/Mar/15 20:31;rohini;+1. I think it is ok as there will be more same tuples for bigger data.,04/Mar/15 22:23;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
Write inputsplits in Tez to disk if the size is huge and option to compress pig input splits,PIG-4443,12779214,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,03/Mar/15 22:50,25/Jun/15 10:02,14/Mar/19 03:08,06/Mar/15 21:24,0.14.0,,,,,0.15.0,,,,,0,,,,,,,"Pig sets the input split information in user payload and when running against a table with 10s of 1000s of partitions, DAG submission fails with
java.io.IOException: Requested data length 305844060 is longer than maximum
configured RPC length 67108864",,,,,,,,,,,,,,,,,,,,06/Mar/15 15:38;rohini;PIG-4443-1.patch;https://issues.apache.org/jira/secure/attachment/12703069/PIG-4443-1.patch,11/Mar/15 05:05;rohini;PIG-4443-Fix-TEZ-2192-2.patch;https://issues.apache.org/jira/secure/attachment/12703837/PIG-4443-Fix-TEZ-2192-2.patch,11/Mar/15 04:35;rohini;PIG-4443-Fix-TEZ-2192.patch;https://issues.apache.org/jira/secure/attachment/12703833/PIG-4443-Fix-TEZ-2192.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2015-03-06 20:01:07.307,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Jun 25 10:02:45 UTC 2015,,,,,,,0|i26bf3:,9223372036854775807,,,,,,,,,,"03/Mar/15 22:56;rohini;More details on the issue with HCatLoader is in

https://issues.apache.org/jira/browse/TEZ-2144?focusedCommentId=14337288&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14337288

Created HIVE-9845 to fix the issue with HCatalog. TEZ-2144 is to have compression so that the serialized size is less and this workaround of serializing the splits to disk will not be required.","06/Mar/15 15:52;rohini;Patch adds two settings


1) pig.compress.input.splits
    This compresses the pig input split information if it is not a FileSplit. Compressing FileSplit did not give much benefits. This can be turned on for HCatLoader till HIVE-9845 and TEZ-2144 are fixed. If TEZ-1244 is fixed, we can always turn this of for Tez as compressing the whole payload will compress way better than compressing individual splits.
2) pig.tez.input.splits.mem.threshold
    Write input splits to disk in Tez if this threshold is hit. Default is 32MB which is half of the default 64MB protobuf transfer limit.

This patch also has an additional change that removes MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY from tez payload as any API that calls TokenCache.obtainTokensForNamenodes on the task will make it fail if pig was run via Oozie. This is because the value will be set to the credential file path in the Oozie launcher job which will not be available on the tasks. This issue was hit by Hive on Tez running with Oozie. MAPREDUCE-3727 is a related issue.",06/Mar/15 20:01;daijy;+1,06/Mar/15 21:24;rohini;Committed to trunk. Thanks for the review Daniel.,11/Mar/15 04:35;rohini;TestTezAutoParallelism.testSkewedJoinIncreaseIntermediateParallelism was actually failing due to TEZ-2192. Attached patch which is a short term workaround fixes that issue till TEZ-2192 is fixed in Tez. More details in https://issues.apache.org/jira/browse/TEZ-2192?focusedCommentId=14356259&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14356259,11/Mar/15 04:43;daijy;+1 for PIG-4443-Fix-TEZ-2192.patch.,11/Mar/15 05:05;rohini;Attaching another patch. Previous one had a bug in adding resource to vertex.,11/Mar/15 05:13;daijy;+1,11/Mar/15 05:14;rohini;Committed to trunk. Thanks Daniel for the review.,"22/Jun/15 10:50;angel2014;I have a script in PIG that loads data from Hive using org.apache.hive.hcatalog.pig.HCatLoader. This script works fine in Pig 0.14, but in Pig 0.15 I'm getting this error:

Requested data length 160452289 is longer than maximum configured RPC length 67108864

In Pig 0.14 I had to deal with this issue too, but I could always make it work by reducing the number of splits in the Hive tables created by Sqoop (using no more than 60 splits). Is there any special configuration needed?","22/Jun/15 16:46;rohini;Just to be sure, are you getting this error with Pig on Tez or Mapreduce? And the error is while submitting the job or after it completes and fetching task reports?","23/Jun/15 08:21;angel2014;I'm only getting this error with Pig on Tez (on Mapreduce works fine) and it is while submitting the job (""Cannot submit DAG"").

I'm using HDP 2.2.0.0-2041, so in order to test Pig 15, I've built this version from its sources, packaged it and added its dependencies to an unique zip file and uploaded to my HDFS. I'm also using LzoCodec to compress the intermediate temporary files (but it doesn't work without using LzoCodec either).

I execute these commands to run my script:

export JAVA_HOME=/usr/jdk64/jdk1.7.0_67
export HDP_VERSION=2.2.0.0-2041
export HADOOP_HOME=/usr/hdp/$HDP_VERSION/hadoop
export HIVE_HOME=/usr/hdp/$HDP_VERSION/hive
export HCAT_HOME=/usr/hdp/$HDP_VERSION/hive-hcatalog
export PIG_OPTS=""-DUSE_TEZ_SESSION=true -Dtez.lib.uris=/hdp/apps/2.2.0.0-2041/tez-0.15.0/tez.tar.gz -Dpig.tmpfilecompression=true -Dpig.tmpfilecompression.codec=lzo -Dtez.runtime.intermediate-output.should-compress=true -Dtez.runtime.intermediate-output.is-compressed=true -Dtez.runtime.intermediate-output.compress.codec=com.hadoop.compression.lzo.LzoCodec -Dtez.runtime.intermediate-input.is-compressed=true -Dtez.runtime.intermediate-input.compress.codec=com.hadoop.compression.lzo.LzoCodec -Dtez.runtime.compress.codec=com.hadoop.compression.lzo.LzoCodec""

./pig-0.15.0-src/bin/pig -useHCatalog -x tez -f myscript.pig 
","23/Jun/15 21:08;rohini;That is odd. This patch is supposed to fix the exact same issue and we have been running fine with this patch for months now. Do you see the below message from this patch in your logs?

{code}
log.info(""Writing input splits to "" + inputSplitsDir
                            + "" for vertex "" + vertex.getName()
                            + "" as the serialized size in memory is ""
                            + splitsSerializedSize + "". Configured ""
                            + PigConfiguration.PIG_TEZ_INPUT_SPLITS_MEM_THRESHOLD
                            + "" is "" + spillThreshold);
{code}

If not, I suspect that you still have the old pig.jar in your classpath and it is not Pig 0.15 that is running.  

bq. I've built this version from its sources, packaged it and added its dependencies to an unique zip file and uploaded to my HDFS.
  This comment also seems to indicate that you have replaced pig jar in hdfs. Not sure why you need pig in HDFS. You need to replace the pig-0.15.0-core-h2.jar in the pig client installation from the node you are running the script.","24/Jun/15 08:15;angel2014;I'm using Pig 15 ... or so it seems ...

   INFO  org.apache.pig.Main - Apache Pig version 0.15.0-SNAPSHOT (r: unknown) compiled Jun 18 2015, 15:39:42

but I don't see that message because in my case the previous if condition is false (splitsSerializedSize=1163342 , spillThreshold=33554432)

{code:java}
                if(splitsSerializedSize > spillThreshold) {
                    ...
                    log.info(""Writing input splits to "" + inputSplitsDir
                            + "" for vertex "" + vertex.getName()
                            + "" as the serialized size in memory is ""
                            + splitsSerializedSize + "". Configured ""
                            + PigConfiguration.PIG_TEZ_INPUT_SPLITS_MEM_THRESHOLD
                            + "" is "" + spillThreshold);
                    ...                       
                } else {
                    // Send splits via RPC to AM
                    userPayLoadBuilder.setSplits(splitsProto);
                }
{code}

I don't know if it's relevant, but comparing the differences in the AM syslog file between Pig 14 and Pig 15, I found this message only while executing Pig 15:

   INFO  org.apache.tez.client.TezClient - Using org.apache.tez.dag.history.ats.acls.ATSHistoryACLPolicyManager to manage Timeline ACLs

Do you have any test I could probe in my environment?

On the other hand, when I said I uploaded an unique zip to my HDFS, I meant only the TEZ 0.7.0 libraries and its dependencies 
(-Dtez.lib.uris=/hdp/apps/2.2.0.0-2041/tez-0.15.0/tez.tar.gz). I have to add this argument to my PIG_OPTS in order to overwrite my /etc/tez/conf/tez-site.xml settings (this file is managed by HDP and, by default, it's pointing to the TEZ 0.5.2 shipped with HDP 2.2.0.0.2041).","24/Jun/15 23:34;rohini;bq. Requested data length 160452289 is longer than maximum configured RPC length 67108864
bq. but I don't see that message because in my case the previous if condition is false (splitsSerializedSize=1163342 , spillThreshold=33554432)
    In your first error you said 160MB (160452289). This time you mentioned 1MB (1163342). Can you recheck? For 1MB, it should not be failing. Also could you paste your full stacktrace so that we can see where exactly is the error.","25/Jun/15 01:54;angel2014;*MASTER_NODE LOG* (from where I'm executing Pig)
{noformat}
2015-06-25 03:32:48,978 [PigTezLauncher-0] ERROR org.apache.pig.backend.hadoop.executionengine.tez.TezJob - Cannot submit DAG - Application id: application_1434957727568_0731
org.apache.tez.dag.api.TezException: com.google.protobuf.ServiceException: java.io.EOFException: End of File Exception between local host is: ""MASTER_NODE""; destination host is: ""AM_NODE"":40698; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
        at org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:476)
        at org.apache.tez.client.TezClient.submitDAG(TezClient.java:391)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezJob.run(TezJob.java:161)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher$1.run(TezLauncher.java:187)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: com.google.protobuf.ServiceException: java.io.EOFException: End of File Exception between local host is: ""MASTER_NODE""; destination host is: ""AM_NODE"":40698; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:246)
        at com.sun.proxy.$Proxy32.submitDAG(Unknown Source)
        at org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:469)
        ... 8 more
Caused by: java.io.EOFException: End of File Exception between local host is: ""MASTER_NODE""; destination host is: ""AM_NODE"":40698; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
        at org.apache.hadoop.ipc.Client.call(Client.java:1472)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        ... 10 more
Caused by: java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
{noformat}

*AM_NODE LOG* (the requested data length may have changed because I deleted and imported again two of the Hive tables)
{noformat}
2015-06-25 03:22:22,437 WARN [Socket Reader #1 for port 42481] ipc.Server: Requested data length 158480507 is longer than maximum configured RPC length 67108864.  RPC came from MASTER_NODE
2015-06-25 03:22:22,438 INFO [Socket Reader #1 for port 42481] ipc.Server: Socket Reader #1 for port 42481: readAndProcess from client MASTER_NODE threw exception [java.io.IOException: Requested data length 158480507 is longer than maximum configured RPC length 67108864.  RPC came from MASTER_NODE]
java.io.IOException: Requested data length 158480507 is longer than maximum configured RPC length 67108864.  RPC came from MASTER_NODE
        at org.apache.hadoop.ipc.Server$Connection.checkDataLength(Server.java:1459)
        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1521)
        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:762)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:636)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:607)
{noformat}
*SPLITS AND SIZES* (all the serialized sizes are less than the spillThreshold=33554432)
{noformat}
2015-06-25 03:21:34,277 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 9, SerializedSize: 1163342
2015-06-25 03:21:34,570 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 28, SerializedSize: 2207068
2015-06-25 03:21:34,858 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 28, SerializedSize: 1479632
2015-06-25 03:21:35,517 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 85, SerializedSize: 17841999
2015-06-25 03:21:35,773 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 1, SerializedSize: 191480
2015-06-25 03:21:36,087 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 3, SerializedSize: 581998
2015-06-25 03:21:36,475 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 11, SerializedSize: 2580419
2015-06-25 03:21:36,897 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 1, SerializedSize: 166474
2015-06-25 03:21:37,337 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 4, SerializedSize: 936317
2015-06-25 03:21:37,780 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 1, SerializedSize: 130474
{noformat}

I'm getting the same error in another Pig script with 18 HCatLoaders but both scripts work fine in Pig 14/Tez 0.5.2.
","25/Jun/15 08:01;rohini;Ok. I get the problem. The sizes of splits of the 18 loaders add up and cross 64 MB. Since this patch only checks that a single loader's split size does not exceed threshold, it does not fix the issue. Not sure how it could have worked with Pig 0.14. Only thing I can think of is that if the hadoop version has changed between when Pig 0.14 was installed and Pig 0.15 is installed. This RPC bounds check was only added in Hadoop 2.6 (HADOOP-10940). Can you check if that is the case?

If you had HIVE-9845, you would not be hitting this issue. From the pig side, you can try running with pig -Dpig.compress.input.splits=true and that should mostly fix it. If it still does not work, then you can try pig -Dipc.maximum.data.length=268435456 (256 MB).","25/Jun/15 10:02;angel2014;In theory, I'm using Hadoop 2.6 in both cases. Adding the parameter -Dipc.maximum.data.length=268435456 ... worked wonders!

Thanks a lot.",,,,,,,,,,
"Some code samples in documentation use Unicode left/right single quotes, which cause a parse failure.",PIG-4440,12778866,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,cnauroth,cnauroth,cnauroth,02/Mar/15 21:31,07/Jun/15 03:48,14/Mar/19 03:08,02/Mar/15 22:42,0.14.0,,,,,0.15.0,,documentation,,,0,,,,,,,"The current Pig documentation contains several code samples that wrap strings as 'string', using the Unicode code points for left single quote (U+2018) and right single quote (U+2019).  This is not valid syntax in a Pig script.  This can cause confusion for new users who are trying to get started by copy-pasting code samples from documentation.",,,,,,,,,,,,,,,,,,,,02/Mar/15 21:32;cnauroth;PIG-4440.001.patch;https://issues.apache.org/jira/secure/attachment/12701984/PIG-4440.001.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-02 22:42:43.108,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Mar 02 22:42:43 UTC 2015,,,,,,,0|i269cv:,9223372036854775807,,,,,,,,,,02/Mar/15 21:32;cnauroth;I'm attaching a patch that replaces occurrences of left single quote and right single quote with the ASCII single quote that is recognized as valid Pig script syntax.  Can someone please assign the issue to me?  Thanks!,"02/Mar/15 22:42;daijy;+1.

Patch committed to trunk. Thanks Chris!",,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix tez unit test failure TestJoinSmoke, TestSkewedJoin",PIG-4437,12778142,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,27/Feb/15 07:14,07/Jun/15 03:47,14/Mar/19 03:08,27/Feb/15 19:00,,,,,,0.15.0,,tez,,,0,,,,,,,"This is introduced by PIG-4410. For the special tuple, a normal tuple is piggybacked so the memSize should not be 0. It should deduct the size of the mark though (in theory, also need to deduct the cost of 1 field in tuple array, but that is very minor).",,,,,,,,,,,,,,,,,,,,27/Feb/15 07:33;daijy;PIG-4437-1.patch;https://issues.apache.org/jira/secure/attachment/12701294/PIG-4437-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-02-27 18:51:11.973,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Feb 27 19:00:10 UTC 2015,,,,,,,0|i264xz:,9223372036854775807,,,,,,,,,,27/Feb/15 18:51;rohini;+1,27/Feb/15 19:00;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Loading bigdecimal in nested tuple does not work,PIG-4433,12777257,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kpriceyahoo,kpriceyahoo,kpriceyahoo,24/Feb/15 19:16,28/Apr/16 05:21,14/Mar/19 03:08,24/Feb/15 23:03,0.14.0,0.14.1,0.15.0,,,0.15.0,,,,,0,,,,,,,"The parsing of BigDecimal data types in a nested tuple, as implemented by Utf8StorageConverter.java, does not work. There's a ""break;"" missing from a switch statement.

Code example that demonstrates the problem:

=== input.txt ===
(17,1234567890.0987654321)

=== pig_script ===:
inp = LOAD 'input.txt' AS (foo:tuple(bar:long, baz:bigdecimal));
STORE inp INTO 'output';

=== output ===
(17,)


With patch, the output becomes the expected:
(17,1234567890.0987654321)
",,,,,,,,,,,,,,,,,,,,24/Feb/15 23:01;daijy;PIG-4433-1.patch;https://issues.apache.org/jira/secure/attachment/12700603/PIG-4433-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-02-24 20:20:24.857,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Apr 28 05:21:15 UTC 2016,,,,,,,0|i25zkn:,9223372036854775807,,,,,,,,,,"24/Feb/15 19:18;kpriceyahoo;diff --git a/src/org/apache/pig/builtin/Utf8StorageConverter.java b/src/org/apache/pig/builtin/Utf8StorageConverter.java
index 814c746..1b905e2 100644
--- a/src/org/apache/pig/builtin/Utf8StorageConverter.java
+++ b/src/org/apache/pig/builtin/Utf8StorageConverter.java
@@ -315,6 +315,7 @@ public class Utf8StorageConverter implements LoadStoreCaster {
             break;
         case DataType.BIGDECIMAL:
             field = bytesToBigDecimal(b);
+            break;
         case DataType.DATETIME:
             field = bytesToDateTime(b);
             break;
diff --git a/test/org/apache/pig/builtin/TestUtf8StorageConverter.java b/test/org/apache/pig/builtin/TestUtf8StorageConverter.java
new file mode 100644
index 0000000..8cc9e55
--- /dev/null
+++ b/test/org/apache/pig/builtin/TestUtf8StorageConverter.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import static org.junit.Assert.assertEquals;
+
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import org.apache.pig.ResourceSchema;
+import org.apache.pig.ResourceSchema.ResourceFieldSchema;
+import org.apache.pig.data.DataByteArray;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
+import org.apache.pig.impl.util.Utils;
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+import org.junit.Test;
+
+public class TestUtf8StorageConverter {
+
+    @Test
+    /* Test that the simple data types convert properly in a tuple context */
+    public void testSimpleTypes() throws Exception {
+        Utf8StorageConverter converter = new Utf8StorageConverter();
+        String schemaString = ""a:int, b:long, c:float, d:double, e:chararray, f:bytearray, g:boolean, h:biginteger, i:bigdecimal, j:datetime"";
+        String dataString = ""(1,2,3.0,4.0,five,6,true,12345678901234567890,1234567890.0987654321,2007-04-05T14:30Z)"";
+
+        ResourceSchema.ResourceFieldSchema rfs = new ResourceFieldSchema(new FieldSchema(""schema"", Utils.getSchemaFromString(schemaString)));
+        Tuple result = converter.bytesToTuple(dataString.getBytes(), rfs);
+        assertEquals(10, result.size());
+        assertEquals(new Integer(1), result.get(0));
+        assertEquals(new Long(2L), result.get(1));
+        assertEquals(new Float(3.0f), result.get(2));
+        assertEquals(new Double(4.0), result.get(3));
+        assertEquals(""five"", result.get(4));
+        assertEquals(new DataByteArray(new byte[] { (byte) '6' }), result.get(5));
+        assertEquals(new Boolean(true), result.get(6));
+        assertEquals(new BigInteger(""12345678901234567890""), result.get(7));
+        assertEquals(new BigDecimal(""1234567890.0987654321""), result.get(8));
+        assertEquals(new DateTime(""2007-04-05T14:30Z"", DateTimeZone.UTC), result.get(9));
+    }
+}
","24/Feb/15 20:20;githubbot;GitHub user kpriceyahoo opened a pull request:

    https://github.com/apache/pig/pull/16

    Fix PIG-4433: Add missing ""break;"" and add unit test to validate the modified method.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/kpriceyahoo/pig branch-0.14

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/pig/pull/16.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #16
    
----
commit 2f1765fec3ddad58f98c9d3cf5e9ca16b56e3580
Author: Kevin Price <kprice@yahoo-inc.com>
Date:   2015-02-24T20:18:08Z

    Fix PIG-4433: Add missing ""break;"" and add unit test to validate the modified method.

----
",24/Feb/15 20:20;kpriceyahoo;Pull request created on github: https://github.com/apache/pig/pull/16,24/Feb/15 23:01;daijy;Pull the patch and attach to Jira.,"24/Feb/15 23:03;daijy;That's an obvious bug. Patch committed to trunk. Thanks Kevin!

Next time, please attach the patch to Jira ticket. Pig does not use pull request to track patches.","25/Feb/15 15:32;kpriceyahoo;Thanks, Daniel! Will do.","28/Apr/16 05:21;githubbot;Github user kpriceyahoo closed the pull request at:

    https://github.com/apache/pig/pull/16
",,,,,,,,,,,,,,,,,,,,,
Built-in VALUELIST and VALUESET UDFs do not preserve the schema when the map value type is a complex type,PIG-4432,12777031,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,erwaman,erwaman,erwaman,24/Feb/15 02:44,07/Jun/15 03:47,14/Mar/19 03:08,27/Feb/15 18:49,0.11.1,,,,,0.15.0,,,,,0,,,,,,,"To reproduce:
{code:title=testValueList.txt}
['a'#('foo')]
['b'#('bar')]
{code}

{code:title=testValueList.pig}
a = load 'testValueList.txt' as (map[(chararray)]);
b = foreach a generate VALUELIST($0); -- or use VALUESET (same problem)
describe b;
{code}

Run the Pig script:
{code}
pig testValueList.pig
{code}

Expected:
{code}
b: {{(val_0: chararray)}}
{code}

Actual:
{code}
b: {{()}}
{code}",,,,,,,,,,,,,PIG-4460,PIG-4445,,,,,,26/Feb/15 20:34;erwaman;PIG-4432.2.patch;https://issues.apache.org/jira/secure/attachment/12701162/PIG-4432.2.patch,24/Feb/15 03:11;erwaman;pig-4432.patch;https://issues.apache.org/jira/secure/attachment/12700324/pig-4432.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-02-24 04:08:02.365,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Feb 27 18:49:26 UTC 2015,,,,,,,0|i25yfz:,9223372036854775807,,,,,,,,,,24/Feb/15 02:44;erwaman;I have a fix; will upload a patch shortly.,24/Feb/15 03:13;erwaman;Uploaded a patch.,"24/Feb/15 04:08;mwagner;Thanks for fixing this, Anthony. I see that VALUESET has the same problem. If you'd like you can update the patch or I'll fix that as well when I commit. +1, otherwise.","24/Feb/15 16:38;erwaman;Thanks, Mark.  I will upload a new patch that also fixes VALUESET soon.",26/Feb/15 20:40;erwaman;Uploaded a new patch that contains the fix for both VALUELIST and VALUESET: [^PIG-4432.2.patch],"27/Feb/15 18:49;daijy;+1 as well. Patch committed to trunk. Thanks Anthony, Mark!",,,,,,,,,,,,,,,,,,,,,,
ReadToEndLoader does not close the record reader for the last input split,PIG-4431,12776689,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rdsr,rdsr,rdsr,22/Feb/15 07:49,07/Jun/15 03:48,14/Mar/19 03:08,25/Feb/15 00:18,,,,,,0.15.0,,,,,0,,,,,,,Attaching patch and simple testcase which demonstrate and fixes the issue.,,,,,,,,,,,,,,,,,,,,03/Mar/15 18:28;daijy;PIG-4431-1.patch;https://issues.apache.org/jira/secure/attachment/12702213/PIG-4431-1.patch,22/Feb/15 07:49;rdsr;ReadToEndLoader.patch;https://issues.apache.org/jira/secure/attachment/12700083/ReadToEndLoader.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-02-25 00:18:51.928,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Mar 03 18:29:21 UTC 2015,,,Patch Available,,,,0|i25wdb:,9223372036854775807,,,,,,,,,,"25/Feb/15 00:18;daijy;License header for TestReadToEndLoader.java is missing. +1 otherwise.

Patch committed to trunk. Thanks Ratandeep!",03/Mar/15 18:28;daijy;This patch breaks TestCollectedGroup. Attach a fix.,03/Mar/15 18:29;daijy;PIG-4431-1.patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,
RowNumber(simple) Rank not producing correct results,PIG-4426,12776308,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,19/Feb/15 23:11,07/Jun/15 03:47,14/Mar/19 03:08,24/Feb/15 23:14,0.15.0,,,,,0.15.0,,,,,0,,,,,,,"After PIG-4392, started seeing TestRank3.testRankWithSplitInMap (and some others) failing with 

{noformat}
Comparing actual and expected results.  expected:<[(1,1,2), (1,1,2), (1,3,1), (2,1,2), (3,1,2), (3,2,3), (3,2,4), (4,2,3), (5,2,4), (5,3,1)]> but was:<[(1,1,2), (1,1,2), (3,2,3), (3,2,4), (5,3,1)]>
{noformat}",,,,,,,,,,,,,,,,,,,,19/Feb/15 23:21;knoguchi;pig-4426-v01.txt;https://issues.apache.org/jira/secure/attachment/12699772/pig-4426-v01.txt,23/Feb/15 20:48;knoguchi;pig-4426-v02.txt;https://issues.apache.org/jira/secure/attachment/12700257/pig-4426-v02.txt,24/Feb/15 19:13;knoguchi;pig-4426-v03.txt;https://issues.apache.org/jira/secure/attachment/12700550/pig-4426-v03.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2015-02-24 22:29:02.657,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Feb 25 15:19:40 UTC 2015,,,,,,,0|i25u1j:,9223372036854775807,,,,,,,,,,"19/Feb/15 23:21;knoguchi;I haven't fully understood the Rank (mapreduce) implementation, but it seems like for simple(rownumber) rank, it runs POCounter on the mapper side whereas for others it uses reducer.

As a result, 
{code:title=JobControlCompiler.java}
 381     private void saveCounters(Job job, String operationID) {
...
 412             for (int i=0;i<job.getJob().getNumReduceTasks();i++) {
{code}

I believe we need to getNumMapTasks() for simple rownumber ranks.

Simple test (TestRank1) should have also failed but the results weren't compared for an exact match.  Changed the test a bit so that it will fail.
Running unit and e2e tests with this patch.
","23/Feb/15 20:48;knoguchi;From v01 patch, getting rid of unnecessary comparisons in tests and improving error messages.

To summarize,  the patch itself fixes,

Unit test TestRank3.testRankWithSplitIn\{Map,Reduce\} and E2E test Rank1,4,5,6.

Also the change to unit test TestRank\{1,2,3\} makes the test fail without the patch for TestRank1.testRank01RowNumber, TestRank1.testRank02RowNumber and TestRank3.testRankCascade  and succeed with the patch.

 ","24/Feb/15 19:13;knoguchi;In PIG-4392, it was pointed out that the trunk doesn't compile on hadoop-1.2. 
{noformat}
    [javac]             for (int i=0;i<job.getJob().getNumReduceTasks();i++) {
    [javac]                               ^
    [javac]   symbol:   method getJob()
    [javac]   location: variable job of type Job
{noformat}

My patch on this ticket (v01&v02) also had this issue.  Uploading another one which simply calls job.getJobConf().","24/Feb/15 22:29;daijy;+1 for v03. Sorry for introducing the test failure, thanks for the fix!",24/Feb/15 23:14;knoguchi;Thanks for the review [~daijy] !  Committed to trunk.,25/Feb/15 01:31;daijy;Also note current counter based approach is not scalable. It would easily hit system counter limit. Need to switch to a hdfs file based approach.,25/Feb/15 15:19;knoguchi;+1 on moving away from counter based approach. (I believe Tez version of Rank already does that.),,,,,,,,,,,,,,,,,,,,,
NullPointerException in JVMReuseImpl,PIG-4418,12774814,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,rohini,zjffdu,zjffdu,13/Feb/15 07:19,13/Nov/15 00:39,14/Mar/19 03:08,22/May/15 17:44,0.14.0,,,,,0.15.0,,,,,1,,,,,,,"{code}
2015-02-13 15:17:11,067 INFO [TezChild] task.TezTaskRunner: Encounted an error while executing task: attempt_1423730493153_0019_1_04_000002_0
java.lang.NullPointerException
	at org.apache.pig.JVMReuseImpl.cleanupStaticData(JVMReuseImpl.java:46)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.close(PigProcessor.java:175)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.close(LogicalIOProcessorRuntimeTask.java:338)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:181)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:166)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}",,,,,,,,,,,,,,,,,,,,15/Apr/15 17:51;rohini;PIG-4418-1.patch;https://issues.apache.org/jira/secure/attachment/12725638/PIG-4418-1.patch,20/May/15 20:11;rohini;PIG-4418-2.patch;https://issues.apache.org/jira/secure/attachment/12734223/PIG-4418-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-02-13 20:23:08.24,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Nov 13 00:39:10 UTC 2015,,,,,,,0|i25l2v:,9223372036854775807,,,,,,,,,,"13/Feb/15 20:23;rohini;[~zjffdu],
   Method seems to be null. Do you have steps to reproduce? ","16/Feb/15 08:31;zjffdu;[~rohini] It happens when I pig e2e tests on tez, e.g. (Cross_5), run it many times today, but could not reproduce it anymore. :(
Will update this jira if I meet it again. ",26/Mar/15 10:00;35niavlys;Same problem for me...,"26/Mar/15 10:03;rohini;[~35niavlys],
   Do you have a reproducible case?","26/Mar/15 14:24;35niavlys;No, I don't. It happens sometimes when I launch a job. I don't use any specific parameters...","09/Apr/15 11:09;Carlos Balduz;I'm having the same problem, although I cannot reproduce it either. It happens when I try to make a self join of a huge table... However after the failed task attempt, the job continues.","09/Apr/15 16:51;rohini;Fixing the NPE is easy. But would be nice to know why it is happening and if there is an underlying issue that needs to be fixed. Probably should just go through with the fix of doing a null check now as there are more reports. 

bq. However after the failed task attempt, the job continues.
    You mean the task goes through fine on retry?","10/Apr/15 07:45;Carlos Balduz;To be honest... I don't know. The job was TOO large so I ended up killing it. After that, I changed my code so that it handled less data and the error disappeared.",15/Apr/15 17:51;rohini;Handled cases where NPE could happen,"16/Apr/15 08:02;daijy;Rohini, did you rootcause it, or just a general prevention of NPE?",16/Apr/15 14:16;rohini;Just general prevention,"17/Apr/15 04:40;daijy;It should be good to prevent in general but in the mean time, can we add more logging and prominent warning message so if that happens again, we will know it and do some good analysis?","17/Apr/15 16:17;rohini;- LOG.warn(""Method is null which is not expected"");
   This logs in case the method is null and should be good.  The other case is when method.getDeclaringClass() is null. Came across that it can be null in Stackoverflow for anonymous classes. But at least in pig code we don't have any anonymous class that has the clean up annotation. 

Problem is these would be in task logs and no one is going to look at the messages unless you come across debugging another issue. What we can do is add logging to root cause it first and then when we know the cause go for this fix. Should I do that?",18/Apr/15 03:54;daijy;The question is why these methods have been added to JVMReuseManager initially? Can we add log when we add methods to JVMReuseManager? I'd like to have some idea about what exactly happens. I am fine if you do some general cover for NPE but I don't want to lose the track when this happens.,"20/May/15 13:34;harisekhon;I'm also getting hit by this with Pig on Tez while running large multi-hour jobs, each job is identical processing a single day's logs (hundreds of millions of records) and is parameterized changing only the date of the day to process on each run. Some jobs fail with this error a short way through their long runtime, it's not clear why some succeed and some fail intermittently however, here is an example failure:
{code}2015-05-20 14:13:41,905 [Timer-0] INFO  org.apache.pig.backend.hadoop.executionengine.tez.TezJob - DAG Status: status=RUNNING, progress=TotalTasks: 2002 Succeeded: 50 Running: 67 Failed: 0 Killed: 0, diagnostics=
2015-05-20 14:13:55,102 [PigTezLauncher-0] INFO  org.apache.pig.backend.hadoop.executionengine.tez.TezJob - DAG Status: status=FAILED, progress=TotalTasks: 2002 Succeeded: 50 Running: 0 Failed: 1 Killed: 1951 FailedTaskAttempts: 1, diagnostics=Vertex failed, vertexName=scope-26, vertexId=vertex_1432022048010_0042_1_00, diagnostics=[Task failed, taskId=task_1432022048010_0042_1_00_000044, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.NullPointerException
        at org.apache.pig.JVMReuseImpl.cleanupStaticData(JVMReuseImpl.java:44)
        at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.close(PigProcessor.java:174)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.close(LogicalIOProcessorRuntimeTask.java:334)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:178)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1432022048010_0042_1_00 [scope-26] killed/failed due to:null]
DAG failed due to vertex failure. failedVertices:1 killedVertices:0, counters=Counters: 16
        org.apache.tez.common.counters.DAGCounter
                NUM_FAILED_TASKS=1
                NUM_KILLED_TASKS=1951
                NUM_SUCCEEDED_TASKS=50
                TOTAL_LAUNCHED_TASKS=117
        File System Counters
                HDFS_BYTES_READ=5604569192
                HDFS_BYTES_WRITTEN=0
                HDFS_READ_OPS=50
                HDFS_LARGE_READ_OPS=0
                HDFS_WRITE_OPS=0
        org.apache.tez.common.counters.TaskCounter
                GC_TIME_MILLIS=84086
                CPU_MILLISECONDS=11549340
                PHYSICAL_MEMORY_BYTES=20238544896
                VIRTUAL_MEMORY_BYTES=178987933696
                COMMITTED_HEAP_BYTES=105483599872
                INPUT_RECORDS_PROCESSED=30867702
                OUTPUT_RECORDS=30867652
{code}","20/May/15 20:09;rohini;[~daijy],
   Removed the null check and logging the list of methods now so that we can first find how the error is happening.

[~harisekhon],
   If it is consistently reproducible for you, could you try the PIG-4418-2.patch and report the stack trace that you get with that?",20/May/15 20:15;daijy;+1,"22/May/15 10:55;harisekhon;Jobs failures were intermittent and seemed non-deterministic so I tried running re-running these jobs on mapreduce backend instead of tez and so far it's been much more reliable successfully indexing billions of docs for many jobs without any more failures with everything else remaining the same.

It may just be coincidence but I started having these problems around the time of upgrade from HDP 2.2.0.0 to HDP 2.2.4.2.

I'm actually migrating this pipeline from Pig to Spark so I'm not intending to do any more testing on this right now given it's working reliably when back on mapreduce.","22/May/15 17:44;rohini;Committed to trunk and branch-0.15. Thanks for the review Daniel.

Note: The final patch committed will only help identify what is the root cause of the issue and does not attempt to fix all the possible places of NPE. Since the NPE is not expected to happen in the first place and is random and not easily reproducible, it is better to identify the root cause in case the issue has something to do with some garbage collection.","13/Nov/15 00:39;rohini;With the log messages added in this jira, found the problem is that the  Method object added in instance.cleanupMethods.add(method); becomes null after sometime and so invoking them hits NPE in https://git.corp.yahoo.com/hadoop/pig/blob/trunk/src/org/apache/pig/JVMReuseImpl.java. I have not been able to figure out why that would become null and could not find anything online on why that can happen. Cannot skip calling clean up on those methods, as most of them are from key classes in Pig and I doubt that those classes went out of scope and they were unloaded. So need to let that task attempt fail and rerun (Tez does not reuse containers that hit an exception). Not sure if GC has anything to do with it. No -XX:+CMSClassUnloadingEnabled is set for classes to be unloaded. Have seen it with ParallelGC in JDK 7 and also JDK 8. So could not attribute it to PermGen space refactoring done with JDK 8 either.

First time I saw the error message, org.apache.pig.impl.util.UDFContext.cleanupStaticData had become null. In couple of recent cases, org.apache.pig.impl.PigContext.staticDataCleanup had become null.

{code}
2015-08-13 20:21:29,646 ERROR [TezChild] pig.JVMReuseImpl: Exception while calling static methods:null,org.apache.pig.impl.PigContext.staticDataCleanup,org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.staticDataCleanup,org.apache.pig.impl.util.SpillableMemoryManager.cleanupStaticData,org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce.staticDataCleanup,org.apache.pig.tools.pigstats.PigStatusReporter.staticDataCleanup. null

2015-11-03 14:27:17,672 [ERROR] [TezChild] |pig.JVMReuseImpl|: Exception while calling static methods:null,org.apache.pig.impl.util.UDFContext.cleanupStaticData,org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.staticDataCleanup,org.apache.pig.impl.util.SpillableMemoryManager.cleanupStaticData,org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce.staticDataCleanup,org.apache.pig.tools.pigstats.PigStatusReporter.staticDataCleanup. null

2015-11-12 17:21:53,577 [ERROR] [TezChild] |pig.JVMReuseImpl|: Exception while calling static methods:null,org.apache.pig.impl.util.UDFContext.cleanupStaticData,org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.staticDataCleanup,org.apache.pig.impl.util.SpillableMemoryManager.cleanupStaticData,org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce.staticDataCleanup,org.apache.pig.tools.pigstats.PigStatusReporter.staticDataCleanup. null



Full list in a normal run:
org.apache.pig.impl.PigContext.staticDataCleanup,org.apache.pig.impl.util.UDFContext.cleanupStaticData,org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.staticDataCleanup,org.apache.pig.impl.util.SpillableMemoryManager.cleanupStaticData,org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce.staticDataCleanup,org.apache.pig.tools.pigstats.PigStatusReporter.staticDataCleanup
{code}

One common thing I see with the errors is that the first element became null in both cases.

For now, filed PIG-4733 to at least call cleanup of builtin classes directly and avoid reflection as that will cover the regular run case and avoid this issue.",,,,,,,,
Fix testRankWithEmptyReduce in tez mode,PIG-4410,12772501,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,04/Feb/15 21:40,07/Jun/15 03:48,14/Mar/19 03:08,04/Feb/15 22:15,,,,,,0.15.0,,tez,,,0,,,,,,,"testRankWithEmptyReduce added in PIG-4392 failed in tez mode. The reason is POReservoirSample produce more sample than necessary. In particular, if the input of the vertex is empty, it produces a fake tuple which does not have the original data, but a marked field plus 0 rowNum. That cause the WeightedRangePartitioner fail:
{code}
Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer
	at org.apache.pig.backend.hadoop.HDataType.getWritableComparableTypes(HDataType.java:115)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPigNullableWritable(WeightedRangePartitioner.java:192)
{code}
Another issue I found is GetMemNumRows, I erroneously add the size of mark tuple, which make the size estimation inaccurate. I put the fix in the same patch.",,,,,,,,,,,,,,,,,,,,04/Feb/15 21:42;daijy;PIG-4410-1.patch;https://issues.apache.org/jira/secure/attachment/12696568/PIG-4410-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-02-04 21:48:06.899,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Feb 04 22:15:05 UTC 2015,,,,,,,0|i257d3:,9223372036854775807,,,,,,,,,,04/Feb/15 21:48;rohini;+1,04/Feb/15 22:15;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
fs.defaultFS is overwritten in JobConf by replicated join at runtime,PIG-4409,12772435,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,cheolsoo,cheolsoo,cheolsoo,04/Feb/15 17:46,07/Jun/15 03:47,14/Mar/19 03:08,04/Feb/15 18:43,0.14.0,,,,,0.14.1,0.15.0,impl,,,0,,,,,,,"This is a regression of PIG-4257.

Pig accidentally overwrites {{fs.defaultFS}} in JobConf during the replicated join at runtime. This can cause various side effects because udfs and store/load funcs might depend on the value of {{fs.defaultFS}} at runtime.

Here is an example. I have a store func that does 2-phase commit to S3. Each reducer writes output to local disk first and copies them to the final destination on S3 during the task commit phase. Once it's done copying, reducer writes a commit log to a hdfs location. During the job commit phase, AM reads all the commit logs and update Hive metastore accordingly.

This store func stopped working in 0.14 when there is a replicate join in the reduce phase. It is because {{fs.defaultFS}} is overwritten to local FS from HDFS by replicated join at runtime.

The root cause is that PIG-4257 changed {{ConfigurationUtil.getLocalFSProperties()}} to return a reference to JobConf instead of a copy object.",,,,,,,,,,,,,,,,,,,,04/Feb/15 17:48;cheolsoo;PIG-4409-1.patch;https://issues.apache.org/jira/secure/attachment/12696509/PIG-4409-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-02-04 18:36:13.108,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 04 18:43:26 UTC 2015,,,,,,,0|i256yn:,9223372036854775807,,,,,,,,,,04/Feb/15 17:48;cheolsoo;Uploading a patch that fixes the issue.,04/Feb/15 18:36;daijy;+1,04/Feb/15 18:43;cheolsoo;Thank you Daniel for the quick review. Committed to 0.14 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,
LOAD with HBaseStorage on secure cluster is broken in Tez,PIG-4404,12771438,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,30/Jan/15 21:59,07/Jun/15 03:47,14/Mar/19 03:08,31/Jan/15 00:29,0.14.0,,,,,0.14.1,0.15.0,tez,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Jan/15 22:26;rohini;PIG-4404-1.patch;https://issues.apache.org/jira/secure/attachment/12695626/PIG-4404-1.patch,31/Jan/15 00:19;rohini;PIG-4404-2.patch;https://issues.apache.org/jira/secure/attachment/12695664/PIG-4404-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-01-30 22:47:50.695,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Jan 31 00:29:25 UTC 2015,,,,,,,0|i250zb:,9223372036854775807,,,,,,,,,,30/Jan/15 22:47;daijy;+1,"31/Jan/15 00:19;rohini;Sorry. Had a older wip version of the patch uploaded. Updated the correct one. 

Changes if (lds != null) to (lds != null && lds.size() > 0) . Without that there will be ""There is more than one load for TezOperator"" exceptions for operators that don't have POLoad.",31/Jan/15 00:29;rohini;Committed to branch-0.14 and trunk (0.15). Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,
Combining -Dpig.additional.jars.uris with -useHCatalog breaks due to combination with colon instead of comma,PIG-4403,12771405,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ovlaere,ovlaere,ovlaere,30/Jan/15 19:14,07/Jun/15 03:48,14/Mar/19 03:08,31/Jan/15 04:57,0.14.0,,,,,0.15.0,,,,,0,,,,,,,"When adding a JSON serde parser as additional jar on running pig with HCatalog:

{noformat}
pig -useHCatalog -Dpig.additional.jars.uris=file:///usr/lib/hive/lib/json-serde-1.3.1-SNAPSHOT-jar-with-dependencies.jar
{noformat}

Pig crashes on 
{noformat}
ERROR 2999: Unexpected internal error. java.net.URISyntaxException: Relative path in absolute URI: hive-hcatalog-pig-adapter-0.14.0.2.2.0.0-2041.jar:file:
{noformat}

{noformat}
2015-01-30 18:21:23,848 [main] INFO  org.apache.pig.Main - Apache Pig version 0.14.0.2.2.0.0-2041 (rexported) compiled Nov 19 2014, 15:24:46
2015-01-30 18:21:23,848 [main] INFO  org.apache.pig.Main - Logging error messages to: /home/olivier/bsft_ds/data/pig.2015-01-30_18-21-22.log
2015-01-30 18:21:24,748 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /home/olivier/.pigbootup not found
2015-01-30 18:21:24,967 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://aws-usw2a-p-hw-master11.blueshift.vpc:8020
2015-01-30 18:21:26,214 [main] ERROR org.apache.pig.Main - ERROR 2999: Unexpected internal error. java.net.URISyntaxException: Relative path in absolute URI: hive-hcatalog-pig-adapter-0.14.0.2.2.0.0-2041.jar:file:
----------------------------
ERROR 2999: Unexpected internal error. java.net.URISyntaxException: Relative path in absolute URI: hive-hcatalog-pig-adapter-0.14.0.2.2.0.0-2041.jar:file:

java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: hive-hcatalog-pig-adapter-0.14.0.2.2.0.0-2041.jar:file:
	at org.apache.hadoop.fs.Path.initialize(Path.java:206)
	at org.apache.hadoop.fs.Path.<init>(Path.java:172)
	at org.apache.hadoop.fs.Path.<init>(Path.java:94)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:211)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1625)
	at org.apache.pig.impl.io.FileLocalizer.fetchFilesInternal(FileLocalizer.java:816)
	at org.apache.pig.impl.io.FileLocalizer.fetchFiles(FileLocalizer.java:767)
	at org.apache.pig.PigServer.registerJar(PigServer.java:558)
	at org.apache.pig.PigServer.addJarsFromProperties(PigServer.java:265)
	at org.apache.pig.PigServer.<init>(PigServer.java:231)
	at org.apache.pig.PigServer.<init>(PigServer.java:214)
	at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:46)
	at org.apache.pig.Main.run(Main.java:487)
	at org.apache.pig.Main.main(Main.java:170)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: hive-hcatalog-pig-adapter-0.14.0.2.2.0.0-2041.jar:file:
	at java.net.URI.checkPath(URI.java:1804)
	at java.net.URI.<init>(URI.java:752)
	at org.apache.hadoop.fs.Path.initialize(Path.java:203)
	... 19 more
================================================================================
{noformat}

Checking with -printDebugCmd I can see that the command looks like:

{noformat}
-Dpig.additional.jars.uris=file:///usr/hdp/2.2.0.0-2041/hive/lib/hive-metastore-0.14.0.2.2.0.0-2041.jar,file:///usr/hdp/2.2.0.0-2041/hive/lib/libthrift-0.9.0.jar,file:///usr/hdp/2.2.0.0-2041/hive/lib/hive-exec-0.14.0.2.2.0.0-2041.jar,file:///usr/hdp/2.2.0.0-2041/hive/lib/libfb303-0.9.0.jar,file:///usr/hdp/2.2.0.0-2041/hive/lib/jdo-api-3.0.1.jar,file://,file://,file:///usr/hdp/2.2.0.0-2041/hive/lib/hive-hbase-handler-0.14.0.2.2.0.0-2041.jar,file:///usr/hdp/2.2.0.0-2041/hive-hcatalog/share/hcatalog/hive-hcatalog-core-0.14.0.2.2.0.0-2041.jar,file://,file:///usr/hdp/2.2.0.0-2041/hive-hcatalog/share/hcatalog/hive-hcatalog-pig-adapter-0.14.0.2.2.0.0-2041.jar:file:/usr/lib/hive/lib/json-serde-1.3.1-SNAPSHOT-jar-with-dependencies.jar 
{noformat}

So it seems that 0.14 combines the -useHCatalog jars (comma separated), with -Dpig.additional.jars.uris (also comma separated) using a colon, and thus breaks the Jar classpath.

Running with either only -useHCatalg or -Dpig.additional.jars.uris does not result in any issues.

Could it be that 0.14 contains legacy code that combines these multiple flags with a symbol that breaks the path?
","Apache Pig version 0.14.0.2.2.0.0-2041 (rexported) compiled Nov 19 2014, 15:24:46
Hortonworks HDP 2.2.0.0-2041",,,,,,,,,,,,,,,,,,,31/Jan/15 00:42;daijy;PIG-4403-1.patch;https://issues.apache.org/jira/secure/attachment/12695683/PIG-4403-1.patch,31/Jan/15 00:49;ovlaere;PIG-4403-fix.patch;https://issues.apache.org/jira/secure/attachment/12695686/PIG-4403-fix.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-01-30 23:50:27.857,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Jan 31 04:57:07 UTC 2015,,,,,,,0|i250rb:,9223372036854775807,,,,,,,,,,"30/Jan/15 23:50;daijy;Can you try setting environment variable PIG_OPTS?

export PIG_OPTS=""-Dpig.additional.jars.uris=file:///usr/lib/hive/lib/json-serde-1.3.1-SNAPSHOT-jar-with-dependencies.jar""","31/Jan/15 00:38;ovlaere;Hi Daniel, i haven't tried that exact line, but similar things and it would break. I cloned the git repo of the pig source, and tried to figure out where the evil happens, and it's here: in line 191-193 of the bin/pig script, the following happens

{noformat}
ADDITIONAL_CLASSPATHS=file://$hiveMetaStoreVersion,file://$thriftVersion,file://$hiveExecVersion,file://$fbJarVersion,file://$jdoECJarVersion,file://$slfJarVersion,file://$hbaseHiveVersion,file://$hcatJarPath,file://$hbaseHCatJarPath,file://$pigHCatJarPath
  if [ ""$additionalJars"" != """" ]; then
    ADDITIONAL_CLASSPATHS=$ADDITIONAL_CLASSPATHS:$additionalJars
{noformat}

As you can see, the last line explicitly concats the defined HIVE/HCat jars with any additional jars using a colon, that will break the path, as ADDITIONAL_CLASSPATHS already contains colons.

The same thing occurs at line 358.

Replacing colon by comma in the script makes it work again.","31/Jan/15 00:42;daijy;Yes, we need to fix -D as well. Attach patch.","31/Jan/15 00:49;ovlaere;Replacing concatenation character from colon to comma in ADDITIONAL_CLASSPATHS, as colons can now occur in the URIs specific with -Dpig.additional.jars.uris and this avoids breaking the path.","31/Jan/15 00:53;ovlaere;Sorry Daniel, didn't see you already attached a patch, I read it as a request :)","31/Jan/15 04:57;daijy;Don't realize you want to submit the patch. The nice thing is the patches are the same :) That's the perfect way for code review.

Patch committed to trunk. Thanks Olivier!",,,,,,,,,,,,,,,,,,,,,,
JavaScript UDF example in the doc is broken,PIG-4402,12771070,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,cheolsoo,cheolsoo,cheolsoo,29/Jan/15 17:57,07/Jun/15 03:48,14/Mar/19 03:08,30/Jan/15 05:04,,,,,,0.15.0,,documentation,,,0,,,,,,,"The following example in the [JS udf doc|http://pig.apache.org/docs/r0.14.0/udf.html#js-udfs] throws an error, which is embarrassing-
{code}
complex.outputSchema = ""word:chararray,num:long"";
{code}",,,,,,,,,,,,,,,,,,,,29/Jan/15 17:58;cheolsoo;PIG-4402-1.patch;https://issues.apache.org/jira/secure/attachment/12695317/PIG-4402-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-01-30 01:44:39.911,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 30 05:04:06 UTC 2015,,,,,,,0|i24yqf:,9223372036854775807,,,,,,,,,,30/Jan/15 01:44;daijy;+1,30/Jan/15 05:04;cheolsoo;Committed to trunk. Thank you Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,
CSVExcelStorage incorrect output if last field value is null,PIG-4397,12770202,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,nielsbasjes,nielsbasjes,26/Jan/15 21:57,07/Jun/15 03:48,14/Mar/19 03:08,25/Feb/15 01:45,,,,,,0.15.0,,,,,0,,,,,,,"I have the following input:

{code}
one two
three
 four
{code}

I run this code
{code}
Lines =
    LOAD 'test.log' USING PigStorage(' ') 
    AS ( First:chararray , Second:chararray );

DUMP Lines;

STORE Lines INTO 'Lines'
USING org.apache.pig.piggybank.storage.CSVExcelStorage('\t', 'NO_MULTILINE', 'WINDOWS', 'WRITE_OUTPUT_HEADER');
{code}

The output from the DUMP is correct:
{code}
(one,two)
(three,)
(,four)
{code}
The output from the CSVExcelStorage is incorrect:
{code}
First   Second
one     two
three   three
        four
{code}

The problem is that if the last field is a null then the previous value is repeated incorrectly (in this case 'three').
",Running the Pig version bundled with HDP 2.1.2:   0.12.1.2.1.2.0-402,,,,,,,,,,,,,,,,,,,30/Jan/15 23:15;daijy;PIG-4397-1.patch;https://issues.apache.org/jira/secure/attachment/12695642/PIG-4397-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-02-06 20:46:13.424,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Feb 25 01:45:44 UTC 2015,,,,,,,0|i24tgf:,9223372036854775807,,,,,,,,,,"27/Jan/15 08:38;nielsbasjes;On a different machine (same pig version) I used this script
{code}
Lines =
    LOAD 'test.log' USING PigStorage(' ')
    AS ( A:chararray, B:chararray, C:chararray);

DUMP Lines;

STORE Lines INTO 'Lines'
USING org.apache.pig.piggybank.storage.CSVExcelStorage('\t', 'NO_MULTILINE', 'WINDOWS', 'WRITE_OUTPUT_HEADER');
{code}

This input
{code}
1 2 3
 4 5
6  7
8
 9
  10
{code}

DUMP:
{code}
(1,2,3)
(,4,5)
(6,,7)
(8,,)
(,9,)
(,,10)
{code}

CSV:
{code}
A       B       C
1       2       3
        4       5
6               7
8               8
        9       9
                10
{code}

Here the error is visible in the '8' and '9'.


",06/Feb/15 20:46;rohini;+1,"25/Feb/15 01:45;daijy;Patch committed to trunk. 

Thanks Niels for reporting the issue and Rohini for review!",,,,,,,,,,,,,,,,,,,,,,,,,
Fix Split_9 and Union_5 e2e failures,PIG-4394,12770004,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,26/Jan/15 05:46,07/Jun/15 03:48,14/Mar/19 03:08,28/Jan/15 17:35,,,,,,0.15.0,,,,,0,,,,,,,"Fails with 

{code}
Caused by: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POReservoirSample.getNextTuple(POReservoirSample.java:111)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:246)
{code}",,,,,,,,,,,,,,,,,,,,26/Jan/15 20:55;rohini;PIG-4394-1.patch;https://issues.apache.org/jira/secure/attachment/12694615/PIG-4394-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-01-27 23:03:41.329,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Jan 28 17:35:48 UTC 2015,,,,,,,0|i24sa7:,9223372036854775807,,,,,,,,,,26/Jan/15 20:55;rohini;Just a simple null check required. NPE introduced due to PIG-4366,27/Jan/15 23:03;daijy;+1,28/Jan/15 17:35;rohini;Committed to trunk (0.15). Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,,
RANK BY fails when default_parallel is greater than cardinality of field being ranked by,PIG-4392,12769896,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,erwaman,erwaman,24/Jan/15 21:32,07/Jun/15 03:48,14/Mar/19 03:08,04/Feb/15 22:14,0.11.1,,,,,0.15.0,,,,,0,,,,,,,"To reproduce:
{code:title=input.txt}
1 2 3
4 5 6
7 8 9
{code}
{code:title=rank.pig}
set default_parallel 4;

d = load 'input.txt' using PigStorage(' ') as (a:int, b:int, c:int);
e = rank d by a;
dump e;
{code}
If {{default_parallel}} is set to {{3}}, the script succeeds. So I'm guessing RANK BY has issues if the {{default_parallel}} exceeds the cardinality of the field being ranked by.

I'm seeing this issue with Pig 0.11.1 (which has the PIG-2932 patch applied) and Hadoop 2.3.0.",,,,,,,,,,,,,PIG-4410,,,,,,,30/Jan/15 22:32;daijy;PIG-4392-1.patch;https://issues.apache.org/jira/secure/attachment/12695628/PIG-4392-1.patch,04/Feb/15 21:33;daijy;PIG-4392-2.patch;https://issues.apache.org/jira/secure/attachment/12696565/PIG-4392-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-01-30 22:32:02.347,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Feb 25 00:50:11 UTC 2015,,,,,,,0|i24rm7:,9223372036854775807,,,,,,,,,,"30/Jan/15 22:32;daijy;Counter value 0 does not show up in the counter list. So for empty reduce, we didn't have the counter. Shall pad 0 for this case.","03/Feb/15 05:42;erwaman;Patch looks good to me. Just some minor comments and questions:
* Add a space after the semicolons in the for loop declaration: {{for (int i=0;i<job.getJob().getNumReduceTasks();i++) {}}
* Is the order of the tuples in {{iter}} in the test case guaranteed?
* Why does the order of the tuples get reversed?","04/Feb/15 11:13;azaroth;Yes, the order is guaranteed by RANK, which uses ORDER BY.
Not sure about the reversed order either, otherwise LGTM +1.","04/Feb/15 21:33;daijy;Thanks for the review. I found another issue when working on the patch. The sort job does not take the parallelism of rank operator. The new patch also fix this issue. The order reverse is caused by the logic Pig combining small splits. Small splits are reversely sorted by the part file size. In this case, part-00003 is the largest and part-00001 is the smallest. That's why the tuple in part-00003 (7 8 9) appears first.

I also find some issue in tez mode. Will create a separate ticket for it.","04/Feb/15 21:46;rohini;bq. Counter value 0 does not show up in the counter list. So for empty reduce, we didn't have the counter.
    Koji addressed this issue in PIG-4220 by initializing the counter value to 0. Does it still exist in 0.14?",04/Feb/15 21:57;rohini;+1. This is still a good fix to have on the client side in addition to PIG-4220.,"04/Feb/15 22:14;daijy;Patch committed to trunk. Thanks for review, Anthony, Gianmarco, Rohini!","04/Feb/15 22:20;daijy;One more clarification, Hadoop does not report the counter with value 0 in this case, so PIG-4220 does not work.","18/Feb/15 19:55;knoguchi;> One more clarification, Hadoop does not report the counter with value 0 in this case, so PIG-4220 does not work.
>
Hmm. It worked for us in hadoop 2.X.  So maybe it depends on the hadoop version.

Apart from this, I'm seeing test failures after this jira (on hadoopversion=23).

Unit tests
>>> org.apache.pig.test.TestRank3.testRankWithSplitInMap 	
>>> org.apache.pig.test.TestRank3.testRankWithSplitInReduce 

E2E tests
>>> Rank.Rank_1 
>>> Rank.Rank_4 
>>> Rank.Rank_5 
>>> Rank.Rank_6 

Haven't had time to take a look.","19/Feb/15 23:23;knoguchi;> Hmm. It worked for us in hadoop 2.X. So maybe it depends on the hadoop version.
>
Never mind.  PIG-4220 was handling a different case.

> Apart from this, I'm seeing test failures after this jira (on hadoopversion=23).
>
Opened PIG-4426.","24/Feb/15 17:25;erwaman;I'm also getting a build error when I run {{ant compile gen}} with this patch applied:
{code}
    [javac] /home/ahsu/github/apache/pig/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java:412: error: cannot find symbol
    [javac]             for (int i=0;i<job.getJob().getNumReduceTasks();i++) {
    [javac]                               ^
    [javac]   symbol:   method getJob()
    [javac]   location: variable job of type Job
{code}
I'm using Java 1.7 and Ant 1.8.4.","24/Feb/15 17:29;erwaman;Seems like {{Job.getJob()}} only exists in Hadoop 2 but not Hadoop 1:
* Hadoop 1: https://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/mapred/jobcontrol/Job.html
* Hadoop 2: https://hadoop.apache.org/docs/r2.3.0/api/org/apache/hadoop/mapred/jobcontrol/Job.html","24/Feb/15 17:31;erwaman;If I run {{ant compile gen -Dhadoopversion=23}}, it succeeds.","24/Feb/15 19:15;knoguchi;Thanks [~erwaman]. My patch in PIG-4426 also had the same issue.  Uploaded another one.  
Can you check to see if this works for you?","25/Feb/15 00:50;erwaman;Thanks for the fix, Koji.  Synced with trunk and now {{ant compile gen}} succeeds.",,,,,,,,,,,,,
Honor yarn settings in tez-site.xml and optimize dag status fetch,PIG-4387,12769091,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,21/Jan/15 20:18,07/Jun/15 03:47,14/Mar/19 03:08,23/Jan/15 05:45,0.14.0,,,,,0.15.0,,,,,0,,,,,,,"  When setting yarn.timeline-service.enabled=true in tez-site.xml, the value was still taken as false. This was because yarn.timeline-service.enabled=false in yarn-site.xml went into PigContext properties in Utils.recomputeProperties(jc, properties); and that took more precedence. ",,,,,,,,,,,,,,,,,,,,21/Jan/15 20:28;rohini;PIG-4387-1.patch;https://issues.apache.org/jira/secure/attachment/12693696/PIG-4387-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-01-21 21:04:29.201,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Jan 23 05:45:11 UTC 2015,,,,,,,0|i24msn:,9223372036854775807,,,,,,,,,,"21/Jan/15 20:28;rohini;Created PIG-4388 to do cleanup of the PigContext properties and Configuration usage as it needs more work and thorough verification. Here just making a simple fix. 

Also optimized the dag status fetch to not fetch counters.

","21/Jan/15 21:04;daijy;That's because tez-site.xml is introduced very late (construct of TezJobCompiler). You will not get tez config before that. Sounds ""yarn.timeline-service.enabled"" is a yarn-site.xml entry, isn't it?","21/Jan/15 21:55;rohini;this.tezConf = new TezConfiguration(conf);  - This tezConf in TezJobCompiler is the one used for the DAG AM. But problem is even though tez-site.xml is added last, it does not override because the setting does not come from yarn-site.xml in conf but through config.set() from the properties in ConfigurationUtil.toConfiguration(Properties properties, boolean loadDefaults). If PigContext properties did not have the yarn-site.xml settings loaded into it, it would have worked fine. So had to load tez-site.xml also into PigContext properties.

bq. Sounds ""yarn.timeline-service.enabled"" is a yarn-site.xml entry, isn't it?
  Yes. As we only wanted the timeline service enabled for tez jobs, trying to enable it only in tez-site.xml.
",21/Jan/15 22:03;daijy;Thanks for explanation. +1,23/Jan/15 05:45;rohini;Committed to trunk. Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,
TezLauncher thread should be deamon thread ,PIG-4384,12768001,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjffdu,zjffdu,zjffdu,16/Jan/15 12:23,07/Jun/15 03:48,14/Mar/19 03:08,17/Jan/15 03:49,,,,,,0.15.0,,tez,,,0,,,,,,,"The following piece of code would hang there because TezLauncher thread is not deamon thread. 

{code}
  public static void main(String[] args) throws IOException,
      InterruptedException {
    FileSystem fs = FileSystem.get(new Configuration());
    fs.delete(new Path(""/tmp/output""), true);
    PigServer pig = new PigServer(new TezExecType());
    pig.registerScript(""scripts/test.pig"");
  }
{code}",,,,,,,,,,,,,,,,,,,,16/Jan/15 12:23;zjffdu;PIG_4384_1.patch;https://issues.apache.org/jira/secure/attachment/12692747/PIG_4384_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-01-17 03:49:16.808,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Jan 17 03:49:16 UTC 2015,,,,,,,0|i24gdr:,9223372036854775807,,,,,,,,,,17/Jan/15 03:49;daijy;Patch committed to trunk. Thanks Jeff!,,,,,,,,,,,,,,,,,,,,,,,,,,,
 PIG grunt shell DEFINE commands fails when it spans multiple lines,PIG-4381,12767805,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,15/Jan/15 18:46,07/Jun/15 03:47,14/Mar/19 03:08,20/Jan/15 22:11,,,,,,0.15.0,,grunt,,,0,,,,,,,"When user executes a define command from the grunt shell and if the command spans across multiple lines the following exception is thrown:

{code}
2014-12-23 11:29:32,637 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Lexical error at line 2, column 0. Encountered: <EOF> after : """" 
Details at logfile: /home/demo/hdp-datascience-demo/demo/pig_1419334148891.log 
grunt> DEFINE preprocess(year_str, airport_code) returns data 
2014-12-23 11:29:43,802 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Lexical error at line 2, column 0. Encountered: <EOF> after : """" 
Details at logfile: /home/demo/hdp-datascience-demo/demo/pig_1419334148891.log 
{code}

The same command works fine with a Pig script file.",,,,,,,,,,,,,,,,,,,,15/Jan/15 19:02;daijy;PIG-4381-1.patch;https://issues.apache.org/jira/secure/attachment/12692564/PIG-4381-1.patch,19/Jan/15 06:11;daijy;PIG-4381-2.patch;https://issues.apache.org/jira/secure/attachment/12693016/PIG-4381-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-01-20 00:41:31.104,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jan 20 22:11:35 UTC 2015,,,,,,,0|i24f8v:,9223372036854775807,,,,,,,,,,"15/Jan/15 19:02;daijy;Here is how it happens:
PIG-2122 do parameter substitution line by line in Grunt. However, PigFileParser (parameter substitution processor) keep track of multiline define statement and skip parameter substitution inside define, so line by line approach does not work here.

New patch process the parameter substitution after GruntParser, and we don't do parameter substitution line by line, instead, statement by statement. One complexity here is GruntParser does not recognize %default and %declare before, this is preprocessed by PigFileParser, now it need to accept them and send to PigFileParser to process.",19/Jan/15 06:11;daijy;There are some problem in the previous patch. Update the patch.,20/Jan/15 00:41;rohini;+1,20/Jan/15 22:11;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
Skewed outer join produce wrong result if a key is oversampled,PIG-4377,12766541,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,09/Jan/15 21:38,18/Jan/16 17:27,14/Mar/19 03:08,27/May/15 19:47,,,,,,0.15.0,,impl,,,0,,,,,,,Skewed outer join produce more than expected rows under certain condition. The extra rows contain null left relation. Can be reproduced reliably with reproduce.patch (run SkewedJoin_11).,,,,,,,,,,,,,,,PIG-4587,PIG-4580,PIG-4541,,,09/Jan/15 21:57;daijy;PIG-4377-1.patch;https://issues.apache.org/jira/secure/attachment/12691402/PIG-4377-1.patch,12/Jan/15 22:15;daijy;PIG-4377-2.patch;https://issues.apache.org/jira/secure/attachment/12691765/PIG-4377-2.patch,15/May/15 05:58;daijy;PIG-4377-3.patch;https://issues.apache.org/jira/secure/attachment/12733068/PIG-4377-3.patch,27/May/15 18:49;daijy;PIG-4377-4.patch;https://issues.apache.org/jira/secure/attachment/12735657/PIG-4377-4.patch,09/Jan/15 21:38;daijy;reproduce.patch;https://issues.apache.org/jira/secure/attachment/12691396/reproduce.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2015-05-08 15:13:07.261,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed May 27 19:47:40 UTC 2015,,,,,,,0|i247p3:,9223372036854775807,,,,,,,,,,"09/Jan/15 21:57;daijy;Attach a fix.

Here is what happens:
1. Certain key x is sampled (by PoissonSampleLoader/PartitionSkewedKeys) to have y reduces
2. Actually, only y1 < y records carry key x
3. There are reduce which suppose to get key x does not get row with key x
4. The reduce does not get x will generate redundant empty left relation (CompilerUtils.addEmptyBagOuterJoin)

What the patch does is:
Only generate empty left relation in the first reduce of key x",12/Jan/15 22:15;daijy;There is still some issue in tez mode. Attach another fix.,"08/May/15 15:13;rohini;+1.   

[~daijy],
   Can you put more details in the description about the problem inplace of ""under certain condition"" and also add a small description of what the fix does? It would be easy for future reference instead of having to read through the patch.",11/May/15 20:38;daijy;Changed the title to be more restrictive. Patch committed to both 0.15 branch and trunk. Thanks Rohini for review!,15/May/15 05:58;daijy;TestTezCompiler is broken due to the patch. I need to revert some part of the patch. It seems the change is unnecessary and I don't remember how this comes to the picture. Attach PIG-4377-3.patch.,15/May/15 14:37;rohini;+1,16/May/15 00:08;daijy;PIG-4377-3.patch committed to 0.15 and trunk. Thanks Rohini!,"27/May/15 17:38;daijy;Actually PIG-4377-3.patch is needed, since IsFirstReduceOfKey of the join vertex need sample input. Attach PIG-4377-4.patch to bring it back, add comments and fix TestTezCompiler failures.",27/May/15 19:28;rohini;+1,27/May/15 19:47;daijy;PIG-4377-4.patch committed to both 0.15 branch and trunk. Thanks Rohini!,,,,,,,,,,,,,,,,,,
NullPointerException accessing a field of an invalid bag from a nested foreach,PIG-4376,12766539,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,kspringborn,kspringborn,kspringborn,09/Jan/15 21:32,07/Jun/15 03:47,14/Mar/19 03:08,15/Jan/15 19:50,0.12.0,0.13.0,0.14.0,,,0.15.0,,parser,,,0,,,,,,,"{noformat}
a = load 'x' as (field);
b = foreach a {
  c = LIMIT invalidName 1;
  generate c.foo;
}
{noformat}

Results in the following stack trace:
[main] ERROR org.apache.pig.PigServer - exception during parsing: Error during parsing. null
Failed to parse: null
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:201)
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1735)
        at org.apache.pig.PigServer$Graph.access$000(PigServer.java:1443)
        at org.apache.pig.PigServer.parseAndBuild(PigServer.java:387)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:412)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:398)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:171)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:741)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:372)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
        at org.apache.pig.Main.run(Main.java:495)
        at org.apache.pig.Main.main(Main.java:170)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.NullPointerException
        at org.apache.pig.newplan.logical.expression.DereferenceExpression.translateAliasToPos(DereferenceExpression.java:210)
        at org.apache.pig.newplan.logical.expression.DereferenceExpression.getFieldSchema(DereferenceExpression.java:149)
        at org.apache.pig.newplan.logical.optimizer.FieldSchemaResetter.execute(SchemaResetter.java:264)
        at org.apache.pig.newplan.logical.expression.AllSameExpressionVisitor.visit(AllSameExpressionVisitor.java:148)
        at org.apache.pig.newplan.logical.expression.DereferenceExpression.accept(DereferenceExpression.java:84)
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visitAll(SchemaResetter.java:67)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:122)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:245)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:114)
        at org.apache.pig.parser.LogicalPlanBuilder.buildForeachOp(LogicalPlanBuilder.java:1055)
        at org.apache.pig.parser.LogicalPlanGenerator.foreach_clause(LogicalPlanGenerator.java:15896)
        at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1933)
        at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1102)
        at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:560)
        at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:191)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-01-15 19:50:21.761,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Jan 15 19:50:21 UTC 2015,,,Patch Available,,,,0|i247ov:,9223372036854775807,,,,,,,,,,"09/Jan/15 21:39;kspringborn;Index: test/org/apache/pig/parser/TestColumnAliasConversion.java
===================================================================
--- test/org/apache/pig/parser/TestColumnAliasConversion.java	(revision 1644458)
+++ test/org/apache/pig/parser/TestColumnAliasConversion.java	(working copy)
@@ -159,6 +159,22 @@
         Assert.fail( ""Query should fail to validate."" );
     }
 
+    @Test
+    public void testInvalidNestedProjection() throws Exception {
+        String query = ""A = load 'x' as (field);"" +
+                       ""B = foreach A {"" +
+                       ""  C = LIMIT invalidName 1;"" +
+                       ""  generate C.foo;"" +
+                       ""};"";
+        try {
+            validate( query );
+        } catch(PlanValidationException ex) {
+            System.out.println(ex.getMessage());
+            return;
+        }
+        Assert.fail( ""Query should fail to validate."" );
+    }
+
     private LogicalPlan validate(String query) throws RecognitionException, ParsingFailureException, IOException {
         LogicalPlan plan = ParserTestingUtils.generateLogicalPlan( query );
         ColumnAliasConversionVisitor visitor = new ColumnAliasConversionVisitor( plan );
Index: src/org/apache/pig/newplan/logical/expression/DereferenceExpression.java
===================================================================
--- src/org/apache/pig/newplan/logical/expression/DereferenceExpression.java	(revision 1644458)
+++ src/org/apache/pig/newplan/logical/expression/DereferenceExpression.java	(working copy)
@@ -206,7 +206,7 @@
             	    throw new FrontendException(""Index ""+rawColumn + "" out of range in schema:"" + schema.toString(false), 1127);
             	}
                 columns.add( (Integer)rawColumn );
-            } else {
+            } else if (schema!=null) {
                 int pos = schema.getFieldPosition((String)rawColumn);
                 if( pos != -1) {
                     columns.add( pos );
",15/Jan/15 19:50;daijy;Patch committed to trunk. Thanks Kevin!,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate snappy.version in libraries.properties,PIG-4371,12765442,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,07/Jan/15 19:06,07/Jun/15 03:48,14/Mar/19 03:08,07/Jan/15 21:13,,,,,,0.15.0,,build,,,0,,,,,,,"There is a duplicate snappy.version entries in libraries.properties, and values are different. Pig ends up pull the wrong version of snappy-java.",,,,,,,,,,,,,,,,,,,,07/Jan/15 19:07;daijy;PIG-4371-1.patch;https://issues.apache.org/jira/secure/attachment/12690589/PIG-4371-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-01-07 19:26:30.781,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Jan 07 21:21:05 UTC 2015,,,,,,,0|i241mv:,9223372036854775807,,,,,,,,,,07/Jan/15 19:26;lbendig;+1,07/Jan/15 21:13;daijy;Patch committed to trunk. Thanks Lorand for review!,"07/Jan/15 21:21;lbendig;Daniel, thanks for fixing it!",,,,,,,,,,,,,,,,,,,,,,,,,
Fix perl script problem in TestStreaming.java,PIG-4361,12763957,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,29/Dec/14 01:35,07/Jun/15 03:47,14/Mar/19 03:08,02/Jan/15 15:47,,,,,,0.15.0,,,,,0,,,,,,,"TestStreaming.java  line 495
{code}
  // Perl script
	    String[] script =
	        new String[] {
	                      ""#!/usr/bin/perl"",
                          ""open(OUTFILE, \"">\"", $ARGV[0]) or die \""Can't open \"".$ARGV[1].\""!: $!\"";"",
                          ""open(OUTFILE2, \"">\"", $ARGV[1]) or die \""Can't open \"".$ARGV[2].\""!: $!\"";"",
                          ""while (<STDIN>) {"",
                          ""  print OUTFILE \""$_\n\"";"",
                          ""  print STDERR \""STDERR: $_\n\"";"",
                          ""  print OUTFILE2 \""A,10\n\"";"",
                          ""}"",
	                     };
{code}
this script has some problem when dealing with the input arguments. made some modifications:
{code}
  // Perl script
	    String[] script =
	        new String[] {
	                      ""#!/usr/bin/perl"",
                          ""open(OUTFILE, \"">\"", $ARGV[0]) or die \""Can't open \"".$ARGV[0].\""!: $!\"";"",
                          ""open(OUTFILE2, \"">\"", $ARGV[1]) or die \""Can't open \"".$ARGV[1].\""!: $!\"";"",
                          ""while (<STDIN>) {"",
                          ""  print OUTFILE \""$_\n\"";"",
                          ""  print STDERR \""STDERR: $_\n\"";"",
                          ""  print OUTFILE2 \""A,10\n\"";"",
                          ""}"",
	                     };
{code}
The same problem happens in line 554.",,,,,,,,,,,,,,,,,,,,29/Dec/14 01:36;kellyzly;PIG-4361.patch;https://issues.apache.org/jira/secure/attachment/12689274/PIG-4361.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-12-30 04:22:07.111,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 02 15:47:59 UTC 2015,,,,,,,0|i23t3j:,9223372036854775807,,,,,,,,,,"29/Dec/14 01:38;kellyzly;patch available, can anyone help review it? Very thanks",30/Dec/14 04:22;brocknoland;[~xuefuz] could you look at this one?,30/Dec/14 16:06;xuefuz;+1,02/Jan/15 15:47;xuefuz;Committed to trunk. Thanks to Liyun for the contribution.,,,,,,,,,,,,,,,,,,,,,,,,
"Piggybank:  XPath cant handle namespace in xpath, nor can it return more than one match",PIG-4355,12760947,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cavanaug,cavanaug,cavanaug,11/Dec/14 06:05,07/Jun/15 03:47,14/Mar/19 03:08,15/Jan/15 19:20,0.14.0,,,,,0.15.0,,piggybank,,,0,,,,,,,"If you pass an xpath that contains a namespace the XPath UDF will always fail to match.

It would be better to either silently remove the namespace or provide a parameter that will remove it.

The reason it is desirable to ignore xpath's with namespaces is that many xml tools when selecting an xpath provide the namespace.   It makes cutting & pasting into a pig script painful if you need to manually remove it.

Additionally XPath only returns the *first* match.   It is often desirable to return all matches and allow for a flattening to process multiple records.   An XPathAll would be useful to have.

A patch is available as a git pullrequest at
     https://github.com/apache/pig/pull/14",,,,,,,,,,,,,,,,,,,,15/Jan/15 19:20;daijy;14.diff;https://issues.apache.org/jira/secure/attachment/12692567/14.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-01-06 03:10:01.885,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Jan 15 19:20:35 UTC 2015,,,Patch Available,,,,0|i23b47:,9223372036854775807,,,,,,,,,,06/Jan/15 03:10;daijy;Can you generate a diff file? Pig does not use pull request to track patches.,"14/Jan/15 18:27;cavanaug;Daniel, you can get a diff at https://github.com/apache/pig/pull/14.diff   (Sneaky little github feature to add "".diff"" at the end)

The diff is for the 0.14 release but should apply against trunk.

","15/Jan/15 19:20;daijy;Pull the patch and attach to Jira since Pig need to leave a self-contained trace for the patch.

Patch committed to trunk cleanly. Thanks John!",,,,,,,,,,,,,,,,,,,,,,,,,
TestPigRunner.simpleTest2 fail on trunk,PIG-4351,12758950,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,02/Dec/14 17:58,07/Jun/15 03:48,14/Mar/19 03:08,09/Dec/14 18:27,,,,,,0.15.0,,impl,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Dec/14 17:53;daijy;PIG-4351-1.patch;https://issues.apache.org/jira/secure/attachment/12685786/PIG-4351-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-12-09 18:21:07.223,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Dec 09 18:27:10 UTC 2014,,,,,,,0|i22z2n:,9223372036854775807,,,,,,,,,,09/Dec/14 18:21;cheolsoo;+1,09/Dec/14 18:27;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
"e2e test ""RubyUDFs_13"" fails because of the different result of ""group a all"" in different engines like ""spark"", ""mapreduce""",PIG-4345,12758111,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,27/Nov/14 02:06,07/Jun/15 03:48,14/Mar/19 03:08,01/Dec/14 19:42,,,,,,0.15.0,,,,,0,,,,,,,"RubyUDFs e2e scrip is on the line 3818 of nightly.conf : 
{code}
                    'num' => 13,
                    'java_params' => ['-Dpig.accumulative.batchsize=5'],
                    'pig' => q\
register ':SCRIPTHOMEPATH:/ruby/morerubyudfs.rb' using jruby as myfuncs;
a = load ':INPATH:/singlefile/studenttab10k' using PigStorage() as (name, age:int, gpa:double);
b = foreach (group a all) generate FLATTEN(myfuncs.AppendIndex(a));
store b into ':OUTPATH:';\,
                    'verify_pig_script' => q\
register :FUNCPATH:/testudf.jar;
a = load ':INPATH:/singlefile/studenttab10k' using PigStorage() as (name, age:int, gpa:double);
b = foreach (group a all) generate FLATTEN(org.apache.pig.test.udf.evalfunc.AppendIndex(a));
store b into ':OUTPATH:';\,
                    },
                ]
            },
{code}
RubyUDFs_13.pig tests ruby udf ""AppendIndex"" in ""morerubyudfs.rb"".  The output is compared with verified script which use java udf ""org.apache.pig.test.udf.evalfunc.AppendIndex"". The output of ""RubyUDFs_13.pig"" is like following:

If test file “studemttab10k” is 

tom thompson	42	0.53
nick johnson	34	0.47
priscilla falkner	55	1.16

the result in spark engine will be:
tom thompson	42	0.53   1
nick johnson	34	0.47   2
priscilla falkner	55	1.16  3


the result in mapreduce engine which verified script uses  will be 
priscilla falkner	55	1.16  1
nick johnson	34	0.47  2
tom thompson	42	0.53  3

The difference between the result in spark and mapreduce engine cause RubyUDFs_13 e2e test failure .
The root cause of the difference is because “group a all” has  different result in different engines. 
 In Spark engine, “group a all” :
all { (tom thompson	42	0.53),( nick johnson	34	0.47),( priscilla falkner	55	1.16)}
In mapreduce engine , “group a all”:
all {( priscilla falkner	55	1.16), ( nick johnson	34	0.47),(tom thompson	42	0.53)}

Using PIG-4345.patch, RubyUDF_13 e2e test passes.
{code}
{
                    'num' => 13,
                    'java_params' => ['-Dpig.accumulative.batchsize=5'],
                    'pig' => q\
register ':SCRIPTHOMEPATH:/ruby/morerubyudfs.rb' using jruby as myfuncs;
a = load ':INPATH:/singlefile/studenttab10k' using PigStorage() as (name, age:int, gpa:double);
a1 = filter a by name == 'nick johnson';
a2 = filter a1 by age == 34;
b =  foreach (group a2 all) generate FLATTEN(myfuncs.AppendIndex(a2));
store b into ':OUTPATH:';\,
                    'verify_pig_script' => q\
register :FUNCPATH:/testudf.jar;
a = load ':INPATH:/singlefile/studenttab10k' using PigStorage() as (name, age:int, gpa:double);
a1 = filter a by name == 'nick johnson';
a2 = filter a1 by age == 34;
b =  foreach (group a2 all) generate FLATTEN(org.apache.pig.test.udf.evalfunc.AppendIndex(a2));
store b into ':OUTPATH:';\,
                    },
                ]
            },
{code}

using PIG-4345.patch, the result in spark and mapreduce engine will be:

nick johnson	34	0.47  1
",,,,,,,,,,,,,,,,,,,,27/Nov/14 02:08;kellyzly;PIG-4345.patch;https://issues.apache.org/jira/secure/attachment/12683978/PIG-4345.patch,01/Dec/14 08:24;kellyzly;PIG-4345_1.patch;https://issues.apache.org/jira/secure/attachment/12684366/PIG-4345_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-12-01 03:10:37.918,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Dec 01 19:42:33 UTC 2014,,,,,,,0|i22tzj:,9223372036854775807,,,,,,,,,,"01/Dec/14 03:10;rohini;Can you just add a order by in nested foreach instead? i.e

{code}
b =  foreach (group a all) {
 a1= ORDER a by name;
 generate FLATTEN(myfuncs.AppendIndex(a1));
}
{code}","01/Dec/14 08:24;kellyzly;Thanks [~rohini]'s comment.  I have uploaded PIG-4345_1.patch.  The changes are:
{code}
 register ':SCRIPTHOMEPATH:/ruby/morerubyudfs.rb' using jruby as myfuncs;
    a = load ':INPATH:/singlefile/studenttab10k' using PigStorage() as (name, age:int, gpa    :double);
   -b = foreach (group a all) generate FLATTEN(myfuncs.AppendIndex(a));
  +b = foreach (group a all){
  +   a1= order a by name,age,gpa;
  +   generate FLATTEN(myfuncs.AppendIndex(a1));
  +}
{code}

When ordering by name,age and gpa these three field, the output of b in 'spark' and 'mapreduce' engines are same.","01/Dec/14 08:25;kellyzly;Hi [~rohini], can you help review PIG-4345_1.patch?",01/Dec/14 19:42;rohini;+1. Committed to trunk (0.15). Thanks Liyun.,,,,,,,,,,,,,,,,,,,,,,,,
PigStorage fails parsing empty map.,PIG-4340,12756908,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,akirakiron,akirakiron,21/Nov/14 06:24,07/Jun/15 03:48,14/Mar/19 03:08,20/Jan/15 23:24,,,,,,0.15.0,,impl,,,0,,,,,,,"I've found that PigStorage doesn't parse empty maps properly.
I'm using pig-0.11.0-cdh4.4.0, but reading the source code, it would be reproduced in the later versions.

An empty map in a field of a tuple is parsed as null.
{code:title=test.txt}
empty []
nonempty [foo#bar]
{code}
{code:title=test.pig}
A = LOAD '/tmp/test.txt' USING PigStorage(' ') AS (a:chararray, b:map[chararray]);
DUMP A;
{code}
{code}
$ pig test.pig
...
(empty,)
(nonempty,[foo#bar])
{code}

Moreover, if the empty map is nested in a parent field, the entire field is interpreted as null.
{code:title=test-nested.txt}
empty (f1,[])
nonempty (f1,[foo#bar])
{code}
{code:title=test.pig}
A = LOAD '/tmp/test.txt' USING PigStorage(' ') AS (a:chararray, (b:chararray, b:map[chararray]));
DUMP A;
{code}
{code}
$ pig test.pig
...
(empty,)
(nonempty,(f1,[foo#bar]))
{code}

Investigating this, I've found it is because {{Utf8StorageConverter#consumeMap}} throws {{IOException}} when it receives empty map as string '[]'. It seems like always assuming there should be a content of map, more specifically '#' character.
{code:java}
    private Map<String, Object> consumeMap(PushbackInputStream in, ResourceFieldSchema fieldSchema) throws IOException {
        int buf;
        
        while ((buf=in.read())!='[') {
            if (buf==-1) {
                throw new IOException(""Unexpect end of map"");
            }
        }
        HashMap<String, Object> m = new HashMap<String, Object>();
        ByteArrayOutputStream mOut = new ByteArrayOutputStream(BUFFER_SIZE);
        while (true) {
            // Read key (assume key can not contains special character such as #, (, [, {, }, ], )
            while ((buf=in.read())!='#') {
                if (buf==-1) {
                    throw new IOException(""Unexpect end of map"");
                }
                mOut.write(buf);
            }
            String key = bytesToCharArray(mOut.toByteArray());
            if (key.length()==0)
                throw new IOException(""Map key can not be null"");
{code}

I would appreciate if you could fix this problem.
Thanks.",,,,,,,,,,,,,,,,,,,,24/Nov/14 22:13;daijy;PIG-4340-1.patch;https://issues.apache.org/jira/secure/attachment/12683431/PIG-4340-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-24 09:34:32.061,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jan 20 23:24:22 UTC 2015,,,,,,,0|i22n2v:,9223372036854775807,,,,,,,,,,"24/Nov/14 09:34;mprim;This sounds related to the two bugs I reported myself recently with using maps:

https://issues.apache.org/jira/browse/PIG-4327
https://issues.apache.org/jira/browse/PIG-4326",20/Jan/15 23:17;rohini;+1,20/Jan/15 23:24;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
e2e test framework assumes default exectype as mapred,PIG-4339,12756493,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,19/Nov/14 18:52,07/Jun/15 03:48,14/Mar/19 03:08,06/Jan/15 15:39,,,,,,0.15.0,,,,,0,,,,,,,"{code}
if ($testCmd->{'exectype'} eq ""tez"") {
        push(@baseCmd, (""-x"", ""tez""));
    }
{code}

It should just push the exectype.",,,,,,,,,,,,,,,,,,,,06/Jan/15 00:52;rohini;PIG-4339-1.patch;https://issues.apache.org/jira/secure/attachment/12690203/PIG-4339-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-01-06 00:56:35.726,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Jan 06 15:39:13 UTC 2015,,,,,,,0|i22km7:,9223372036854775807,,,,,,,,,,06/Jan/15 00:52;rohini;Also updated the junit xmlReport.pl to group tests based on exec type.,06/Jan/15 00:56;daijy;+1,06/Jan/15 15:39;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,
Fix test failures with JDK8,PIG-4338,12756467,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,19/Nov/14 17:33,07/Jun/15 03:48,14/Mar/19 03:08,09/Dec/14 20:41,,,,,,0.15.0,,,,,0,,,,,,,"See 15 failures
1) TestTezJobControlCompiler.testTezParallelismEstimatorSplitBranch
junit.framework.AssertionFailedError: expected:<-1> but was:<7>
	at org.apache.pig.tez.TestTezJobControlCompiler.testTezParallelismEstimatorSplitBranch(TestTezJobControlCompiler.java:272)

2) TestAvroStorage.testLoadRecordsWithMaps
3) TestTezCompiler - 13 failures.

2 and 3 are due to relying on hashmap order",,,,,,,,,,,,,,,,,,,,08/Dec/14 22:48;rohini;PIG-4338-1.patch;https://issues.apache.org/jira/secure/attachment/12685874/PIG-4338-1.patch,09/Dec/14 20:40;rohini;PIG-4338-2.patch;https://issues.apache.org/jira/secure/attachment/12686094/PIG-4338-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Dec 09 20:41:06 UTC 2014,,,,,,,0|i22kgf:,9223372036854775807,,,,,,,,,,"08/Dec/14 22:48;rohini;Changes done:
   - Fixed test failures caused by HashSet iteration order change in JDK 8.  
   - Changed javac version to 1.6 in build.xml. Should be changing it to 1.7 before we release pig 0.15. But for now at least made it 1.6
   - Moved threadpool initialization to launchPig in TezLauncher. It was unnecessarily creating threadpool even for explain (TestTezCompiler). ",09/Dec/14 20:40;rohini;Updated javac.version to 1.7 based on reviewboard comments.,09/Dec/14 20:41;rohini;Committed to trunk (0.15). Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,
Pig release tarball miss tez classes,PIG-4335,12755695,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,17/Nov/14 01:31,21/Nov/14 05:58,14/Mar/19 03:08,17/Nov/14 01:37,0.14.0,,,,,0.14.0,,build,,,0,,,,,,,tez classes is missing in pig-0.14.0-SNAPSHOT-core-h2.jar.,,,,,,,,,,,,,,,,,,,,17/Nov/14 01:32;daijy;PIG-4335-1.patch;https://issues.apache.org/jira/secure/attachment/12681817/PIG-4335-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-17 01:34:05.793,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Nov 17 01:37:09 UTC 2014,,,,,,,0|i22fsf:,9223372036854775807,,,,,,,,,,17/Nov/14 01:34;rohini;+1,17/Nov/14 01:37;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for capturing this and review the patch!,,,,,,,,,,,,,,,,,,,,,,,,,,
"update README, '-x' option in usage to include tez",PIG-4331,12755517,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,thejas,thejas,14/Nov/14 23:18,07/Jun/15 03:48,14/Mar/19 03:08,28/Apr/15 22:52,0.14.0,,,,,0.14.1,0.15.0,,,,0,,,,,,,"Pig queries can be run using tez, by specifying ""pig -x tez"". The output of pig --help needs to be updated to indicate that.",,,,,,,,,,,,,,,,,,,,14/Nov/14 23:32;thejas;PIG-4331.1.patch;https://issues.apache.org/jira/secure/attachment/12681648/PIG-4331.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-14 23:37:20.334,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Apr 28 22:52:39 UTC 2015,,,,,,,0|i22epb:,9223372036854775807,,,,,,,,,,"14/Nov/14 23:37;daijy;Patch committed to trunk, thanks Thejas!

We shall also backport to 0.14 branch once 0.14.0 is release. Keep the ticket open.",28/Apr/15 22:52;daijy;Patch committed to 0.14 branch as well.,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression test for PIG-3584 - AvroStorage does not correctly translate arrays of strings,PIG-4330,12755158,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,brocknoland,brocknoland,brocknoland,13/Nov/14 20:55,07/Jun/15 03:48,14/Mar/19 03:08,13/Nov/14 21:51,,,,,,0.15.0,,,,,0,,,,,,,,,,,,,,,,,,,,PIG-3584,,,,,,,13/Nov/14 21:38;brocknoland;PIG-4330.patch;https://issues.apache.org/jira/secure/attachment/12681403/PIG-4330.patch,13/Nov/14 21:13;brocknoland;PIG-4330.patch;https://issues.apache.org/jira/secure/attachment/12681399/PIG-4330.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-11-13 21:31:45.932,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Nov 13 21:51:37 UTC 2014,,,,,,,0|i22cjr:,9223372036854775807,,,,,,,,,,13/Nov/14 21:31;daijy;Seems you miss recordsOfStringArrays.json in patch.,"13/Nov/14 21:38;brocknoland;Interesting looks like it's being ignored by .gitignore

{noformat}
test/org/apache/pig/builtin/avro/data
{noformat}

Added the file in the latest patch.","13/Nov/14 21:51;daijy;+1.

Patch committed to trunk. Thanks Brock!

Also remove test/org/apache/pig/builtin/avro/data from .gitignore, that does not look right.",,,,,,,,,,,,,,,,,,,,,,,,,
Fetch optimization should be disabled when limit is not pushed up,PIG-4329,12754980,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,cheolsoo,cheolsoo,13/Nov/14 05:46,07/Jun/15 03:47,14/Mar/19 03:08,13/Nov/14 17:00,,,,,,0.15.0,,,,,0,,,,,,,"Although PIG-4135 disable fetch optimization when there is no limit in the plan, that doesn't solve the problem completely. In fact, fetch optimization should be still disabled if limit is not pushed up. Consider the following query-
{code}
random_lists = load 'prodhive.schakraborty.search_server_denorm_impressions' using DseStorage();
random_lists = filter random_lists by entity_section=='random';
random_lists = limit random_lists 10;
dump random_lists;
{code}
Because the {{filter by}} blocks limit from being pushed up, POLoad actually scans the full table. In this case, fetch optimization makes the job extremely slow.",,,,,,,,,,,,,,,,,,,,13/Nov/14 05:51;cheolsoo;PIG-4329-1.patch;https://issues.apache.org/jira/secure/attachment/12681263/PIG-4329-1.patch,13/Nov/14 11:12;lbendig;PIG-4329-2.patch;https://issues.apache.org/jira/secure/attachment/12681311/PIG-4329-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-11-13 11:12:31.796,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 13 17:15:34 UTC 2014,,,,,,,0|i22bgf:,9223372036854775807,,,,,,,,,,13/Nov/14 05:51;cheolsoo;Uploading a patch that disables fetch optimization when limit is not pushed up.,"13/Nov/14 11:12;lbendig;Cheolsoo, thanks for the fix.
A minor addition: if FetchablePlanVisitor returns false then we can return immediately, without checking whether there's a limit that has been pushed up to the loaders. I also fixed a failing testcase in TestFetch.",13/Nov/14 16:41;cheolsoo;+1. I'll commit it shortly. Thank you Lorand!,13/Nov/14 17:00;cheolsoo;Committed to trunk.,"13/Nov/14 17:15;lbendig;Thank you, Cheolsoo!",,,,,,,,,,,,,,,,,,,,,,,
Schema of map with value that has an alias can't be parsed again,PIG-4327,12754812,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mprim,mprim,mprim,12/Nov/14 17:21,07/Jun/15 03:48,14/Mar/19 03:08,14/Nov/14 19:14,0.12.0,0.13.0,,,,0.15.0,,parser,,,0,,,,,,,"Tried to create a map of a primitive type, the resulting schema can't be parsed again by the parser if there is a alias set for the value.

I could not set an alias, but the alias gets set by pig itself, e.g. when converting avro schemas to pig schemas and there was a map of records in avro.

See also my other bug report https://issues.apache.org/jira/browse/PIG-4326 , even without that fix, pig produces schemas of maps with values that have an alias.

You can easily reproduce the crash, using those two unit tests. The second one should actually succeed but throws a ParserException instead

{code}
@Test
public void testWorksWithoutAlias() throws FrontendException {
    List<FieldSchema> innerFields = new ArrayList<>();
    innerFields.add(new FieldSchema(null, DataType.LONG));
    List<FieldSchema> fields = new ArrayList<>();
    fields.add(new FieldSchema(""mapAlias"", new Schema(innerFields), DataType.MAP));

    Schema inputSchema = new Schema(fields);
    Schema fromString = Utils.getSchemaFromBagSchemaString(inputSchema.toString());
    assertEquals(inputSchema.toString(), fromString.toString());
}

@Test
public void testBreaksWithAlias() throws FrontendException {
    List<FieldSchema> innerFields = new ArrayList<>();
    innerFields.add(new FieldSchema(""valueAlias"", DataType.LONG));
    List<FieldSchema> fields = new ArrayList<>();
    fields.add(new FieldSchema(""mapAlias"", new Schema(innerFields), DataType.MAP));

    Schema inputSchema = new Schema(fields);
    Schema fromString = Utils.getSchemaFromBagSchemaString(inputSchema.toString());
    assertEquals(inputSchema.toString(), fromString.toString());
}
{code}

I suppose that the issue is in the grammar itself and easy to fix for someone knowing antlr. I don't think the issue is related to the actual type of the value, as I could also provide tests that fail if we don't use a primitive but complex type with an alias.",,,,,,,,,,,,,,,,,,,,13/Nov/14 11:07;mprim;0001-Extend-map-type-to-allow-for-alias-of-its-values.patch;https://issues.apache.org/jira/secure/attachment/12681310/0001-Extend-map-type-to-allow-for-alias-of-its-values.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-14 00:33:02.222,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Nov 14 19:14:27 UTC 2014,,,,,,,0|i22ah3:,9223372036854775807,,,,,,,,,,13/Nov/14 11:07;mprim;I attached a patch to the pig grammar and extended the test cases.,14/Nov/14 00:33;daijy;LGTM. Will check in after tests.,14/Nov/14 19:14;daijy;Patch committed to trunk. Thanks Michael!,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorageSchemaConversionUtilities does not properly convert schema for maps of arrays of records,PIG-4326,12754760,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mprim,mprim,mprim,12/Nov/14 14:07,07/Jun/15 03:48,14/Mar/19 03:08,08/Dec/14 17:57,0.12.0,0.13.0,,,,0.15.0,,impl,,,0,,,,,,,"I tried to convert the avro schema of a map of arrays of records into the proper pig schema and got always empty map schemas in pig.

The reason is that the AvroStorageSchemaConversionUtilities does only assume records or primitive types as content of the map. However, a map of arrays, or a map of map, could have a schema itself and requires recursive calling to derive the full schema.

I wrote a unit test to test for maps of arrays of records which fails with every pig release since the AvroStorage was rewritten (I think this was in 0.12), and there have been no changes since then in the trunk. 

Further the attached patch contains the (rather simple) fix that makes the schema conversion utils succeed.

Would appreciate further comments and if this can be included upstream.",,,,,,,,,,,,,,,,,,,,14/Nov/14 21:19;daijy;PIG-4326-0.patch;https://issues.apache.org/jira/secure/attachment/12681614/PIG-4326-0.patch,12/Nov/14 14:09;mprim;mapsOfArraysOfRecords.patch;https://issues.apache.org/jira/secure/attachment/12681070/mapsOfArraysOfRecords.patch,17/Nov/14 13:29;mprim;supportForMapsOfArraysOfRecords.patch;https://issues.apache.org/jira/secure/attachment/12681894/supportForMapsOfArraysOfRecords.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-11-13 23:16:10.964,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Dec 08 17:57:11 UTC 2014,,,Patch Available,,,,0|i22a5j:,9223372036854775807,,,,,,,,,,"12/Nov/14 14:09;mprim;I attached the patch, including a (breaking) unit test on the current trunk, and the fix to make the test work.","13/Nov/14 23:16;daijy;It adds one more level to array:
parameters: map[array: (array: \{innerRecord: (k: chararray,v: int)\})

Should be:
parameters: map\[array: \{innerRecord: (k: chararray,v: int)\}\]}

The fix should look like:
{code}
      case RECORD:
      case MAP:
      case ARRAY:
        ResourceSchema innerResourceSchema =
            avroSchemaToResourceSchema(fieldSchema.getValueType(), schemasInStack,
            alreadyDefinedSchemas, allowRecursiveSchema);
        rf.setSchema(innerResourceSchema);
        break;
{code}","14/Nov/14 10:59;mprim;Thanks for the feedback, I realized that during development and was actually also a bit surprised. However removing this extra layer, breaks the already existing testLoadRecordsWithMapOfRecords test, and would be not backward compatible.

Further, if you create a map<arrray<InnerRecord>> using avro avdl files, it is just syntactic sugar for actually having some map<WrapperRecord> where WrapperRecord has one field, namely an array of InnerRecord. As neither the WrapperRecord has an alias, nor the array of InnerRecords itself, it is a bit confusing that both get the ""array"" alias.

So we could stick to the old behavior for records and drop the wrapping tuple only for maps and arrays, but then the resulting output will look different than the input I think.","14/Nov/14 21:19;daijy;I tried and seems testLoadRecordsWithMapOfRecords pass. I attached my change in AvroStorageSchemaConversionUtilities. 

If there is inconsistency between avro schema and Pig, we shall fill this gap. For example, if avro map always contain a record, we shall remove it when translating to Pig. Pig always has a tuple inside bag, when translating to avro, we shall remove the tuple. If we are not doing that, then it is a bug we shall fix.","17/Nov/14 13:29;mprim;Did include your previous comment wrongly, my mistake, your attached patch works with the test. Anyway, I still think we should not change the existing behavior for maps of records.

Previously, for maps of records the AvroStorageSchemaConversionUtils did create:
{code}
map[ MyRecord: (fielda: int, ...., fieldz: int) ]
{code}
which I think is what we want as the record should be one tuple and you want to preserve a possible alias. Your fix removes this tuple and the schema looks like 
{code}
map[ fielda: int, ...., fieldz: int ]
{code}
So I uploaded a new proposal for a patch, which keeps the original behavior for maps of records, whereas for maps of maps and maps of arrays, it removes the additional nesting tuple, thus resulting in e.g.
{code}
map[ array: { MyRecord: (fielda: int, ...., fieldz: int) } ]
{code}","24/Nov/14 16:13;mprim;[~daijy] After pig 0.14 is out, can you comment on my updated patch? I need a solution for our production system and would really like to see it being an upstream solution instead of a potentially upstream-incompatible one :)","24/Nov/14 21:49;daijy;Sorry for the delay. Sure my patch get the wrong schema for map, ""map[ fielda: int, ...., fieldz: int ]"" is wrong, it should be ""map[ MyRecord: (fielda: int, ...., fieldz: int) ]"". Your proposal is right, we shall fix the array case and keep map case. Are you saying you have uploaded a new patch? I didn't see it.","25/Nov/14 08:47;mprim;The supportForMapsOfArraysOfRecords.patch file is the updated one, should I delete the first attempts from the attached files? ",03/Dec/14 09:44;mprim;[~daijy] Did you have a chance to test the new patch? Would like to get this done and out of my scope :),"03/Dec/14 21:24;daijy;supportForMapsOfArraysOfRecords.patch looks good, both map and map of array schema look right to me. Will commit shortly.",08/Dec/14 17:57;daijy;supportForMapsOfArraysOfRecords.patch committed to trunk. Thanks Michael!,,,,,,,,,,,,,,,,,
StackOverflow when spilling InternalCachedBag,PIG-4325,12754665,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,12/Nov/14 04:32,21/Nov/14 05:59,14/Mar/19 03:08,12/Nov/14 23:52,,,,,,0.14.0,,impl,,,0,,,,,,,"See the following stack:
{code}
exceptionThrown=java.lang.StackOverflowError
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.pig.data.InternalCachedBag.addDone(InternalCachedBag.java:121)
	at org.apache.pig.data.InternalCachedBag.iterator(InternalCachedBag.java:158)
	at org.apache.pig.data.DefaultAbstractBag.hashCode(DefaultAbstractBag.java:363)
	at java.util.WeakHashMap.hash(WeakHashMap.java:365)
	at java.util.WeakHashMap.get(WeakHashMap.java:464)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger.warn(PigHadoopLogger.java:72)
	at org.apache.pig.data.DefaultAbstractBag.incSpillCount(DefaultAbstractBag.java:446)
	at org.apache.pig.data.InternalCachedBag.updateSpillRecCounter(InternalCachedBag.java:114)
	at org.apache.pig.data.InternalCachedBag.addDone(InternalCachedBag.java:129)
	at org.apache.pig.data.InternalCachedBag.iterator(InternalCachedBag.java:158)
	at org.apache.pig.data.DefaultAbstractBag.hashCode(DefaultAbstractBag.java:363)
	at java.util.WeakHashMap.hash(WeakHashMap.java:365)
	at java.util.WeakHashMap.get(WeakHashMap.java:464)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger.warn(PigHadoopLogger.java:72)
	at org.apache.pig.data.DefaultAbstractBag.incSpillCount(DefaultAbstractBag.java:446)
	at org.apache.pig.data.InternalCachedBag.updateSpillRecCounter(InternalCachedBag.java:114)
	at org.apache.pig.data.InternalCachedBag.addDone(InternalCachedBag.java:129)
	at org.apache.pig.data.InternalCachedBag.iterator(InternalCachedBag.java:158)
	at org.apache.pig.data.DefaultAbstractBag.hashCode(DefaultAbstractBag.java:363)
	at java.util.WeakHashMap.hash(WeakHashMap.java:365)
	at java.util.WeakHashMap.get(WeakHashMap.java:464)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger.warn(PigHadoopLogger.java:72)
	at org.apache.pig.data.DefaultAbstractBag.incSpillCount(DefaultAbstractBag.java:446)
	at org.apache.pig.data.InternalCachedBag.updateSpillRecCounter(InternalCachedBag.java:114)
	at org.apache.pig.data.InternalCachedBag.addDone(InternalCachedBag.java:129)
	at org.apache.pig.data.InternalCachedBag.iterator(InternalCachedBag.java:158)
	at org.apache.pig.data.DefaultAbstractBag.hashCode(DefaultAbstractBag.java:363)
	at java.util.WeakHashMap.hash(WeakHashMap.java:365)
	at java.util.WeakHashMap.get(WeakHashMap.java:464)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger.warn(PigHadoopLogger.java:72)
	at org.apache.pig.data.DefaultAbstractBag.incSpillCount(DefaultAbstractBag.java:446)
	at org.apache.pig.data.InternalCachedBag.updateSpillRecCounter(InternalCachedBag.java:114)
	at org.apache.pig.data.InternalCachedBag.addDone(InternalCachedBag.java:129)
	at org.apache.pig.data.InternalCachedBag.iterator(InternalCachedBag.java:158)
	at org.apache.pig.data.DefaultAbstractBag.hashCode(DefaultAbstractBag.java:363)
	at java.util.WeakHashMap.hash(WeakHashMap.java:365)
	at java.util.WeakHashMap.get(WeakHashMap.java:464)
......
{code}

Pig made recursive call in InternalCachedBag.hashCode.",,,,,,,,,,,,,,,,,,,,12/Nov/14 04:33;daijy;PIG-4325-1.patch;https://issues.apache.org/jira/secure/attachment/12680995/PIG-4325-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-12 23:45:20.913,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Nov 12 23:52:10 UTC 2014,,,,,,,0|i229kn:,9223372036854775807,,,,,,,,,,12/Nov/14 23:32;daijy;All tests pass.,12/Nov/14 23:45;rohini;+1. ,12/Nov/14 23:52;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
Remove jsch-LICENSE.txt,PIG-4324,12754517,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,11/Nov/14 17:42,15/Nov/17 18:32,14/Mar/19 03:08,11/Nov/14 17:44,,,,,,0.14.0,,documentation,,,0,,,,,,,jsch dependency is removed in PIG-3522. We shall remove the license file.,,,,,,,,,,,,,,,,,,,,13/Nov/17 12:37;nkollar;PIG-4324_amend.patch;https://issues.apache.org/jira/secure/attachment/12897313/PIG-4324_amend.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-11-13 12:37:31.598,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 15 18:32:30 UTC 2017,,,,,,,0|i228pz:,9223372036854775807,,,,,,,,,,11/Nov/14 17:44;daijy;Removed.,"13/Nov/17 12:37;nkollar;[~daijy] it looks like NOTICE.txt still have a reference to jsch, should we remove that reference too? Attached a patch for it, can we amend it to this commit, or better have a separate low-priority Jira for this?",15/Nov/17 18:32;rohini;+1. Should be fine. Committed it. Thanks [~nkollar] for fixing it.,,,,,,,,,,,,,,,,,,,,,,,,,
MergeJoin or Split followed by order by gives NPE in Tez,PIG-4315,12753886,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,08/Nov/14 02:37,08/Jun/16 20:48,14/Mar/19 03:08,02/Sep/15 21:57,,,,,,0.16.0,,tez,,,0,,,,,,,"TestHBaseStorage.testMergeJoin() fails. connectingLR is null in CombinerOptimizer and throws NPE in 

{code}
from.plan.getOperator(connectingLR.getOperatorKey())
{code}

When splitting DAGs and doing moveTree TezCompilerUtil.connect() overwrites the output key of sampler LocalRearrange to the partitioner vertex.

Split followed by orderby also had issues using the same TezCompilerUtil.connect()  method.
",,,,,,,,,,,,,PIG-4316,,,,,,,02/Sep/15 05:14;rohini;PIG-4315-1.patch;https://issues.apache.org/jira/secure/attachment/12753682/PIG-4315-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-04-29 20:25:33.019,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Sep 02 21:57:27 UTC 2015,,,,,,,0|i224zr:,9223372036854775807,,,,,,,,,,29/Apr/15 20:25;daijy;Fixed as part of PIG-4315.,"30/Apr/15 05:28;rohini;[~daijy],
   This is not fixed. I was hoping to fix it by rewriting merge join to use broadcast like replicate join, but never got to finish that.","30/Apr/15 17:09;daijy;Oh, yes, sorry, I just notice you skip this test in Tez.","02/Sep/15 05:20;rohini;TestPigSplit.testSchemaWithSplit - testcase for Split followed by order by. Have been failing recently. Most likely exposed by PIG-4574 which might have caused getSuccessors to return in a different order. Problem was there earlier as well, but would be successful if sampler operator is connected after partitioner operator instead of the other way round. ",02/Sep/15 21:37;daijy;+1,02/Sep/15 21:57;rohini;Committed to trunk. Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,
BigData_5 hang on some machine,PIG-4314,12753883,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,08/Nov/14 02:20,21/Nov/14 05:59,14/Mar/19 03:08,11/Nov/14 18:15,,,,,,0.14.0,,impl,,,0,,,,,,,"Hang for both tez and mr. Here is the log:
{code}
2014-11-07 19:49:45,799 INFO [TezChild] relationalOperators.POPartialAgg: Aggregated 10000 raw tuples. Processed tuples before aggregation = 0, after aggregation = 676
2014-11-07 19:49:45,799 INFO [TezChild] relationalOperators.POPartialAgg: After reduction, processed map: 676; raw map: 0
2014-11-07 19:49:45,800 INFO [TezChild] relationalOperators.POPartialAgg: Observed reduction factor: from 10000 to 676 => 14.
2014-11-07 19:49:45,992 INFO [TezChild] relationalOperators.POPartialAgg: Getting mem limits; considering 1 POPArtialAgg objects. with memory percentage 0.2
2014-11-07 19:49:46,290 INFO [TezChild] relationalOperators.POPartialAgg: Estimated total tuples to buffer, based on 10000 tuples that took up 2364864 bytes: 1052574
2014-11-07 19:49:46,290 INFO [TezChild] relationalOperators.POPartialAgg: Setting thresholds. Primary: 977390. Secondary: 75184
2014-11-07 19:49:50,641 INFO [Service Thread] util.SpillableMemoryManager: first memory handler call - Collection threshold init = 216530944(211456K) used = 724849000(707860K) committed = 927989760(906240K) max = 1146093568(1119232K)
2014-11-07 19:49:50,644 INFO [Service Thread] relationalOperators.POPartialAgg: Spill triggered by SpillableMemoryManager
2014-11-07 19:49:50,993 INFO [TezChild] relationalOperators.POPartialAgg: Aggregated 346545 raw tuples. Processed tuples before aggregation = 676, after aggregation = 1352
2014-11-07 19:49:50,998 INFO [TezChild] relationalOperators.POPartialAgg: Aggregated 1352 processed tuples to 676 tuples
2014-11-07 19:49:50,998 INFO [TezChild] relationalOperators.POPartialAgg: Avoided emitting records during spill memory call.
2014-11-07 19:49:51,046 INFO [Service Thread] relationalOperators.POPartialAgg: Finished spill for SpillableMemoryManager call
2014-11-07 19:49:51,851 INFO [Service Thread] util.SpillableMemoryManager: Spilled an estimate of 81943920 bytes from 1 objects. init = 216530944(211456K) used = 724849000(707860K) committed = 927989760(906240K) max = 1146093568(1119232K)
2014-11-07 19:49:59,644 INFO [Service Thread] relationalOperators.POPartialAgg: Spill triggered by SpillableMemoryManager
{code}",,,,,,,,,,,,,,,,,,,,08/Nov/14 02:22;daijy;PIG-4314-1.patch;https://issues.apache.org/jira/secure/attachment/12680350/PIG-4314-1.patch,08/Nov/14 22:36;daijy;PIG-4314-2.patch;https://issues.apache.org/jira/secure/attachment/12680436/PIG-4314-2.patch,11/Nov/14 02:33;daijy;PIG-4314-3.patch;https://issues.apache.org/jira/secure/attachment/12680727/PIG-4314-3.patch,11/Nov/14 17:58;daijy;PIG-4314-4.patch;https://issues.apache.org/jira/secure/attachment/12680824/PIG-4314-4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-11-08 15:56:52.617,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Nov 11 18:15:56 UTC 2014,,,,,,,0|i224zb:,9223372036854775807,,,,,,,,,,08/Nov/14 02:21;daijy;We shall always do spill if we are in handleNotification. Attach a patch.,"08/Nov/14 15:56;rohini;[~daijy],
   +1. Can we comment out the old code for now instead of removing it and put a TODO. I would like to go back and investigate for 0.14.1 why just aggregating is not good enough and it hangs. ","08/Nov/14 22:36;daijy;Sure, I put a TODO tag in new patch.",08/Nov/14 22:38;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,"11/Nov/14 02:27;daijy;The patch does not completely solve the issue. See job hanging again. Here is the stack:
{code}
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.pig.impl.util.SpillableMemoryManager.registerSpillable(SpillableMemoryManager.java:327)
	- waiting to lock <0x00000000d9d02648> (a java.util.LinkedList)
	at org.apache.pig.data.DefaultAbstractBag.markSpillableIfNecessary(DefaultAbstractBag.java:129)
	at org.apache.pig.data.DefaultAbstractBag.add(DefaultAbstractBag.java:118)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg.createValueTuple(POPartialAgg.java:482)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg.aggregate(POPartialAgg.java:419)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg.aggregateFirstLevel(POPartialAgg.java:453)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg.aggregateBothLevels(POPartialAgg.java:436)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg.getNextTuple(POPartialAgg.java:207)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:307)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNextTuple(POLocalRearrange.java:291)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:283)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:278)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
{code}

",11/Nov/14 02:33;daijy;POPartialAgg.spill hold spillables and prevent POPartialAgg.aggregate to register bag. Revert PIG-4314-2.patch and attach PIG-4314-3.patch which does not use DefaultDataBag which need to register to SpillableMemoryManager.,11/Nov/14 17:58;daijy;Some refine on memory usage.,11/Nov/14 18:01;rohini;+1,"11/Nov/14 18:15;daijy;PIG-4314-4.patch committed to both trunk and 0.14 branch. Thanks again, Rohini!",,,,,,,,,,,,,,,,,,,
TestStreamingUDF tez mode leave orphan process on Windows,PIG-4312,12753274,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,06/Nov/14 01:34,21/Nov/14 05:58,14/Mar/19 03:08,08/Nov/14 22:39,,,,,,0.14.0,,tez,,,0,,,,,,,"The test pass, however, there is an orphan process holding junit.tmp.dir. ",,,,,,,,,,,,,,,,,,,,06/Nov/14 01:35;daijy;PIG-4312-1.patch;https://issues.apache.org/jira/secure/attachment/12679751/PIG-4312-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-08 15:57:25.836,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Nov 08 22:39:56 UTC 2014,,,,,,,0|i221cf:,9223372036854775807,,,,,,,,,,06/Nov/14 01:35;daijy;TestStreamingUDF does not properly shutdown MiniTezCluster. Attach patch.,08/Nov/14 15:57;rohini;+1,08/Nov/14 22:39;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
SpillableMemoryManager assumes tenured heap incorrectly,PIG-4299,12753229,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,prkommireddi,prkommireddi,prkommireddi,05/Nov/14 23:47,21/Nov/14 05:58,14/Mar/19 03:08,07/Nov/14 23:28,0.13.0,,,,,0.14.0,,,,,0,,,,,,,"{code}
for (MemoryPoolMXBean b: mpbeans) {
            log.debug(""Found heap ("" + b.getName() +
                "") of type "" + b.getType());
            if (b.getType() == MemoryType.HEAP) {
                /* Here we are making the leap of faith that the biggest
                 * heap is the tenured heap
                 */
                long size = b.getUsage().getMax();
                totalSize += size;
                if (size > biggestSize) {
                    biggestSize = size;
                    biggestHeap = b;
                }
            }
        }
{code}

A memory pool being the biggest MemoryType.HEAP does not guarantee it being tenured. Moreover, we must check whether usage threshold is supported by heap before trying to set usage threshold on it.

Here is the stacktrace that resulted from this bug

java.lang.UnsupportedOperationException: Usage threshold is not supported
at sun.management.MemoryPoolImpl.setUsageThreshold(MemoryPoolImpl.java:114)
at org.apache.pig.impl.util.SpillableMemoryManager.<init>(SpillableMemoryManager.java:130)
at org.apache.pig.impl.util.SpillableMemoryManager.getInstance(SpillableMemoryManager.java:135)
at org.apache.pig.data.BagFactory.<init>(BagFactory.java:123)
at org.apache.pig.data.DefaultBagFactory.<init>(DefaultBagFactory.java:69)
at org.apache.pig.data.BagFactory.getInstance(BagFactory.java:81)
at search.dashboard.VariableLengthTupleToBag.<clinit>(VariableLengthTupleToBag.java:27)",,,,,,,,,,,,,,,,,,,,07/Nov/14 06:38;prkommireddi;PIG-4299_1.patch;https://issues.apache.org/jira/secure/attachment/12680104/PIG-4299_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-07 22:47:19.149,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Nov 07 23:43:15 UTC 2014,,,Patch Available,,,,0|i2212f:,9223372036854775807,,,,,,,,,,"07/Nov/14 06:38;prkommireddi;Tenured (oldgen) is not always the largest.
In an environment running with larger young gen (so to increase the chances that object die young), this will be a problem as per the current code which makes the assumption that the largest pool is ""tenured"".

Changed the way a pool is determined as tenured. ""Old gen"" or tenured is the only memory pool that allows setting usage threshold.

Unit tests pass around SpillableMemoryManager, namely TestDataBag and TestPOPartialAgg.",07/Nov/14 22:47;daijy;+1,07/Nov/14 23:28;daijy;Patch committed to both trunk and 0.14 branch. Thanks Prashant!,07/Nov/14 23:43;prkommireddi;Thanks Daniel! Seems like our commits to branch 0.14 went around the same time and mine messed up the earlier commit. I just did a fresh commit once again. I didn't touch trunk so your commit was fine.,,,,,,,,,,,,,,,,,,,,,,,,
Descending order-by is broken in some cases when key is bytearrays ,PIG-4298,12753085,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,05/Nov/14 16:36,21/Nov/14 05:59,14/Mar/19 03:08,07/Nov/14 21:57,,,,,,0.14.0,,,,,0,,,,,,,"Here is a repo script (using [PigPen|https://github.com/Netflix/PigPen] )-
{code}
REGISTER pigpen.jar;

load4254 = LOAD 'input.clj'
    USING PigStorage('\n')
    AS (value:chararray);

DEFINE udf4265 pigpen.PigPenFnDataBag('(clojure.core/require (quote [pigpen.runtime]) (quote [clojure.edn]))','(pigpen.runtime/exec [(pigpen.runtime/process->bind (pigpen.runtime/pre-process :pig :native)) (pigpen.runtime/map->bind clojure.edn/read-string) (pigpen.runtime/key-selector->bind clojure.core/identity) (pigpen.runtime/process->bind (pigpen.runtime/post-process :pig :native-key-frozen-val))])');

generate4263 = FOREACH load4254 GENERATE
    FLATTEN(udf4265(value));
generate4257 = FOREACH generate4263 GENERATE
    $0 AS key,
    $1 AS value;

order4258 = ORDER generate4257 BY key DESC; <-- sort order isn't changed by DESC
dump order4258;
{code}
This script returns the same result for both ASC and DESC orders.

The problem is as follows-
# {{PigBytesRawComparator}} calls {{BinInterSedesTupleRawComparator.compare()}}.
# {{BinInterSedesTupleRawComparator}} applies descending order.
# {{PigBytesRawComparator}} applies descending order again to what {{BinInterSedesTupleRawComparator}} returns.

Therefore, descending order is never applied.",,,,,,,,,,,,,,,,,,,,05/Nov/14 16:43;cheolsoo;PIG-4298-1.patch;https://issues.apache.org/jira/secure/attachment/12679560/PIG-4298-1.patch,06/Nov/14 23:37;daijy;PIG-4298-2.patch;https://issues.apache.org/jira/secure/attachment/12679996/PIG-4298-2.patch,05/Nov/14 16:48;cheolsoo;repo.tar.gz;https://issues.apache.org/jira/secure/attachment/12679561/repo.tar.gz,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-11-06 05:50:11.93,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 07 21:57:26 UTC 2014,,,,,,,0|i2207b:,9223372036854775807,,,,,,,,,,05/Nov/14 16:43;cheolsoo;Uploading a patch.,05/Nov/14 16:48;cheolsoo;Uploading the repo script in case someone want to reproduce the error.,06/Nov/14 05:50;daijy;Can we add a testcase?,06/Nov/14 16:59;rohini;We should put this in 0.14 as well.,"06/Nov/14 22:55;daijy;It seems in the script, the schema for generate4263 is not properly declared. If I change the script to the following, I get the right result:

generate4263 = FOREACH load4254 GENERATE FLATTEN(udf4265(value)) as (key:long, value:bytearray);","06/Nov/14 23:10;cheolsoo;[~daijy], I think it goes to a different code path if you make that change. So it doesn't hit the bug.","06/Nov/14 23:16;daijy;Yes, PigBytesRawComparator should handle any datatype. Even the data is long, PigBytesRawComparator still should be able to compare that. This is a valid bug.","06/Nov/14 23:37;daijy;Fix looks good.

I added a test case in the new patch.",07/Nov/14 00:12;cheolsoo;Thanks Daniel! The new patch LGTM.,07/Nov/14 21:57;cheolsoo;Committed to 0.14 and trunk.,,,,,,,,,,,,,,,,,,
ToDate has incorrect timezone offsets,PIG-4267,12752765,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,bridiver,bridiver,bridiver,04/Nov/14 16:55,21/Nov/14 05:58,14/Mar/19 03:08,09/Nov/14 20:02,0.12.0,0.13.1,,,,0.14.0,,,,,0,,,,,,,"If you set the timezone to 'America/New_York'
Pig will return ""2012-03-30 -05:00"" for ToDate(1333166400000) instead of the correct ""2012-03-31 -04:00""",java version 1.7.0_72,,,,,,,,,,,,,,,,,,,08/Nov/14 22:28;daijy;PIG-4267-5.patch;https://issues.apache.org/jira/secure/attachment/12680434/PIG-4267-5.patch,07/Nov/14 20:49;bridiver;patch;https://issues.apache.org/jira/secure/attachment/12680259/patch,07/Nov/14 19:55;bridiver;patch;https://issues.apache.org/jira/secure/attachment/12680245/patch,05/Nov/14 14:25;bridiver;patch;https://issues.apache.org/jira/secure/attachment/12679534/patch,04/Nov/14 19:27;bridiver;patch;https://issues.apache.org/jira/secure/attachment/12679287/patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-11-04 20:23:27.092,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sun Nov 09 20:02:15 UTC 2014,,,,,,,0|i21y8v:,9223372036854775807,,,,,,,,,,04/Nov/14 17:02;bridiver;ran the jre timezone update tool to get the latest data and it still returns an impossible date/offset combination,"04/Nov/14 17:23;bridiver;it appears that pig is using the current offset rather than the offset for the time it's actually parsing


    private static final DateTimeFormatter isoDateTimeFormatter = ISODateTimeFormat
            .dateOptionalTimeParser().withOffsetParsed();


","04/Nov/14 17:31;bridiver;It appears that basically all of Pig's timezone support is broken in the same way. Offsets are calculated based on current time/zone, not the time you are using","04/Nov/14 18:35;bridiver;Ok, I believe this is the source of the problem. The offset is set explicitly rather than using the zone itself.

        String dtzStr = conf.get(""pig.datetime.default.tz"");
        if (dtzStr != null && dtzStr.length() > 0) {
            // ensure that the internal timezone is uniformly in UTC offset style
            DateTimeZone.setDefault(DateTimeZone.forOffsetMillis(DateTimeZone.forID(dtzStr).getOffset(null)));
        }
",04/Nov/14 18:43;bridiver;I guess a test for this would have to check to see if it's current DST or not and set the test date based on that to put it in standard if it is or vice versa,04/Nov/14 19:28;bridiver;I submitted a patch but there is not test because there is no reliable way to run it. There is always an edge case if you run the test during the change over between daylight and standard time where it will break because the but is related to the current time.,"04/Nov/14 20:23;daijy;Did you set ""pig.datetime.default.tz"" explicitly? If it is not set, dtzStr==null and ""DateTimeZone.setDefault"" will not kick in.","04/Nov/14 20:51;bridiver;Yes, I did and I tested the patch and it fixes the problem. It's obvious just from looking at the code that it will always use the offset from the current time/timezone combo rather than the time you are actually using in the script.","05/Nov/14 02:00;rohini;bq. It appears that pig is using the current offset rather than the offset for the time it's actually parsing
   Patch sounds good. Could you add testcases for EST,EDT and for the two dates in the year when the switch happens for daylight savings? For a given date and time, the offset (whether it is EST or EDT) should not change right?","05/Nov/14 02:04;rohini;  Timezones and dates are always tricky. Asking the original author of the code as to what was the intent of the original code to ensure we are not breaking something.

[~zjshen],

{code}
// ensure that the internal timezone is uniformly in UTC offset style
DateTimeZone.setDefault(DateTimeZone.forOffsetMillis(DateTimeZone.forID(dtzStr).getOffset(null)));
{code}

Can you tell us why this was done and and the impact of removing this? ","05/Nov/14 02:18;bridiver;I can tell you the only impact I saw and we have a lot of timezone dependent code: the ""z"" parameter in the date format will not always return the offset. If you pass in an offset for the timezone it will return an offset, but if you pass in something like America/New_York it will return EDT. That's not a pig issue, that's a joda issue and I think it's way more important to actually handle dates correctly than be backwards compatible with a format output.

As far as test cases go, I can't. The issue is that it depends on whether it's EDT or EST when you run the test. I could pick a date based on whether it was currently EDT or EST, but those dates have changed before and could change again which would break the test. There's also always an edge case if you run the test during the switch over.","05/Nov/14 02:20;bridiver;The issue is that for a given date, whether it is EST or EDT will change depending on what the current date is.","05/Nov/14 02:31;bridiver;I'm going to try to add a little better explanation for posterity. With the current code when pig runs it reads in the timezone and calculates the offset which it then uses as the default timezone. The problem is that now it's going to use that same offset regardless of the date. So if it's currently EDT (computer clock time) and you instantiate a DateTime with a date that would normally be EST, it will have the EDT time offset instead of the EST time offset. ","05/Nov/14 03:08;rohini;bq. As far as test cases go, I can't. The issue is that it depends on whether it's EDT or EST when you run the test.
   If code behaves based on whether it is EDT or EST now then I believe it is wrong. It should look at the time in question and give the correct time zone offset for that time. OOZIE-1573 is a similar issue.  Given a UTC time/java millis, it should always return the offset on that time.

For Eg:
  Given http://www.timeanddate.com/time/change/usa/new-york  

Sunday, March 9, 2014, 2:00:00 AM clocks were turned forward 1 hour to 
Sunday, March 9, 2014, 3:00:00 AM local daylight time instead

Sunday, November 2, 2014, 2:00:00 AM clocks were turned backward 1 hour to 
Sunday, November 2, 2014, 1:00:00 AM local standard time instead

I would expect the following outputs irrespective of whether it is EDT or EST at the time of running it.
GMT -> America NewYork
March 9, 2014 5:00:00 AM -> March 9, 2014 00:00:00 AM EST	UTC-5 hours
March 9, 2014 6:00:00 AM -> March 9, 2014 01:00:00 AM EST	UTC-5 hours
March 9, 2014 7:00:00 AM -> March 9, 2014 03:00:00 AM EDT	UTC-4 hours
Nov   2, 2014 4:00:00 AM -> Nov   2, 2014 00:00:00 AM EDT	UTC-4 hours
Nov   2, 2014 5:00:00 AM -> Nov   2, 2014 01:00:00 AM EDT	UTC-4 hours
Nov   2, 2014 6:00:00 AM -> Nov   2, 2014 01:00:00 AM EST	UTC-5 hours
Nov   2, 2014 7:00:00 AM -> Nov   2, 2014 02:00:00 AM EDT	UTC-5 hours

Used GMT instead of java time in millis (since 1970) in above example for easy reading. Used http://www.timeanddate.com/worldclock/converted.html?iso=20141102T05&p1=136&p2=179 for getting the above values.","05/Nov/14 04:59;bridiver;Yea, I guess I was focused on DST start/stop dates changing, but for a given date in the past it's fixed. So if the test has one date in EST and one in EDT, regardless of when you run the test it will cover the case that fails now. I'll write it tomorrow.",05/Nov/14 14:25;bridiver;added test,"05/Nov/14 14:26;bridiver;added a test to the patch. On another note, it seems silly that this code exists in 3 different places. I wonder if there's a good way to dry it up?","06/Nov/14 06:22;daijy;There are bunch of tests still using old style: TestOrderBy, TestConversions, TestStore, TestBuiltin and TestDefaultDateTimeZone.testTimeZone. You can do a grep for ""DateTimeZone.setDefault"", or go to PIG-1314 which introduce this code to find them.",06/Nov/14 14:05;bridiver;But those aren't really relevant to the bug and all of them are setting UTC or a fixed offset so it's perfectly fine to use the offset as the default.,"06/Nov/14 17:13;bridiver;Really all of this code needs to be refactored. It should never have been so tightly coupled to joda in the first place. There should be a wrapper method for setting the default time zone that can be used internally and for the tests. I'm happy to make that change if you have a good suggestion for where that method should go. There are a lot of other issues I see as well looking through this code. For instance: CustomFormatToISO sets the default timezone to UTC. I haven't tried it, but I'm guessing that using that function will obliterate your existing timezone settings since it never captures and returns to the existing timezone. There could also be some multithreading issues there even if it did return to it??",06/Nov/14 21:50;bridiver;I'm going to fix the existing ISO functions that don't properly return to the original timezone and then call this one done. If we want to do any more refactoring it's probably better to open another ticket.,"07/Nov/14 15:46;bridiver;ISOHelper has some more serious issues because the tests are at odds with the docs. For instance, it says that it should parse with the default timezone if one is set, but the test checks for UTC instead of the default timezone. Should I fix the test or keep the existing behavior?","07/Nov/14 16:24;bridiver;for that matter, why are all these piggybank ISO functions converting everything to UTC in the first place? It seems like what happened is that timezone support was added and that broke existing tests that all used UTC. The fix led to incorrect behavior by always using UTC as the default instead of the configured default timezone.",07/Nov/14 19:55;bridiver;fixes handling of timezones in piggybank date functions,07/Nov/14 20:49;bridiver;missing import,"08/Nov/14 22:28;daijy;This bring a major change in DateTime internal representation. After patch, we use local time instead of UTC. This change looks good since the old approach does not deal with DTS and in most use cases people deal with local time. I also made several changes:
1. We shall also remove all occurrence of 
""DateTimeZone.setDefault(DateTimeZone.forOffsetMillis(DateTimeZone.UTC.getOffset(null)))"" in tests. Those apparently try to mimic the Pig behavior in tests, and those tests are not testing the right scenario now. Some tests fail because we use local time internally in DateTime now. I made changes to fix them
2. Change ConstantCalculator to set the TimeZone
3. Rebase with trunk.

I will run tests and if all pass, I will commit the change.",09/Nov/14 20:02;daijy;Patch committed to both trunk and 0.14 branch. Thanks Brian!,
SUM functions returns different value in spark and mapreduce engine,PIG-4265,12752614,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,04/Nov/14 02:32,07/Jun/15 03:48,14/Mar/19 03:08,06/Nov/14 16:10,,,,,,0.15.0,,,,,0,,,,,,,"$PIG_HOME/bin/pig -x local RubyUDFs_10.pig
#RubyUDFs_10.pig

a = load 'studenttab10k' using PigStorage() as (name, age:int, gpa:double);
b = group a by name;
c = foreach b generate group, SUM(a.age), SUM(a.gpa);
d = foreach c generate $0, $1, (double)((int)$2*100)/100;
store d into 'local.output/RubyUDFs_10_benchmark.out';

the result in RubyUDFs_10.out/part
#grep ""david s"" RubyUDFs_10.out/part-r-00000 
david steinbeck	266	15.0

#grep ""david s"" studenttab10k
david steinbeck	21	2.44
david steinbeck	33	1.17
david steinbeck	42	1.94
david steinbeck	42	1.35
david steinbeck	31	2.77
david steinbeck	40	2.42
david steinbeck	57	3.91


when runing Ruby_UDFs.pig in spark, the sum(a.gpa) is 16.0 and (double)((int)$2*100)/100 will be ""david steinbeck	266	16.0"".
when running Ruby_UDFs.pig in mapreduce mode, the sum(a.gpa) is 15.999999999999998 and (double)((int)$2*100)/100 will be ""david steinbeck	266	15.0"".

I don't know why the same code by different execution engines(spark and mapreduce) on the same os returns different results. 

",,,,,,,,,,,,,,,,,,,,06/Nov/14 02:34;kellyzly;PIG-4265.patch;https://issues.apache.org/jira/secure/attachment/12679765/PIG-4265.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-04 14:54:26.775,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 06 16:10:34 UTC 2014,,,,,,,0|i21xbr:,9223372036854775807,,,,,,,,,,"04/Nov/14 02:36;kellyzly;The modification of this patch is:  Use BigDecimal to solve the problem
{code}
AlgebraicDoubleMathBase.java
    private static Double doWork(Double arg1, Double arg2, KNOWN_OP op) {
        if (arg1 == null) {
            return arg2;
        } else if (arg2 == null) {
            return arg1;
        } else {
            switch (op) {
            case MAX: return Math.max(arg1, arg2);
            case MIN: return Math.min(arg1, arg2);
            case SUM: return  BigDecimal.valueOf(arg1).add(BigDecimal.valueOf(arg2)).doubleValue();
            default: return null;
            }
        }
    }
{code}","04/Nov/14 14:54;xuefuz;I'm not sure if this is a problem of the algebra. It's expected that a double value 16 is represented by a system as 15.99999999999 or 16.000000001. The problem seems to be the casting of the double value to int. Am I missing anything? If greater precision is needed, then decimal type is the alternative.","05/Nov/14 07:24;kellyzly;Thanks [~xuefuz]'s comment. I made some mistakes in previous bug description. After further investigation, i found the problem is not on ""Java double precision problems"" of AlgebraicDoubleMathBase.java
but on other issues maybe.
{code}
Ruby_UDFs.pig
a = load 'studenttab10k' using PigStorage() as (name, age:int, gpa:double);
b = group a by name;
c = foreach b generate group, SUM(a.age), SUM(a.gpa);
d = foreach c generate $0, $1, (double)((int)$2*100)/100;
store d into 'RubyUDFs_10_benchmark.out';
{code}
run Ruby_UDFs.pig in spark, the sum(a.gpa) is 16.0  and (double)((int)$2*100)/100  will be  ""david steinbeck	266	16.0"".
run Ruby_UDFs.pig in mapreduce mode, the sum(a.gpa) is 15.999999999999998 and (double)((int)$2*100)/100 will  be ""david steinbeck	266	15.0"".

As [~xuefuz] said 
{quote}
 It's expected that a double value 16 is represented by a system as 15.99999999999 or 16.000000001. The problem seems to be the casting of the double value to int
{quote}
I don't know why the same code by different execution engines(spark and mapreduce) on the same os returns different results.  I will investigate more.
I will rename the bug title to SUM functions returns different value in spark and mapreduce engine.","05/Nov/14 14:53;xuefuz;The difference could be caused by different compilers, scala vs java. Here is a link to demo complier effect in dealing with double precision: http://stackoverflow.com/questions/18840730/different-behaviours-for-double-precision-on-different-compiler

Regardless, the value given by different compilers should be close. However, the problem here is that the casting happens before multiplication and division. The result might be different if you put casting last.

Also, double value comparison is usually meaningless unless an error threshold is given. Thus, 15.9999999999998 and 16.0000000001 are equal if we compare them in double precision terms.","06/Nov/14 02:33;kellyzly;Thanks [~xuefuz]'s comment.
I agreed that 15.9999999999998 and 16.0000000001 are equal if we compare them in double precision terms.
The problem is on the casting:
when $2=15.9999999999998
(double)((int)$2*100)/100 = (double)((int)15.9999999999998*100)/100 = (double)(15*100)/100 = 1500.0/100=15.0

when $2=16.0000000001
(double)((int)$2*100)/100 = (double)((int)16.0000000001*100)/100 = (double)(16*100)/100 = 1600.0/100 = 16.0

so if casting method changes like following, i think double precision problem can be avoid:
(double)(ROUND($2*100))/100 
when $2 = 15.9999999999998
(double)(ROUND($2*100))/100 = (double)(ROUND(1599.99999999998))/100 = (double)(1600)/100 = 16.0
when $2 = 16.0000000001
(double)(ROUND($2*100))/100 = (double)(ROUND(1600.00000001))/100 = (double)(1600)/100 = 16.0

The RubyUDFs_10.pig is generated by test/e2e/pig/tests/nightly.conf  Line 3776~3793
{code}
3776                   {
3777                     # test accumulator functions
3778                     'num' => 10,
3779                     'java_params' => ['-Dpig.accumulative.batchsize=5'],
3780                     'pig' => q\
3781 register ':SCRIPTHOMEPATH:/ruby/scriptingudfs.rb' using jruby as myfuncs;
3782 a = load ':INPATH:/singlefile/studenttab10k' using PigStorage() as (name, age:int, gpa:double);
3783 b = group a by name;
3784 c = foreach b generate group, myfuncs.Sum(a.age), myfuncs.Sum(a.gpa);
3785 d = foreach c generate $0, $1, (double)((int)$2*100)/100;
3786 store d into ':OUTPATH:';\,
3787                                     'verify_pig_script' => q\
3788 a = load ':INPATH:/singlefile/studenttab10k' using PigStorage() as (name, age:int, gpa:double);
3789 b = group a by name;
3790 c = foreach b generate group, SUM(a.age), SUM(a.gpa);
3791 d = foreach c generate $0, $1,  (double)((int)$2*100)/100 ;
3792 store d into ':OUTPATH:';\,
3793                     },
{code}

Now RubyUDFs_10.pig of e2e tests in  mapreduce mode successes while  in spark mode fails. 
I find that if we change from ""d = foreach c generate $0, $1,  (double)((int)$2*100)/100""
to ""d = foreach c generate $0, $1, (double)(ROUND($2*100))/100"",problem can be avoided(patch available), Can you help review?",06/Nov/14 03:20;xuefuz;+1,06/Nov/14 03:33;kellyzly;Thanks [~xuefuz],06/Nov/14 16:10;xuefuz;Committed to trunk. Thanks to Liyun for the contribution.,,,,,,,,,,,,,,,,,,,,
Skip shipping local resources in tez local mode,PIG-4261,12752247,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,01/Nov/14 22:22,21/Nov/14 05:58,14/Mar/19 03:08,03/Nov/14 06:03,,,,,,0.14.0,,tez,,,0,,,,,,,"In tez local mode, it is not necessary to ship resources. Attach a patch to skip this step to save time.",,,,,,,,,,,,,,,,,,,,01/Nov/14 22:23;daijy;PIG-4261-1.patch;https://issues.apache.org/jira/secure/attachment/12678727/PIG-4261-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-02 12:07:36.492,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Nov 03 06:03:35 UTC 2014,,,,,,,0|i21v4f:,9223372036854775807,,,,,,,,,,02/Nov/14 12:07;rohini;+1,03/Nov/14 06:03;daijy;Patch committed to trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
SpillableMemoryManager.spill should revert spill on all exception,PIG-4260,12752207,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,daijy,daijy,01/Nov/14 17:49,21/Jun/17 09:15,14/Mar/19 03:08,18/Jan/17 15:41,,,,,,0.16.1,0.17.0,impl,,,0,,,,,,,"Found by Rohini when working on PIG-4250.

bq. If there is a exception during spill() called by SpillableMemoryManager it will be just ignored. We do not track that there was an exception during spill and throw that back when the bag is accessed next time",,,,,,,,,,,,,,,,,,,,11/Jan/17 22:40;rohini;PIG-4260-1.patch;https://issues.apache.org/jira/secure/attachment/12847095/PIG-4260-1.patch,11/Jan/17 23:34;rohini;PIG-4260-2-nowhitespacechanges.patch;https://issues.apache.org/jira/secure/attachment/12847108/PIG-4260-2-nowhitespacechanges.patch,11/Jan/17 23:34;rohini;PIG-4260-2.patch;https://issues.apache.org/jira/secure/attachment/12847109/PIG-4260-2.patch,13/Jan/17 22:17;rohini;PIG-4260-3-nowhitespacechanges.patch;https://issues.apache.org/jira/secure/attachment/12847429/PIG-4260-3-nowhitespacechanges.patch,13/Jan/17 22:17;rohini;PIG-4260-3.patch;https://issues.apache.org/jira/secure/attachment/12847430/PIG-4260-3.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2017-01-11 22:46:20.406,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Jan 18 15:41:47 UTC 2017,,,,,,,0|i21uvr:,9223372036854775807,,,,,,,,,,"11/Jan/17 22:46;rohini;Ran into this when one user had below code in his UDF.
{code}
List<Tuple> data = Arrays.asList(result.getSamples()); // Returns a ArrayList implementation which is different from java.util.ArrayList
DataBag sampleBag = BagFactory.getInstance().newDefaultBag(data);
{code}

Arrays.asList creates a ArrayList which does not implement the clear() method. So it throws error when we call mContents.clear() when spilling. Since we swallow that error, next time when that bag is accessed it has twice the records - in mContents and in spill file.

With this patch, if there is any error we revert the spill. So it would either end up working fine or fail hitting OOM.",11/Jan/17 22:54;knoguchi;Please add a testcase.,11/Jan/17 23:36;rohini;Only DefaultDataBag can be made to fail with the problem mentioned above.  Updated patch with test case for that. ,"13/Jan/17 18:49;knoguchi;{code}
     protected void warn(String msg, Enum warningEnum, Exception e) {
+        warn(msg, warningEnum, e);
+    }
+
+    protected void warn(String msg, Enum warningEnum, Throwable e) {
{code}

I think this would create an infinite loop with stackoverflow when called with {{Exception}}.

{{warn(msg, warningEnum, (Throwable) e);}}  ?
","13/Jan/17 21:09;knoguchi;Other than the above 'warn' issue, +1. ",13/Jan/17 22:17;rohini;Actually removed the overloaded method and just changed the method signature. Should be fine as it is just a protected method and not public. ,18/Jan/17 15:35;knoguchi;+1,18/Jan/17 15:41;rohini;Committed to branch-0.16 and trunk. Thanks Koji for the review.,,,,,,,,,,,,,,,,,,,,
Fix several e2e tests on Windows,PIG-4258,12751881,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,31/Oct/14 04:41,21/Nov/14 05:58,14/Mar/19 03:08,31/Oct/14 07:32,,,,,,0.14.0,,impl,,,0,,,,,,,"Several issues found in Windows e2e tests:
1. Inconsistency in existing conf file, tmpPath is tmp/pigtest in default.conf, but /tmp/pigtest in others; We do perl cleanup using ""$me =~ s/[^a-zA-Z0-9]*//g"" in default.conf, but ""chomp $me"" in rpm.conf
2. build.xml only test tarball install, for rpm install which does not have the whole tarball, compiling test udf fail
3. Some tests fail due to different OS error message in grunt.conf
4. Windows cmd such as perl/awk use a different quoting to pass parameters",,,,,,,,,,,,,,,,,,,,31/Oct/14 04:42;daijy;PIG-4258-1.patch;https://issues.apache.org/jira/secure/attachment/12678407/PIG-4258-1.patch,31/Oct/14 07:17;daijy;PIG-4258-2.patch;https://issues.apache.org/jira/secure/attachment/12678434/PIG-4258-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-10-31 05:35:13.256,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Oct 31 07:32:14 UTC 2014,,,,,,,0|i21swv:,9223372036854775807,,,,,,,,,,"31/Oct/14 05:35;rohini;Few comments:
 - why touch in +cat touch /bin/pig_test_file ? cat tries to just check for a file called touch. Isn't cat /bin/pig_nonexistent_file good?
 - Newly added <property name=""pig.jar.dir"" value=""${pig.dir}"" /> not used anywhere. Can be removed.","31/Oct/14 07:17;daijy;Address Rohini's review comment. Removed pig.jar.dir. ""cat touch"" is actually ""cat nonexist"". Change the patch to make it clear.",31/Oct/14 07:20;rohini;+1,31/Oct/14 07:32;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
Fix several e2e tests on secure cluster,PIG-4257,12751875,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,31/Oct/14 04:02,21/Nov/14 05:58,14/Mar/19 03:08,03/Nov/14 17:57,,,,,,0.14.0,,impl,,,0,,,,,,,"There are several tests fail on some secure cluster setting. For example: Bloom_3, Union_\[7,8,13\], Join_\[6-8\]. Here is one stack:
{code}
Error: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing (Name: Local Rearrange[tuple]{chararray}(false) - scope-78 Operator Key: scope-78): org.apache.pig.backend.executionengine.ExecException: ERROR 2081: Unable to setup the load function.
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:310)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNextTuple(POLocalRearrange.java:291)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.setUpHashMap(POFRJoin.java:409)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin.getNextTuple(POFRJoin.java:241)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:282)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2081: Unable to setup the load function.
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad.getNextTuple(POLoad.java:131)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:301)
	... 14 more
Caused by: java.io.IOException: Can't get Master Kerberos principal for use as renewer
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:116)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:242)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigFileInputFormat.listStatus(PigFileInputFormat.java:37)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:385)
	at org.apache.pig.impl.io.ReadToEndLoader.init(ReadToEndLoader.java:190)
	at org.apache.pig.impl.io.ReadToEndLoader.<init>(ReadToEndLoader.java:146)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad.setUp(POLoad.java:99)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad.getNextTuple(POLoad.java:127)
	... 15 more
{code}",,,,,,,,,,,,,,,,,,,,31/Oct/14 04:03;daijy;PIG-4257-1.patch;https://issues.apache.org/jira/secure/attachment/12678401/PIG-4257-1.patch,01/Nov/14 22:01;daijy;PIG-4257-2.patch;https://issues.apache.org/jira/secure/attachment/12678724/PIG-4257-2.patch,03/Nov/14 08:28;daijy;PIG-4257-3.patch;https://issues.apache.org/jira/secure/attachment/12678895/PIG-4257-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-10-31 05:21:03.192,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Nov 03 17:57:01 UTC 2014,,,,,,,0|i21svj:,9223372036854775807,,,,,,,,,,31/Oct/14 05:21;rohini;Weird. I have never encountered this till now. Is there some special case or way of deployment when this happens?,31/Oct/14 06:52;daijy;I only see it in our AD-MIT (active directory + mit kerberos) deployment. Have no idea why it makes a difference.,"01/Nov/14 20:44;daijy;In this setting, yarn-site.xml only defined on gw. On task node, it is empty. So the backend have to get ""yarn.resourcemanager.principal"" from the jobConf, configuration object from ""new Configuration()"" will not contain this entry. That's why certain operators such as replicated join fail.",01/Nov/14 22:01;daijy;Attach a better fix based on the prio analysis.,02/Nov/14 12:04;rohini;+1,03/Nov/14 06:55;daijy;WeightedRangePartitioner should still be there. Attach new patch.,03/Nov/14 15:06;rohini;+1,03/Nov/14 17:57;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,
Fix StreamingPythonUDFs e2e test failure on Windows,PIG-4256,12751832,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,31/Oct/14 00:20,21/Nov/14 05:58,14/Mar/19 03:08,31/Oct/14 06:49,,,,,,0.14.0,,impl,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,31/Oct/14 00:22;daijy;PIG-4256-1.patch;https://issues.apache.org/jira/secure/attachment/12678364/PIG-4256-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-31 05:02:39.392,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Oct 31 06:49:32 UTC 2014,,,,,,,0|i21smf:,9223372036854775807,,,,,,,,,,"31/Oct/14 00:31;daijy;There are 3 issues found:
1. controller.py use /dev/null which does not work on Windows
2. Cannot find userUdfFile in PigScriptUDF.jar, it shipped as /C/path
3. empty parameter fileCachePath does not honored as a separate parameter",31/Oct/14 05:02;rohini;+1,31/Oct/14 06:49;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
Tez container reuse fail when using script udf,PIG-4252,12751195,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,28/Oct/14 18:45,21/Nov/14 05:58,14/Mar/19 03:08,28/Oct/14 20:48,,,,,,0.14.0,,tez,,,1,,,,,,,"If two job use the same script udf, we should reuse container. However, this is broken because the md5 of PigScriptUDF.jar is different even though the contents are the same. ",,,,,,,,,,,,,,,,,,,,28/Oct/14 19:00;daijy;PIG-4252-1.patch;https://issues.apache.org/jira/secure/attachment/12677661/PIG-4252-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-28 20:44:56.945,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Oct 28 20:48:17 UTC 2014,,,,,,,0|i21oqn:,9223372036854775807,,,,,,,,,,28/Oct/14 20:44;rohini;+1,28/Oct/14 20:48;daijy;Patch committed to 0.14 branch and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Security Risks found by Coverity,PIG-4250,12750918,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,27/Oct/14 23:13,21/Nov/14 05:59,14/Mar/19 03:08,31/Oct/14 21:46,,,,,,0.14.0,,impl,,,0,,,,,,,Here is the report: https://scan.coverity.com/projects/3026 (Need to register to see). Most belong to one pattern: not close stream when exception happens.,,,,,,,,,,,,,,,,,,,,27/Oct/14 23:14;daijy;PIG-4250-1.patch;https://issues.apache.org/jira/secure/attachment/12677442/PIG-4250-1.patch,29/Oct/14 19:24;daijy;PIG-4250-2.patch;https://issues.apache.org/jira/secure/attachment/12677966/PIG-4250-2.patch,31/Oct/14 06:09;daijy;PIG-4250-3.patch;https://issues.apache.org/jira/secure/attachment/12678420/PIG-4250-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-10-31 00:37:22.546,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Oct 31 22:51:03 UTC 2014,,,,,,,0|i21n2f:,9223372036854775807,,,,,,,,,,29/Oct/14 19:24;daijy;There are several test failures in previous patch. Attach a new patch.,"31/Oct/14 00:37;thejas;blocks such as following aren't necessary - 
{code}
   } catch (IOException e) {
                throw e
        } 
{code}

You can use Hadoop IOUtils.closeStream or cleanup to call close. You don't need the addtional try-catch and null check with that.
","31/Oct/14 05:15;rohini;DistinctDataBag.java,InternalDistinctBag.java, InternalSortedBag.java , SortedDataBag.java in the patch have out.close() with exceptions swallowed. For output streams it is best advised to not use IOUtils.closeStream or cleanup. Just null check is enough. When output stream is closed quietly it can lead to corrupted or partial data and have encountered that issue multiple times with hdfs data. Input streams can always be closed quietly though.  ",31/Oct/14 06:09;daijy;Address review comments from Thejas and Rohini.,"31/Oct/14 06:19;rohini;+1. 

Missed the out.flush() in the Bag classes and exception would be thrown when that fails. So it is good to just log a warning when out.close() fails. But I see a bigger problem. If there is a exception during spill() called by SpillableMemoryManager it will be just ignored. We do not track that there was an exception during spill and throw that back when the bag is accessed next time. But this is a different issue not related to this jira and we can follow this up in a new one. ","31/Oct/14 21:46;daijy;Patch committed to both trunk and 0.14 branch. Thanks Thejas and Rohini for review!

Rohini, do you want me file a ticket for exception handling for SpillableMemoryManager?",31/Oct/14 22:51;rohini;Sure. Exception handling has to be done in the bags.,,,,,,,,,,,,,,,,,,,,,
S3 properties are not picked up from core-site.xml in local mode,PIG-4247,12750176,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,23/Oct/14 19:46,07/Jun/15 03:47,14/Mar/19 03:08,26/Oct/14 21:12,,,,,,0.15.0,,,,,0,,,,,,,"Even in local mode, {{fs.s3}} properties need to be set if the job accesses s3 files (for eg, registering jars on s3, loading input files on s3, etc). In particular, {{fs.s3.awsSecretAccessKey}} and {{fs.s3.awsAccessKey}} are usually set in {{core-site.xml}}, but since local mode doesn't load {{core-site.xml}}, these properties have to be set again in pig.properties. This adds operational overhead because whenever rotating aws keys, {{pig.properties}} also needs to be updated. So it would be nice if {{fs.s3}} properties can be set even in local mode.",,,,,,,,,,,,,,,PIG-4556,,,,,23/Oct/14 19:50;cheolsoo;PIG-4247-1.patch;https://issues.apache.org/jira/secure/attachment/12676690/PIG-4247-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-24 20:41:03.446,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 16 07:51:57 UTC 2015,,,,,,,0|i21ijz:,9223372036854775807,,,,,,,,,,23/Oct/14 19:50;cheolsoo;Uploading a patch that load s3-related properties from {{core-site.xml}} regardless whether it's local mode or not.,24/Oct/14 20:41;daijy;+1,26/Oct/14 21:12;cheolsoo;Committed to trunk. Thank you Daniel for reviewing!,16/May/15 07:51;daijy;The patch break the local mode in some cases. Will create a new ticket for it.,,,,,,,,,,,,,,,,,,,,,,,,
For indented xmls with multiline content (e.g. wikipedia) XMLLoader cuts out the begining of every line,PIG-4242,12749079,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,holdfenytolvaj,holdfenytolvaj,holdfenytolvaj,19/Oct/14 02:18,07/Jun/15 03:47,14/Mar/19 03:08,24/Oct/14 21:00,,,,,,0.15.0,,piggybank,,,0,,,,,,,"XMLLoader finds the first matching position for the required tag, but applies this offset for all following lines as well until the closing tag. This causes content losses for indented xml formats with multiline contents such as the wikipedia xml dump:

--- example input ---
{code:xml}
    <page>Look, 
not a thing is missing.</page>
{code}

--- current ouput ---
{code:xml}
<page>Look, a thing is missing.</page>
{code}

--- expected ouput ---
{code:xml}
<page>Look, not a thing is missing.</page>
{code}",,,,,,,,,,,,,,,,,,,,19/Oct/14 02:19;holdfenytolvaj;XMLLoaderMissingContent.patch;https://issues.apache.org/jira/secure/attachment/12675707/XMLLoaderMissingContent.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-24 21:00:01.348,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Oct 27 14:58:43 UTC 2014,,,Patch Available,,,,0|i21btj:,9223372036854775807,,,,,,,,,,"19/Oct/14 02:23;holdfenytolvaj;--- example input ---
{code:xml}
    <page>You have
not missed it</page>
{code}",24/Oct/14 21:00;daijy;Patch committed to trunk. Thanks Geza!,27/Oct/14 14:58;holdfenytolvaj;Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,
Auto local mode mistakenly converts large jobs to local mode when using with Hive tables,PIG-4241,12748777,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,17/Oct/14 00:49,21/Nov/14 05:59,14/Mar/19 03:08,27/Oct/14 17:13,,,,,,0.14.0,,impl,,,0,,,,,,,"The current implementation of auto local mode has two severe problems-
# It assumes file-based inputs, and it always converts jobs with non-file-based inputs into local mode unless the {{LoadMetadata.getStatistics().getSizeInBytes()}} returns >100M. This is particularly problematic when using Pig with Hive tables with custom LoadFuncs that did not implement LoadMetadata interface.
# It lists all the files to compute the total size. The algorithm is like this. First, compute the total size. Second, compare it against the configured max bytes. This is very time-consuming when Pig job loads a large number of files. It will list all the files only to compute the total size. Instead, we should stop computing the sum of input sizes as soon as it becomes the max bytes-
{code:title=JobControlCompiler.java}
long totalInputFileSize = InputSizeReducerEstimator.getTotalInputFileSize(conf, lds, job); // THIS IS BAD!
long inputByteMax = conf.getLong(PigConfiguration.PIG_AUTO_LOCAL_INPUT_MAXBYTES, 100*1000*1000l);
log.info(""Size of input: "" + totalInputFileSize +"" bytes. Small job threshold: "" + inputByteMax );
if (totalInputFileSize < 0 || totalInputFileSize > inputByteMax) {
        return false;
}
{code}",,,,,,,,,,,,,,,,,,,,17/Oct/14 20:03;cheolsoo;PIG-4241-1.patch;https://issues.apache.org/jira/secure/attachment/12675564/PIG-4241-1.patch,23/Oct/14 20:18;cheolsoo;PIG-4241-2.patch;https://issues.apache.org/jira/secure/attachment/12676703/PIG-4241-2.patch,26/Oct/14 21:04;cheolsoo;PIG-4241-3.patch;https://issues.apache.org/jira/secure/attachment/12677205/PIG-4241-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-10-24 20:38:27.666,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 27 17:13:15 UTC 2014,,,,,,,0|i219zj:,9223372036854775807,,,,,,,,,,"17/Oct/14 20:03;cheolsoo;Attached patch includes the following changes-
# When Hive table name is interpreted as a hdfs file, {{globStatus()}} returns null. In this case, {{InputSizeReducerEstimator.getTotalInputFileSize()}} returns -1 now. Therefore, big jobs with Hive table input do not get converted to local mode.
# Max parameter is add to {{InputSizeReducerEstimator.getTotalInputFileSize()}}. Now when it computes the total input size recursively, it exits as soon as it reaches the max. This helps avoid listing all the files to determine whether the job can be converted to local mode or not.",23/Oct/14 20:18;cheolsoo;Fixed {{TestInputSizeReducerEstimator}} in a new patch.,"24/Oct/14 20:38;daijy;Does all non-file-based job convert to auto local mode? If so, let's put the fix also in 0.14. Can you add a comment to explain the method parameter max in getPathLength and getTotalInputFileSize?","26/Oct/14 21:04;cheolsoo;{quote}
Does all non-file-based job convert to auto local mode?
{quote}
Not all but many. The issue is that not all LoadFuncs implement {{getSizeInBytes()}}, and even if they do, this method works on a best effort basis. When it is not implemented or returns null, {{getTotalInputFileSize()}} returns 0 for Hive tables because they're mis-interpreted as HDFS file paths. Then, auto local mode thinks the size of the input path is 0 bytes, and thus, it is runnable in local mode. As a result, big jobs run in local mode filling up local disk on gateways (Genie nodes).

I updated the patch adding comments about the {{max}} parameter.",27/Oct/14 17:01;daijy;+1. And let's put the patch to 0.14 branch as well.,27/Oct/14 17:13;cheolsoo;Thank you Daniel for reviewing. Committed to 0.14 and trunk.,,,,,,,,,,,,,,,,,,,,,,
Property 'pig.job.converted.fetch' should be unset when fetch finishes,PIG-4238,12748450,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,lbendig,lbendig,15/Oct/14 23:01,21/Nov/14 05:58,14/Mar/19 03:08,16/Oct/14 17:44,0.13.0,,,,,0.14.0,,,,,0,,,,,,,"If plan is fetchable, PIG-4171 sets 
{code}
pc.getProperties().setProperty(PigImplConstants.CONVERTED_TO_FETCH, ""true"");
{code}
When working in grunt this session-scoped property remains set after the result is fetched. For example, if subsequent jobs aren't fetchable or  the user disables fetch, this property will be still there.
We'd need to remove this property from PigContext after the fetch job finishes.

",,,,,,,,,,,,,,,,,,,,16/Oct/14 11:51;lbendig;PIG-4238.patch;https://issues.apache.org/jira/secure/attachment/12675264/PIG-4238.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-16 17:14:54.472,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Oct 18 09:58:43 UTC 2014,,,,,,,0|i21827:,9223372036854775807,,,,,,,,,,16/Oct/14 17:14;daijy;+1,"16/Oct/14 17:44;lbendig;Patch committed to trunk. Thanks Daniel, for the review!",17/Oct/14 21:23;daijy;This should be fixed on 0.14 branch as well. Committed to the branch.,"18/Oct/14 09:58;lbendig;Oh, yes.. Thanks Daniel!",,,,,,,,,,,,,,,,,,,,,,,,
Fix unit test failures on Windows,PIG-4235,12748148,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Oct/14 21:54,21/Nov/14 05:58,14/Mar/19 03:08,30/Oct/14 06:00,,,,,,0.14.0,,impl,,,0,,,,,,,"Bunch of unit tests fail on trunk and 0.14 branch, we need to fix them.",,,,,,,,,,,,,,,,,,,,14/Oct/14 21:55;daijy;PIG-4235-1.patch;https://issues.apache.org/jira/secure/attachment/12674857/PIG-4235-1.patch,29/Oct/14 21:34;daijy;PIG-4235-2.patch;https://issues.apache.org/jira/secure/attachment/12678010/PIG-4235-2.patch,29/Oct/14 21:34;daijy;PIG-4235-removeescape.patch;https://issues.apache.org/jira/secure/attachment/12678011/PIG-4235-removeescape.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-10-15 04:45:22.002,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Oct 30 06:00:23 UTC 2014,,,,,,,0|i2169b:,9223372036854775807,,,,,,,,,,"15/Oct/14 04:45;rohini;Few comments:
  -  Can remove the line <property name=""windows"" value=""false""/>
  -  You can either use line or value with jvmarg (http://ant.apache.org/manual/using.html#arg). So the new jvmarg value will be overriding the existing jvmarg line one. For which unit test failure do we need to add ${hadoop.root}\bin to library path?
  - Do we need both Util.generateURI and Util.encodeEscape. Can we inline or call Util.encodeEscape in Util.generateURI ?
  - TestUnion.java - Can we also inline or call Util.encodeEscape in Util.createInputFile. This would help avoid having to fix new tests often for Windows and can also remove Util.encodeEscape from existing tests.","29/Oct/14 21:34;daijy;PIG-4235-2.patch addresses Rohini's comments #1, #2, #4.

I also upload PIG-4235-removeescape (on top of PIG-4235-2.patch) to address comments #3.",30/Oct/14 03:15;rohini;+1. Thanks Daniel for cleaning up lot more of code.,30/Oct/14 06:00;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
Documentation fix: first nested foreach example is incomplete,PIG-4230,12747611,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,lbendig,lbendig,lbendig,12/Oct/14 21:06,21/Nov/14 05:59,14/Mar/19 03:08,13/Oct/14 00:24,0.13.0,,,,,0.14.0,,documentation,,,0,,,,,,,"http://pig.apache.org/docs/r0.13.0/basic.html#nestedblock : 
is:
{code}
 user = load 'user' as (uid, age, gender, region);
session = load 'session' as (uid, region);
C = cogroup user by uid, session by uid;
D = foreach C {
    crossed = cross user, session;
    generate crossed;  
{code}

should:
{code}
user = load 'user' as (uid, age, gender, region);
session = load 'session' as (uid, region);
C = cogroup user by uid, session by uid;
D = foreach C {
    crossed = cross user, session;
    generate crossed;
}
dump D;
{code}",,,,,,,,,,,,,,,,,,,,12/Oct/14 21:08;lbendig;PIG-4230.patch;https://issues.apache.org/jira/secure/attachment/12674431/PIG-4230.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-13 00:24:02.496,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Oct 13 08:39:29 UTC 2014,,,,,,,0|i2132f:,9223372036854775807,,,,,,,,,,13/Oct/14 00:24;daijy;Patch committed to trunk and 0.14 branch. Thanks Lorand!,"13/Oct/14 08:39;lbendig;Thanks, Daniel!",,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming Python UDF handles bag outputs incorrectly,PIG-4227,12747171,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,10/Oct/14 01:06,21/Nov/14 05:59,14/Mar/19 03:08,16/Oct/14 04:27,,,,,,0.14.0,,,,,0,,,,,,,"I have a udf that generates different outputs when running as jython and streaming python.
{code:title=jython}
{([[BBC Worldwide]])}
{code} 
{code:title=streaming python}
{(BC Worldwid)}
{code}
The problem is that streaming python encodes a bag output incorrectly. For this particular example, it serializes the output string as follows-
{code}
|{_[[BBC Worldwide]]|}_
{code}
where '|' and '\_' wrap bag delimiters '\{' and '\}'. i.e. '\{' => '|\{\_' and '\}' => '|\}\_'.

But this is wrong because bag must contain tuples not chararrays. i.e. the correct encoding is as follows-
{code}
|{_|(_[[BBC Worldwide]]|)_|}_
{code}
where '|' and '_' wrap tuple delimiters '(' and ')' as well as bag delimiters.

This results in truncated outputs.",,,,,,,,,,,,,,,,,,,,10/Oct/14 01:10;cheolsoo;PIG-4227-1.patch;https://issues.apache.org/jira/secure/attachment/12674076/PIG-4227-1.patch,15/Oct/14 19:08;daijy;PIG-4227-2.patch;https://issues.apache.org/jira/secure/attachment/12675086/PIG-4227-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-10-10 06:59:49.797,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Oct 16 04:27:44 UTC 2014,,,,,,,0|i210fz:,9223372036854775807,,,,,,,,,,10/Oct/14 01:10;cheolsoo;Attaching a patch.,10/Oct/14 06:59;daijy;+1,12/Oct/14 00:04;cheolsoo;Committed to trunk. Thank you Daniel for reviewing the patch!,12/Oct/14 03:00;rohini;Wrong results is not good. Took the liberty of checking this into 0.14 branch as well.,14/Oct/14 22:16;daijy;Seems this breaks TestStreamingUDF. I am looking.,14/Oct/14 22:36;daijy;e2e tests StreamingPythonUDFs_5 and StreamingPythonUDFs_12 also fail due to this.,"15/Oct/14 07:42;daijy;[~cheolsoo], looked at scriptingudf.complexTypes, python udf return a list of tuples for the bag field. When serialize the output, bag adds |{_ and tuple adds |(_. So this part seems Ok. 

I don't totally understand the issue in the description, is that because jython adds tuple inside a list automatically but python does not?","15/Oct/14 17:17;cheolsoo;[~daijy], sorry for breaking unit tests.
{quote}
I don't totally understand the issue in the description, is that because jython adds tuple inside a list automatically but python does not?
{quote}
You're right that Jython udf usually doesn't return a list of Python tuples but just returns a list of Python objects. In that case, Pig converts it to a bag of tuples automatically by wrapping objects with tuples. However, Python streaming udf serializes it as a bag of non-tuples, and they're never wrapped with tuples. The problem is that outputSchema is defined as something like {{bag:\{tuple\:( chararray )\}}}, and now deserialization code skips bytes to skip tuple delimiters that do not exist. That results in truncating 3 chars at the beginning and the end.

So the root cause is that Jython and Python streaming handles a Python list of non-tuples differently. This makes it not possible to run the same udf in the two modes. With my patch, I can run the same udf in the two modes and get the same result. For eg, here is the diff in one of udfs before and after my patch. This should clarify the difference-
{code}
34c34
<                             output.append(recos[r]['id'])
---
>                             output.append(tuple([recos[r]['id']]))
44c44
<                             output.append(recos[r]['id'])
---
>                             output.append(tuple([recos[r]['id']]))
49c49
<                     output.append(items[i]['id'])
---
>                     output.append(tuple([items[i]['id']]))
84c84
<                             output.append(recos[r]['id'])
---
>                             output.append(tuple([recos[r]['id']]))
96c96
<                             output.append(recos[r]['id'])
---
>                             output.append(tuple([recos[r]['id']]))
101c101
<                     output.append(items[i]['id'])
---
>                     output.append(tuple([items[i]['id']]))
105c105
<                 return [-1]
---
>                 return [tuple([-1])]
{code}",15/Oct/14 17:27;daijy;Shall we make python udf insert tuple automatically? Otherwise we break python udf which do insert tuples.,"15/Oct/14 17:44;cheolsoo;{quote}
Otherwise we break python udf which do insert tuples.
{quote}
True, but I hardly see udfs that insert tuples because in Jython, you never had to do that. Since I deployed streaming udf in prod, few users have inserted tuples only because they had to. Now I deployed my patch to prod and haven't heard any complaints.

But I do agree that if a udf returns a list of tuples, there will be an extra layer of tuple. That's a valid corner case, indeed.","15/Oct/14 18:01;daijy;This is a corner case if a tuple contains a single item, but if a tuple contains multiple items, wrapping it in a tuple seems the only way, right?","15/Oct/14 18:07;cheolsoo;Yes, you're right.","15/Oct/14 19:08;daijy;Attach a patch to add tuple automatically. [~cheolsoo], can you check if that works for you?","16/Oct/14 02:05;cheolsoo;Yes, it works! I haven't verified the broken tests, but if they all pass, please go ahead commit it.

Thank you Daniel!","16/Oct/14 04:27;daijy;Yes, test pass.

Committed to trunk and 0.14 branch. Thanks for verify, Cheolsoo!",,,,,,,,,,,,,
Upgrade Tez to 0.5.1,PIG-4226,12747122,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,09/Oct/14 20:44,21/Nov/14 05:58,14/Mar/19 03:08,10/Oct/14 06:59,,,,,,0.14.0,,tez,,,0,,,,,,,Tez 0.5.1 is released. Pig should use 0.5.1 which fixed the tez unit test hanging issue.,,,,,,,,,,,,,,,,,,,,09/Oct/14 20:45;daijy;PIG-4226-1.patch;https://issues.apache.org/jira/secure/attachment/12673996/PIG-4226-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-10 01:03:52.056,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Oct 10 06:59:08 UTC 2014,,,,,,,0|i2105j:,9223372036854775807,,,,,,,,,,10/Oct/14 01:03;rohini;+1,10/Oct/14 06:59;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Upload Tez payload history string to timeline server,PIG-4224,12746847,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,08/Oct/14 22:35,21/Nov/14 05:58,14/Mar/19 03:08,08/Nov/14 22:41,,,,,,0.14.0,,tez,,,0,,,,,,,"As discussed in TEZ-1119, Pig needs to upload configuration as payload history string, so that Tez UI can present it. ",,,,,,,,,,,,,,,,,,,,15/Oct/14 20:41;daijy;PIG-4224-1.patch;https://issues.apache.org/jira/secure/attachment/12675100/PIG-4224-1.patch,07/Nov/14 22:18;daijy;PIG-4224-2.patch;https://issues.apache.org/jira/secure/attachment/12680287/PIG-4224-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-11-08 15:59:56.867,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Nov 08 22:41:33 UTC 2014,,,,,,,0|i20yhb:,9223372036854775807,,,,,,,,,,15/Oct/14 20:41;daijy;Upload configuration as xml since it is easier.,07/Nov/14 22:18;daijy;Change the history string format according to TEZ-1736.,08/Nov/14 15:59;rohini;+1,08/Nov/14 22:41;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
MapReduce-based Rank failing with NPE due to missing Counters,PIG-4220,12745563,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,02/Oct/14 18:29,21/Nov/14 05:58,14/Mar/19 03:08,06/Oct/14 21:35,,,,,,0.14.0,,,,,0,,,,,,,"User reported his pig job with Rank was failing at 
{noformat}
Pig Stack Trace
---------------
ERROR 2043: Unexpected error during execution.

org.apache.pig.backend.executionengine.ExecException: ERROR 2043: Unexpected
error during execution.
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1296)
        at
org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1270)
        at org.apache.pig.PigServer.execute(PigServer.java:1260)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:354)
        at
org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:138)
        at
org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:200)
        at
org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:171)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
        at org.apache.pig.Main.run(Main.java:480)
        at org.apache.pig.Main.main(Main.java:157)
Caused by: java.lang.RuntimeException: Error to read counters into Rank
operation counterSize 277
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.saveCounters(JobControlCompiler.java:384)
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.updateMROpPlan(JobControlCompiler.java:330)
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:385)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1285)
        ... 9 more
Caused by: java.lang.NullPointerException
        at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.saveCounters(JobControlCompiler.java:375)
        ... 12 more
{noformat}

(this is different from PIG:3985 NPE)",,,,,,,,,,,,,,,,,,,,02/Oct/14 18:37;knoguchi;pig-4220-v01.txt;https://issues.apache.org/jira/secure/attachment/12672600/pig-4220-v01.txt,02/Oct/14 19:07;knoguchi;pig-4220-v02.txt;https://issues.apache.org/jira/secure/attachment/12672607/pig-4220-v02.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-10-02 18:41:38.359,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Oct 06 21:35:41 UTC 2014,,,,,,,0|i20qrj:,9223372036854775807,,,,,,,,,,"02/Oct/14 18:37;knoguchi;NPE was due to some PigMapReduceCounter.PigMapCounter getting empty inputs thus not creating corresponding counters.  However, JobControlCompiler(a reader) assumes all mappers to create the counters.
Fix can be
* Adding counter of 0 at initialization of the tasks
or
* In the JobControlCompiler, assume 0 for non-existence counters.

This patch takes the first approach",02/Oct/14 18:41;rohini;Why not just call PigMapReduceCounter.PigReduceCounter.incrementCounter(0);?,"02/Oct/14 19:06;knoguchi;Rohini pointed out that we need the same change on the PigReduceCounter side.  Added.

Also, getting rid of the code segment where it catches Exception and ignoring it.  This may to incorrect results. 

Other than that, trying to keep the original format.",02/Oct/14 19:07;knoguchi;Accidentally added a patch with debug statement.  Reattaching.,02/Oct/14 20:05;rohini;+1,03/Oct/14 18:34;rohini;Need to call PigStatusReporter.setContext(context); before incrementCounter in PigReduceCounter,"03/Oct/14 20:17;knoguchi;bq. Need to call PigStatusReporter.setContext(context); before incrementCounter in PigReduceCounter

Thanks for catching that.",03/Oct/14 20:22;rohini;+1,03/Oct/14 20:27;knoguchi;hmm. My v3 patch fails on compile error.  I'll upload another one.,03/Oct/14 20:59;rohini;Sorry that change was required only for backporting to 0.11. Your v2 patch is good and you can go ahead and check that in. ,06/Oct/14 21:35;knoguchi;Committed pig-4220-v02.txt patch.  Thanks Rohini for the review!,,,,,,,,,,,,,,,,,
"When parsing a schema, pig drops tuple inside of Bag if it contains only one field",PIG-4219,12745362,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,julienledem,julienledem,01/Oct/14 22:03,21/Nov/14 05:59,14/Mar/19 03:08,10/Oct/14 17:08,0.13.0,,,,,0.14.0,,,,,0,,,,,,,"Example
{code:java}
//We generate a schema object and call toString()
String schemaStr = ""my_list: {array: (array_element: (num1: int,num2: int))}"";
// Reparsed using org.apache.pig.impl.util.Utils
Schema schema = Utils.getSchemaFromString(schemaStr);
// But no longer matches the original structure
schema.toString();
// => {my_list: {array_element: (num1: int,num2: int)}}
{code}
",,,,,,,,,,PARQUET-110,,,,,,,,,,10/Oct/14 11:28;lbendig;PIG-4219-2.patch;https://issues.apache.org/jira/secure/attachment/12674153/PIG-4219-2.patch,07/Oct/14 17:04;lbendig;PIG-4219.patch;https://issues.apache.org/jira/secure/attachment/12673383/PIG-4219.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-10-07 17:04:35.909,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Oct 10 17:53:01 UTC 2014,,,,,,,0|i20pf3:,9223372036854775807,,,,,,,,,,"07/Oct/14 17:04;lbendig;It seems to me that if the outermost tuple in a bag has only one field it will be thrown away when the logical plan is created.
LogicalPlanGenerator.g:
{code}
bag_type returns[LogicalSchema logicalSchema]
 : ^( BAG_TYPE IDENTIFIER? tuple_type? )
   {
       if ($tuple_type.logicalSchema!=null && $tuple_type.logicalSchema.size()==1 &&  
            $tuple_type.logicalSchema.getField(0).type==DataType.TUPLE) {
           $logicalSchema = $tuple_type.logicalSchema;
       }
       else {
           LogicalSchema s = new LogicalSchema();
           s.addField(new LogicalFieldSchema($IDENTIFIER.text, $tuple_type.logicalSchema, DataType.TUPLE));
           $logicalSchema = s;
       }
   }
;
{code}
By only having the code from the else branch solves the problem and all unit tests pass as well. I attached a patch, however, I'm not sure about the consequences. 
[~daijy] what do you think (if/else branch has been introduced with PIG-1876) ?","10/Oct/14 00:40;daijy;I don't completely recall but that probably to address the backward compatibility. Before we can define the bag as bag{num1: int,num2: int} and compiler will automatically add a tuple. So when we get bag schema back, we don't know the inner tuple is artificially added or the original one, so we use this brutal rule. Now I don't think it is a problem. bag{num1: int,num2: int} syntax is deprecated for versions, we don't need this rule, which is wrong anyway.

So I +1 for the patch, but please add a test case.","10/Oct/14 11:28;lbendig;Daniel, thank you for the clarification. Added a testcase.","10/Oct/14 15:48;rohini;bq. syntax is deprecated for versions, we don't need this rule, which is wrong anyway.
  Shouldn't we also remove the deprecated syntax and throw error now that we are removing the check for it?",10/Oct/14 17:06;daijy;This syntax is already not allowed.,10/Oct/14 17:08;daijy;Patch committed to both trunk and 0.14 branch. Thanks Lorand!,10/Oct/14 17:53;lbendig;Thanks Rohini and Daniel for the review!,,,,,,,,,,,,,,,,,,,,,
Pig OrcStorage fail to load a map with null key,PIG-4218,12745345,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,01/Oct/14 20:13,21/Nov/14 05:58,14/Mar/19 03:08,01/Oct/14 23:07,,,,,,0.14.0,,impl,,,0,,,,,,,"Error message:

Backend error message
---------------------
AttemptID:attempt_1403634189382_0006_m_000000_1 Info:Error: java.lang.NullPointerException
at org.apache.pig.impl.util.orc.OrcUtils.convertOrcToPig(OrcUtils.java:97)
at org.apache.pig.impl.util.orc.OrcUtils.convertOrcToPig(OrcUtils.java:82)
at org.apache.pig.builtin.OrcStorage.getNext(OrcStorage.java:312)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:211)
at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:533)
at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)",,,,,,,,,,,,,,,,,,,,01/Oct/14 21:06;daijy;PIG-4218-1.patch;https://issues.apache.org/jira/secure/attachment/12672410/PIG-4218-1.patch,01/Oct/14 21:06;daijy;nullmapkey.orc;https://issues.apache.org/jira/secure/attachment/12672411/nullmapkey.orc,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-10-01 22:57:39.799,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Oct 01 23:07:12 UTC 2014,,,,,,,0|i20p8v:,9223372036854775807,,,,,,,,,,"01/Oct/14 21:06;daijy;Ignore the map entry with null key. Hive does it the same way (HIVE-8115). Hive also plan to disable storing map with null key. For legacy data, Pig should not die. ","01/Oct/14 22:57;rohini;Clarified with Daniel why we can't support null keys in Pig. Pig can support null keys, but we are dropping them only to have same behavior while processing as hive 0.14 and HCatLoader.

+1.  ",01/Oct/14 23:07;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
Fix documentation in BuildBloom,PIG-4217,12745232,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,praveenr019,praveenr019,praveenr019,01/Oct/14 11:10,21/Nov/14 05:58,14/Mar/19 03:08,01/Oct/14 17:57,,,,,,0.14.0,,,,,0,,,,,,,"/**
 * Build a bloom filter for use later in Bloom.  This UDF is intended to run
 * in a group all job.  For example:
 * define bb BuildBloom('jenkins', '100', '0.1');
 * A = load 'foo' as (x, y);
 * B = group A all;
 * C = foreach B generate BuildBloom(A.x);
 * store C into 'mybloom';
 * The bloom filter can be on multiple keys by passing more than one field
 * (or the entire bag) to BuildBloom.
 * The resulting file can then be used in a Bloom filter as:
 * define bloom Bloom(mybloom);
 * A = load 'foo' as (x, y);
 * B = load 'bar' as (z);
 * C = filter B by Bloom(z);
 * D = join C by z, A by x;
 * It uses {@link org.apache.hadoop.util.bloom.BloomFilter}.
 */

Pig script inside above doc strings doesn't work",,,,,,,,,,,,,,,,,,,,01/Oct/14 11:14;praveenr019;PIG-4217-1.patch;https://issues.apache.org/jira/secure/attachment/12672307/PIG-4217-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-01 17:57:55.093,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Oct 01 17:57:55 UTC 2014,,,,,,,0|i20okf:,9223372036854775807,,,,,,,,,,01/Oct/14 17:57;daijy;Patch committed to trunk and 0.14 branch. Thanks Praveen!,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unit test failure TestParamSubPreproc and TestMacroExpansion,PIG-4215,12745051,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,30/Sep/14 20:25,21/Nov/14 05:59,14/Mar/19 03:08,01/Oct/14 06:47,,,,,,0.14.0,,impl,,,0,,,,,,,Unit tests are broken by PIG-4080. We shall fix them.,,,,,,,,,,,,,,,,,,,,30/Sep/14 20:25;daijy;PIG-4215-1.patch;https://issues.apache.org/jira/secure/attachment/12672127/PIG-4215-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-30 23:51:40.104,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Oct 01 06:47:07 UTC 2014,,,,,,,0|i20nh3:,9223372036854775807,,,,,,,,,,30/Sep/14 23:51;rohini;+1,01/Oct/14 06:47;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unit test fail TestMRJobStats,PIG-4214,12745044,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,30/Sep/14 19:55,21/Nov/14 05:58,14/Mar/19 03:08,01/Oct/14 18:01,,,,,,0.14.0,,impl,,,0,,,,,,,TestMRJobStats is broken by PIG-4050. We shall fix it.,,,,,,,,,,,,,,,,,,,,30/Sep/14 19:56;daijy;PIG-4214-1.patch;https://issues.apache.org/jira/secure/attachment/12672109/PIG-4214-1.patch,01/Oct/14 06:42;daijy;PIG-4214-2.patch;https://issues.apache.org/jira/secure/attachment/12672266/PIG-4214-2.patch,01/Oct/14 18:00;daijy;PIG-4214-3.patch;https://issues.apache.org/jira/secure/attachment/12672370/PIG-4214-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-10-01 02:40:04.382,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Oct 01 18:01:59 UTC 2014,,,,,,,0|i20nfj:,9223372036854775807,,,,,,,,,,01/Oct/14 02:40;rohini;Can do java.util.Arrays.asList(mapTaskReports).iterator() instead of using org.apache.commons.collections4.iterators.ArrayIterator.  commons-collection4 dependency is only there with tez and will not be there for hadoop 1.x. ,01/Oct/14 06:42;daijy;Good catch! Attach revised patch.,01/Oct/14 17:06;rohini;+1.  Could you change it to import java.util.Arrays instead of referencing full package name every time before checking in?,"01/Oct/14 18:00;daijy;Sure, reattach patch.",01/Oct/14 18:01;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,
Allow LIMIT of 0 for variableLimit (constant 0 is already allowed),PIG-4212,12744978,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,knoguchi,knoguchi,knoguchi,30/Sep/14 15:31,21/Nov/14 05:58,14/Mar/19 03:08,01/Oct/14 18:07,,,,,,0.14.0,,,,,0,,,,,,,"Somehow 
    limit A 0 
is currently allowed but not
   limit A B.count - B.count 

I'd like the latter to be also allowed.",,,,,,,,,,,,,,,,,,,,30/Sep/14 15:33;knoguchi;pig-4212-v1.patch;https://issues.apache.org/jira/secure/attachment/12672047/pig-4212-v1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-30 18:14:32.358,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Oct 01 18:07:57 UTC 2014,,,,,,,0|i20n13:,9223372036854775807,,,,,,,,,,"30/Sep/14 15:32;knoguchi;One of our users has a logic in their script that requires LIMIT on 0.

This user wants to store at most N outputs total from contentA and contentB.  Giving preference on contentA. 
{noformat}
limitContentA = limit contentA N; 
groupedContent_A = GROUP limitContentA ALL;
limitContentB = limit contentA N - totalStoredContent_A.$0

... store both limitContentA and limitContentB
{noformat}
",30/Sep/14 15:33;knoguchi;Straightforward patch that accepts 0 variablelimit.,30/Sep/14 18:14;daijy;+1,01/Oct/14 03:18;rohini;Can you just add a newline before @Test before checking in the patch?,"01/Oct/14 18:07;knoguchi;Committed to 0.14 and trunk (with extra line added pointed out by Rohini).

Thanks Daniel and Rohini for the review! ",,,,,,,,,,,,,,,,,,,,,,,
e2e test property-check does not check all prerequisites,PIG-4205,12744523,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,28/Sep/14 02:00,21/Nov/14 05:58,14/Mar/19 03:08,28/Sep/14 03:10,,,,,,0.14.0,,,,,0,,,,,,,"env:hadoop1 pig-0.14
according to https://cwiki.apache.org/confluence/display/PIG/HowToTest#HowToTest-HowtoRune2eTests
how to build:
ant jar
run test-e2e-deploy:
ant -Dharness.cluster.conf=$HADOOP_CONF_DIR -Dharness.cluster.bin=$HADOOP_BIN  test-e2e-deploy>output/test-e2e-deploy.hadoop1

 following error is found in log:
>> 67 Going to run /home/zly/prj/oss/pig/test/e2e/pig/../../../bin/pig 
>> -e mkdir /user/pig/out/root-1411632015-nightly.conf/
>>     168 Cannot locate pig-core-h2.jar. do 'ant -Dhadoopversion=23 
>> jar', and try again
",,,,,,,,,,,,,,,,,,,,28/Sep/14 02:02;kellyzly;PIG-4205.patch;https://issues.apache.org/jira/secure/attachment/12671679/PIG-4205.patch,28/Sep/14 02:52;kellyzly;PIG-4205_1.patch;https://issues.apache.org/jira/secure/attachment/12671682/PIG-4205_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-09-28 02:36:39.595,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sun Sep 28 03:10:43 UTC 2014,,,,,,,0|i20k8v:,9223372036854775807,,,,,,,,,,"28/Sep/14 02:10;kellyzly;we need also set -Dharness.hadoop.home in command ""ant -Dharness.cluster.conf=$HADOOP_CONF_DIR -Dharness.cluster.bin=$HADOOP_BIN -Dharness.hadoop.home=$HADOOP_HOME test-e2e-deploy"".  property-check in $PIG_HOME/test/e2e/pig/build.xml  should check whether ""harness.cluster.conf"",""harness.cluster.bin"" and ""harness.hadoop.home"" variables are set or not.
{code:xml}
<target name=""property-check"">
    <fail message=""Please set the property harness.cluster.conf to the conf directory of your hadoop installation and harness.hadoop.home to HADOOP_HOME for your Hadoop installation."">
        <condition>
            <not>
                <and>
                <isset property=""harness.cluster.conf""/>
                <isset property=""harness.hadoop.home""/>
                </and>
            </not>
        </condition>
    </fail>
    <fail message=""Please set the property harness.cluster.bin to the binary executable of your hadoop installation and harness.hadoop.home to HADOOP_HOME for your Hadoop installation."">
        <condition>
            <not>
                <and>
                <isset property=""harness.cluster.bin""/>
                <isset property=""harness.hadoop.home""/>
                </and>
            </not>
        </condition>
    </fail>
  </target>
{code}
",28/Sep/14 02:36;daijy;We shall use a single condition with all three checks. Also change the title to describe the issue more accurately.,"28/Sep/14 02:52;kellyzly;Update PIG-4205_1.patch. Use a single condition with all three checks(harness.cluster.conf, harness.hadoop.home, harness.cluster.bin).",28/Sep/14 03:10;daijy;Patch committed to trunk. Thanks Liyun!,,,,,,,,,,,,,,,,,,,,,,,,
Native e2e tests fail when run against old version of pig,PIG-4201,12744051,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,25/Sep/14 15:19,21/Nov/14 05:59,14/Mar/19 03:08,25/Sep/14 18:57,,,,,,0.14.0,,,,,0,,,,,,,"Native_1 and Native_2 fail while running the benchmark against old version of pig as they don't understand the new ""native"" keyword. Need to include a verify_pig_script section for them with ""mapreduce"" keyword instead. ",,,,,,,,,,,,,,,,,,,,25/Sep/14 15:57;rohini;PIG-4201-1.patch;https://issues.apache.org/jira/secure/attachment/12671245/PIG-4201-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-25 18:48:29.683,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Sep 25 18:57:04 UTC 2014,,,,,,,0|i20hen:,9223372036854775807,,,,,,,,,,25/Sep/14 18:48;daijy;+1,25/Sep/14 18:57;rohini;Committed to trunk (0.14). Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo in Job Stats header: MinMapTIme => MinMapTime,PIG-4197,12743690,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,jmartell7,jmartell7,jmartell7,24/Sep/14 01:47,21/Nov/14 05:58,14/Mar/19 03:08,24/Sep/14 05:35,0.12.0,,,,,0.14.0,,,,,0,,,,,,,There is a capitalization typo in the Job Stats header: MinMapTIme => MinMapTime.,,,,,,,,,,,,,,,,,,,,24/Sep/14 01:48;jmartell7;typo.patch;https://issues.apache.org/jira/secure/attachment/12670872/typo.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-24 05:35:34.136,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Sep 24 05:35:34 UTC 2014,,,Patch Available,,,,0|i20f7z:,9223372036854775807,,,,,,,,,,24/Sep/14 05:35;daijy;Patch committed to trunk. Thanks Joshua contributing!,,,,,,,,,,,,,,,,,,,,,,,,,,,
Auto ship udf jar is broken,PIG-4196,12743591,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,23/Sep/14 19:59,21/Nov/14 05:59,14/Mar/19 03:08,01/Oct/14 18:03,,,,,,0.14.0,,impl,,,0,,,,,,,The mechanism to ship udf containing jar is broken in PIG-4054. Attach a quick fix.,,,,,,,,,,,,,,,,,,,,23/Sep/14 20:00;daijy;PIG-4196-0.patch;https://issues.apache.org/jira/secure/attachment/12670791/PIG-4196-0.patch,26/Sep/14 06:45;daijy;PIG-4196-1.patch;https://issues.apache.org/jira/secure/attachment/12671392/PIG-4196-1.patch,01/Oct/14 06:56;daijy;PIG-4196-2.patch;https://issues.apache.org/jira/secure/attachment/12672274/PIG-4196-2.patch,03/Oct/14 04:04;daijy;PIG-4196-fixtest.patch;https://issues.apache.org/jira/secure/attachment/12672729/PIG-4196-fixtest.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-10-01 02:47:33.581,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Oct 03 04:04:18 UTC 2014,,,,,,,0|i20elr:,9223372036854775807,,,,,,,,,,26/Sep/14 06:45;daijy;Add test case.,"01/Oct/14 02:47;rohini;Shouldn't it be

{code}
String jar = JarManager.findContainingJar(clazz);
if (jar!=null) {
   jar = new File(jar).toURI().toURL();
   if(!allJars.contains(jar)) {
     allJars.add();
  }
}
{code}","01/Oct/14 06:56;daijy;Thanks, you are absolutely right! Revised patch.",01/Oct/14 17:07;rohini;+1,01/Oct/14 18:03;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,03/Oct/14 04:04;daijy;TestGrunt is broken. Commit a quick fix.,,,,,,,,,,,,,,,,,,,,,,
Support loading char/varchar data in OrcStorage,PIG-4195,12743417,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,23/Sep/14 01:58,21/Nov/14 05:58,14/Mar/19 03:08,25/Sep/14 21:30,,,,,,0.14.0,,internal-udfs,,,0,,,,,,,Loading Char/Varchar is missing in OrcStorage. It should be straightforward to fix.,,,,,,,,,,,,,,,,,,,,23/Sep/14 01:59;daijy;PIG-4195-0.patch;https://issues.apache.org/jira/secure/attachment/12670578/PIG-4195-0.patch,25/Sep/14 20:45;daijy;PIG-4195-1.patch;https://issues.apache.org/jira/secure/attachment/12671306/PIG-4195-1.patch,25/Sep/14 20:54;daijy;charvarchar.orc;https://issues.apache.org/jira/secure/attachment/12671309/charvarchar.orc,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-09-23 03:33:22.173,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Sep 25 21:30:29 UTC 2014,,,,,,,0|i20djj:,9223372036854775807,,,,,,,,,,23/Sep/14 01:59;daijy;Attach patch pending testcase.,23/Sep/14 03:33;rohini;+1,25/Sep/14 20:45;daijy;Adding test case.,25/Sep/14 20:53;rohini;+1.  The binary file charvarchar.orc is not uploaded. Please do not miss checking that in.,25/Sep/14 21:30;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,
ReadToEndLoader does not call setConf on pigSplit in initializeReader,PIG-4194,12743373,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,shadanan,shadanan,shadanan,22/Sep/14 23:11,21/Nov/14 05:58,14/Mar/19 03:08,23/Sep/14 04:06,0.12.0,,,,,0.14.0,,impl,,,0,easyfix,patch,,,,,"When using ReadToEndLoader to wrap a LoadFunc, if the LoadFunc uses the conf object in the prepareToRead() method, the conf object will be null because ReadToEndLoader does not call setConf on the pigSplit in initializeReader().",,,,,,,,,,,,,,,,,,,,23/Sep/14 03:05;shadanan;read_to_end_loader.patch;https://issues.apache.org/jira/secure/attachment/12670591/read_to_end_loader.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-23 03:48:44.488,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Sep 23 04:05:23 UTC 2014,,,,,,,0|i20d7z:,9223372036854775807,,,,,,,,,,23/Sep/14 03:03;shadanan;Here's the patch with included unit test to verify that downstream readers will have access to a non-null conf object from the pigSplit.,"23/Sep/14 03:48;rohini;[~shadanan],
   You will have to mark the jira patch available for it to be reviewed and committed. Once that is done, the committer will resolve the jira.",23/Sep/14 04:02;shadanan;Patch is available as read_to_end_loader.patch.,"23/Sep/14 04:03;rohini; +1. Committed to trunk (0.14). Thanks for the patch [~shadanan]. 

Missed committing the test. But left it at that as this is a simple NPE fix and would like to keep the number of unit tests less to more important ones to keep the time to run unit tests down. ",23/Sep/14 04:03;shadanan;Thanks Rohini for the help. I wasn't sure about the procedure.,23/Sep/14 04:05;rohini;Not a problem at all. We are here to help with that. https://cwiki.apache.org/confluence/display/PIG/HowToContribute has good information.,,,,,,,,,,,,,,,,,,,,,,
Fix Orc e2e tests,PIG-4187,12742965,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,20/Sep/14 17:54,21/Nov/14 05:58,14/Mar/19 03:08,21/Sep/14 04:30,,,,,,0.14.0,,impl,,,0,,,,,,,"Several Orc e2e tests still fail on trunk. There are several causes:
1. kryo.jar is missing if using PPD
2. UDFContext.tss is not updated in tez container reuse (Orc_3 on tez fail, the second vertex get old UDFContext)
3. An old time bug PreCombinerLocalRearrange inner plan does not reset parentPlan (Orc_3 on mr fail, the third MR job fail with ClassNotFound exception for Orc class, because a stale parentPlan contains the whole physicalPlan)",,,,,,,,,,,,,,,,,,,,20/Sep/14 17:55;daijy;PIG-4187-1.patch;https://issues.apache.org/jira/secure/attachment/12670239/PIG-4187-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-20 21:11:30.767,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sun Sep 21 04:30:07 UTC 2014,,,,,,,0|i20aqv:,9223372036854775807,,,,,,,,,,20/Sep/14 21:11;rohini;+1,21/Sep/14 04:30;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix e2e run against new build of pig and some enhancements,PIG-4186,12742833,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,19/Sep/14 20:09,21/Nov/14 05:59,14/Mar/19 03:08,19/Sep/14 22:38,,,,,,0.14.0,,,,,0,,,,,,,"  Some recent changes in trunk has broken the ability to run against a new packaged version of pig. Running against source code is fine. Also making few enhancements like 
  - making piggybank jar path really configurable, 
  - adding a script that we used to parse the e2e result log and output in junit xml format for jenkins job.",,,,,,,,,,,,,,,,,,,,19/Sep/14 20:16;rohini;PIG-4186-1.patch;https://issues.apache.org/jira/secure/attachment/12670086/PIG-4186-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-19 21:44:08.688,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Sep 20 14:44:36 UTC 2014,,,,,,,0|i209uf:,9223372036854775807,,,,,,,,,,19/Sep/14 21:44;daijy;Is that because you have a different name for piggybank.jar?,"19/Sep/14 21:48;rohini;No. Before it was expected to be always under contrib/piggybank/java. In our package, we have it in the lib directory.",19/Sep/14 22:10;daijy;I see. +1,"19/Sep/14 22:38;rohini; Committed to trunk (0.14). Thanks Daniel for the review.

 Also added information on the xmlReport.pl in
https://cwiki.apache.org/confluence/pages/diffpagesbyversion.action?pageId=27825753&selectedPageVersions=15&selectedPageVersions=14 . I did not change other code blocks but the diff shows some spacing changes in them. Checked that it is not an issue and page looks fine .","19/Sep/14 23:04;hudson;FAILURE: Integrated in Hive-branch-0.12-hadoop1 #45 (See [https://builds.apache.org/job/Hive-branch-0.12-hadoop1/45/])
PIG-4186: Fix e2e run against new build of pig and some enhancements (rohini) (rohini: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1626356)
* /pig/trunk/CHANGES.txt
* /pig/trunk/test/e2e/harness/TestDriver.pm
* /pig/trunk/test/e2e/harness/xmlReport.pl
* /pig/trunk/test/e2e/pig/build.xml
* /pig/trunk/test/e2e/pig/conf/default.conf
* /pig/trunk/test/e2e/pig/conf/local.conf
* /pig/trunk/test/e2e/pig/conf/tez.conf
* /pig/trunk/test/e2e/pig/drivers/TestDriverPig.pm
* /pig/trunk/test/e2e/pig/tests/nightly.conf
* /pig/trunk/test/e2e/pig/udfs/java/build.xml
","20/Sep/14 14:44;hudson;FAILURE: Integrated in Hive-branch-0.12-hadoop2 #35 (See [https://builds.apache.org/job/Hive-branch-0.12-hadoop2/35/])
PIG-4186: Fix e2e run against new build of pig and some enhancements (rohini) (rohini: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1626356)
* /pig/trunk/CHANGES.txt
* /pig/trunk/test/e2e/harness/TestDriver.pm
* /pig/trunk/test/e2e/harness/xmlReport.pl
* /pig/trunk/test/e2e/pig/build.xml
* /pig/trunk/test/e2e/pig/conf/default.conf
* /pig/trunk/test/e2e/pig/conf/local.conf
* /pig/trunk/test/e2e/pig/conf/tez.conf
* /pig/trunk/test/e2e/pig/drivers/TestDriverPig.pm
* /pig/trunk/test/e2e/pig/tests/nightly.conf
* /pig/trunk/test/e2e/pig/udfs/java/build.xml
",,,,,,,,,,,,,,,,,,,,,,
UDF backward compatibility issue after POStatus.STATUS_NULL refactory,PIG-4184,12742551,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/Sep/14 18:48,10/Jan/16 01:47,14/Mar/19 03:08,17/Oct/14 21:30,,,,,,0.14.0,,impl,,,0,,,,,,,"This is the same issue we discussed in PIG-3739 and PIG-3679. However, our previous fix does not solve the issue, in fact, it make things worse and it is totally my fault.

Consider the following UDF and script:
{code}
    public class IntToBool extends EvalFunc<Boolean> {
        @Override
        public Boolean exec(Tuple input) throws IOException {
            if (input == null || input.size() == 0)
                return null;
            Integer val = (Integer)input.get(0);
            return (val == null || val == 0) ? false : true;
        }
    }
{code}
{code}
a = load '1.txt' as (i0:int, i1:int);
b = foreach a generate IntToBool(i0);
store b into 'output';
{code}
1.txt
{code}
1
2   3
{code}
With Pig 0.12, we get:
{code}
(false)
(true)
{code}
With Pig 0.13/0.14, we get:
{code}
()
(true)
{code}
The reason is in 0.12, Pig pass first row as a tuple with a null item to IntToBool, with 0.13/0.14, Pig swallow the first row, which is not right. And this wrong behavior is brought by PIG-3739 and PIG-3679.

Before that (but after POStatus.STATUS_NULL refactory PIG-3568), we do have a behavior change which makes e2e test StreamingPythonUDFs_10 fail with NPE. However, I think this is an inconsistent behavior of 0.12. Consider the following scripts:
{code}
a = load '1.txt' as (name:chararray, age:int, gpa:double);
b = foreach a generate ROUND((gpa>3.0?gpa+1:gpa));
store b into 'output';
{code}
{code}
a = load '1.txt' as (name:chararray, age:int, gpa:double);
b = foreach a generate ROUND(gpa);
store b into 'output';
{code}
If gpa field is null, script 1 skip the row and script 2 fail with NPE, which does not make sense. So my thinking is:
1. Pig 0.12 is wrong and POStatus.STATUS_NULL refactory fix this behavior (we don't need related fix in PIG-3739/PIG-3679)
2. ROUND (and some other UDF) is wrong anyway, we shall fix it",,,,,,,,,,,,,PIG-4774,,,,,,,18/Sep/14 19:16;daijy;PIG-4184-1.patch;https://issues.apache.org/jira/secure/attachment/12669775/PIG-4184-1.patch,13/Oct/14 00:12;daijy;PIG-4184-piggybank-1.patch;https://issues.apache.org/jira/secure/attachment/12674440/PIG-4184-piggybank-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-10-10 21:00:24.656,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Oct 17 21:30:23 UTC 2014,,,,,,,0|i2085r:,9223372036854775807,,,,,,,,,,"10/Oct/14 21:00;cheolsoo;+1.

I ran into this problem while using another UDF (Datafu [TransposeTupleToBag|http://datafu.incubator.apache.org/docs/datafu/1.1.0/datafu/pig/util/TransposeTupleToBag.html]). The output differs between Pig 0.12 and 0.13 when passing null tuples.","10/Oct/14 21:20;rohini;[~daijy], 
   if (input == null || input.size() == 0) is in way more UDF classes in piggybank and builtin than the classes you just changed. Wouldn't all those UDFs and user UDFs doing the same break?
","13/Oct/14 00:12;daijy;Going through the piggybank and fix the null handling part. Note those UDFs do not handle null before, even without PIG-4184-1.patch. However, before PIG-4184-1.patch, some NPE does not manifest if we don't feed null to UDF directly.","17/Oct/14 15:27;rohini;+1 for both the patches.
 ","17/Oct/14 21:30;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini, Cheolsoo!",,,,,,,,,,,,,,,,,,,,,,,
e2e tests Scripting_[1-12] fail on Windows,PIG-4182,12741953,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Sep/14 21:29,21/Nov/14 05:59,14/Mar/19 03:08,01/Nov/14 17:45,,,,,,0.14.0,,impl,,,0,,,,,,,"Error message:
{code}
2014-09-11 12:37:56,622 [main] ERROR org.apache.pig.tools.pigstats.PigStats - ERROR 0: org.apache.pig.backend.executionengine.ExecException: ERROR 2997: Unable to recreate exception from backed error: AttemptID:attempt_1410405156681_1228_m_000000_3 Info:Error: java.io.IOException: Deserialization error: could not instantiate 'org.apache.pig.scripting.jython.JythonFunction' with arguments '[D:\hdp\pig-0.14.0.2.2.0.0-1181\test\e2e\pig\testdist\libexec\python\scriptingudf.py, square]'
	at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:62)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.setup(PigGenericMapBase.java:180)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
Caused by: java.lang.RuntimeException: could not instantiate 'org.apache.pig.scripting.jython.JythonFunction' with arguments '[D:\hdp\pig-0.14.0.2.2.0.0-1181\test\e2e\pig\testdist\libexec\python\scriptingudf.py, square]'
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:778)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.instantiateFunc(POUserFunc.java:124)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.readObject(POUserFunc.java:584)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1004)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1891)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.ArrayList.readObject(ArrayList.java:733)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1004)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1891)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.HashMap.readObject(HashMap.java:1155)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1004)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1891)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.ArrayList.readObject(ArrayList.java:733)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1004)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1891)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.HashMap.readObject(HashMap.java:1155)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1004)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1891)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:60)
	... 9 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:746)
	... 75 more
Caused by: java.lang.IllegalStateException: Could not initialize: D:\hdp\pig-0.14.0.2.2.0.0-1181\test\e2e\pig\testdist\libexec\python\scriptingudf.py
	at org.apache.pig.scripting.jython.JythonFunction.<init>(JythonFunction.java:92)
	... 80 more
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 1121: Python Error. Traceback (most recent call last):
  File ""D:\hdp\pig-0.14.0.2.2.0.0-1181\test\e2e\pig\testdist\libexec\python\scriptingudf.py"", line 21, in <module>
ImportError: No module named stringutil

	at org.apache.pig.scripting.jython.JythonScriptEngine$Interpreter.execfile(JythonScriptEngine.java:249)
	at org.apache.pig.scripting.jython.JythonScriptEngine$Interpreter.init(JythonScriptEngine.java:163)
	at org.apache.pig.scripting.jython.JythonScriptEngine.getFunction(JythonScriptEngine.java:388)
	at org.apache.pig.scripting.jython.JythonFunction.<init>(JythonFunction.java:55)
	... 80 more
Caused by: Traceback (most recent call last):
  File ""D:\hdp\pig-0.14.0.2.2.0.0-1181\test\e2e\pig\testdist\libexec\python\scriptingudf.py"", line 21, in <module>
ImportError: No module named stringutil

	at org.python.core.Py.ImportError(Py.java:304)
	at org.python.core.imp.import_first(imp.java:755)
	at org.python.core.imp.import_module_level(imp.java:837)
	at org.python.core.imp.importName(imp.java:917)
	at org.python.core.ImportFunction.__call__(__builtin__.java:1220)
	at org.python.core.PyObject.__call__(PyObject.java:357)
	at org.python.core.__builtin__.__import__(__builtin__.java:1173)
	at org.python.core.imp.importOne(imp.java:936)
	at org.python.pycode._pyx3.f$0(D:\hdp\pig-0.14.0.2.2.0.0-1181\test\e2e\pig\testdist\libexec\python\scriptingudf.py:97)
	at org.python.pycode._pyx3.call_function(D:\hdp\pig-0.14.0.2.2.0.0-1181\test\e2e\pig\testdist\libexec\python\scriptingudf.py)
	at org.python.core.PyTableCode.call(PyTableCode.java:165)
	at org.python.core.PyCode.call(PyCode.java:18)
	at org.python.core.Py.runCode(Py.java:1275)
	at org.python.util.PythonInterpreter.execfile(PythonInterpreter.java:235)
	at org.apache.pig.scripting.jython.JythonScriptEngine$Interpreter.execfile(JythonScriptEngine.java:217)
	... 83 more
{code}",,,,,,,,,,,,,,,,,,,,16/Sep/14 21:29;daijy;PIG-4182-1.patch;https://issues.apache.org/jira/secure/attachment/12669220/PIG-4182-1.patch,30/Oct/14 02:06;daijy;PIG-4182-2.patch;https://issues.apache.org/jira/secure/attachment/12678093/PIG-4182-2.patch,01/Nov/14 17:44;daijy;PIG-4182-3.patch;https://issues.apache.org/jira/secure/attachment/12678694/PIG-4182-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-09-26 22:23:34.911,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sat Nov 01 17:45:43 UTC 2014,,,,,,,0|i204jr:,9223372036854775807,,,,,,,,,,26/Sep/14 22:23;rohini;Wouldn't the windows file name start with drive letter (C:\\somedir\somefile)? Earlier it was removing the : after the drive letter for Windows and will also remove / in absolute paths of Linux. Dont understand what case the new regex address.  What is it for?,"30/Oct/14 02:06;daijy;This is for name not path. Actually we get a wrong name which starts with a ""\"" (or ""/"" on Linux). Attach a better fix.",30/Oct/14 03:19;rohini;+1,30/Oct/14 06:02;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,01/Nov/14 05:40;daijy;This breaks e2e tests Scripting and StreamingPythonUDFs. Actually PIG-4182-1.patch works but not PIG-4182-2.patch. ,"01/Nov/14 17:44;daijy;The pos change alone in PIG-4182-2.patch should fix the issue.

We need to ship two types of jython scripts:
1. The one we register in Pig script
2. Dependent modules of #1

For #1, addScriptFile uses absolute path of the script when adding to PigScriptUDF.jar, eg, /home/hortonji/pig/test/e2e/pig/testdist/libexec/python/scriptingudf.py on Linux, or D:\Users\daijy\pig\test\e2e\pig\testdist\libexec\python\scriptingudf.py on Windows. However, the prefix ""/"" has to be removed, otherwise, we have trouble to read back in ScriptEngine.getScriptAsStream. That's why PIG-4182-3.patch fail.

For #2, JythonScriptEngine.getModuleState has a bug, which doesn't remove the path separator in the dependent script name, eg, we get /stringutil.py on Linux, or get \stringutil.py on Windows. That's why Windows test fail before the patch.","01/Nov/14 17:45;daijy;PIG-4182-3.patch committed to both trunk and 0.14 branch.
",,,,,,,,,,,,,,,,,,,,,
Cannot launch tez e2e test on Windows,PIG-4181,12741951,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Sep/14 21:25,21/Nov/14 05:58,14/Mar/19 03:08,14/Oct/14 06:30,,,,,,0.14.0,,e2e harness,,,0,,,,,,,"whoami gives an extra ""^M"" in tez.conf. Have similar fix in other configs. ",,,,,,,,,,,,,,,,,,,,16/Sep/14 21:26;daijy;PIG-4181-1.patch;https://issues.apache.org/jira/secure/attachment/12669217/PIG-4181-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-13 23:30:15.745,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Oct 14 06:30:27 UTC 2014,,,,,,,0|i204jb:,9223372036854775807,,,,,,,,,,13/Oct/14 23:30;rohini;+1,14/Oct/14 06:30;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e test Native_3 fail on Hadoop 2,PIG-4180,12741948,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Sep/14 21:23,21/Nov/14 05:59,14/Mar/19 03:08,28/Sep/14 01:39,,,,,,0.14.0,,impl,,,0,,,,,,,"Test harness should use hadoop-0.23.0-streaming.jar to run Native_3 on Hadoop 2. See failure on Windows, not sure why it does not manifest on Linux.",,,,,,,,,,,,,,,,,,,,16/Sep/14 21:23;daijy;PIG-4180-1.patch;https://issues.apache.org/jira/secure/attachment/12669211/PIG-4180-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-26 22:17:19.986,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sun Sep 28 01:39:35 UTC 2014,,,,,,,0|i204in:,9223372036854775807,,,,,,,,,,26/Sep/14 22:17;rohini;+1,28/Sep/14 01:39;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
HCatDDL_[1-3] fail on Windows,PIG-4178,12741946,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Sep/14 21:18,21/Nov/14 05:59,14/Mar/19 03:08,28/Sep/14 01:38,,,,,,0.14.0,,impl,,,0,,,,,,,"Pig fail to invoke ""python hcat.py"", which is the supposed approach to invoke hcat on Windows.",,,,,,,,,,,,,,,,,,,,16/Sep/14 21:19;daijy;PIG-4178-1.patch;https://issues.apache.org/jira/secure/attachment/12669208/PIG-4178-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-26 22:29:33.159,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sun Sep 28 01:38:01 UTC 2014,,,,,,,0|i204i7:,9223372036854775807,,,,,,,,,,26/Sep/14 22:29;rohini;+1,28/Sep/14 01:38;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
BigData_1 fail after PIG-4149,PIG-4177,12741945,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Sep/14 21:14,21/Nov/14 05:58,14/Mar/19 03:08,19/Sep/14 17:34,,,,,,0.14.0,,impl,,,0,,,,,,,"This happens when samples size is very small (smaller than numQuantiles). Error message:
{code}
2014-09-09 05:58:27,255 [main] ERROR org.apache.pig.tools.grunt.GruntParser - ERROR 0: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [POUserFunc (Name: POUserFunc(org.apache.pig.impl.builtin.FindQuantiles)[tuple] - scope-62 Operator Key: scope-62) children: null at []]: java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 1
{code}",,,,,,,,,,,,,,,,,,,,16/Sep/14 21:15;daijy;PIG-4177-1.patch;https://issues.apache.org/jira/secure/attachment/12669206/PIG-4177-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-19 04:11:16.198,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Sep 19 17:34:34 UTC 2014,,,,,,,0|i204hz:,9223372036854775807,,,,,,,,,,18/Sep/14 20:37;daijy;This also fix the currently failed tez unit test: TestEvalPipeline2.testLimitAutoReducer.,19/Sep/14 04:11;rohini;+1,19/Sep/14 17:34;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
Fix tez e2e test Bloom_[1-3],PIG-4176,12741943,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,16/Sep/14 21:08,21/Nov/14 05:58,14/Mar/19 03:08,26/Sep/14 22:00,,,,,,0.14.0,,tez,,,0,,,,,,,"Test fail with message:
{code}
                    : ], TaskAttempt 1 failed, info=[Error: Failure while running task:org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing (Name: F: Store(/user/pig/out/daijy-1410899730-nightly.conf/Bloom_1.out:org.apache.pig.builtin.PigStorage) - scope-62 Operator Key: scope-62): org.apache.pig.backend.executionengine.ExecException: ERROR 2078: Caught error from UDF: Bloom [./tmp_daijy-1410899730-nightly.conf_mybloom_1/part-r-00000 (No such file or directory)]
                    : at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:310)
                    : at org.apache.pig.backend.hadoop.executionengine.tez.POStoreTez.getNextTuple(POStoreTez.java:113)
                    : at org.apache.pig.backend.hadoop.executionengine.tez.PigProcessor.runPipeline(PigProcessor.java:319)
                    : at org.apache.pig.backend.hadoop.executionengine.tez.PigProcessor.run(PigProcessor.java:198)
                    : at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
                    : at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:180)
                    : at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:172)
                    : at java.security.AccessController.doPrivileged(Native Method)
                    : at javax.security.auth.Subject.doAs(Subject.java:394)
                    : at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
                    : at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:172)
                    : at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:167)
                    : at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
                    : at java.util.concurrent.FutureTask.run(FutureTask.java:138)
                    : at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
                    : at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
                    : at java.lang.Thread.run(Thread.java:695)
{code}",,,,,,,,,,,,,,,,,,,,16/Sep/14 21:09;daijy;PIG-4176-1.patch;https://issues.apache.org/jira/secure/attachment/12669203/PIG-4176-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-26 21:29:14.555,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Fri Sep 26 22:00:47 UTC 2014,,,,,,,0|i204hj:,9223372036854775807,,,,,,,,,,26/Sep/14 17:36;daijy;bump,26/Sep/14 21:29;rohini;+1,26/Sep/14 22:00;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
PIG CROSS operation follow by STORE produces non-deterministic results each run,PIG-4175,12741787,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,jimhuang,jimhuang,16/Sep/14 13:48,21/Nov/14 05:58,14/Mar/19 03:08,01/Oct/14 06:45,0.11,0.12.0,,,,0.14.0,,,,,0,,,,,,,"Three files will be attached to help visualize this issue.

1. mktestdata.py - to generate test data to feed the pig script
2. test_cross.pig - the PIG script using CROSS and STORE
3. test_cross.out - the PIG console output showing the input/output records delta

To reproduce this PIG CROSS operation problem, you need to use the supplied Python script,
mktestdata.py, to generate an input file that is at least 13,948,228,930 bytes (> 13GB).

The CROSS between raw_data (m records) and cross_count (1 record) should yield exactly (m records) as the output.  
The STORE results from the CROSS operations yielded about 1/3 of input record in raw_data as the output.  

If I joined the both of the CROSS operations together, the STORE results from the CROSS operations yielded about 2/3
of the input records in raw-data as the output.  
-- data = CROSS raw_data, field04s_count, subsection1_field04s_count, subsection2_field04s_count;


We have reproduced this using both Pig 0.11 (Hadoop 1.x) and Pig 0.12 (Hadoop 2.x) clusters.  
The default HDFS block size is 128MB.  


",RHEL 6/64-bit,,,,,,,,,,,,,,,,,,,21/Sep/14 19:13;daijy;PIG-4175-1.patch;https://issues.apache.org/jira/secure/attachment/12670310/PIG-4175-1.patch,01/Oct/14 19:29;rohini;PIG-4175-Debug.patch;https://issues.apache.org/jira/secure/attachment/12672384/PIG-4175-Debug.patch,01/Oct/14 22:05;rohini;PIG-4175-additional-1.patch;https://issues.apache.org/jira/secure/attachment/12672429/PIG-4175-additional-1.patch,16/Sep/14 13:56;jimhuang;mktestdata.py;https://issues.apache.org/jira/secure/attachment/12669063/mktestdata.py,16/Sep/14 13:58;jimhuang;pig_testcross_plan.png;https://issues.apache.org/jira/secure/attachment/12669066/pig_testcross_plan.png,16/Sep/14 13:58;jimhuang;test_cross.out;https://issues.apache.org/jira/secure/attachment/12669065/test_cross.out,16/Sep/14 13:58;jimhuang;test_cross.pig;https://issues.apache.org/jira/secure/attachment/12669064/test_cross.pig,,,,,7.0,,,,,,,,,,,,,,,,,,,2014-09-18 02:02:11.277,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Oct 01 23:07:12 UTC 2014,,,,,,,0|i203kn:,9223372036854775807,,,,,,,,,,18/Sep/14 02:02;daijy;I bet you've hit PIG-4057.,"18/Sep/14 15:18;jimhuang;Thanks Daniel.  I see PIG-4057 is targeted for PIG 0.14 and TEZ.  
I will ask Rohit to back port PIG-4057 to older versions of PIG (0.11/0.12) so we can confirm your patch can fix my issue. 
There are lots of PIG jobs running on a daily basis world wide, something as fundamental as a CROSS operation not being able to produce consistent results can skew a lot of results is huge.  ","21/Sep/14 19:13;daijy;Sure. In the mean time, I tried the script with Pig 0.14 and it produces right result. However, we can do better since cross is using only 1 reduce. I shall use Rohini's suggestion ""One way to fix this would be to always have GFCross UDF as part of map task of the actual cross job and never do it as part of previous job's map or reduce."". Attach patch.",01/Oct/14 03:34;rohini;+1,01/Oct/14 06:45;daijy;Patch committed to both trunk and 0.14 branch. Thanks Rohini for review!,"01/Oct/14 19:29;rohini;[~daijy],
   I was trying to enhance the test to compare actual results so that test is more foolproof. But found that the output of cross was all bytearray even though dump of the schema is as expected. 
C: {long}
D: {A::a0: int,A::a1: chararray,long}

Am I missing something? Attaching the debug patch.",01/Oct/14 19:53;rohini;Is it because GFCross does not define outputSchema ?,"01/Oct/14 21:20;daijy;pigServer.openIterator(""D"") gives the right datatype. Seems to be an issue in job.getResults().","01/Oct/14 21:32;daijy;What job.getResults does is load the stored data with the original loadFunc (PigStorage here), thus lose the type info. To get the original datatype, should use PigServer.openIterator instead of PigServer.store + job.getResults.",01/Oct/14 21:33;rohini;Thanks. I will just attach a additional patch to this jira to modify the testcase to exactly verify the results shortly. ,01/Oct/14 22:05;rohini;Enhanced the test with PIG-4175-additional-1.patch. Also moved the constant out of PigConfiguration and into PigImplConstants as it is not user configurable. ,01/Oct/14 22:11;daijy;+1,01/Oct/14 23:07;rohini;Committed PIG-4175-additional-1.patch to both branch 0.14 and trunk.,,,,,,,,,,,,,,,
Streaming UDF fails when direct fetch optimization is enabled,PIG-4171,12741637,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,cheolsoo,cheolsoo,cheolsoo,16/Sep/14 00:43,21/Nov/14 05:58,14/Mar/19 03:08,16/Sep/14 18:02,0.13.0,,,,,0.14.0,,,,,0,,,,,,,"To reproduce the error, register any udf as {{streaming_python}} and run it in direct fetch mode.

It fails with the following error in my environment-
{code}
    sys.argv[5], sys.argv[6], sys.argv[7], sys.argv[8])
  File ""/mnt/pig_tmp/prodpig/controller4894777320356829424.py"", line 77, in main
    self.output_stream = open(output_stream_path, 'a')
IOError: [Errno 13] Permission denied: '/mnt/var/lib/hadoop/tmp/udfOutput/sanitize.out'
{code}
The problem is that Streaming UDF tries to write out a log, but the user doesn't have write permission to the default location ({{hadoop.tmp.dir}}).

In fact, Streaming UDF handles local mode properly by using {{pig.udf.scripting.log.dir}} instead of {{hadoop.log.dir}} or {{hadoop.tmp.dir}}. We should do the same for direct fetch mode.




",,,,,,,,,,,,,,,,,,,,16/Sep/14 00:45;cheolsoo;PIG-4171-1.patch;https://issues.apache.org/jira/secure/attachment/12668923/PIG-4171-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-16 02:36:15.7,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 16 18:02:26 UTC 2014,,,,,,,0|i202mv:,9223372036854775807,,,,,,,,,,16/Sep/14 00:45;cheolsoo;Uploading a patch.,16/Sep/14 02:36;daijy;+1.,16/Sep/14 18:02;cheolsoo;Committed to trunk. Thank you Daniel for reviewing!,,,,,,,,,,,,,,,,,,,,,,,,,
Multiquery with different type of key gives wrong result,PIG-4170,12741400,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,15/Sep/14 05:51,21/Nov/14 05:58,14/Mar/19 03:08,16/Sep/14 05:43,0.13.0,,,,,0.14.0,,impl,,,0,,,,,,,"The following script produce wrong result:
{code}
A = load '1.txt' as (i:int, s:chararray);
B = group A by i;
C = group A by s;
store B into 'ooo1';
store C into 'ooo2';
{code}
1.txt:
{code}
1       h
1       a
{code}
Expected: 
{code}
ooo1:
1       {(1,a),(1,h)}
ooo2:
a       {(1,a)}
h       {(1,h)}
{code}
Actual:
{code}
ooo1:
1       {((1),a),((1),h)}
ooo2:
a       {(1,(a))}
h       {(1,(h))}
{code}
This happens after PIG-3591.",,,,,,,,,,,,,,,,,,,,15/Sep/14 06:21;daijy;PIG-4170-0.patch;https://issues.apache.org/jira/secure/attachment/12668716/PIG-4170-0.patch,16/Sep/14 04:22;daijy;PIG-4170-1.patch;https://issues.apache.org/jira/secure/attachment/12668970/PIG-4170-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-09-16 05:13:10.825,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Sep 16 05:43:05 UTC 2014,,,,,,,0|i2017r:,9223372036854775807,,,,,,,,,,"15/Sep/14 06:21;daijy;Attach draft patch, need to add test case.",16/Sep/14 04:22;daijy;Add test case. Patch is ready for review.,16/Sep/14 05:13;cheolsoo;+1,16/Sep/14 05:43;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,
NPE in ConstantCalculator,PIG-4169,12741099,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,12/Sep/14 16:52,21/Nov/14 05:58,14/Mar/19 03:08,16/Sep/14 02:37,,,,,,0.14.0,,impl,,,0,,,,,,,"To reproduce the issue, run the following query-
{code}
a = LOAD 'foo' AS (x:int);
b = FOREACH a GENERATE TOTUPLE((chararray)null);
DUMP b;
{code}
As can be seen, it is calling TOTUPLE with null. This causes a front-end exception with the following stack trace-
{code}
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 0: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [POUserFunc (Name: POUserFunc(org.apache.pig.builtin.TOTUPLE)[tuple] - scope-13 Operator Key: scope-13) children: null at []]: java.lang.NullPointerException
	at org.apache.pig.newplan.logical.rules.ConstantCalculator$ConstantCalculatorTransformer$ConstantCalculatorExpressionVisitor.execute(ConstantCalculator.java:154)
	at org.apache.pig.newplan.logical.expression.AllSameExpressionVisitor.visit(AllSameExpressionVisitor.java:143)
	at org.apache.pig.newplan.logical.expression.UserFuncExpression.accept(UserFuncExpression.java:112)
	at org.apache.pig.newplan.ReverseDependencyOrderWalkerWOSeenChk.walk(ReverseDependencyOrderWalkerWOSeenChk.java:69)
	at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
	at org.apache.pig.newplan.logical.optimizer.AllExpressionVisitor.visitAll(AllExpressionVisitor.java:72)
	at org.apache.pig.newplan.logical.optimizer.AllExpressionVisitor.visit(AllExpressionVisitor.java:131)
	at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:245)
	at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
	at org.apache.pig.newplan.logical.optimizer.AllExpressionVisitor.visit(AllExpressionVisitor.java:124)
	at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:87)
	at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
	at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:52)
	at org.apache.pig.newplan.logical.rules.ConstantCalculator$ConstantCalculatorTransformer.transform(ConstantCalculator.java:181)
	at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:110)
	... 16 more
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [POUserFunc (Name: POUserFunc(org.apache.pig.builtin.TOTUPLE)[tuple] - scope-13 Operator Key: scope-13) children: null at []]: java.lang.NullPointerException
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:360)
	at org.apache.pig.newplan.logical.rules.ConstantCalculator$ConstantCalculatorTransformer$ConstantCalculatorExpressionVisitor.execute(ConstantCalculator.java:151)
	... 30 more
Caused by: java.lang.NullPointerException
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:284)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNextTuple(POUserFunc.java:383)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:355)
	... 31 more
{code}",,,,,,,,,,,,,PIG-4128,,,,,,,12/Sep/14 16:54;cheolsoo;PIG-4169-1.patch;https://issues.apache.org/jira/secure/attachment/12668382/PIG-4169-1.patch,15/Sep/14 18:36;daijy;PIG-4169-2.patch;https://issues.apache.org/jira/secure/attachment/12668817/PIG-4169-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-09-12 18:03:55.987,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Sep 16 02:37:54 UTC 2014,,,,,,,0|i1zzcn:,9223372036854775807,,,,,,,,,,12/Sep/14 16:54;cheolsoo;Uploading a patch that fixes the NPE.,12/Sep/14 18:03;daijy;+1,12/Sep/14 18:55;cheolsoo;Committed to trunk. Thanks Daniel for reviewing!,"15/Sep/14 18:36;daijy;The previous fix breaks a few unit tests, eg: TestQueryParser.testNullInBinCondNoSpace. The real cause of the NPE is actually pigLogger is null. Need to rollback PIG-4169-1.patch and attach PIG-4169-2.patch as a new fix.","15/Sep/14 18:52;cheolsoo;[~daijy], thank you for catching it! Let's revert {{PIG-4169-1.patch}} and commit your patch. +1",16/Sep/14 02:37;daijy;Patch committed. Thanks Cheolsoo!,,,,,,,,,,,,,,,,,,,,,,
Collected group drops last record when combined with merge join,PIG-4166,12740863,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,bridiver,bridiver,bridiver,11/Sep/14 20:25,21/Nov/14 05:58,14/Mar/19 03:08,30/Oct/14 18:08,0.12.0,,,,,0.14.0,,,,,0,,,,,,,"If the final two keys in each relation join, they will never make it to the final output. The reason is that POMergeJoin does a read-ahead and POCollectedGroup doesn't call processInput when this.parentPlan.endOfAllInput == true. This prevents the final join from being output because POMergeJoin never sees endOfAllInput == true.
{code}
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java
index c355d1d..8fd44fa 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java
@@ -127,28 +127,30 @@ public class POCollectedGroup extends PhysicalOperator {
     @Override
     public Result getNextTuple() throws ExecException {
 
-        // Since the output is buffered, we need to flush the last
-        // set of records when the close method is called by mapper.
-        if (this.parentPlan.endOfAllInput) {
-            if (outputBag != null) {
-                Tuple tup = mTupleFactory.newTuple(2);
-                tup.set(0, prevKey);
-                tup.set(1, outputBag);
-                outputBag = null;
-                return new Result(POStatus.STATUS_OK, tup);
-            }
-
-            return new Result(POStatus.STATUS_EOP, null);
-        }
+        
 
         Result inp = null;
         Result res = null;
 
         while (true) {
             inp = processInput();
+
             if (inp.returnStatus == POStatus.STATUS_EOP ||
                     inp.returnStatus == POStatus.STATUS_ERR) {
-                break;
+               // Since the output is buffered, we need to flush the last
+                // set of records when the close method is called by mapper.
+                if (this.parentPlan.endOfAllInput) {
+                    if (outputBag != null) {
+                        Tuple tup = mTupleFactory.newTuple(2);
+                        tup.set(0, prevKey);
+                        tup.set(1, outputBag);
+                        outputBag = null;
+                        return new Result(POStatus.STATUS_OK, tup);
+                    }
+
+                    return new Result(POStatus.STATUS_EOP, null);
+                } else
+                       break;
             }
 
             if (inp.returnStatus == POStatus.STATUS_NULL) {
{code}",,,,,,,,,,,,,,,,,,,,30/Oct/14 18:06;daijy;PIG-4166-2.patch;https://issues.apache.org/jira/secure/attachment/12678255/PIG-4166-2.patch,29/Oct/14 20:30;bridiver;patch;https://issues.apache.org/jira/secure/attachment/12677989/patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-10-16 21:46:38.299,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Oct 30 18:08:03 UTC 2014,,,Patch Available,,,,0|i1zxwn:,9223372036854775807,,,,,,,,,,"11/Sep/14 20:29;bridiver;This probably affects the final record no matter what, but I haven't tested that yet.",16/Oct/14 19:28;bridiver;Why does this keep getting pushed back? It isn't handling the final status correctly and will drop the last record in many instances,"16/Oct/14 21:46;daijy;I unlink it from 0.14 since this sounds like a corner use case and no one seems actively working on it. Are you working on it, or do you have a test case, so I can take a look.","16/Oct/14 21:52;bridiver;I included the patch above. I didn't create a specific test case for it, but all tests still pass.","16/Oct/14 22:00;daijy;bq. This probably affects the final record no matter what, but I haven't tested that yet.
What did you test and what's not? And we do need a test case here to make sure future change will not break your script again. ",16/Oct/14 22:03;bridiver;I can put together a test case for it. It affects the last record when it's combined with any other operator that caches records (like merge join) because the previous operator never gets a chance to flush it's final output.,16/Oct/14 22:25;daijy;Sounds good.,29/Oct/14 17:03;bridiver;I'm getting failures in TestCollectedGroup in branch 0.14 before any changes,"29/Oct/14 17:10;bridiver;testMapsideGroupByOneColumn

```
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias C
  at org.apache.pig.PigServer.openIterator(PigServer.java:923)
  at org.apache.pig.test.TestCollectedGroup.testMapsideGroupByOneColumn(TestCollectedGroup.java:199)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
  at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
  at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
  at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
  at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
  at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
  at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
  at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias C
  at org.apache.pig.PigServer.storeEx(PigServer.java:1026)
  at org.apache.pig.PigServer.store(PigServer.java:985)
  at org.apache.pig.PigServer.openIterator(PigServer.java:898)
  ... 27 more
Caused by: org.apache.pig.backend.hadoop.executionengine.JobCreationException: ERROR 2017: Internal error creating job configuration.
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:998)
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:323)
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:196)
  at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:280)
  at org.apache.pig.PigServer.launchPlan(PigServer.java:1378)
  at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1363)
  at org.apache.pig.PigServer.storeEx(PigServer.java:1022)
  ... 29 more
Caused by: java.lang.NullPointerException
  at java.io.File.<init>(File.java:277)
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:628)
  ... 35 more
```",29/Oct/14 17:34;daijy;It runs fine for me. Are you running it inside eclipse?,"29/Oct/14 17:36;bridiver;Yes, is that a know issue?","29/Oct/14 17:39;bridiver;They all pass in branch-0.13, although they're painfully slow to run","29/Oct/14 17:39;daijy;In eclipse, you will need to add pig-0.14.0-SNAPSHOT-core-h2.jar and folder build/classes into classpath of the test.","29/Oct/14 17:47;brian@brianjohnson.cc;Yes, is that an known issue?



","29/Oct/14 17:54;daijy;Yes, this is a known prerequisite to run unit tests in eclipse in 0.14.",29/Oct/14 20:18;bridiver;Are you sure you want a test for this change? The cost of the test is very high against the small chance that someone would return this code to it's broken state. These tests would benefit from a single startup/shutdown for the pig server. It's not really practical to have so many unit tests that take several minutes to run.,29/Oct/14 20:30;bridiver;added test,29/Oct/14 20:31;bridiver;patch against branch-0.13,"29/Oct/14 20:31;bridiver;in any case, here it is with the passing test. It would be great if you could move the fix back to 0.14",30/Oct/14 18:06;daijy;Attach an equivalent for trunk and 0.14 branch.,"30/Oct/14 18:08;daijy;+1.

Patch committed to both trunk and 0.14 branch. 

Thanks Brian for the fix and testcase.",,,,,,,
"After Pig job finish, Pig client spend too much time retry to connect to AM",PIG-4164,12740627,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,10/Sep/14 22:17,21/Nov/14 05:59,14/Mar/19 03:08,01/Oct/14 22:09,,,,,,0.14.0,,impl,,,0,,,,,,,"For some script, after job finish, Pig spend a lot time try to connect AM before get redirect to JobHistoryServer. Here is the message we saw:

{code}
2014-09-10 15:13:55,370 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: daijymacpro-2.local/10.11.2.30:55223. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
2014-09-10 15:13:56,371 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: daijymacpro-2.local/10.11.2.30:55223. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
2014-09-10 15:13:57,372 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: daijymacpro-2.local/10.11.2.30:55223. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
2014-09-10 15:13:57,476 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server
{code}",,,,,,,,,,,,,,,,,,,,10/Sep/14 22:23;daijy;PIG-4164-0.patch;https://issues.apache.org/jira/secure/attachment/12667874/PIG-4164-0.patch,01/Oct/14 22:06;daijy;PIG-4164-1.patch;https://issues.apache.org/jira/secure/attachment/12672430/PIG-4164-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-09-17 04:16:18.051,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Oct 01 22:09:13 UTC 2014,,,,,,,0|i1zwhr:,9223372036854775807,,,,,,,,,,"10/Sep/14 22:23;daijy;According to [~vinodkv], Pig shall not reuse the Job object to pull the taskReport/counters since it will keep trying AM. We shall construct a new Job object with designated Job ID to do that.

Note this issue only happen in Hadoop 2 since Hadoop 1 does not have AM/JobHistoryServer (will always query JobTracker).

Attach initial patch. In patch, I created a Cluster object every time which can be further optimized.","17/Sep/14 04:16;rohini;+1. 

bq. I created a Cluster object every time which can be further optimized.
  How do you intend to cache and reuse the Cluster object?

Listing places called.
   getTaskReports - Called once for every job at the end of the job. 
  getCounters - Called once for every job and also called once for computingWarningAggregate at the end of the job.  We can further optimize by clubbing computingWarningAggregate in MRJobStats when collecting stats. Also used in RANK jobs, but that is only for that one job.
  ","26/Sep/14 18:12;daijy;I check the code again and I cannot find a good answer. Ideally we can create a Cluster object outside the loop, however, Cluster is hadoop 2 only and we can only put in shims. That will inevitably make code much complex. Consider number of cluster object created is linear to the number of jobs, it does not seems too bad. I would like to check in the code as is, sounds ok?",26/Sep/14 19:37;rohini;That's fine. +1.,26/Sep/14 20:46;daijy;Patch committed to trunk. Thanks Rohini for review!,26/Sep/14 23:23;daijy;Rollback the patch temporarily since it break local mode.,01/Oct/14 22:06;daijy;Add a null check in case of local mode.,01/Oct/14 22:08;rohini;+1,01/Oct/14 22:09;daijy;Patch committed to trunk and 0.14 branch.,,,,,,,,,,,,,,,,,,,
Always check for latest Hive snapshot dependencies,PIG-4161,12740038,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,09/Sep/14 01:51,21/Nov/14 05:59,14/Mar/19 03:08,11/Sep/14 20:01,,,,,,0.14.0,,build,,,0,,,,,,,"Similar to PIG-4027, we need to check modified Hive snapshot (until Hive 0.14.0 released).",,,,,,,,,,,,,,,,,,,,09/Sep/14 01:53;daijy;PIG-4161-1.patch;https://issues.apache.org/jira/secure/attachment/12667320/PIG-4161-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-11 19:30:37.835,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Sep 11 20:01:56 UTC 2014,,,,,,,0|i1ztgv:,9223372036854775807,,,,,,,,,,11/Sep/14 19:30;rohini;+1,11/Sep/14 20:01;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
TestGroupConstParallelTez and TestJobSubmissionTez should be excluded in Hadoop 20 unit tests,PIG-4159,12739736,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,06/Sep/14 23:38,21/Nov/14 05:58,14/Mar/19 03:08,08/Sep/14 18:10,,,,,,0.14.0,,,,,0,,,,,,,"These tests are Tez-specific, so should not run in Hadoop 20.",,,,,,,,,,,,,,,,,,,,06/Sep/14 23:39;cheolsoo;PIG-4159-1.patch;https://issues.apache.org/jira/secure/attachment/12667054/PIG-4159-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-08 04:44:29.493,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 08 18:10:18 UTC 2014,,,,,,,0|i1zrnz:,9223372036854775807,,,,,,,,,,08/Sep/14 04:44;daijy;+1,08/Sep/14 18:10;cheolsoo;Committed to trunk. Thank you Daniel for reviewing.,,,,,,,,,,,,,,,,,,,,,,,,,,
TestAssert is broken in trunk,PIG-4158,12739735,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,06/Sep/14 23:33,21/Nov/14 05:59,14/Mar/19 03:08,06/Sep/14 23:41,,,,,,0.14.0,,,,,0,,,,,,,"This is a regression of PIG-4135. Since the test query no longer runs with fetch optimization, the error message is different now.",,,,,,,,,,,,,,,,,,,,06/Sep/14 23:35;cheolsoo;PIG-4518-1.patch;https://issues.apache.org/jira/secure/attachment/12667053/PIG-4518-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-06 23:38:32.598,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Sat Sep 06 23:41:24 UTC 2014,,,,,,,0|i1zrnr:,9223372036854775807,,,,,,,,,,06/Sep/14 23:38;lbendig;+1 Thanks for fixing it!,06/Sep/14 23:41;cheolsoo;Committed to trunk. Thank you Lorand for reviewing!,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig compilation failure due to HIVE-7208,PIG-4157,12739642,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,06/Sep/14 00:16,21/Nov/14 05:59,14/Mar/19 03:08,08/Sep/14 02:38,,,,,,0.14.0,,impl,,,0,,,,,,,"Orc API change.

Compile fail with:
javac /Users/daijy/pig3/src/org/apache/pig/builtin/OrcStorage.java:558: error: cannot find symbol
javac Builder builder = SearchArgument.FACTORY.newBuilder();
javac ^
javac symbol: variable FACTORY
javac location: interface SearchArgument",,,,,,,,,,,,,,,,,,,,06/Sep/14 00:16;daijy;PIG-4157-1.patch;https://issues.apache.org/jira/secure/attachment/12666949/PIG-4157-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-07 01:13:28.742,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Sep 25 20:20:02 UTC 2014,,,,,,,0|i1zr3b:,9223372036854775807,,,,,,,,,,07/Sep/14 01:13;cheolsoo;+1.,08/Sep/14 02:38;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,25/Sep/14 20:15;rohini;Hitting the problem again even with this patch and is due to ivy jars getting corrupted. ivy is not that good with snapshot jars. Only way to get around is to delete ~/.ivy2/cache/org.apache.hive and ~/.ivy2/local/org.apache.hive,25/Sep/14 20:20;rohini;Wrong comment above. Ignore. Mistook this jira for PIG-4161.,,,,,,,,,,,,,,,,,,,,,,,,
[PATCH] fix NPE when running scripts stored on hdfs://,PIG-4156,12739608,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,acoliver,acoliver,acoliver,05/Sep/14 22:13,21/Nov/14 05:58,14/Mar/19 03:08,09/Sep/14 01:38,0.12.0,0.12.1,0.13.0,0.13.1,,0.14.0,,impl,,,0,,,,,,,"pig -useHCatalog hdfs://myserver:8020/load/scripts/mydir/myscript.pig

throws a NPE due to a bogus if statement (patch included)

Error before Pig is launched
----------------------------
ERROR 2999: Unexpected internal error. null

java.lang.NullPointerException
        at org.apache.pig.impl.io.FileLocalizer.fetchFilesInternal(FileLocalizer.java:799)
        at org.apache.pig.impl.io.FileLocalizer.fetchFiles(FileLocalizer.java:767)
        at org.apache.pig.PigServer.registerJar(PigServer.java:546)
        at org.apache.pig.PigServer.addJarsFromProperties(PigServer.java:253)
        at org.apache.pig.PigServer.<init>(PigServer.java:231)
        at org.apache.pig.PigServer.<init>(PigServer.java:214)
        at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:46)
        at org.apache.pig.Main.run(Main.java:603)
        at org.apache.pig.Main.main(Main.java:164)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
================================================================================",Linux / CentOS 6,172800,172800,,0%,172800,172800,,,,,,,,,,,,,05/Sep/14 22:25;acoliver;fixnpe-trunk.patch;https://issues.apache.org/jira/secure/attachment/12666916/fixnpe-trunk.patch,05/Sep/14 22:15;acoliver;fixnpe.patch;https://issues.apache.org/jira/secure/attachment/12666913/fixnpe.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-09-08 04:59:00.144,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Tue Sep 09 01:38:04 UTC 2014,,,Patch Available,,,,0|i1zqvz:,9223372036854775807,I will also post one for trunk,,,,,,,,,"05/Sep/14 22:14;acoliver;Index: src/org/apache/pig/impl/io/FileLocalizer.java
===================================================================
--- src/org/apache/pig/impl/io/FileLocalizer.java	(revision 1622795)
+++ src/org/apache/pig/impl/io/FileLocalizer.java	(working copy)
@@ -734,7 +734,8 @@
         FileSystem srcFs;
         if ( (!""true"".equals(properties.getProperty(""pig.jars.relative.to.dfs""))
                 && uri.getScheme() == null )||
-                uri.getScheme().equals(""local"") ) {
+                (uri.getScheme() != null && uri.getScheme().equals(""local"")) 
+           ) {
             srcFs = localFs;
         } else {
             srcFs = path.getFileSystem(conf);
",05/Sep/14 22:15;acoliver;this version is for 0.12.x as exists in svn today,05/Sep/14 22:25;acoliver;this patch fixes trunk,"08/Sep/14 04:59;daijy;Does this alone fix your issue? When the script is on hdfs, Pig assumes all register jar without full scheme is also on hdfs, including the hive/hcat jars brought by -useHCatalog. So once we fix NPE, does Pig end up with file not found exception?","08/Sep/14 05:50;acoliver;This fixes AN issue, but not all of my issues. If I put my hive/etc jars on HDFS then without this patch I get an NPE. (And that code is obviously bogus in that it doesn't check for null in the second part of an OR condition that it does in the first clause). 

I wanted to at least fix the NPE so things work. I was hoping to get some level of ""yes that's a good idea"" before proposing a new flag to force jars to go local so I opened a thread for that. If there is some level of consensus I'll provide a different patch for the ""I don't want all my jars remote just cause my script is"" problem.

Does that make sense?",09/Sep/14 01:19;acoliver;See PIG-4160 for the new forcelocal feature I proposed adding.,09/Sep/14 01:38;daijy;Fair enough. I committed the patch to trunk. Thanks Andrew!,,,,,,,,,,,,,,,,,,,,,
Quitting grunt shell using CTRL-D character throws exception,PIG-4155,12739451,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,abhishek.agarwal,abhishek.agarwal,abhishek.agarwal,05/Sep/14 10:32,21/Nov/14 05:59,14/Mar/19 03:08,08/Sep/14 05:04,0.14.0,,,,,0.14.0,,grunt,,,0,,,,,,,"jline returns null when CTRL-D is given as input. This results in NPE in grunt because StringReader constructor throws NPE.

{noformat}
String line = super.readLine();
String paramSubLine = pc.doParamSubstitution(new BufferedReader(new StringReader(line)));
return paramSubLine;
{noformat}

{noformat}
java.lang.NullPointerException
        at java.io.StringReader.<init>(StringReader.java:33)
        at org.apache.pig.Main$ConsoleReaderWithParamSub.readLine(Main.java:1057)
        at jline.ConsoleReaderInputStream$ConsoleLineInputStream.read(ConsoleReaderInputStream.java:92)
        at java.io.InputStream.read(InputStream.java:154)
        at java.io.SequenceInputStream.read(SequenceInputStream.java:191)
        at java.io.SequenceInputStream.read(SequenceInputStream.java:194)
        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:264)
{noformat}","Hadoop 2.4.0.2.1.4.0-632
java version ""1.7.0_55""
pig - trunk",,,,,,,,,,,,,,,,,,,05/Sep/14 10:34;abhishek.agarwal;PIG-4155.patch;https://issues.apache.org/jira/secure/attachment/12666772/PIG-4155.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-08 05:04:57.442,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Sep 08 05:04:57 UTC 2014,,,,,,,0|i1zpxr:,9223372036854775807,,,,,,,,,,"08/Sep/14 05:04;daijy;+1.

Patch committed to trunk. Thanks Abhishek!",,,,,,,,,,,,,,,,,,,,,,,,,,,
ScriptState#setScript(File) does not close resources,PIG-4154,12739435,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,lars_francke,lars_francke,lars_francke,05/Sep/14 09:09,21/Nov/14 05:58,14/Mar/19 03:08,08/Sep/14 05:24,,,,,,0.14.0,,,,,0,,,,,,,When calling {{setScript(File)}} it opens two readers and passes them on. Unfortunately it doesn't close them. That means the underlying files are locked until the next run of the Garbage Collector.,,,,,,,,,,,,,,,,,,,,05/Sep/14 09:19;lars_francke;PIG-4154.1.patch;https://issues.apache.org/jira/secure/attachment/12666764/PIG-4154.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-08 05:24:20.977,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Sep 08 05:24:20 UTC 2014,,,,,,,0|i1zpu7:,9223372036854775807,,,,,,,,,,05/Sep/14 09:19;lars_francke;I've attached my stab at this.,"08/Sep/14 05:24;daijy;Nice catch, thanks Lars!

Patch committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Cannot Write Empty Maps to HBase,PIG-4151,12738923,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,03/Sep/14 21:17,21/Nov/14 05:59,14/Mar/19 03:08,15/Oct/14 04:51,,,,,,0.14.0,,internal-udfs,,,0,,,,,,,"Pig is unable to write empty maps to HBase. Instruction for reproduce:

input file pig_data_bad.txt:
{code}
row1;Homer;Morrison;[1#Silvia,2#Stacy]
row2;Sheila;Fletcher;[1#Becky,2#Salvador,3#Lois]
row4;Andre;Morton;[1#Nancy]
row3;Sonja;Webb;[]
{code}

Create table in hbase:
create 'test', 'info', 'friends'

Pig script:
{code}
source = LOAD '/pig_data_bad.txt' USING PigStorage(';') AS (row:chararray, first_name:chararray, last_name:chararray, friends:map[]);

STORE source INTO 'hbase://test' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('info:fname info:lname friends:*');
{code}

Stack:
java.lang.NullPointerException
at org.apache.pig.backend.hadoop.hbase.HBaseStorage.putNext(HBaseStorage.java:880)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)
at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:635)
at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:284)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)",,,,,,,,,,,,,,,,,,,,03/Sep/14 22:00;daijy;PIG-4151-1.patch;https://issues.apache.org/jira/secure/attachment/12666335/PIG-4151-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-15 03:01:18.909,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Oct 15 04:52:07 UTC 2014,,,,,,,0|i1zndr:,9223372036854775807,,,,,,,,,,15/Oct/14 03:01;thejas;+1,15/Oct/14 04:51;daijy;Patch committed to both trunk and 0.14 branch.,15/Oct/14 04:52;daijy;Thanks Thejas for review!,,,,,,,,,,,,,,,,,,,,,,,,,
Rounding issue in FindQuantiles,PIG-4149,12738350,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,02/Sep/14 06:19,21/Nov/14 05:58,14/Mar/19 03:08,04/Sep/14 00:00,,,,,,0.14.0,,impl,,,0,,,,,,,"In FindQuantiles, Pig calculates an integer toSkip inside sample, and skip ""toSkip"" sample records to find the next boundary. However, toSkip should not be an integer, this will cause rounding issue and all the remainder will goes to the last partition.",,,,,,,,,,,,,,,,,,,,02/Sep/14 06:22;daijy;PIG-4149-0.patch;https://issues.apache.org/jira/secure/attachment/12665854/PIG-4149-0.patch,02/Sep/14 22:51;daijy;PIG-4149-1.patch;https://issues.apache.org/jira/secure/attachment/12666064/PIG-4149-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-09-03 23:15:41.763,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Sep 04 00:00:12 UTC 2014,,,,,,,0|i1zkdz:,9223372036854775807,,,,,,,,,,02/Sep/14 06:22;daijy;Attach a draft patch to illustrate the idea. Need to add new test and run through existing tests.,03/Sep/14 23:15;cheolsoo;+1,04/Sep/14 00:00;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,
VertexManagerEvent.getUserPayload returns ReadOnlyBuffer after TEZ-1449,PIG-4140,12736299,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,24/Aug/14 02:44,21/Nov/14 05:58,14/Mar/19 03:08,24/Aug/14 03:19,,,,,,0.14.0,,tez,,,0,,,,,,,"Order by script throws java.nio.ReadOnlyBufferException:
        at java.nio.ByteBuffer.array(ByteBuffer.java:942)
        at org.apache.pig.backend.hadoop.executionengine.tez.PartitionerDefinedVertexManager.onVertexManagerEventReceived(PartitionerDefinedVertexManager.java:81)
        at org.apache.tez.dag.app.dag.impl.VertexManager.onVertexManagerEventReceived(VertexManager.java:255)
        at org.apache.tez.dag.app.dag.impl.VertexImpl$RouteEventTransition.transition(VertexImpl.java:3395)
        at org.apache.tez.dag.app.dag.impl.VertexImpl$RouteEventTransition.transition(VertexImpl.java:3296)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:1354)
        at org.apache.tez.dag.app.dag.impl.VertexImpl.handle(VertexImpl.java:168)
        at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1638)
        at org.apache.tez.dag.app.DAGAppMaster$VertexEventDispatcher.handle(DAGAppMaster.java:1624)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:81)
        at java.lang.Thread.run(Thread.java:662)",,,,,,,,,,,,,,,,,,,,24/Aug/14 02:46;daijy;PIG-4140-1.patch;https://issues.apache.org/jira/secure/attachment/12663893/PIG-4140-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-24 03:15:45.485,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Sun Aug 24 03:19:45 UTC 2014,,,,,,,0|i1z9qv:,9223372036854775807,,,,,,,,,,24/Aug/14 03:15;cheolsoo;+1,24/Aug/14 03:19;daijy;Patch committed to trunk. Thanks Cheolsoo for super quick review!,,,,,,,,,,,,,,,,,,,,,,,,,,
pig query throws error java.lang.NoSuchFieldException: jobsInProgress on MRv1,PIG-4139,12735886,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,suhassatish,ssatish,ssatish,21/Aug/14 20:30,21/Nov/14 05:58,14/Mar/19 03:08,24/Aug/14 04:46,0.11.1,0.12.0,,,,0.13.1,0.14.0,,,,0,,,,,,,"The following exception is caught and printed on stdout with logging level WARN  under the environment mentioned above. Its a harmless msg and the pig job falls back to using HadoopShoms 0.20 API and completes successfully. But the exception can be confusing to the user. Increasing logging level to debug in the patch. 

-----
2014-08-20 11:12:00,879 [main] WARN 
org.apache.pig.backend.hadoop23.PigJobControl - falling back to default
JobControl (not using hadoop 0.23 ?)
java.lang.NoSuchFieldException: jobsInProgress
    at java.lang.Class.getDeclaredField(Class.java:1899)
    at
org.apache.pig.backend.hadoop23.PigJobControl.<clinit>(PigJobControl.java:58)
    at
org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims.newJobControl(HadoopShims.java:104)
    at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:287)
    at
org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:192)
    at org.apache.pig.PigServer.launchPlan(PigServer.java:1322)
    at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1307)
    at org.apache.pig.PigServer.execute(PigServer.java:1297)
    at org.apache.pig.PigServer.executeBatch(PigServer.java:375)
    at org.apache.pig.PigServer.executeBatch(PigServer.java:353)
    at
org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:140)
    at
org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:202)
    at
org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:173)
    at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:84)
    at org.apache.pig.Main.run(Main.java:607)
    at org.apache.pig.Main.main(Main.java:156)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:212)

Also reported at  - 
http://mail-archives.apache.org/mod_mbox/pig-user/201402.mbox/%3C2014022712405096933026@gmail.com%3E

and 

http://stackoverflow.com/questions/17533154/pig-java-lang-nosuchfieldexception-jobsinprogress-exception",Pig jar running on yarn configured to run MRv1,,,,,,,,,,,,,,,,,,,21/Aug/14 20:32;ssatish;PIG-4139.patch;https://issues.apache.org/jira/secure/attachment/12663490/PIG-4139.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-24 03:58:50.172,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 24 05:17:11 UTC 2014,,,Patch Available,,,,0|i1z78n:,9223372036854775807,,,,,,,,,,24/Aug/14 03:58;cheolsoo;+1,24/Aug/14 04:46;cheolsoo;Committed to trunk and branch 0.13.,24/Aug/14 05:17;suhassatish;thanks [~cheolsoo],,,,,,,,,,,,,,,,,,,,,,,,,
Fix hadoopversion 23 compilation due to TEZ-1469,PIG-4137,12735661,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,21/Aug/14 06:45,21/Nov/14 05:58,14/Mar/19 03:08,21/Aug/14 22:45,,,,,,0.14.0,,tez,,,0,,,,,,,One more API change.,,,,,,,,,,,,,,,,,,,,21/Aug/14 06:52;daijy;PIG-4137-1.patch;https://issues.apache.org/jira/secure/attachment/12663353/PIG-4137-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-21 16:31:20.29,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Thu Aug 21 22:45:37 UTC 2014,,,,,,,0|i1z69r:,9223372036854775807,,,,,,,,,,21/Aug/14 16:31;cheolsoo;+1,21/Aug/14 22:45;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetch optimization should be disabled if plan contains no limit,PIG-4135,12735593,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,20/Aug/14 23:27,21/Nov/14 05:59,14/Mar/19 03:08,21/Aug/14 21:24,,,,,,0.14.0,,,,,0,,,,,,,"After deploying fetch optimization in production, a couple of users ran into this situation. They had fairly large input data, but after filtering it by a regular expression, it becomes small. So they didn't add limit to the query. 

The problem is that even though the output is small, processing the input must be done in the cluster not in the client. However, fetch optimization blindly fetches the entire input into the client since the plan is map-only job and finishes with dump.",,,,,,,,,,,,,,,,,,,,20/Aug/14 23:33;cheolsoo;PIG-4135-1.patch;https://issues.apache.org/jira/secure/attachment/12663269/PIG-4135-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-21 05:31:13.136,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 21 21:24:49 UTC 2014,,,,,,,0|i1z5un:,9223372036854775807,,,,,,,,,,"20/Aug/14 23:33;cheolsoo;Uploading a patch that marks the plan as fetchable only if it's map-only job, finishes with dump, and includes limit. I also updated the Pig docs about this condition.
",21/Aug/14 05:31;daijy;+1,"21/Aug/14 08:28;lbendig;[~cheolsoo], thanks for pointing out this issue. Filter is a good safeguard, but it also reduces the use cases of fetch.
I'm wondering, whether we can have an input size estimation instead, like pig.auto.local.input.maxbytes ?","21/Aug/14 16:25;cheolsoo;Won't the input size reduce the use cases more than limit? For example, if fetch optimizer is disabled when input size > 100MB, the example that I described in this jira won't use fetch optimization even with limit. In fact, it will be disabled in most cases. On the other hand, limit is effective since it is pushed down into the load, no more records than the limit will be loaded.

I think the main use case of fetch optimization is to peek a few sample records from input files quickly. Requiring a limit doesn't reduce the use cases, does it?","21/Aug/14 20:15;lbendig;Yes, that's the intended use case. I suddenly was thinking of some other examples but you are correct, using filter should be the cleanest and safest way to pick those map-only jobs that can be quickly fetched from grunt. So +1, thank you for the fix!
","21/Aug/14 21:24;cheolsoo;Committed to trunk.

Thank you Daniel and Lorand for the review!",,,,,,,,,,,,,,,,,,,,,,
TEZ-1449 broke the build,PIG-4134,12735480,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,knoguchi,knoguchi,knoguchi,20/Aug/14 18:19,21/Nov/14 05:58,14/Mar/19 03:08,20/Aug/14 19:16,,,,,,0.14.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Aug/14 18:24;knoguchi;pig-4134-v01.patch;https://issues.apache.org/jira/secure/attachment/12663177/pig-4134-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-20 19:07:27.337,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Mon Aug 25 14:45:07 UTC 2014,,,,,,,0|i1z55j:,9223372036854775807,,,,,,,,,,20/Aug/14 19:07;cheolsoo;+1,20/Aug/14 19:16;knoguchi;Thanks Cheolsoo for the review.  Patch committed to trunk.,"20/Aug/14 20:28;knoguchi;After my patch, trunk successfully gets compiled but I'm seeing about 60 more e2e-tez failures.  Not sure if this is due to my patch or something else that changed on the tez side.
(Also, my test environment isn't stable...)
","25/Aug/14 14:45;knoguchi;bq. After my patch, trunk successfully gets compiled but I'm seeing about 60 more e2e-tez failures.  Not sure if this is due to my patch or something else that changed on the tez side.

This was due to change on Tez side.  [~daijy] fixed it in PIG-4140.  Thanks Daniel!
About 60 e2e tests fixed at once :)
",,,,,,,,,,,,,,,,,,,,,,,,
Need to update the default $HCAT_HOME dir in the PIG script.,PIG-4133,12735206,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,mnarayan,mnarayan,mnarayan,19/Aug/14 22:37,21/Nov/14 05:59,14/Mar/19 03:08,21/Aug/14 00:16,0.12.0,,,,,0.13.1,0.14.0,grunt,,,0,,,,,,,"Looks like we need to update the default HCAT_HOME in /bin/Pig to /usr/lib/hive-hcatalog.

This is what it has currently.

<snip>
if [ ""$HCAT_HOME"" == """" ]; then
  if [ -d ""/usr/lib/hcatalog"" ]; then
    HCAT_HOME=/usr/lib/hcatalog
  else
    echo ""Please initialize HCAT_HOME""
    exit -1
  fi
  fi
</snip>",,,,,,,,,,,,,,,,,,,,20/Aug/14 21:49;mnarayan;PIG-4133.patch;https://issues.apache.org/jira/secure/attachment/12663233/PIG-4133.patch,19/Aug/14 22:45;mnarayan;PIG-4133.patch;https://issues.apache.org/jira/secure/attachment/12662885/PIG-4133.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-08-20 17:38:57.879,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 21 00:16:50 UTC 2014,,,,,,,0|i1z2pb:,9223372036854775807,,,,,,,,,,"20/Aug/14 17:38;mgrover;Thanks, Mani. Should {{if [ -d ""/usr/lib/hive-hcatalog"" ]; then}} be an else if instead?","20/Aug/14 20:36;mnarayan;Thanks for taking a look at this Mark.

if [ -d ""/usr/lib/hive-hcatalog"" ]; looks right to me. We only want to check for the existence of this directory if HCAT_HOME is not set. Right?","20/Aug/14 21:07;mgrover;But, if {{/usr/lib/hcatalog}} already exists, should we still go through this check? What should happen in the corner case where both {{/usr/lib/hcatalog}} and {{/usr/lib/hive-hcatalog}} exist (say because 2 versions of HCatalog are installed, one from pre-hive times and one from after the project was merged into Hive?",20/Aug/14 21:54;mgrover;+1 (non-committer),"20/Aug/14 21:57;mnarayan;The first patch replaced checking for /usr/lib/hcatalog with /usr/lib/hive-hcatalog so, if we had both versions of HCatalog side by side I think it would have still worked. But, I agree that if the user only has the previous version installed, this check would have not set the HCAT_HOME and exited incorrectly.

Thanks! Updated patch. ",20/Aug/14 23:48;cheolsoo;+1. Will commit it shortly.,21/Aug/14 00:16;cheolsoo;Committed to trunk and 0.13 branch. Thank you guys!,,,,,,,,,,,,,,,,,,,,,
TEZ-1246 and TEZ-1390 broke a build,PIG-4132,12735181,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,knoguchi,knoguchi,knoguchi,19/Aug/14 21:29,21/Nov/14 05:58,14/Mar/19 03:08,20/Aug/14 02:17,,,,,,0.14.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Aug/14 21:33;knoguchi;pig-4132-v01.txt;https://issues.apache.org/jira/secure/attachment/12662855/pig-4132-v01.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-19 22:49:59.302,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,,Wed Aug 20 02:17:18 UTC 2014,,,,,,,0|i1z2i7:,9223372036854775807,,,,,,,,,,"19/Aug/14 21:33;knoguchi;Cannot wait for Tez API to stabilize...

For TEZ-1246, replaced all ""new"" calls to ""Classname.create"".
For TEZ-1390, replaced one line from "".toByteArray()"" to "".toByteString().asReadOnlyByteBuffer())"".

Again, not understanding Tez at all.  Just uploading changes so that compilation passes.",19/Aug/14 22:49;daijy;+1,20/Aug/14 02:17;knoguchi;Thanks Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,,,,
Pig -Dhadoopversion=23 compile fail after TEZ-1426,PIG-4129,12734682,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/Aug/14 04:51,21/Nov/14 05:58,14/Mar/19 03:08,18/Aug/14 17:40,,,,,,0.14.0,,impl,,,0,,,,,,,Another Tez API change.,,,,,,,,,,,,,,,,,,,,18/Aug/14 04:51;daijy;PIG-4129-1.patch;https://issues.apache.org/jira/secure/attachment/12662421/PIG-4129-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-18 12:14:44.967,,,no_permission,,,,,,,,,,,,,412622,Reviewed,,,,Mon Aug 18 17:40:17 UTC 2014,,,,,,,0|i1yzen:,412608,,,,,,,,,,18/Aug/14 12:14;cheolsoo;+1,18/Aug/14 17:40;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Build failure due to TEZ-1132 and TEZ-1416,PIG-4127,12734589,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,lbendig,lbendig,16/Aug/14 14:39,21/Nov/14 05:58,14/Mar/19 03:08,16/Aug/14 20:03,,,,,,0.14.0,,build,tez,,0,,,,,,,"Classes renamed in TEZ-1132 that need to be adjusted in Pig:

ShuffledMergedInput -> OrderedGroupedKVInput
ShuffledUnorderedKVInput -> UnorderedKVInput
OnFileUnorderedKVOutput -> UnorderedKVOutput
SortedGroupedMergedInput -> OrderedGroupedMergedKVInput
OnFileSortedOutput -> OrderedPartitionedKVOutput
OnFileUnorderedPartitionedKVOutput -> UnorderedPartitionedKVOutput

TEZ-1416 moved ObjectRegistry from org.apache.tez.runtime.common.objectregistry
to org.apache.tez.runtime.api",,,,,,,,,,,,,,,,,,,,16/Aug/14 14:43;lbendig;PIG-4127.patch;https://issues.apache.org/jira/secure/attachment/12662293/PIG-4127.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-16 19:10:38.723,,,no_permission,,,,,,,,,,,,,412529,Reviewed,,,,Sat Aug 16 20:03:32 UTC 2014,,,,,,,0|i1yyu7:,412516,,,,,,,,,,16/Aug/14 19:10;cheolsoo;+1. Thank you Lorand!,"16/Aug/14 20:03;lbendig;Patch committed to trunk. Thanks Cheolsoo, for the review!",,,,,,,,,,,,,,,,,,,,,,,,,,
TEZ-1347 broke the build,PIG-4125,12734115,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,,knoguchi,knoguchi,14/Aug/14 17:52,21/Nov/14 05:58,14/Mar/19 03:08,14/Aug/14 18:04,,,,,,0.14.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Aug/14 17:54;knoguchi;pig-4125-v01.txt;https://issues.apache.org/jira/secure/attachment/12661753/pig-4125-v01.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-14 17:55:07.322,,,no_permission,,,,,,,,,,,,,412142,Reviewed,,,,Thu Aug 14 18:04:18 UTC 2014,,,,,,,0|i1ywhb:,412131,,,,,,,,,,14/Aug/14 17:54;knoguchi;Attaching a patch with just renaming couple of method calls.,14/Aug/14 17:55;daijy;+1,14/Aug/14 18:04;knoguchi;Patch committed to trunk. Thanks for the quick review Daniel!,,,,,,,,,,,,,,,,,,,,,,,,,
Increase memory for TezMiniCluster,PIG-4123,12733839,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,13/Aug/14 18:33,21/Nov/14 05:59,14/Mar/19 03:08,31/Oct/14 04:53,,,,,,0.14.0,,tez,,,0,,,,,,,"As discussed in PIG-4114, some tez unit tests fail due to OOM (eg, TestNewPlanImplicitSplit). Temporary increase the memory to let test pass, and we need to further investigate if there is some real memory leak recently introduced by tez.",,,,,,,,,,,,,,,,,,,,13/Aug/14 18:36;daijy;PIG-4123-1.patch;https://issues.apache.org/jira/secure/attachment/12661499/PIG-4123-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-31 04:39:28.512,,,no_permission,,,,,,,,,,,,,411867,,,,,Fri Oct 31 04:53:01 UTC 2014,,,,,,,0|i1yutj:,411858,,,,,,,,,,13/Aug/14 18:44;daijy;Commit PIG-4123-1.patch first to let test run as we discussed in PIG-4114. Leave the ticket open to investigate why we need more memory.,30/Oct/14 18:51;daijy;We didn't see any memory leak for weeks. Believe Tez just use more memory and nothing is wrong. Close the ticket.,"31/Oct/14 04:39;rohini;[~daijy],
   This patch has been committed. This should be ""Resolved"". ",31/Oct/14 04:53;daijy;Sounds good.,,,,,,,,,,,,,,,,,,,,,,,,
Fix hadoopversion 23 compilation due to TEZ-1194,PIG-4122,12733836,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,13/Aug/14 18:19,21/Nov/14 05:59,14/Mar/19 03:08,13/Aug/14 18:38,,,,,,0.14.0,,tez,,,0,,,,,,,Another Tez API change.,,,,,,,,,,,,,,,,,,,,13/Aug/14 18:20;daijy;PIG-4122-1.patch;https://issues.apache.org/jira/secure/attachment/12661495/PIG-4122-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-13 18:28:29.919,,,no_permission,,,,,,,,,,,,,411864,Reviewed,,,,Wed Aug 13 18:38:39 UTC 2014,,,,,,,0|i1yusv:,411855,,,,,,,,,,13/Aug/14 18:28;cheolsoo;+1,13/Aug/14 18:38;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix hadoopversion 23 compilation due to TEZ-1237/TEZ-1407,PIG-4118,12733636,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,12/Aug/14 23:56,21/Nov/14 05:58,14/Mar/19 03:08,13/Aug/14 05:52,,,,,,0.14.0,,impl,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,12/Aug/14 23:57;daijy;PIG-4118-1.patch;https://issues.apache.org/jira/secure/attachment/12661341/PIG-4118-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-13 05:49:51.525,,,no_permission,,,,,,,,,,,,,411664,Reviewed,,,,Wed Aug 13 05:52:17 UTC 2014,,,,,,,0|i1ytlj:,411655,,,,,,,,,,13/Aug/14 05:49;rohini;+1,13/Aug/14 05:52;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Pig doc about Hadoop 2 Streaming Python UDF support,PIG-4116,12733270,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,11/Aug/14 16:15,21/Nov/14 05:58,14/Mar/19 03:08,11/Aug/14 17:17,,,,,,0.14.0,,documentation,,,0,,,,,,,"PIG-3478 fixed Python UDF in Hadoop 2, but the Pig doc still says-
{quote}
Python UDFs are an experimental feature. It does not work with Hadoop 2.
{quote}
",,,,,,,,,,,,,,,,,,,,11/Aug/14 16:17;cheolsoo;PIG-4116-1.patch;https://issues.apache.org/jira/secure/attachment/12661007/PIG-4116-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-11 16:54:08.656,,,no_permission,,,,,,,,,,,,,411298,,,,,Mon Aug 11 17:17:48 UTC 2014,,,,,,,0|i1yre7:,411290,,,,,,,,,,11/Aug/14 16:54;lbendig;+1. Thank you for taking care of this.,11/Aug/14 17:17;cheolsoo;Committed to trunk. Thank you Lorand for the quick review!,,,,,,,,,,,,,,,,,,,,,,,,,,
TEZ-1386 breaks hadoop 2 compilation in trunk,PIG-4113,12733120,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,10/Aug/14 20:02,21/Nov/14 05:58,14/Mar/19 03:08,10/Aug/14 22:29,,,,,,0.14.0,,tez,,,0,,,,,,,"{code}
    [javac] /Users/cheolsoop/workspace/pig-apache/src/org/apache/pig/backend/hadoop/executionengine/tez/TezDagBuilder.java:599: error: no suitable method found for createMRInputPayload(byte[],MRSplitsProto)
    [javac]                             .setUserPayload(MRHelpers.createMRInputPayload(
    [javac]                                                      ^
    [javac]     method MRHelpers.createMRInputPayload(ByteString,MRSplitsProto,boolean) is not applicable
    [javac]       (actual and formal argument lists differ in length)
    [javac]     method MRHelpers.createMRInputPayload(Configuration,MRSplitsProto) is not applicable
    [javac]       (actual argument byte[] cannot be converted to Configuration by method invocation conversion)
    [javac]     method MRHelpers.createMRInputPayload(byte[]) is not applicable
    [javac]       (actual and formal argument lists differ in length)
    [javac] /Users/cheolsoop/workspace/pig-apache/src/org/apache/pig/backend/hadoop/executionengine/tez/optimizers/LoaderProcessor.java:133: error: method generateInputSplitsToMem in class MRHelpers cannot be applied to given types;
    [javac]             tezOp.getLoaderInfo().setInputSplitInfo(MRHelpers.generateInputSplitsToMem(conf));
    [javac]                                                              ^
{code}",,,,,,,,,,,,,,,,,,,,10/Aug/14 20:03;cheolsoo;PIG-4113-1.patch;https://issues.apache.org/jira/secure/attachment/12660871/PIG-4113-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-10 21:56:42.565,,,no_permission,,,,,,,,,,,,,411148,,,,,Sun Aug 10 22:29:49 UTC 2014,,,,,,,0|i1yqh3:,411140,,,,,,,,,,10/Aug/14 20:03;cheolsoo;Uploading a patch.,10/Aug/14 21:56;rohini;+1,10/Aug/14 22:29;cheolsoo;Committed to trunk. Thank you Rohini for the review.,,,,,,,,,,,,,,,,,,,,,,,,,
TEZ-1382 breaks Hadoop 2 compilation,PIG-4110,12732437,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,07/Aug/14 00:41,21/Nov/14 05:59,14/Mar/19 03:08,07/Aug/14 04:06,,,,,,0.14.0,,tez,,,0,,,,,,,"{code}
    [javac] /Users/cheolsoop/workspace/pig-apache/src/org/apache/pig/backend/hadoop/executionengine/tez/ObjectCache.java:23: error: cannot find symbol
    [javac] import org.apache.tez.runtime.common.objectregistry.ObjectLifeCycle;
    [javac]                                                    ^
    [javac]   symbol:   class ObjectLifeCycle
    [javac]   location: package org.apache.tez.runtime.common.objectregistry
    [javac] /Users/cheolsoop/.ivy2/cache/org.apache.hbase/hbase-common/jars/hbase-common-0.96.0-hadoop2.jar(org/apache/hadoop/hbase/io/ImmutableBytesWritable.class): warning: Cannot find annotation method 'value()' in type 'SuppressWarnings': class file for edu.umd.cs.findbugs.annotations.SuppressWarnings not found
    [javac] /Users/cheolsoop/.ivy2/cache/org.apache.hbase/hbase-common/jars/hbase-common-0.96.0-hadoop2.jar(org/apache/hadoop/hbase/io/ImmutableBytesWritable.class): warning: Cannot find annotation method 'justification()' in type 'SuppressWarnings'
    [javac] /Users/cheolsoop/workspace/pig-apache/src/org/apache/pig/backend/hadoop/executionengine/tez/ObjectCache.java:43: error: cannot find symbol
    [javac]       registry.add(ObjectLifeCycle.VERTEX, key, value);
    [javac]                    ^
    [javac]   symbol:   variable ObjectLifeCycle
    [javac]   location: class ObjectCache
{code}",,,,,,,,,,,,,,,,,,,,07/Aug/14 00:43;cheolsoo;PIG-4110-1.patch;https://issues.apache.org/jira/secure/attachment/12660282/PIG-4110-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-07 00:48:43.646,,,no_permission,,,,,,,,,,,,,410465,,,,,Thu Aug 07 04:06:18 UTC 2014,,,,,,,0|i1ymbr:,410459,,,,,,,,,,07/Aug/14 00:43;cheolsoo;Uploading a patch.,07/Aug/14 00:48;daijy;+1,07/Aug/14 04:06;cheolsoo;Committed to trunk. Thanks Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,,,,
register local jar fail on Windows when Pig script is remote,PIG-4109,12732347,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,06/Aug/14 17:50,21/Nov/14 05:59,14/Mar/19 03:08,12/Aug/14 21:58,,,,,,0.14.0,,impl,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Aug/14 20:37;daijy;PIG-4109-1.patch;https://issues.apache.org/jira/secure/attachment/12660222/PIG-4109-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-11 04:08:42.618,,,no_permission,,,,,,,,,,,,,410375,Reviewed,,,,Tue Aug 12 21:58:50 UTC 2014,,,,,,,0|i1ylr3:,410364,,,,,,,,,,"06/Aug/14 17:58;daijy;The following script fail on Windows:
{code}
pig wasb://daijytest@hdponazuretesting2.blob.core.windows.net/1.pig
{code}
1.pig:
{code}
register c:/hdp/hive-0.13.0.2.1.3.0-1948/hcatalog/share/hcatalog/hive-hcatalog-core-0.13.0.2.1.3.0-1948.jar
a = load '1.txt';
store a into 'output';
{code}
Error message:
{code}
java.lang.NullPointerException
	at org.apache.pig.impl.io.FileLocalizer.fetchFilesInternal(FileLocalizer.java:735)
	at org.apache.pig.impl.io.FileLocalizer.fetchFiles(FileLocalizer.java:703)
	at org.apache.pig.PigServer.registerJar(PigServer.java:512)
	at org.apache.pig.PigServer.addJarsFromProperties(PigServer.java:238)
	at org.apache.pig.PigServer.<init>(PigServer.java:225)
	at org.apache.pig.PigServer.<init>(PigServer.java:207)
	at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:47)
	at org.apache.pig.Main.run(Main.java:611)
	at org.apache.pig.Main.main(Main.java:168)
{code}
The pig script does not necessary live in wasb, as long as it is remote, the issue manifest.","06/Aug/14 20:37;daijy;If script is remote, the register command assumes jar without scheme is also remote. However, in Windows, if a jar start with driver:, it is obvious it is local. Attach a patch to detect that.",11/Aug/14 04:08;rohini;Why set pig.jars.relative.to.dfs to true in the test? Isn't that actually false with the above script?,"11/Aug/14 05:46;daijy;The test is to simulate the script is remote (hdfs, s3, wasb, etc). In Main, we set ""pig.jars.relative.to.dfs"" to true if the script is remote, and thus all registered jar does not has the full scheme is also remote.",11/Aug/14 14:17;rohini;+1,12/Aug/14 21:58;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,
Pig -Dhadoopversion=23 compile fail after TEZ-1317,PIG-4108,12732345,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,06/Aug/14 17:44,21/Nov/14 05:58,14/Mar/19 03:08,06/Aug/14 19:43,,,,,,0.14.0,,impl,,,0,,,,,,,Another tez api change.,,,,,,,,,,,,,,,,,,,,06/Aug/14 17:45;daijy;PIG-4108-1.patch;https://issues.apache.org/jira/secure/attachment/12660183/PIG-4108-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-06 19:39:47.943,,,no_permission,,,,,,,,,,,,,410373,Reviewed,,,,Wed Aug 06 19:43:23 UTC 2014,,,,,,,0|i1ylqn:,410362,,,,,,,,,,06/Aug/14 19:39;cheolsoo;+1,06/Aug/14 19:43;daijy;Patch committed to trunk. Thanks Cheolsoo for quick review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Describe shouldn't trigger execution in batch mode,PIG-4106,12732322,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,06/Aug/14 16:04,21/Nov/14 05:59,14/Mar/19 03:08,06/Aug/14 23:13,0.13.0,,,,,0.13.1,0.14.0,,,,0,,,,,,,"This is a regression of PIG-3204.

Here is the comment from a Pig user-
https://issues.apache.org/jira/browse/PIG-3204?focusedCommentId=14078971&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14078971",,,,,,,,,,,,,,,,,,,,06/Aug/14 16:09;cheolsoo;PIG-4106-1.patch;https://issues.apache.org/jira/secure/attachment/12660164/PIG-4106-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-06 21:50:30.948,,,no_permission,,,,,,,,,,,,,410350,,,,,Thu Aug 07 04:00:49 UTC 2014,,,,,,,0|i1yllj:,410339,,,,,,,,,,06/Aug/14 16:09;cheolsoo;Uploading a patch.,06/Aug/14 21:50;daijy;+1,06/Aug/14 23:13;cheolsoo;Committed to trunk. Thank you Daniel for the review!,07/Aug/14 04:00;cheolsoo;I committed this patch to 0.13 branch too.,,,,,,,,,,,,,,,,,,,,,,,,
Fix TestAvroStorage with ibm jdk,PIG-4105,12732271,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,fang fang chen,fang fang chen,fang fang chen,06/Aug/14 10:44,21/Nov/14 05:59,14/Mar/19 03:08,06/Aug/14 21:53,0.13.0,,,,,0.14.0,,,,,0,,,,,,,Method positioning on stack strace in IBM jdk is different with sun jdk. Same kind issue with PIG-4025. ,,,,,,,,,,,,,,,,,,,,06/Aug/14 10:57;fang fang chen;PIG-4105.patch;https://issues.apache.org/jira/secure/attachment/12660117/PIG-4105.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-06 21:53:48.369,,,no_permission,,,,,,,,,,,,,410299,Reviewed,,,,Wed Aug 06 21:53:48 UTC 2014,,,,,,,0|i1ylan:,410289,,,,,,,,,,06/Aug/14 21:53;daijy;+1. Patch committed to trunk. Thanks Fang fang!,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""ant copypom"" failed with ""could not find file $PIG_HOME/ivy/pig.pom to copy""",PIG-4099,12731661,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,fang fang chen,fang fang chen,fang fang chen,04/Aug/14 03:21,21/Nov/14 05:58,14/Mar/19 03:08,04/Aug/14 03:43,0.13.0,site,,,,0.14.0,,build,,,0,,,,,,,"[root@hostname pig]# ant copypom
Buildfile: /root/ff/git/pig/build.xml

ivy-init-dirs:
    [mkdir] Created dir: /root/ff/git/pig/build/ivy
    [mkdir] Created dir: /root/ff/git/pig/build/ivy/lib
    [mkdir] Created dir: /root/ff/git/pig/build/ivy/report
    [mkdir] Created dir: /root/ff/git/pig/build/ivy/maven

copypom:

BUILD FAILED
/root/ff/git/pig/build.xml:1718: Warning: Could not find file /root/ff/git/pig/ivy/pig.pom to copy.

Total time: 0 seconds
",,,,,,,,,,,,,,,,,,,,04/Aug/14 03:26;fang fang chen;PIG-4099.patch;https://issues.apache.org/jira/secure/attachment/12659607/PIG-4099.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-04 03:43:43.382,,,no_permission,,,,,,,,,,,,,409690,,,,,Mon Aug 04 03:43:43 UTC 2014,,,,,,,0|i1yhm7:,409682,,,,,,,,,,04/Aug/14 03:27;fang fang chen;The patch is based on trunk branch.,04/Aug/14 03:43;cheolsoo;+1. Committed to trunk. Thank you Fangfang!,,,,,,,,,,,,,,,,,,,,,,,,,,
Vertex Location Hint api update after TEZ-1041,PIG-4098,12731659,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jeagles,jeagles,jeagles,04/Aug/14 03:13,21/Nov/14 05:59,14/Mar/19 03:08,04/Aug/14 03:41,,,,,,0.14.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,04/Aug/14 03:14;jeagles;PIG-4098-v1.patch;https://issues.apache.org/jira/secure/attachment/12659606/PIG-4098-v1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-04 03:41:00.471,,,no_permission,,,,,,,,,,,,,409688,,,,,Mon Aug 04 03:41:00 UTC 2014,,,,,,,0|i1yhlr:,409680,,,,,,,,,,04/Aug/14 03:14;jeagles;Simple API rename,04/Aug/14 03:41;cheolsoo;+1. Committed to trunk. Thank you Jonathan!,,,,,,,,,,,,,,,,,,,,,,,,,,
TestMultiQuery.testMultiQueryJiraPig1169 fails in trunk after PIG-4079 in Hadoop 1,PIG-4089,12731133,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,31/Jul/14 18:12,21/Nov/14 05:59,14/Mar/19 03:08,31/Jul/14 18:25,,,,,,0.14.0,,,,,0,,,,,,,"The job fails with the following error in *Hadoop 1* local mode-
{code}
2014-07-31 05:55:06,630 [Thread-75] WARN  org.apache.hadoop.mapred.LocalJobRunner  - job_local_0021
java.io.IOException: Illegal partition for Null: false index: 0 5 (1)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1073)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:691)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map.collect(PigGenericMapReduce.java:121)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:284)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
{code}
This is because Hadoop 1 doesn't support multiple reducers in local mode.",,,,,,,,,,,,,,,,,,,,31/Jul/14 18:15;cheolsoo;PIG-4089-1.patch;https://issues.apache.org/jira/secure/attachment/12658975/PIG-4089-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-31 18:18:24.42,,,no_permission,,,,,,,,,,,,,409205,,,,,Thu Jul 31 18:25:18 UTC 2014,,,,,,,0|i1yeov:,409201,,,,,,,,,,31/Jul/14 18:15;cheolsoo;Attaching a patch that changes parallelism of reducers from 3 to 1 so that the test case will pass in both hadoop 1 and 2. I don't think there is any reason why the parallelism needs to be greater than 1 in the test case.,31/Jul/14 18:18;daijy;+1,31/Jul/14 18:25;cheolsoo;Committed to trunk. Thank you Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,,,,
TEZ-1346 breaks hadoop 2 compilation in trunk,PIG-4088,12731123,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,31/Jul/14 17:42,21/Nov/14 05:59,14/Mar/19 03:08,31/Jul/14 19:51,,,,,,0.14.0,,tez,,,0,,,,,,,"TEZ-1346 is not published into apache snapshot repo yet, but once it's, it will break Pig trunk-
{code}
    [javac] /Users/cheolsoop/workspace/pig-stash/src/org/apache/pig/backend/hadoop/executionengine/tez/PigProcessor.java:67: error: PigProcessor is not abstract and does not override abstract method initialize() in Processor
    [javac] public class PigProcessor implements LogicalIOProcessor {
    [javac]        ^
    [javac] /Users/cheolsoop/workspace/pig-stash/src/org/apache/pig/backend/hadoop/executionengine/tez/PigProcessor.java:102: error: method does not override or implement a method from a supertype
    [javac]     @Override
    [javac]     ^
{code}",,,,,,,,,,,,,,,,,,,,31/Jul/14 17:44;cheolsoo;PIG-4088-1.patch;https://issues.apache.org/jira/secure/attachment/12658965/PIG-4088-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-31 18:18:47.3,,,no_permission,,,,,,,,,,,,,409195,,,,,Thu Jul 31 19:51:02 UTC 2014,,,,,,,0|i1yemn:,409191,,,,,,,,,,31/Jul/14 17:44;cheolsoo;Uploading a patch.,31/Jul/14 18:18;daijy;+1,31/Jul/14 19:51;cheolsoo;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Orc e2e tests for tez,PIG-4086,12731006,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,31/Jul/14 07:28,21/Nov/14 05:59,14/Mar/19 03:08,05/Aug/14 00:11,,,,,,0.14.0,,impl,,,0,,,,,,,"All Orc e2e tests fail on tez.

There are two issue:
1. hivelibdir etc is not set in tez.conf
2. OrcStorage produce empty output file

Digging into #2, the problem is in this code in PigProcessor:
{code}
                if (fileOutput.isCommitRequired()) {
                    fileOutput.commit();
                }
{code}
fileOutput.commit() invokes both RecordWriter.close() and committer.commitTask(). However, OrcNewOutputFormate will generate output file only after RecordWriter.close (if the output file is small), fileOutput.isCommitRequired will not detect this file, thus skip fileOutput.commit().

Changing the code to invoke fileOutput.close explicitly fix the issue. fileOutput.commit will invoke close again, but there is no side effect since close will check if it has been already called.
",,,,,,,,,,,,,,,,,,,TEZ-1351,31/Jul/14 07:29;daijy;PIG-4086-1.patch;https://issues.apache.org/jira/secure/attachment/12658857/PIG-4086-1.patch,02/Aug/14 00:26;daijy;PIG-4086-2.patch;https://issues.apache.org/jira/secure/attachment/12659288/PIG-4086-2.patch,05/Aug/14 00:10;daijy;PIG-4086-3.patch;https://issues.apache.org/jira/secure/attachment/12659768/PIG-4086-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-07-31 16:20:28.018,,,no_permission,,,,,,,,,,,,,409078,Reviewed,,,,Tue Aug 05 00:11:31 UTC 2014,,,,,,,0|i1ydwn:,409074,,,,,,,,,,"31/Jul/14 16:20;rohini;[~sseth]  told me when I did the patch to selectively start inputs that we should never call close() methods of input or output and only framework should do that. [~daijy], can you check with him?",02/Aug/14 00:26;daijy;TEZ-1351 will give us a flush hook. Update the patch to reflect that. That should fix the test once TEZ-1351 checked in.,02/Aug/14 04:49;rohini;+1,"05/Aug/14 00:10;daijy;Added ""getContext().canCommit()"" as discussed in TEZ-1351.",05/Aug/14 00:11;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,
TEZ-1303 broke hadoop 2 compilation in trunk,PIG-4085,12730959,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,31/Jul/14 00:29,21/Nov/14 05:58,14/Mar/19 03:08,31/Jul/14 16:42,,,,,,0.14.0,,tez,,,0,,,,,,,"{code}
    [javac] /Users/cheolsoop/workspace/pig-apache/src/org/apache/pig/backend/hadoop/executionengine/tez/PartitionerDefinedVertexManager.java:45: error: PartitionerDefinedVertexManager is not abstract and does not override abstract method initialize() in VertexManagerPlugin
    [javac] public class PartitionerDefinedVertexManager extends VertexManagerPlugin {
    [javac]        ^
    [javac] /Users/cheolsoop/workspace/pig-apache/src/org/apache/pig/backend/hadoop/executionengine/tez/PartitionerDefinedVertexManager.java:53: error: method does not override or implement a method from a supertype
    [javac]     @Override
    [javac]     ^
{code}",,,,,,,,,,,,,,,,,,,,31/Jul/14 00:31;cheolsoo;PIG-4085-1.patch;https://issues.apache.org/jira/secure/attachment/12658796/PIG-4085-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-31 07:30:21.652,,,no_permission,,,,,,,,,,,,,409031,,,,,Thu Jul 31 16:42:32 UTC 2014,,,,,,,0|i1ydm7:,409027,,,,,,,,,,31/Jul/14 00:31;cheolsoo;Uploading a patch.,31/Jul/14 07:30;daijy;+1,31/Jul/14 16:42;cheolsoo;Committed to trunk. Thanks Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,,,,
TEZ-1278 broke hadoop 2 compilation in trunk,PIG-4082,12730657,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,29/Jul/14 23:47,21/Nov/14 05:58,14/Mar/19 03:08,30/Jul/14 03:45,,,,,,0.14.0,,tez,,,0,,,,,,,"The error-
{code}
    [javac] /Users/cheolsoop/workspace/pig-svn/src/org/apache/pig/backend/hadoop/executionengine/tez/TezSessionManager.java:91: error: unreported exception InterruptedException; must be caught or declared to be thrown
    [javac]         tezClient.waitTillReady();
    [javac]                                ^
{code}",,,,,,,,,,,,,,,,,,,,30/Jul/14 00:17;cheolsoo;PIG-4082-1.patch;https://issues.apache.org/jira/secure/attachment/12658543/PIG-4082-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-30 03:24:38.616,,,no_permission,,,,,,,,,,,,,408730,,,,,Wed Jul 30 03:45:23 UTC 2014,,,,,,,0|i1ybsn:,408729,,,,,,,,,,30/Jul/14 00:17;cheolsoo;Uploading a patch.,30/Jul/14 03:24;daijy;+1,30/Jul/14 03:45;cheolsoo;Committed to trunk. Thanks Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,,,,
Parallel clause is not honored in local mode,PIG-4079,12730312,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,28/Jul/14 20:36,21/Nov/14 05:58,14/Mar/19 03:08,29/Jul/14 05:24,,,,,,0.14.0,,impl,,,0,,,,,,,"I want to debug my custom partitioner in local mode. But it's not possible with Pig because:
# MR does not invoke partitioner if # of reducers is equal to 1.
https://issues.apache.org/jira/browse/MAPREDUCE-1287
# Pig always forces a single reducers in local mode.
{code:title=LogicalPlanBuilder.java}
     void setParallel(LogicalRelationalOperator op, Integer parallel) {
         if( parallel != null ) {
            op.setRequestedParallelism( pigContext.getExecType() == ExecType.LOCAL ? 1 : parallel );
         }
     }
{code}

After I change Pig to honor the parallel clause, my custom partitioner gets invoked in local mode.",,,,,,,,,,,,,,,PIG-4089,,,,,28/Jul/14 20:40;cheolsoo;PIG-4079-1.patch;https://issues.apache.org/jira/secure/attachment/12658239/PIG-4079-1.patch,29/Jul/14 16:36;daijy;PIG-4079-2.patch;https://issues.apache.org/jira/secure/attachment/12658442/PIG-4079-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-07-28 23:33:57.829,,,no_permission,,,,,,,,,,,,,408385,,,,,Tue Jul 29 23:39:49 UTC 2014,,,,,,,0|i1y9r3:,408387,,,,,,,,,,28/Jul/14 20:40;cheolsoo;The patch changes LogicalPlanBuilder to honor the requested parallelism in local mode.,28/Jul/14 23:33;daijy;But local mode always use 1 reduce even you set parallelism to 2. Does things change?,"29/Jul/14 00:12;rohini;I think with Hadoop 2, multiple reducers are possible in local mode. Let me try find the jira.",29/Jul/14 00:13;rohini;MAPREDUCE-434 - was fixed in Hadoop 2.3.0,"29/Jul/14 02:19;cheolsoo;Yes, multiple reducers can be used in Hadoop 2.",29/Jul/14 05:01;rohini;+1,29/Jul/14 05:24;cheolsoo;Committed to trunk. Thank you Rohini/Daniel for the review!,29/Jul/14 16:36;daijy;TestLogicalPlanBuilder.testQuery18 after the patch. Attach PIG-4079-2.patch.,"29/Jul/14 16:38;rohini;+1. Can you remove the comment ""//Local mode, paraallel = 1"" before checkin?","29/Jul/14 23:39;cheolsoo;Ok, I removed the outdated comment. Thank you guys.",,,,,,,,,,,,,,,,,,
Fix pom file,PIG-4076,12729966,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,25/Jul/14 21:55,21/Nov/14 05:59,14/Mar/19 03:08,24/Aug/14 03:22,,,,,,0.14.0,,build,,,0,,,,,,,There are several entries are missing in Pig pom file. This create problem for downstream projects. For example: automaton.jar.,,,,,,,,,,,,,,,,,,,,22/Aug/14 06:28;daijy;PIG-4076-1.patch;https://issues.apache.org/jira/secure/attachment/12663607/PIG-4076-1.patch,24/Aug/14 03:13;daijy;PIG-4076-2.patch;https://issues.apache.org/jira/secure/attachment/12663895/PIG-4076-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-08-24 03:18:41.86,,,no_permission,,,,,,,,,,,,,408039,Reviewed,,,,Sun Aug 24 03:22:58 UTC 2014,,,,,,,0|i1y7nj:,408047,,,,,,,,,,24/Aug/14 03:18;cheolsoo;+1,24/Aug/14 03:22;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
TEZ-1311 broke Hadoop2 compilation,PIG-4075,12729952,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,cheolsoo,cheolsoo,cheolsoo,25/Jul/14 21:13,21/Nov/14 05:58,14/Mar/19 03:08,25/Jul/14 22:30,,,,,,0.14.0,,,,,0,,,,,,,"{code}
    [javac] /Users/cheolsoop/workspace/pig-apache/src/org/apache/pig/backend/hadoop/executionengine/tez/ObjectCache.java:25: error: cannot find symbol
    [javac] import org.apache.tez.runtime.common.objectregistry.ObjectRegistryFactory;
    [javac]                                                    ^
    [javac]   symbol:   class ObjectRegistryFactory
    [javac]   location: package org.apache.tez.runtime.common.objectregistry
    [javac] /Users/cheolsoop/.ivy2/cache/org.apache.hbase/hbase-common/jars/hbase-common-0.96.0-hadoop2.jar(org/apache/hadoop/hbase/io/ImmutableBytesWritable.class): warning: Cannot find annotation method 'value()' in type 'SuppressWarnings': class file for edu.umd.cs.findbugs.annotations.SuppressWarnings not found
    [javac] /Users/cheolsoop/.ivy2/cache/org.apache.hbase/hbase-common/jars/hbase-common-0.96.0-hadoop2.jar(org/apache/hadoop/hbase/io/ImmutableBytesWritable.class): warning: Cannot find annotation method 'justification()' in type 'SuppressWarnings'
    [javac] /Users/cheolsoop/workspace/pig-apache/src/org/apache/pig/backend/hadoop/executionengine/tez/ObjectCache.java:31: error: cannot find symbol
    [javac]     private final ObjectRegistry registry = ObjectRegistryFactory.getObjectRegistry();
    [javac]                                             ^
    [javac]   symbol:   variable ObjectRegistryFactory
    [javac]   location: class ObjectCache
{code}",,,,,,,,,,,,,,,,,,,,25/Jul/14 21:17;cheolsoo;PIG-4075-1.patch;https://issues.apache.org/jira/secure/attachment/12657899/PIG-4075-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-25 21:26:36.299,,,no_permission,,,,,,,,,,,,,408025,,,,,Fri Jul 25 22:30:47 UTC 2014,,,,,,,0|i1y7kn:,408034,,,,,,,,,,25/Jul/14 21:26;rohini;+1,25/Jul/14 22:30;cheolsoo;Committed to trunk. Thank you Rohini for reviewing the patch.,,,,,,,,,,,,,,,,,,,,,,,,,,
mapreduce.client.submit.file.replication is not honored in cached files,PIG-4074,12729926,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,25/Jul/14 21:00,21/Nov/14 05:58,14/Mar/19 03:08,27/Jul/14 06:37,,,,,,0.14.0,,impl,,,0,,,,,,,"Pig ships files to hdfs in several cases (e.g. replicated join, streaming cached files, etc). But {{mapreduce.client.submit.file.replication}} (or {{mapred.submit.replication}} for Hadoop 1.x) is not honored, and this has performance impact since many tasks read the same hdfs blocks in a large cluster.",,,,,,,,,,,,,,,,,,,,25/Jul/14 21:11;cheolsoo;PIG-4074-1.patch;https://issues.apache.org/jira/secure/attachment/12657897/PIG-4074-1.patch,26/Jul/14 01:06;cheolsoo;PIG-4074-2.patch;https://issues.apache.org/jira/secure/attachment/12657958/PIG-4074-2.patch,26/Jul/14 01:10;cheolsoo;PIG-4074-3.patch;https://issues.apache.org/jira/secure/attachment/12657959/PIG-4074-3.patch,26/Jul/14 01:25;cheolsoo;PIG-4074-4.patch;https://issues.apache.org/jira/secure/attachment/12657960/PIG-4074-4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-07-25 21:25:38.883,,,no_permission,,,,,,,,,,,,,407999,,,,,Sun Jul 27 06:37:24 UTC 2014,,,,,,,0|i1y7ev:,408008,,,,,,,,,,25/Jul/14 21:11;cheolsoo;The uploaded patch calls {{dfs.setReplication()}} after {{dfs.copyFromLocal()}}.,"25/Jul/14 21:25;rohini;Minor comments:
 - Can you leave the default at 3?
 - Also can you create a MapredConfig class and define the config as constant there. ","26/Jul/14 01:06;cheolsoo;[~rohini], I created a MRConfiguration class and refactored all the MR properties into constants. My patch got big, so I uploaded it to RB too-
https://reviews.apache.org/r/23960/

Thanks!",27/Jul/14 05:18;rohini;+1. That is awesome. Thanks for doing it for all the MR settings in the code base.,"27/Jul/14 06:37;cheolsoo;Committed to trunk. I rebased the patch to trunk.

Thank you Rohini for the review!",,,,,,,,,,,,,,,,,,,,,,,
"Fix TestStore.testSetStoreSchema, TestParamSubPreproc.testGruntWithParamSub, TestJobSubmission.testReducerNumEstimation",PIG-4071,12729606,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,24/Jul/14 23:05,21/Nov/14 05:59,14/Mar/19 03:08,29/Jul/14 00:20,,,,,,0.14.0,,impl,,,0,,,,,,,"There are 3 test failures when running unit tests in MR:
TestStore.testSetStoreSchema: fail only in Hadoop 1, we change the test sequence PIG-3935 (before that, we test mr then local, after that, local first then mr), some dirty data in local mode is not cleaned

TestParamSubPreproc.testGruntWithParamSub: the test is added by PIG-2122, not sure what happen, perhaps the test never work before

TestJobSubmission.testReducerNumEstimation: This is due to the upgrade HBase to 0.96 and only happen in Hadoop 1",,,,,,,,,,,,,,,,,,,,24/Jul/14 23:08;daijy;PIG-4071-1.patch;https://issues.apache.org/jira/secure/attachment/12657709/PIG-4071-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-29 00:08:27.132,,,no_permission,,,,,,,,,,,,,407680,Reviewed,,,,Tue Jul 29 00:20:37 UTC 2014,,,,,,,0|i1y5g7:,407690,,,,,,,,,,28/Jul/14 23:18;daijy;bump,29/Jul/14 00:08;rohini;+1,29/Jul/14 00:20;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
TestAllLoader in piggybank fails with new hive version,PIG-4067,12728894,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,22/Jul/14 16:36,21/Nov/14 05:59,14/Mar/19 03:08,23/Jul/14 16:35,,,,,,0.14.0,,,,,0,,,,,,,"After changing hive version from 0.8 to 0.14, TestAllLoader fails as RC file has magic byte in header as RCF instead of SEQ in newer versions of hive.",,,,,,,,,,,,,,,,,,,,22/Jul/14 16:38;rohini;PIG-4067-1.patch;https://issues.apache.org/jira/secure/attachment/12657138/PIG-4067-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-22 22:33:13.022,,,no_permission,,,,,,,,,,,,,406968,Reviewed,,,,Wed Jul 23 16:35:12 UTC 2014,,,,,,,0|i1y15b:,406987,,,,,,,,,,22/Jul/14 22:33;daijy;+1,23/Jul/14 16:35;rohini;Committed to trunk (0.14). Thanks for the review Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
Use single config in Tez for input and output,PIG-4058,12727470,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,15/Jul/14 20:57,21/Nov/14 05:58,14/Mar/19 03:08,16/Jul/14 00:02,,,,,,0.14.0,,,,,0,,,,,,,TEZ-1272 gets rid of separate configuration for input and output and has a common configuration.,,,,,,,,,,,,,,,,,,,,15/Jul/14 23:57;rohini;PIG-4058-1.patch;https://issues.apache.org/jira/secure/attachment/12655933/PIG-4058-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-15 23:59:02.488,,,no_permission,,,,,,,,,,,,,405576,Reviewed,,,,Wed Jul 16 00:02:13 UTC 2014,,,,,,,0|i1xsnz:,405602,,,,,,,,,,15/Jul/14 23:59;daijy;+1,16/Jul/14 00:02;rohini;Committed to trunk (0.14). Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,
Group All followed by CROSS with default parallelism produces wrong results,PIG-4057,12727135,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,rohini,rohini,14/Jul/14 17:52,21/Nov/14 05:59,14/Mar/19 03:08,28/Jul/14 02:54,,,,,,0.14.0,,,,,0,,,,,,,"SET default_parallel 199;
......
by_size = ...
uniq_vals = .....
grpd = group uniq_vals all;
all_vals = FOREACH grpd GENERATE uniq_vals;
cross_result = CROSS by_size, all_vals;
store cross_result into '/tmp/roh/cross/out/recipient_asns';

Job1: grpd, all_vals, cross_result (The plan does GFCross function here for
all_vals assuming cross parallelism to be 1 taking that of the current job even
though it should consider default parallelism 199 of Job 2. Parallelism of Job1
is 1 because of group all)
Job2: cross_result (Actual CROSS of by_size and all_vals)",,,,,,,,,,,,,,,,,,,,22/Jul/14 04:15;daijy;PIG-4057-1.patch;https://issues.apache.org/jira/secure/attachment/12657053/PIG-4057-1.patch,24/Jul/14 00:45;daijy;PIG-4057-2.patch;https://issues.apache.org/jira/secure/attachment/12657504/PIG-4057-2.patch,25/Jul/14 00:19;daijy;PIG-4057-3.patch;https://issues.apache.org/jira/secure/attachment/12657731/PIG-4057-3.patch,25/Jul/14 01:35;daijy;PIG-4057-4.patch;https://issues.apache.org/jira/secure/attachment/12657747/PIG-4057-4.patch,25/Jul/14 23:34;daijy;PIG-4057-5.patch;https://issues.apache.org/jira/secure/attachment/12657944/PIG-4057-5.patch,28/Jul/14 02:51;daijy;PIG-4057-6.patch;https://issues.apache.org/jira/secure/attachment/12658062/PIG-4057-6.patch,28/Jul/14 21:24;daijy;PIG-4057-7.patch;https://issues.apache.org/jira/secure/attachment/12658255/PIG-4057-7.patch,29/Jul/14 06:45;daijy;PIG-4057-8.patch;https://issues.apache.org/jira/secure/attachment/12658368/PIG-4057-8.patch,,,,8.0,,,,,,,,,,,,,,,,,,,2014-07-16 22:05:38.635,,,no_permission,,,,,,,,,,,,,405242,Reviewed,,,,Tue Jul 29 15:39:04 UTC 2014,,,,,,,0|i1xqnj:,405271,,,,,,,,,,"14/Jul/14 18:06;rohini;There are more issues to address. 
   - Cross would have problems even if group by was not group all and specified a different parallelism than cross. 
   - Another case would be if there was no default parallel specified and different reducers were estimated for Job1 and Job2. 

Since GFCross explicitly uses mapred.reduce.tasks need to also see how it is working in Tez.",14/Jul/14 19:03;rohini;One way to fix this would be to always have GFCross UDF as part of map task of the actual cross job and never do it as part of previous job's map or reduce. Trying to see if there is a better alternative for GFCross implementation that could do away with relying on parallelism of the reducer as it will cause problems with Tez auto parallelism. ,"16/Jul/14 22:05;daijy;The problem is GFCross divide to difference number of cross groups. If the number of groups are the same, then result is correct, whether it is equal to number of reducers does not matter.

However, when number of cross groups correlates to the number of reducers, the performance is the best, in that:
1. If there are too much groups, every record will be duplicated many times, there are more intermediate results generated
2. If there are too few groups, some reducers takes no groups to process, cause imbalance load

I can fix it by forcing all GFCross use the same cross groups at the frontend. If tez auto-parallelism is used, we may end up with non-optimal cross groups. Auto-parallelism works by over estimate the number of reduce in the frontend, so we might end up having more cross groups, which means larger intermediate results. But I don't see an easy way to peek the number of reduce at the runtime and pass to GFCross.",22/Jul/14 04:25;daijy;RB link: https://reviews.apache.org/r/23787/,25/Jul/14 00:19;daijy;Address [~cheolsoo]'s review comments.,25/Jul/14 06:28;cheolsoo;+1,25/Jul/14 23:34;daijy;Another patch addressing Rohini's review comments.,26/Jul/14 03:27;rohini;+1,28/Jul/14 02:51;daijy;Fix unit test failures and resync with trunk.,28/Jul/14 02:54;daijy;Patch committed to trunk. Thanks Cheolsoo and Rohini for review!,28/Jul/14 02:55;rohini;+1,28/Jul/14 21:24;daijy;TestPigContext still fail in tez. Attach PIG-4057-7.patch to fix it.,28/Jul/14 21:25;daijy;PIG-4057-7.patch committed to trunk.,29/Jul/14 06:45;daijy;Another unit test failure caused by the patch (PIG-4057-8.patch).,29/Jul/14 15:39;rohini;+1,,,,,,,,,,,,,
Remove PhysicalOperator.setAlias,PIG-4056,12726974,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,13/Jul/14 00:12,21/Nov/14 05:58,14/Mar/19 03:08,17/Jul/14 21:38,,,,,,0.14.0,,,,,0,,,,,,,  I added the method for Tez. But it did exist before and was removed in PIG-2659 to have it always associated with orginial location. ,,,,,,,,,,,,,,,,,,,,13/Jul/14 00:13;rohini;PIG-4056-1.patch;https://issues.apache.org/jira/secure/attachment/12655425/PIG-4056-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-17 21:27:16.16,,,no_permission,,,,,,,,,,,,,405081,Reviewed,,,,Thu Jul 17 21:38:03 UTC 2014,,,,,,,0|i1xppj:,405117,,,,,,,,,,17/Jul/14 21:27;daijy;+1,17/Jul/14 21:38;rohini;Committed to trunk (0.14). Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,
Build broke after TEZ-1130 API rename,PIG-4055,12726884,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,knoguchi,knoguchi,knoguchi,11/Jul/14 22:06,21/Nov/14 05:58,14/Mar/19 03:08,12/Jul/14 02:30,,,,,,0.14.0,,,,,0,,,,,,,"API rename was done in TEZ-1130
(""Replace confusing names on Vertex API"").

Need to change the pig side accordingly to fix the build.",,,,,,,,,,,,,,,,,,,,11/Jul/14 22:11;knoguchi;pig-4055-v01.patch;https://issues.apache.org/jira/secure/attachment/12655321/pig-4055-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-11 22:25:17.498,,,no_permission,,,,,,,,,,,,,404991,,,,,Sat Jul 12 02:30:35 UTC 2014,,,,,,,0|i1xp5z:,405027,,,,,,,,,,"11/Jul/14 22:11;knoguchi;A simple patch renaming the following
* vertex.setTaskLocalResources -> vertex.setTaskLocalFiles
* vertex.setJavaOpts -> vertex.setTaskLaunchCmdOpts
* vertex.getVertexName() -> vertex.getName()

No test is attached since this is just a rename.",11/Jul/14 22:25;rohini;+1,12/Jul/14 01:50;jeagles;This patch works for me as well,"12/Jul/14 02:30;knoguchi;Committed to trunk.
Thanks Rohini and Jon for the review!",,,,,,,,,,,,,,,,,,,,,,,,
TestMRCompiler succeeded with sun jdk 1.6 while failed with sun jdk 1.7,PIG-4053,12726509,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,fang fang chen,fang fang chen,10/Jul/14 06:09,21/Nov/14 05:59,14/Mar/19 03:08,27/Jul/14 06:10,0.13.0,,,,,0.14.0,,,,,0,,,,,,,"TestMRCompiler succeeded with sun jdk 1.6.0_45 while failed with sun jdk 1.7.0_60 with following error message:
Testcase: testMergeJoin took 0.193 sec
	FAILED
expected:<....PigStorage','eNqtVT[tvE0EQnlzixJiQhGeDKBCv7k6iQqKABIgwHNgiaXDF5G5zPti7XXb3wpkCiQYKKKFAAomCkt+AhCiooaRC9NSUMLu248MBUZgtLO/M7jy+/b65t9+hphXsv41b6Bcm5f6yUtgLU23Kh5+PvPiIr6ZhqgkzOr3PSgkAU/dm7C9dui5U4qPEqMt8mSb+BkZ3WB77XYyFkD4rWUQRRc7yJM3pSLen0wh5iD2mfMkx1357YGvTDvprygOvA3soUGtzmfNmLgsTQk3IDKWBYyElDfpJA0oapJnkgS08uFZwk15DebZUcGKsNHvKpfRbkik0QtmMT9/pl1/DD10P6iE0slUlsktxwvRdeADTlDO7ynrawGJo0RkkofghzGUhwy1GvqWKz4JGzpmsJV2IWgiz2Q0hjNvNhrCQrYlNM55m3lnXRdVWz6r7UhLaR//UknuxYeMDAD0PpmwVZHFVuNt7Rw98GXWXfLW5L+8/HLr1aRq8VWhwgfEqRgRME3aZrmK6K3hcynPnXcz5e3X6XbJ/S1dTY4fDMuL4P2EnRCvvfAW8NCagdSQkM7CvDyadT4I1o9I8OVsOu+qawTFKc3MS4hGLqTtN7mFNRMNWW4nbLDKj2mY7sJDqgZkeI4870BBbTFmkGG0OSiUiGyhPVjBpba4XkjPdgQMUnR5kjeycOSO5m1DXBpW5IDixJxK8yHJiz8EKe7Z1Z/m78b87vLTDNmoVKGE4ScKhineGnu9ADaOoyEjRqVXysjE2R9y0ON0tSEIsbqNCzhlPdbYCjT586z3JiBjIU3R8W6CNC2Dg8PgIiNFg4JAm9c26U0PF7eFULSZsXWHElIFT43cly4firRykOHuFSqlValFEaDsfhpwThalkaEhULDd2nBhoj4cfwBj0YQzGYAx+gzFwIzGojkSqY1rZMXNxosA3HKSWV0f/KdDR27khQno1cPKPQ9deCyq6Jq1aTBYBftI6LWk5kNxyH41GKe0sWpmEa/1eKjNkBeYVM4XK1wyaQhMD+gz6fZy0NqyOqURvOFBgu7j+F62xPSXtBb+ZG5Ywte/b6zc/Hj4+49mPYG0LecEIxqXRuetFtsHUo7fPj+x+9vWJFZNLUJbj4e328F+Hp/M6+ModoDlD/S83YUI3yPIXqjl9HQ=]=','','','scope','tr...> but was:<....PigStorage','eNqtVT[1vE0EQnVzixJiQhM8GUSC+ujuJCokCEiDCcGCLpMEVk7vN+WDvdtndC2cKJBoooIQCCSQKSn4DEqKghpIK0VNTwuzaji8OCImwheWd2Z038/bN3NvvUNMK9t/GDfQLk3J/USnshak25cPPR158xFeTMNGEKZ3eZ6UEgIl7U/aXLl0XKvFRYtRlvkwTfw2jOyyP/S7GQkiflSyiiCJneZLmdKTb02mEPMQeU77kmGu/PbC1aQf9NeGB14E9FKi1vsh5M5eFCaEmZIbSwLGQQIM+aECgQZpJHtjEg2sFN+k1lGdLBSfGUrOnHKTfkkyhEcoiPn2nX34NP3Q9qIfQyJaVyC7FCdN34QFMEmZ2lfW0gfnQsjMAofghzGQhww1GvoWKz5JGzqmsJV2IWgjT2Q0hjNtNhzCXrYh1Mw4z66yromqrZ9V9KYnto78ryb3YsPABgZ4HEzYLsrgs3O29owe+jLpLvtrMl/cfDt36NAneMjS4wHgZIyKmCbtMVzHdFTwu5bnzLubCvbr9tX9Ll1Nj4JjddFhFHP8r7cRo5Z2vgJfGRLSOhGQG9vXJpPNJsGJUmidny2FVXTM4RjA3dyI8UjFVp8k9zIlk2GorcZtFZpTbdAfmUj0w02PkcQcaYoMpyxSjzUGpRGQD5ckSJq311UJypjtwgKLTg6yQnTNnJHcT6tqgMhcEJ/VEghdZTuo5WFHPZt9Z/a797wovbbONSgUCDHcCOOzi7aFnO1DDKCoy6ujUdvKiMRYjblqe7hbUQixuo0LOGU91tgSNPn2rPclIGMhTdHqbo40LYODw+AiI0WDgmKbum3anhh23h1O2mLBVhRFTBk6N35UsHzZv5SDF2StUSqVSiSJCW/kw5IwoTAWhIVGx3NhxYqA9Hn5AY9CnMRijMdhCY+BGYlAdiZTHpLJj5uKOAt9wlFpdHf1rg47ezg0R6lcDJ387dO21oNLX1KuWk3mAn7ROS1qOJLfcRwNKaWfR0k601q+lMkOWYFYxU6h8xaApNCmgr6Ct46S1ZvuYUvSGAwU2kxt90fpT0l7wm7lhCVP7vr1+8+Ph4zOe/QjWNpAXjGhcGJ27XmRrTD16+/zI7mdfn9hmcgBlOR7ebg//cXg6r6Ov3EaaM9S33Nw2j//ZDbL8BaHefQc]=','','','scope','tr...>
junit.framework.AssertionFailedError: expected:<....PigStorage','eNqtVT[tvE0EQnlzixJiQhGeDKBCv7k6iQqKABIgwHNgiaXDF5G5zPti7XXb3wpkCiQYKKKFAAomCkt+AhCiooaRC9NSUMLu248MBUZgtLO/M7jy+/b65t9+hphXsv41b6Bcm5f6yUtgLU23Kh5+PvPiIr6ZhqgkzOr3PSgkAU/dm7C9dui5U4qPEqMt8mSb+BkZ3WB77XYyFkD4rWUQRRc7yJM3pSLen0wh5iD2mfMkx1357YGvTDvprygOvA3soUGtzmfNmLgsTQk3IDKWBYyElDfpJA0oapJnkgS08uFZwk15DebZUcGKsNHvKpfRbkik0QtmMT9/pl1/DD10P6iE0slUlsktxwvRdeADTlDO7ynrawGJo0RkkofghzGUhwy1GvqWKz4JGzpmsJV2IWgiz2Q0hjNvNhrCQrYlNM55m3lnXRdVWz6r7UhLaR//UknuxYeMDAD0PpmwVZHFVuNt7Rw98GXWXfLW5L+8/HLr1aRq8VWhwgfEqRgRME3aZrmK6K3hcynPnXcz5e3X6XbJ/S1dTY4fDMuL4P2EnRCvvfAW8NCagdSQkM7CvDyadT4I1o9I8OVsOu+qawTFKc3MS4hGLqTtN7mFNRMNWW4nbLDKj2mY7sJDqgZkeI4870BBbTFmkGG0OSiUiGyhPVjBpba4XkjPdgQMUnR5kjeycOSO5m1DXBpW5IDixJxK8yHJiz8EKe7Z1Z/m78b87vLTDNmoVKGE4ScKhineGnu9ADaOoyEjRqVXysjE2R9y0ON0tSEIsbqNCzhlPdbYCjT586z3JiBjIU3R8W6CNC2Dg8PgIiNFg4JAm9c26U0PF7eFULSZsXWHElIFT43cly4firRykOHuFSqlValFEaDsfhpwThalkaEhULDd2nBhoj4cfwBj0YQzGYAx+gzFwIzGojkSqY1rZMXNxosA3HKSWV0f/KdDR27khQno1cPKPQ9deCyq6Jq1aTBYBftI6LWk5kNxyH41GKe0sWpmEa/1eKjNkBeYVM4XK1wyaQhMD+gz6fZy0NqyOqURvOFBgu7j+F62xPSXtBb+ZG5Ywte/b6zc/Hj4+49mPYG0LecEIxqXRuetFtsHUo7fPj+x+9vWJFZNLUJbj4e328F+Hp/M6+ModoDlD/S83YUI3yPIXqjl9HQ=]=','','','scope','tr...> but was:<....PigStorage','eNqtVT[1vE0EQnVzixJiQhM8GUSC+ujuJCokCEiDCcGCLpMEVk7vN+WDvdtndC2cKJBoooIQCCSQKSn4DEqKghpIK0VNTwuzaji8OCImwheWd2Z038/bN3NvvUNMK9t/GDfQLk3J/USnshak25cPPR158xFeTMNGEKZ3eZ6UEgIl7U/aXLl0XKvFRYtRlvkwTfw2jOyyP/S7GQkiflSyiiCJneZLmdKTb02mEPMQeU77kmGu/PbC1aQf9NeGB14E9FKi1vsh5M5eFCaEmZIbSwLGQQIM+aECgQZpJHtjEg2sFN+k1lGdLBSfGUrOnHKTfkkyhEcoiPn2nX34NP3Q9qIfQyJaVyC7FCdN34QFMEmZ2lfW0gfnQsjMAofghzGQhww1GvoWKz5JGzqmsJV2IWgjT2Q0hjNtNhzCXrYh1Mw4z66yromqrZ9V9KYnto78ryb3YsPABgZ4HEzYLsrgs3O29owe+jLpLvtrMl/cfDt36NAneMjS4wHgZIyKmCbtMVzHdFTwu5bnzLubCvbr9tX9Ll1Nj4JjddFhFHP8r7cRo5Z2vgJfGRLSOhGQG9vXJpPNJsGJUmidny2FVXTM4RjA3dyI8UjFVp8k9zIlk2GorcZtFZpTbdAfmUj0w02PkcQcaYoMpyxSjzUGpRGQD5ckSJq311UJypjtwgKLTg6yQnTNnJHcT6tqgMhcEJ/VEghdZTuo5WFHPZt9Z/a797wovbbONSgUCDHcCOOzi7aFnO1DDKCoy6ujUdvKiMRYjblqe7hbUQixuo0LOGU91tgSNPn2rPclIGMhTdHqbo40LYODw+AiI0WDgmKbum3anhh23h1O2mLBVhRFTBk6N35UsHzZv5SDF2StUSqVSiSJCW/kw5IwoTAWhIVGx3NhxYqA9Hn5AY9CnMRijMdhCY+BGYlAdiZTHpLJj5uKOAt9wlFpdHf1rg47ezg0R6lcDJ387dO21oNLX1KuWk3mAn7ROS1qOJLfcRwNKaWfR0k601q+lMkOWYFYxU6h8xaApNCmgr6Ct46S1ZvuYUvSGAwU2kxt90fpT0l7wm7lhCVP7vr1+8+Ph4zOe/QjWNpAXjGhcGJ27XmRrTD16+/zI7mdfn9hmcgBlOR7ebg//cXg6r6Ov3EaaM9S33Nw2j//ZDbL8BaHefQc]=','','','scope','tr...>
	at org.apache.pig.test.TestMRCompiler.run(TestMRCompiler.java:1148)
	at org.apache.pig.test.TestMRCompiler.testMergeJoin(TestMRCompiler.java:1014)

",,,,,,,,,,,,,,,,,,,,27/Jul/14 02:52;daijy;PIG-4053-1.patch;https://issues.apache.org/jira/secure/attachment/12658010/PIG-4053-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-27 02:52:07.642,,,no_permission,,,,,,,,,,,,,404616,Reviewed,,,,Sun Jul 27 06:10:12 UTC 2014,,,,,,,0|i1xmvj:,404654,,,,,,,,,,"27/Jul/14 02:52;daijy;In general, TestMRCompiler.testMergeJoin is a constant trouble maker. I would like to rewrite it.","27/Jul/14 05:03;cheolsoo;+1.
{quote}
org.junit.Assume.assumeFalse(""Skip this test for hadoop 0.20.2. See PIG-3194"", VersionInfo.getVersion().equals(""0.20.2""));
{quote}
With your new test, this line can be removed, right? Can you delete it?","27/Jul/14 06:10;daijy;Yes, this line should be removed.

Patch plus removing the line committed to trunk. Thanks Cheolsoo for review!",,,,,,,,,,,,,,,,,,,,,,,,,
"TestJobControlSleep, TestInvokerSpeed are unreliable",PIG-4052,12726508,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,daijy,fang fang chen,fang fang chen,10/Jul/14 06:03,21/Nov/14 05:58,14/Mar/19 03:08,27/Jul/14 06:13,0.13.0,,,,,0.14.0,,,,,0,,,,,,,"This test case failed on both sun jdk 1.7 and sun jdk 1.6.

sun jdk 1.6 error msg:
Testcase: testLocalModeTakesLessThan5secs took 6.397 sec
        FAILED
must take less than 5 seconds
junit.framework.AssertionFailedError: must take less than 5 seconds
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.TestJobControlSleep.testLocalModeTakesLessThan5secs(TestJobControlSleep.java:53)

sun jdk 1.7 error msg:
Testcase: testLocalModeTakesLessThan5secs took 6.629 sec
	FAILED
must take less than 5 seconds
junit.framework.AssertionFailedError: must take less than 5 seconds
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.TestJobControlSleep.testLocalModeTakesLessThan5secs(TestJobControlSleep.java:53)

","jdk version:
    java version ""1.7.0_60""
    java version ""1.6.0_45""
ant version
    Apache Ant(TM) version 1.9.4 compiled on April 29 2014",,,,,,,,,,,,,,,,,,,27/Jul/14 03:03;daijy;PIG-4052-1.patch;https://issues.apache.org/jira/secure/attachment/12658011/PIG-4052-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-27 03:03:44.298,,,no_permission,,,,,,,,,,,,,404615,Reviewed,,,,Sun Jul 27 06:13:38 UTC 2014,,,,,,,0|i1xmvb:,404653,,,,,,,,,,27/Jul/14 03:03;daijy;These tests occasionally fail no matter jdk 1.6 or jdk 1.7. They are unstable in nature and make our CI unstable from time to time. I'd like to ignore them.,"27/Jul/14 04:54;cheolsoo;+1.

Alternatively, we could remove them. I am fine either way.","27/Jul/14 06:13;daijy;All right, let's remove it.

The change committed to trunk. Thanks Cheolsoo for review!",,,,,,,,,,,,,,,,,,,,,,,,,
Pigmix returns an error ,PIG-4051,12725870,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,keren3000,keren3000,07/Jul/14 22:47,21/Nov/14 05:58,14/Mar/19 03:08,11/Oct/14 08:37,0.13.0,,,,,0.14.0,,,,,0,,,,,,,"I am getting an exception as I deploy Pigmix as follows:

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/pig/tools/cmdline/CmdLineParser
    [exec] Skimming users
    [exec] at org.apache.pig.test.pigmix.datagen.DataGenerator.run(DataGenerator.java:77)",,,,,,,,,,,,,,,,,,,,07/Jul/14 22:49;daijy;PIG-4051-1.patch;https://issues.apache.org/jira/secure/attachment/12654408/PIG-4051-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-11 08:37:13.967,,,no_permission,,,,,,,,,,,,,404028,,,,,Sat Oct 11 08:37:13 UTC 2014,,,,,,,0|i1xjav:,404069,,,,,,,,,,11/Oct/14 08:37;daijy;This issue is fixed as part of PIG-4047.,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopShims.getTaskReports() can cause OOM with Hadoop 2,PIG-4050,12725818,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,07/Jul/14 19:11,21/Nov/14 05:59,14/Mar/19 03:08,28/Sep/14 00:59,,,,,,0.14.0,,,,,0,,,,,,,Details in https://issues.apache.org/jira/browse/PIG-4043?focusedCommentId=14046878&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14046878 ,,,,,,,,,,,,,,,,,,,,25/Sep/14 00:10;rohini;PIG-4050-1.patch;https://issues.apache.org/jira/secure/attachment/12671098/PIG-4050-1.patch,25/Sep/14 16:34;rohini;PIG-4050-2.patch;https://issues.apache.org/jira/secure/attachment/12671251/PIG-4050-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-09-26 22:05:11.757,,,no_permission,,,,,,,,,,,,,403976,Reviewed,,,,Sun Sep 28 00:59:02 UTC 2014,,,,,,,0|i1xizj:,404018,,,,,,,,,,"25/Sep/14 00:11;rohini;[~cheolsoo],
    With this patch, can you check if you still need to turn off fetching task reports?",25/Sep/14 16:34;rohini;Had missed changes to hadoop20 shims. Updated patch. ,26/Sep/14 22:05;daijy;Patch LGTM. +1.,28/Sep/14 00:59;rohini;Committed to trunk (0.14). Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,
TEZ-692 has a incompatible API change removing TezSession,PIG-4048,12725523,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,04/Jul/14 19:04,21/Nov/14 05:58,14/Mar/19 03:08,06/Jul/14 16:08,,,,,,0.14.0,,,,,0,,,,,,, TEZ-692 simplifies job submission by only keeping TezClient and removing TezSession. Need to update pig code to use TezClient instead of TezSession.,,,,,,,,,,,,,,,,,,,,05/Jul/14 06:54;rohini;PIG-4048-1.patch;https://issues.apache.org/jira/secure/attachment/12654170/PIG-4048-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-06 03:33:08.858,,,no_permission,,,,,,,,,,,,,403682,,,,,Sun Jul 06 16:08:46 UTC 2014,,,,,,,0|i1xh6n:,403725,,,,,,,,,,"05/Jul/14 06:54;rohini;Ran couple of e2e tests in Join and MergeJoin and TestFinish unit test. Found that unit tests like TestBinaryExpressionOps, TestFRJoin were hanging with last task waiting for input. See that it was happening even before TEZ-692. Will investigate and file a separate jira for that. ",06/Jul/14 03:33;cheolsoo;+1,"06/Jul/14 06:39;mrflip;+1 -- Pig stopped compiling when hadoopversion=23 was set, this fixed the issue. ",06/Jul/14 16:08;rohini;Committed the patch. Thanks Cheolsoo and Philip for the review,,,,,,,,,,,,,,,,,,,,,,,,
PiggyBank DBStorage DATETIME should use setTimestamp with java.sql.Timestamp,PIG-4046,12724494,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,sinchii,tfriest,tfriest,30/Jun/14 17:57,21/Nov/14 05:59,14/Mar/19 03:08,28/Sep/14 01:06,0.12.1,,,,,0.14.0,,piggybank,,,0,,,,,,,"In Pig 0.12.1 PiggyBank DBStorage (contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/DBStorage.java), the code uses the wrong setter and Java data type for the SQL DATETIME datatype.

DATETIME has both date and time, so should use java.sql.Timestamp instead of java.sql.Date (which zeros the time fields).

replace line 121
          case DataType.DATETIME:
-            ps.setDate(sqlPos, new Date(((DateTime) field).getMillis()));
            sqlPos++;
            break;
with
          case DataType.DATETIME:
+            ps.setTimestamp(sqlPos, new Timestamp(((DateTime) field).getMillis(
)));
            sqlPos++;
            break;
",CentOS 6.5,,,,,,,,,,,,,,,,,,,27/Sep/14 12:40;sinchii;PIG-4046.001.patch;https://issues.apache.org/jira/secure/attachment/12671646/PIG-4046.001.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-27 12:40:24.827,,,no_permission,,,,,,,,,,,,,402677,Reviewed,,,,Sun Sep 28 01:06:04 UTC 2014,,,,,,,0|i1xb5j:,402744,,,,,,,,,,27/Sep/14 12:40;sinchii;I attach the patch file on the basis of former comment.,"28/Sep/14 01:06;daijy;Patch committed to trunk. Thanks Shinichi, Timothy!",,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should use avro-mapred-hadoop2.jar instead of avro-mapred.jar when compile with hadoop 2,PIG-4044,12724378,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,30/Jun/14 04:14,21/Nov/14 05:58,14/Mar/19 03:08,30/Jun/14 19:42,,,,,,0.14.0,,impl,,,0,,,,,,,"We see the following stack when using AvroStorage when using hadoop 2:
2014-06-28 22:42:25,656 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
        at org.apache.avro.mapreduce.AvroRecordReaderBase.initialize(AvroRecordReaderBase.java:87)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.initialize(PigRecordReader.java:192)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:525)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)",,,,,,,,,,,,,,,,,,,,30/Jun/14 04:17;daijy;PIG-4044-1.patch;https://issues.apache.org/jira/secure/attachment/12653091/PIG-4044-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-30 06:00:56.62,,,no_permission,,,,,,,,,,,,,402561,Reviewed,,,,Mon Jun 30 19:42:52 UTC 2014,,,,,,,0|i1xafz:,402628,,,,,,,,,,30/Jun/14 06:00;cheolsoo;+1,30/Jun/14 19:42;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
JobClient.getMap/ReduceTaskReports() causes OOM for jobs with a large number of tasks,PIG-4043,12724271,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,28/Jun/14 02:24,21/Nov/14 05:59,14/Mar/19 03:08,30/Jun/14 03:30,,,,,,0.14.0,,,,,0,,,,,,,"With Hadoop 2.4, I often see Pig client fails due to OOM when there are many tasks (~100K) with 1GB heap size.

The heap dump (attached) shows that TaskReport[] occupies about 80% of heap space at the time of OOM.

The problem is that JobClient.getMap/ReduceTaskReports() returns an array of TaskReport objects, which can be huge if the number of task is large.",,,,,,,,,,,,,PIG-4050,,,,,,,28/Jun/14 02:40;cheolsoo;PIG-4043-1.patch;https://issues.apache.org/jira/secure/attachment/12652992/PIG-4043-1.patch,28/Jun/14 22:56;cheolsoo;PIG-4043-2.patch;https://issues.apache.org/jira/secure/attachment/12653028/PIG-4043-2.patch,29/Jun/14 23:54;cheolsoo;PIG-4043-3.patch;https://issues.apache.org/jira/secure/attachment/12653078/PIG-4043-3.patch,30/Jun/14 03:11;cheolsoo;PIG-4043-4.patch;https://issues.apache.org/jira/secure/attachment/12653085/PIG-4043-4.patch,30/Jun/14 03:29;cheolsoo;PIG-4043-5.patch;https://issues.apache.org/jira/secure/attachment/12653087/PIG-4043-5.patch,28/Jun/14 02:25;cheolsoo;heapdump.png;https://issues.apache.org/jira/secure/attachment/12652990/heapdump.png,,,,,,6.0,,,,,,,,,,,,,,,,,,,2014-06-28 15:04:01.732,,,no_permission,,,,,,,,,,,,,402456,,,,,Wed Sep 17 04:04:50 UTC 2014,,,,,,,0|i1x9s7:,402523,,,,,,,,,,"28/Jun/14 02:37;cheolsoo;One thing to note is that the size of TaskReport object seems to have increased significantly in Hadoop 2 as compared to Hadoop 1. The same job can run with no problem in Hadoop 1 using the same size of heap, but it fails in Hadoop 2.

The attached patch introduces a new property {{pig.stats.noTaskReport}} via which retrieving TaskReports can be disabled for large jobs. By default, it is set to false, so JobStats will still use TaskReports.

I also documented this new property in {{pig.properties}}.","28/Jun/14 15:04;rohini;Patch is good. Just one minor comment. Can you make pig.stats.noTaskReport configuration all lower case?

But before having to try add an option to turn off taskreports, I would like to suggest trying another thing. HadoopShims.java getTaskReports() for Hadoop 2.x does the below

{code}
org.apache.hadoop.mapreduce.TaskReport[] reports = mrJob.getTaskReports(type);
            return DowngradeHelper.downgradeTaskReports(reports); 
{code}

I think the OOM is because there are two huge arrays during the same time unlike Hadoop 1.x HadoopShims. I would suggest getTaskReports return an iterator instead of array and for every .next() call do TaskReport.downgrade() for the TaskReport to be returned. That way there is only the initial big array and only one TaskReport at a time.","28/Jun/14 22:48;cheolsoo;{quote}
I think the OOM is because there are two huge arrays during the same time unlike Hadoop 1.x HadoopShims.
{quote}
This isn't true. In fact, I am seeing OOM in 0.12 that doesn't include the code you're referring to (introduced by PIG-3913). In 0.12, there are no two copies of TaskReport arrays. If you look at the heap dump, it is a single array object that is as big as 800MB.

In addition, I see the same issue in Lipstick, for example, [here|https://github.com/Netflix/Lipstick/blob/master/lipstick-console/src/main/java/com/netflix/lipstick/pigtolipstick/BasicP2LClient.java#L414]. The Pig dies as soon as calling {{JobClient.getTaskMapReports()}}. I've been running several tests so far. It's clear that I cannot run my job (100K mappers) with any {{JobClient.getTaskMapReports()}} call in both Pig and Lipstick in Hadoop 2.4.

Unless {{JobClient.getTaskMapReports()}} itself returns an iterator, we need a way of disabling it.","28/Jun/14 22:56;cheolsoo;Per Rohini, make the property name all lowercase.","29/Jun/14 19:58;rohini;In that case we can keep this property. In this patch, I only see shims/src/hadoop20 HadoopShims.java modified and not hadoop23. You seem to have missed including it in the patch. Can you also
  -  modify one of the existing MiniCluster unit tests and pass this property? 
  - log a message about not fetching task reports as it is turned off

I will create a separate jira later to avoid the two arrays created by PIG-3913. The two arrays could cause more jobs to fail that were passing in 0.12 and we have to get rid of it. 
",29/Jun/14 23:54;cheolsoo;Thank you Rohini. Here is a new patch that incorporates all your comments.,"29/Jun/14 23:57;cheolsoo;Ah, wait. I misread what you asked for MiniCluster unit tests. I thought you're asking to turn off task reports for all MiniCluster unit tests.

Let me submit a new patch...",29/Jun/14 23:59;rohini;Sorry. Just realized you mistook my previous comment and changed MiniCluster itself. That would turn it off for all unit tests. What I meant was one of the unit tests using MiniCluster. Also noticed a small typo TaskRepors ->TaskReports.,"30/Jun/14 03:11;cheolsoo;Okie, hopefully this is the last patch. I added a new test to TestMRJobStats using a mini cluster and revert my previous changes to MiniCluster classes. Thanks! 
","30/Jun/14 03:15;rohini;+1. 

Changing getAvgREduceTime() to getAvgReduceTime() breaks Oozie. So need to keep it as it is. Can you revert that part of the patch and check in. ","30/Jun/14 03:29;cheolsoo;Sure, I took it out. Attaching the final patch for record.

Committed to trunk. Thank you Ronihi for your review!","17/Sep/14 04:04;rohini;bq.  In 0.12, there are no two copies of TaskReport arrays
  Actually. Even it has. The two arrays are inside JobClient.java in mapreduce though. ",,,,,,,,,,,,,,,,
org.apache.pig.backend.hadoop.executionengine.tez.util.MRToTezHelper compiling error,PIG-4041,12723974,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jeagles,airbots,airbots,26/Jun/14 21:10,21/Nov/14 05:58,14/Mar/19 03:08,26/Jun/14 22:48,0.14.0,,,,,0.14.0,,,,,0,,,,,,,Since TezConfiguration.TEZ_AM_JAVA_OPTS does not exists in Tez-0.5.0 anymore.,,,,,,,,,,,,,,,,,,,,26/Jun/14 22:29;cheolsoo;PIG-4041-2.patch;https://issues.apache.org/jira/secure/attachment/12652701/PIG-4041-2.patch,26/Jun/14 21:26;jeagles;PIG-4041.patch;https://issues.apache.org/jira/secure/attachment/12652692/PIG-4041.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-06-26 21:26:32.463,,,no_permission,,,,,,,,,,,,,402159,,,,,Thu Jun 26 22:48:33 UTC 2014,,,,,,,0|i1x7yn:,402224,,,,,,,,,,26/Jun/14 21:26;jeagles;Posting a starter patch. Not sure this is the correct fix for TEZ-1127 corresponding change.,26/Jun/14 21:33;airbots;Anyone can assign this JIRA to [~jeagles]. Thanks!,"26/Jun/14 22:29;cheolsoo;I am also seeing this error after TEZ-1234-
{code}
    [javac] /Users/cheolsoop/workspace/pig-svn/src/org/apache/pig/backend/hadoop/executionengine/tez/PartitionerDefinedVertexManager.java:45: error: interface expected here
    [javac] public class PartitionerDefinedVertexManager implements VertexManagerPlugin {
{code}
Here is a patch fixes both errors.","26/Jun/14 22:33;cheolsoo;[~daijy], [~rohini], can someone +1 this?",26/Jun/14 22:43;daijy;+1,26/Jun/14 22:48;cheolsoo;Committed to trunk. Thanks everyone!,,,,,,,,,,,,,,,,,,,,,,
SPRINTF should return NULL on any NULL input,PIG-4038,12723746,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,mrflip,mrflip,mrflip,25/Jun/14 20:46,21/Nov/14 05:58,14/Mar/19 03:08,26/Jun/14 04:24,,,,,,0.14.0,,internal-udfs,,,0,null,sprintf,udf,,,,"SPRINTF will currently interpolate the string 'null' if supplied with null arguments, which is ""helpful but not help-ing"". This patch makes it return null if any argument -- format or fodder -- is null.

(See PIG-3939 for SPRINTF implementation)",,,,,,,,,,,,,,,,,,,,25/Jun/14 20:47;mrflip;0001-PIG-4038-SPRINTF-returns-null-on-any-null-argument.patch;https://issues.apache.org/jira/secure/attachment/12652499/0001-PIG-4038-SPRINTF-returns-null-on-any-null-argument.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-26 04:24:29.258,,,no_permission,,,,,,,,,,,,,401931,Reviewed,,,,Thu Jun 26 04:24:29 UTC 2014,,,Patch Available,,,,0|i1x6kn:,401999,SPRINTF returns null on any null argument,,,,,,,,,"25/Jun/14 20:47;mrflip;attached patch: docs, test, code",26/Jun/14 04:24;daijy;+1. Patch committed to trunk. Thanks Philip!,,,,,,,,,,,,,,,,,,,,,,,,,,
"TestHBaseStorage, TestAccumuloPigCluster has failures with hadoopversion=23",PIG-4037,12723455,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,rohini,rohini,24/Jun/14 20:12,07/Jul/14 18:07,14/Mar/19 03:08,26/Jun/14 20:38,,,,,,0.13.0,0.14.0,,,,0,,,,,,,"PIG-4005 enabled TestHBaseStorage to be run with hadoopversion=23, but there are 5 test failures and 1 error. TestAccumuloPigCluster fail for the same reason.",,,,,,,,,,,,,,,,,,,,25/Jun/14 05:58;daijy;PIG-4037-1.patch;https://issues.apache.org/jira/secure/attachment/12652344/PIG-4037-1.patch,26/Jun/14 04:16;daijy;PIG-4037-2.patch;https://issues.apache.org/jira/secure/attachment/12652548/PIG-4037-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-06-25 05:58:05.531,,,no_permission,,,,,,,,,,,,,401642,Reviewed,,,,Thu Jun 26 20:38:24 UTC 2014,,,,,,,0|i1x4tr:,401715,,,,,,,,,,25/Jun/14 05:58;daijy;Move PIG-4005-3.patch to here.,26/Jun/14 04:16;daijy;TestAccumuloPigCluster share the same issue.,26/Jun/14 13:15;rohini;I am not sure if we should get rid of Minicluster mode testing all together. We don't have e2e tests for hbase. So it would be to have at least the unit tests running in MiniCluster mode so we have some better testing. ,"26/Jun/14 17:40;daijy;It's not this patch getting rid of MiniCluster. TestHBaseStorage is in local mode even before auto local mode change, TestAccumuloPigCluster is in local mode since added. The do have MiniCluster and injecting configuration to Pig, but that's only cause side effect. We do not run job on MiniCluster.","26/Jun/14 19:12;rohini;Ah. Did not realize we were executing in ExecType.LOCAL mode.

+1",26/Jun/14 20:38;daijy;Patch committed to both 0.13 branch and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,
"Fix e2e failures - JobManagement_3, CmdErrors_3 and BigData_4",PIG-4036,12723064,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,rohini,rohini,23/Jun/14 04:48,07/Jul/14 18:08,14/Mar/19 03:08,27/Jun/14 05:12,,,,,,0.13.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,24/Jun/14 07:14;daijy;PIG-4036-0.patch;https://issues.apache.org/jira/secure/attachment/12652152/PIG-4036-0.patch,25/Jun/14 01:46;daijy;PIG-4036-1.patch;https://issues.apache.org/jira/secure/attachment/12652327/PIG-4036-1.patch,26/Jun/14 23:31;daijy;PIG-4036-2.patch;https://issues.apache.org/jira/secure/attachment/12652708/PIG-4036-2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-06-23 21:41:35.219,,,no_permission,,,,,,,,,,,,,401251,Reviewed,,,,Fri Jun 27 05:12:00 UTC 2014,,,,,,,0|i1x2fr:,401328,,,,,,,,,,"23/Jun/14 04:55;rohini;JobManagement_3, CmdErrors_3 fail with  java.lang.NoClassDefFoundError: org/apache/zookeeper/Shell . StreamingCommand.java - import org.apache.zookeeper.Shell should be import org.apache.hadoop.util.Shell;","23/Jun/14 05:46;rohini; BigData_4 stacktrace is below. 

{code}
2014-06-23 04:54:16,640 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:2219)
	at java.util.ArrayList.grow(ArrayList.java:213)
	at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:187)
	at java.util.ArrayList.add(ArrayList.java:411)
	at org.apache.pig.data.InternalCachedBag.add(InternalCachedBag.java:82)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.CombinerPackager.getNext(CombinerPackager.java:128)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage.getNextTuple(POPackage.java:271)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.processOnePackageOutput(PigCombiner.java:177)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:168)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:51)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:170)
	at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1631)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1568)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1417)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:664)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:333)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:158)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1300)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:153)
{code}

Problem is with protected DataBag[] bags; in Packager.java. While POCombinerPackage in older version iterated through the tuple, current code in POPackage iterates through the tuple and puts in a bag (in BigData_4 that becomes 600+MB) and passes the bag to POCombinerPackage which causes OOM when trying to add to the bag there as memory is already occupied by the other bag. We need to do some rework on POPackage and Packager.  ","23/Jun/14 21:41;daijy;I saw we materialized the bag in Packager if readOnce flag is set (which is the case for most script). Not sure what does readOnce do and why we need to materialize the bag. [~mwagner], can you give me some hint?","23/Jun/14 22:13;mwagner;The readOnce flag is the method by which POPackage communicates which bags the Packager can assume are already persisted in memory and which are ephemeral and must be copied into memory. This enables the streaming optimization on the last bag in classic-MR and all inputs for Tez. The Packager base class requires all bags to actually be persisted, so it copies into memory. The CombinerPackager can have it's one and only input streamed so it should overwrite attachInput() to reflect that (which it doesn't right now).

I'll work on verifying that fix.","23/Jun/14 22:18;rohini;Even in other cases like join, persisting to memory instead of iterating and processing immediately does not sound good to me. Will hit OOM issues and the extra copy will cause performance regression. ","23/Jun/14 22:33;mwagner;That's the goal of using readOnce. The idea was that the POPackage could be optimistic/lazy about what bags actually need to be in memory (assume that they don't need to be), and the classes extending Packager can bring into memory what it needs. The JoinPackager does not hold the bag in memory and just iterates over it.

Looking through the other code, it looks like this may be a bug in LitePackager as well.",24/Jun/14 01:38;daijy;Looks like the ReadOnce bag optimization is not currently be used. It anyway will be materialized first. The JoinPackager will use NonSpillableDataBag not ReadOnce bag. Can we change it back to InternalCachedBag?,"24/Jun/14 07:14;daijy;[~rohini], POCombinerPackage also materialize the bag. Now in Packager, we do materialize from PeekedBag to InternalCachedBag, PeekedBag doing streaming, though I cannot see the value for doing that, this should not use more memory. I believe the problem is we pass the wrong bagCount to InternalCachedBag, so InternalCachedBag is using unlimited memory. I attach a draft patch, can you give a try.","24/Jun/14 12:36;rohini;[~daijy],
   I can try the patch and might work with this test, but I still have my doubts that it will fail with some other script with more memory requirements that passed with older releases as there are two copies of the bag even though the tuples are referenced and not copied. Also it will not solve the problem of performance degradation due to overhead of putting the tuples in two InternalCachedBags.",24/Jun/14 12:43;rohini;Was bag count being set to 0 before in this case?,"24/Jun/14 17:06;daijy;Yes, bag count set to 0 in BigData_4.",24/Jun/14 17:11;rohini;Wouldn't that have caused divide by zero error in MemoryLimits.java when doing maxMemUsage = (long) ((maxMem * percent) / bagCount);,"24/Jun/14 18:05;daijy;It doesn't in the float context, and we end up with a Long.MAX_VALUE.","25/Jun/14 01:47;daijy;Rohini also find MultiQueryPackager still using InternalCachedBag, and materialized into another InternalCachedBag. I fix that in the new patch.",25/Jun/14 17:11;daijy;There are some test failures. I will work on another patch.,26/Jun/14 17:42;daijy;All e2e and unit tests pass for me.,26/Jun/14 23:31;daijy;Forget to attach updated patch.,27/Jun/14 01:11;rohini;+1,27/Jun/14 05:12;daijy;Patch committed to 0.13 branch and trunk. Thanks Rohini for review!,,,,,,,,,
Fix CollectedGroup e2e tests for tez,PIG-4035,12723062,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,23/Jun/14 04:35,21/Nov/14 05:58,14/Mar/19 03:08,23/Jun/14 04:59,,,,,,0.14.0,,tez,,,0,,,,,,,,,,,,,,,,,,,,PIG-3834,,,,,,,23/Jun/14 04:35;daijy;PIG-4035-1.patch;https://issues.apache.org/jira/secure/attachment/12651927/PIG-4035-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-23 04:52:36.483,,,no_permission,,,,,,,,,,,,,401249,Reviewed,,,,Mon Jun 23 04:59:02 UTC 2014,,,,,,,0|i1x2fb:,401326,,,,,,,,,,23/Jun/14 04:52;rohini;+1,23/Jun/14 04:59;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude TestTezAutoParallelism when -Dhadoopversion=20,PIG-4034,12723057,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,23/Jun/14 03:46,21/Nov/14 05:58,14/Mar/19 03:08,23/Jun/14 03:55,,,,,,0.14.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Jun/14 03:48;cheolsoo;PIG-4034-1.patch;https://issues.apache.org/jira/secure/attachment/12651919/PIG-4034-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-23 03:50:34.356,,,no_permission,,,,,,,,,,,,,401244,,,,,Mon Jun 23 03:55:57 UTC 2014,,,,,,,0|i1x2ef:,401322,,,,,,,,,,23/Jun/14 03:50;daijy;+1,23/Jun/14 03:55;cheolsoo;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix MergeSparseJoin e2e tests on tez,PIG-4033,12723051,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,23/Jun/14 02:23,21/Nov/14 05:58,14/Mar/19 03:08,23/Jun/14 03:10,,,,,,0.14.0,,tez,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Jun/14 02:23;daijy;PIG-4033-1.patch;https://issues.apache.org/jira/secure/attachment/12651914/PIG-4033-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-23 02:56:32.364,,,no_permission,,,,,,,,,,,,,401238,Reviewed,,,,Mon Jun 23 03:10:56 UTC 2014,,,,,,,0|i1x2d3:,401316,,,,,,,,,,23/Jun/14 02:56;rohini;+1,23/Jun/14 03:10;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
BloomFilter fails with s3 path in Hadoop 2.4,PIG-4032,12722997,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,22/Jun/14 06:52,21/Nov/14 05:59,14/Mar/19 03:08,22/Jun/14 20:10,,,,,,0.14.0,,,,,0,,,,,,,"BloomFilter is broken with s3 path in Hadoop 2. Here is a simple example-
{code}
DEFINE bloomtest Bloom('s3n://foo/bar/bloom');
a = LOAD 's3n://foo/bar/test.txt' using PigStorage('\t') as (k:int, v:int) ;
split a into yes if bloomtest(k,v), no otherwise;
dump yes;
{code}
This query fails with the following error-
{code}
14/06/22 06:28:58 INFO jobcontrol.ControlledJob: PigLatin:test.pig got an error while submitting
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: s3n:__foo_bar_bloom
	at org.apache.hadoop.fs.Path.initialize(Path.java:206)
	at org.apache.hadoop.fs.Path.<init>(Path.java:172)
	at org.apache.hadoop.mapreduce.v2.util.MRApps.parseDistributedCacheArtifacts(MRApps.java
{code}
The problem is that the distributed cache file name {{s3n:__foo_bar_bloom}} causes a uri syntax error because of the s3n prefix.

In fact, this is a regression of HADOOP-8562 that includes the following change-
{code:title=Path.java}
-      this.uri = new URI(scheme, authority, normalizePath(path), null, fragment)
+      this.uri = new URI(scheme, authority, normalizePath(scheme, path), null, fragment)
{code}
Since the scheme was ignored in Hadoop 1, s3 path used to work accidentally. But in Hadoop 2, it starts failing.",,,,,,,,,,,,,,,,,,,,22/Jun/14 07:02;cheolsoo;PIG-4032-1.patch;https://issues.apache.org/jira/secure/attachment/12651847/PIG-4032-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-22 19:55:54.117,,,no_permission,,,,,,,,,,,,,401184,,,,,Sun Jun 22 20:10:04 UTC 2014,,,,,,,0|i1x21b:,401263,,,,,,,,,,22/Jun/14 07:01;cheolsoo;The patch discards the scheme from the given path and only uses the rest to create a distributed cache file name.,22/Jun/14 19:55;daijy;+1,22/Jun/14 20:10;cheolsoo;Committed to trunk. Thank you Daniel for review the patch.,,,,,,,,,,,,,,,,,,,,,,,,,
Provide Counter aggregation for Tez,PIG-4031,12722908,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,20/Jun/14 23:23,21/Nov/14 05:58,14/Mar/19 03:08,22/Jun/14 03:10,,,,,,0.14.0,,tez,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Jun/14 23:25;daijy;PIG-4031-1.patch;https://issues.apache.org/jira/secure/attachment/12651768/PIG-4031-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-21 15:45:32.584,,,no_permission,,,,,,,,,,,,,401095,Reviewed,,,,Sun Jun 22 03:10:18 UTC 2014,,,,,,,0|i1x1if:,401176,,,,,,,,,,"20/Jun/14 23:56;daijy;This fix e2e tests failure Warning_[1,2,4,5].",21/Jun/14 15:45;cheolsoo;+1,22/Jun/14 03:10;daijy;Patch committed to trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,
"TestGrunt, TestPigRunner fail after PIG-3892",PIG-4030,12722885,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,20/Jun/14 22:03,07/Jul/14 18:08,14/Mar/19 03:08,22/Jun/14 02:51,,,,,,0.13.0,0.14.0,,,,0,,,,,,,Both tests hard code pig-withouthadoop.jar.,,,,,,,,,,,,,,,,,,,,20/Jun/14 22:11;daijy;PIG-4030-1.patch;https://issues.apache.org/jira/secure/attachment/12651749/PIG-4030-1.patch,22/Jun/14 02:50;daijy;PIG-4030-2-trunk.patch;https://issues.apache.org/jira/secure/attachment/12651839/PIG-4030-2-trunk.patch,22/Jun/14 02:41;daijy;PIG-4030-2.patch;https://issues.apache.org/jira/secure/attachment/12651838/PIG-4030-2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-06-21 15:45:24.81,,,no_permission,,,,,,,,,,,,,401072,Reviewed,,,,Sun Jun 22 02:51:03 UTC 2014,,,,,,,0|i1x1dr:,401155,,,,,,,,,,21/Jun/14 15:45;cheolsoo;+1,22/Jun/14 02:41;daijy;There is another occurrence of the same patten. ,22/Jun/14 02:51;daijy;Patch committed to both 0.13 branch and trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,
TestMRCompiler is broken after PIG-3874,PIG-4029,12722858,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,20/Jun/14 20:09,07/Jul/14 18:07,14/Mar/19 03:08,22/Jun/14 03:07,,,,,,0.13.0,0.14.0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,22/Jun/14 03:05;daijy;PIG-4029-1-trunk.patch;https://issues.apache.org/jira/secure/attachment/12651840/PIG-4029-1-trunk.patch,20/Jun/14 21:37;daijy;PIG-4029-1.patch;https://issues.apache.org/jira/secure/attachment/12651740/PIG-4029-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-06-21 15:48:48.432,,,no_permission,,,,,,,,,,,,,401045,Reviewed,,,,Sun Jun 22 03:07:20 UTC 2014,,,,,,,0|i1x17z:,401129,,,,,,,,,,"20/Jun/14 21:37;daijy;Besides the golden file change, I also invoke PigServer.shutdown() to remove the temporary files. The next test will not generate temporary file collide, thus make the golden file stable.",21/Jun/14 15:48;cheolsoo;+1,22/Jun/14 03:07;daijy;Patch committed to both 0.13 branch and trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,,,,,,,,
add a flag to control the ivy resolve/retrieve output,PIG-4028,12722672,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,gkesavan,gkesavan,gkesavan,20/Jun/14 00:56,21/Nov/14 05:58,14/Mar/19 03:08,20/Jun/14 01:28,0.13.0,0.14.0,,,,0.14.0,,build,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,20/Jun/14 01:17;gkesavan;PIG-4028.txt;https://issues.apache.org/jira/secure/attachment/12651582/PIG-4028.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-20 01:18:50.134,,,no_permission,,,,,,,,,,,,,400863,Reviewed,,,,Fri Jun 20 01:28:44 UTC 2014,,,,,,,0|i1x03r:,400948,,,,,,,,,,"20/Jun/14 01:17;gkesavan;this patch sets the ivy:resolve and ivy:retrieve log level to quiet by default. 
if one want's to get the verbose resolve/retrieve output, they can pass 
{code}-Dloglevel=default|download-only{code}
",20/Jun/14 01:18;daijy;+1,20/Jun/14 01:28;daijy;Patch committed to trunk. Thanks Giri!,,,,,,,,,,,,,,,,,,,,,,,,,
Always check for latest Tez snapshot dependencies,PIG-4027,12722632,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,lbendig,lbendig,lbendig,19/Jun/14 23:04,21/Nov/14 05:58,14/Mar/19 03:08,19/Jun/14 23:57,,,,,,0.14.0,,build,tez,,0,,,,,,,"The ivy resolver for apache-snapshots sets {{checkModified=""true""}} which updates the metadata of Tez dependencies if there are modifications, but since the intermediary Tez builds are published under the same revision (0.5.0-incubating-SNAPSHOT) the local cache will only be populated for the first time with the jars and it won't get updated afterwards.

In order to force ivy to always look up the repository whether there are changes in the Tez snapshots jars, {{changing=""true""}} should be set on the Tez dependencies.

Remark: when Tez dependencies are bumped up to a stable release, this flag needs to be removed.
",,,,,,,,,,,,,,,,,,,,19/Jun/14 23:05;lbendig;PIG-4027.patch;https://issues.apache.org/jira/secure/attachment/12651548/PIG-4027.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-19 23:47:44.659,,,no_permission,,,,,,,,,,,,,400823,,,,,Fri Jun 20 08:16:47 UTC 2014,,,,,,,0|i1wzvb:,400910,,,,,,,,,,19/Jun/14 23:47;cheolsoo;+1,19/Jun/14 23:57;cheolsoo;Committed to trunk. Thanks Lorand!,"20/Jun/14 08:16;lbendig;Cheolsoo, thanks for committing it!",,,,,,,,,,,,,,,,,,,,,,,,,
BigDec/Int sort is broken,PIG-4023,12722400,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ahireanup,ahireanup,ahireanup,19/Jun/14 02:15,21/Nov/14 05:59,14/Mar/19 03:08,25/Jun/14 05:42,0.12.0,,,,,0.14.0,,,,,0,,,,,,,"BigDec/Int sort seems broken. I am getting following errors.

java.lang.ClassCastException: java.math.BigInteger incompatible with java.lang.Double
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigBigIntegerRawComparator.compare(PigBigIntegerRawComparator.java:99)
	at java.util.Arrays.binarySearch(Arrays.java:700)
	at java.util.Arrays.binarySearch(Arrays.java:305)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:81)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:60)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:689)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map.collect(PigGenericMapReduce.java:122)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:284)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:140)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:751)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:368)
	----------------------------------------------------------------------------

	java.lang.ClassCastException: java.math.BigDecimal incompatible with java.lang.Double
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigBigDecimalRawComparator.compare(PigBigDecimalRawComparator.java:99)
	at java.util.Arrays.binarySearch(Arrays.java:700)
	at java.util.Arrays.binarySearch(Arrays.java:305)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:81)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:60)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:689)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map.collect(PigGenericMapReduce.java:122)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:284)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:140)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:751)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:368)

	
	PigBig(Decimal/Intger)RawComparator wrongly casts BigInteger and BigDecimal to Double.

Also, Big(Decimal/Integer)Writable classes are not using buffer offset and length while initializing ByteArrayInputStream in compare method",,,,,,,,,,,,,,,,,,,,24/Jun/14 00:10;ahireanup;PIG-4023.patch;https://issues.apache.org/jira/secure/attachment/12652091/PIG-4023.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-25 05:42:45.068,,,no_permission,,,,,,,,,,,,,400591,Reviewed,,,,Wed Jun 25 19:15:49 UTC 2014,,,,,,,0|i1wyhb:,400685,,,,,,,,,,19/Jun/14 02:15;ahireanup;I can provide a patch,24/Jun/14 00:47;ahireanup;Attached patch !,"25/Jun/14 05:42;daijy;A clear bug. Thanks Anup!

Patch committed to trunk.",25/Jun/14 19:15;ahireanup;Thanks Daniel !,,,,,,,,,,,,,,,,,,,,,,,,
Fix tez e2e test SkewedJoin_6,PIG-4022,12722394,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,19/Jun/14 01:28,21/Nov/14 05:59,14/Mar/19 03:08,20/Jun/14 18:33,,,,,,0.14.0,,tez,,,0,,,,,,,SkewedJoin_6 hang when run with tez.,,,,,,,,,,,,,,,,,,,,19/Jun/14 18:55;daijy;PIG-4022-1.patch;https://issues.apache.org/jira/secure/attachment/12651469/PIG-4022-1.patch,19/Jun/14 21:37;daijy;PIG-4022-2.patch;https://issues.apache.org/jira/secure/attachment/12651502/PIG-4022-2.patch,19/Jun/14 22:10;daijy;PIG-4022-3.patch;https://issues.apache.org/jira/secure/attachment/12651520/PIG-4022-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-06-19 21:09:48.891,,,no_permission,,,,,,,,,,,,,400585,Reviewed,,,,Fri Jun 20 18:33:40 UTC 2014,,,,,,,0|i1wyfz:,400679,,,,,,,,,,19/Jun/14 21:09;rohini;+1,"19/Jun/14 21:11;rohini;A related question. We return sample even on POStatus.STATUS_ERR and endOfAllInput. Should we be doing that?
{code}
else if (res.returnStatus == POStatus.STATUS_EOP
                    || res.returnStatus == POStatus.STATUS_ERR) {
                if (this.parentPlan.endOfAllInput) {
                    return createNumRowTuple((Tuple)newSample.result);
                } else {
                    continue;
                }
            }
{code}","19/Jun/14 21:31;daijy;no, we shall return STATUS_ERR instead of making the return tuple normal. Let me change that in patch.","19/Jun/14 22:01;rohini;bq. no, we shall return STATUS_ERR instead of making the return tuple normal. Let me change that in patch.
 Actually patch does not make it return STATUS_ERR. It continues to the next record in the skipping block and in the next block it will return currentSample","19/Jun/14 22:10;daijy;Sorry, lost in curly brackets :)",19/Jun/14 22:30;rohini;+1,20/Jun/14 18:33;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,
Fix TestHBaseStorage failure after auto local mode change (PIG-3463),PIG-4021,12722360,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/Jun/14 22:14,07/Jul/14 18:07,14/Mar/19 03:08,22/Jun/14 03:09,,,,,,0.13.0,0.14.0,impl,,,0,,,,,,,"There are 5 failures and 1 abort after PIG-3463.

[~aniket486], do you know if HBaseStorage suppose to work in auto local mode?",,,,,,,,,,,,,,,,,,,,19/Jun/14 06:27;daijy;PIG-4021-1.patch;https://issues.apache.org/jira/secure/attachment/12651373/PIG-4021-1.patch,19/Jun/14 23:53;daijy;PIG-4021-2.patch;https://issues.apache.org/jira/secure/attachment/12651558/PIG-4021-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-06-19 00:20:10.91,,,no_permission,,,,,,,,,,,,,400551,Reviewed,,,,Sun Jun 22 03:09:03 UTC 2014,,,,,,,0|i1wy8f:,400645,,,,,,,,,,19/Jun/14 00:20;aniket486;I do not know why it should not. I may have also seen it work in production (although later we disabled it). What's the failure? I will try to investigate more.,"19/Jun/14 06:27;daijy;The test only fail on Hadoop 1.

The test runs with local mode before PIG-3463, but now it runs with MiniCluster mode. The attached patch force the test run with local mode, and test pass. Still don't have an explanation why. However, the test is written in a mixed mode which is not typical, this does not seems something we need to worry.","19/Jun/14 21:15;rohini;I think we should make the tests all minicluster mode instead of this change. Even though it works, it is really confusing and I am not sure how cluster.getConfiguration() with mapred.job.tracker=local is even supposed to behave. We might hit other problems later.","19/Jun/14 21:49;daijy;Believe it is originally in MiniCluster mode and intentionally change to local mode to speedup test. I actually less concern about the test itself. The change just force local mode and nothing tricky. I worry about PIG-3463 must change sequence of configuration propagation, and might lead to some other issue.",19/Jun/14 21:56;aniket486;PIG-3463 is optional feature that is turned off by default.,19/Jun/14 21:59;daijy;The test fail even after disabling auto local mode. Seems there is some side effect when we change the way propagating the configuration.,19/Jun/14 23:53;daijy;Attach another patch to fix the configuration propagation. I will run all tests to make sure it does not break anything.,20/Jun/14 22:08;daijy;Unit test all clear with the patch.,21/Jun/14 16:01;cheolsoo;+1,22/Jun/14 03:09;daijy;Patch committed to both 0.13 branch and trunk. Thanks Cheolsoo for review!,,,,,,,,,,,,,,,,,,
"Fix tez e2e tests MapPartialAgg_[2-4], StreamingPerformance_[6-7]",PIG-4020,12722296,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/Jun/14 18:40,21/Nov/14 05:58,14/Mar/19 03:08,18/Jun/14 20:58,,,,,,0.14.0,,tez,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,18/Jun/14 18:41;daijy;PIG-4020-1.patch;https://issues.apache.org/jira/secure/attachment/12651212/PIG-4020-1.patch,20/Jun/14 18:35;daijy;PIG-4020-2.patch;https://issues.apache.org/jira/secure/attachment/12651705/PIG-4020-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-06-18 19:16:32.288,,,no_permission,,,,,,,,,,,,,400487,Reviewed,,,,Fri Jun 20 20:54:33 UTC 2014,,,,,,,0|i1wxuf:,400582,,,,,,,,,,18/Jun/14 19:16;rohini;Can you explain the failure root cause and what the change does?  Doesn't doing getStreamCloseResult() after processInput() call processInput() again which will skip the current input and try to fetch the next one? Or the input is always set by attachInput() for POSplit  and that should not be a problem?,"18/Jun/14 20:40;daijy;In short, processInput() in MR will not result endOfAllInput flag set, but in Tez that's no longer true.

In MR, we run the pipeline once per key. During a particular key, we keep pulling the bottom of the pipeline until see a EOP. endOfAllInput will not be set during the process. In cleanup, we will set endOfAllInput flag and pull the pipeline again. 

In Tez, we run the pipeline once per task. During the process, we keep pulling the input, and endOfAllInput will be set during our pulling. 

So in MR, after processInput (POSplit:214), we don't need to check if endOfAllInput is set or not, but in Tez, we need to check. If it is set, then we need to pull till the pipeline becomes empty to finalize the data processing, instead of simply return a EOP, which will result the loss of the later part of data.",18/Jun/14 20:42;rohini;+1,18/Jun/14 20:58;daijy;Patch committed to trunk. Thanks Rohini for review!,20/Jun/14 18:35;daijy;Miss the STATUS_EOP check. Add another patch.,20/Jun/14 20:54;rohini;+1,,,,,,,,,,,,,,,,,,,,,,
Compilation broken after TEZ-1169,PIG-4019,12722294,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,18/Jun/14 18:34,21/Nov/14 05:58,14/Mar/19 03:08,18/Jun/14 18:56,,,,,,0.14.0,,tez,,,0,,,,,,,"Error message:
{code}
    [javac] /Users/daijy/pig/src/org/apache/pig/backend/hadoop/executionengine/tez/PartitionerDefinedVertexManager.java:95: setVertexParallelism(int,org.apache.tez.dag.api.VertexLocationHint,java.util.Map<java.lang.String,org.apache.tez.dag.api.EdgeManagerDescriptor>,java.util.Map<java.lang.String,org.apache.tez.runtime.api.RootInputSpecUpdate>) in org.apache.tez.dag.api.VertexManagerPluginContext cannot be applied to (int,<nulltype>,java.util.Map<java.lang.String,org.apache.tez.dag.api.EdgeManagerDescriptor>)
    [javac]                 context.setVertexParallelism(dynamicParallelism, null, edgeManagers);
    [javac]                        ^
{code}",,,,,,,,,,,,,,,,,,,,18/Jun/14 18:38;daijy;PIG-4019-1.patch;https://issues.apache.org/jira/secure/attachment/12651209/PIG-4019-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-18 18:53:18.812,,,no_permission,,,,,,,,,,,,,400485,Reviewed,,,,Wed Jun 18 18:56:05 UTC 2014,,,,,,,0|i1wxtz:,400580,,,,,,,,,,18/Jun/14 18:53;alangates;+1,18/Jun/14 18:56;daijy;Patch committed to trunk. Thanks Alan for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema validation fails with UNION ONSCHEMA,PIG-4018,12722262,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,tmwoodruff,tmwoodruff,tmwoodruff,18/Jun/14 16:44,21/Nov/14 05:58,14/Mar/19 03:08,22/Jun/14 03:11,0.13.0,,,,,0.14.0,,,,,0,,,,,,,"When relations with differing schemas are unioned (using UNION ONSCHEMA), schema validation can fail with this exception:

{{org.apache.pig.impl.plan.PlanValidationException: Logical plan invalid state: invalid uid -1 in schema}}

This worked before the fix for PIG-3492.

The merged schema (from {{LOUnion.getSchema()}}) does not contain uids for columns not in the schema of the first input (uids are set to -1). This is because only the first input's schema is used for looking up ""cached"" uids.

Normally, this isn't a problem because {{UnionOnSchemaSetter}} comes along and fixes the missing fields.

However, when {{ImplicitSplitInsertVisitor}} is active, it is called before {{UnionOnSchemaSetter}}. {{ImplicitSplitInsertVisitor}} calls {{schemaResetter.visit()}}, which throws the validation exception because {{UnionOnSchemaSetter}} has not had a chance to create the missing fields (and thus uids are still -1 for these fields).",,,,,,,,,,,,,,,,,,,,20/Jun/14 19:41;tmwoodruff;PIG-4018-2.patch;https://issues.apache.org/jira/secure/attachment/12651710/PIG-4018-2.patch,18/Jun/14 17:00;tmwoodruff;PIG-4018.patch;https://issues.apache.org/jira/secure/attachment/12651186/PIG-4018.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-06-20 00:04:38.517,,,no_permission,,,,,,,,,,,,,400453,Reviewed,,,,Sun Jun 22 03:11:50 UTC 2014,,,,,,,0|i1wxnz:,400552,,,,,,,,,,"18/Jun/14 17:00;tmwoodruff;This is an attempt at a patch. It moves UnionOnSchemaSetter above ImplicitSplitInsertVisitor in LogicalPlan.validate().

It also contains a small test that demonstrates the problem (apply the test but not the LogicalPlan patch to see it fail).

This seems to work in my testing, but I'm not super familiar with the logical plan generation process, so I'm not 100% sure about the effects here. The operations between the old location and the new are 

- {{ImplicitSplitInsertVisitor}} -- expected
- {{DuplicateForEachColumnRewriteVisitor}} -- UnionOnSchemaSetter adds a few more FOREACHes for this to check, but doesn't seem like this should have a noticeable impact
- {{TypeCheckingRelVisitor}} -- Again, a couple more FOREACHes to check, but there should be no functional impact.
","20/Jun/14 00:04;cheolsoo;[~tmwoodruff], I ran the unit tests and found the following failures-
{code}
>>> org.apache.pig.test.TestUnionOnSchema.testUnionOnSchemaUdfTypeEvolution2
>>> org.apache.pig.test.TestUnionOnSchema.testUnionOnSchemaUdfTypeEvolution
>>> org.apache.pig.test.TestUnionOnSchema.testUnionOnSchemaIncompatibleTypes
{code}
Can you take a look at them?",20/Jun/14 19:35;tmwoodruff;Sorry about that. I completely overlooked that test somehow. New patch forthcoming.,"20/Jun/14 19:41;tmwoodruff;Looks like TypeCheckingRelVisitor adds type conversion, so my previous patch is no good. Need a more complicated patch, unfortunately.

This patch updates LOUnion.getSchema() to ensure that all output schema fields have a uid set. It also makes some small changes to minimize calls to getSchema() on inputs, since several additional iterations over inputs have been added, and getScema() calls can be quite costly (for example, PigStorage reloads from HDFS every time getSchema() is called).

Also moved the test for this issue into TestUnionOnSchema.",21/Jun/14 04:47;daijy;PIG-4018-2.patch looks pretty good. I will +1 once tests pass.,"22/Jun/14 03:11;daijy;All unit tests pass.

Patch committed to trunk. Thanks Travis!",,,,,,,,,,,,,,,,,,,,,,
NPE thrown from JobControlCompiler.shipToHdfs,PIG-4017,12721801,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,cheolsoo,cheolsoo,cheolsoo,17/Jun/14 22:29,07/Jul/14 18:07,14/Mar/19 03:08,17/Jun/14 23:15,,,,,,0.13.0,0.14.0,,,,0,,,,,,,"I ran into this NPE while running e2e Native tests with release source tarball. When the file that Pig tries to ship to hdfs doesn't exist, it fails with the following error-
{code}
Caused by: java.lang.NullPointerException
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.shipToHDFS(JobControlCompiler.java:1707)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.putJarOnClassPathThroughDistributedCache(JobControlCompiler.java:1612)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:605)
    ... 19 more
{code}
The problem is that this NPE covers up the root cause.",,,,,,,,,,,,,,,,,,,,17/Jun/14 22:34;cheolsoo;PIG-4701-1.patch;https://issues.apache.org/jira/secure/attachment/12650910/PIG-4701-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-17 23:05:05.615,,,no_permission,,,,,,,,,,,,,399997,,,,,Tue Jun 17 23:15:36 UTC 2014,,,,,,,0|i1wuvz:,400100,,,,,,,,,,17/Jun/14 22:34;cheolsoo;Added a null check.,17/Jun/14 23:05;daijy;+1,17/Jun/14 23:15;cheolsoo;Committed to 0.13 and trunk. Thank you Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Rank e2e test failures on tez,PIG-4014,12721225,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Jun/14 01:02,21/Nov/14 05:59,14/Mar/19 03:08,17/Jun/14 18:24,,,,,,0.14.0,,tez,,,0,,,,,,,Rank e2e tests all fail once we enable multiple parallelism (part of PIG-3846).,,,,,,,,,,,,,,,,,,,,14/Jun/14 01:04;daijy;PIG-4014-1.patch;https://issues.apache.org/jira/secure/attachment/12650426/PIG-4014-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-15 01:56:31.808,,,no_permission,,,,,,,,,,,,,399421,Reviewed,,,,Tue Jun 17 18:24:30 UTC 2014,,,,,,,0|i1wrdj:,399530,,,,,,,,,,15/Jun/14 01:56;rohini;Previously isRowNumber() was calling incrementLocalCounter() and that was doing totalTaskRecords++ and now same is being done with incrementReduceCounter(). Don't see the difference. Can you explain the change and the actual cause of failure? ,"15/Jun/14 02:25;daijy;addCounterValue will invoke incrementReduceCounter and addToLocalCounter, so for every record, totalTaskRecords will be incremented by 2, thus we get the wrong #records.",15/Jun/14 16:37;rohini;Ah. Got it. Problem is with dense rank. +1,17/Jun/14 18:24;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
Order by multiple column fail on Tez,PIG-4013,12721224,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Jun/14 00:59,21/Nov/14 05:58,14/Mar/19 03:08,17/Jun/14 18:22,,,,,,0.14.0,,tez,,,0,,,,,,,Order by multiple column get the wrong result (data is not sorted correctly). This is due to comparator in WeightedRangePartitioner is wrong. Unfortunately existing e2e tests does not capture it. Upload a patch with a new e2e test case.,,,,,,,,,,,,,,,,,,,,14/Jun/14 01:00;daijy;PIG-4013-1.patch;https://issues.apache.org/jira/secure/attachment/12650425/PIG-4013-1.patch,15/Jun/14 02:39;daijy;PIG-4013-2.patch;https://issues.apache.org/jira/secure/attachment/12650464/PIG-4013-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-06-15 01:38:32.461,,,no_permission,,,,,,,,,,,,,399420,Reviewed,,,,Tue Jun 17 18:22:39 UTC 2014,,,,,,,0|i1wrdb:,399529,,,,,,,,,,15/Jun/14 01:38;rohini;Looks good. But can use the utility method org.apache.tez.runtime.library.common.ConfigUtils.getIntermediateInputKeyComparator(),"15/Jun/14 02:39;daijy;Yes, that's much better. Updated the patch.",15/Jun/14 16:01;rohini;+1,17/Jun/14 18:22;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.IllegalArgumentException: Comparison method violates its general contract! SpillableMemoryManager,PIG-4012,12721113,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,ddreyfus,ddreyfus,13/Jun/14 13:53,23/May/15 18:33,14/Mar/19 03:08,16/Jun/14 12:50,0.12.0,,,,,0.15.0,,impl,,,0,,,,,,,"java.lang.IllegalArgumentException: Comparison method violates its general contract!
        at java.util.TimSort.mergeHi(TimSort.java:868)
        at java.util.TimSort.mergeAt(TimSort.java:485)
        at java.util.TimSort.mergeForceCollapse(TimSort.java:426)
        at java.util.TimSort.sort(TimSort.java:223)
        at java.util.TimSort.sort(TimSort.java:173)
        at java.util.Arrays.sort(Arrays.java:659)
        at java.util.Collections.sort(Collections.java:217)
        at org.apache.pig.impl.util.SpillableMemoryManager.handleNotification(SpillableMemoryManager.java:199)
        at sun.management.NotificationEmitterSupport.sendNotification(NotificationEmitterSupport.java:156)
        at sun.management.MemoryImpl.createNotification(MemoryImpl.java:168)
        at sun.management.MemoryPoolImpl$PoolSensor.triggerAction(MemoryPoolImpl.java:301)
        at sun.management.Sensor.trigger(Sensor.java:137)

From SpillableMemoryManager.java:
                /**
                 * We don't lock anything, so this sort may not be stable if a WeakReference suddenly
                 * becomes null, but it will be close enough.
                 * Also between the time we sort and we use these spillables, they
                 * may actually change in size - so this is just best effort
                 */
Issue may be due to Java 7 and reporting vs ignoring the exception.
Trying      
-Djava.util.Arrays.useLegacyMergeSort=true

http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6804124
suggests the newer MergeSort is much faster.

Someone may want to make the sorting stable in SpillableMemoryManager so that the new merge sort can be used without failure.","java version ""1.7.0_60-ea""
Java(TM) SE Runtime Environment (build 1.7.0_60-ea-b02)
Java HotSpot(TM) 64-Bit Server VM (build 24.60-b04, mixed mode)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-10-15 22:19:32.962,,,no_permission,,,,,,,,,,,,,399310,,,,,Sat May 23 18:33:24 UTC 2015,,,,,,,0|i1wqpj:,399420,,,,,,,,,,16/Jun/14 12:48;ddreyfus;Addressed this issue in PIG-3979.,16/Jun/14 12:50;ddreyfus;Combined this issue with PIG-3979.,15/Oct/14 22:19;rohini;This should just be a temporary exception right? Looking at java source code it just seems to log the message if there is an Exception from handleNotifications(). Are you saying that it actually fails the job for you.  I would expect the next handleNotifications call will go through and spill. We have been running with jdk 7 for 1.5+ years now and have never seen a job fail with this exception. ,16/Oct/14 15:24;ddreyfus;I don't recall if the job fails or not. I don't think we should have code that generates exceptions that are easily fixable.,"16/Oct/14 20:32;rohini;When you have lot of objects (which is usually the case), creating another list and copying over the Spillable to another list adds both memory and time penalty. And you especially do not want to create another big list when you are handling a notification for high memory usage. The comment in the code already calls out that the sort is not stable and is only a best effort.

{code}
/**
                 * We don't lock anything, so this sort may not be stable if a WeakReference suddenly
                 * becomes null, but it will be close enough.
                 * Also between the time we sort and we use these spillables, they
                 * may actually change in size - so this is just best effort
                 */  
{code}

Unless the job fails due to the error, I don't see a need to create another copy of list. The next handleNotifications() should go through.","17/Oct/14 18:05;ddreyfus;The issue has nothing to do with unstable sorting or the fact that spillables change in size.
Earlier versions of Java gracefully handled non stable sorting. Current versions do not - generating the exception.
The issue is the exception that is generated. I started looking at the sun memory management code. I didn't see anything that silently logs and ignores exceptions, so I'll assume the job fails on this exception. Even if the exception is caught (a big if), no memory reduction will take place when the exception does occur.

If the whole purpose of the sorting is to identify a few spillables to spill so as to minimize the performance impact of spilling everything, and you are concerned about the time and memory it takes to copy the list of spillables (not their content, obviously), you might also be concerned about the nlog(n) time it takes to sort the list. Perhaps this should be addressed:
1) A configuration option that just causes Pig to spill everything if the number of spillables is above a threshold.
2) A minimum spill size such that on notification we spill everything spillable with estimated size greater than this threshold.
In both of these solutions sorting can be eliminated. It might be ideal for those use cases were the size of the spillables list dominates memory usage.

Another approach would be to sort without copying, capture the exception, and either keep on retrying, or then copy and sort. My guess is this would ultimately be slower in those cases where the exception does occur.

My guess is that the memory consumed by the list is tiny relative to the size of the spillables and the spilling we are trying to avoid, but my vision is limited by my use cases.","07/May/15 15:57;rohini;Saw this exception in a job recently and spent some time looking into it. The job does not fail and the SpillableMemoryManager runs fine when JVM calls the handleNotification next time. Agree that this can be fixed so that the SpillableMemoryManager runs the first time and frees up memory early.

The actual problem is with the below piece of code in the Comparator.  

{code}
                 if (o1 == null && o2 == null) {
                        return 0;
                    }
                   if (o1 == null) {
                        return 1;
                    }
                    if (o2 == null) {
                        return -1;
                    }
{code}

References: 
http://stackoverflow.com/questions/6626437/why-does-my-compare-method-throw-exception-comparison-method-violates-its-gen
http://dertompson.com/2012/11/23/sort-algorithm-changes-in-java-7/
https://docs.oracle.com/javase/7/docs/api/java/lang/Comparable.html

Half way through sorting if o1 or o2 become null due to garbage collection, o1.compareTo(o2) and o2.compareTo(o1) return different results and violates the Comparable contract. [~ddreyfus] idea of taking the Spillable out of WeakReference and copying it to a new list and sorting the new list (https://issues.apache.org/jira/secure/attachment/12674030/PIG-3979-3.patch) would work. But that patch still has same null checks for Spillable in the comparator and would run into same exception mentioned in this jira. The comparator will have to change to only do size check (SpillablePtr has the size as well) and no null checks at all.  A new LinkedList might add to memory pressure in case there are lot of bag of bags, but avoiding the getMemorySize() call multiple times should offset that.

[~ddreyfus],
     Do you want to take it up as changes to the Comparator to only compare sizes will be on top of your old patch?
     
This jira is marked as Closed which should not be the case. I somehow can't find a way to Re-open this one even though I changed the Fix Version to 0.16 from 0.13. ","07/May/15 19:19;rohini;bq. I somehow can't find a way to Re-open this one
  This is because https://issues.apache.org/jira/plugins/servlet/project-config/PIG/workflows has no-reopen-closed and that seems to be the default configuration for other projects as well and something that cannot be modified by Project Administrators.  Since there is lot of history here not creating a duplicate one. Will leave it with the version change and ensure it actually gets fixed soon in trunk.",23/May/15 18:33;rohini;  Made this patch (removing the null checks in the Comparator) as part of PIG-4564.,,,,,,,,,,,,,,,,,,,
Pig should not put PigContext in job.jar to help jar dedup,PIG-4011,12721009,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,aniket486,aniket486,12/Jun/14 23:28,21/Nov/14 05:58,14/Mar/19 03:08,02/Oct/14 00:03,0.13.0,,,,,0.14.0,,,,,0,,,,,,,"Job jars are largely identical from job to job (just the pig classes and their dependencies). However, there is pigContext in the job jar, and that seems to change from job to job.As a result, the job jars are basically uncacheable in the shared cache. It would be great if we can find a way to separate the pig context and store it in the distributed cache separate from the (stable) job jar. That would give us a better chance to store the job jar.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-06-12 23:30:02.36,,,no_permission,,,,,,,,,,,,,399207,,,,,Thu Oct 02 00:03:43 UTC 2014,,,,,,,0|i1wq3j:,399320,,,,,,,,,,12/Jun/14 23:30;rohini;Any idea why PigContext differs in each jar?,"20/Jun/14 01:40;daijy;This is because pigContext.getScriptFiles() also goes to job.jar. That's also cause problem for tez, we won't be able to reuse session if the script files are different.",11/Jul/14 18:36;daijy;Actually I misread the description. The work I mentioned is more than the scope of the Jira. Open PIG-4054 for that and this Jira is a part of it.,02/Oct/14 00:03;daijy;This is solved as part of PIG-4054.,,,,,,,,,,,,,,,,,,,,,,,,
Error is thrown by JobStats.getOutputSize() when storing to a Hive table ,PIG-4003,12720213,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,09/Jun/14 21:52,21/Nov/14 05:59,14/Mar/19 03:08,24/Jun/14 21:55,,,,,,0.14.0,,,,,0,,,,,,,"Here is an example of stack trace printed to console output. Technically, this is a warning message and does not make the job fail. However, this is certainly not user-friendly.
{code}
4/06/09 16:20:28 WARN pigstats.JobStats: unable to find the output file
java.io.FileNotFoundException: File hdfs://10.61.10.185:9000/user/cheolsoop/prodhive.benchmark.unittest_vhs_bitrate_asn_sum_stg_test2 does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:654)
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)
	at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:708)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:708)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader.getOutputSize(FileBasedOutputSizeReader.java:65)
	at org.apache.pig.tools.pigstats.JobStats.getOutputSize(JobStats.java:352)
{code}
The issue is that FileBasedOutputSizeReader mis-interprets hive table name as hdfs path.
{code}
@Override
public boolean supports(POStore sto, Configuration conf) {
    return UriUtil.isHDFSFileOrLocalOrS3N(getLocationUri(sto), conf);
}
{code}",,,,,,,,,,,,,,,,,,,,09/Jun/14 22:01;cheolsoo;PIG-4003-1.patch;https://issues.apache.org/jira/secure/attachment/12649467/PIG-4003-1.patch,10/Jun/14 15:44;cheolsoo;PIG-4003-2.patch;https://issues.apache.org/jira/secure/attachment/12649597/PIG-4003-2.patch,24/Jun/14 20:09;cheolsoo;PIG-4003-3.patch;https://issues.apache.org/jira/secure/attachment/12652260/PIG-4003-3.patch,24/Jun/14 20:40;cheolsoo;PIG-4003-4.patch;https://issues.apache.org/jira/secure/attachment/12652268/PIG-4003-4.patch,24/Jun/14 21:26;cheolsoo;PIG-4003-5.patch;https://issues.apache.org/jira/secure/attachment/12652285/PIG-4003-5.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-06-24 20:21:58.518,,,no_permission,,,,,,,,,,,,,398412,,,,,Tue Jun 24 21:55:50 UTC 2014,,,,,,,0|i1wlbb:,398538,,,,,,,,,,"09/Jun/14 22:01;cheolsoo;The attached patch addresses two issues-
# MRJobStats.addOutputStatistics() has redundant code. It handles the case of {{# of stores == 1}} separately, but it is not only unnecessary but also confusing since it adds an extra code path.
# Make FileBasedOutputSizeReader.support() return false for hive table names. If there is no scheme in uri, assumes it is not a hdfs path. ","10/Jun/14 15:44;cheolsoo;I reverted #1 due to regression- 10 unit test cases fail.

I also found a better way of doing #2. Now {{HadoopShims.hasFileSystemImpl()}} returns false when schema is null. Fixed in both 20 and 23 shims.","24/Jun/14 20:09;cheolsoo;Discussed with Rohini offline.
# We should keep {{HadoopShims.hasFileSystemImpl()}} as it. i.e. when no scheme is given, we should assume it's a hdfs file following hadoop convention.
# Instead, I introduced a new property {{pig.stats.output.size.reader.unsupported}} via which certain store funcs can be excluded.

Attaching the patch.","24/Jun/14 20:21;rohini;Patch looks good. Can you add the property to pig-default.properties and have org.apache.hcatalog.pig.HCatStorer,org.apache.hive.hcatalog.pig.HCatStorer  as default values as they are well know ones and also add a unit test.",24/Jun/14 20:40;cheolsoo;Incorporated Rohini's comments.,24/Jun/14 20:50;rohini;Can you add a unit test to TestMRJobStats? We can just pass PigStorage in the unsupported list and verify that the output size in jobstats is -1. ,24/Jun/14 21:26;cheolsoo;Of course. Added a test case as suggested.,24/Jun/14 21:39;rohini;Thanks Cheolsoo. +1,24/Jun/14 21:55;cheolsoo;Committed to trunk. Thank you Rohini for reviewing the patch!,,,,,,,,,,,,,,,,,,,
Minor documentation fix for PIG-3642,PIG-4000,12719181,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,lbendig,lbendig,lbendig,09/Jun/14 10:06,07/Jul/14 18:08,14/Mar/19 03:08,09/Jun/14 16:08,,,,,,0.13.0,0.14.0,documentation,,,0,,,,,,,"The root cause of the issue described in PIG-3886 is that direct fetch doesn't support udfs that interact with the distributed cache.
This should be documented so that users can easier track down such issues.",,,,,,,,,,,,,,,,,,,,09/Jun/14 10:07;lbendig;PIG-4000.patch;https://issues.apache.org/jira/secure/attachment/12648942/PIG-4000.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-09 16:08:22.877,,,no_permission,,,,,,,,,,,,,397380,,,,,Mon Jun 09 16:08:22 UTC 2014,,,,,,,0|i1wey7:,397507,,,,,,,,,,09/Jun/14 10:07;lbendig;Is it possible to have this patch in 0.13.0?,"09/Jun/14 16:08;cheolsoo;+1. Committed to branch-0.13 and trunk.

Thank you Lorand!",,,,,,,,,,,,,,,,,,,,,,,,,,
Document PIG-3388,PIG-3999,12719174,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,lbendig,lbendig,lbendig,09/Jun/14 10:01,07/Jul/14 18:07,14/Mar/19 03:08,09/Jun/14 16:12,,,,,,0.13.0,0.14.0,documentation,,,0,,,,,,,"PIG-3388 added the {{-regex}} option to HBaseStorage, this should be documented",,,,,,,,,,,,,,,,,,,,09/Jun/14 10:03;lbendig;PIG-3999.patch;https://issues.apache.org/jira/secure/attachment/12648939/PIG-3999.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-09 16:12:46.311,,,no_permission,,,,,,,,,,,,,397373,,,,,Mon Jun 09 16:12:46 UTC 2014,,,,,,,0|i1wewn:,397500,,,,,,,,,,09/Jun/14 10:03;lbendig;Is it possible to have this patch in 0.13.0?,"09/Jun/14 16:12;cheolsoo;+1. Committed to 0.13 and trunk.

Thank you Lorand!",,,,,,,,,,,,,,,,,,,,,,,,,,
"Documentation fix: invalid page links, wrong Groovy udf example",PIG-3998,12719172,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,lbendig,lbendig,lbendig,09/Jun/14 09:58,07/Jul/14 18:07,14/Mar/19 03:08,09/Jun/14 16:26,0.12.0,,,,,0.13.0,0.14.0,documentation,,,0,,,,,,,"Many internal page links are not working due to the case-sensitive nature of the fragement identifiers. Aditionally, there's a small typo in the Groovy udf example.",,,,,,,,,,,,,,,,,,,,09/Jun/14 10:00;lbendig;PIG-3998.patch;https://issues.apache.org/jira/secure/attachment/12648938/PIG-3998.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-09 16:26:01.883,,,no_permission,,,,,,,,,,,,,397371,,,,,Mon Jun 09 16:38:40 UTC 2014,,,,,,,0|i1wew7:,397498,,,,,,,,,,09/Jun/14 10:00;lbendig;Is it possible to have this patch in 0.13.0?,"09/Jun/14 16:26;cheolsoo;+1. Committed to 0.13 and trunk.

Thank you Lorand!","09/Jun/14 16:38;lbendig;Cheolsoo, thanks for the commits!",,,,,,,,,,,,,,,,,,,,,,,,,
Issue on Pig docs ( Testing and Diagnostics ),PIG-3997,12719125,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,zjffdu,zjffdu,zjffdu,09/Jun/14 07:23,07/Jul/14 18:07,14/Mar/19 03:08,09/Jun/14 16:34,,,,,,0.13.0,0.14.0,documentation,,,0,,,,,,,Bugs on the document of Testing and Diagnostics,,,,,,,,,,,,,,,,,,,,09/Jun/14 07:24;zjffdu;PIG_3997.patch;https://issues.apache.org/jira/secure/attachment/12648926/PIG_3997.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-09 16:34:12.931,,,no_permission,,,,,,,,,,,,,397324,,,,,Mon Jun 09 16:34:12 UTC 2014,,,,,,,0|i1welr:,397451,,,,,,,,,,"09/Jun/14 16:34;cheolsoo;+1. Committed to 0.13 and trunk. Thank you Jeff.

One minor typo fix that I made when committing your patch-
{code}
< +A: {name: chararray,age: int,gpa: float}
---
> +B: {name: chararray,age: int,gpa: float}
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete zebra from svn,PIG-3996,12719109,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,09/Jun/14 05:07,26/Sep/14 16:09,14/Mar/19 03:08,09/Jun/14 05:31,,,,,,0.13.0,0.14.0,,,,0,,,,,,,Zebra has been deprecated for a while. Let's delete dead code!,,,,,,,,,,,,,,,,,,,,09/Jun/14 05:18;cheolsoo;PIG-3996-1.patch;https://issues.apache.org/jira/secure/attachment/12648914/PIG-3996-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-09 05:12:45.039,,,no_permission,,,,,,,,,,,,,397308,,,,,Fri Sep 26 16:09:33 UTC 2014,,,,,,,0|i1wei7:,397435,Zebra is deleted in 0.13.,,,,,,,,,09/Jun/14 05:12;daijy;+1,"09/Jun/14 05:18;cheolsoo;In addition to svn delete {{contrib/zebra}}, I am also removing zebra-related code in {{build.xml}} and {{test/e2e/pig/drivers/TestDriverScript.pm}}.",09/Jun/14 05:31;cheolsoo;Committed to trunk and branch-0.13.,"25/Sep/14 18:55;kamboj;Is there any specific reason that zebra was deprecated from pig 0.13 onwards? Is there a separate code branch that is maintained currently? I have couple of performance optimization changes in zebra, so wonder whether I can contribute them.","25/Sep/14 21:34;daijy;[~kamboj], thank you for your interest, but Pig team no longer provide support for Zebra.","25/Sep/14 21:38;kamboj;Thanks Daniel! Is zebra code not maintained even in some other independent repo now, I mean whatever is in Pig 0.12, is latest for zebra?","25/Sep/14 21:41;daijy;Yes, it has not been changed for a while. Zebra code in Pig 0.12.1 release should be the latest.",26/Sep/14 16:09;kamboj;Thanks Daniel for the information!,,,,,,,,,,,,,,,,,,,,
Tez unit tests shouldn't run when -Dhadoopversion=20,PIG-3995,12719090,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,08/Jun/14 22:33,21/Nov/14 05:58,14/Mar/19 03:08,09/Jun/14 19:34,,,,,,0.14.0,,,,,0,,,,,,,"After merging tez branch, {{ant test -Dhadoopversion=20}} fails due to tez unit tests. They should only run when {{-Dhadoopversion=23}}.",,,,,,,,,,,,,,,,,,,,08/Jun/14 22:37;cheolsoo;PIG-3995-1.patch;https://issues.apache.org/jira/secure/attachment/12648895/PIG-3995-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-09 01:49:00.906,,,no_permission,,,,,,,,,,,,,397289,,,,,Mon Jun 09 19:34:17 UTC 2014,,,,,,,0|i1wedz:,397416,,,,,,,,,,08/Jun/14 22:37;cheolsoo;Added {{excluded-tests-20}} that contains tez unit test files.,09/Jun/14 01:49;daijy;+1,09/Jun/14 19:34;cheolsoo;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,
TestErrorHandling.tesNegative7 is broken in trunk/branch-0.13,PIG-3991,12719037,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,07/Jun/14 23:17,07/Jul/14 18:08,14/Mar/19 03:08,08/Jun/14 00:50,,,,,,0.13.0,0.14.0,,,,0,,,,,,,"To reproduce, run {{ant clean test -Dtestcase=TestErrorHandling}}. It fails with an assertion failure.",,,,,,,,,,,,,,,,,,,,07/Jun/14 23:18;cheolsoo;PIG-3991-1.patch;https://issues.apache.org/jira/secure/attachment/12648839/PIG-3991-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-08 00:29:11.884,,,no_permission,,,,,,,,,,,,,397236,,,,,Sun Jun 08 00:50:02 UTC 2014,,,,,,,0|i1we27:,397363,,,,,,,,,,07/Jun/14 23:18;cheolsoo;Uploading a fix.,08/Jun/14 00:29;daijy;＋1， thanks [~cheolsoo]!,08/Jun/14 00:50;cheolsoo;Committed to branch 0.13 and trunk. Thank you Daniel for quick review!,,,,,,,,,,,,,,,,,,,,,,,,,
ant docs is broken in trunk/branch-0.13,PIG-3990,12719036,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,07/Jun/14 23:06,07/Jul/14 18:07,14/Mar/19 03:08,08/Jun/14 00:47,,,,,,0.13.0,0.14.0,documentation,,,0,,,,,,,"To reproduce, run {{ant docs}}. It fails with errors.",,,,,,,,,,,,,,,,,,,,07/Jun/14 23:09;cheolsoo;PIG-3990-1.patch;https://issues.apache.org/jira/secure/attachment/12648837/PIG-3990-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-08 00:29:25.551,,,no_permission,,,,,,,,,,,,,397235,,,,,Sun Jun 08 00:47:48 UTC 2014,,,,,,,0|i1we1z:,397362,,,,,,,,,,07/Jun/14 23:09;cheolsoo;Uploading a patch.,08/Jun/14 00:29;daijy;+1,08/Jun/14 00:47;cheolsoo;Committed to trunk and branch-0.13. Thank you Daniel for quick review!,,,,,,,,,,,,,,,,,,,,,,,,,
PIG_OPTS does not work with some version of HADOOP,PIG-3989,12718976,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,06/Jun/14 23:47,07/Jul/14 18:08,14/Mar/19 03:08,06/Jun/14 23:53,,,,,,0.13.0,,impl,,,0,,,,,,,"Some version of Hadoop does not take HADOOP_OPTS, it only takes HADOOP_CLIENT_OPTS. That's why Accumulator_5/UdfDistributedCache_1 fail.",,,,,,,,,,,,,,,,,,,,06/Jun/14 23:49;daijy;PIG-3989-1.patch;https://issues.apache.org/jira/secure/attachment/12648760/PIG-3989-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,397175,Reviewed,,,,Fri Jun 06 23:52:56 UTC 2014,,,,,,,0|i1wdnz:,397298,,,,,,,,,,06/Jun/14 23:52;daijy;Already get +1 from Cheolsoo in PIG-3886. Patch committed to both trunk and 0.13 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorage: CommandLineParser is not thread safe,PIG-3988,12718882,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,tmwoodruff,tmwoodruff,tmwoodruff,06/Jun/14 16:55,21/Nov/14 05:58,14/Mar/19 03:08,11/Jun/14 19:27,0.13.0,,,,,0.14.0,,,,,0,,,,,,,"PigStorage uses commons-cli to parse options. The CommandLineParser is stored as a static member. This can cause issues when two threads instantiate PigStorage at the same time, since CommandLineParser stores state in a non-thread-safe way.

OptionBuilder, used in populateValidOptions(), is also not thread safe, as it stores its state in static members.",,,,,,,,,,,,,,,,,,,,06/Jun/14 16:59;tmwoodruff;PIG-3988-1.patch;https://issues.apache.org/jira/secure/attachment/12648659/PIG-3988-1.patch,11/Jun/14 18:51;tmwoodruff;PIG-3988-2.patch;https://issues.apache.org/jira/secure/attachment/12649848/PIG-3988-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-06-07 23:49:11.002,,,no_permission,,,,,,,,,,,,,397081,,,,,Wed Jun 11 19:27:59 UTC 2014,,,,,,,0|i1wd33:,397204,,,,,,,,,,"06/Jun/14 17:40;tmwoodruff;This patch moves the option parsing members to local variables in the constructor, since that's the only place they're used. Since the CommandLineParser is no longer static, access by multiple threads is no longer an issue.

It also removes use of the OptionBuilder class and configures the Option via setters instead.
","07/Jun/14 23:49;cheolsoo;+1. Thank you for the patch!

[~tmwoodruff], do you mind committing this patch into 0.14 instead of 0.13? Given we're in the middle of release process, I'd like to avoid any commit into 0.13 except test failures and critical bug fixes.",08/Jun/14 00:53;cheolsoo;Committed to trunk.,08/Jun/14 02:56;tmwoodruff;0.14 is fine. Thanks.,"11/Jun/14 17:47;rohini;Encountered this exception in line Option overwrite = new Option(null, ""Overwrites the destination.""); in PigStorage.java while running from eclipse. It was because commons-cli-1.0.jar was in my classpath.  Passes while running TestPigStorage though as it has commons-cli-1.2.jar which was only recently upgraded in pig.

{code}
Caused by: java.lang.IllegalArgumentException: opt is null
	at org.apache.commons.cli.Option.validateOption(Option.java:159)
	at org.apache.commons.cli.Option.<init>(Option.java:250)
	at org.apache.commons.cli.Option.<init>(Option.java:222)
	at org.apache.pig.builtin.PigStorage.populateValidOptions(PigStorage.java:167)
{code}

What is the reason behind changing 
{code}
Option overwrite = OptionBuilder.hasOptionalArgs(1).withArgName(""overwrite"").withLongOpt(""overwrite"").withDescription(""Overwrites the destination."").create();
{code}

to 

{code}
Option overwrite = new Option(null, ""Overwrites the destination."");
        overwrite.setLongOpt(""overwrite"");
        overwrite.setOptionalArg(true);
        overwrite.setArgs(1);
        overwrite.setArgName(""overwrite"");
{code}

I don't see a difference. We should revert back so that it is backward compatible with commons-cli-1.0.jar as well","11/Jun/14 18:51;tmwoodruff;Looks like commons-cli 1.0 passes "" "" instead of null to the Option constructor in OptionBuilder. 

Attaching PIG-3988-2.patch. This is a patch on trunk (assumes no revert of previous patch).

Tested and working on both commons-cli 1.0 and 1.2.","11/Jun/14 18:53;tmwoodruff;Reason behind change is that OptionBuilder is not thread safe (CLI-209). We encounter sporadic exceptions due to this.

","11/Jun/14 19:07;rohini;> Reason behind change is that OptionBuilder is not thread safe
  Thanks for the clarification. My mistake as I din't pay attention to the jira description. Will commit the change. ",11/Jun/14 19:27;rohini;Committed PIG-3988-2.patch which fixes the IllegalArgumentException to trunk. Thanks Travis.,,,,,,,,,,,,,,,,,,,
"Multiquery execution of RANK with RANK BY causes NPE JobCreationException ""ERROR 2017: Internal error creating job configuration""",PIG-3985,12718048,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,mrflip,mrflip,03/Jun/14 02:10,21/Nov/14 05:58,14/Mar/19 03:08,02/Oct/14 17:15,,,,,,0.14.0,,,,,0,nullpointerexception,rank,udf,,,,"A script with both RANK and RANK BY will crash with a Null Pointer Exception in JobControlCompiler.java when multiquery is enabled.

The following script will work for any combination of the RANK BY operations; or if there is one RANK operation only (i.e. no other RANK or RANK BY operation). Non-BY-RANKS will perish together but succeed alone.

Disabling multiquery execution makes everything work again.

I am using Hadoop 2.4.0 with Pig Trunk (d24d06a48, after PIG-3739). The error occurs in local or mapreduce mode.

{code}
-- disable multiquery and you can rank all day long
-- SET opt.multiquery false

citypops = LOAD 'us_city_pops.tsv' AS (city:chararray, state:chararray, pop_2011:int);
citypops_o = ORDER citypops BY city;

--
-- if you have one non-by RANK you may not have any other RANKs
--

citypops_nosort_inplace    = RANK citypops;
citypops_presorted_inplace = RANK citypops_o;
citypops_ties_cause_skips  = RANK citypops   BY city;
citypops_ties_no_skips     = RANK citypops   BY city  DENSE;
citypops_presorted_ranked  = RANK citypops_o BY city;

STORE citypops_nosort_inplace    INTO '/tmp/citypops_nosort_inplace'    USING PigStorage('\t', '--overwrite true');
-- STORE citypops_presorted_inplace INTO '/tmp/citypops_presorted_inplace' USING PigStorage('\t', '--overwrite true');

STORE citypops_ties_cause_skips  INTO '/tmp/citypops_ties_cause_skips'  USING PigStorage('\t', '--overwrite true');
-- STORE citypops_ties_no_skips     INTO '/tmp/citypops_ties_no_skips'     USING PigStorage('\t', '--overwrite true');
-- STORE citypops_presorted_ranked  INTO '/tmp/citypops_presorted_ranked'  USING PigStorage('\t', '--overwrite true');
{code}

{code}
Pig Stack Trace
---------------
ERROR 2017: Internal error creating job configuration.

org.apache.pig.backend.hadoop.executionengine.JobCreationException: ERROR 2017: Internal error creating job configuration.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:946)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:322)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:200)
     --- SNIP ----
Caused by: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:886)
        ... 19 more
{code}

The proximate offense seems to be that globalCounters.get(operationID) returns null:

{code}
            if(mro.isRankOperation()) {
                Iterator<String> operationIDs = mro.getRankOperationId().iterator();

                while(operationIDs.hasNext()) {
                    String operationID = operationIDs.next();
                    Iterator<Pair<String, Long>> itPairs = globalCounters.get(operationID).iterator();
                    Pair<String,Long> pair = null;
                    while(itPairs.hasNext()) {
                        pair = itPairs.next();
                        conf.setLong(pair.first, pair.second);
                    }
                }
            }
{code}

PORank.java line 184 seems to need a counter value, and so this part does need to happen.
",,,,,,,,,,,,,,,,,,,,02/Oct/14 15:35;rohini;PIG-3985-2.patch;https://issues.apache.org/jira/secure/attachment/12672563/PIG-3985-2.patch,03/Jun/14 02:51;mrflip;many_ranks_much_sadness.pig;https://issues.apache.org/jira/secure/attachment/12648058/many_ranks_much_sadness.pig,01/Oct/14 17:39;knoguchi;pig-3985-v01.txt;https://issues.apache.org/jira/secure/attachment/12672363/pig-3985-v01.txt,03/Jun/14 02:51;mrflip;us_city_pops.tsv;https://issues.apache.org/jira/secure/attachment/12648059/us_city_pops.tsv,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-10-01 17:39:49.701,,,no_permission,,,,,,,,,,,,,396250,Reviewed,,,,Thu Oct 02 17:15:22 UTC 2014,,,,,,,0|i1w7yv:,396372,Multiquery execution of RANK with RANK BY causes NPE,,,,,,,,,03/Jun/14 02:51;mrflip;Added script and sample data.,"01/Oct/14 17:39;knoguchi;{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:938)
{noformat}
{code:title=JobControlCompiler.java|borderStyle=solid}
 938                     Iterator<Pair<String, Long>> itPairs = globalCounters.get(operationID).iterator();
{code}
This was due to globalCounters not containing operationID.
This itself was caused by saveCounters not being called due to mro.isCounterOperation incorrectly returning false.
{code:title=JobControlCompiler.java|borderStyle=solid}
 358                 if (!pigContext.inIllustrator && mro.isCounterOperation())
 359                     saveCounters(job,mro.getOperationID());
{code}

This was caused by  mro.isCounterOperation assuming that POCount is always placed at the leaf level.
{code:title=MapReduceOper.java|borderStyle=solid}
511     public boolean isCounterOperation() {
512         return (getCounterOperation() != null);
513     }
...
525     private POCounter getCounterOperation() {
526         PhysicalOperator operator;
527         Iterator<PhysicalOperator> it =  this.mapPlan.getLeaves().iterator();
528
529         while(it.hasNext()) {
530             operator = it.next();
531             if(operator instanceof POCounter)
532                 return (POCounter) operator;
533         }
...
{code}

For the sample pig test program given by Philip, mapreduce plan showed ""SPLIT"" as the only leaf.
{noformat}
MapReduce node scope-34
Map Plan
Split - scope-69
|   |
|   Store(file:/tmp/temp465448860/tmp1018450824:org.apache.pig.impl.io.InterStorage) - scope-38
|   |
|   |---citypops_nosort_inplace: POCounter[tuple] - scope-14
|   |
|   citypops_ties_cause_skips: Local Rearrange[tuple]{chararray}(false) - scope-21
|   |   |
|   |   Project[chararray][0] - scope-22
|
|---citypops: New For Each(false,false,false)[bag] - scope-10
    |   |
    |   Cast[chararray] - scope-2
    |   |
    |   |---Project[bytearray][0] - scope-1
    |   |
    |   Cast[chararray] - scope-5
    |   |
    |   |---Project[bytearray][1] - scope-4
    |   |
    |   Cast[int] - scope-8
    |   |
    |   |---Project[bytearray][2] - scope-7
    |
    |---citypops: Load(file:///Users/knoguchi/git/pig/pig-3985/us_city_pops.tsv:org.apache.pig.builtin.PigStorage) - scope-0--------
{noformat}

I initially tried fixing MapReduceOper.getCounterOperation() so that it'll find the POCounter even if it's part of the split.  However, I soon learned that POCount requires different map-reduce class (PigMapReduceCounter.PigMapCounter.class and PigReduceCounter.class) and it currently doesn't work if they are mixed with other operations.

Instead of rewriting Rank, for now made a change so that all POCount starts a new mapreduce job.","02/Oct/14 15:35;rohini;[~knoguchi],
     Creating a new job every time will impact performance. Fixed this in a different way by not merging the splits if it is a counter operation. Can you review the patch?","02/Oct/14 16:26;knoguchi;bq. Creating a new job every time will impact performance. Fixed this in a different way by not merging the splits if it is a counter operation. Can you review the patch?

Looks good to me.  I should have spent time at MultiQueryOptimizer.java myself.  Thanks~.
",02/Oct/14 17:15;rohini;Committed to branch-0.14 and trunk. Thanks Koji for the patch and the review. ,,,,,,,,,,,,,,,,,,,,,,,
PigServer.shutdown remove the tez resource folder,PIG-3984,12718045,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,03/Jun/14 01:46,21/Nov/14 05:58,14/Mar/19 03:08,06/Jul/14 20:58,,,,,,0.14.0,,tez,,,0,,,,,,,"PIG-3978 make tez resource folder live across sessions. However, if PigServer.shutdown is called, tez resource folder will be removed and following job will fail. Bunch of tez unit test fail due to this.",,,,,,,,,,,,,,,,,,,,03/Jun/14 04:36;daijy;PIG-3984-1.patch;https://issues.apache.org/jira/secure/attachment/12648071/PIG-3984-1.patch,25/Jun/14 17:12;daijy;PIG-3984-2.patch;https://issues.apache.org/jira/secure/attachment/12652457/PIG-3984-2.patch,06/Jul/14 20:56;rohini;PIG-3984-3.patch;https://issues.apache.org/jira/secure/attachment/12654231/PIG-3984-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-06-23 05:54:45.302,,,no_permission,,,,,,,,,,,,,396247,,,,,Sun Jul 06 20:56:23 UTC 2014,,,,,,,0|i1w7y7:,396369,,,,,,,,,,23/Jun/14 05:19;daijy;bump,"23/Jun/14 05:54;rohini;Shouldn't we clean the resource path up when exiting the grunt shell,end of execution of pig script and termination due to Ctrl+C(ShutdownHook) ?",25/Jun/14 17:12;daijy;Addressing Rohini's review comments.,"06/Jul/14 20:56;rohini;+1. I was trying to get all tez unit tests to passing before uploading patch for PIG-3935. So went ahead and committed to trunk. Thanks Daniel.

I made two minor changes to the patch before committing. Moved FileLocalizer.deleteTempResourceFiles(); inside if(deleteTempFiles) and changed if (resourcePath != null)  to if (resourcePath.get() != null) . Uploaded the final patch as PIG-3984-3.patch",,,,,,,,,,,,,,,,,,,,,,,,
TestGrunt.testKeepGoigFailed fail on tez mode,PIG-3983,12718036,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,03/Jun/14 00:47,21/Nov/14 05:59,14/Mar/19 03:08,17/Jun/14 18:19,,,,,,0.14.0,,tez,,,0,,,,,,,The expected exception is not captured. The same test with MR mode pass.,,,,,,,,,,,,,,,,,,,,16/Jun/14 23:25;daijy;PIG-3983-1.patch;https://issues.apache.org/jira/secure/attachment/12650685/PIG-3983-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-17 17:15:11.596,,,no_permission,,,,,,,,,,,,,396238,Reviewed,,,,Tue Jun 17 18:19:25 UTC 2014,,,,,,,0|i1w7wf:,396361,,,,,,,,,,16/Jun/14 22:45;daijy;StoreFunc.cleanupOnFailure is not called on tez. I will upload a patch shortly.,17/Jun/14 17:15;rohini;+1. I had this as part of PIG-3935 changelist for TestStore and did not realize it affected this test as well.,17/Jun/14 18:19;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
ant target test-tez should depend on jackson-pig-3039-test-download,PIG-3982,12717996,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,daijy,daijy,02/Jun/14 21:20,21/Nov/14 05:58,14/Mar/19 03:08,26/Jul/14 19:42,,,,,,0.14.0,,tez,,,0,,,,,,,Otherwise TestRegisteredJarVisibility fail.,,,,,,,,,,,,,,,,,,,,02/Jun/14 21:22;daijy;PIG-3982-1.patch;https://issues.apache.org/jira/secure/attachment/12648002/PIG-3982-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-04 08:22:42.651,,,no_permission,,,,,,,,,,,,,396198,Reviewed,,,,Sat Jul 26 19:42:03 UTC 2014,,,,,,,0|i1w7nj:,396321,,,,,,,,,,04/Jun/14 08:22;rohini;+1,26/Jul/14 19:42;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Container reuse does not across PigServer,PIG-3978,12717749,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,31/May/14 20:17,21/Nov/14 05:59,14/Mar/19 03:08,02/Jun/14 21:13,,,,,,0.14.0,,tez,,,0,,,,,,,"Container reuse across PigServer does not happen in current code. We use different TezResourceManager per PigServer, and different TezResourceManager will upload LocalResource to different staging directory, so LocalResource is different for different PigServer, thus no container reuse across PigServer. This will affect Pig embedding where multiple PigServer is used for a Pig script. 

The issue is partly due to PIG-3785 which we switch TezResourceManager to non-singleton. The root cause for PIG-3785 is not the singleton issue. The real issue is TezResourceManager.addTezResource is not synchronized, there is a race condition when one PigServer already upload the resource and use it in container, and the other PigServer overwrite at the same time.  ",,,,,,,,,,,,,,,,,,,,31/May/14 20:28;daijy;PIG-3978-1.patch;https://issues.apache.org/jira/secure/attachment/12647784/PIG-3978-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,395953,Reviewed,,,,Mon Jun 02 21:13:23 UTC 2014,,,,,,,0|i1w65z:,396079,,,,,,,,,,02/Jun/14 18:46;daijy;RB link: https://reviews.apache.org/r/22155/,02/Jun/14 21:13;daijy;Patch committed to trunk. Thanks for Rohini for review (review notes is on RB)!,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo correction in JobStats breaks Oozie,PIG-3976,12717678,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,30/May/14 22:54,07/Jul/14 18:07,14/Mar/19 03:08,30/May/14 23:37,,,,,,0.13.0,0.14.0,,,,0,,,,,,,java.lang.NoSuchMethodError: org.apache.pig.tools.pigstats.JobStats.getAvgREduceTime(),,,,,,,,,,,,,,,,,,,,30/May/14 23:22;rohini;PIG-3976-1-branch13.patch;https://issues.apache.org/jira/secure/attachment/12647719/PIG-3976-1-branch13.patch,30/May/14 22:59;rohini;PIG-3976-1.patch;https://issues.apache.org/jira/secure/attachment/12647713/PIG-3976-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-05-30 23:31:48.258,,,no_permission,,,,,,,,,,,,,395882,Reviewed,,,,Fri May 30 23:37:04 UTC 2014,,,,,,,0|i1w5o7:,395999,,,,,,,,,,30/May/14 23:31;daijy;+1,30/May/14 23:37;rohini;Committed to branch 0.13 and trunk (0.14). Thanks Daniel for the review,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Scalar reference calls leading to missing records,PIG-3975,12717656,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,knoguchi,knoguchi,knoguchi,30/May/14 21:29,07/Jul/14 18:08,14/Mar/19 03:08,20/Jun/14 21:57,0.10.1,0.11.1,0.12.2,0.8.1,0.9.2,0.13.0,,,,,0,,,,,,,"We noticed that multiple pig runs with same input were producing different outputs.

Simplified script looked like this.

{noformat}
A = load 'input1' as (a1:int);
B = group A by a1 parallel 200;
C = load 'input2' as (c1:int);
D = foreach C generate B.$0;
store D into '/tmp/deletemeD';
E = load 'input3' as (c1:int);
F = foreach E generate B.$0;
store F into '/tmp/deletemeF';
{noformat}",,,,,,,,,,,,,,,,,,,,30/May/14 23:07;knoguchi;pig-3975-v01_withouttest.patch;https://issues.apache.org/jira/secure/attachment/12647714/pig-3975-v01_withouttest.patch,12/Jun/14 21:30;knoguchi;pig-3975-v02_withtests.patch;https://issues.apache.org/jira/secure/attachment/12650148/pig-3975-v02_withtests.patch,20/Jun/14 20:29;rohini;pig-3975-v03-additional-fix.patch;https://issues.apache.org/jira/secure/attachment/12651719/pig-3975-v03-additional-fix.patch,20/Jun/14 20:29;rohini;pig-3975-v03-trunk.patch;https://issues.apache.org/jira/secure/attachment/12651720/pig-3975-v03-trunk.patch,18/Jun/14 00:35;rohini;pig-3975-v03.patch;https://issues.apache.org/jira/secure/attachment/12650931/pig-3975-v03.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-06-14 06:58:21.435,,,no_permission,,,,,,,,,,,,,395860,,,,,Fri Jun 20 21:57:48 UTC 2014,,,,,,,0|i1w5jb:,395977,,,,,,,,,,"30/May/14 21:42;knoguchi;With the script from the description, job DAG looked like below.
{noformat}
Job DAG:
job_1399356417814_189120        ->      job_1399356417814_189121,job_1399356417814_189122,
job_1399356417814_189121        ->      job_1399356417814_189123,
job_1399356417814_189123
job_1399356417814_189122
{noformat}

Looking at the plan, I see that even though job_1399356417814_189122 and job_1399356417814_189123 read from output of job_1399356417814_189121, somehow job_1399356417814_189122 is missing that dependency.

{noformat}
==============================================================
job_1399356417814_189120 
pig.inputs ================
hdfs:/aaa.bbb.ccc:8020/user/knoguchi/input1:org.apache.pig.builtin.PigStorage
pig.mapPlan=================
B: Local Rearrange[tuple]{int}(false) - scope-8
|   |
|   Project[int][0] - scope-9
|
|---A: New For Each(false)[bag] - scope-5
    |   |
    |   Cast[int] - scope-3
    |   |
    |   |---Project[bytearray][0] - scope-2
pig.reducePlan=================
Empty Plan!
pig.reduce.stores=================
[(Name: B: Store(hdfs://aaa.bbb.ccc:8020/tmp/temp-912971374/tmp-113052755:org.apache.pig.impl.io.TFileStorage) - scope-10

==============================================================
job_1399356417814_189121
pig.inputs ================
hdfs://aaa.bbb.ccc:8020/tmp/temp-912971374/tmp-113052755:org.apache.pig.impl.io.TFileStorage
pig.mapPlan=================
Empty Plan!
pig.map.stores=================
[(Name: Store(hdfs://aaa.bbb.ccc:8020/tmp/temp-912971374/tmp-690789368:org.apache.pig.impl.io.InterStorage) - scope-25 Operator Key: scope-25)]
pig.reducePlan=================
null
pig.reduce.stores=================
[]
==============================================================
job_1399356417814_189122
pig.inputs ================
hdfs://aaa.bbb.ccc:8020/user/knoguchi/input3:org.apache.pig.builtin.PigStorage
pig.mapPlan=================
F: New For Each(false)[bag] - scope-20
|   |
|   POUserFunc(org.apache.pig.impl.builtin.ReadScalars)[int] - scope-19
|   |
|   |---Constant(0) - scope-17
|   |
|   |---Constant(hdfs://aaa.bbb.ccc:8020/tmp/temp-912971374/tmp-690789368) - scope-18
pig.map.stores=================
[(Name: F: Store(/tmp/deletemeF:org.apache.pig.builtin.PigStorage) - scope-21 Operator Key: scope-21)]
pig.reducePlan=================
null
pig.reduce.stores=================
[]
==============================================================
job_1399356417814_189123
pig.inputs ================
hdfs://aaa.bbb.ccc:8020/user/knoguchi/input2.txt:org.apache.pig.builtin.PigStorage
pig.mapPlan=================
D: New For Each(false)[bag] - scope-14
|   |
|   POUserFunc(org.apache.pig.impl.builtin.ReadScalars)[int] - scope-13
|   |
|   |---Constant(0) - scope-11
|   |
|   |---Constant(hdfs://aaa.bbb.ccc:8020/tmp/temp-912971374/tmp-690789368) - scope-12
pig.map.stores=================
[(Name: D: Store(/tmp/deletemeD:org.apache.pig.builtin.PigStorage) - scope-15 Operator Key: scope-15)]
pig.reducePlan=================
null
pig.reduce.stores=================
[]
{noformat}


","30/May/14 21:52;knoguchi;Due to the missing dependency, job_1399356417814_189122 may have some tasks receive empty  scalar value since file is not created yet.

This issue became more observable after PIG-2629 where we added caching instead of retrying when previous value was null.  (Instead of missing scalar value for *some* records, now scalar value is empty for the entire task.)

","30/May/14 23:07;knoguchi;Attaching a preliminary patch.
I believe what's happening is, 
MRCompiler.connectSoftLink() from PIG-1605  connects MRPlan based on PhysicalPlan's softlink.  But this is being called BEFORE MRCompiler.aggregateScalarsFiles() from PIG-1458 that creates an extra concatenate mapreduce job.

As a result, only the first scalar reference is getting the MRPlan dependency updated and rest are untouched.

I think there are two approaches for fixing this.
(1) Updating MRPlan dependency INSIDE MRCompiler.aggregateScalarsFiles().
OR
(2) Moving MRCompiler.connectSoftLink() to AFTER MRCompiler.aggregateScalarsFiles().

I took the second approach since it has an added benefit of MRCompiler.hasTooManyInputFiles() no longer depending on the job with scalar outputs.  I'm hoping this would also reduce the number of unnecessary concatenate jobs being created.
","12/Jun/14 21:30;knoguchi;Attaching a patch with added testcases.
One for the missing dependency and another for unnecessary concat job creation.  I'll run through the unit and e2e shortly.",12/Jun/14 21:47;knoguchi;ReviewBoard: https://reviews.apache.org/r/22535/,"14/Jun/14 06:58;daijy;[~knoguchi] Patch looks good to me. However this patch apply on 0.13 branch but not trunk, can you upload a patch for trunk?","15/Jun/14 01:59;rohini;[~daijy],
   We should put this into 0.13 as well. It is a very bad bug giving incorrect results.","15/Jun/14 02:26;daijy;That's fine for me. [~aniket486], is it possible to get it to 0.13.0?","16/Jun/14 15:23;knoguchi;bq. Koji Noguchi Patch looks good to me. However this patch apply on 0.13 branch but not trunk, can you upload a patch for trunk?

Thanks [~daijy]!  I wasn't aware that my local branch was that behind...
Tested with trunk.  I see that PIG-3970 made some space-only changes that caused my patch to break.  Instead of creating an identical patch with some space changes, can you apply the patch with 'patch -p0 --ignore-whitespace' on trunk ?",16/Jun/14 19:39;aniket486;We are redoing the RC for pig 0.13 so we should be able to apply this patch before we make another release. Please feel free to commit on the pig 13 branch.,"16/Jun/14 21:23;daijy;[~knoguchi], no problem. I see Rohini post another review comment. Once that resolved, I can commit the patch.","17/Jun/14 22:59;rohini;[~knoguchi],
   I ran the full unit test suite applying the patch and also adding  comp.aggregateScalarsFiles(); to Util.buildMRPlan. They passed. So can you just add that before check in?",18/Jun/14 00:35;rohini;Committed pig-3975-v03.patch which has the above suggested change to branch-0.13.  Saw one of the new tests fails in trunk after rebasing and applying the patch. Investigating that. ,"19/Jun/14 23:54;rohini;Just got back to this issue. seen.add(newSpec); is required after new FindStoreNameVisitor(pl, newSpec, oldSpec).visit();. Investigating why this issue did not manifest in 0.13 when it should have. Will wrap this one shortly. 
       ","20/Jun/14 20:29;rohini;Problem was ScalarVisitor.java change of PIG-3757 is not in branch 0.13. Without that scalar referenced by testSoftLinkDependencyWithMultipleScalarReferences() by F points to B: POPackage instead of B:POStore and gets skipped because it is not an instance of POStore. So it worked fine there. Will make it consistent with trunk patch by adding that to branch-0.13 with pig-3975-v03-additional-fix.patch. Also uploaded patch for trunk which includes ""seen.add(newSpec);"" change. Can someone review?","20/Jun/14 20:55;knoguchi;Had a loong chat with Rohini on learning what is happening.  This is my patch but Rohini knew much more on what actually is happening.  (as always)

So as I understand,  my testcase in the patch caught another bug introduced after PIG-3757.
On trunk, with my testcase but without my patch, testcase fails at 
{noformat}
junit.framework.AssertionFailedError: Unexpected number of mapreduce job. Missing concat job? expected:<4> but was:<5>
  at org.apache.pig.test.TestFRJoin2.testSoftLinkDependencyWithMultipleScalarReferences(TestFRJoin2.java:125)
{noformat}
This is even before my check on the missing dependency.  It's failing due to each scalar *reference* creating a new concat jobs.  By having extra "" seen.add(newSpec);"", it can avoid the repetitive concat creation attempt.

+1 on both the patches. 


",20/Jun/14 21:40;daijy;+1 for pig-3975-v03-additional-fix.patch.,20/Jun/14 21:57;rohini;Committed the additional patch to 0.13 and full patch to trunk. Thanks Koji for figuring out and fixing this tough issue. Thanks Daniel for the review.,,,,,,,,,,
E2E test data generation fails in cluster mode,PIG-3974,12717654,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,lbendig,lbendig,30/May/14 21:06,21/Nov/14 05:58,14/Mar/19 03:08,02/Jun/14 17:10,0.12.0,,,,,0.14.0,,e2e harness,,,0,,,,,,,"When test-e2e-deploy target is called to generate data for e2e tests, Pig fails to execute fs -mkdir /user/pig/tests/data and returns with the following error:
{quote}
Failed to generate data for testing: <Failed running /home/my/pig/trunk/test/e2e/pig/../../../bin/pig -e fs -mkdir /user/pig/tests/data>
{quote}
This issue only happens if the parent directory {{/user/pig/tests}} doesn't exist. In this case org.apache.hadoop.fs.shell.Mkdir.processNonexistentPath() throws a PathNotFoundException in the background.

I suspect, that the full parent directories should be created along the path. LocalDeployer.pm uses mkdir -p, but ExistingClusterDeployer.pm doesn't which leads to this issue.",,,,,,,,,,,,,,,,,,,,30/May/14 21:07;lbendig;PIG-3974.patch;https://issues.apache.org/jira/secure/attachment/12647686/PIG-3974.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-02 16:53:18.051,,,no_permission,,,,,,,,,,,,,395858,,,,,Mon Jun 02 22:21:24 UTC 2014,,,,,,,0|i1w5iv:,395975,,,,,,,,,,02/Jun/14 16:53;cheolsoo;+1. Will commit it shortly.,02/Jun/14 17:10;cheolsoo;Committed to trunk. Thank you Lorand!,"02/Jun/14 22:21;lbendig;Cheolsoo, thanks for committing it!",,,,,,,,,,,,,,,,,,,,,,,,,
"Ant ""test-tez"" target should set property hadoopversion=23 and exectype=tez",PIG-3973,12717631,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,daijy,daijy,30/May/14 19:35,21/Nov/14 05:59,14/Mar/19 03:08,15/Sep/14 18:35,,,,,,0.14.0,,build,,,0,,,,,,,"From the RB: https://reviews.apache.org/r/22058/

Cheolsoo suggest: Is it possible to automatically set -Dhadoopversion=23 and -Dexectype=tez in test-tez? Since the default values are 20 and mr, running ""ant test-tez"" will just fail",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-09-15 18:35:31.033,,,no_permission,,,,,,,,,,,,,395835,,,,,Mon Sep 15 18:35:31 UTC 2014,,,,,,,0|i1w5dr:,395952,,,,,,,,,,15/Sep/14 18:35;rohini;This issue is fixed as part of the patch for PIG-3935.,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javascript UDF fails if no output schema is defined,PIG-3969,12717337,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,lbendig,lbendig,29/May/14 19:14,21/Nov/14 05:58,14/Mar/19 03:08,02/Jun/14 17:07,0.12.0,,,,,0.14.0,,,,,0,,,,,,,"Example to reproduce the issue:
{code}
cat simple.js
function test(word) {
    return word;
}
register 'simple.js' using org.apache.pig.scripting.js.JsScriptEngine as myfuncs;
A = LOAD 'data' as (a:chararray);
B = FOREACH A GENERATE myfuncs.test($0);
{code}

Pig Stack Trace
---------------
ERROR 1200: <line 1, column 3>  mismatched input '.' expecting EOF
...
Caused by: java.lang.IllegalArgumentException: test.outputSchema is not a valid schema: <line 1, column 3>  mismatched input '.' expecting EOF
	at org.apache.pig.scripting.js.JsFunction.<init>(JsFunction.java:184)
	... 38 more
Caused by: Failed to parse: <line 1, column 3>  mismatched input '.' expecting EOF
	at org.apache.pig.parser.QueryParserDriver.parseSchema(QueryParserDriver.java:94)
	at org.apache.pig.parser.QueryParserDriver.parseSchema(QueryParserDriver.java:108)
	at org.apache.pig.impl.util.Utils.parseSchema(Utils.java:222)
	at org.apache.pig.impl.util.Utils.getSchemaFromString(Utils.java:196)
	at org.apache.pig.scripting.js.JsFunction.<init>(JsFunction.java:182)
	... 38 more

Problem: if no output schema is defined, an {{org.mozilla.javascript.Undefined}} object will be passed to {{Utils#getSchemaFromString}} which causes the exception.

Should: if there's no schema defined, udf should return an unnamed bytearray field.

",,,,,,,,,,,,,,,,,,,,29/May/14 19:16;lbendig;PIG-3969.patch;https://issues.apache.org/jira/secure/attachment/12647424/PIG-3969.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-02 17:01:58.418,,,no_permission,,,,,,,,,,,,,395541,,,,,Mon Jun 02 22:22:00 UTC 2014,,,,,,,0|i1w3m7:,395666,,,,,,,,,,02/Jun/14 17:01;cheolsoo;+1. Will commit this shortly.,02/Jun/14 17:07;cheolsoo;Committed to trunk. Thank you Lorand!,"02/Jun/14 22:22;lbendig;Cheolsoo, thanks for committing it!",,,,,,,,,,,,,,,,,,,,,,,,,
OperatorPlan.serialVersionUID is not defined,PIG-3968,12717322,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,daijy,daijy,29/May/14 18:33,21/Nov/14 05:59,14/Mar/19 03:08,29/May/14 18:43,,,,,,0.14.0,,impl,,,0,,,,,,,"The consequence is any change made to OperatorPlan.java may change the serialization string of the physical plan, and cause TestMRCompiler.testMergeJoin fail.",,,,,,,,,,,,,,,,,,,,29/May/14 18:34;daijy;PIG-3968-1.patch;https://issues.apache.org/jira/secure/attachment/12647408/PIG-3968-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-29 18:36:47.031,,,no_permission,,,,,,,,,,,,,395526,Reviewed,,,,Thu May 29 18:43:06 UTC 2014,,,,,,,0|i1w3iv:,395651,,,,,,,,,,29/May/14 18:36;rohini;+1,29/May/14 18:43;daijy;Patch committed to trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Grunt fail if we running more statement after first store,PIG-3967,12717208,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,29/May/14 04:16,07/Jul/14 18:07,14/Mar/19 03:08,30/May/14 00:29,,,,,,0.13.0,,impl,,,0,,,,,,,"We will hit output validation fail. The issue is caused by PIG-3545. We change PigServer.Graph.validateQuery() to invoke LogicalPlan.validate(), which will do the output validation. In Grunt mode, even after the first store, we will compile the entire statement cache, so the first store will be in the logical plan, validate on output fail. 

This makes datagenerator of Pigmix fail.",,,,,,,,,,,,,,,PIG-3991,,,,,29/May/14 04:17;daijy;PIG-3967-1.patch;https://issues.apache.org/jira/secure/attachment/12647291/PIG-3967-1.patch,29/May/14 04:54;daijy;PIG-3967-2.patch;https://issues.apache.org/jira/secure/attachment/12647295/PIG-3967-2.patch,30/May/14 00:17;cheolsoo;PIG-3967-suggestion.patch;https://issues.apache.org/jira/secure/attachment/12647491/PIG-3967-suggestion.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-05-29 16:32:47.022,,,no_permission,,,,,,,,,,,,,395412,Reviewed,,,,Fri May 30 00:29:12 UTC 2014,,,,,,,0|i1w2u7:,395540,,,,,,,,,,29/May/14 04:54;daijy;Add test case.,"29/May/14 16:32;cheolsoo;# Can we avoid setting a boolean flag _before_ and _after_ validateQuery()? Why don't we set a boolean flag inside the Grunt constructor instead? Perhaps name it as {{inGrunt}} since it might come handy for other purposes?
{code}
+                pigContext.inGruntValidation = true;
                 validateQuery();
+                pigContext.inGruntValidation = false;
{code}
# The unit test fails for me-
{code}
org.apache.pig.impl.plan.VisitorException: ERROR 6000:
<line 2, column 0> Output Location Validation Failed for: 'file:///Users/cheolsoop/workspace/pig-apache/output More info to follow:
Output directory file:/Users/cheolsoop/workspace/pig-apache/output already exists
{code} ",29/May/14 16:34;cheolsoo;Oh nevermind about #2. I had {{output}} dir in my workspace...,"29/May/14 17:01;cheolsoo;Actually, can you change {{output}} to something like {{tempDir/testGruntValidation}} in the test case? The {{tempDir}} is deleted by {{TestPigServer.tearDown()}}, so this seems better than create/delete {{output}} in the pwd.

When running the test more than one time, I found {{output}} doesn't get cleaned if the test case fails. In fact, we should always clean it up.","29/May/14 17:51;daijy;Comment#1: the problem is even in Grunt mode, there are two different cases: validateQuery happens when replaying every statement in cache, in which we need to disable output validation; When the real LogicalPlan is constructed, it removes previous LOStore (Graph.skipStores), and after this, the output validation should happen. A flag in Grunt does not distinguish these two cases.

Comment#2: Sure, I will change the test to use tempDir.","30/May/14 00:07;cheolsoo;[~daijy], thank you for the explanation. If that's the case, let's pass a boolean param to {{LogicalPlan.validate()}}. So instead we can do the following-
{code}
diff --git a/src/org/apache/pig/PigServer.java b/src/org/apache/pig/PigServer.java
index 0b2285e..d7ec664 100644
--- a/src/org/apache/pig/PigServer.java
+++ b/src/org/apache/pig/PigServer.java
@@ -1677,12 +1677,12 @@ public class PigServer {
             }
         }

-        void validateQuery() throws FrontendException {
+        private void validateQuery() throws FrontendException {
             String query = buildQuery();
             QueryParserDriver parserDriver = new QueryParserDriver( pigContext, scope, fileNameMap );
             try {
                 LogicalPlan plan = parserDriver.parse( query );
-                plan.validate(pigContext, scope);
+                plan.validate(pigContext, scope, true);
             } catch(FrontendException ex) {
                 scriptCache.remove( scriptCache.size() -1 );
                 throw ex;
@@ -1741,7 +1741,7 @@ public class PigServer {
         }

         private void compile() throws IOException {
-            lp.validate(pigContext, scope);
+            lp.validate(pigContext, scope, false);
             currDAG.postProcess();
         }

diff --git a/src/org/apache/pig/newplan/logical/relational/LogicalPlan.java b/src/org/apache/pig/newplan/logical/relational/LogicalPlan.java
index dbbd5d1..b753121 100644
--- a/src/org/apache/pig/newplan/logical/relational/LogicalPlan.java
+++ b/src/org/apache/pig/newplan/logical/relational/LogicalPlan.java
@@ -165,8 +165,9 @@ public class LogicalPlan extends BaseOperatorPlan {

         return Integer.toString(hos.getHashCode().asInt());
     }
-
-    public void validate(PigContext pigContext, String scope) throws FrontendException {
+
+    public void validate(PigContext pigContext, String scope, boolean skipInputOutputValidation)
+            throws FrontendException {

         new DanglingNestedNodeRemover(this).visit();
         new ColumnAliasConversionVisitor(this).visit();
@@ -204,7 +205,7 @@ public class LogicalPlan extends BaseOperatorPlan {
         // compute whether output data is sorted or not
         new SortInfoSetter(this).visit();

-        if (!(pigContext.inExplain || pigContext.inDumpSchema)) {
+        if (!(skipInputOutputValidation || pigContext.inExplain || pigContext.inDumpSchema)) {
             // Validate input/output file
             new InputOutputFileValidatorVisitor(this, pigContext).visit();
         }
{code}
My main concern is that if you set a boolean flag to true and false before and after a method call, it will make someone wonder about the logic. It certainly did to me.

I can upload a patch that includes my suggestions. Let me know.","30/May/14 00:17;cheolsoo;Here is the patch that includes my both suggestions. Feel free to reject it. :-)

Verified that it passes the test case, and Grunt doesn't fail running more than one stores.","30/May/14 00:29;daijy;The suggested patch looks good to me. Thanks Cheolsoo!

Patch committed to trunk and 0.13 branch.",,,,,,,,,,,,,,,,,,,,
Compile fail against Hadoop 2.4.0 after PIG-3913,PIG-3960,12716157,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,22/May/14 18:27,07/Jul/14 18:07,14/Mar/19 03:08,22/May/14 18:59,,,,,,0.13.0,0.14.0,impl,,,0,,,,,,,"Error message:
{code}
    [javac] /grid/0/workspace/champlain-hdp-compile/bigtop/build/pig/rpm/BUILD/pig-0.13.0.2.2.0.0/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java:122: exception java.lang.InterruptedException is never thrown in body of corresponding try statement
    [javac]         } catch (InterruptedException ir) {
    [javac]           ^
    [javac] /grid/0/workspace/champlain-hdp-compile/bigtop/build/pig/rpm/BUILD/pig-0.13.0.2.2.0.0/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java:199: exception java.lang.InterruptedException is never thrown in body of corresponding try statement
    [javac]         } catch (InterruptedException ir) {
    [javac]           ^
    [javac] /grid/0/workspace/champlain-hdp-compile/bigtop/build/pig/rpm/BUILD/pig-0.13.0.2.2.0.0/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java:210: exception java.lang.InterruptedException is never thrown in body of corresponding try statement
    [javac]         } catch (InterruptedException ir) {
{code}

This is due to the method signature change of Hadoop, such as Job.getCounters(), no longer throw InterruptedException.",,,,,,,,,,,,,,,,,,,,22/May/14 18:29;daijy;PIG-3960-1.patch;https://issues.apache.org/jira/secure/attachment/12646353/PIG-3960-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-22 18:54:02.424,,,no_permission,,,,,,,,,,,,,394366,Reviewed,,,,Thu May 22 18:59:50 UTC 2014,,,,,,,0|i1vwhz:,394504,,,,,,,,,,22/May/14 18:29;daijy;Trivial fix.,22/May/14 18:54;rohini;+1,22/May/14 18:59;daijy;Patch committed to trunk and 0.13 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,
TestMRJobStats is broken in 0.13 and trunk,PIG-3958,12716117,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,cheolsoo,cheolsoo,22/May/14 16:18,07/Jul/14 18:07,14/Mar/19 03:08,28/May/14 15:27,,,,,,0.13.0,0.14.0,,,,0,,,,,,,"This is a regression of PIG-3913. It changed some method signature, and that breaks the unit test.",,,,,,,,,,,,,,,,,,,,28/May/14 05:32;aniket486;PIG-3958.patch;https://issues.apache.org/jira/secure/attachment/12647063/PIG-3958.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-28 00:15:33.383,,,no_permission,,,,,,,,,,,,,394326,,,,,Wed May 28 15:28:20 UTC 2014,,,,,,,0|i1vw93:,394464,,,,,,,,,,"28/May/14 00:15;aniket486;We should be allowed to change method signatures of non-public methods.

Problem is that this test mocks behavior of jobClient's getTaskReport api using mockito and then checks whether we get min, max, avg, mean correct or not. getTaskReport from jobClient is broken in hadoop for local mode. With PIG-3913, getTaskReport hides behind HadoopShims (to workaround hadoop issue) in a static method which cannot be mocked (https://code.google.com/p/mockito/wiki/FAQ).

I will refactor the code a little bit to make this work.","28/May/14 00:39;cheolsoo;Thank you Aniket for looking into this. Since you've started working on this, I am assigning it to you.","28/May/14 06:01;daijy;+1, works for me.",28/May/14 15:28;aniket486;Committed to trunk and branch-0.13! Thanks [~daijy] for reviewing.,,,,,,,,,,,,,,,,,,,,,,,,
UDF profile is often misleading,PIG-3956,12715831,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,21/May/14 19:03,21/Nov/14 05:58,14/Mar/19 03:08,22/May/14 16:12,,,,,,0.14.0,,impl,,,0,,,,,,,"Pig provides UDF profiling via the {{pig.udf.profile}} perperty. How it works is that it samples every 100th UDF invocation and increments a counter by 100 x elapsed time.
{code}
private final static int TIMING_FREQ = 100;
...
PigStatusReporter.getInstance().incrCounter(counterGroup, TIMING_COUNTER,
    Math.round((System.nanoTime() - startNanos) / 1000) * TIMING_FREQ);
{code}
The problem is that this assumes every invocation takes about the same time, which doesn't hold true oftentimes.

Here is an example. TIMING_FREQ = 100.
{code}
set pig.udf.profile true;
set opt.fetch false;
l = LOAD 'foo' AS (value:chararray);
f = FOREACH l GENERATE LOWER(value);
DUMP f;
{code}

This job gives me the following counters-
{code}
org.apache.pig.builtin.LOWER - approx_invocations: 6,144,600
org.apache.pig.builtin.LOWER - approx_microsecs: 130,913,800
{code}

However, if I change TIMING_FREQ to 1, I get the followings-
{code}
org.apache.pig.builtin.LOWER - approx_invocations: 6,144,248
org.apache.pig.builtin.LOWER - approx_microsecs: 19,711,842
{code}

As can be seen, approx_microsecs is 6x larger when sampling every 100th invocation as compared to when sampling every invocation.",,,,,,,,,,,,,,,,,,,,21/May/14 19:54;cheolsoo;PIG-3956-1.patch;https://issues.apache.org/jira/secure/attachment/12646086/PIG-3956-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-21 22:14:27.562,,,no_permission,,,,,,,,,,,,,394116,,,,,Thu May 22 16:12:18 UTC 2014,,,,,,,0|i1vuyn:,394254,,,,,,,,,,"21/May/14 19:54;cheolsoo;The attached patch makes the frequency of profiling configurable. The default is 100, so the default behavior remains the same.

I also updated {{conf/pig.properties}} and the performance page in Pig manual.",21/May/14 22:14;daijy;+1,22/May/14 16:12;cheolsoo;Committed to trunk. Thank you Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,,,,
Remove url.openStream() file descriptor leak from JCC,PIG-3955,12715557,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,aniket486,aniket486,aniket486,20/May/14 18:13,07/Jul/14 18:07,14/Mar/19 03:08,28/May/14 19:57,0.13.0,,,,,0.13.0,,,,,0,,,,,,,"As mentioned in PIG-2672, we need to fix this file descriptor leak.",,,,,,,,,,,,,,,,,,,,28/May/14 18:57;aniket486;PIG-3955-1.patch;https://issues.apache.org/jira/secure/attachment/12647183/PIG-3955-1.patch,28/May/14 15:48;aniket486;PIG-3955.patch;https://issues.apache.org/jira/secure/attachment/12647151/PIG-3955.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,393842,,,,,Wed May 28 19:57:44 UTC 2014,,,,,,,0|i1vtcf:,393990,,,,,,,,,,28/May/14 19:57;aniket486;RB:https://reviews.apache.org/r/21979/,28/May/14 19:57;aniket486;Committed to trunk and branch-0.13! Thanks [~rohini] and [~cheolsoo] for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,
Removing empty file PColFilterExtractor.java speeds up rebuilds,PIG-3950,12715322,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,mrflip,mrflip,mrflip,19/May/14 23:29,07/Jul/14 18:07,14/Mar/19 03:08,20/May/14 01:14,,,,,,0.13.0,,build,,,0,,,,,,,"The file src/org/apache/pig/newplan/PColFilterExtractor.java is empty. It looks like its soul but not its earthly vessel left in 6b162e4b01b1a624fc59f93a160b37791df45026, PIG-3590

The empty file forces reassembly of the jar on every build even if nothing material had changed. Is there a reason for this file to exist as empty? If so, it should instead have a '/* (placeholder) */' line or something.

Trivial patch attached.",,,,,,,,,,,,,,,,,,,,19/May/14 23:30;mrflip;PIG-3950-Removing-empty-file-PColFilterExtractor-speeds-builds.patch;https://issues.apache.org/jira/secure/attachment/12645676/PIG-3950-Removing-empty-file-PColFilterExtractor-speeds-builds.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-20 01:14:11.196,,,no_permission,,,,,,,,,,,,,393608,,,,,Tue May 20 01:14:11 UTC 2014,,,Patch Available,,,,0|i1vs07:,393768,removed empty file src/org/apache/pig/newplan/PColFilterExtractor.java,,,,,,,,,"20/May/14 01:14;cheolsoo;Gosh. Thank you for finding this!

Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveColumnarStorage compile failure with Hive 0.14.0,PIG-3949,12715304,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,19/May/14 21:49,07/Jul/14 18:07,14/Mar/19 03:08,26/May/14 02:28,,,,,,0.13.0,,build,,,0,,,,,,,"HIVE-6430 remove ByteStream.Output.getCount(), fortunately ByteStream.Output.getLength() is available in all Hive versions and provides the same function.",,,,,,,,,,,,,,,,,,,,19/May/14 21:50;daijy;PIG-3949-1.patch;https://issues.apache.org/jira/secure/attachment/12645647/PIG-3949-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-26 00:13:19.97,,,no_permission,,,,,,,,,,,,,393590,Reviewed,,,,Mon May 26 02:28:50 UTC 2014,,,,,,,0|i1vrw7:,393750,,,,,,,,,,"26/May/14 00:13;cheolsoo;+1.

Btw, did you mean Hive 0.14 because HIVE-6430 is committed in 0.14? Anyhow, looks good to me.","26/May/14 02:28;daijy;Oh, yes, it is for Hive 0.14. Thanks for point out. Change the title to reflect it.

Patch committed to both 0.13 branch and trunk. Thanks Cheolsoo for review!",,,,,,,,,,,,,,,,,,,,,,,,,,
Ant not sending hadoopversion to piggybank sub-ant,PIG-3945,12714987,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mrflip,mrflip,mrflip,18/May/14 02:44,07/Jul/14 18:07,14/Mar/19 03:08,19/May/14 21:26,0.12.0,,,,,0.13.0,,build,piggybank,,0,ant,build,piggybank,,,,"The piggybank ant script needs to know what hadoopversion to use; but it is described in the main build script with inheritAll false, meaning that none of the main ant task's properties are sent along. 

This matters because the JobHistoryLoader needs to be excluded when building against Hadoop 2.0 (0.23) version branch. Running 'ant piggybank' from the top level was failed for me with errors about org.apache.hadoop.mapred.DefaultJobHistoryParser not found in contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HadoopJobHistoryLoader.java

This patch leaves the inheritAll flag alone but selectively passes along the hadoopversion property. Alternatively, I can put in a patch that instead flips inheritAll to be true.",,,,,,,,,,,,,,,,,,,,18/May/14 02:44;mrflip;0001-Ant-not-sending-hadoopversion-to-piggybank-sub-ant.patch;https://issues.apache.org/jira/secure/attachment/12645440/0001-Ant-not-sending-hadoopversion-to-piggybank-sub-ant.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-19 21:23:34.142,,,no_permission,,,,,,,,,,,,,393300,,,,,Mon May 19 21:26:28 UTC 2014,,,Patch Available,,,,0|i1vq4f:,393462,"Ant sends hadoopversion to piggybank sub-ant, fixes building piggybank from top level",,,,,,,,,19/May/14 21:23;cheolsoo;+1. Will commit this today.,19/May/14 21:26;cheolsoo;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,
PigNullableWritable toString method throws NPE on null value,PIG-3944,12714949,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mauzhang,mauzhang,mauzhang,17/May/14 08:16,07/Jul/14 18:07,14/Mar/19 03:08,19/May/14 21:37,0.12.1,0.13.0,,,,0.13.0,,,,,0,,,,,,,"mValue could be null here (when ""mNull == true"")
{code}
@Override
public String toString() {
  return ""Null: "" + mNull + "" index: "" + mIndex + "" "" + mValue.toString();
}
{code}",,,,,,,,,,,,,,,,,,,,17/May/14 08:37;mauzhang;PIG-3944.patch;https://issues.apache.org/jira/secure/attachment/12645400/PIG-3944.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-19 20:23:29.889,,,no_permission,,,,,,,,,,,,,393262,,,,,Mon May 19 21:37:30 UTC 2014,,,,,,,0|i1vpvz:,393424,,,,,,,,,,19/May/14 20:23;cheolsoo;+1. Will commit it today.,19/May/14 21:37;cheolsoo;Committed to trunk. Thank you Manu!,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException writing .pig_header for field with null name (in JsonMetadata.java),PIG-3940,12714748,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mrflip,mrflip,mrflip,16/May/14 11:10,07/Jul/14 18:08,14/Mar/19 03:08,19/May/14 21:39,0.12.0,,,,,0.13.0,,impl,,,0,PigStorage,schema,storage,,,,"Pig throws an NPE writing .pig_header for field with null name. Attached patch has it instead write field name as '$i' corresponding to the field's zero-based position -- i.e., if the second field is unnamed, its header will be written as $1.

{code}
Caused by: java.lang.NullPointerException
	at org.apache.pig.builtin.JsonMetadata.storeSchema(JsonMetadata.java:319)
	at org.apache.pig.builtin.PigStorage.storeSchema(PigStorage.java:578)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter.storeCleanup(PigOutputCommitter.java:144)
{code}",,,,,,,,,,,,,,,,,,,,16/May/14 11:11;mrflip;0001-Fixed-NullPointerException-writing-.pig_header-for-f.patch;https://issues.apache.org/jira/secure/attachment/12645217/0001-Fixed-NullPointerException-writing-.pig_header-for-f.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-19 20:13:57.43,,,no_permission,,,,,,,,,,,,,393061,,,,,Mon May 19 21:39:30 UTC 2014,,,Patch Available,,,,0|i1voov:,393228,Fixed NullPointerException writing schema headers for fields with null names. They are now written as '$i' corresponding to the field's zero-based position.,,,,,,,,,"16/May/14 11:11;mrflip;Patch, with test for bug",19/May/14 20:13;cheolsoo;+1. Will commit it today.,19/May/14 21:39;cheolsoo;Committed to trunk. Thank you Philip!,,,,,,,,,,,,,,,,,,,,,,,,,
DBStorage fails on storing nulls for non varchar columns,PIG-3936,12713958,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jeremykarn,jeremykarn,jeremykarn,13/May/14 17:59,07/Jul/14 18:07,14/Mar/19 03:08,19/May/14 21:35,,,,,,0.13.0,,piggybank,,,0,,,,,,,"When trying to store a null value into an integer column in Postgres we were getting a JDBC error on the insert statement.  I tracked it down to always using varchar as the type when writing nulls in Pig.  

To fix the issue I use the schema (if present) to determine the type of the field with the null value.  ",,,,,,,,,,,,,,,,,,,,13/May/14 18:00;jeremykarn;PIG-3936.patch;https://issues.apache.org/jira/secure/attachment/12644666/PIG-3936.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-19 21:18:06.47,,,no_permission,,,,,,,,,,,,,392271,,,,,Mon May 19 21:35:31 UTC 2014,,,Patch Available,,,,0|i1vjz3:,392461,,,,,,,,,,19/May/14 21:18;cheolsoo;+1. Will commit this today.,19/May/14 21:35;cheolsoo;Committed to trunk. Thank you Jeremy!,,,,,,,,,,,,,,,,,,,,,,,,,,
"""java.io.IOException: Cannot initialize Cluster"" in local mode with hadoopversion=23 dependencies",PIG-3930,12712981,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,jira.shegalov,jira.shegalov,jira.shegalov,08/May/14 00:16,07/Jul/14 18:08,14/Mar/19 03:08,09/May/14 16:39,0.12.0,,,,,0.13.0,,build,,,0,,,,,,,"Steps to reproduce with script wc.pig
{code}
A = load 'README.txt';
B = foreach A generate flatten(TOKENIZE((chararray)$0)) as word;
C = group B by word;
D = foreach C generate COUNT(B), group;
store D into 'wc.out';
{code}
# $ cd workspace/pig-trunk
# $ ant clean -Dhadoopversion=23 jar
# $ bin/pig -x local  ~/wc.pig 

The problem is that in the dependency {{./build/ivy/lib/Pig/hadoop-mapreduce-client-common-2.0.3-alpha.jar}} the file {{META-INF/services/org.apache.hadoop.mapreduce.protocol.ClientProtocolProvider}} lacks the entry {{org.apache.hadoop.mapred.LocalClientProtocolProvider}}.

When the dependency is folded into fat pig.jar {{src/META-INF/services/org.apache.hadoop.mapreduce.protocol.ClientProtocolProvider}} is overwritten. 

This can be fixed by using duplicate=""preserve"" in ant jar task.",,,,,,,,,,,,,,,,,,,,08/May/14 00:23;jira.shegalov;PIG-3930.v01.patch;https://issues.apache.org/jira/secure/attachment/12643880/PIG-3930.v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-09 04:06:09.532,,,no_permission,,,,,,,,,,,,,391297,,,,,Fri May 09 16:39:41 UTC 2014,,,,,,,0|i1ve6n:,391517,,,,,,,,,,09/May/14 04:06;cheolsoo;+1. I'll commit it tomorrow.,09/May/14 16:39;cheolsoo;Committed to trunk. Thank you Gera!,,,,,,,,,,,,,,,,,,,,,,,,,,
pig.temp.dir should allow to substitute vars as hadoop configuration does,PIG-3929,12712912,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,07/May/14 18:34,07/Jul/14 18:07,14/Mar/19 03:08,20/May/14 18:44,0.13.0,,,,,0.13.0,,,,,0,,,,,,,"pig.temp.dir should allow substituting system variables so that it can be configured to have user level temp directory, if required.",,,,,,,,,,,,,,,,,,,,13/May/14 19:10;aniket486;PIG-3929-1.patch;https://issues.apache.org/jira/secure/attachment/12644682/PIG-3929-1.patch,13/May/14 19:10;aniket486;PIG-3929-2.patch;https://issues.apache.org/jira/secure/attachment/12644681/PIG-3929-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-05-08 06:23:21.604,,,no_permission,,,,,,,,,,,,,391228,,,,,Tue May 20 18:44:29 UTC 2014,,,,,,,0|i1vdrj:,391449,,,,,,,,,,"08/May/14 06:23;jrottinghuis;Why not simply default this to hadoop.tmp.dir ?
True that hadoop.tmp.dir often has variables in it itself (for example /tmp/hadoop-${user}) however, if pig.temp.dir cannot directly refer to hadoop.tmp.dir then both need to be kept in sync by hand ?","12/May/14 17:55;aniket486;[~jrottinghuis], that means we assume that pig alway runs on hadoop (which is sort of true). Also, removing pig.temp.dir will not be backwards compatible.
Unfortunately, Configuration#substituteVars is not declared public (and static), so copying would be the only option left to enable this. Thoughts?",13/May/14 19:11;aniket486;I have attached PIG-3929-1.patch and PIG-3929-2.patch with two different approaches. Comments?,"13/May/14 22:36;jcoveney;[~aniket486]: I assume that hadoop applies the substituteVars when it does the conversion to the Configuration? If that's the case that seems like a fine approach, since it means we'll be consistent with how hadoop does it.","14/May/14 17:56;aniket486;Hadoop does substituteVars on conf.get.

[~jcoveney], Which one do you prefer? 1. props to conf 2. copying substituteVars.
converting props to conf has one downside- it makes pig internal code path depend on hadoop.","15/May/14 01:34;jcoveney;Hmm, would be nice to avoid that dependency if we can. I'm +1 approach 1.","20/May/14 18:44;aniket486;Committed to trunk. Thanks for the review, [~jcoveney].",,,,,,,,,,,,,,,,,,,,,
Gitignore file should ignore all generated artifacts,PIG-3923,12712543,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,mrflip,mrflip,mrflip,06/May/14 09:51,07/Jul/14 18:07,14/Mar/19 03:08,10/May/14 23:11,,,,,,0.13.0,,build,,,0,build,git,,,,,"There are several artifacts that get created by build targets which are neither versioned nor .gitignore'd. The attached patch excludes these files:

* .ant-targets-build.xml
* contrib/piggybank/java/piggybank.jar
* conf/log4j.properties
* lib/jdiff/pig_*SNAPSHOT.xml


",,,,,,,,,,,,,,,,,,,,10/May/14 06:33;mrflip;0001-.gitignore-excludes-avro-generated-files-and-test-re.patch;https://issues.apache.org/jira/secure/attachment/12644249/0001-.gitignore-excludes-avro-generated-files-and-test-re.patch,06/May/14 09:52;mrflip;0001-gitignore-file-excludes-generated-artifacts.patch;https://issues.apache.org/jira/secure/attachment/12643527/0001-gitignore-file-excludes-generated-artifacts.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-05-09 04:08:01.636,,,no_permission,,,,,,,,,,,,,390859,,,,,Sat May 10 23:11:38 UTC 2014,,,Patch Available,,,,0|i1vbl3:,391095,.gitignore ignores some generated artifacts it should ignore,,,,,,,,,06/May/14 09:52;mrflip;Trivial patch for the .gitignore file to exclude a few generated artifacts,09/May/14 04:08;cheolsoo;+1.,09/May/14 04:18;cheolsoo;Committed to trunk. Thank you Philip!,"10/May/14 06:33;mrflip;A couple more generated artifacts showed up once I got more of the build options to work: test/org/apache/pig/builtin/avro/data/*, test/resources/*.jar
","10/May/14 06:34;mrflip;Patch attached -- it should be applied *over* the other one (as in, both should be applied).",10/May/14 23:11;cheolsoo;Thank you for catching that! Committed the additional patch to trunk.,,,,,,,,,,,,,,,,,,,,,,
Increase Forrest heap size to avoid OutOfMemoryError building docs,PIG-3922,12712546,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,mrflip,mrflip,mrflip,06/May/14 10:04,07/Jul/14 18:08,14/Mar/19 03:08,09/May/14 04:17,0.12.0,,,,,0.13.0,,,,,0,ant,build,docs,fop,forrest,javadoc,"Trying to build the 'tar' target fails for me with an OutOfMemoryError from forrest deep in the build process.

The attached patch increases the heap size for the forrest task from 64m to 256m by changing the single line 'forrest.maxmemory=256m' in src/docs/forrest.properties.

Here's the stacktrace, mostly to make the issue more googleable.

{code}
     [exec] * [44/5]    [0/0]     7.135s 0b      basic.pdf
     [exec] Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded
     [exec] 	at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:64)
     [exec] 	at java.lang.StringBuffer.<init>(StringBuffer.java:96)
     [exec] 	at org.apache.fop.render.pdf.PDFRenderer.renderSpace(PDFRenderer.java:1494)
    (...snip...)
     [exec] Java Result: 1
     [exec]   Copying broken links file to site root.
     [exec] BUILD FAILED
     [exec] /usr/local/share/apache-forrest/main/targets/site.xml:224: Error building site.
     [exec]   - See /Users/flip/ics/data_science_fun_pack/pig/pig/src/docs/build/site/broken-links.xml
BUILD FAILED
/Users/flip/ics/data_science_fun_pack/pig/pig/build.xml:633: exec returned: 1
{code}

(Incidentally: after fixing that I also hit an NPE from Ant on a later stage of the javadoc build. That was fixed by moving from ant 1.9.3 to ant 1.9.4):

{code}
/Users/flip/ics/data_science_fun_pack/pig/pig/build.xml:1464: java.lang.NullPointerException
	at org.apache.tools.ant.taskdefs.Javadoc.postProcessGeneratedJavadocs(Javadoc.java:2450)
	at org.apache.tools.ant.taskdefs.Javadoc.execute(Javadoc.java:1790)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:292)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
{code}",,,,,,,,,,,,,,,,,,,,06/May/14 10:05;mrflip;0001-Increased-Forrest-heap-size-to-fix-build-of-docs.patch;https://issues.apache.org/jira/secure/attachment/12643529/0001-Increased-Forrest-heap-size-to-fix-build-of-docs.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-09 03:44:40.644,,,no_permission,,,,,,,,,,,,,390862,,,,,Fri May 09 04:17:11 UTC 2014,,,Patch Available,,,,0|i1vblr:,391098,,,,,,,,,,06/May/14 10:05;mrflip;one-liner fix to increase the heap size,09/May/14 03:44;cheolsoo;+1. I'll commit it tomorrow.,09/May/14 04:17;cheolsoo;Committed to trunk. Thank you Philip!,,,,,,,,,,,,,,,,,,,,,,,,,
Obsolete entries in piggybank javadoc build script,PIG-3921,12712532,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,mrflip,mrflip,mrflip,06/May/14 08:49,07/Jul/14 18:08,14/Mar/19 03:08,09/May/14 04:25,0.12.0,,,,,0.13.0,,piggybank,,,0,build,javadoc,piggybank,,,,"The piggybank build.xml still refers to {code}${src.dir}/{filtering,grouping,comparison}{code}, which no longer exist in the repo. This is a trivial patch to remove those lines from the build script",,,,,,,,,,,,,,,,,,,,06/May/14 08:50;mrflip;0001-Removed-obsolete-entries-from-piggybank-Javadoc-buil.patch;https://issues.apache.org/jira/secure/attachment/12643512/0001-Removed-obsolete-entries-from-piggybank-Javadoc-buil.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-09 04:21:12.729,,,no_permission,,,,,,,,,,,,,390848,,,,,Fri May 09 04:25:57 UTC 2014,,,Patch Available,,,,0|i1vbin:,391084,,,,,,,,,,06/May/14 08:50;mrflip;Patch attached,09/May/14 04:21;cheolsoo;+1. I'll also remove those directories in svn since they're empty.,"09/May/14 04:25;cheolsoo;Committed to trunk.

I also removed the empty directories-
{code}
Sending        CHANGES.txt
Sending        contrib/piggybank/java/build.xml
Deleting       contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/comparison
Deleting       contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/filtering
Deleting       contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/grouping
Deleting       contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/comparison
Deleting       contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/filtering
Deleting       contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/grouping
{code}",,,,,,,,,,,,,,,,,,,,,,,,,
isEmpty should not be early terminating,PIG-3916,12711814,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,rohini,rohini,rohini,01/May/14 22:01,07/Jul/14 18:07,14/Mar/19 03:08,02/May/14 18:38,0.11,,,,,0.12.2,0.13.0,,,,0,,,,,,,"PIG-2066 makes isEmpty early terminating which is very wrong. When there is a binary condition with isEmpty() followed by something like SUM, it skips records leading to wrong results. ",,,,,,,,,,,,,,,,,,,,01/May/14 23:48;rohini;PIG-3916-1.patch;https://issues.apache.org/jira/secure/attachment/12642958/PIG-3916-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-02 07:18:24.479,,,no_permission,,,,,,,,,,,,,390135,Reviewed,,,,Fri May 02 18:38:19 UTC 2014,,,,,,,0|i1v76n:,390372,,,,,,,,,,"02/May/14 07:18;daijy;Seems isEmpty is the main use case for PIG-2066. I am fine commit the patch as an immediate remedy but we also need to either rollback PIG-2066 completely (other early terminating UDF will share the same issue), or redo PIG-2066 in a right way (only terminate IsEmpty but not SUM).","02/May/14 15:21;rohini;Tried to see if only terminating isEmpty() was possible, but it is lot more work. For now reverting isEmpty() as it causes data loss. 

A more detailed description of impact for users to assess damage.
Impact:
The isEmpty() function in a FOREACH clause causes wrong results when used
after a GROUP or COGROUP clause when accumulative mode is used (i.e there are
no other non-Accumulator UDFs in the FOREACH clause). Also saw isEmpty() being evaluated to true when it was not.

For eg:
   - If the FOREACH had only isEmpty() and SUM() functions in a binary
condition which are both Accumulator UDFs, then wrong results are produced for
the SUM() where it only sums up the first batch and terminates after that.
SUM,COUNT, MIN, MAX are all accumulator UDFs.
   - If the FOREACH had any another non-accumulator UDF (For eg: STRSPLIT()),
then it is not affected as accumulative mode would not be used. ","02/May/14 17:00;daijy;+1 for the patch. As I mentioned before, let's reverting isEmpty first. ",02/May/14 18:38;rohini;Committed to branch-0.12 and trunk. Thanks Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,
MapReduce queries in Pigmix outputs different results than Pig's,PIG-3915,12711811,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kereno,keren3000,keren3000,01/May/14 21:53,30/Oct/14 00:39,14/Mar/19 03:08,29/May/14 05:13,0.12.0,,,,,0.13.0,,tools,,,0,,,,,,,"Hello,

The Pigmix benchmark has 17 queries comparing Pig to MapReduce Java. Some of these queries are not outputting the same results in Pig and MapReduce. Looking into the outputs, it seems the errors reside in the MapReduce  code. For example, L6 has no output because the output of the map function sends the wrong key to the reducer: ""query_term"" (field 3) instead of ""timespent"" (field 2). Hence an exception is thrown, and there is no output to the query in MapReduce. I am planning to submit a patch once I fixed all the queries in MapReduce :)

Thanks,
Keren  ",,,,,,,,,,,,,,,,,,,,23/May/14 20:02;keren3000;PIG-3915.2.patch;https://issues.apache.org/jira/secure/attachment/12646589/PIG-3915.2.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-29 05:13:59.674,,,no_permission,,,,,,,,,,,,,390132,,,,,Thu Oct 30 00:39:02 UTC 2014,,,,,,,0|i1v75z:,390369,Bug fixes for the Pigmix queries,,,,,,,,,14/May/14 00:15;keren3000;Bug fixes for Pigmix queries based on the mapred(old) API,14/May/14 00:17;keren3000;Thanks to Kerim Yasin Oktay for his help & code review!,15/May/14 00:23;keren3000;https://reviews.apache.org/r/21475/,"23/May/14 20:02;keren3000;Patch for the Pigmix queries - already reviewed by Daniel Dai on:
https://reviews.apache.org/r/21475/
","29/May/14 05:13;daijy;Run pigmix with the patch, all query runs and number looks reasonable. Patch committed to 0.13 branch and trunk. 

Also need to mention Pigmix datagenerator is broken by PIG-3967.","29/Oct/14 17:30;ssatish;[~daijy] - When I use runpigmix.pl related to a map-reduce job on a hadoop-0.20.2 core, I get this error - 

hadoop jar /opt/mapr/pig/pig-0.13/pigperf.jar org.apache.pig.test.pigmix.mapreduce.L1 /pigmix /pigmixresults/mapreduce 40
Exception in thread ""main"" java.lang.VerifyError: (class: org/apache/pig/test/pigmix/mapreduce/L1, method: main signature: ([Ljava/lang/String;)V) Incompatible argument to function
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:190)

Was wondering since this change set is based on mapred(old) API (as per [~keren3000]'s 1st comment) its causing some byte code incompatibilities?
",29/Oct/14 17:38;daijy;It should not be. Both old/new API should be ok to run mapreduce.,"29/Oct/14 17:49;ssatish;just to clarify, we are running pig-0.13 after compiling against hadoop-core.version=1.0.3 in ivy/libraries.properties
No problems so far running regular pig scripts (after a small change to HadoopShims). 

Just wanted to know the environment where you tested the org.apache.pig.test.pigmix.mapreduce.L*.java
Which version of hadoop-core did you compile & run against? ","29/Oct/14 18:00;daijy;I've tested 2.4.0 and 1.0.4. The error message complaining about L1.main, sounds like a Java issue not a hadoop issue. Which version of java are you using?","29/Oct/14 18:02;keren3000;Hi Suhas,

I am running Pig version 13 on Hadoop version 1.2.1.
It's running without editing version properties.

There's a patch on https://issues.apache.org/jira/browse/PIG-4004 in case you wanted to upgrade your MR Pigmix queries to the new API.

Cheers,
Keren


",30/Oct/14 00:39;ssatish;Thanks for the reply guys. The issue turned out to be that the jar containing the pigmix classes was compiled against hadoop-2 and was being run against hadoop-1. After building 2 separate jars 1 against hadoop-1 and another against hadoop-2 and using the appropriate one solved the issue. ,,,,,,,,,,,,,,,,,
Pig should use job's jobClient wherever possible (fixes local mode counters),PIG-3913,12711345,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,30/Apr/14 00:13,07/Jul/14 18:08,14/Mar/19 03:08,20/May/14 17:50,0.13.0,,,,,0.13.0,,,,,1,,,,,,,MapreduceLauncher initializes a statsJobClient to poll counter information of jobs. This works fine in mapreduce mode but it reports incorrect information in local (auto-local) mode. Pig code should try to use org.apache.hadoop.mapred.jobcontrol.Job's getJobClient api to get handle to jobClient wherever possible. statsJobClient (and wherever its references are passed) should be deprecated.,,,,,,,,,,,,,,,PIG-3958,,,,,30/Apr/14 00:17;aniket486;PIG-3913-1.patch;https://issues.apache.org/jira/secure/attachment/12642568/PIG-3913-1.patch,09/May/14 22:39;aniket486;PIG-3913-3.patch;https://issues.apache.org/jira/secure/attachment/12644202/PIG-3913-3.patch,12/May/14 17:37;aniket486;PIG-3913-5.patch;https://issues.apache.org/jira/secure/attachment/12644442/PIG-3913-5.patch,14/May/14 18:46;aniket486;PIG-3913-6.patch;https://issues.apache.org/jira/secure/attachment/12644866/PIG-3913-6.patch,20/May/14 17:46;aniket486;PIG-3913-7.patch;https://issues.apache.org/jira/secure/attachment/12645834/PIG-3913-7.patch,07/May/14 22:47;jira.shegalov;PIG-3913-MRv2.v01.patch;https://issues.apache.org/jira/secure/attachment/12643869/PIG-3913-MRv2.v01.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2014-04-30 00:24:38.532,,,no_permission,,,,,,,,,,,,,389666,,,,,Tue May 20 17:50:33 UTC 2014,,,,,,,0|i1v4br:,389908,,,,,,,,,,"30/Apr/14 00:24;julienledem;This looks good to me.
Please add javadoc for deprecated methods with information about the new way of doing the same thing.
+1","30/Apr/14 18:07;aniket486;Thanks for the review, [~julienledem]. I realized that we have some code in pig that will avoiding printing counters in local mode (since they were broken). Let me clean up that code so that counters are available again with local mode.

Also, I will test with hadoop 2 to see if this fix works with it.

[~daijy], any known issues I should be aware of here?","07/May/14 22:47;jira.shegalov;Hi [~aniket486], I am uploading a patch that I think demonstrates how it should work with MRv2. It won't compile with hadoop20. ","08/May/14 00:39;aniket486;Thanks [~jira.shegalov], this looks great. Let's wrap it up and commit it.","08/May/14 21:38;aniket486;Looking at this, it seems main difference is - in hadoop 1, jobcontrol.Job does not support an api to get the mapreduce/mapred.Job object from it (in fact it does not even store it). In hadoop 2, jobcontrol.Job is just a wrapper around ControlledJob which stores a pointer to the real mapreduce.Job. In hadoop 1 for querying stats, we should get jobClient of the job and ask for the runningJob given the JobID for the job. In hadoop 2, we should directly the real job using getJob of jobcontrol.Job and query it for stats.

I will construct necessary shims to hide this behind interfaces.",09/May/14 22:39;aniket486;Added a new patch for this.,"10/May/14 22:00;jira.shegalov;[~aniket486], PIG-3913-3 patch is incomplete, DowngradeHelper.java is missing.",12/May/14 17:37;aniket486;I've attached PIG-3913-5.patch.,13/May/14 22:38;jcoveney;Can you throw this in the apache reviewboard? #highmaintenace,14/May/14 18:46;aniket486;#GoodIdea: https://reviews.apache.org/r/21454/,16/May/14 04:06;jira.shegalov;+1 (non-binding) modulo nits on RB,"19/May/14 21:59;cheolsoo;+1. Let's commit this.

Btw, the patch needs to be rebased to trunk. In addition, {{hadoop23/HadoopShims.java}} has unused imports. Please fix them when committing the patch.

Thanks!",19/May/14 22:12;aniket486;Thanks [~jira.shegalov] and [~cheolsoo] for the review. I will incorporate your comments and commit this to trunk!,20/May/14 17:50;aniket486;Committed to trunk! (review updated and patch attached).,,,,,,,,,,,,,,
Type Casting issue,PIG-3909,12709959,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,polisan,polisan,23/Apr/14 07:35,07/Jul/14 18:08,14/Mar/19 03:08,25/Apr/14 20:39,0.11.1,0.12.0,,,,0.13.0,,parser,,,0,,,,,,,"  This issue is very close to https://issues.apache.org/jira/browse/PIG-1191, which had been closed for version 0.6.0. Steps to reproduce the issue:

  Pig script as below:
{code:title=input7.pig}
A = load 'polisan/input7.txt' as (bagofmap:{});
B = foreach A generate FLATTEN((IsEmpty(bagofmap) ? null : bagofmap)) AS bagofmap;
C = filter B by (chararray)bagofmap#'fieldkey1' matches 'po.*';
D = foreach C generate (chararray)bagofmap#'fieldkey2';
dump D;
{code}

  input data as below: 
{code:title=polisan/input7.txt}
{([fieldkey1#polisan,fieldkey2#lily])}
{code}

  run command ""pig -x local -f input7.pig"".  Exception will be thrown out like below:
{noformat}
org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine how to convert the bytearray to string.
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:935)
{noformat}

  I tried to dig into the source code, and found there were something wrong with generation of Logical Plan(LP), ""new TypeCheckingRelVisitor( lp, collector).visit();"" particularly. For the pig script I pasted in the ticket, logical plan was like this,

{noformat}
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
D: (Name: LOStore Schema: #21:chararray)
|
|---D: (Name: LOForEach Schema: #21:chararray)
    |   |
    |   (Name: LOGenerate[false] Schema: #21:chararray)
    |   |   |
    |   |   (Name: Cast Type: chararray Uid: 21)
    |   |   |
    |   |   |---(Name: Map Type: bytearray Uid: 21 Key: fieldkey2)
    |   |       |
    |   |       |---(Name: Cast Type: map Uid: 16)
    |   |           |
    |   |           |---bagofmap:(Name: Project Type: bytearray Uid: 16 Input: 0 Column: (*))
    |   |
    |   |---(Name: LOInnerLoad[0] Schema: bagofmap#16:bytearray)
    |
    |---C: (Name: LOFilter Schema: bagofmap#16:bytearray)
        |   |
        |   (Name: Regex Type: boolean Uid: 20)
        |   |
        |   |---(Name: Cast Type: chararray Uid: 17)
        |   |   |
        |   |   |---(Name: Map Type: bytearray Uid: 17 Key: fieldkey1)
        |   |       |
        |   |       |---(Name: Cast Type: map Uid: 26)        ——> Uid was assigned to 26, while other  places were 16
        |   |           |
        |   |           |---bagofmap:(Name: Project Type: bytearray Uid: 16 Input: 0 Column: 0)
        |   |
        |   |---(Name: Constant Type: chararray Uid: 19)
        |
        |---B: (Name: LOForEach Schema: bagofmap#16:bytearray)
            |   |
            |   (Name: LOGenerate[true] Schema: bagofmap#16:bytearray)
            |   |   |
            |   |   (Name: BinCond Type: bag Uid: 16)
            |   |   |
            |   |   |---(Name: UserFunc(org.apache.pig.builtin.IsEmpty) Type: boolean Uid: 14)
            |   |   |   |
            |   |   |   |---bagofmap:(Name: Project Type: bag Uid: 12 Input: 0 Column: (*))
            |   |   |
            |   |   |---(Name: Cast Type: bag Uid: 15)
            |   |   |   |
            |   |   |   |---(Name: Constant Type: bytearray Uid: 15)
            |   |   |
            |   |   |---bagofmap:(Name: Project Type: bag Uid: 12 Input: 1 Column: (*))
            |   |
            |   |---bagofmap: (Name: LOInnerLoad[0] Schema: null)
            |   |
            |   |---bagofmap: (Name: LOInnerLoad[0] Schema: null)
            |
            |---A: (Name: LOLoad Schema: bagofmap#12:bag{#13:tuple()})RequiredFields:null
{noformat}

    I followed the code, and found at first all uid of bagofmap were all 16, then TypeCheckingRelVisitor.visit() was called, some cast were added, e.g., to cast bagofmap from bytearray to map, at the same time, uid were also recaculated. When alias C was processed, uid of bagofmap(bytearray type) was changed to 26, and bagofmap in inserted CastExpression was also assigned 26. While processing D, the foreach sentence, bagofmap in project expression was merged back into 16, while other bagofmap of bytearray were sharing the schema object, leaving the one of map type in filter-sentence 26. This leaded to, loadFunction for uid 26 was missing in uid2LoadFuncMap, then caster was assigned to null, and then the exception at last.

  I tried serveral ways to make the code go well.
1) add implementation of function visit(CastException) for class LineageFindExpVisitor, to add <26, org.apache.pig.builtin.PigStorage()> to uid2LoadFuncMap, then caster will be assigned with right function. 
{code:java}
        @Override
        public void visit(CastExpression cast) throws FrontendException {
        updateUidMap(cast, cast.getExpression());        
        }
{code}
2)  to hack code of function getFieldSchema() of class ProjectExpression, to make sure when uid of bagofmap were re-caculated, ""26"" would not be merged back to 16, then  <26, org.apache.pig.builtin.PigStorage()> was passed into uid2LoadFuncMap when lineageFinder.visit(); was called to generate the map.

3) run the script in debug mode using Eclipse, and hack the result of mergeUid() to make all uid 26 be merged back to 16, then <16, org.apache.pig.builtin.PigStorage()> in uid2LoadFuncMap would be enough.

  I'm not sure which one should be ok, preferred, or none of them. But I believe LP generated was not correct, and there should be some bug on getFieldSchema() function of ProjectExpression class. Please confirm.

  Besides, I wonder what uidOnlyFieldSchema, and fieldSchema mean, and their difference exactly for LogicalExpression, and then I could understand better implementation of getFieldSchema(), when cloneUid() should be called, and when mergeUid() should be called, and when getNextUid().

  Thanks.

  
",,,,,,,,,,,,,,,,,,,,24/Apr/14 22:42;daijy;PIG-3909-1.patch;https://issues.apache.org/jira/secure/attachment/12641813/PIG-3909-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-24 22:42:23.472,,,no_permission,,,,,,,,,,,,,388281,Reviewed,,,,Mon Apr 28 06:00:09 UTC 2014,,,,,,,0|i1uvvz:,388536,,,,,,,,,,24/Apr/14 22:42;daijy;In ProjectExpression we forget clone the fieldSchema. Attach the patch.,25/Apr/14 17:46;rohini;+1,25/Apr/14 20:39;daijy;Patch committed to trunk. Thanks Rohini for review!,"28/Apr/14 04:03;polisan;hi, there,
  after patching, i got LP like this,
<noformat>
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
D: (Name: LOStore Schema: #21:chararray)
|
|---D: (Name: LOForEach Schema: #21:chararray)
    |   |
    |   (Name: LOGenerate[false] Schema: #21:chararray)
    |   |   |
    |   |   (Name: Cast Type: chararray Uid: 21)
    |   |   |
    |   |   |---(Name: Map Type: bytearray Uid: 21 Key: fieldkey2)
    |   |       |
    |   |       |---(Name: Cast Type: map Uid: 16)
    |   |           |
    |   |           |---bagofmap:(Name: Project Type: bytearray Uid: 16 Input: 0 Column: (*))
    |   |
    |   |---(Name: LOInnerLoad[0] Schema: bagofmap#26:bytearray)
    |
    |---C: (Name: LOFilter Schema: bagofmap#26:bytearray)
        |   |
        |   (Name: Regex Type: boolean Uid: 20)
        |   |
        |   |---(Name: Cast Type: chararray Uid: 17)
        |   |   |
        |   |   |---(Name: Map Type: bytearray Uid: 17 Key: fieldkey1)
        |   |       |
        |   |       |---(Name: Cast Type: map Uid: 26)
        |   |           |
        |   |           |---bagofmap:(Name: Project Type: bytearray Uid: 26 Input: 0 Column: 0)
        |   |
        |   |---(Name: Constant Type: chararray Uid: 19)
        |
        |---B: (Name: LOForEach Schema: bagofmap#26:bytearray)
            |   |
            |   (Name: LOGenerate[true] Schema: bagofmap#26:bytearray)
            |   |   |
            |   |   (Name: BinCond Type: bag Uid: 16)
            |   |   |
            |   |   |---(Name: UserFunc(org.apache.pig.builtin.IsEmpty) Type: boolean Uid: 14)
            |   |   |   |
            |   |   |   |---bagofmap:(Name: Project Type: bag Uid: 12 Input: 0 Column: (*))
            |   |   |
            |   |   |---(Name: Cast Type: bag Uid: 15)
            |   |   |   |
            |   |   |   |---(Name: Constant Type: bytearray Uid: 15)
            |   |   |
            |   |   |---bagofmap:(Name: Project Type: bag Uid: 12 Input: 1 Column: (*))
            |   |
            |   |---bagofmap: (Name: LOInnerLoad[0] Schema: null)
            |   |
            |   |---bagofmap: (Name: LOInnerLoad[0] Schema: null)
            |
            |---A: (Name: LOLoad Schema: bagofmap#12:bag{#13:tuple()})RequiredFields:null

<noformat>

   then we can see, for alias C, D, the uid for bagofmap are different, 26 for C and 16 for D. are you sure this is correct?

Thanks.","28/Apr/14 04:06;polisan;sorry for wrong format. reattach the LP.
{noformat}
#-----------------------------------------------
# New Logical Plan:
#-----------------------------------------------
D: (Name: LOStore Schema: #21:chararray)
|
|---D: (Name: LOForEach Schema: #21:chararray)
    |   |
    |   (Name: LOGenerate[false] Schema: #21:chararray)
    |   |   |
    |   |   (Name: Cast Type: chararray Uid: 21)
    |   |   |
    |   |   |---(Name: Map Type: bytearray Uid: 21 Key: fieldkey2)
    |   |       |
    |   |       |---(Name: Cast Type: map Uid: 16)
    |   |           |
    |   |           |---bagofmap:(Name: Project Type: bytearray Uid: 16 Input: 0 Column: (*))
    |   |
    |   |---(Name: LOInnerLoad[0] Schema: bagofmap#26:bytearray)
    |
    |---C: (Name: LOFilter Schema: bagofmap#26:bytearray)
        |   |
        |   (Name: Regex Type: boolean Uid: 20)
        |   |
        |   |---(Name: Cast Type: chararray Uid: 17)
        |   |   |
        |   |   |---(Name: Map Type: bytearray Uid: 17 Key: fieldkey1)
        |   |       |
        |   |       |---(Name: Cast Type: map Uid: 26)
        |   |           |
        |   |           |---bagofmap:(Name: Project Type: bytearray Uid: 26 Input: 0 Column: 0)
        |   |
        |   |---(Name: Constant Type: chararray Uid: 19)
        |
        |---B: (Name: LOForEach Schema: bagofmap#26:bytearray)
            |   |
            |   (Name: LOGenerate[true] Schema: bagofmap#26:bytearray)
            |   |   |
            |   |   (Name: BinCond Type: bag Uid: 16)
            |   |   |
            |   |   |---(Name: UserFunc(org.apache.pig.builtin.IsEmpty) Type: boolean Uid: 14)
            |   |   |   |
            |   |   |   |---bagofmap:(Name: Project Type: bag Uid: 12 Input: 0 Column: (*))
            |   |   |
            |   |   |---(Name: Cast Type: bag Uid: 15)
            |   |   |   |
            |   |   |   |---(Name: Constant Type: bytearray Uid: 15)
            |   |   |
            |   |   |---bagofmap:(Name: Project Type: bag Uid: 12 Input: 1 Column: (*))
            |   |
            |   |---bagofmap: (Name: LOInnerLoad[0] Schema: null)
            |   |
            |   |---bagofmap: (Name: LOInnerLoad[0] Schema: null)
            |
            |---A: (Name: LOLoad Schema: bagofmap#12:bag{#13:tuple()})RequiredFields:null


{noformat}","28/Apr/14 06:00;daijy;Yes, the explain plan does not look right on trunk due to PIG-3508, we need to checkin PIG-3545 to bring it right. The plan shown lacks another round of uid regeneration (uid for D need to be regenerated).",,,,,,,,,,,,,,,,,,,,,,
ant site errors out,PIG-3906,12709612,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nielsbasjes,cos,cos,21/Apr/14 19:53,17/Jul/16 08:00,14/Mar/19 03:08,11/Apr/16 20:58,0.12.1,0.15.0,,,,0.16.0,,build,site,,0,,,,,,,"While running 

{noformat}
ant -Djavac.version=1.6 -Dforrest.home=/usr/local/apache-forrest -Dversion=0.12.1 -Dhadoopversion=23 -buildfile contrib/zebra/build.xml clean jar
{noformat}

site target errors out (see the comment for detailed message)",,,,,,,,,,,,,,,BIGTOP-1303,,,,,08/Apr/16 22:23;nielsbasjes;PIG-3906-2016-04-08.patch;https://issues.apache.org/jira/secure/attachment/12797809/PIG-3906-2016-04-08.patch,24/Feb/16 11:10;nielsbasjes;PIG-3906-Disable-PDF.patch;https://issues.apache.org/jira/secure/attachment/12789537/PIG-3906-Disable-PDF.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-05-09 03:39:07.178,,,no_permission,,,,,,,,,,,,,387934,Reviewed,,,,Sun Jul 17 07:59:10 UTC 2016,,,,,,,0|i1utsn:,388194,,,,,,,,,,"21/Apr/14 19:55;cos;{noformat}
     [exec]
     [exec] Static site will be generated at:
     [exec] /home/cos/work/bigtop/build/pig/rpm/BUILD/pig-0.12.1/src/docs/build/site
     [exec]
     [exec] Cocoon will report the status of each document:
     [exec]   - in column 1: *=okay X=brokenLink ^=pageSkipped (see FAQ).
     [exec]
     [exec] Warning: /home/cos/work/bigtop/build/pig/rpm/BUILD/pig-0.12.1/src/docs/src/documentation/skins/common not found.
     [exec] Warning: /home/cos/work/bigtop/build/pig/rpm/BUILD/pig-0.12.1/src/docs/src/documentation/skins/pelt not found.
     [exec] ------------------------------------------------------------------------
     [exec] cocoon 2.1.12-dev
     [exec] Copyright (c) 1999-2007 Apache Software Foundation. All rights reserved.
     [exec] ------------------------------------------------------------------------
     [exec]
     [exec]
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [1/24]    [24/26]   1.918s 8.0Kb   linkmap.html
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [2/24]    [1/58]    6.468s 281.4Kb basic.html
     [exec] X [0]                                     linkmap.pdf       BROKEN: No pipeline matched request: linkmap.pdf
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [5/22]    [1/33]    0.46s  64.5Kb  cont.html
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [6/23]    [2/27]    0.27s  7.6Kb   index.html
     [exec] * [7/22]    [0/0]     0.172s 1.2Kb   skin/print.css
     [exec] * [8/21]    [0/0]     0.04s  1.8Kb   images/built-with-forrest-button.png
     [exec] * [9/20]    [0/0]     0.033s 4.5Kb   skin/profile.css
     [exec] X [0]                                     cont.pdf  BROKEN: No pipeline matched request: cont.pdf
     [exec] * [11/18]   [0/0]     0.02s  9.2Kb   images/hadoop-logo.jpg
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [13/17]   [1/57]    0.269s 40.3Kb  start.html
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [14/17]   [1/505]   0.311s 50.6Kb  pig-index.html
     [exec] X [0]                                     pig-index.pdf     BROKEN: No pipeline matched request: pig-index.pdf
     [exec] * [16/15]   [0/0]     0.021s 5.2Kb   images/pig-logo.gif
     [exec] * [18/13]   [0/0]     0.333s 348b    skin/images/rc-b-l-15-1body-2menu-3menu.png
     [exec] X [0]                                     start.pdf BROKEN: No pipeline matched request: start.pdf
     [exec] X [0]                                     basic.pdf BROKEN: No pipeline matched request: basic.pdf
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [21/11]   [1/26]    0.362s 110.6Kb udf.html
     [exec] X [0]                                     udf.pdf   BROKEN: No pipeline matched request: udf.pdf
     [exec] X [0]                                     index.pdf BROKEN: No pipeline matched request: index.pdf
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [25/8]    [1/26]    0.102s 6.7Kb   admin.html
     [exec] X [0]                                     admin.pdf BROKEN: No pipeline matched request: admin.pdf
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [27/7]    [1/55]    0.193s 59.6Kb  perf.html
     [exec] X [0]                                     perf.pdf  BROKEN: No pipeline matched request: perf.pdf
     [exec] * [29/18]   [13/13]   0.095s 12.3Kb  skin/screen.css
     [exec] * [30/17]   [0/0]     0.016s 199b    skin/images/rc-t-l-5-1header-2tab-unselected-3tab-unselected.png
     [exec] * [32/15]   [0/0]     0.013s 209b    skin/images/rc-t-l-5-1header-2tab-selected-3tab-selected.png
     [exec] * [34/13]   [0/0]     0.0090s 214b    skin/images/rc-t-r-5-1header-2searchbox-3searchbox.png
     [exec] * [35/12]   [0/0]     0.01s  390b    skin/images/rc-t-r-15-1body-2menu-3menu.png
     [exec] * [36/11]   [0/0]     0.011s 200b    skin/images/rc-b-r-5-1header-2tab-selected-3tab-selected.png
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [37/11]   [1/32]    0.262s 27.8Kb  cmds.html
     [exec] X [0]                                     cmds.pdf  BROKEN: No pipeline matched request: cmds.pdf
     [exec] * [39/9]    [0/0]     0.029s 2.9Kb   skin/basic.css
     [exec] * [40/8]    [0/0]     0.013s 215b    skin/images/rc-t-r-5-1header-2tab-selected-3tab-selected.png
     [exec] * [41/7]    [0/0]     0.015s 214b    skin/images/rc-t-r-5-1header-2tab-unselected-3tab-unselected.png
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [42/7]    [1/39]    2.293s 162.0Kb func.html
     [exec] X [0]                                     func.pdf  BROKEN: No pipeline matched request: func.pdf
     [exec] * [45/4]    [0/0]     0.015s 319b    skin/images/rc-b-r-15-1body-2menu-3menu.png
     [exec] * [46/3]    [0/0]     0.0080s 199b    skin/images/rc-t-l-5-1header-2searchbox-3searchbox.png
     [exec] * [47/2]    [0/0]     0.0060s 285b    images/instruction_arrow.png
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [48/2]    [1/29]    0.191s 41.2Kb  test.html
     [exec] X [0]                                     test.pdf  BROKEN: No pipeline matched request: test.pdf
     [exec] Java Result: 1
     [exec]
     [exec] BUILD FAILED
     [exec] /usr/local/apache-forrest/main/targets/site.xml:224: Error building site.
     [exec]
     [exec] There appears to be a problem with your site build.
     [exec]
     [exec] Read the output above:
     [exec] * Cocoon will report the status of each document:
     [exec]     - in column 1: *=okay X=brokenLink ^=pageSkipped (see FAQ).
     [exec] * Even if only one link is broken, you will still get ""failed"".
     [exec] * Your site would still be generated, but some pages would be broken.
     [exec]   - See /home/cos/work/bigtop/build/pig/rpm/BUILD/pig-0.12.1/src/docs/build/site/broken-links.xml

     [exec] Total time: 18 seconds
     [exec] Total time: 0 minutes 15 seconds,  Site size: 922,708 Site pages: 30

{noformat}
",08/May/14 22:54;cos;Any chance someone familiar with the matter can take a look? This blocks Bigtop release 0.8.0 completely ;(,"09/May/14 03:39;cheolsoo;Sorry for the late reply. I can't reproduce the error. I am running this command, and it works fine on my Ubuntu/Mac-
{code}
ant -Djavac.version=1.6 -Dforrest.home=/home/cheolsoo/workspace/apache-forrest-0.9 -Dversion=0.12.1 -Dhadoopversion=23 docs
{code}","09/May/14 17:51;cos;Weird, cause reproduce it 100% on three different systems, including CentOs and Ubuntu.",09/May/14 17:51;cos;That is with very same command and versions of the Forrest and javac. I am building against Hadoop 2.3.0,"11/May/14 00:56;cos;According to this [comment|https://issues.apache.org/jira/browse/BIGTOP-1303?focusedCommentId=13993971&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13993971] bumping up forrest's heap side is seen to get the issue goes away. Unfortunately, not for me - I keep seen this problem ;(","13/May/14 00:20;dougc33333;Can't repoduce error. My build is here, different than cos' output

[exec] Static site will be generated at:
     [exec] /home/dc/bigtop-0.7.0/dl/tmppig/pig-0.12.1/src/docs/build/site
     [exec] 
     [exec] Cocoon will report the status of each document:
     [exec]   - in column 1: *=okay X=brokenLink ^=pageSkipped (see FAQ).
     [exec]   
     [exec] ------------------------------------------------------------------------ 
     [exec] cocoon 2.1.12-dev
     [exec] Copyright (c) 1999-2007 Apache Software Foundation. All rights reserved.
     [exec] ------------------------------------------------------------------------ 
     [exec] 
     [exec] 
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [1/24]    [24/26]   2.134s 8.0Kb   linkmap.html
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [2/24]    [1/33]    1.415s 64.5Kb  cont.html
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [3/24]    [1/55]    0.739s 59.6Kb  perf.html
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [4/24]    [1/58]    4.278s 281.4Kb basic.html
     [exec] * [5/23]    [0/0]     0.261s 1.2Kb   skin/print.css
     [exec] * [6/22]    [0/0]     0.901s 9.8Kb   linkmap.pdf
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [7/23]    [2/27]    0.117s 7.6Kb   index.html
     [exec] * [8/22]    [0/0]     0.099s 8.7Kb   index.pdf
     [exec] WARN - Line 1 of a paragraph overflows the available area by 1750mpt. (fo:block, ""chararray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 1760mpt. (fo:block, ""bytearray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 1750mpt. (fo:block, ""chararray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 1760mpt. (fo:block, ""bytearray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 1750mpt. (fo:block, ""chararray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 1760mpt. (fo:block, ""bytearray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 1750mpt. (fo:block, ""chararray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 1760mpt. (fo:block, ""bytearray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 5841mpt. (fo:block, ""chararray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 5851mpt. (fo:block, ""bytearray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 5841mpt. (fo:block, ""chararray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 5851mpt. (fo:block, ""bytearray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 4078mpt. (fo:block, ""double"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 14608mpt. (fo:block, ""chararray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 14618mpt. (fo:block, ""bytearray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 11298mpt. (fo:block, ""datetime"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 17408mpt. (fo:block, ""biginteger"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 21298mpt. (fo:block, ""bigdecimal"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 3 of a paragraph overflows the available area by 8518mpt. (fo:block, ""cast as boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 3 of a paragraph overflows the available area by 8518mpt. (fo:block, ""cast as boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 3 of a paragraph overflows the available area by 8518mpt. (fo:block, ""cast as boolean  "")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 4078mpt. (fo:block, ""double"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 3 of a paragraph overflows the available area by 8518mpt. (fo:block, ""cast as boolean  "")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 14608mpt. (fo:block, ""chararray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 3 of a paragraph overflows the available area by 8518mpt. (fo:block, ""cast as boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 14618mpt. (fo:block, ""bytearray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 11298mpt. (fo:block, ""datetime"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 17408mpt. (fo:block, ""biginteger"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 21298mpt. (fo:block, ""bigdecimal"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 4078mpt. (fo:block, ""double"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 14608mpt. (fo:block, ""chararray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 14618mpt. (fo:block, ""bytearray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 11298mpt. (fo:block, ""datetime"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 17408mpt. (fo:block, ""biginteger"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 21298mpt. (fo:block, ""bigdecimal"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 2 of a paragraph overflows the available area by 17948mpt. (fo:block, ""boolean (bytearray cast as int)"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean (bytearray cast as int)"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 2 of a paragraph overflows the available area by 17948mpt. (fo:block, ""boolean (bytearray cast as long)"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean (bytearray cast as long)"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 2 of a paragraph overflows the available area by 17948mpt. (fo:block, ""boolean (bytearray cast as float)"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean (bytearray cast as float)"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 4078mpt. (fo:block, ""double"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 5 of a paragraph overflows the available area by 7408mpt. (fo:block, ""boolean (bytearray cast as double)"")
     [exec] WARN - Line 2 of a paragraph overflows the available area by 17948mpt. (fo:block, ""boolean (bytearray cast as double)"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean (bytearray cast as double)"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 14608mpt. (fo:block, ""chararray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 5 of a paragraph overflows the available area by 17938mpt. (fo:block, ""boolean (bytearray cast as chararray)"")
     [exec] WARN - Line 2 of a paragraph overflows the available area by 17948mpt. (fo:block, ""boolean (bytearray cast as chararray)"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean (bytearray cast as chararray)"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 14618mpt. (fo:block, ""bytearray"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 11298mpt. (fo:block, ""datetime"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 17408mpt. (fo:block, ""biginteger"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 21298mpt. (fo:block, ""bigdecimal"")
     [exec] WARN - Line 1 of a paragraph overflows the available area by 8518mpt. (fo:block, ""boolean"")
     [exec] WARN - Line 2 of a paragraph overflows the available area by 4020mpt. (fo:block, ""This works, hadoop fs -ls /mydata/20110423{00,01,02,03,04,05,06,07,08)
     [exec] WARN - Line 2 of a paragraph overflows the available area by 4020mpt. (fo:block, ""This does not work, LOAD '/mydata/20110423{00,01,02,03,04,05,06,07,08)
     [exec] * [10/20]   [0/0]     7.52s  271.1Kb basic.pdf
     [exec] * [11/19]   [0/0]     0.261s 2.9Kb   skin/basic.css
     [exec] * [12/18]   [0/0]     0.511s 348b    skin/images/rc-b-l-15-1body-2menu-3menu.png
     [exec] * [13/17]   [0/0]     0.055s 4.5Kb   skin/profile.css
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [14/17]   [1/57]    0.254s 40.3Kb  start.html
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [15/17]   [1/26]    0.25s  110.6Kb udf.html
     [exec] * [16/29]   [13/13]   0.208s 12.3Kb  skin/screen.css
     [exec] * [17/28]   [0/0]     0.015s 209b    skin/images/rc-t-l-5-1header-2tab-selected-3tab-selected.png
     [exec] * [20/25]   [0/0]     0.011s 200b    skin/images/rc-b-r-5-1header-2tab-selected-3tab-selected.png
     [exec] * [21/24]   [0/0]     0.772s 77.0Kb  cont.pdf
     [exec] * [22/23]   [0/0]     0.011s 199b    skin/images/rc-t-l-5-1header-2tab-unselected-3tab-unselected.png
     [exec] * [23/22]   [0/0]     0.013s 214b    skin/images/rc-t-r-5-1header-2tab-unselected-3tab-unselected.png
     [exec] * [26/19]   [0/0]     0.014s 214b    skin/images/rc-t-r-5-1header-2searchbox-3searchbox.png
     [exec] * [28/17]   [0/0]     0.015s 199b    skin/images/rc-t-l-5-1header-2searchbox-3searchbox.png
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [30/16]   [1/26]    0.124s 6.7Kb   admin.html
     [exec] * [31/15]   [0/0]     0.043s 8.4Kb   admin.pdf
     [exec] * [33/13]   [0/0]     0.0090s 319b    skin/images/rc-b-r-15-1body-2menu-3menu.png
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [34/13]   [1/29]    0.215s 41.2Kb  test.html
     [exec] * [35/12]   [0/0]     0.793s 123.1Kb udf.pdf
     [exec] * [37/10]   [0/0]     0.024s 390b    skin/images/rc-t-r-15-1body-2menu-3menu.png
     [exec] WARN - Line 1 of a paragraph overflows the available area by more than 50 points. (fo:block, ""grunt> visits = LOAD 'visits.txt' AS (user:chararray, url:chararray, )
     [exec] WARN - Line 1 of a paragraph overflows the available area by more than 50 points. (fo:block, ""grunt> visits = LOAD 'visits.txt' AS (user:chararray, url:chararray, )
     [exec] WARN - Line 1 of a paragraph overflows the available area by more than 50 points. (fo:block, ""grunt> visits = LOAD 'visits.txt' AS (user:chararray, url:chararray, )
     [exec] * [38/9]    [0/0]     0.262s 50.5Kb  test.pdf
     [exec] * [39/8]    [0/0]     0.022s 215b    skin/images/rc-t-r-5-1header-2tab-selected-3tab-selected.png
     [exec] * [41/6]    [0/0]     0.354s 54.5Kb  start.pdf
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [43/5]    [1/32]    0.366s 27.8Kb  cmds.html
     [exec] * [44/4]    [0/0]     0.17s  38.5Kb  cmds.pdf
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [46/3]    [1/39]    3.019s 162.0Kb func.html
     [exec] * [47/2]    [0/0]     1.507s 218.6Kb func.pdf
     [exec] ^                                    api/
     [exec] ^                                    jdiff/changes.html
     [exec] * [48/2]    [1/505]   0.391s 50.6Kb  pig-index.html
     [exec] * [49/1]    [0/0]     0.3s   145.7Kb pig-index.pdf
     [exec] * [50/0]    [0/0]     0.365s 81.6Kb  perf.pdf
     [exec] Total time: 0 minutes 29 seconds,  Site size: 2,019,835 Site pages: 38
     [exec] 
     [exec]   Copying broken links file to site root.
     [exec]       
     [exec] Copying 1 file to /home/dc/bigtop-0.7.0/dl/tmppig/pig-0.12.1/src/docs/build/site
     [exec] 
     [exec] -----------------------------
     [exec] Static site was successfully generated at:
     [exec] /home/dc/bigtop-0.7.0/dl/tmppig/pig-0.12.1/src/docs/build/site
     [exec] ------------------------------
     [exec]     
     [exec] 
     [exec] BUILD SUCCESSFUL
     [exec] Total time: 33 seconds
     [copy] Copying 88 files to /home/dc/bigtop-0.7.0/dl/tmppig/pig-0.12.1/build/docs
     [copy] Copied 7 empty directories to 3 empty directories under /home/dc/bigtop-0.7.0/dl/tmppig/pig-0.12.1/build/docs

BUILD SUCCESSFUL
Total time: 1 minute 52 seconds
[dc@localhost pig-0.12.1]$ 
",13/May/14 01:04;dougc33333;OK I get it.. cos is right. Can't repo the exact error but there is a diff between building using the pig do-component-build and without,01/May/15 21:31;nielsbasjes;I can reproduce the errors I see in this issue using the patch I created for PIG-4526. Simply start the docker based environment and run {code}ant docs{code}.,"01/May/15 21:39;cos;Are you saying that docs target needs to be executed separately now? Bigtop creates installable packages for Pig, so we need to build binaries/libs as well. Or are you saying that the build needs to be run in a container? Sorry - I am a bit confused; could you clarify please? Thanks!","02/May/15 14:09;nielsbasjes;In this issue I see several comments indicating a ""cannot reproduce"" which apparently caused this issue to remain open. What I intended to say is that I can reproduce the mentioned error messages by patching the pig source with the patch in PIG-4526 ( which contains a docker based build environment)  and simply run {code}ant docs{code} to reproduce the problem.
Using this docker based setup the build environment in which to reproduce such problems will be identical for everyone , instead of depending on what has been locally installed over the last few years (which will be different for everyone) 
I expect that using this as a reproduction path we can converge much better towards a solution. ","03/May/15 18:26;cos;Ah, great! Thank you very much for finding a way to repro it consistently!","24/Feb/16 11:10;nielsbasjes;I just checked and this problem is still reproducible using the current trunk.
Apparently this problem is related to forrest and generating pdf files.
So I create a rather brutal fix (workaround) to stop this error from happening: 
Simply disable the PDF link at the top right of all pages.

The main question is: Is this an acceptable way of fixing this issue?
","24/Feb/16 11:12;nielsbasjes;Is removing the PDFs an acceptable way of fixing this problem ?
","08/Apr/16 14:12;nielsbasjes;I found the reason why it was failing on my system: 
The 'plugins' directory in the Apache Forrest installation that I was using must be WRITABLE for the user running the task.
This is needed because the pdf plugin needs to be installed in there.

Can you guys please double check if this helps on your systems also?","08/Apr/16 22:23;nielsbasjes;The cause of the bug report is a problem in the local build environment.

This patch will trigger a failure of the build and give a clear explanation when trying to build the documentation and the problem situation (the plugins directory is not writable) is present.",11/Apr/16 20:58;daijy;Patch committed to trunk. Thanks Niels for tracing it down!,"17/Jul/16 07:59;daijy;This patch needs to be reverted. We can get false alarm when the plugin is installed but the dir is not writable (eg, the user issue ant command don't have write privilege of forrest plugin dir).

And with PIG-4526, the issue the original patch try to address become less an issue.",,,,,,,,,,
0.12.1 release can't be build for Hadoop2,PIG-3905,12709375,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,daijy,cos,cos,18/Apr/14 23:41,07/Jul/14 18:08,14/Mar/19 03:08,24/Apr/14 22:46,0.12.1,,,,,0.13.0,,build,,,0,,,,,,,"While building Pig against {{hadoopversion=23}},  attempts to resolve {{hadoop-core}} dependency no matter what. The root cause of the bug is the following lines in ivy.xml
{noformat}
    <dependency org=""com.sun.jersey"" name=""jersey-core"" rev=""${jersey-core.version}""
      conf=""hadoop20->default""/>
    <dependency org=""org.apache.hadoop"" name=""hadoop-core"" rev=""${hadoop-core.version}""
      conf=""hadoop20->default""/>
    <dependency org=""org.apache.hadoop"" name=""hadoop-test"" rev=""${hadoop-test.version}""
      conf=""hadoop20->default""/>
{noformat}

hadoop20 configuration is getting evaluated _always_. Unless a build machine has dirty local .m2 repository, the build will fail.

The bug blocks BIGTOP-1110",,,,,,,,,,BIGTOP-1278,,,,,,,,,,21/Apr/14 19:13;daijy;PIG-3905-1.patch;https://issues.apache.org/jira/secure/attachment/12641102/PIG-3905-1.patch,23/Apr/14 22:00;daijy;PIG-3905-2.patch;https://issues.apache.org/jira/secure/attachment/12641591/PIG-3905-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-04-19 06:16:51.52,,,no_permission,,,,,,,,,,,,,387697,Reviewed,,,,Thu Apr 24 22:46:30 UTC 2014,,,,,,,0|i1usc7:,387958,,,,,,,,,,19/Apr/14 06:16;daijy;I can build Hadoop2 after wipe out .ivy2 and .m2. Actually there is no change of ivy.xml between 0.12.0 and 0.12.1. Are you able to build Pig 0.12.0?,"19/Apr/14 07:31;cos;I haven't tried 0.12. However, on a clean VM, I am hitting the above issue. And it is gone with removal of these three lines. I don't have that setup handy, but I will supply the environment details tomorrow.","19/Apr/14 23:00;cheolsoo;I think this is a valid bug. Even though the build doesn't fail for me (because hadoop-core-1.0.0.jar is found in maven repo), hadoop20 and hadoop23 confs are always resolved regardless hadoopversion-
{code}
[ivy:resolve] :: resolving dependencies :: org.apache.pig#pig;0.12.2-SNAPSHOT
[ivy:resolve] 	confs: [master, default, runtime, compile, test, javadoc, releaseaudit, jdiff, checkstyle, buildJar, hadoop20, hadoop23, hbase94, hbase95]
{code}
This seems wrong to me.

For example, sqoop only resolves one hadoop conf specified by hadoopversion- 
{code}
[ivy:resolve] :: resolving dependencies :: com.cloudera.sqoop#sqoop;working@lgml-cheolsoop
[ivy:resolve] 	confs: [hadoop20]
{code}

We should fix this. I don't have an explanation why this becomes an issue in 0.12.1 because it has existed since the beginning.",20/Apr/14 18:53;cos;Thanks for confirming. It has became an issue as new Bigtop CI is wiping out ~/.m2 repos before each build now. In other words - we are building a clean room process for the Hadoop stack.,21/Apr/14 19:13;daijy;Attach a patch. Can anyone give a try?,21/Apr/14 19:26;cheolsoo;+1. It works for me. Thanks for the patch Daniel!,21/Apr/14 19:28;cos;That seems to be working ok. Thank you Daniel!,23/Apr/14 22:00;daijy;The previous patch is not proper. conf hadoop20/hadoop23 only resolves Hadoop specific jars. Should use conf compile which extends hadoop20/hadoop23 depends on hadoopversion and include general jars. Attach patch.,24/Apr/14 16:26;cheolsoo;+1. Good catch.,"24/Apr/14 22:46;daijy;Compilation fail only when use a local maven repository. Commit to trunk first.

Also the fix speeds up the compilation. ",,,,,,,,,,,,,,,,,,
PigServer creates cycle,PIG-3902,12709088,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thedatachef,thedatachef,thedatachef,17/Apr/14 16:15,07/Jul/14 18:07,14/Mar/19 03:08,10/May/14 22:48,0.13.0,,,,,0.13.0,,,,,0,,,,,,,"Under certain conditions PigServer creates a cycle in the logical plan. Consider the following pseudocode:

{code}
A = load from 'A' using F1;
...process...
B = store X into 'B' using F2;
C = load from 'B' using F3;
...process...
D = store Y into 'A' using F1;
{code}

PigServer will, in ordinary cases, notice that an output path is equal to an input path, and, if there's no path from the input to the output, make the input a dependency of the output. However, PigServer orders the loads and stores arbitrarily during that logic. Sometimes, in the code above, C is correctly wired as a dependency of B and, since that creates a path from A to D, A won't be made a dependency of D and we're good. On occasion though, the ordering being arbitrary, A is wired as a dependency of D. That's no good. To be fair, it's not actually a cycle, since when A is wired to D, there's a path between C and B so the cycle won't actually get created. But it's still a broken plan.

The offending PigServer code: https://github.com/apache/pig/blob/branch-0.11/src/org/apache/pig/PigServer.java#L1678-L1693

And here's some actual pig code that should reproduce the broken plan. Notice I had to use a store function that wouldn't check the output. If you're just using PigStorage this won't be reproducible since you can't write to the same location you read from in that case.

{code}
A = load '$A' as (line:chararray);
A = foreach A generate flatten(TOKENIZE(LOWER(line))) as token;
store A into '$B';
B = load '$B' as (token:chararray);
B = filter B by SIZE(token) > 3;
store B into '$A' using org.apache.pig.piggybank.storage.DBStorage('com.mysql.jdbc.Driver', 'dbc:mysql://localhost/test', 'INSERT INTO foobar (token) VALUES(?)');
{code}

As far as a fix goes... I'd love some input. I've got some workarounds in mind for the specific use case that brought this up, but the general problem is more difficult. 

As an aside, there's other issues with the PigServer code referenced above. For example, it should almost certainly be using the full path (after LoadFunc/StoreFunc.relativeToAbsolutePath) no? Try storing to a relative path then loading from the absolute representation of that path in the same script... Also, why isn't it checking the FuncSpec as well as the location? Just trying to open up the discussion.",,,,,,,,,,,,,,,,,,,,17/Apr/14 16:16;thedatachef;broken-plan.png;https://issues.apache.org/jira/secure/attachment/12640652/broken-plan.png,17/Apr/14 20:47;thedatachef;cycle.diff;https://issues.apache.org/jira/secure/attachment/12640704/cycle.diff,09/May/14 17:48;thedatachef;multiquery-20140509.diff;https://issues.apache.org/jira/secure/attachment/12644138/multiquery-20140509.diff,05/May/14 21:05;thedatachef;multiquery-cycle.diff;https://issues.apache.org/jira/secure/attachment/12643427/multiquery-cycle.diff,02/May/14 16:45;thedatachef;multiquery-cycle.diff;https://issues.apache.org/jira/secure/attachment/12643070/multiquery-cycle.diff,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-04-21 15:55:30.497,,,no_permission,,,,,,,,,,,,,387411,,,,,Sat May 10 22:48:17 UTC 2014,,,Patch Available,,,,0|i1uqk7:,387673,,,,,,,,,,17/Apr/14 16:16;thedatachef;Here's the lipstick representation of the broken plan using the code provided.,"17/Apr/14 20:47;thedatachef;I'm not happy with this fix as it relies on location in the script. However, when writing a pig script by hand, a developer most often (in fact I can't think of a reason I wouldn't do it this way) puts the store _before_ the load that relies on it in the script. ","21/Apr/14 15:55;cheolsoo;[~thedatachef], your patch breaks the following tests. The area of code that you're modifying seems to affect multi query optimization. Can you take a look?
{code}
>>> org.apache.pig.test.TestMultiQueryBasic.testMultiQueryWithCoGroup_2 	3.7 sec	1
>>> org.apache.pig.test.TestMultiQueryCompiler.testMultiQueryWithCoGroup 	0.13 sec	1
>>> org.apache.pig.test.TestMultiQueryCompiler.testMultiQueryWithIntermediateStores 	0.1 sec	1
>>> org.apache.pig.test.TestMultiQueryCompiler.testStoreOrder 	0.18 sec	1
>>> org.apache.pig.test.TestMultiQueryCompiler.testUnnecessaryStoreRemoval 	0.13 sec	1
>>> org.apache.pig.test.TestMultiQueryCompiler.testUnnecessaryStoreRemovalCollapseSplit 	0.14 sec	1
>>> org.apache.pig.test.TestMultiQueryLocal.testStoreOrder 
{code}
Canceling the patch for now.","21/Apr/14 18:26;thedatachef;Without the location in the script/query being a reliable proxy for order there's really no other way I can think of to fix this. Currently, a workaround is to manually place an exec statement in the script to force the plans to be executed in the right order. This is unfortunate because, as far as I can tell, computation isn't shared across exec statements.","02/May/14 16:44;thedatachef;This goes deep. I updated the LogicalPlanBuilder such that, when building a load op, it checks the plan that's been generated ""so far"" for stores that match the location the load references. If so it links them. 

Some of the multiquery compilation code exploited the <i>bug</i> that plan.getSinks() always returned the stores. You could get away with that before the ""postProcess"" method on PigServer got called because loads were not yet linked to the stores they depended on. Hence, all the stores were leaves of the plan. However, with this patch that bug exploitation will no longer work. In order to get the loads you'll have to get the operators of the plan explicitly and create a list of stores. Presumably this happens elsewhere but I only ran the multiquery tests with my patch.","05/May/14 17:27;cheolsoo;{quote}Presumably this happens elsewhere but I only ran the multiquery tests with my patch.{quote}
Here are unit test failures-
{code}
>>> org.apache.pig.test.TestGrunt.testIllustrate 	0.23 sec	1
>>> org.apache.pig.test.TestNativeMapReduce.testNativeMRJobSimple 	1 min 27 sec	1
>>> org.apache.pig.test.TestNativeMapReduce.testNativeMRJobMultiStoreOnPred 	1 min 26 sec	1
>>> org.apache.pig.test.TestPigStorage.testSchemaConversion2 	6.6 sec	1
>>> org.apache.pig.test.TestPigStorage.testSchemaConversion 
{code}","05/May/14 21:05;thedatachef;Added test, fixed latest round of unit test failures. Should be good to go now. ","08/May/14 01:00;cheolsoo;[~thedatachef], thank you for the patch. Overall looks good to me. I ran full unit tests and verified there's no failure.

I have few minor comments as follows-
# Can you rebase your patch to trunk? We always commit a patch into trunk and, if necessary, we back-port it into a branch. The fixVersion should be also 0.13.
# Can you add a comment that explains what your unit test is for? Possibly, you can describe how it fails in-deterministically without the fix.
# In several places, you're extracting a type of operators from operator plan through a loop. Since this is a common thing to do, I am wondering whether you can write a helper function for that. I have two suggestions-
## Write a function to [PlanHelper.getPhysicalOperators(PhysicalPlan plan, Class<C> opClass)|https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/util/PlanHelper.java#L124] for LogicalPlan, or
## Use Guava [Iterables.filter(Iterable<?> unfiltered, Class<T> type)|http://docs.guava-libraries.googlecode.com/git-history/v11.0.2/javadoc/com/google/common/collect/Iterables.html#filter(java.lang.Iterable, java.lang.Class)].
# In PigServer#skipStores(), can't you change processedStores to LOStore collection? Currently, countExecutedStores() constructs a LOStore collection and counts it. Then, skipStores() constructs another LOStore collection and skips as many LOStores as processedStores. Why not keep the original LOStore collection constructed by countExecutedStores() and remove its entries from plan in skipStores()? I might be misunderstanding the logic.
# In the following code, can't you use FileSpec#equals() instead of String#compareTo()?
{code}
+            String ifile = op.getFileSpec().getFileName();
+            String ofile = store.getFileSpec().getFileName();
+            if (ofile.compareTo(ifile) == 0) { // --> if (load.getFileSpec().equals(store.getFileSpec())) {
+                inputAliases.add( store.getAlias() );
+            }
{code}
# Please remove trailing whitespaces and tabs.","08/May/14 20:57;thedatachef;[~cheolsoo] Regarding #5, FileSpec encapsulates both the location uri as well as the FuncSpec. There are several cases where there is a load that depends on a store where the two FuncSpecs could be different. I do think it should actually be the -full- path resolved via the Load or Store func's 'relativeToAbsolutePath' method, but I'd rather address that with a separate jira.

I'll submit another patch soon that addresses the other points. Thanks!",09/May/14 17:48;thedatachef;Latest patch. Added a utility function for getting operators of a specific type from the logical plan. Builds against trunk.,09/May/14 19:04;cheolsoo;Great! I'll run unit tests with the new patch and commit it. Thank you Jacob!,"10/May/14 22:48;cheolsoo;All unit tests passed. Committed to trunk.

I made one minor change to the final patch. I replaced {{ifile.compareTo(ofile) == 0}} with {{ifile.equals(ofile)}} for clarity. According to the [javadoc|http://docs.oracle.com/javase/7/docs/api/java/lang/String.html#compareTo(java.lang.String)], {{compareTo returns 0 exactly when the equals(Object) method would return true}}.",,,,,,,,,,,,,,,,
Pigmix run script has compilation error,PIG-3895,12708377,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,14/Apr/14 21:25,07/Jul/14 18:07,14/Mar/19 03:08,14/Apr/14 21:41,,,,,,0.13.0,,,,,0,,,,,,,"Scalar found where operator expected at ./bin/runpigmix.pl line 86, near "") 
   $multiplier""
(Missing operator before $multiplier?)
syntax error at ./bin/runpigmix.pl line 86, near "") 
   $multiplier ""
Execution of ./bin/runpigmix.pl aborted due to compilation errors.",,,,,,,,,,,,,,,,,,,,14/Apr/14 21:32;rohini;PIG-3895-1.patch;https://issues.apache.org/jira/secure/attachment/12640149/PIG-3895-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-14 21:29:05.715,,,no_permission,,,,,,,,,,,,,386700,,,,,Mon Apr 14 21:41:48 UTC 2014,,,,,,,0|i1um73:,386964,,,,,,,,,,14/Apr/14 21:29;daijy;+1,14/Apr/14 21:41;rohini;Committed to trunk. Thanks Daniel.,,,,,,,,,,,,,,,,,,,,,,,,,,
"Datetime function AddDuration, SubtractDuration and all Between functions don't check for null values in the input tuple.",PIG-3894,12708375,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jennythompson,jennythompson,jennythompson,14/Apr/14 21:22,07/Jul/14 18:08,14/Mar/19 03:08,21/Apr/14 19:48,,,,,,0.13.0,,,,,0,datetime,easyfix,,,,,"The simple datetime functions: GetMonth, GetHour, ToDate, GetYear etc all have safe treatment of null values in the input tuple.

e.g.
{code:title=GetMonth.java|borderStyle=solid}
@Override
public Integer exec(Tuple input) throws IOException {
    if (input == null || input.size() < 1 || input.get(0) == null) {
        return null;
    }
    return ((DateTime) input.get(0)).getMonthOfYear();
}
{code}

However, all of the ""Duration"" or ""Between"" functions are missing this null-checking, and so fail completely when the are passed tuples with null values.

e.g.
{code:title=AddDuration.java|borderStyle=solid}
@Override
public DateTime exec(Tuple input) throws IOException {
    if (input == null || input.size() < 2) {
        return null;
    }
    return ((DateTime) input.get(0)).plus(new Period((String) input.get(1)));
}
{code}

This is inconsistent, problematic and easy to fix.
",,,,,,,,,,,,,,,,,,,,14/Apr/14 21:28;jennythompson;PIG-3894.patch;https://issues.apache.org/jira/secure/attachment/12640146/PIG-3894.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-21 19:42:42.584,,,no_permission,,,,,,,,,,,,,386698,,,,,Mon Apr 21 19:48:30 UTC 2014,,,,,,,0|i1um6n:,386962,,,,,,,,,,21/Apr/14 19:42;cheolsoo;+1. Will commit it shortly.,21/Apr/14 19:48;cheolsoo;Committed to trunk. Thank you Jenny!,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig distribution for hadoop 2,PIG-3892,12708345,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,14/Apr/14 19:08,12/Jul/14 02:53,14/Mar/19 03:08,20/Jun/14 00:57,,,,,,0.13.0,0.14.0,build,,,0,,,,,,,"Currently Pig distribution only bundle pig.jar for Hadoop 1. For Hadoop 2 users they need to compile again using -Dhadoopversion=23 flag. That is a quite confusing process. We need to make Pig work with Hadoop 2 out of box. I am thinking two approaches:

1. Bundle both pig-h1.jar and pig-h2.jar in distribution, and bin/pig will chose the right pig.jar to run

2. Make two Pig distributions for Hadoop 1 and Hadoop 

Any opinion?",,,,,,,,,,,,,,,,,,,,23/Apr/14 22:08;daijy;PIG-3892-1.patch;https://issues.apache.org/jira/secure/attachment/12641597/PIG-3892-1.patch,28/May/14 05:27;daijy;PIG-3892-2.patch;https://issues.apache.org/jira/secure/attachment/12647062/PIG-3892-2.patch,19/Jun/14 20:56;daijy;PIG-3892-3.patch;https://issues.apache.org/jira/secure/attachment/12651490/PIG-3892-3.patch,20/Jun/14 07:06;daijy;PIG-3892-4.patch;https://issues.apache.org/jira/secure/attachment/12651630/PIG-3892-4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-04-15 13:41:10.702,,,no_permission,,,,,,,,,,,,,386668,Reviewed,,,,Sat Jul 12 02:53:53 UTC 2014,,,,,,,0|i1ulzz:,386932,,,,,,,,,,15/Apr/14 13:41;rohini;We should go with 1. It is easy to do and one installation works for both Hadoop 1 and 2 depending upon what HADOOP_HOME or HADOOP_PREFIX points to. We just check for presence of hadoop-core*.jar in hadoop classpath and if present put pig-h1.jar in classpath else put pig-h2.jar in classpath. ,"15/Apr/14 20:10;alangates;+1 for 1.  IIRC bin/hadoop has a -version option, so we don't even need to depend on magic jars being present, we can just ask hadoop.","15/Apr/14 20:23;prkommireddi;+1 for 1. 

[~daijy] - would the way to invoke a certain version be passed as an argument to bin/pig, an env variable, both, something else?",15/Apr/14 21:48;daijy;It should be automatic. Do you have a use case user need to explicit pass the version number?,"16/Apr/14 19:20;prkommireddi;Great, thanks. No use case, was wondering what the approach might be. Sounds good.","23/Apr/14 22:08;daijy;Attach patch. Include several changes:
1. A new ant target ""jar-all-h12"" which build pig-withouthadoop-h1.jar, pig-withouthadoop-h2.jar, pig-h1.jar. Intentionally skip pig-h2.jar since one fat pig.jar should be enough
2. ""jar-all-h12"" is only invoked when doing release (ant package/tar)
3. ""jar/jar-withouthadoop/jar-all"" targets remains the same, except for adding h1/h2 suffix according to hadoopversion
4. maven artifacts remain the same (no suffix, h2 classifier for h2 artifacts)
5. pig script will chose h1/h2 jar based on existence of hadoop-core.jar (hadoop-core.jar indicates h1)","26/May/14 01:37;cheolsoo;# Whether hadoop1 or hadoop2, {{HADOOP_CORE_JAR}} is not zero-length, so the following doesn't work-
{code}
+HADOOP_CORE_JAR=`echo ${HADOOP_HOME}/hadoop-core*.jar`
+
+if [ -z ""$HADOOP_CORE_JAR"" ]; then
+    HADOOP_VERSION=2
+else
+    HADOOP_VERSION=1
+fi
{code}
{code:title=hadoop1}
HADOOP_CORE_JAR=/Users/cheolsoop/workspace/hadoop-1.0.3/hadoop-core-1.0.3.jar
{code}
{code:title=hadoop2}
HADOOP_CORE_JAR='/Users/cheolsoop/workspace/hadoop-2.3.0/hadoop-core*.jar'
{code}
# jar-all-h12 shouldn't depend on {{jar}} and {{jar-withouthadoop}}-
{code}
+    <target name=""jar-all-h12"" depends=""jar,jar-withouthadoop""
{code}
# Won't this break BigTop for the same reason as PIG-3905 because ivy will resolve both hadoop1 and hadoop2 dependencies no matter what the value of {{hadoopversion}} is?
{code}
-    <target name=""package"" depends=""docs, api-report, jar, piggybank"" description=""Create a Pig tar release"">
+    <target name=""package"" depends=""jar-all-h12, docs, api-report, piggybank"" description=""Create a Pig tar release"">
{code}","28/May/14 05:26;daijy;bq. Whether hadoop1 or hadoop2, HADOOP_CORE_JAR is not zero-length, so the following doesn't work
How do you install Hadoop 2? I download Apache tar ball and it does not contain hadoop-core.jar. Is there an easy way to distinguish h2 from h1 from your installation?
bq. jar-all-h12 shouldn't depend on jar and jar-withouthadoop
It should not, new patch will remove this
bq.Won't this break BigTop for the same reason as PIG-3905 because ivy will resolve both hadoop1 and hadoop2 dependencies no matter what the value of hadoopversion is? 
Yes, ""ant package"" will create the distribution layout containing both pig-h1.jar and pig-h2.jar, which will need both hadoop 1 and hadoop 2 dependency. It might create some trouble if the private maven repository only carry h2 build artifacts. But can BIGTOP specify public maven repository as an alternative? [~gkesavan], [~cos], want to comment on this?",28/May/14 05:44;cos;During the build we sed'ing the {{ivy/ivysettings.xml}} to pick up artifacts from {{~/.m2/repository}} which are seeded from the Hadoop part of Bigtop build. We only build with {{-Dhadoopversion=23}} with explicit set of the targets like {{ jar jar-withouthadoop pigunit-jar smoketests-jar}},"28/May/14 06:05;daijy;[~cos], jar/jar-withouthadoop/pigunit-jar/smoketests-jar will remain the same. I only change target ""package"" (and ""tar"" indirectly)","28/May/14 15:52;cheolsoo;{quote}
How do you install Hadoop 2? I download Apache tar ball and it does not contain hadoop-core.jar.
{quote}
[~daijy], hadoop-core.jar does not exist in hadoop2. But the condition {{-z ""$HADOOP_CORE_JAR""}} doesn't hold true when it doesn't exist since {{$HADOOP_CORE_JAR}} is always not be zero length. I think you should check whether hadoop-core.jar exists or not instead.","28/May/14 18:35;daijy;Oh, I see what you mean. But we first set ""shopt -s nullglob"" in bin/pig, so the codesnip you posted is supposed to check existence of hadoop-core*.jar, right? I actually copied the code how we check existence of other jars (such as JYTHON_JAR at line 220), and tested works.","28/May/14 19:53;cheolsoo;Thank you for the clarification. I didn't know about {{shopt -s nullglob}}. +1.

Disclaimer: I assumes the Windows part of changes works.
","28/May/14 23:11;cos;bq. But we first set ""shopt -s nullglob"" in bin/pig
that'd work of course, but I agree with other comment: doing {{[ -e -s ]}} might be less confusing. Not a show stopper though ;)","28/May/14 23:46;daijy;Ok, since there are many places in bin/pig use this patten, I will open a new ticket to do the refactory. Thanks for point out though.

I would also like to wait for consent from [~gkesavan] before commit.",29/May/14 22:00;gkesavan;[~daijy] I will look into this patch later today. ,31/May/14 17:43;daijy;Unlink it from 0.13.0 to unblock 0.13.0 release. We will still try get it into 0.13.0 if possible.,"19/Jun/14 20:56;daijy;Let's move this forward. Now create new target package-h12 and tar-h12, so the original package and tar target will not be affected. When we do Apache release, use tar-h12 target instead. Will update the wiki page once the patch check in.","20/Jun/14 00:54;gkesavan;[~daijy]
Thanks for not renaming the currently in use build targets. 
My only concern is about renaming the jar's
{code}
pig-0.13.1-SNAPSHOT-h2.jar -> ./pig-0.13.1-SNAPSHOT.jar
pig-0.13.1-SNAPSHOT-withouthadoop-h2.jar -> pig-0.13.1-SNAPSHOT-withouthadoop.jar
{code}
maybe we can track that in a separate jira and see how we can deal with that. 
and Here is my +1",20/Jun/14 00:57;daijy;Patch committed to 0.13 branch and trunk. Thanks Cheolsoo and Giri for review!,20/Jun/14 07:06;daijy;Also need to change the test to work with pig-h*.jar.,"12/Jul/14 02:45;jeagles;[~daijy], after this checkin for bin/pig script, now HADOOP_VERSION is always 1 since HADOOP_CORE_JAR is never zero length string. Perhaps a file exists check is in order. What do you think?

{code}
296 HADOOP_CORE_JAR=`echo ${HADOOP_HOME}/hadoop-core*.jar`
297
298 if [ -z ""$HADOOP_CORE_JAR"" ]; then
299     HADOOP_VERSION=2
300 else
301     HADOOP_VERSION=1
302 fi
{code}","12/Jul/14 02:53;jeagles;So sorry, I have jumped to a conclusion that was incorrect. This change is working for me. Apologies. ",,,,,
Direct fetch doesn't set job submission timestamps,PIG-3889,12708245,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,lbendig,lbendig,14/Apr/14 12:32,07/Jul/14 18:08,14/Mar/19 03:08,15/Apr/14 17:19,,,,,,0.13.0,,,,,0,,,,,,,"The following query fails in fetch mode:
{code}
A = load 'data' as (a:chararray);   
B = FOREACH A generate 'a', CurrentTime();     
dump B;
{code}

Reason: CurrentTime() throws an exception if {{pig.job.submitted.timestamp}} is not set.",,,,,,,,,,,,,,,,,,,,14/Apr/14 21:31;lbendig;PIG-3889-2.patch;https://issues.apache.org/jira/secure/attachment/12640148/PIG-3889-2.patch,14/Apr/14 12:34;lbendig;PIG-3889.patch;https://issues.apache.org/jira/secure/attachment/12640064/PIG-3889.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-04-14 16:25:41.153,,,no_permission,,,,,,,,,,,,,386568,,,,,Tue Apr 15 17:26:19 UTC 2014,,,,,,,0|i1uldr:,386832,,,,,,,,,,14/Apr/14 12:34;lbendig;The patch sets the missing conf. properties similarly to {{MapReduceLauncher#launchPig()}}.,14/Apr/14 16:25;cheolsoo;+1.,"14/Apr/14 19:35;cheolsoo;[~lbendig], actually I found your new test case fails with NPE after PIG-3888. Since you set JobConf to null after the execution of fetch mode, the following line causes NPE-
{code:title=TestFetch.java}
String expected = UDFContext.getUDFContext().getJobConf().get(""pig.job.submitted.timestamp"");
{code}
Can you please fix it?","14/Apr/14 21:31;lbendig;[~cheolsoo], sorry for the inattention. I fixed the testcase.",15/Apr/14 17:19;cheolsoo;Committed to trunk. Thank you Lorand!,"15/Apr/14 17:26;lbendig;Cheolsoo, thanks for committing it!",,,,,,,,,,,,,,,,,,,,,,
Direct fetch doesn't differentiate between frontend and backend sides,PIG-3888,12708223,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,lbendig,lbendig,14/Apr/14 10:51,07/Jul/14 18:07,14/Mar/19 03:08,14/Apr/14 19:22,,,,,,0.13.0,,,,,0,,,,,,,"While testing PIG-3558, I found that fetch mode doesn't set the task id in the jobconf before running. As a consequence, {{UdfContext#isFrontend()}} will always return true. Loaders and Storers which initialize themselves based on this flag may fail.
",,,,,,,,,,,,,,,,,,,,14/Apr/14 11:01;lbendig;PIG-3888.patch;https://issues.apache.org/jira/secure/attachment/12640049/PIG-3888.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-14 16:35:00.893,,,no_permission,,,,,,,,,,,,,386546,Reviewed,,,,Mon Apr 14 20:55:23 UTC 2014,,,,,,,0|i1ul8v:,386810,,,,,,,,,,"14/Apr/14 11:01;lbendig;The patch sets the task attempt id and also restores the jobconf at the end of the fetch task. Thus, the subsequent operators won't be affected by the messed up jobConf which could otherwise cause wrong initialization issues.",14/Apr/14 16:35;cheolsoo;+1.,"14/Apr/14 19:22;daijy;Verified TestOrcStorage works with ""opt.fetch"" on with the patch. Patch committed to trunk. Thanks Lorand, Cheolsoo!","14/Apr/14 20:55;lbendig;[~daijy], thanks for committing it. I've also checked it against all unit tests.",,,,,,,,,,,,,,,,,,,,,,,,
TestMRJobStats is broken in trunk,PIG-3887,12708166,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,13/Apr/14 22:24,07/Jul/14 18:08,14/Mar/19 03:08,14/Apr/14 18:28,,,,,,0.13.0,,,,,0,,,,,,,"This is a regression from PIG-3884. The test assumes getOutputSize() is declared in MRJobStats, which is no longer true since PIG-3884 moved it to JobStats.",,,,,,,,,,,,,,,,,,,,13/Apr/14 22:25;cheolsoo;PIG-3887-1.patch;https://issues.apache.org/jira/secure/attachment/12639999/PIG-3887-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-14 17:46:48.821,,,no_permission,,,,,,,,,,,,,386489,,,,,Mon Apr 14 18:28:15 UTC 2014,,,,,,,0|i1ukw7:,386753,,,,,,,,,,13/Apr/14 22:25;cheolsoo;Attaching a fix.,"14/Apr/14 17:46;rohini;+1. Thanks for fixing it. Encountered the issue and created a patch, but you did beat me to posting it :).",14/Apr/14 18:28;cheolsoo;Committed to trunk. Thanks Rohini for the review!,,,,,,,,,,,,,,,,,,,,,,,,,
Multiquery off mode execution is not done in batch and very inefficient,PIG-3882,12707826,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,10/Apr/14 23:13,07/Jul/14 18:08,14/Mar/19 03:08,11/Apr/14 13:58,0.12.0,,,,,0.13.0,,,,,0,,,,,,,"Currently, if it is multiquery off mode, the pig script is executed whenever a STORE is encountered. This becomes even more inefficient in Tez as multiple DAGs are launched. And it is also not possible to do explain if multiquery is off because of this. Should switch to batch mode. Only difference should be that MultiQueryOptimizer is not applied in multiquery off mode.",,,,,,,,,,,,,,,,,,,,10/Apr/14 23:27;rohini;PIG-3882-1.patch;https://issues.apache.org/jira/secure/attachment/12639681/PIG-3882-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-10 23:36:38.242,,,no_permission,,,,,,,,,,,,,386149,,,,,Fri Apr 11 13:58:43 UTC 2014,,,,,,,0|i1uisv:,386414,,,,,,,,,,10/Apr/14 23:36;daijy;+1,11/Apr/14 13:58;rohini;Committed to trunk. Thanks for the review Daniel,,,,,,,,,,,,,,,,,,,,,,,,,,
FileLocalizer temp path can sometimes be non-unique,PIG-3874,12707506,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,chitnis,chitnis,chitnis,09/Apr/14 17:33,11/Feb/15 18:28,14/Mar/19 03:08,30/May/14 01:11,0.13.0,,,,,0.13.0,,,,,0,,,,,,,"In some rare corner cases, more than one process can arrive at the same randomly generated temporary path to localize task files. This needs to be handled with a check to see if location already exists and to get a unique path.",,,,,,,,,,,,,,,,,,,,14/Apr/14 22:39;chitnis;PIG-3874-1.patch;https://issues.apache.org/jira/secure/attachment/12640158/PIG-3874-1.patch,28/Apr/14 15:46;chitnis;PIG-3874-2.patch;https://issues.apache.org/jira/secure/attachment/12642260/PIG-3874-2.patch,29/May/14 23:17;chitnis;PIG-3874-3.patch;https://issues.apache.org/jira/secure/attachment/12647482/PIG-3874-3.patch,30/May/14 01:03;cheolsoo;PIG-3874-4.patch;https://issues.apache.org/jira/secure/attachment/12647502/PIG-3874-4.patch,14/Apr/14 20:39;chitnis;PIG-3874.patch;https://issues.apache.org/jira/secure/attachment/12640139/PIG-3874.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-04-15 18:51:47.744,,,no_permission,,,,,,,,,,,,,385829,,,,,Fri May 30 01:11:53 UTC 2014,,,,,,,0|i1ugtz:,386093,,,,,,,,,,14/Apr/14 20:39;chitnis;Patch attached,14/Apr/14 22:39;chitnis;updated the patch to account for Windows filesystem,"15/Apr/14 18:51;rohini;Can you simplify

{code}
String tempPath= FileLocalizer.getTemporaryPath(pigContext).toString();
        Path path = new Path(tempPath);
        URI uri = path.toUri();
        String prefix = """";
        if (uri.getScheme() != null) {
            prefix = uri.getScheme() + "":"";
        }
        assertTrue(tempPath.startsWith(prefix + pigTempDir.getPath()));
{code}

to 

{code}
String tempPath= FileLocalizer.getTemporaryPath(pigContext).toString();
Path path = new Path(tempPath);
assertTrue(tempPath.startsWith(pigTempDir.toURI()));
{code}",28/Apr/14 15:46;chitnis;updated patch PIG-3874-2.patch after review comment,"28/May/14 16:02;aniket486;Patch needs to be rebased.
Why not try n-times instead of two-  that will also avoid code duplication.

Something like-
{code}
int retryCount = 0;
Exception caughtException = null;
while (true) {
  try {
     // createRelativeRoot etc.
     // and then
     break;
   } catch (IOException ioe) {
     caughtException = ioe;
   }
   if (retryCount >= retryLimit) { // retryLimit = 5 hardcoded
     throw new IOException(caughtException);
   }
   retryCount++;
   //Thread.sleep() etc.
}
{code}
",28/May/14 20:01;aniket486;Missed the while loop in the patch. LGTM now. +1,29/May/14 23:17;chitnis;rebased patch to updated trunk. checked unit test in patch passes,"30/May/14 01:03;cheolsoo;[~chitnis], I think you have a mistake in the last patch.
{code}
-    private static void verifyStringContained(List<URL> list, String name, boolean included) {
+    private static void verifyStringContained(Set<URL> list, String name, boolean included) {
{code}
This changes breaks compilation in the following lines of TestPigServer.java-
{code}
verifyStringContained(pig.getPigContext().extraJars, jarName, false); // extraJars is List not Set
{code}
I am attaching a new patch that fixes this error and will commit this.",30/May/14 01:11;cheolsoo;Committed to trunk and branch-0.13. Thank you Mona!,,,,,,,,,,,,,,,,,,,
Replace org.python.google.* with com.google.* in imports,PIG-3871,12706664,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,04/Apr/14 17:13,07/Jul/14 18:07,14/Mar/19 03:08,04/Apr/14 20:31,,,,,,0.13.0,tez-branch,,,,0,,,,,,,"In several places are org.python.google.* imported. I believe this is a side effect of IDE auto completion (e.g. Eclipse) rather than intentional.

The problem is that this cause a runtime error depending on classpath. For eg,
{code}
ERROR 2998: Unhandled internal error. org/python/google/common/collect/Sets

java.lang.NoClassDefFoundError: org/python/google/common/collect/Sets
        at org.apache.pig.backend.hadoop.executionengine.tez.TezPlanContainerUDFCollector.<init>(TezPlanContainerUDFCollector.java:28)
{code}",,,,,,,,,,,,,,,,,,,,04/Apr/14 17:17;cheolsoo;PIG-3871-tez.patch;https://issues.apache.org/jira/secure/attachment/12638721/PIG-3871-tez.patch,04/Apr/14 17:17;cheolsoo;PIG-3871-trunk.patch;https://issues.apache.org/jira/secure/attachment/12638722/PIG-3871-trunk.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-04-04 17:19:27.871,,,no_permission,,,,,,,,,,,,,384987,,,,,Fri Apr 04 20:31:51 UTC 2014,,,,,,,0|i1ubnr:,385254,,,,,,,,,,"04/Apr/14 17:17;cheolsoo;Uploading two patches. One for trunk, and one for tez branch.",04/Apr/14 17:19;rohini;+1,04/Apr/14 20:31;cheolsoo;Committed to trunk and tez branch. Thank you Rohini for the review!,,,,,,,,,,,,,,,,,,,,,,,,,
STRSPLITTOBAG UDF,PIG-3870,12706604,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cryptoe,Praveenesh,Praveenesh,04/Apr/14 10:46,21/Nov/14 05:59,14/Mar/19 03:08,26/Sep/14 04:58,0.14.0,,,,,0.14.0,,,,,0,,,,,,,"I had a scenario, which required me to change the STRSPLIT code. The scenario was as follows:

I have a data like:
1       A|1|1   some
2       B|2|2   data
3       C|3|3   hadoop


Need output like this :

1    A    some
1    1    some
1    1    some
2    B    data
2    2     data
2    2     data
3    C    hadoop
3    3    hadoop
3    3    hadoop

I was trying to use STRSPLIT($1,'\\\|') which was returning a tuple, If I do flatten on it, it converts the data into columns.

If we return a bag of tuples, we can easily use flatten() to convert it into rows, plus can also convert that into Tuple using TOTUPLE() UDF (if someone just want to use it as tuple)

After the suggestion from [~daijy], I am creating a JIRA ticket to create a new UDF STRSPLITTOBAG, which will return a bag of tuples as suggested above.",,,,,,,,,,,,,,,,,,,,10/Sep/14 13:53;cryptoe;helloWorld.patch;https://issues.apache.org/jira/secure/attachment/12667668/helloWorld.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-05 14:00:46.707,,,no_permission,,,,,,,,,,,,,384927,Reviewed,,,,Wed Sep 10 13:53:22 UTC 2014,,,Patch Available,,,,0|i1ubaf:,385194,Patch committed to trunk. thanks Karan!,,,,,,,,,05/Sep/14 14:00;cryptoe;[~daijy] I would like to work on this jira. Please assign it to me.,"10/Sep/14 13:51;cryptoe;I have added the functionality of STRSPLITTOBAG .
Request code review.
",10/Sep/14 13:53;cryptoe;Request for code review,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Iterator_1 e2e test on windows,PIG-3868,12706600,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ssvinarchukhorton,ssvinarchukhorton,ssvinarchukhorton,04/Apr/14 09:41,07/Jul/14 18:08,14/Mar/19 03:08,11/Apr/14 00:02,0.12.0,,,,,0.13.0,,e2e harness,,,0,,,,,,,"On windows Iterator_1 test failed, because python automatically write windows end line character and then compares with unix end line character.
For fix this need open fine as binary file.
",,,,,,,,,,,,,,,,,,,,04/Apr/14 09:41;ssvinarchukhorton;PIG-3868.patch;https://issues.apache.org/jira/secure/attachment/12638656/PIG-3868.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-11 00:02:50.286,,,no_permission,,,,,,,,,,,,,384923,Reviewed,,,,Fri Apr 11 00:02:50 UTC 2014,,,,,,,0|i1ub9j:,385190,,,,,,,,,,04/Apr/14 09:42;ssvinarchukhorton;Added patch with fix,11/Apr/14 00:02;rohini;+1. Committed to trunk. Thanks Sergey for the patch.,,,,,,,,,,,,,,,,,,,,,,,,,,
Added hadoop home to build classpath for build pig with unit test on windows,PIG-3867,12706597,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,ssvinarchukhorton,ssvinarchukhorton,ssvinarchukhorton,04/Apr/14 09:31,07/Jul/14 18:08,14/Mar/19 03:08,08/May/14 22:52,0.12.0,,,,,0.13.0,,build,,,0,,,,,,,For build pig with unit tests on windows need added to build path hadoop_home\bin.,,,,,,,,,,,,,,,,,,,,04/Apr/14 09:31;ssvinarchukhorton;PIG-3867.patch;https://issues.apache.org/jira/secure/attachment/12638655/PIG-3867.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-08 22:52:05.648,,,no_permission,,,,,,,,,,,,,384920,,,,,Thu May 08 22:52:05 UTC 2014,,,,,,,0|i1ub8v:,385187,,,,,,,,,,04/Apr/14 09:32;ssvinarchukhorton;Added path with fix.,08/May/14 22:52;alangates;Patch committed.  Thanks Sergey.,,,,,,,,,,,,,,,,,,,,,,,,,,
duplicate jars get added to distributed cache,PIG-3861,12706328,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,chitnis,chitnis,chitnis,03/Apr/14 00:08,09/Jul/18 22:09,14/Mar/19 03:08,23/Oct/14 20:49,,,,,,0.14.0,,,,,0,,,,,,,"PigContext's scriptJars should handle de-duplication of jars to account for script engines e.g. JythonScriptEngine performing various jar loading for module and sometimes adding same jar twice. AlsoJobControlCompiler.shipToHdfs() needs a check against adding the same jar more than once, under different randomly incremented sub-dirs.",,,,,,,,,,,,,,,,,,PIG-3798,,03/Apr/14 00:13;chitnis;PIG-3681-1.patch;https://issues.apache.org/jira/secure/attachment/12638380/PIG-3681-1.patch,15/Apr/14 00:34;chitnis;PIG-3861-2.patch;https://issues.apache.org/jira/secure/attachment/12640180/PIG-3861-2.patch,28/Apr/14 15:48;chitnis;PIG-3861-3.patch;https://issues.apache.org/jira/secure/attachment/12642261/PIG-3861-3.patch,29/May/14 20:36;chitnis;PIG-3861-4.patch;https://issues.apache.org/jira/secure/attachment/12647442/PIG-3861-4.patch,21/Oct/14 22:07;rohini;PIG-3861-5.patch;https://issues.apache.org/jira/secure/attachment/12676199/PIG-3861-5.patch,26/Oct/14 03:27;daijy;PIG-3861-fixtest.patch;https://issues.apache.org/jira/secure/attachment/12677148/PIG-3861-fixtest.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2014-04-03 19:58:43.884,,,no_permission,,,,,,,,,,,,,384651,Reviewed,,,,Sun Oct 26 03:27:29 UTC 2014,,,,,,,0|i1u9lr:,384919,,,,,,,,,,03/Apr/14 00:13;chitnis;patch attached,"03/Apr/14 19:58;rohini;Few comments:
  - Please convert to Set for skipJars, extraJars, etc as well
  - The changes to shipToHDFS is very bad and adds FS calls and also logic is flimsy if user passes jars/files via distributedcache like in Oozie. Please revert it and check the conf to see if DistributedCache already has that file. Take into account symlinks while doing that. 
  - TestJobControlCompiler.java - Add an assert to check that it is in DistributedCache only once. ",15/Apr/14 00:34;chitnis;patch updated with review comments. removed unnecessary complexity,28/Apr/14 15:29;chitnis;Uploading final patch PIG-3861-3.patch after review,"28/Apr/14 15:48;chitnis;uploading patch again with ""--no-prefix"" for git diff",29/May/14 20:36;chitnis;Patch (v4) updated to resolve issue where pig job submitted via Oozie would report local FS path for jar instead of on distributed cache. Also rebased to updated trunk. Tested locally,21/Oct/14 22:07;rohini;Rebased the patch. Removed the List to Set conversion as Daniel said order needs to be maintained.,23/Oct/14 18:31;daijy;+1,23/Oct/14 20:49;rohini;Committed to branch-0.14 and trunk. Thanks Mona for the patch and Daniel for the review.,26/Oct/14 03:27;daijy;TestGrunt is broken by the patch. Commit fix PIG-3861-fixtest.patch.,,,,,,,,,,,,,,,,,,
Refactor PigStatusReporter and PigLogger for non-MR execution engine,PIG-3860,12706304,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,02/Apr/14 21:51,07/Jul/14 18:08,14/Mar/19 03:08,28/Apr/14 22:03,,,,,,0.13.0,tez-branch,,,,0,,,,,,,"PIG-3829 handles some cases, but NPE can be still thrown for custom counters in Tez mode. For example, the following query fails with NPE-
{code}
SET opt.fetch false;
SET pig.udf.profile true;
a = LOAD 'foo' AS (x:chararray);
b = FOREACH a GENERATE LOWER(x);
DUMP b;
{code}

The problem is that PigStatusReport.getInstance().getCounter() returns null since TezStatusReporter is used in Tez.
{code}
PigStatusReporter.getInstance().getCounter(counterGroup, INVOCATION_COUNTER).increment(TIMING_FREQ);
{code}",,,,,,,,,,,,,PIG-3858,,,,,,,27/Apr/14 21:11;cheolsoo;PIG-3860-1-tez.patch;https://issues.apache.org/jira/secure/attachment/12642154/PIG-3860-1-tez.patch,27/Apr/14 21:11;cheolsoo;PIG-3860-1-trunk.patch;https://issues.apache.org/jira/secure/attachment/12642153/PIG-3860-1-trunk.patch,28/Apr/14 21:37;cheolsoo;PIG-3860-2-tez.patch;https://issues.apache.org/jira/secure/attachment/12642336/PIG-3860-2-tez.patch,28/Apr/14 21:37;cheolsoo;PIG-3860-2-trunk.patch;https://issues.apache.org/jira/secure/attachment/12642335/PIG-3860-2-trunk.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-04-03 02:47:51.193,,,no_permission,,,,,,,,,,,,,384627,,,,,Mon Apr 28 22:03:10 UTC 2014,,,,,,,0|i1u9gf:,384895,,,,,,,,,,03/Apr/14 02:47;daijy;Blame me on this. We may have to keep both TaskInputOutputContext and TezProcessorContext in PigStatusReporter.,"03/Apr/14 02:59;cheolsoo;[~daijy], no worries. :-)

In fact, Lorand and I were discussion the exactly same problem in PIG-3858. We might end up putting TaskInputOutputContext, TezProcessorContext, and FetchContext in PigStatusReporter. I was wondering whether there is a better way to fix this.","27/Apr/14 21:11;cheolsoo;RB link-
https://reviews.apache.org/r/20761/",28/Apr/14 21:37;cheolsoo;Update patches incorporating Daniel's comment.,28/Apr/14 21:44;daijy;+1,28/Apr/14 22:03;cheolsoo;Committed to trunk and tez branch. Thank you Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,
auto local mode should not modify reducer configuration,PIG-3859,12706297,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,aniket486,aniket486,aniket486,02/Apr/14 21:33,07/Jul/14 18:08,14/Mar/19 03:08,29/Apr/14 22:38,0.13.0,,,,,0.13.0,,,,,0,,,,,,,"Along with mapred.reduce.tasks, auto local mode conversion should also ignore mapreduce.job.reduces parameter from default configuration.",,,,,,,,,,,,,,,,,,,,28/Apr/14 18:58;aniket486;PIG-3859.patch;https://issues.apache.org/jira/secure/attachment/12642305/PIG-3859.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-28 20:20:59.552,,,no_permission,,,,,,,,,,,,,384620,,,,,Tue Apr 29 22:37:58 UTC 2014,,,,,,,0|i1u9ev:,384888,,,,,,,,,,28/Apr/14 20:20;daijy;+1,29/Apr/14 22:37;aniket486;Committed to trunk! Thanks [~daijy] for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,
PigLogger/PigStatusReporter is not set for fetch tasks,PIG-3858,12706174,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lbendig,lbendig,lbendig,02/Apr/14 13:43,07/Jul/14 18:07,14/Mar/19 03:08,04/Apr/14 14:53,,,,,,0.13.0,,,,,0,,,,,,,"PigLogger initialization is missing which leads to NPE if counters are accessed during a fetch tasks. Some queries to reproduce this issue:

1.
{code}
set opt.fetch true;
A = load ...
B = foreach A generate TOBAG(TOTUPLE(null));
dump B;
{code}
-> NPE when PigWarning.SKIP_UDF_CALL_FOR_NULL is incremented

2.
{code}
set opt.fetch true;
set pig.udf.profile true;

A = load ...
B = foreach A generate [any UDF]
{code}
-> NPE when POUserFunc.INVOCATION_COUNTER is incremented",,,,,,,,,,,,,,,,,,,,02/Apr/14 13:51;lbendig;PIG-3858.patch;https://issues.apache.org/jira/secure/attachment/12638256/PIG-3858.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-04-02 17:24:52.323,,,no_permission,,,,,,,,,,,,,384497,,,,,Fri Apr 04 19:21:51 UTC 2014,,,,,,,0|i1u8nj:,384765,,,,,,,,,,02/Apr/14 13:51;lbendig;The attached patch introduces a dummy FetchContext which can be accessed through PigStatusReporter.,"02/Apr/14 17:24;cheolsoo;[~lbendig], thank you for catching this! This is my fault (PIG-3679).

Your patch looks fine, but I have a suggestion. Why don't we just check whether pigLogger is null or not in POUserFunc? In fact, this is the 2nd time that we ran into this problem. (Pig-on-Tez also had run into this issue before we implemented customer counter support.) Assuming that counters are not meaningful in fetch mode, it seems better to make pigLogger optional. What do you think?","02/Apr/14 20:30;lbendig;[~cheolsoo], thank you for your remarks. In case of aggregate warnings checking only pigLogger is fine. But what about those cases where counters are accessed via PigStatusReporter? E.g in POUserFunc:
{code}PigStatusReporter.getInstance().getCounter(counterGroup, INVOCATION_COUNTER).increment(TIMING_FREQ);{code}
It is usually assumed that counters are always available, but this is not the case in fetch mode where TaskInputOutputContext is null. The mock context will prevent POs/UDFs throwing NPEs and will be probably handy once PIG-2620 is accomplished (FailOnThreshold error handler would use counters).

","02/Apr/14 21:45;cheolsoo;[~lbendig], you're right. I've just confirmed that the same problem exists in tez branch.

I was going to suggest to implement a separate StatusReporter for fetch mode like PIG-3829 did, but that won't work because of these static calls. Bad, bad... :( I'd like to avoid having FetchContext and TaskInputOutputContext in PigStatusReporter, but I don't have a better suggestion.","02/Apr/14 22:46;lbendig;[~cheolsoo] Well, me neither... I also thought of subclassing TaskInputOutputContext but this wouldn't be straightforward because in Hadoop1/Hadoop2 it's an abstract class/interface.","03/Apr/14 16:32;cheolsoo;+1. Let's commit this now. We can improve it if needed in the future.

I'll commit this tomorrow if I don't hear any objections.",04/Apr/14 14:53;cheolsoo;Committed to trunk. Thank you Lorand!,"04/Apr/14 19:21;lbendig;Cheolsoo, thanks for committing it!",,,,,,,,,,,,,,,,,,,,
ant pigperf target is broken in trunk,PIG-3837,12703857,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,26/Mar/14 23:17,07/Jul/14 18:07,14/Mar/19 03:08,28/Mar/14 00:39,,,,,,0.13.0,,build,,,0,,,,,,,"To reproduce the issue, run {{ant pigperf}}. It fails with the following error-
{code}
BUILD FAILED
/Users/cheolsoop/workspace/pig-apache/build.xml:799: The archive sdsuLibJKD12.jar doesn't exist
{code}

Although sdsuLibJKD12.jar is checked into trunk (see [here|https://github.com/apache/pig/tree/trunk/test/perf/pigmix/lib]). But the path to the jar in build.xml seems broken.
{code}
-            <zipfileset src=""${lib.dir}/sdsuLibJKD12.jar"" />
+            <zipfileset src=""test/perf/pigmix/lib/sdsuLibJKD12.jar"" />
{code}",,,,,,,,,,,,,,,,,,,,26/Mar/14 23:17;cheolsoo;PIG-3837-1.patch;https://issues.apache.org/jira/secure/attachment/12637043/PIG-3837-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-27 23:23:38.068,,,no_permission,,,,,,,,,,,,,382191,,,,,Fri Mar 28 00:39:56 UTC 2014,,,,,,,0|i1tugn:,382462,,,,,,,,,,26/Mar/14 23:17;cheolsoo;Attaching a fix.,27/Mar/14 23:23;daijy;+1,28/Mar/14 00:39;cheolsoo;Committed to trunk. Thank you Daniel for the review.,,,,,,,,,,,,,,,,,,,,,,,,,
Pig signature has has guava version dependency,PIG-3836,12703816,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,amatsukawa,amatsukawa,amatsukawa,26/Mar/14 20:46,07/Jul/14 18:08,14/Mar/19 03:08,27/Mar/14 16:16,,,,,,0.13.0,,,,,0,,,,,,,"We currently use Hashing.goodFastHash(32) to generate signatures for the logical plan. Under the hood, this is a murmur32 hash.

Guava 11, which pig directly depends on, always seeds the hash with 0.

http://docs.guava-libraries.googlecode.com/git-history/v11.0/javadoc/src-html/com/google/common/hash/Hashing.html#line.85

In future versions of Guava, it is seeded by current time: http://docs.guava-libraries.googlecode.com/git-history/v14.0/javadoc/src-html/com/google/common/hash/Hashing.html#line.47

So when future versions of guava is pulled into the classpath (which if often), we no longer get the same signature for the same logical plan between each executions of pig. This introduces unnecessary complexity for managing the classpath.",,,,,,,,,,,,,,,,,,,,26/Mar/14 23:11;amatsukawa;0001-use-murmur-hash-with-0-seed.patch;https://issues.apache.org/jira/secure/attachment/12637039/0001-use-murmur-hash-with-0-seed.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-26 22:51:49.679,,,no_permission,,,,,,,,,,,,,382150,,,,,Thu Mar 27 16:16:31 UTC 2014,,,Patch Available,,,,0|i1tu7z:,382423,,,,,,,,,,26/Mar/14 20:48;amatsukawa;Attached patch directly uses a murmur hash seeded with 0 to remove this version dependency on guava.,"26/Mar/14 22:51;cheolsoo;[~amatsukawa], thank you for the patch. But it breaks the compilation.
{code}
    [javac] /Users/cheolsoop/workspace/pig-apache/src/org/apache/pig/newplan/logical/relational/LogicalPlan.java:130: cannot find symbol
    [javac] symbol  : method murmur_32(int)
    [javac] location: class com.google.common.hash.Hashing
    [javac]         HashFunction hf = Hashing.murmur_32(0);
    [javac]                                  ^
{code}
Did you mean {{murmur3_32(int)}}? I see it in JavaDoc [here|http://docs.guava-libraries.googlecode.com/git-history/v11.0/javadoc/com/google/common/hash/Hashing.html#murmur3_32(int)].","26/Mar/14 23:11;amatsukawa;Patch should have said ""murmur3_32"" instead of ""murmur_32"". Fixed.","26/Mar/14 23:24;cheolsoo;+1. I'll commit it tomorrow.

Thank you Akihiro!",27/Mar/14 00:13;amatsukawa;Thanks!,27/Mar/14 16:16;cheolsoo;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,
Relation loaded by AvroStorage with schema is projected incorrectly in foreach statement.,PIG-3833,12703532,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jeongjinku,jeongjinku,jeongjinku,25/Mar/14 17:56,15/Apr/14 20:44,14/Mar/19 03:08,27/Mar/14 16:35,0.12.1,,,,,0.12.1,0.13.0,,,,0,,,,,,,"This example has a correct behavior. 

in = LOAD '$INFILE' USING AvroStorage('','-f schema.avsc');
out = FOREACH in GENERATE *;

But if we try to pick and project specific fields like the following example,

in = LOAD '$INFILE' USING AvroStorage('','-f schema.avsc');
out = FOREACH in GENERATE $0,$1,$7;

Actual result is ""$0, $1, $2"" not ""$0,$1,$7""

This bug is always reproducible.

",,,,,,,,,,,,,,,,,,,,27/Mar/14 01:43;jeongjinku;PIG-3833-2.patch;https://issues.apache.org/jira/secure/attachment/12637069/PIG-3833-2.patch,26/Mar/14 00:13;jeongjinku;PIG-3833.patch;https://issues.apache.org/jira/secure/attachment/12636823/PIG-3833.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-03-26 22:44:38.017,,,no_permission,,,,,,,,,,,,,381866,,,,,Fri Mar 28 16:21:21 UTC 2014,,,,,,,0|i1tshr:,382141,,,,,,,AvroStorage,,,"26/Mar/14 22:44;cheolsoo;[~jeongjinku], thank you for the patch. It looks good. Do you mind making a minor change to it?

Instead of duplicating the code, why not call updateSchemaFromInputAvroSchema() from getInputAvroSchema()? That is-
{code}
  public final Schema getInputAvroSchema() {
    if (schema == null) {
      updateSchemaFromInputAvroSchema();
    }
    return schema;
  }

  private final void updateSchemaFromInputAvroSchema() {
      String schemaString = getProperties().getProperty(INPUT_AVRO_SCHEMA);
      if (schemaString != null) {
          Schema s = new Schema.Parser().parse(schemaString);
          schema = s;
      }
  }
{code}
Also, can we make updateSchemaFromInputAvroSchema() private?

If you update the patch, I'll commit it. Thanks!","27/Mar/14 01:46;jeongjinku;Thanks [~cheolsoo], 
I applied your suggestion in PIG-3833-2.patch.","27/Mar/14 01:58;cheolsoo;+1. I'll commit it tomorrow.

FYI, next time please put together all the changes in a patch instead of uploading incremental patches. That makes it easier for a reviewer to apply them at once. I'll take care of it this time, no worries. :-)

Thanks!",27/Mar/14 02:08;jeongjinku;Thanks! :),27/Mar/14 16:35;cheolsoo;Committed to trunk.,"27/Mar/14 18:05;jeongjinku;Thanks [~cheolsoo] !
Can we also apply this patch to 0.12.1 branch?
Maybe any branch after PIG-3015 should have this fix.
","27/Mar/14 18:31;cheolsoo;[~jeongjinku], I can do that. PIG-3015 was released in 0.12, so there is no more branches that need the fix.

I'll commit this in 0.12 tomorrow.
",28/Mar/14 16:21;cheolsoo;Committed to 0.12 branch.,,,,,,,,,,,,,,,,,,,,
Fix piggybank test compilation failure after PIG-3449,PIG-3832,12703335,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,24/Mar/14 22:21,07/Jul/14 18:08,14/Mar/19 03:08,25/Mar/14 20:47,0.13.0,,,,,0.13.0,,,,,0,,,,,,,"JobCreationException has moved from org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException to org.apache.pig.backend.hadoop.executionengine 
{code}
[javac] import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException;
    [javac]                                                                    ^
    [javac]   symbol:   class JobCreationException
    [javac]   location: package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer
    [javac] /home/builds/thread2/workspace/Pig-Tez-Component/pig/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/TestAvroStorage.java:852: error: cannot find symbol
    [javac]         } catch (JobCreationException e) {
{code}",,,,,,,,,,,,,,,,,,,,25/Mar/14 06:05;rohini;PIG-3832-1.patch;https://issues.apache.org/jira/secure/attachment/12636536/PIG-3832-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-25 00:09:58.809,,,no_permission,,,,,,,,,,,,,381673,Reviewed,,,,Tue Mar 25 20:47:50 UTC 2014,,,,,,,0|i1trav:,381948,,,,,,,,,,"24/Mar/14 22:33;rohini;Actually JobCreationException is in org.apache.pig.backend.hadoop.executionengine.mapReduceLayer only in trunk branch. Only in tez branch it is different. 

[~cheolsoo],
   Shouldn't PIG-3449 go into pig trunk so that it is there when branching for 0.13?","25/Mar/14 00:09;cheolsoo;[~rohini], sure, I can commit PIG-3449 into trunk. I'll do it tomorrow unless anyone objects.",25/Mar/14 06:05;rohini;Fixed a failure with TestMultiStorageCompression as well. Will commit this patch to trunk as well after PIG-3449 goes into trunk,25/Mar/14 16:26;cheolsoo;+1. Please go ahead commit to trunk too. Thanks!,25/Mar/14 20:47;rohini;Committed to trunk. Thanks Cheolsoo for the review.,,,,,,,,,,,,,,,,,,,,,,,
Custom partitioner is not picked up with secondary sort optimization,PIG-3827,12702811,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,prkommireddi,prkommireddi,21/Mar/14 06:09,15/Apr/14 20:45,14/Mar/19 03:08,22/Mar/14 00:11,0.12.0,,,,,0.12.1,0.13.0,,,,0,,,,,,,"Custom partitioner is ignored currently in case of secondary sort optimization.

",,,,,,,,,,,,,,,,,,,,21/Mar/14 23:45;daijy;PIG-3827-1.patch;https://issues.apache.org/jira/secure/attachment/12636143/PIG-3827-1.patch,22/Mar/14 00:00;daijy;PIG-3827-2.patch;https://issues.apache.org/jira/secure/attachment/12636148/PIG-3827-2.patch,22/Mar/14 00:09;daijy;PIG-3827-3.patch;https://issues.apache.org/jira/secure/attachment/12636151/PIG-3827-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-03-22 00:00:55.594,,,no_permission,,,,,,,,,,,,,381149,Reviewed,,,,Sat Mar 22 00:11:57 UTC 2014,,,,,,,0|i1to3b:,381425,,,,,,,,,,"22/Mar/14 00:00;daijy;Sorry, miss WrongCustomPartitioner.java.","22/Mar/14 00:03;prkommireddi;+1

Verified patch against the script that was broken due to this issue. Ran ""test-commit"", LGTM",22/Mar/14 00:05;prkommireddi;[~daijy] can you please add comments to the testcase before committing?,22/Mar/14 00:09;daijy;Added comments.,22/Mar/14 00:11;daijy;Patch committed to trunk and 0.12 branch. Thanks Prashant for review!,"22/Mar/14 00:11;prkommireddi;Perfect, thanks Daniel.",,,,,,,,,,,,,,,,,,,,,,
Outer join with PushDownForEachFlatten generates wrong result,PIG-3826,12702793,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,21/Mar/14 02:12,15/Apr/14 20:45,14/Mar/19 03:08,21/Mar/14 03:58,,,,,,0.12.1,0.13.0,impl,,,0,,,,,,,"The following script generates wrong result:
A = load 'A.txt' using PigStorage(',') as (id:chararray, value:double);
B = load 'B.txt' using PigStorage(',') as (id:chararray, name:chararray);

t1 = group A by id;
t2 = foreach t1 { r1 = filter $1 by (value>1); r2 = limit r1 1; generate group as id, FLATTEN(r2.value) as value; };

t3 = join B by id LEFT OUTER, t2 by id;
dump t3;

A.txt:
1,1.5
2,0
3,-2.0
4,8.9

B.txt:
1,Ofer
2,Jordan
3,Noa
4,Daniel

Expected output:
(1,Ofer,1,1.5)
(2,Jordan,,)
(3,Noa,,)
(4,Daniel,4,8.9)

But we get:
(1,Ofer,1,1.5)
(4,Daniel,4,8.9)

With the option ""-t PushDownForEachFlatten"", the issue goes away.",,,,,,,,,,,,,,,,,,,,21/Mar/14 02:59;daijy;PIG-3826-1.patch;https://issues.apache.org/jira/secure/attachment/12635942/PIG-3826-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-21 03:39:55.776,,,no_permission,,,,,,,,,,,,,381131,Reviewed,,,,Fri Mar 21 03:58:49 UTC 2014,,,,,,,0|i1tnzb:,381407,,,,,,,,,,21/Mar/14 03:39;rohini;+1,21/Mar/14 03:58;daijy;Patch committed to both trunk and 0.12 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
Stats collection needs to be changed for hadoop2 (with auto local mode),PIG-3825,12702792,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,21/Mar/14 02:07,07/Jul/14 18:08,14/Mar/19 03:08,27/May/14 19:06,0.13.0,,,,,0.13.0,,,,,0,,,,,,,"With auto local mode, hadoop 2 getJob, getMapTaskReports,  getReduceTaskReports api cause NumberFormatException with stack similar to following-
(I have also noticed that getMapTaskReports causes memory leak and OOMs for long running jobs as hadoop 2 map task reports are quite bulky, but that is a separate issue)
{noformat}
Caused by: java.lang.NumberFormatException: For input string: ""local154006779""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
        at java.lang.Long.parseLong(Long.java:410)
        at java.lang.Long.parseLong(Long.java:468)
        at org.apache.hadoop.mapreduce.JobID.toClusterTimeStamp(JobID.java:172)
        at org.apache.hadoop.mapreduce.JobID.getAppId(JobID.java:167)
        at org.apache.hadoop.mapreduce.TypeConverter.toYarn(TypeConverter.java:79)
        at org.apache.hadoop.mapred.ClientServiceDelegate.<init>(ClientServiceDelegate.java:114)
        at org.apache.hadoop.mapred.ClientCache.getClient(ClientCache.java:68)
        at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:550)
        at org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:182)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:586)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:584)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1478)
        at org.apache.hadoop.mapred.JobClient.getJobUsingCluster(JobClient.java:584)
        at org.apache.hadoop.mapred.JobClient.getTaskReports(JobClient.java:638)
        at org.apache.hadoop.mapred.JobClient.getMapTaskReports(JobClient.java:632)
        at org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addMapReduceStatistics(MRJobStats.java:318)
        at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:308)
        at org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:240)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:363)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:374)
        ... 16 more
{noformat}",,,,,,,,,,,,,PIG-3913,,,,,,,21/Mar/14 02:11;aniket486;PIG-3825.patch;https://issues.apache.org/jira/secure/attachment/12635939/PIG-3825.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-23 21:04:53.384,,,no_permission,,,,,,,,,,,,,381130,,,,,Tue May 27 19:06:46 UTC 2014,,,,,,,0|i1tnz3:,381406,,,,,,,,,,"23/Mar/14 21:04;cheolsoo;[~aniket486], I am not sure it's a good idea to always ignore exceptions. Can't we explicitly use ""local<number>"" in the JobID to identify the local mode and handle it differently? I am worried that with your approach, not only NumberFormatException but also any other exceptions are swallowed. That might cover up some real problems.

Can you also tell me which Hadoop version you're using? I tried EMR 2.2, but auto local mode doesn't seem to work at all. Basically, jobs never store anything. I am going to test it with EMR 2.3 as soon as it's released.","24/Mar/14 21:18;aniket486;[~cheolsoo], I didn't like this approach myself a lot. But, it seems there is no non-hacky way of solving this. I will give it another try to make it the way you suggested.

We are using hadoop-1.0.4 and 2.0.5 with pig_13. Auto local mode works for both the cases well. If jobs are not storing anything (when you are writing to s3 for example), you should check if your local job is getting required configuration or not.","24/Mar/14 22:13;cheolsoo;[~aniket486], thanks for letting me know the version you're using. It doesn't work with 2.2. I am not storing to s3 but to hdfs. The job succeeds but the output dir only has _temporary with an empty file. Let me investigate it more.",13/May/14 18:37;aniket486;I've tried to solve this with PIG-3913 that build's a shim for hadoop 2 to avoid above exception.,27/May/14 19:06;aniket486;Fixed in PIG-3913.,,,,,,,,,,,,,,,,,,,,,,,
TestAvroStorage fail on some OS,PIG-3820,12702487,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,19/Mar/14 21:23,15/Apr/14 20:45,14/Mar/19 03:08,19/Mar/14 23:10,,,,,,0.12.1,0.13.0,,,,0,,,,,,,"See test fail on SUSE11, CentOS5, CentOS6. Error message:
org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
	at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:223)
	at org.xerial.snappy.Snappy.<clinit>(Snappy.java:48)
	at org.apache.avro.file.SnappyCodec.compress(SnappyCodec.java:43)
	at org.apache.avro.file.DataFileStream$DataBlock.compressUsing(DataFileStream.java:349)
	at org.apache.avro.file.DataFileWriter.writeBlock(DataFileWriter.java:348)
	at org.apache.avro.file.DataFileWriter.sync(DataFileWriter.java:360)
	at org.apache.avro.file.DataFileWriter.flush(DataFileWriter.java:367)
	at org.apache.avro.file.DataFileWriter.close(DataFileWriter.java:375)
	at org.apache.avro.tool.DataFileWriteTool.run(DataFileWriteTool.java:111)
	at org.apache.pig.builtin.TestAvroStorage.generateAvroFile(TestAvroStorage.java:269)
	at org.apache.pig.builtin.TestAvroStorage.generateInputFiles(TestAvroStorage.java:223)
	at org.apache.pig.builtin.TestAvroStorage.setup(TestAvroStorage.java:127)

Seems we hit https://github.com/xerial/snappy-java/issues/17

Upgrade snappy-java version make the test failure go away.",,,,,,,,,,,,,,,,,,,,19/Mar/14 21:23;daijy;PIG-3820-1.patch;https://issues.apache.org/jira/secure/attachment/12635645/PIG-3820-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-19 21:49:38.286,,,no_permission,,,,,,,,,,,,,380826,Reviewed,,,,Wed Mar 19 23:10:55 UTC 2014,,,,,,,0|i1tm4v:,381105,,,,,,,,,,19/Mar/14 21:49;rohini;+1,19/Mar/14 23:10;daijy;Patch committed to both 0.12 branch and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,
"e2e tests containing ""perl -e ""print $_;"" fails on Hadoop 2",PIG-3819,12702483,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,19/Mar/14 21:12,15/Apr/14 20:44,14/Mar/19 03:08,02/Apr/14 22:53,,,,,,0.12.1,0.13.0,impl,,,0,,,,,,,"Before submitting to Hadoop, ""$_"" will be interpreted and replaced with something totally unrelated ($_ means last command executed). The tests runs fine under Hadoop 1 though.",,,,,,,,,,,,,,,,,,,,19/Mar/14 21:14;daijy;PIG-3819-1.patch;https://issues.apache.org/jira/secure/attachment/12635643/PIG-3819-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-19 21:52:18.202,,,no_permission,,,,,,,,,,,,,380822,Reviewed,,,,Wed Apr 02 22:53:47 UTC 2014,,,,,,,0|i1tm3z:,381101,,,,,,,,,,19/Mar/14 21:52;rohini;What interprets $ to last command executed? Is it not possible to fix the source of the problem as user will also have to change the pig scripts.,"19/Mar/14 22:48;daijy;Sorry, Jira eats my _ character. I mean ""$\_"" has been interpreted. My guess is Hadoop will invoke bash, and bash does the variable substitution. Yes, if user using ""$\_"" will find it does not work in Hadoop 2. We shall ask user to use single quote instead of double quote. I am not sure how hard it is to fix on Hadoop 2 side, but if you want, I can talk with Hadoop folks.","02/Apr/14 20:05;daijy;[~rohini], any comment on this? Would like to check this into 0.12.1 release.","02/Apr/14 20:55;rohini; I find it failing with same error in my laptop, but on our actual clusters it succeeds. So I think hadoop2 itself is not a problem.","02/Apr/14 22:51;rohini;Daniel found the cause. It was working in our clusters because 0.11 was run against hadoop 2. This error was introduced with PIG-3069 in 0.12. This patch basically reverts part of PIG-3069.

+1",02/Apr/14 22:53;daijy;Patch committed to both 0.12 branch and trunk. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,
PIG-2499 is accidently reverted,PIG-3818,12702480,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,19/Mar/14 21:03,15/Apr/14 20:44,14/Mar/19 03:08,19/Mar/14 23:08,,,,,,0.12.1,0.13.0,impl,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Mar/14 21:07;daijy;PIG-3818-1.patch;https://issues.apache.org/jira/secure/attachment/12635640/PIG-3818-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-19 21:58:34.106,,,no_permission,,,,,,,,,,,,,380819,Reviewed,,,,Wed Mar 19 23:08:06 UTC 2014,,,,,,,0|i1tm3b:,381098,,,,,,,,,,19/Mar/14 21:05;daijy;Accidentally reverted by PIG-2798. Seeing occasionally fails in TestGrunt.testShellCommand again.,"19/Mar/14 21:58;rohini;+1. Since this will fix the race condition, can we have the comment ""// We need to fix this because there is a race condition with pipes. ......"" removed before checkin?","19/Mar/14 23:08;daijy;Yes, those comments should be removed.

Patch committed to both trunk and 0.12 branch. Thanks Rohini!",,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect Javadoc for launchPlan() method,PIG-3816,12702056,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,kyunghoj,kyunghoj,kyunghoj,18/Mar/14 04:09,07/Jul/14 18:08,14/Mar/19 03:08,18/Mar/14 14:17,,,,,,0.13.0,,documentation,,,0,,,,,,,"Javadoc of {{protected PigStats launchPlan(LogicalPlan lp, String jobName)}} incorrectly describes that the method takes a physical plan as an argument.",,,,,,,,,,,,,,,,,,,,18/Mar/14 04:11;kyunghoj;PIG-3816.patch;https://issues.apache.org/jira/secure/attachment/12635232/PIG-3816.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-18 05:55:24.97,,,no_permission,,,,,,,,,,,,,380396,,,,,Tue Mar 18 14:17:56 UTC 2014,,,,,,,0|i1tjin:,380677,,,,,,,,,,"18/Mar/14 05:55;prkommireddi;Thanks for the contribution Kyungho. Patch committed to trunk.

[~daijy] [~cheolsoo] how can I add Kyungho to contributors list? This JIRA needs to be assigned to him before marking it resolved.",18/Mar/14 14:17;cheolsoo;Added Kyungho to the contributors. I also added Prashant to the admins.,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop bug causes to pig to fail silently with jar cache,PIG-3815,12702048,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,18/Mar/14 01:52,12/Sep/17 22:23,14/Mar/19 03:08,23/Mar/14 06:48,0.13.0,,,,,0.13.0,,,,,0,,,,,,,"Pig uses DistributedCache.addFileToClassPath api that puts jars on distributed cache configuration. This uses : to separate list of files to be put of classpath via distributed cache. If fs.default.name has port number in it, it causes the tokenization logic to fail in hadoop for retrieving list of cache filenames in backend.",,,,,,,,,,,,,PIG-5290,,,,,,,18/Mar/14 18:29;aniket486;PIG-3815-1.patch;https://issues.apache.org/jira/secure/attachment/12635362/PIG-3815-1.patch,18/Mar/14 20:21;aniket486;PIG-3815-2.patch;https://issues.apache.org/jira/secure/attachment/12635391/PIG-3815-2.patch,19/Mar/14 05:25;aniket486;PIG-3815-3.patch;https://issues.apache.org/jira/secure/attachment/12635492/PIG-3815-3.patch,18/Mar/14 01:53;aniket486;PIG-3815.patch;https://issues.apache.org/jira/secure/attachment/12635220/PIG-3815.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-03-18 15:08:26.304,,,no_permission,,,,,,,,,,,,,380388,,,,,Wed Mar 19 19:13:57 UTC 2014,,,,,,,0|i1tjgv:,380669,,,,,,,,,,"18/Mar/14 15:08;cheolsoo;# Can you delete this? It's unused.
{code}
+import org.codehaus.plexus.util.IOUtil;
{code}
# Do you mind fixing JobControlCompiler.java#L1700 too? Looks like we can use IOUtils.closeQuietly() here too.
{code}
        OutputStream os = fs.create(dst);
        try {
            IOUtils.copyBytes(url.openStream(), os, 4096, true);
        } finally {
            // IOUtils can not close both the input and the output properly in a finally
            // as we can get an exception in between opening the stream and calling the method
            os.close();
        }
{code}","18/Mar/14 18:31;julienledem;same comment as 1. from Cheolsoo
otherwise, this looks good to me.","18/Mar/14 18:38;aniket486;Thanks for the review, [~julienledem] and [~cheolsoo]. I have attached revised patch and committed it to trunk!","18/Mar/14 20:09;rohini;Actually I see some issue with this patch. Reopening jira.

   1) Changing os.close() to IOUtils.closeQuietly(os); is not good. You can close the input quietly, but not output especially HDFS outputstream. HDFS can create empty files  without data which can be accessed through NN fine if os.close() failed. We have been bitten by this a lot of time. In internal projects, we delete the file and retry if os.close() failed.  So please let the pig script fail if os.close() failed rather than causing unexpected behavior.

   2) addFileToClassPath is already doing  file.toUri().getPath(). I don't see where the hadoop bug is coming from. 

http://svn.apache.org/viewvc/hadoop/common/branches/branch-1.0/src/mapred/org/apache/hadoop/filecache/DistributedCache.java?revision=1206848&view=markup

{code}
public static void addFileToClassPath
           (Path file, Configuration conf, FileSystem fs)
        throws IOException {
    String filepath = file.toUri().getPath();
    String classpath = conf.get(""mapred.job.classpath.files"");
    conf.set(""mapred.job.classpath.files"", classpath == null
        ? filepath
        : classpath + System.getProperty(""path.separator"") + filepath);
    URI uri = fs.makeQualified(file).toUri();
    addCacheFile(uri, conf);
  }
{code}","18/Mar/14 20:29;aniket486;Thanks for your comments, [~rohini]. I was not aware of limitations on the HDFS streams, I have attached a patch (PIG-3815-2.patch) to fix those problems.

Hadoop Jira: https://issues.apache.org/jira/browse/MAPREDUCE-2361. Looks like this was fixed here - http://svn.apache.org/viewvc?view=revision&revision=1077790. 

","18/Mar/14 20:40;julienledem;[~rohini] in the code you quoted, don't you think it is putting the port back in the following line?
{noformat}
URI uri = fs.makeQualified(file).toUri();
{noformat}","18/Mar/14 20:40;rohini;Yes. It has been fixed 3 years ago. I am not sure what version of hadoop you are using and hitting this issue. But since we still support 0.20 as well there is no harm in doing .toUri().getPath() in pig as well. 

+1. Since the issue is not with hadoop 1.0, please update your comment when checking in this patch from  ""// PIG-3815 In hadoop 1.0, addFileToClassPath uses : as separator"" to say hadoop 0.20. ","18/Mar/14 21:17;rohini;[~julienledem],
    It is being qualified only to be used in addCacheFile() which sets the mapred.cache.files which is required. conf.set(""mapred.job.classpath.files"") uses just the file path after removing scheme and port.",18/Mar/14 21:24;aniket486;I have committed PIG-3815-2.patch to trunk! Thanks everyone for your comments.,19/Mar/14 05:26;aniket486;I just realized that there is a better way to refactor this code. Can someone review the patch attached?,"19/Mar/14 15:54;cheolsoo;Ha, that looks a lot better to me. +1.","19/Mar/14 19:13;aniket486;Thanks [~cheolsoo], I committed it to trunk.",,,,,,,,,,,,,,,,
Rank column is assigned different uids everytime when schema is reset,PIG-3813,12701409,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,cheolsoo,ssatish,ssatish,14/Mar/14 04:17,15/Apr/14 20:45,14/Mar/19 03:08,18/Mar/14 23:17,0.12.0,,,,,0.12.1,0.13.0,impl,,,0,,,,,,,"When the following script is run, pig goes into an infinite loop. This was reproduced on pig trunk as of March 12, 2014 on apache hadoop 1.2. test_data.txt has been attached. 

test.pig
tWeek = LOAD '/tmp/test_data.txt' USING PigStorage ('|') AS (WEEK:int, DESCRIPTION:chararray, END_DATE:chararray, PERIOD:int);

gTWeek = FOREACH tWeek GENERATE WEEK AS WEEK, PERIOD AS PERIOD;

pWeek = FILTER gTWeek BY PERIOD == 201312;

pWeekRanked = RANK pWeek BY WEEK ASC DENSE;

gpWeekRanked = FOREACH pWeekRanked GENERATE $0;
store gpWeekRanked into 'gpWeekRanked';
describe gpWeekRanked;
---------------------------------------------------

The res object of class Result, gets its value from leaf.getNextTuple()
This gets an empty tuple 
() 
with STATUS_OK.

SO the while(true) condition never gets an End of Processing (EOP) and so does not exit. 
 
",,,,,,,,,,,,,,,,,,,,17/Mar/14 22:44;cheolsoo;PIG-3813-1.patch;https://issues.apache.org/jira/secure/attachment/12635181/PIG-3813-1.patch,18/Mar/14 20:44;cheolsoo;PIG-3813-2.patch;https://issues.apache.org/jira/secure/attachment/12635395/PIG-3813-2.patch,14/Mar/14 04:20;ssatish;test_data.txt;https://issues.apache.org/jira/secure/attachment/12634638/test_data.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-03-17 01:57:57.871,,,no_permission,,,,,,,,,,,,,379755,,,,,Fri Mar 28 16:21:55 UTC 2014,,,,,,,0|i1tflj:,380040,,,,,,,,,,"17/Mar/14 01:57;cheolsoo;[~ssatish] helped me to reproduce the issue at the meet-up. Thanks!

This seems like a column pruner bug. The query runs fine if I add {{SET pig.optimizer.rules.disabled ColumnMapKeyPrune}}.

Assigning to me.



","17/Mar/14 22:44;cheolsoo;Attaching a patch. Here is what I found-

# PushUpFilter moves up the filter by (pWeek) to above the foreach (gTWeek).
# Since pWeek is a direct predecessor of the rank (pWeekRanked), the PushUpFilter changes the output schema of pWeekRanked from rank_pWeek (uid:18) to rank_gTWeek (uid:24).
# After this, ColumnPruneVistor visits the 2nd foreach (gpWeekRanked). It checks whether the column $0 exists in the input schema of gpWeekRanked, which is equivalent to the output schema of pWeekRanked as follows-
{code}
if (!inputUids.contains(project.getFieldSchema().uid))
    innerLoadsToRemove.add(innerLoad);
{code}
The problem is that rank_pWeek (uid:18) is no longer in the input schema of gpWeekRanked, but rank_gTWeek (uid:24) is. So ColumnPruneVistor removes the column $0 from gpWeekRanked.
# Now gpWeekRanked generates empty tuples.

The attached patch changes the condition in ColumnPruneVistor as follows-
{code}
LogicalSchema.LogicalFieldSchema fs = project.findReferent().getSchema().getField(project.getColNum());
if (!inputUids.contains(fs.uid)) {
    innerLoadsToRemove.add(innerLoad);
}
{code}
Basically, it uses the schema of the referent instead of that of the field itself in LOGenerate. This way, any changes in its predecessor are reflected correctly.

I have never worked on this area of code. Please let me know if this fix is not proper.","18/Mar/14 18:40;daijy;Seems LORank is generating the wrong uid. One thing missing in LORank is to keep generated uid across session, so a schema reset will not erase the old uid. We can refer to LOCogroup.groupKeyUidOnlySchema to fix LORank.","18/Mar/14 19:06;cheolsoo;[~daijy], thank you for the comment. Let me try your suggestion.",18/Mar/14 20:44;cheolsoo;Here is the 2nd patch that keeps track of the uid of the rank column.,18/Mar/14 20:46;cheolsoo;Updating the title to describe the root cause.,18/Mar/14 22:55;daijy;+1,"18/Mar/14 23:17;cheolsoo;Committed to trunk.
Thank you Daniel for reviewing the patch!
Thank you Suhas for reporting the issue!",18/Mar/14 23:42;ssatish;Thanks for the quick turn around Cheolsoo,"27/Mar/14 16:52;cheolsoo;I'd like to backport this into branch-0.12. Please let me know if you object, or I'll commit it in 0.12 tomorrow. Thanks!",27/Mar/14 17:28;rohini;We should certainly backport this to 0.12.1 as it is a major bug for Rank operator.,27/Mar/14 17:32;prkommireddi;+1 to backport,27/Mar/14 18:16;suhassatish;+1,28/Mar/14 16:21;cheolsoo;Committed to 0.12 branch.,,,,,,,,,,,,,,
Several clean-ups in PigStats and JobStats ,PIG-3812,12701392,Bug,Closed,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,cheolsoo,cheolsoo,14/Mar/14 00:48,07/Jul/14 18:07,14/Mar/19 03:08,18/Mar/14 00:23,0.13.0,,,,,0.13.0,,,,,0,,,,,,,"While implementing counter support in Pig-on-Tez (PIG-3603), I made several changes to PigStats and JobStats classes. To avoid breaking backward compatibility between 0.13 and 0.13+, I'd like to commit this patch into trunk prior to 0.13 release.

The changes include-
# Move getHdfsBytes\[Read|Written\]() from MRPigStatsUtil to PigStatsUtil since these are not MR specific.
# Move \[MAP|REDUCE\]_\[IN|OUT\]PUT_RECORDS from MRPigStatsUtil to PigStatsUtil since Tez MRInput and MROutput also use them.
# Fix a typo in JobStats#getAvgREduceTime(): REduce -> Reduce.
# Fix white spaces.",,,,,,,,,,,,,,,,,,,,14/Mar/14 01:09;cheolsoo;PIG-3812-1.patch;https://issues.apache.org/jira/secure/attachment/12634604/PIG-3812-1.patch,17/Mar/14 16:03;cheolsoo;PIG-3812-2.patch;https://issues.apache.org/jira/secure/attachment/12635104/PIG-3812-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,379738,,,,,Tue Mar 18 00:23:08 UTC 2014,,,,,,,0|i1tfhr:,380023,,,,,,,,,,14/Mar/14 01:09;cheolsoo;Uploading the patch. I am going to run full unit tests now.,"17/Mar/14 16:03;cheolsoo;RB link-
https://reviews.apache.org/r/19298/",18/Mar/14 00:23;cheolsoo;Committed to trunk. Thank you Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,,,,
